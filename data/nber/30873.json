{
    "id": 30873,
    "citation_title": "Rationalizable Learning",
    "citation_author": [
        "Andrew Caplin",
        "Daniel J. Martin",
        "Philip Marx"
    ],
    "citation_publication_date": "2023-01-23",
    "issue_date": "2023-01-19",
    "revision_date": "None",
    "topics": [
        "\n",
        "Microeconomics",
        "\n",
        "Economics of Information",
        "\n",
        "Behavioral Economics",
        "\n"
    ],
    "program": [
        "\n",
        "Technical Working Papers",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nThe central question we address in this paper is: what can an analyst infer from choice data about what a decision maker has learned? The key constraint we impose, which is shared across models of Bayesian learning, is that any learning must be rationalizable. To implement this constraint, we introduce two conditions, one of which refines the mean preserving spread of Blackwell (1953) to take account for optimality, and the other of which generalizes the NIAC condition (Caplin and Dean 2015) and the NIAS condition (Caplin and Martin 2015) to allow for arbitrary learning. We apply our framework to show how identification of what was learned can be strengthened with additional assumptions on the form of Bayesian learning.\n\n",
    "acknowledgement": "\nWe thank participants in the Sloan-NOMIS Summer School on Cognitive Foundation of Economic Behavior for helpful comments. Caplin thanks the Alfred P. Sloan Foundation and the NOMIS Foundation for their support for the broader research program on the cognitive foundations of economic behavior. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}