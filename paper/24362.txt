NBER WORKING PAPER SERIES

THE TAIL THAT KEEPS THE RISKLESS RATE LOW
Julian Kozlowski
Laura Veldkamp
Venky Venkateswaran
Working Paper 24362
http://www.nber.org/papers/w24362

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
February 2018

We thank Jonathan Parker, Marty Eichenbaum and Mark Gertler for helpful comments and
suggestions. The views expressed herein are those of the authors and do not necessarily reflect
the views of the National Bureau of Economic Research.
At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w24362.ack
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2018 by Julian Kozlowski, Laura Veldkamp, and Venky Venkateswaran. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.

The Tail that Keeps the Riskless Rate Low
Julian Kozlowski, Laura Veldkamp, and Venky Venkateswaran
NBER Working Paper No. 24362
February 2018
JEL No. E43,E44,G01,G14
ABSTRACT
Riskless interest rates fell in the wake of the financial crisis and have remained low. We explore a
simple explanation: This recession was perceived as an extremely unlikely event before 2007.
Observing such an episode led all agents to re-assess macro risk, in particular, the probability of
tail events. Since changes in beliefs endure long after the event itself has passed, perceived tail
risk remains high, generates a demand for riskless, liquid assets, and continues to depress the
riskless rate. We embed this mechanism in a simple production economy with liquidity
constraints and use observable macro data, along with standard econometric tools, to discipline
beliefs about the distribution of aggregate shocks. When agents observe an extreme, adverse
realization, they re-estimate the distribution and attach a higher probability to such events
recurring. As a result, even transitory shocks have persistent effects because, once observed, the
shock stays forever in the agents' data set. We show that our belief revision mechanism can help
explain the persistent nature of the fall in the risk-free rates.
Julian Kozlowski
New York University
19 W. 4th Street - 6th Floor
New York, NY 10012
kozjuli@nyu.edu
Laura Veldkamp
Stern School of Business
New York University
44 W Fourth Street,Suite 7-77
New York, NY 10012
and NBER
lveldkam@stern.nyu.edu

Venky Venkateswaran
Stern School of Business
New York University
7-81 44 West 4th Street
New York, NY 10012
and NBER
vvenkate@stern.nyu.edu

1

Introduction

Interest rates on safe assets fell sharply during the 2008 financial crisis. This is not particularly
surprising: there are many reasons, from an increased demand for safe assets to monetary
policy responses, why riskless rates fall during a period of financial turmoil. However, even
after financial markets calmed down, this state of affairs has persisted. In fact, by 2017, several
years after the crisis, government bond yields still show no sign of rebounding. In Figure 1, we
show the change in longer term government yields in a number of countries since the financial
crisis. Looking at longer term rates allows us to abstract from transitory monetary policy and
interpret the graph as evidence of a persistent decline in the level of riskless interest rates.
Of course, the decline in interest rates following the financial crisis took place in the context
of a general downward trend in real rates since the early 1980’s. Obviously, this longer-run
trend cannot be attributed to the financial crisis. Instead, it may come from a gradual change
in expectations following the high inflation in the 1970’s, or a surge in savings from emerging
markets seeking safe assets to stabilize their exchange rates. This longer run trend taking place
in the background is hugely important, but distinct from our question. We seek to explain the
fact that interest rates fell (relative to this long-run trend) during the financial crisis and failed
to rebound.

Figure 1: Low Interest Rates Are Persistent. Change in percentage points of 10 -year government
bond yield since July 3, 2006. A similar pattern emerges, even if we control for inflation. Source: NYT June
28, 2016.

We explore a simple explanation for this phenomenon: before 2008, no one believed that
a major recession sparked by financial crisis with market freezes, failure of major banks etc.
could happen in the US. The events in 2008 and 2009 taught us that this is more likely than we
thought. Today, the question of whether the financial crisis might repeat itself arises frequently.
Although we are no longer on the precipice, the knowledge we gained from observing 2008-09
2

stays with us and reshapes our beliefs about the probability of large adverse shocks. This
persistent increase in perceived tail risk makes safe, liquid assets more valuable, keeping their
rates of return depressed for many years. The contribution of this to paper is to measure how
much tail risk rose, explain why it remains elevated, and quantitatively explore its consequences
for riskless interest rates.
At its core is a simple theory about how agents form beliefs about the probability of rare, tail
events. Our agents do not know the distribution of shocks hitting the economy and use macro
data and standard econometric tools to estimate the distribution, in a flexible, non-parametric
way. Transitory shocks have persistent effects on beliefs because, once observed, the shocks
remain forever in the agents’ data set. Then, we embed our belief formation mechanism into a
standard production economy with liquidity constraints. When we feed a historical time-series
of macro data for the post-war US into our model, and let our agents re-estimate the distribution
from which the data is drawn each period, our belief revision mechanism goes a long way in
explaining the persistent post-crisis decline in government bond yields since 2008-09.
The link between heightened tail risk and rates of return on in the model comes from two
intuitive mechanisms. First, the increase in consumption risk makes safe assets more valuable,
lowering the required return on riskless government bonds. The second stems from the fact
that government bonds also provide liquidity services that are particularly valuable in very bad
states. Intuitively, in those states, the liquidity available from other sources falls. The main
contribution of this paper is to combine these standard forces with the aforementioned theory
of beliefs in a simple, tractable and empirically disciplined framework and show that rare events
like the 2008-09 recession generate large, persistent drops in riskless interest rates.
130
128
126
124
122
120
118
116
114
112
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016

110

Years

Figure 2: The SKEW Index.
A measure of the market price of tail risk on the S&P 500, constructed using option prices. Source: Chicago
Board Options Exchange (CBOE). 1990:2016.

Apart from being quantitatively successful, our explanation is also consistent with other
evidence of heightened “tail risk’.’ For example, in their VAR analysis, Del Negro et al. (2017)
3

find that most of the decline in riskless rates is attributable to changes in the value of safety and
liquidity. From 2007 to 2017, they estimate a 52 basis point change in the convenience yield of
US Treasury secruties (which is about 80% of the estimated drop in the natural riskless real rate
over the same time period). A second piece of evidence comes from the SKEW index, an optionimplied measure of tail risk in equity markets. Figure 2 shows a clear rise since the financial
crisis, with no subsequent decline.1 In our quantitative analysis, we show the implied changes
in tail probabilities are roughly in line with the predictions of our calibrated model. Finally,
popular narratives about the stagnation emphasize a change in “attitudes" or “confidence," that
we capture with belief changes, and the reductions in debt financing that result:
“[Y]ears after U.S. investment bank Lehman Brothers collapsed, triggering a global
financial crisis and shattering confidence worldwide, ... ‘The attitude toward risk is
permanently reset.’ A flight to safety on such a global scale is unprecedented since
the end of World War II.” (Huffington Post Oct.6, 2013)
In many macro models, including belief-driven ones, deviations of aggregate variables from
trend inherit the exogenously specified persistence of the driving shocks.2 Therefore, these
theories cannot explain why interest rates have remained persistently low. In our setting,
when agents repeatedly re-estimate the distribution of shocks, persistence is endogenous and
state-dependent. Extreme events, like the recent crisis, are rare and therefore, lead to significant
belief changes (and through them, aggregate variables like riskless rates) that outlast the events
themselves. More ‘normal’ events, e.g. milder downturns, on the other hand, show up relatively
more frequently in the agents’ data set and therefore, additional realizations have relativel small
effects on beliefs. In other words, while all changes to beliefs are in a sense long-lived, rarer
events induce larger, more persistent belief changes and interest rate responses. Rare event
beliefs are more persistent because rare event data is scarce. It takes many observations without
a rare event to convince an observer that the event is much more rare than they thought.
This mechanism for generating persistent responses to transitory shocks is simple to execute
and can be easily combined with a variety of sophisticated, quantitative macro models to
introduce persistent effects of rarely observed shocks. While our focus in this paper is on
interest rates, it could be applied to other phenomena, including labor force participation rates,
corporate debt issuance and cash hoarding, house prices, export decisions and trade credit. The
1

This index is constructed from market prices of out-of-the-money options on the S&P 500. See
http://www.cboe.com/micro/skew/introduction.aspx. It is designed to mimic movements in the skewness of
risk-adjusted skewness – a higher level indicates a more negatively skewed distribution. Note that this is different from the VIX, which measures implied volatility, i.e. the second moment. The VIX rose dramatically in
the immediate aftermath of the crisis, but came down quite sharply afterwards.
2
See, for example, Angeletos and La’O (2013) and Maćkowiak and Wiederholt (2010). Backus et al. (2015)
analyze propagation in business cycle models.

4

crucial ingredients of the model are some non-linearity – typically, a constraint that binds in
bad states – some actions that compromises efficiency in the current state, but hedges the risk
of this binding state, and then a large, negative shock. If those ingredients are present, then
adding agents who learn like econometricians is likely to induce sizeable, persistent responses.
Since the novel part of the paper is using this belief formation mechanism to explore interest
rates, Section 2 starts by examining the belief-formation mechanism in a simple context. We
construct a time series of ‘quality’ shocks to US non-residential capital and use it to show
how our non-parametric estimation works. Agents estimate the underlying distribution of the
shocks by fitting a kernel density function to the data in their information set. When they
see the extreme negative realizations from the financial crisis, it raises their estimate of large
negative outcomes. More importantly, this effect is persistent. The theoretical underpinning
of the persistence is the martingale property of beliefs. Intuitively, once observed, an event
stays in agents’ data set and informs their probability assessment, even after the event itself
has passed. Decades later, the probability distribution still reflects a level of tail risk that is
higher than it was pre-crisis. Knowing that a crisis is possible influences risk assessment for
many years to come.
We embed this mechanism in a standard production economy with a liquidity friction. Every
period, in addition to their usual production, firms have access to an additional investment
opportunity. However, in order to exploit this opportunity, they need liquidity in the form of
pledgable collateral. Both capital and government bonds act as collateral, but only a fraction
of the former’s value can be pledged. An adverse shock lowers the value of pledgable capital
and therefore, liquidity.
Section 5 presents our quantitative results. We perform two sets of exercises. The first
involves long-run predictions under the assumption that crises continue to occur. Specifically,
we simulate long-run outcomes (i.e. stochastic steady states) drawing shocks from the updated
beliefs. Our calibrated model predicts that the increase in tail risk is associated with a 1.45%
drop in interest rates on government bonds in the long run. Most of this drop can be attributed
to the liquidity mechanism. The modest degree of risk aversion in our calibration implies that
the increase in consumption risk by itself induces only a very small change in interest rates.
We also show that the implications of the model for changes in equity market variables – such
as the equity premium, tail risk implied by options – line up reasonably well with the data.
Next, we generate time paths for the economy under the assumption that the financial crisis
we saw in 2008-09 was a one-off event and will never recur again. Then, the economy eventually
returns to its pre-crisis stochastic steady state, but we show that this occurs at a very slow rate.
Even after several years, interest rates on safe assets remain depressed. Intuitively, learning
about rare events is in a sense ‘local’: probabilities in the tail respond sharply to extreme
5

realizations, but only slowly to realizations from elsewhere in the support. As a result, it takes
a very long period without extreme events to convince agents that they can be safely ignored.
Finally, to demonstrate that belief revisions are key to the model’s ability to generate sustained
drops in interest rates, we also generate counterfactual time paths with the belief mechanism
turned off. In other words, we endow agents with knowledge of the true distribution from the
very beginning. We find that the initial impact of the shock on interest rates is similar, but they
start to rebound almost immediately and return to the pre-crisis levels at a much faster rate.
In other words, without changes to beliefs, the financial crisis would induce a fairly transitory
fall in interest rates.
Comparison to the literature Our paper speaks to a large body of work that focuses on
the macroeconomic consequences of beliefs3 . Most, if not all, these papers focus on uncertainty
(or second moment changes) and perhaps more importantly, rely on exogenous assumptions
about persistence of shocks for propagation. Essentially, beliefs about time-varying states are
only persistent to the extent that the underlying states are assumed to be persistent. 4 Our
mechanism, on the other hand, generates persistence endogenously and helps explain why
some recessions have long-lasting effects, while others do not. A second advantage of our
contribution is that, by tying beliefs to observable data, we are able to impose considerable
empirical discipline on the role of belief revisions, a key challenge for this whole literature.
The non-parametric belief formation process specified in this paper is similar to other adaptive learning approaches. Kozlowski et al. (2017) use a similar belief formation mechanism to
explain persistence in real output fluctations. That paper, however, abstracts from liquidity, an
important amplification mechanism and therefore, cannot match the large decline in the riskless
rate. In constant gain learning (Sargent, 1999), agents combine last period’s forecast with a
constant times the contemporaneous forecast error. Such a process gives recent observations
more weight, similar to the behavior of agents in Malmendier and Nagel (2011), following the
Great Depression. The reason why we use a non-parametric belief formation process is that we
want to model time-varying changes in perceived tail risk, which requires a richer specification
of the distribution of state variables.
Our belief formation process also has similarities to the parameter learning models by Johannes et al. (2015) and Orlik and Veldkamp (2014) and is advocated by Hansen (2007).
Similarly, in least-square learning (Marcet and Sargent, 1989) agents have bounded rationality
3

These include papers on news shocks, such as, Beaudry and Portier (2004), Lorenzoni (2009), Veldkamp
and Wolfers (2007), uncertainty shocks, such as Jaimovich and Rebelo (2006), Bloom et al. (2014), Berger et al.
(2017), Nimark (2014) and higher-order belief shocks, such as Angeletos and La’O (2013) or Huo and Takayama
(2015).
4
For example, in Moriera and Savov (2015), learning about a hidden two-state Markov process with exogenously known persistence changes demand for shadow banking (debt) assets.

6

and use past data to estimate the parameters of the law of motion for the state variables. However, these papers do not have meaningful changes in tail risk and do not analyze the potential
for persistent effects on interest rates. Pintus and Suda (2015) embed parameter learning in
a production economy, but feed in persistent leverage shocks and explore the potential for
amplification when agents hold erroneous initial beliefs about persistence. Sundaresan (2015)
generates persistence by deterring information acquisition. Weitzman (2007) shows the parameter uncertainty about the variance of a thin-tailed distribution can help resolve many of the
asset pricing puzzles confronted by the rational expectations paradigm.
Finally, our paper contributes to a growing literature on low interest rates. Recent contributions include Hall (2017), Barro et al. (2014), Bernanke et al. (2011), Carvalho et al. (2016),
Caballero et al. (2016), Bigio (2015) and Del Negro et al. (2017). To this body of work, we add
a novel mechanism, one which predicts persistent drops in riskless interest rates in response to
rare, transitory shocks and demonstrate its quantitative and empirical relevance.

2

How Belief Updating Creates Persistence

The main contribution of this paper is to explain why tail risk fluctuates and show how an
extreme event like the Great Recession can induce a persistent drop in riskless rates. Before
laying out the whole model, we begin by explaining the novel part of the paper – how agents
form beliefs and the effect on tail events on beliefs. This will highlight the broader insight –
that unusual events induce larger and more persistent belief changes. Later, we layer on top
the economic model, to show how this mechanism affects interest rates.
The story that this model formalizes is that, before the financial crisis hit, most people in the
US thought that such crises only happened elsewhere – e.g. in emerging markets – and that bank
runs were a topic for historians. Observing the events of 2007-09 changed those views. Many
journalists, academics and policy makers now routinely ask whether the financial architecture is
stable. But formalizing this story requires a departure from the standard rational expectations
paradigm, where the distributions of all random events are assumed to be known. Then,
observing an unusual event should not change one’s probability assessment of that event in the
future. Instead, we need a machinery that allows agents to not know the true distribution, so
that upon seeing something they thought shouldn’t happen, they can revise their beliefs. There
are many ways to depart from full knowledge of distributions. One that is realistic, quantifiable
and tractable is treating agents like classical econometricians. The agents in our model have
a finite data set – the history of all realized shocks – and they estimate the distribution from
which those shocks are drawn, using tools from a first year econometrics class.
Obviously, learning models are not new to the macro literature. A common approach is
7

to assume a normal distribution and estimate its mean and variance. However, the normal
distribution has thin tails, making it less useful to think about changes in the risk of extreme
negative realizations. We could choose an alternative distribution with more flexibility in higher
moments. However, this will raise obvious concerns about the sensitivity of results to the specific
functional form used. To minimize such concerns, we take a non-parametric approach and let
the data inform the shape of the distribution.
Instead, our agents take all the data they have observed and use a kernel density procedure
to estimate the probability distribution from which these data were draw. One of most common
approaches in non-parametric estimation, essentially, a kernel density takes a histogram of all
observed data and draws a smooth line over that historgram. There are a variety of ways to
smooth the line. The most common is called the normal kernel. It does not result in normal
(Gaussian) distributions. We also studied a handful of other kernels and (sufficiently flexible)
parametric specifications, which yielded similar results.5 The kernel density approach allows
for flexibility in the shape of the distribution, while strictly tying the learning process to data
that we, as economists, can observe. We do not need to guess or calibrate the precision of some
signal. Instead, we take a macro data series, apply this econometric procedure to it, and read
off the agents’ beliefs.
Next, we describe the Gaussian kernel. Consider the shock φt whose true density g is
unknown to agents in the economy. The agents do know that the shock φt is i.i.d. The
information set at time t, denoted It , includes the history of all shocks φt observed up to and
including t. They use this available data to construct an estimate ĝt of the true density g.
Formally, at every date t, agents construct the following normal kernel density estimator of the
pdf: g


nt −1
φ − φt−s
1 X
Ω
ĝt (φ) =
nt κt s=0
κt
where Ω (·) is the standard normal density function, κt is the smoothing or bandwidth parameter
and nt is the number of available observations of at date t. As new data arrives, agents add the
new observation to their data set and update their estimates, generating a sequence of beliefs
{ĝt }.
Finally, back to the main point: Why does this estimated distribution change in such a
persistent way, in response to a tail event? First, we’ll explain this graphically, and then
mathematically. Figure 3 shows three panels. In the left panel, is the histogram of a data
5

Other kernels we explored included other non-parametric kernels like Epinechnikov, kernels designed to
better capture tail risk like Champernowne, as well as semi-parametric kernels with Pareto tails and the gand-h family which covers several transformations of the normal distribution. Each alternative yielded similar
economic predictions because new data increased the tail probabilities of each distribution in a similar way. For
a detailed discussion of nonparametric estimation, see Hansen (2015).

8

Figure 3: The Persistence of Estimated Probabilities
Data in the histograms are capital quality shocks, measured as described in Section 4. Kernel densities constructed
with the normal kernel in (3) and the optimal bandwidth.

series. In this case, the data series happens to a some measures of capital quality, which we
describe in detail later. For right now, this is an arbitrary sequence of data, generated from an
unknown distribution. The smooth line over the histogram is the estimated normal kernel. The
second panel shows what happens when two data points are obseved that are negative outliers.
The location of the two new observations is highlighted in the histogram in red. Notice that
the new kernel estimator, and thus agents beliefs, now place greater probability weight on the
possibility of future negative outcomes. If next period, the state returns to normal, those two
red data points are still in the histogram and still create the bump on the left. Although the
tail event has passed, tail risk remains elevated. The right panel adds 30 additional years of
additional observations, drawn to look just like the preceding years, except without any crisis
events. The kernel on the right still shows a left bump. Smaller than it was before, but still
present, elevated tail risk still persists 30 years after the tail event was observed.
The persistence in Figure 3 has its origins in the so-called martingale property of beliefs –
i.e. conditional on time-t information (It ), the estimated distribution is a martingale. Thus, on
average, the agent expects her future belief to be the same as her current beliefs. This property
holds exactly if the bandwidth parameter κt is set to zero6 . In our empirical implementation,
6

As κt → 0, the CDF of the kernel converges to Ĝ0t (φ) =
h
Et

Ĝ0t+j

i

"

(φ) It = Et

h
i
Et Ĝ0t+j (φ) It =

1
nt + j

1
nt

ntX
+j−1

Pnt −1
s=0

1 {φt−s ≤ φ}. Then, for any φ, j ≥ 1
#

1 {φt+j−s ≤ φ} It

s=0

j
nt
Ĝ0 (φ) +
Et [ 1 {φt+1 ≤ φ}| It ]
nt + j t
nt + j

Thus, future beliefs are, in expectation, a weighted average of two terms - the current belief and the distribution
from which the new draws of the data φt are made. Whenh shocks are also
i drawn from using the current belief
0
distribution, the two terms are exactly equal, implying Et Ĝt+j (φ) It = Ĝ0t (φ) .

9

in line with the literature on non-parametric assumption, we use the optimal bandwidth7 . This
leads to a smoother density but also means that the martingale property does not hold exactly.
Numerically, the deviations are minuscule for our application. In other words, the kernel density
estimator is, for all practical purposes, a martingale Et [ ĝt+j (φ)| It ] ≈ ĝt (φ).
Now, in the simulations underlying the right panel in Figure 3 above, we drew future shock
sequences from the pre-crisis distribution (i.e. ĝ2007 instead of the revised belief ĝ2009 ). This
implies that beliefs will revert, i.e. the bump in the left tail will eventually disappear. However,
the rate at which this occurs is very slow. This has to do with the fact that under our nonparametric approach, outlier observations play a crucial role in learning about the frequency
of tail events. Ordinary events are just not very informative about those tail probabilities.
And since data on tail events is scarce, observing one makes the resulting belief revisions large
and extremely persistent (even if they are ultimately transitory). It is worth pointing out this
slow convergence need not necessarily obtain with a parametric specification of the learning
process. For example, suppose there is uncertainty about the standard deviation of a thinktailed distribution, as in Weitzman (2007). Since all realizations are informative about standard
deviations, the effect of observing a tail event is more muted (since there is a lot more relevant
data) and relatively less persistent (convergence to the true distribution occurs at a faster rare).

3

Model

Preferences and Technology The economy is populated by a representative firm, which
produces output with capital and labor, according to a standard Cobb-Douglas production
function:
Yt = AKtα Nt1−α ,

(1)

where A is total factor productivity (TFP), which is the same for all firms and constant over
time. The firm is subject to an aggregate shock to capital quality φt : formally, it enters the
period with capital K̂t and is hit by a shock φt , leaving it with ‘effective’ capital Kt :
Kt = φt K̂t .

(2)

These capital quality shocks are i.i.d. over time and are the only aggregate disturbances in
our economy. The i.i.d. assumption is made in order to avoid an additional, exogenous, source
of persistence.8 . They are drawn from a distribution g(·): this is the object agents are learning
7
8

See Hansen (2015).
The i.i.d. assumption also has empirical support. In the next section we use macro data to construct a time

10

about.
As we see from equation (2), these shocks scale up or down the effective capital stock. Of
course, this is not to be interpreted literally – it is hard to visualize shocks that regularly wipe
out fractions of the capital or create it out of thin air. Instead, these shocks are a simple
if imperfect, way to model the extreme and unusual effect on the 2008-’09 recession on the
economic value and returns to non-residential capital. It allows us to capture the idea that
a hotel built in 2007 in Las Vegas may still be standing after the Great Recession, but may
deliver much less economic value. The use of such shocks in macroeconomics and finance goes
back at least to Merton (1973), but they have become more popular more recently (precisely in
order to generate large fluctuations in the returns to capital), e.g. in Gourio (2012), as well as
in a number of recent papers on financial frictions, crises and the Great Recession (e.g., Gertler
et al. (2010), Gertler and Karadi (2011), Brunnermeier and Sannikov (2014)).
Finally, the firm is owned by a representative household, whose preferences over consumption
Ct and labor supply Nt are given by a flow utility function U (Ct , Nt ), along with a constant
discount rate β.
Liquidity: We now introduce liquidity considerations, which will act as an amplification
mechanism for tail risk changes. We model them in a stylized but tractable specification in the
spirit of Lagos and Wright (2005): firms have access to a productive opportunity, but require
liquidity in the form of pledgeable collateral order to exploit it. As in Venkateswaran and
Wright (2014), capital and riskless government bonds both can be pledged, albeit to different
degrees. Bonds are fully pledgable, but only a fraction of the effective capital can be used as
collateral. An increase in tail risk now has an additional effect – it reduces the liquidity value of
capital, increasing the demand for an alternate source of liquidity, namely riskless government
bonds, amplifying the interest rate response.
Formally, at the beginning of each period, firms can invest in a project: which costs Xt and
yields a payoff H (Xt ) (both denominated in the single consumption/investment good). The
function H is assumed to be strictly increasing and concave, which implies that the net surplus
from the project, namely H(X) − X has a unique maximum at X ∗ . In the absence of other
constraints, therefore, every firm presented with this opportunity will invest X ∗ . However, the
firm faces a liquidity constraint
Xt ≤ Bt + ηKt
In other words, the investment in the project cannot exceed the sum of pledgeable collateral –
which comprises a fraction η of its effective capital Kt and the value of its liquid assets (riskless
series for φt . We estimate an autocorrelation of 0.15, statistically insignificant.

11

government bonds) Bt 9 Therefore,
Xt = min (X ∗ , Bt + ηKt )
After this stage, production takes place according to equation (1).
Timing and value functions: The timing of events in each period t is as follows:
1. Firm enters the period with a capital stock K̂t and liquid assets Bt .
2. The aggregate capital quality shock φt is realized.
3. Firm chooses Xt subject to the liquidity constraint.
4. Firm chooses labor and production takes place.
5. Firm chooses capital and liquid asset positions for t + 1.
Denoting the aggregate state by St (described in detail later in this section), the economywide wage rate by Wt , the price of the riskless bond by Pt and stochastic discount factor Mt+1 ,
we can write the problem of the firm in recursive form as follows:
V (Kt , Bt , St ) =

H (Xt ) − Xt + F (Kt , Nt ) − Wt Nt + Kt (1 − δ) + Bt

max
Xt ,Nt ,Bt+1 ,K̂t+1

−Pt Bt+1 − K̂t+1 + βEt Mt+1 V (Kt+1 , Bt+1 , St+1 )

s.t.

(3)

Xt ≤ Bt + ηKt ,
Kt+1 = φt+1 K̂t+1 .

The stochastic discount factor Mt+1 and the wage Wt are determined by the marginal utility
of the representative household, i.e.
U2 (Ct , Nt )
,
U1 (Ct , Nt )
U1 (Ct+1 , Nt+1 )
.
=
U1 (Ct , Nt )

Wt = −
Mt+1

(4)
(5)

The aggregate state St consists of (Πt , It ) where Πt ≡ H(Xt ) − Xt + AKtα L1−α
+ (1 − δ)Kt is
t
the aggregate resources available and It is the economy-wide information set. Standard market
9

It is straightforward to allow for some unsecured debt – this has a negligible effect on our results.

12

clearing conditions yield
Ct = Πt − K̂t+1 ,

(6)

Bt = B̄ .

(7)

where B̄ is the exogenous supply of the riskless government bond. The interest expenses on
these bonds is financed through lumpsum taxes.
Information, beliefs and equilibrium The set It includes the history of all shocks φt
observed up to and including time-t. For now, we specify a general function, denoted Ψ, which
maps It into an appropriate probability space. The expectation operator Et is defined with
respect to this space. In the following section, we make this more concrete using the kernel
density estimation procedure to map the information set into beliefs.
For a given belief function Ψ, a recursive equilibrium is a set of functions for (i) aggregate
consumption and labor supply that maximize household utility subject to a budget constraint,
(ii) bond price that clears the market for bonds (iii) firm values and policies that solve (3),
taking as given the stochastic discount factor and wages according to (4)-(5) as well as the
bond price and (iv) aggregate consumption and labor are consistent with individual choices
and the bond market clears.
Characterization and solution The equilibrium of the economic model is a solution to a
set of non-linear equations, namely the optimality conditions of the firm and household, along
with resource constraints. The optimality conditions of the firm (3) are:
1 = βEt {Mt+1 φt+1 [F1 (Kt+1 , Nt+1 ) + 1 − δ + ηµt+1 ]}
Pt = βEt {Mt+1 (1 + µt+1 )}
µt = H 0 (Xt ) − 1

(8)
(9)
(10)

Wt = F2 (Kt , Nt )

(11)

where µt is the Lagrange multiplier on the liquidity constraint. The first two equations are
the Euler equation for capital and liquid assets respectively. The value of liquidity services is
reflected on the right hand side (in the term involving µt ). The third equation characterizes µt .
In states of the world where liquidity is sufficiently abundant, Xt = X ∗ and µt = 0. Otherwise,
µt > 0. The expectation of µt+1 (weighted by the SDF Mt+1 ) raises the price of the liquid bond
Pt , or equivalently, lowers the riskfree rate. An increase in tail risk, i.e. the likelihood of large
adverse realizations of φt+1 makes the constraint more likely to bind and therefore, raises the
13

liquidity premium on the riskless bond.
Belief Formation Next, we choose a particular estimation procedure for how agents form
beliefs. Specifically, we employ the kernel density estimation procedure, which we described in
Section 2.
Consider the shock φt whose true density g is unknown to agents in the economy. The
agents do know that the shock φt is i.i.d. The information set at time t, denoted It , includes
the history of all shocks φt observed up to and including t. They use this available data to
construct an estimate ĝt of the true density g. Formally, at every date t, agents construct the
following normal kernel density estimator of the pdf: g


nt −1
φ − φt−s
1 X
Ω
ĝt (φ) =
nt κt s=0
κt
where Ω (·) is the standard normal density function, κt is the smoothing or bandwidth parameter
and nt is the number of available observations of at date t. As new data arrives, agents add the
new observation to their data set and update their estimates, generating a sequence of beliefs
{ĝt }.

4

Measurement and Calibration

In this section, we describe how we use macro data to construct a time series for φt and pin
down beliefs. One of the key strengths of our belief-driven theory is that, by assuming that
agents form beliefs as an econometrician would, we can use observable data to discipline beliefs.
We also parameterize the model to match key features of the US economy and describe key
aspects of our computational approach.

4.1

Measuring capital quality shocks

To construct a time series of φt , we follow the approach in Kozlowski et al. (2017). It uses
data on non-financial assets in the US economy, reported in the Flow of Funds tables, both at
V
historical cost, which we will denote N F AHC
as well as at market value, N F AM
. The latter
t
t
series corresponds to the nominal value of effective capital, Kt in the model. Letting Xt−1
denote investment in period t − 1 and Ptk the nominal price of capital goods in t, the two time

14

series can be mapped into their model counterparts as follows:
V
Ptk Kt = N F AM
t
k
k
V
Pt−1
K̂t = (1 − δ)N F AM
t−1 + Pt−1 Xt−1
HC
V
− (1 − δ) N F AHC
= (1 − δ)N F AM
t−1
t−1 + N F At

To adjust for changes in nominal prices, we use the price index for non-residential investment
from the National Income and Product Accounts (denoted P IN DXt ).10 This allows us to
recover the quality shock φt
φt =
=

Kt
K̂t


=

Ptk Kt

!

k
Pt−1
K̂t

V
(1 − δ)N F AM
t−1
Pk

k
Pt−1
Ptk



V
N F AM
t
+ N F AHC
− (1 − δ) N F AHC
t
t−1



k
P IN DXt−1
P IN DXtk


(12)

P IN DX k

where the second line replaces Pt−1
with P IN DXt−1
k
k .
t
t
Using the measurement equation (12) (and a value for δ = 0.03), we construct an annual
time series for capital quality shocks for the US economy since 1950, plotted in the left panel
of Figure 4. For most of the sample period, the shock realizations are in a relatively tight
range around 1, but at the onset of the recent Great Recession, we saw two large adverse
realizations: 0.93 in 2008 and 0.84 in 2009. To put these numbers in context, the mean and
standard deviation of the series from 1950-2007 were 1 and 0.03 respectively.
We then apply our kernel density estimation procedure to this time series to construct a
sequence of beliefs. In other words, for each t, we construct {ĝt } using the available time series
until that point. The resulting estimates for two dates – 2007 and 2009 – are shown in the
right panel of Figure 4. They show that the Great Recession induced a significant increase in
the perceived likelihood of extreme negative shocks. The estimated density for 2007 implies
almost zero mass below 0.90, while the one for 2009 attach a non-trivial (approximately 2.5%)
probability to this region of the state space.

4.2

Calibration

We begin by specifying the functional form
of preferences
and technology. The period utility


1+γ

C− N1+γ

1−σ

. The risk aversion parameter σ is set to
function of the household is U (C, N ) =
1−σ
√
0.5. The payoff from the project is H(X) = 2ζ X − ξ. The labor supply parameter, γ, is set to
10

Our results are robust to alternative measures of nominal price changes, e.g. computed from the price index
for GDP or Personal Consumption Expenditure.

15

1.2

40
2007
2009

Density

1.1

1

0.9

0.8
1950

1960

1970

1980

1990

2000

30

2

20

1

10

0
0.8

0
0.8

2010

0.85

0.85

0.9

0.9

0.95

1

1.05

1.1

Figure 4: Capital quality shocks.
The left panel shows the time series of φt measured from the US data using (12). The right panel shows the
estimated kernel densities in 2007 (solid) and 2009 (dashed) respectively. The change in left tail shows the effect
of the Great Recession.

0.5, corresponding to a Frisch elasticity of 2 in line with Midrigan and Philippon (2011). The
labor disutility parameter π are normalized to 1. The parameter ξ acts like a fixed cost and
separates the liquidity premium (which is a function only of H 0 (X)) from the level of the net
surplus, a flexibility that proves helpful in the calibration process.
A period is interpreted as a year. Accordingly, we choose the discount factor β = 0.95 and
depreciation δ = 0.03. The share of capital in the production is set to 0.40, while the TFP
parameter A is normalized to 1.
Next, we turn to the liquidity-related parameters. The parameter governing the pledgability
of capital, η, is set to match the ratio of short-term obligations of US non-financial corporations
to the capital stock in the Flow of Funds. Short term obligations comprise commercial paper
(row 27, Table B.103), bank loans (row 31) and trade payables (row 34). Capital stock is the
market value of non-financial assets (row 2). This ratio stood at 0.16 in 200711 .
There are 3 other parameters to be determined: the supply of liquid assets, B̄ and the
technology parameters ζ and ξ. These are chosen to jointly target the following moments:
(i) the ratio of liquid asset holdings12 of US nonfinancial corporations, which stood at 0.09
in 2007 (ii) an interest rate of 2% on government bonds (which corresponds to the pre-crisis
average for real interest rates in the US) and (iii) a capital-output ratio of 3.5. In the model,
the analogous objects are averages in the stochastic steady state under the pre-crisis belief
distribution. Though this calibration is done jointly, a heuristic argument can be made for
identification – the first moment is informative about B̄, the second about ζ and the third
helps us pin down ξ. Table 1 summarizes the resulting parameter choices.
11

Calibrating to the average values during 1950-2007 yields almost identical results.
Liquid assets are defined as total financial assets (row 7, Table B.103, the Flow of Funds) less long-term
financial assets (rows 21 through 24 of the same table).
12

16

Parameter
Preferences:
β
γ
π
σ
Technology:
α
δ
Liquidity:
η
B̄
ζ
ξ

Value

Description

0.95
0.50
1
0.5

Discount factor
1/Frisch elasticity
Labor disutility
Risk aversion

0.40
0.06

Capital share
Depreciation rate

0.16
4.93
3.93
9.00

Pledgability of capital
Supply of liquid assets
Investment technology (affects liquidity)
Investment fixed cost
Table 1: Parameters

5

Results

Our main goal in this section is to quantify the size and persistence of the response of riskfree
rates to a large but transitory shock φt in an economy where agents are learning about the
distribution. We begin by computing the stochastic steady state associated with ĝ2007 , the
distribution estimated using pre-crisis data13 . Then, starting from this steady state, we subject
the model economy to the two adverse realizations observed in 2008 and 2009, namely 0.93 and
0.84. As we saw in the previous section, this leads to a revised estimate for the distribution,
ĝ2009 which shows an increase in perceived tail risk.
We perform two exercises to demonstrate the quantitative bite of our belief revision mechanism. First, we compare the stochastic steady states implied by the two distributions, ĝ2007
and ĝ2009 , both for aggregate macroeconomic quantities (like output, capital and labor) as well
as for asset prices. This corresponds to the long-term behavior of the US economy under the
assumption that crises continue to occur with the same likelihood as the updated beliefs (formally, if future shocks are drawn from the post-crisis distribution ĝ2009 ). Second, we simulate
time paths for the economy under the assumption that there are no future crises, i.e. with
future shocks drawn from the pre-crisis distribution ĝ2007 . In other words, we assume that the
2008-09 recession was a one-off adverse realization. As a result, beliefs will eventually revert to
their pre-crisis levels. However, the effects of the tail events in 2008-09 on beliefs (and therefore,
aggregate outcomes) turn out to be quite persistent and remain significant over a relatively long
13

The steady state is obtained by simulating the model for 1000 periods using the ĝ2007 and the associated
policy functions, discarding the first 500 observations and time-averaging across the remaining periods.

17

horizon.
Long-run analysis The results from the first exercise, where we compare long-run averages
under ĝ2007 and ĝ2009 , are reported in Table 2. As the table shows, the rise in tail risk causes
the economy to invest and produce less, leading to lower output and capital. This occurs
because investing now has a lower mean return but is also significantly riskier. The change in
beliefs leads to a sharp drop in the riskfree rate – in the new steady state, government bonds
yield almost 1.3% lower. There are two forces which contribute to this drop. First, future
consumption is riskier, which has the usual effect of lowering the required return on riskfree
claims. Second, the liquidity premium rises. This is in part because there is less liquidity in
the economy (due to the lower levels of capital in the new steady state) but also due to the
increase in liquidity risk. A tail event also implies states with very low levels of liquidity, which
translates into a higher premium on the liquid asset.

ln F (K, N )
ln X
ln K
Riskless rate (Rf ), in %
Return on capital (Rv ) in %
Premium (Rv − Rf ) in %

ĝ2007
2.39
2.68
4.10
2.31
5.30
2.99

ĝ2009
2.36
2.65
4.06
0.86
5.29
4.43

Change
-0.03
-0.03
-0.04
-1.45
-0.01
1.44

Table 2: Steady State Interest Rates and Macro Aggregates, Pre- and Post-Crisis
Rf is the interest rate on government bonds, while Rv is the average expected returns on un-levered claims to
the firm.

How do these predictions compare to the post-2008 data? Table 3 compares the drop in
interest rates predicted by the model (the first row in the table) to different measures of changes
in riskfree rates since the Great Recession. The second row reports the change from 2007 to
2017 in short-term real rate. This is defined as the difference between 1-year nominal Treasury
yield (taken from the H15 release) and 1-year expected inflation from the Federal Reserve Bank
of Cleveland’s inflation forecasting model. The next three rows contain estimates of changes
in longer term real rates. The second row shows the change in the 5y real rates, 5 years
forward. To estimate this, we use the nominal 5y rate, 5 years forward (computed from the
constant maturity nominal Treasury yield curve) and the corresponding expected inflation (i.e.
the expected 5-year inflation rate, 5 years forward, which can be computed using the 5y and
10y expected inflation series from the Federal Reserve Bank of Cleveland). The third reports
the change in the HP-trend component of the 5y5y real rate (computed using annual data
from 1982-2017 with a smoothing parameter of 6). The fourth row shows the change in the
18

estimate of the long-run natural rate from Del Negro et al. (2017)14 , who use a flexible VAR
specification to extract the permanent component of the real interest rate from data on nominal
bond returns, inflation, and their long-run survey expectations (from the Survey of Professional
Forecasters). Taken together, the show that belief revisions can go long a way in explaining the
drop in interest rates since the financial crisis.
Change, %
Model
Riskless rate, Rf
Data
1-year real rate
5-year real rate, 5 years forward
5-year real rate, 5 years forward (HP trend)
Natural real rate (from Del Negro et al. (2017))
Liquidity premium (from Del Negro et al. (2017))

-1.45
-2.48
-1.57
-1.78
-0.66
0.52

Table 3: Interest rates, Model and Data
The change in the Data panel are differences between average levels in 2017 and 2007. See text for details.

For macroeconomic quantities, the predicted drops in Table 2 generally underpredict the deviations from pre-crisis trends observed in the data. For example, at the end of 2017, output was
about 14% below the 1952-2007 trend. This suggests a need for additional amplification mechanisms. In our related earlier work in Kozlowski et al. (2017), we explore two such mechanisms
– Epstein-Zin utility (which allows us to separate risk aversion and intertemporal elasticity of
substitution) and defaultable debt (higher tail risk makes default debt less attractive, curtailing
borrowing and investment) – and show that they help bring the model’s predictions much closer
to the data. Here, given our focus on interest rates, we abstract from these modifications. This
allows us to highlight, in a more transparent fashion, the interaction of tail risk with liquidity
considerations.
Role of liquidity To understand the role played by liquidity, we repeat the analysis above
setting the pledgability of capital to 0. This implies that shocks to capital do not directly affect
the available liquidity in the economy (since bonds are the only liquid asset in the economy).
The remaining parameters are calibrated using the same strategy as before. The results are
shown in Table 4. The table shows that, without liquidity effects, the increase in tail risk has a
very small effect on the riskless rate. The interest rate on government bonds in the new steady
state is only 2 bps lower. In other words, almost all of the drop in our baseline analysis comes
14

We thank the authors of that paper for sharing their estimates with us.

19

from the interaction of tail risk and liquidity15 . This finding is consistent with that of Del Negro
et al. (2017), who find that most of the change in the natural real rate comes from a rise in
the convenience yield associated with US government bonds. Their VAR estimate16 is reported
in the last row of Table 3 (labeled Liquidity Premium) – the change in convenience yield since
2007 constitutes almost 80% of the drop in real rates.
ĝ2007
ln F (K, N )
2.27
ln X
1.29
ln K
3.93
f
Riskless rate (R ) in %
2.31
Risky return (Rv ) in %
5.28
v
f
Risk premium (R − R ) in % 2.97

ĝ2009
2.19
1.29
3.80
2.29
5.27
2.98

Change
-0.09
0.00
-0.13
-0.02
-0.01
0.01

Table 4: Interest rates and macro aggregates in the long-run, without liquidity effects
Rf is the interest rate on government bonds, while Rv is the average expected returns on un-levered claims to
the firm.

Comparing the implications for macroeconomic aggregates in Table 4 to Table 2 shows that
liquidity dampens the effect of increased tail risk on capital and output (the predicted drops
in Table 4 are smaller). Intuitively, when capital also provides liquidity, an increase in tail
risk induces a precautionary response – firms hold more capital to buffer against the drop in
liquidity due to an adverse shock. As a result, steady state capital (and therefore, output) do
not fall by as much as they would have in the absence of liquidity considerations.
Evidence from equity markets Our model stays relatively close to the standard neoclassical paradigm and inherits many of its limitations when it comes to matching asset pricing
facts, particularly asset price volatility.17 With that caveat in mind, we confront the model’s
predictions for equity markets with the data in Table 5.
In order to do this, we interpret equity as a levered claim on the value of the firm in the
model. The main role of leverage is to amplify the volatility of equity returns. We use a leverage
(defined as the ratio of debt to total assets) of 0.8. This is higher than most estimates in the
15

This is in part due to the low levels of risk aversion in our parameterization. In Appendix A, we repeat this
analysis with higher risk aversion (specifically, σ = 2 and σ = 10). Then, tail risk has a somewhat larger effect
on interest rates, even in the absence of liquidity.
16
They add the spread between Baa corporate bonds and Treasuries to their VAR to identify the convenience
yield component.
17
However, the model actually implies a sizable equity premium even in the pre-2008 steady state. This
stems almost entirely from liquidity considerations, which drive down the required return on government bonds
relative to all illiquid assets, e.g. equity. This is essentially the mechanism in Lagos (2010), who shows that
a model with liquidity considerations can help rationalize many asset pricing anomalies, including the equity
premium puzzle.

20

literature – e.g. Kozlowski et al. (2017) use 0.7, an estimate which combines both operating
and financial leverage. We discuss the reasons behind the higher leverage assumption later.
The implications of the model for various equity market variables are shown in Table 5
below. The increase in tail risk leads to a slight fall in the expected return on equity claims
in the new steady state. Since rates on riskless assets drop significantly, this implies a big rise
in the equity premium. In data, expected returns on the S&P 500 – computed following the
methodology18 of Cochrane (2011) and Hall (2015) – also show a small drop relative to pre-crisis
levels. The small drop in expected returns also means that the model-implied value of equity
claims (per unit capital) is actually higher in the new steady state. Of course, in the data, we
observe a much larger run-up in equity prices over the last few years. We are not claiming that
the model can rationalize such a large increase – but it is worth noting that increased tail risk
does not necessarily imply a precipitous fall in valuations.

Return on equity, E(Re ) (%)
ln Equity/Capital
E(Re − R̄e )3
Pr(Re − R̄e ≤ −0.30)

Changes
Model
Data
−0.065 −0.184
0.010
0.225
−0.002 −0.002
0.022
0.015

Table 5: Implications for equity markets
The model changes represent the difference between the average value under ĝ2009 and that under ĝ2007 . The
change in the data is the difference between the average value from 2013 through 2017 and the pre-crisis average
(from 2005 to 2007).

Of course, evidence on returns and valuations are best very indirect measures of tail risk.
We therefore turn to options prices, arguably a better source for evidence on changes in tail
risk. The model, even with the relatively high leverage adjustment, does not generate sufficient
variability in equity returns. The model-implied value for VIX under the pre-2008 beliefs is 8.37
(the average from 1990-2007 in the data was 19). Furthermore, in the data, the VIX spiked
in the immediate aftermath of the crisis, averaging 32 during 2008-09 but then fell sharply
to historically low levels in 2017. The model, on the other hand, predicts a more modest
but persistent increase (from 8.37 to 11.35). The SKEW index reported by the CBOE is a
18

The one-year ahead forecast of returns is obtained using a regression where the left-hand variable is the
one-year real return on the S&P and the right hand variables are a constant, the log of the ratio of the S&P
at the beginning of the period to its dividends averaged over the prior year, and the log of the ratio of real
consumption to disposable income in the month prior to the beginning of the period.

21

transformation of the standardized third moment, i.e.
SKEWt = 100 − 10

E(Re − R̄e )3
.
(V IXt /100)3

Since this is a function of the VIX, the model’s difficulty in matching the time variation in the
VIX also spill over to the SKEW index. For example, the SKEW spiked in part due to the
shapr drop in VIX. Fixing these issues, i.e. matching the levels and time variation in volatility
measures would require adding more shocks – almost certainly with heteroskedastic processes
– and mechanisms to address the well-known excess volatility puzzles, an exercise beyond the
scope of the current paper. Instead, we use the two reported indices to construct two indicators
of tail risk – namely, the non-standardized third moment of the risk-neutral distribution (the
numerator in the second term of SKEW equation above) and the (risk-neutral) probability of
an extreme negative return realization (defined as 30% below the mean)19 . As the table shows,
the model predicts significant increases in both objects. These predictions line up reasonably
well with the changes in their empirical counterparts relative to their pre-crisis levels. In other
words, while the model cannot exactly match the time path of asset market variables, the
evidence from asset markets appears to be broadly consistent with the idea that tail risk rose
sharply since 2008.
What if there are no more crises? Next, we compute time paths for riskless interest
rates, starting from the average long-run values under ĝ2007 . These paths are generated using
two different assumptions about future shocks. The first corresponds to the stochastic steady
state analysis from earlier and draws shocks from the updated belief distribution, ,ĝ2009 . The
second assumes that crises do not recur, i.e. the shock sequences drawn from ĝ2007 . For
each sequence of shocks, we compute beliefs, equilibrium prices and quantities at each date.
Finally, we average over all these paths and plot the mean change in interest rates (relative
to the starting level) in the two panels of Figure 5 (the solid line). It shows that, under both
assumptions, they remain depressed for a prolonged period. In the no-crisis version (the right
panel), while the economy eventually returns to its pre-crisis stochastic steady state, learning
about tail probabilities is sufficiently slow that interest rates are almost 1% lower 20 years after
the crisis. This occurs because learning about tail events is “local” under our non-parametric
approach: beliefs about the likelihood changes by a lot when such events are observed, but
are less responsive to realizations elsewhere in the support of the distribution. In contrast, if
we imposed a parametric assumption (say, a normal distribution), then all realizations contain
information about parameters (the mean and the variance) and so beliefs (and therefore, interest
19

Details of the computation are in Appendix B.

22

rates) would converge back to their pre-crisis levels relatively quickly.
With crisis

No more crisis

0

0

-0.01

-0.01

-0.02

-0.02
Learning
No Learning
Data

-0.03

-0.03

-0.04

-0.04
2010

2015

2020

2025

2030

2010

2015

2020

2025

2030

Figure 5: Risk free rate.
The left panel (with crisis) shows the change in the risk free rate when the data generating process is ĝ2009 . The
right panel (no more crisis) is an identical model in which future shocks are drawn from ĝ2007 . The solid blue
line of both panels show the solution when agents update their beliefs and the dashed green line shows the model
under no learning. The red circles show changes in 1y real rates from the US data for the period 2008-2017.

Turning Off Belief Updating To demonstrate the central role of learning, we also plot
average simulated outcomes from an otherwise identical economy where agents know the final
distribution ĝ2009 with certainty, from the very beginning (dashed line in Figure 5). These agents
do not revise their beliefs. This corresponds to a standard rational expectations econometrics
approach, where agents are assumed to know the true distribution of shocks hitting the economy
and the econometrician estimates this distribution using all the available data. The post-2009
paths are simulated as follows: each economy is assumed to be at its stochastic steady state
in 2007 and is subjected to the same sequence of shocks – two large negative ones in 2008 and
2009. After 2009, the sequence of shocks is drawn from the estimated 2009 distribution.
In the absence of belief revisions, the negative shock causes the real rate to surge and then
recover. The rise in the interest rate arises because, as the economy is recovering back to the
previous steady state, there is a lower demand for debt.20 This shows that learning is what
generates long-lived reductions in economic activity.
20

Since the no-learning economy is endowed with the same end-of-sample beliefs as the learning model, they
both ultimately converge to the same levels. But they start at different steady states (normalized to 0 for each
series).

23

6

Conclusion

No one knows the true distribution of shocks to the economy. Economists typically assume
that agents in their models do know this distribution as a way to discipline beliefs. For many
applications, assuming full knowledge has little effect on outcomes and offers tractability. But
for outcomes that are sensitive to tail probabilities, the difference between knowing these probabilities and estimating them with real-time data can be large. In this paper, we present one
such application: the effect of large, unusual events on riskless interest rates rates.
The central mechanism is that observing tail events like the Great Recession leads agents
to assign higher likelihood to such events going forward. Importantly, this change in beliefs is
relatively persistent, even if crises never recur. As a result, assets which are safe and liquid,
such as government bonds become more valuable.
When we quantify this mechanism and use capital price and quantity data to directly estimate beliefs, the model predicts large, persistent drops in interest rates similar to the observed
decline in government yields in the years following the Great Recession. These results suggests
that perhaps persistently low interest rates took hold because, after seeing how fragile our
financial sector is, market participants will never think about tail risk in the same way again.

24

References
Angeletos, G.-M. and J. La’O (2013). Sentiments. Econometrica 81 (2), 739–779.
Backus, D., A. Ferriere, and S. Zin (2015). Risk and ambiguity in models of the business cycle.
Journal of Monetary Economics forthcoming.
Backus, D. K., S. Foresi, and L. Wu (2008). Accounting for biases in black-scholes. SSRN
Working Paper Series.
Bakshi, G., N. Kapadia, and D. Madan (2003). Stock return characteristics, skew laws, and the
differential pricing of individual equity options. Review of Financial Studies 16 (1), 101–143.
Barro, R. J., J. Fernández-Villaverde, O. Levintal, and A. Mollerus (2014). Safe assets. Technical
report, National Bureau of Economic Research.
Beaudry, P. and F. Portier (2004). An exploration into pigou’s theory of cycles. Journal of
Monetary Economics 51, 1183–1216.
Berger, D., I. Dew-Becker, and S. Giglio (2017). Uncertainty shocks as second-moment news
shocks. Technical report, National Bureau of Economic Research.
Bernanke, B. S., C. C. Bertaut, L. Demarco, and S. B. Kamin (2011). International capital
flows and the return to safe assets in the united states, 2003-2007.
Bigio, S. (2015). Endogenous liquidity and the business cycle. American Economic Review .
Bloom, N., M. Floetotto, N. Jaimovich, I. Sapora-Eksten, and S. Terry (2014). Really uncertain
business cycles. NBER working paper 13385.
Brunnermeier, M. and Y. Sannikov (2014). A macroeconomic model with a financial sector.
American Economic Review 104 (2).
Caballero, R. J., E. Farhi, and P.-O. Gourinchas (2016). Safe asset scarcity and aggregate
demand. American Economic Review 106 (5), 513–18.
Carvalho, C., A. Ferrero, and F. Nechio (2016). Demographics and real interest rates: Inspecting
the mechanism. European Economic Review 88, 208–226.
Cochrane, J. H. (2011). Presidential address: Discount rates. The Journal of Finance 66 (4),
1047–1108.

25

Del Negro, M., D. Giannone, M. P. Giannoni, and A. Tambalotti (2017). Safety, liquidity, and
the natural rate of interest. Brookings Papers on Economic Activity 2017 (1), 235–316.
Gertler, M. and P. Karadi (2011). A model of unconventional montary policy. Journal of
Monetary Economics 58, 17–34.
Gertler, M., N. Kiyotaki, et al. (2010). Financial intermediation and credit policy in business
cycle analysis. Handbook of monetary economics 3 (3), 547–599.
Gourio, F. (2012). Disaster risk and business cycles. American Economic Review 102(6),
2734–66.
Hall, R. E. (2015, October). High discounts and high unemployment. Hoover Institution,
Stanford University.
Hall, R. E. (2017). Low interest rates: Causes and consequences. International Journal of
Central Banking.
Hansen, B. E. (2015). Econometrics.
Hansen, L. (2007). Beliefs, doubts and learning: Valuing macroeconomic risk. American
Economic Review 97(2), 1–30.
Huo, Z. and N. Takayama (2015). Higher order beliefs, confidence, and business cycles. University of Minnesota working paper.
Jaimovich, N. and S. Rebelo (2006). Can news about the future drive the business cycle?
American Economic Review 99(4), 1097–1118.
Johannes, M., L. Lochstoer, and Y. Mou (2015). Learning about consumption dynamics.
Journal of Finance forthcoming.
Kozlowski, J., L. Veldkamp, and V. Venkateswaran (2017). The tail that wags the economy:
Belief-driven business cycles and persistent stagnation. New York University working paper.
Lagos, R. (2010). Asset prices and liquidity in an exchange economy. Journal of Monetary
Economics 57 (8), 913–930.
Lagos, R. and R. Wright (2005). A unified framework for monetary theory and policy analysis.
Journal of Political Economy 113 (3), 463–484.
Lorenzoni, G. (2009). A theory of demand shocks. American Economic Review 99 (5), 2050–84.

26

Maćkowiak, B. and M. Wiederholt (2010). Business cycle dynamics under rational inattention.
Review of Economic Studies 82 (4), 1502–1532.
Malmendier, U. and S. Nagel (2011). Depression babies: do macroeconomic experiences affect
risk taking? The Quarterly Journal of Economics 126 (1), 373–416.
Marcet, A. and T. J. Sargent (1989). Convergence of least squares learning mechanisms in
self-referential linear stochastic models. Journal of Economic theory 48 (2), 337–368.
Merton, R. (1973). An intertemporal capital asset pricing model. Econometrica 41 (5), 867–887.
Midrigan, V. and T. Philippon (2011). Household leverage and the recession.
Moriera, A. and A. Savov (2015). The macroeconomics of shadow banking. New York University
working paper.
Nimark, K. (2014). Man-bites-dog business cycles. American Economic Review 104(8), 2320–
67.
Orlik, A. and L. Veldkamp (2014). Understanding uncertainty shocks and the role of the black
swan. Working paper.
Pintus, P. and J. Suda (2015). Learning financial shocks and the great recession. Aix Marseille
working paper.
Sargent, T. J. (1999). The conquest of American inflation. Princeton University Press.
Sundaresan, S. (2015). Rare events and the persistence of uncertainty. Imperial College working
paper.
Veldkamp, L. and J. Wolfers (2007). Aggregate shocks or aggregate information? costly information and business cycle comovement. Journal of Monetary Economics 54, 37–55.
Venkateswaran, V. and R. Wright (2014). Pledgability and liquidity: A new monetarist model
of financial and macroeconomic activity. NBER Macroeconomics Annual 28 (1), 227–270.
Weitzman, M. L. (2007). Subjective expectations and asset-return puzzles. American Economic
Review 97 (4), 1102–1130.

27

Appendix
A

Role of risk aversion

In Table 6, we show how higher risk aversion translates into a larger sensitivity of interest to tail risk, even in
the absence of liquidity effects.

σ=2
Riskless rate (Rf ) in %
σ = 10
Riskless rate (Rf ) in %

ĝ2007

ĝ2009

Change

2.31

2.23

-0.08

2.31

1.67

-0.64

Table 6: Interest rates in the long-run, without liquidity effects

B

Computing Option-implied Tail Probabilities

To compute tail probabilities, we follow Backus et al. (2008) and use a Gram-Charlier expansion of the distribution function. The CBOE also follows this method in their white paper on the SKEW Index to compute implied
probabilities. This yields an approximate density function for the standardized random variable, ω = x−µ
σ :
"

3ω − ω 3
f (ω) = ϕ (ω) 1 − γ
6

#



where

x−µ
γ=E
σ

3

where ϕ (ω) is the density function of a standard normal random variable and γ is the skewness.21

21
The Gram-Charlier expansion also includes a term for the excess kurtosis, but is omitted from the expansion
because, as shown by Bakshi et al. (2003), it is empirically not significant.

28

