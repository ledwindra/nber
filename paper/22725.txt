NBER WORKING PAPER SERIES

LEARNING AND EARNING:
AN APPROXIMATION TO COLLEGE VALUE ADDED IN TWO DIMENSIONS
Evan Riehl
Juan E. Saavedra
Miguel Urquiola
Working Paper 22725
http://www.nber.org/papers/w22725

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
October 2016

For useful comments we thank Joseph Altonji, Caroline Hoxby, Kevin Stange, and participants of
the NBER conference on the productivity of higher education. All remaining errors are our own. The
authors acknowledge financial support from the NBER conference on the productivity of higher education
and no additional funding sources. The views expressed herein are those of the authors and do not
necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
¬© 2016 by Evan Riehl, Juan E. Saavedra, and Miguel Urquiola. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including ¬© notice, is given to the source.

Learning and Earning: An Approximation to College Value Added in Two Dimensions
Evan Riehl, Juan E. Saavedra, and Miguel Urquiola
NBER Working Paper No. 22725
October 2016
JEL No. I23,J24,J44
ABSTRACT
This paper explores the implications of measuring college productivity in two different
dimensions: earning and learning. We compute system-wide measures using administrative data
from the country of Colombia that link social security records to students‚Äô performance on a
national college graduation exam. In each case we can control for individuals‚Äô college entrance
exam scores in an approach akin to teacher value added models. We present three main findings:
1) colleges‚Äô earning and learning productivities are far from perfectly correlated, with private
institutions receiving relatively higher rankings under earning measures than under learning
measures; 2) earning measures are significantly more correlated with student socioeconomic
status than learning measures; and 3) in terms of rankings, earning measures tend to favor
colleges with engineering and business majors, while colleges offering programs in the arts and
sciences fare better under learning measures.
Evan Riehl
Columbia University
420 W 118th St
New York, NY 10027
eer2131@columbia.edu
Juan E. Saavedra
Dornsife Center for Economic
and Social Research
University of Southern California
635 Downey Way
Los Angeles, CA 90089
and NBER
juansaav@usc.edu

Miguel Urquiola
Columbia University
SIPA and Economics Department
1022 IAB, MC 3308
420 West 118th Street
New York, NY 10027
and NBER
msu2101@columbia.edu

1. Introduction
Colleges produce outputs in various dimensions. Parents and students, for instance, care
about colleges‚Äô ability to place graduates on good career trajectories. As a result, the U.S.
and other countries now provide information on the labor market earnings of graduates from
various colleges and majors.1 A drawback of such measures is that they typically do not
adjust for ability; some colleges might perform better, for instance, simply because they
attract more able students.
The earnings dimension, however, is not the only one that parents, students, and especially
policymakers care about. A second dimension of interest is learning‚Äînamely, the ability of
colleges to enhance human capital and skills. System-wide measures of learning are uncommon, in part because most countries lack nationwide college graduation exams. Questions
remain, therefore, on the extent to which these two dimensions of college productivity relate
to each other‚Äîwhether colleges that improve student earning also improve their learning.
This is the first study to simultaneously analyze system-wide measures of the earning and
learning productivity of colleges. We use data from the country of Colombia to arguablyimprove upon the measures in the literature to date. Our detailed administrative records
provide the earnings of nearly all graduates in the country upon labor market entry. With
these data we can control for a measure of ability‚Äîperformance on a national standardized
admission exam‚Äîand for characteristics related to students‚Äô socioeconomic backgrounds.
Further, the Colombian setting allows us to propose and implement measures of college productivity in the learning dimension, as all graduates are required to take a national college
exit exam. In measuring learning performance we can similarly control for individual characteristics and pre-college ability. In particular, some components of the college exit exam
are also assessed in the entrance exam, enabling us to implement an approach akin to those
commonly used in the teacher value added literature.2 In short, our earning and learning
measures may not fully isolate college value added, but they have advantages relative to
measures previously used in the context of measuring college productivity.
We then show how these measures of college productivity relate to each other and to
characteristics of colleges‚Äô entering classes. This yields three findings. First, we find that
measures of college productivity on earning and learning are far from perfectly correlated.
This implies that college rankings based on earnings differ from those based on learning; in
other words, the colleges that seem to add most to students‚Äô post-graduation earnings are
1

Other countries, such as Chile and Colombia, have similar initiatives. These are relevant in view of evidence
that, at least in some cases, college identity can have a causal impact on graduates‚Äô earnings (e.g., Hoekstra,
2009, Saavedra, 2009, Dale and Krueger, 2014, and MacLeod et al., 2015). This finding is not universal; see
Stange (2012) for contrasting findings among community colleges.
2
See for instance Chetty et al. (2014). Our empirical approach is also closely related to the one in Saavedra
and Saavedra (2011), discussed below.
2

not necessarily the ones that add most to their measured learning.3 For instance, we find
that on average the top private schools seem to do relatively better on earning, whereas the
top public institutions perform better on learning.
Second, the measures of earnings productivity are significantly more correlated with student socioeconomic status than the learning measures; not surprisingly, earnings are also
more correlated with colleges‚Äô tuition levels. This leaves open the possibility that learning
measures do a better job of isolating a college‚Äôs contribution to students‚Äô human capital, even
when one focuses on early-career earnings, as we do. For example, learning may be more
easily influenced by factors that colleges can control directly, such as teaching, as opposed
to factors such as parental connections and signaling. Consistent with this, we show that a
college‚Äôs measured performance can vary substantially depending on whether earnings are
measured right after graduation or later in workers‚Äô careers. This illustrates that colleges
have only partial control over the earnings paths of their graduates.
Our third finding is that a college‚Äôs ranking under the earning and learning measures can
differ depending on its mix of majors. We show that the earning measures tend to favor
majors related to engineering, business, and law; more specialized majors, such as those in
fine arts, education, and social/natural sciences, are relatively higher ranked under learning
metrics. Thus if measures like the ones we calculate became salient, they could lead colleges
to make strategic choices on which majors they offer.
Taken together, our findings imply that the design of accountability systems may influence
colleges‚Äô relative performance‚Äîand therefore applicants‚Äô school choices‚Äîas well as colleges‚Äô
responses. Policy makers may wish to keep these implications in mind as they begin to
release more college performance information to the public.
Our study relates to two strands of work on college productivity: that related to learning
and to earning. In terms of learning, a variety of standardized tests exist in the U.S. that
could in principle be used to measure student-learning outcomes. These tests include the
Measure of Academic Proficiency and Progress (MAPP), the Collegiate Assessment of Academic Profiency (CAAP), the Collegiate Learning Assessment (CLA), the California Critical
Thinking Skills Test (CCST), the Watson-Glaser Critical Thinking Appraisal, and the Cornell Critical Thinking Tests (Pascarella and Terenzini, 2005; Sullivan et al., 2012). However,
these tests are not systematically used across the country.
Few studies investigate the extent to which variation in learning value added relates to institutional characteristics. In general, these studies find little systematic relationship between
learning growth and institutional characteristics. Arum and Roksa (2011) use longitudinal
3

With learning measures, a concern often arises regarding whether these capture anything that the market
and therefore students actually value. In the Colombian setting, student performance on the field-specific
component of the exit exam is predictive of student wages, even after controlling for students‚Äô performance
on the admission exam, college reputation, and socioeconomic status.
3

CLA data from students at twenty-three U.S. colleges and find no systematic relationship
between critical thinking value added and institutional characteristics. The Council for Aid
to Education (2013) uses cross-sectional CLA data from students at 158 U.S. colleges to
document how colleges exhibit similar growth of critical thinking skills regardless of ownership status, institution size, Carnegie classification or selectivity. Hagedorn et al. (2009)
use longitudinal data from students in twenty-three U.S. colleges taking the CAAP test and
find that peer composition does not predict critical thinking value added. Saavedra and
Saavedra (2011) use cross-sectional data from an administration of Australia‚Äôs Graduate
Skills Assessment (GSA) to estimate educational value added in a nationally representative
sample of freshmen and seniors at seventeen Colombian colleges.4 After controlling for incoming student characteristics Saavedra and Saavedra (2011) find that private ownership is
related to value added, but that measures of college quality‚Äîlike resources, selectivity, and
reputation‚Äîare not.
Our work also relates to a long and growing literature measuring productivity in higher
education (e.g. Cooke, 1910; Sullivan et al., 2012). For instance, recent system-wide studies
from Norway, the U.S., and Chile that credibly address selection bias using administrative
data find mixed evidence on the labor market payoffs to attending more selective colleges
(Kirkeboen et al., 2016; Hoxby and Bulman, 2015; Hastings et al., 2013). In this volume,
Hoxby (2016) uses administrative data to estimate the productivity of all post-secondary
institutions in the U.S. However, unlike prior studies that credibly address issues of selection
bias, Hoxby (2016) is able to estimate both per-pupil lifetime earnings outcomes and perpupil costs for each institution. She finds that more selective colleges produce higher lifetime
earnings but do so at proportionally higher cost. As a result, among the one thousand most
selective U.S. colleges, there is little relationship between earnings value added per unit of
input and institutional selectivity.
The remainder of the paper is structured as follows. Section 2 presents background on the
Colombian higher education sector, and Section 3 describes our data and sample. Section
4 discusses the computation of our productivity measures, and Section 5 presents results.
Section 6 concludes with broader implications.
2. Background
This section provides background on Colombia‚Äôs higher education system.
2.1. Access to college. In the past decades, Latin American countries have seen a marked
expansion in access to secondary and tertiary education. Access to the latter has actually
risen faster, although from a lower base. As Figure 1 shows, the gap between secondary
4

The GSA, which is most similar to the CLA in the U.S., measures four general skill domains: critical
thinking, problem solving, writing and interpersonal skills.
4

Gross enrollment rate (% of age group)

100
Latin America
SECONDARY

80
Colombia

60
TERTIARY

40

Latin America
Colombia

20
1995

2000

2005

2010

2015

Year

Figure 1. Enrollment trends in Colombia and Latin America
Notes: The data come from the World Bank indicators (http://databank.worldbank.org, consulted on April
7, 2016). The figure plots gross secondary and tertiary enrollment rates for Colombia and the corresponding
aggregate for the Latin America as a whole. Gross secondary enrollment rate is the number of individuals
enrolled in secondary school as a fraction of the total number of individuals 12 to 17 years of age. Gross
tertiary enrollment rate is the number of individuals enrolled in tertiary education as a fraction of the total
number of individuals 18 to 24 years of age.

and tertiary enrollment in the region narrowed from 60 percentage points in 1996 to 50
percentage points by 2013. By this year, about 43 percent of the population had enrolled
in some type of tertiary education. The evolution in Colombia has generally mirrored that
in the rest of the region, although the gap between both types of enrollment has remained
stable at about 45 percentage points.5
Throughout the region, there are constraints for further tertiary expansion. In the case of
Colombia these partially reflect market structure. Private and public providers co-exist, and
while public colleges are significantly subsidized, their capacity is strained. Table 1 shows
that public colleges account for 23 percent of institutions but 52 percent of total tertiary
enrollments.6

5

The salient difference between Colombia and the rest of the region is that secondary rose faster initially
and then stagnated. Tertiary enrollment trends are essentially identical in Colombia and the region as a
whole.
6 Throughout this paper we use the term ‚Äúcolleges‚Äù to refer to both universities and technical institutions,
as depicted in Table 1.
5

Table 1. Colombian higher education market structure
Institutions

Enrollment

Public

Private

Total

Public

Private

Total

Universities

47
0.17

142
0.53

189
0.70

495,855
0.25

799,673 1,295,528
0.40
0.65

Technical schools

15
0.06

65
0.24

80
0.30

524,007
0.27

163,886
0.08

Total

62
0.23

207
0.77

269
1.00

659,142
0.52

601,744 1,983,421
0.48
1.00

687,893
0.35

Notes: Calculations based on the Colombian national higher education information system (SNIES) for
2013, the last year with data available. Enrollment data only includes undergraduate students. The category
‚Äúuniversities‚Äù combines universities and university institutes. Technical schools combines technical institutes,
technological institutes, and the National Job Training Agency (SENA).

There is little regulation on the entry of tuition-charging, unsubsidized private providers,
and these generally offer few financial aid opportunities.7 As a result, private colleges represent 77 percent of all institutions but only 48 percent of total enrollment.
Colleges and universities are also geographically concentrated: 50 percent are in Colombia‚Äôs three largest cities, which account for 26 percent of the population. Bogot√°, the capital,
is home to 31 percent of all colleges. In addition about 75 percent of tertiary students attend
a college in the city of their birth (Saavedra and Saavedra, 2011).
2.2. College entrance exam. To apply to college, Colombian students must take a standardized entrance exam called the Icfes, which is administered by a government agency.8
The Icfes is generally analogous to the SAT in the U.S., but it is taken by the vast majority of high school seniors regardless of whether they intend to apply to college.9 The Icfes
also plays a larger role in admissions in Colombia than the SAT does in the U.S. In addition to using it as an application requirement, many schools extend admission offers based
7

Technically there are no for-profit colleges in Colombia. It is widely perceived, however, that many nonselective private colleges are de facto for-profit, as their owners are the residual claimants of excess revenue
typically distributed through wages, rental charges, investments, etc. In this sense the situation resembles
that which has existed during certain periods in other countries with large private college sectors, such as
Chile.
8 Icfes stands for Institute for the Promotion of Higher Education, the former acronym for the agency that
administers the exam. The Colombian Institute for Educational Evaluation, as it is now called, was created
in 1968 and is a State agency under the authority of the national Ministry of Education. The Icfes exam is
now known as Saber 11¬∞, reflecting the fact that students usually take it in the 11th grade. We use the name
Icfes to match the designation during the period covered by our data.
9
Angrist et al. (2006) and our personal communications with the Colombian Institute for Educational
Evaluation suggest that more than 90 percent of high school seniors take the exam. The test-taking rate is
high in part because the government uses Icfes exam results to evaluate high schools.
6

solely on students‚Äô entrance exam performance. Others consider additional factors like high
school grades while heavily weighting the Icfes, and a handful administer their own exams.
Applications and admissions are major-specific; students apply to a college/major pair.
The Icfes tests multiple subject areas including biology, chemistry, English, math, reading/language arts, social science, philosophy, and physics.
2.3. College exit exam. In 2004 the agency that administers the Icfes introduced, with
considerable publicity, new field-specific college graduation exams. These exit exams are
standardized and administered at every institution that offers a related program.10 The
exams are intended to assess senior students‚Äô competencies in fields ranging from relatively
academic in orientation (e.g., economics and physics) to relatively professional (e.g., nursing
and occupational therapy).
The creation of the exit exams was a major undertaking, as it required coordination
among departments in multiple colleges. The stated intent of this effort was to improve
quality, transparency, and accountability in the higher education sector. Consistent with
this, school-level aggregate scores were made available and have been used by news outlets
as part of college rankings.
Field-specific exams became available for most majors in 2004, with several majors receiving field exams in subsequent years. A few fields such as political science, anthropology,
history, and philosophy never received a corresponding field-specific exam. In part because
of this, for the first few years taking the exit exam was optional, although the majority of
students in tested fields took the exam. This changed in 2009, when the exit exam became
a graduation requirement for all students. A generic test was introduced for majors that did
not previously have a field-specific exam. In addition, from 2009 onward the exam included
several common components in subjects such as English and reading comprehension, which
were taken by all students regardless of their field.
Increasingly, colleges and students use results on the college exit exam as a signal of
ability. For example, students may report whether they obtained a top score nationally, or
their score in comparison to the university or the national average. Some universities use exit
exam results in admissions to graduate programs, and the Colombian Student Loan Institute
offers a postgraduate study credit line (of up to 16,000 dollars) exclusively to the best ten
nationwide scorers. In addition, every year the Colombian President and Education Minister
publicly recognize the individuals with the top ten scores in each field. Anecdotally, the best
scorers receive job offers based on public knowledge of their test scores, and MacLeod et al.
(2015) provide evidence that the exit exams affect graduates‚Äô labor market earnings.
10

These tests were initially labeled Ecaes, which stands for Ex√°menes de Calidad de Educaci√≥n Superior,
i.e., higher education quality exams. They are now called Saber Pro.
7

3. Data and sample
This section describes our sources of data and the sample we use for our analysis.
3.1. Data. We use individual-level administrative datasets from three sources:
(1) The Colombian Institute for Educational Evaluation, which administers the college
entrance and exit exams, provided records for both tests. This includes scores for all
high school seniors who took the entrance exam between 1998 and 2012, as well as
college exit exam scores for all exam takers in 2004‚Äì2011.
(2) The Ministry of Education provided enrollment and graduation records for students
entering college between 1998 and 2012. These include each individual‚Äôs college,
program of study, and enrollment and graduation dates. These data cover roughly
90 percent of all college enrollees; the Ministry omits a number of smaller colleges
due to poor and inconsistent reporting.
(3) The Ministry of Social Protection provided monthly earnings records for formal sector
workers during 2008‚Äì2012. These come from data on contributions to pension and
health insurance funds.
We link these data sources using student names, birthdates, and national ID numbers. The
resulting dataset includes students from nearly all colleges in Colombia, with information on
their entrance exam scores and, if applicable, their exit exam performance and formal labor
market earnings.
3.2. Sample. We select a sample that allows us to cleanly compare measures of college
performance on earning and learning. Thus we set aside other relevant outcomes, such as
graduation, and focus on college graduates for whom we observe both exit exam scores and
formal labor market earnings.
Specifically, we restrict our sample to graduates who satisfy two important criteria. First,
we include only students who took the college exit exam in 2009‚Äì2011. As noted above, the
exit exam was voluntary prior to 2009, so we exclude pre-2009 exam takers to limit selection
into taking the exam. Second, we include only graduates for whom we observe initial labor
market earnings. Since students typically take the exit exam one year before graduating, this
means that we include only 2010‚Äì2012 graduates with earnings observed in their graduation
year.
In addition to these restrictions, we drop individuals with missing values on any of the other
variables we use, including entrance exam scores, high school of origin, mother‚Äôs education,
and previous year‚Äôs tuition.11 This ensures that all performance measures calculated below
11

The entrance exam underwent a major overhaul in 2000, and so we also exclude the small number of
students who graduated in 2010‚Äì2012 but took the entrance exam prior to 2000. Since one of our learning
outcomes below is a student‚Äôs English exit exam score, we additionally drop the fewer than one percent of
8

Table 2. Sample and college types
Mother Entrance
went to
exam
college
pctile

No. of
colleges

No. of
grads

Admit
rate

Annual
tuition

Public (most selective)
Public (medium selective)
Public (least selective)

12
24
12

15,642
13,228
6,063

0.20
0.55
0.87

$369
$509
$535

0.42
0.29
0.23

0.82
0.67
0.59

Top private
Other private (high cost)
Other private (low cost)

8
51
50

9,653
19,229
17,489

0.64
0.82
0.86

$2,584
$1,696
$1,079

0.90
0.59
0.31

0.90
0.72
0.63

157

81,304

0.65

$1,134

0.46

0.72

College type

Total

Notes: Admission rate data are from Colombian national higher education information system (SNIES) and
average over 2007‚Äì2012. Tuition data are from the exit exam records, which report each exam takers‚Äô tuition
in the previous year in six categories. We compute the average across all students using the midpoint of
each category and convert to U.S. dollars using 2012 exchange rates. Entrance exam percentiles are relative
to all exam takers in each year, including those who did not attend college.

are based on the same set of individuals. Lastly, to obtain reasonable precision for each of
our performance measures, we restrict our analysis to colleges that have at least 50 graduates
satisfying the above criteria.
The resulting sample includes approximately 81,000 graduates from 157 colleges. This is
much larger than samples available in previous studies that use longitudinal data to compute
college performance measures (e.g., Klein, Steedle and Kugelmas, 2010). The last row in
Table 2 presents summary statistics on our sample.
3.3. College categorization. Table 2 additionally categorizes colleges into six types with
the aim of providing a useful portrayal of the college market in Colombia. The top three
rows separate public colleges into three groups based on quartiles of their admission rates.
We define the most selective public colleges as those in the quartile with the lowest admission
rates, and the least selective colleges as those in the highest admission rate quartile. Medium
selective colleges are those in the middle two quartiles.12 Table 2 shows that the most selective
public colleges admit 20 percent of their applicants on average, while the least selective are
essentially open enrollment.13
students who took the French or German entrance exams, which were offered until 2006, rather than the
English exam.
12
We use quartiles rather than terciles to define these three groups to provide more detail on colleges at the
extremes of the distribution.
13
Note that non-selective colleges often have admission rates that are slightly less than one in Table 2. This
reflects that students may fail to follow all application procedures or may withdraw their applications before
admission.
9

Selectivity defined by admission rates has limited usefulness in categorizing private colleges
in Colombia, as most private colleges admit nearly all of their applicants. Instead, sorting into
private colleges is defined more strongly by the tuition rates they charge. We therefore define
‚Äútop private‚Äù colleges as those few that are actually selective‚Äîi.e., they reject some of their
applicants‚Äîand in which average annual graduate tuition exceeds the equivalent of about
twenty-five hundred dollars.14 This definition picks out eight colleges which represent the
most elite private schools in the country. We divide the remaining private institutions‚Äîwhich
we label ‚Äúother private‚Äù‚Äîinto two types based on the average tuition payments reported by
their graduates. We define high cost private colleges as those above the median tuition, and
low cost colleges as those below.15
Average annual tuition varies significantly across private college types, with a mean of
roughly one thousand dollars at low cost private colleges. Average tuition is significantly
lower at all public college types, as they offer substantial discounts to low SES students.
The last two columns of Table 2 summarize the socioeconomic and academic backgrounds
of graduates from each college type. Graduates from private colleges are much more likely
to have mothers with a college education. For instance, 90 percent of students at top private
colleges do so. Academic preparation, as defined by each student‚Äôs entrance exam percentile
in the full distribution of test takers, also varies starkly across college types. Average entrance
exam performance is at the 82nd percentile at the most selective public colleges and the 90th
percentile at top private schools. Graduates from the lowest college types, both public and
private, have average entrance exam scores near the 60th percentile.
We use the sample and college categorization in Table 2 for our analysis of college performance measures below.
4. Measures
This section describes the outcome variables we use, and the measures we employ to
approximate college earning and learning productivity.
4.1. Earning and learning variables. Our earnings variable is log average daily formal
labor market earnings, which we calculate by dividing base monthly earnings for pension
contributions by the number of employment days in each month and averaging across the
14

Specifically, we use a four million peso cutoff for top private colleges, and we define their selectivity using
a 2002 report from the Colombian Institute for Educational Evaluation entitled Estad√≠sticas de la Educaci√≥n
Superior. Selective private colleges as those for which the number of applicants exceeded the number of
offered slots, according to this report.
15
We note that we do not use an institution‚Äôs level of training (university or technical, as in Table 1) to
define these six college categories. We find that this distinction provides little additional information on
average college characteristics conditional on the categories defined by financing, selectivity, and tuition.
10

year. We use earnings in the year of each student‚Äôs graduation (2010‚Äì2012) and demean
earnings in each year.
Our learning variables are based on students‚Äô scores on the college exit exam. During the
exam years we analyze (2009‚Äì2011), this test included a field-specific component related to
a student‚Äôs major (e.g., economics or mechanical engineering) as well as several components
taken by all students. We focus on three of these: i) the field-specific score, ii) a reading
common component score, and iii) an English common component score.
These components have different strengths and weaknesses in measuring college productivity. The field exit score, because it typically reflects each student‚Äôs college major, provides
arguably the best measure of the material studied in college. However, in general there is no
direct analog on the entrance exam. The English component of the exit and entrance exams
are very similar and thus well placed to measure progress, but English proficiency may be less
directly related to college productivity. Since the exit and entrance exams include a similar
but not identical reading/language arts component, the reading component lies arguably in
the middle of the comparability and relevance spectrums.
Using these three exit exam scores, we calculate each student‚Äôs percentile relative to all
other students in our sample in the same exam field and cohort. We use exam score percentiles
because the entrance and exit exams are not on a common scale and thus cannot measure
growth in human capital. As a result, our learning measures will capture a college‚Äôs relative
rather than absolute performance. The same caveat applies to our earning measures since
we do not observe a pre-college measure of earnings.
4.2. Calculation of productivity measures. We use four procedures to measure learning
and earning performance. Some of these procedures are simple and require less-detailed
information, and thus they correspond to measures that may be more commonly reported
in the media or easier for policymakers to compute. Other procedures use comprehensive
information on students‚Äô backgrounds and align more closely with ‚Äúvalue-added‚Äù methods
employed in other areas of economic research. These four procedures, which we describe in
the following subsections, allow us to explore the sensitivity of our results to different data
requirements and methodologies.
4.2.1. Raw means. Our first performance measure is the average log earnings, or the average
exit exam percentile, at each college:
(1)

Œ∏c = E{yic |i ‚àà c},

where yic is either outcome for individual i who graduated from college c. We label Œ∏c
the raw means measure, as it implements the simplest and least data-intensive of our four
11

procedures. Note that it does not adjust for differences across colleges in incoming student
characteristics‚Äîi.e., in the student ‚Äúinputs‚Äù to college production.
4.2.2. Entrance exam residuals. Our second performance measure adjusts for differences in
college inputs by controlling for students‚Äô entrance exam performance. We do this through
an individual-level regression of the following form:
(2)

yic = Œ≤ 0 ti + Œ∏ÃÉc + Àúic ,

where ti is a vector of student i‚Äôs entrance exam percentiles on eight components, which include reading/language arts and English.16 We decompose the residual from this regression
into a school-specific term, Œ∏ÃÉc , and an idiosyncratic component, Àúic . Our second college productivity measure, which we call entrance exam residuals, is the Œ∏ÃÉc coefficient from equation
(2).
4.2.3. Entrance exam + SES residuals. Our third performance measure is closely related to
the second, but we include additional controls for students‚Äô socioeconomic background in
regression (3):
(3)

yic = Œ≤ 0 ti + Œ≥ 0 xi + Œ∏ÃÇc + ÀÜic ,

where xi represents dummies for four categories of mother‚Äôs education (primary, secondary,
vocational, university), which are fully interacted with dummies for each of the approximately
six thousand high schools in our sample. The entrance exam + SES residuals measure for
each college is the Œ∏ÃÇc coefficient from this regression. This coefficient is identified from variation in college attendance across students with the same high school and mother‚Äôs education
combination. This measure is most analogous to benchmark ‚Äúvalue added‚Äù models in other
work in economics, which control for a broad array of initial individual characteristics.
4.2.4. College-level residuals. Our fourth performance measure controls for college-level characteristics in addition to individual-level characteristics. This is motivated by research in
Altonji and Mansfield (2014), which shows that in the estimation of group-level treatment
effects, including group average characteristics can in some cases control for between-group
sorting on unobservable individual traits.
We control for both individual and college characteristics using a two-step procedure.
‚àó
First, we estimate equation (3) and calculate residuals yic
from the individual characteristics
16

The other components are biology, chemistry, math, social sciences, philosophy, and physics. As with the
exit exam scores, we convert entrance exam scores into percentiles within each exit exam field and cohort.
12

‚àó
only. That is, we calculate yic
= yic ‚àí Œ≤ÃÇ 0 ti ‚àí Œ≥ÃÇ 0 xi , where Œ≤ÃÇ and Œ≥ÃÇ are the estimated coefficients
from regression (3).17
‚àó
‚àó
|i ‚àà c}, and estimate
for each college, yc‚àó = E{yic
Second, we calculate the mean value of yic
the following college-level regression:

(4)

yc‚àó = Œ≤ 0 tc + Œ≥ 0 xc + Œ∏c ,

where tc is the vector of college mean percentiles for each of the eight entrance exam components, and xc is the fraction of students with a college-educated mother at college c.18 The
college-level residuals measure is the residual from regression (4), Œ∏c . As we discuss below,
this measure has properties that differ from those of measures based on individual residuals
because it is uncorrelated with college mean entrance scores by construction. Altonji and
Mansfield (2014) note that under certain conditions, the variance in Œ∏c also serves as a lower
bound to the true variance of college treatment effects, in part because these treatment
effects are likely correlated with tc and xc .
4.3. Correlations of productivity measures with inputs. For our earning and each of
our three learning variables, the above procedures yield four separate productivity measures‚Äî
in short, 16 measures for each college in our sample. We normalize each of these to have
mean zero and standard deviation one across the 157 colleges. This normalization is convenient because it makes the coefficient from a linear regression of one measure on another
equal to their pairwise correlation coefficient.
To provide context on these measures, we show how they relate to a college characteristic
that is in principle easily observable to many agents: colleges‚Äô mean entrance exam score.
We begin with a graphical exposition using only one learning outcome: the field-specific
exam score. The four panels of Figure 2 depict our four measures for this outcome. The
grey circles are the 157 colleges in our sample. The vertical axis in each panel represents the
learning performance under each measure, while the horizontal axis depicts the raw mean
entrance exam score at each college.19 The solid line depicts the linear relationship between
these two measures, with the slope indicated on the graph.
Panel A shows that the correlation between a college‚Äôs raw mean field exit score (Œ∏c from
equation (1)) and its mean entrance exam score is 0.93. Panel B shows that controlling
for individual entrance exam scores (using Œ∏ÃÉc from equation (2)) reduces this correlation
only slightly. Note that while Œ∏ÃÉc ensures that individual exit residuals are uncorrelated
17

Note that this first-step regression also includes group-level (i.e., college) fixed effects, as is common in
the teacher value added literature (Chetty et al., 2014).
18 Observations in regression (4) are weighted by the number of graduates from each college. All college-level
computations in this paper use these same weights.
19 Raw mean entrance score is the average percentile across the same eight components included in regressions
(2)-(4), also normalized to mean zero and standard deviation one.
13

3

3

2

2

0.93***

Field exit score

Field exit score

0.75***

1
0
‚àí1
‚àí2

1
0
‚àí1
‚àí2

‚àí3

‚àí3
‚àí3

‚àí2

‚àí1
0
1
Entrance exam score

2

3

‚àí3

Panel A. Raw means

‚àí1
0
1
Entrance exam score

2

3

Panel B. Entrance exam residuals

3

3

2

2

0.79***

Field exit score

Field exit score

‚àí2

1
0
‚àí1
‚àí2

1
0

‚àí0.01

‚àí1
‚àí2

‚àí3

‚àí3
‚àí3

‚àí2

‚àí1
0
1
Entrance exam score

2

3

‚àí3

Panel C. Exam + SES residuals

‚àí2

‚àí1
0
1
Entrance exam score

2

3

Panel D. College-level residuals

Figure 2. Illustration of field-specific learning measures
Notes: Grey circles represent the 157 colleges in our sample. The solid line depicts the linear relationship
between the learning measures and college mean entrance scores, with colleges weighted by their number of
graduates. Stars on the slope coefficients indicate statistical significance with robust standard errors.
* p < 0.10, ** p < 0.05, *** p < 0.01.

with individual entrance exam scores, it allows college-level exit scores to be correlated with
college-level entrance exam performance. This can arise if other individual characteristics
that affect exit exam performance, such as socioeconomic background, also affect the colleges
students choose to attend.
Panel C partially addresses this issue by using the entrance exam + SES residual measure
(Œ∏ÃÇc from equation (3)), which controls for students‚Äô observable background. Panel C shows
that these controls have little effect on the correlation of the exit field score with college
mean entrance exam performance; in fact, the correlation coefficient increases slightly. This
14

Table 3. Correlations with college mean entrance scores

Field exit score
Reading exit score
English exit score
Log earnings

(A)

(B)

(C)

(D)

Raw
means

Entrance
exam
residuals

Exam
+ SES
residuals

Collegelevel
residuals

0.93‚àó‚àó‚àó
0.90‚àó‚àó‚àó
0.88‚àó‚àó‚àó
0.70‚àó‚àó‚àó

0.75‚àó‚àó‚àó
0.59‚àó‚àó‚àó
0.73‚àó‚àó‚àó
0.63‚àó‚àó‚àó

0.79‚àó‚àó‚àó
0.65‚àó‚àó‚àó
0.71‚àó‚àó‚àó
0.57‚àó‚àó‚àó

‚àí0.01
‚àí0.03
‚àí0.04
0.06

Notes: This table displays coefficients from linear regressions of college mean entrance exam scores on each
of our 16 learning and earning measures. All regressions have 157 observations with weights equal to each
college‚Äôs number of graduates. Stars indicate statistical significance with robust standard errors.
* p < 0.10, ** p < 0.05, *** p < 0.01.

illustrates that our individual learning productivity measures may still be correlated with
unobservable student characteristics that affect both college choice and exit exam performance.
Panel D illustrates that our last productivity measure, the college-level residual (Œ∏c from
equation (4)), is uncorrelated with college mean entrance exam performance by construction.20 This addresses the issue that individual characteristics may be correlated with
college mean entrance scores (as well as college mean mother‚Äôs education). However, the
college residual measure, Œ∏c , rules out the possibility that colleges with high mean entrance
scores systematically produce better learning outcomes that colleges with low average scores.
Rather, this measure is better suited for comparing the performance of colleges with similar
inputs as defined by mean entrance scores.
As stated we have 16 outcome measures in total (log earnings plus three learning measures,
each calculated using the procedures in equations (1)-(4)). Table 3 displays the correlations
of each of these measures with college mean entrance scores. The top row refers to the field
exit score and replicates the correlation coefficients depicted in Figure 2. The remaining
three rows cover the other measures. The manner in which the correlation measures change
as one moves across columns is similar accross all rows; in other words, the above discussion
applies to all our of learning and earning measures. This provides an additional justification
for using multiple methods to calculate productivity in examining our key findings below.

20

The correlation between the two measures in Panel D is not strictly zero because the horizontal axis is
the average of the eight entrance exam components, not any individual component from regression (4).
15

Table 4. Correlations with earning measure

Field exit score
Reading exit score
English exit score

(A)

(B)

(C)

(D)

Raw
means

Entrance
exam
residuals

Exam
+ SES
residuals

Collegelevel
residuals

0.62‚àó‚àó‚àó
0.58‚àó‚àó‚àó
0.71‚àó‚àó‚àó

0.45‚àó‚àó‚àó
0.29‚àó‚àó‚àó
0.62‚àó‚àó‚àó

0.45‚àó‚àó‚àó
0.41‚àó‚àó‚àó
0.51‚àó‚àó‚àó

0.07
0.16‚àó‚àó
‚àí0.09

Notes: This table displays coefficients from linear regressions of our earning measures on each of our learning
measures. All regressions have 157 observations with weights equal to each college‚Äôs number of graduates.
Stars indicate statistical significance with robust standard errors.
* p < 0.10, ** p < 0.05, *** p < 0.01.

5. Results
This section presents empirical results related to three questions: 1) How are the earning
and learning measures related to each other? 2) How are they related to other factors that
influence students‚Äô choice of colleges? 3) How do these measures vary with the majors a
college offers?
5.1. Comparing learning and earning measures. Our first empirical task is to explore
how the learning and earning measures relate to each other. Table 4 shows the correlation
coefficients for each of our three learning measures with our earning measure, where each
has been calculated according to the procedure listed in the column.
A simple but important result is that the learning measures are mostly positively related
to our earning measure, but far from perfectly so, with correlations ranging from -0.09
to 0.71 across the learning outcomes and the four procedures. The raw mean learning
and earning measures are more strongly correlated than those that control for individual
characteristics. The college-level residual measures are mostly uncorrelated, with only one
correlation coefficients that is statistically different from zero. It is also notable that the
English learning measures are generally more correlated with earnings, which may reflect a
stronger socioeconomic component to English education relative to the other subjects.
Figure 3 depicts the relation between the earning measures (vertical axis) and the fieldspecific learning measures (horizontal axis). The imperfect correlations from Table 4 are
evident here in the dispersion of the dots, which is most prevalent for the college-level residual method in Panel D. Each panel also contains a 45 degree line that represents the boundary
between whether colleges appear more productive on the learning or earning measures. In
all four panels, the most selective public colleges (indicated by the light red triangles) typically lie below the diagonal line‚Äîthese colleges appear in a more favorable light when we
16

Top private

3

3

2

2

1

1

Log earnings

Log earnings

Public (most selective)

0
‚àí1
‚àí2

0
‚àí1
‚àí2

‚àí3

‚àí3
‚àí3

‚àí2

‚àí1
0
1
Field exit score

2

3

‚àí3

‚àí1
0
1
Field exit score

2

3

Panel B. Entrance exam residuals

3

3

2

2

1

1

Log earnings

Log earnings

Panel A. Raw scores

‚àí2

0
‚àí1
‚àí2

0
‚àí1
‚àí2

‚àí3

‚àí3
‚àí3

‚àí2

‚àí1
0
1
Field exit score

2

3

‚àí3

Panel C. Exam + SES residuals

‚àí2

‚àí1
0
1
Field exit score

2

3

Panel D. College-level residuals

Figure 3. Earning vs. field-specific learning
Notes: Light red triangles represent the most selective public colleges as defined in Table 2. Dark blue
squares represent top private colleges, and grey circles depict all other colleges.

define productivity by learning. Conversely, top private colleges (dark blue squares) mostly
lie above the 45 degree line; this means that they appear in a more favorable light when
performance is defined in terms of earnings. Note that these conclusions hold across all four
procedures for calculating productivity despite the different properties discussed above. We
also find that they hold when we measure earnings eight years after graduation rather than
in the year of graduation. We note, however, that this comparison requires that we calculate
earning and learning measures using different samples, as we discuss in further detail below.
17

Table 5. Average institution rank by college type
Panel A. Entrance
exam residuals

Panel B. Collegelevel residuals

Field
exit score

Log
earnings

Field
exit score

Log
earnings

Public (most selective)
Public (medium selective)
Public (least selective)

0.88
0.54
0.26

0.58
0.44
0.20

0.63
0.47
0.45

0.56
0.57
0.48

Top private
Other private (high cost)
Other private (low cost)

0.89
0.63
0.36

0.95
0.70
0.49

0.44
0.59
0.42

0.68
0.51
0.58

College type

Notes: This table displays percentile ranks of colleges using the measures listed in the column header. We
sort all colleges according to each measure, and then calculate average ranks within the college types depicted
in Table 2. Averages are weighted by each college‚Äôs number of graduates.

Table 5 elaborates on this point by presenting the average institution rank that arises
from the use of learning or earning measures. Specifically, we sort colleges according to each
measure and calculate their percentile rank among the 157 schools. We then compute the
average rank in each of the six college types defined in Table 2. We repeat this calculation
for the field-specific learning measures and the earning measures from the entrance exam
residual method (Panel A) and the college-level residual procedure (Panel B). For instance,
using the field exit score and individual entrance exam residuals, the most selective public
colleges have an average rank at the 88th percentile, while the average rank of a top private
college is the 89th percentile.
The main conclusion from Table 5 is that public colleges receive higher rankings from
the learning measures than from the earning measures. Conversely, private colleges are
relatively higher ranked using earnings. This finding holds for all college categories using the
individual-level measures. It also holds for most categories under the college-level measures,
though the result is flipped for middle-ranked public and private institutions.
The different measures can thus lead to starkly different conclusions about colleges‚Äô relative
productivity. In Panel A, for example, high cost private colleges are ranked higher on average
than the most selective public colleges using earnings, but their average rank is 25 percentile
points lower using the learning measure. As discussed above, comparisons of colleges with
different mean entrance scores are more complicated under the college-level residual method
of Panel B. Nonetheless, a similar conclusion applies to the relative rankings of the most
selective public colleges and top private colleges, which have similar mean entrance scores
18

(see Table 2). Top private colleges receive higher ranks under the earning measure, while
selective public colleges appear more favorably when one uses the learning measure.

5.2. Correlations with other college characteristics. The fact that the learning and
earning measures are not perfectly correlated suggests that they likely have different relationships with other student and college characteristics. In this section we explore how
learning and earning productivity are related to two other factors that influence students‚Äô
college choice. We first consider socioeconomic status as defined by whether a student‚Äôs
mother attended college. We then consider a proxy for student demand: each graduate‚Äôs
annual tuition in the prior year.
For both the SES and tuition variables, we follow the same procedures described in Section
4.2 to compute college averages. This yields measures of college mean SES and college mean
tuition corresponding to the raw means, entrance exam residuals, entrance exam + SES
residuals, and college-level residuals methods. Note that we do not present the SES measures
from equation (3) as this method includes SES controls also defined by mother‚Äôs education.
Similarly, we exclude the SES variables (xi and xc ) from equation (4) when we calculate the
college-level residual measures for Figure 4 and Table 6 below; this allows us to compare
their correlations with mother‚Äôs education. As above, we normalize each measure to mean
zero and standard deviation one across the sample of 157 colleges.
Figure 4 displays the correlations of SES with the field-specific learning measures and
the earning measures. In all cases, the earning measures are more strongly correlated with
SES than that learning measures, though the difference between the two is not statistically
different from zero using raw means.21
Table 6 presents these correlations for all of our learning and earning measures. The top
panel displays the correlation of the measures with college mean SES, while the bottom panel
displays the difference between each learning measure and the earning measure. In nearly all
cases, the learning measures are less correlated with SES than the earning measures, and this
difference is statistically significant using the two residual methods (columns (B) and (C)).
The only exceptions arise with two of the English learning measures, which, as noted above,
may be more influenced by socioeconomic background than the field and reading scores.
Table 7 is analogous to Table 6, but it presents the correlations of learning and earning
measures with tuition rather than with SES. The same pattern holds; the learning measures
are in all cases substantially less correlated with graduates‚Äô average tuition than the earning
measures.
21

The same patterns arise when we measure earnings eight years after graduation rather than in the year
of graduation.
19

Field exit score

Log earnings

90% confidence interval

1.0

Correlation with SES

0.8

0.6

0.4

0.2

0.0
Raw means

Entrance
exam residuals

College‚àí
level residuals

Figure 4. Correlations with SES
Notes: Light red bars depict the correlations of our SES measures with our field-specific learning measures
(the first row in Table 6). Black bars show the correlation of our SES measures with our earning measures
(the fourth row in Table 6). Dashed lines are 90 percent confidence intervals using robust standard errors.
We exclude the xi and xc variables in calculating the college-level residual measures for this figure (see
equation (4)).

The results in Tables 6 and 7 are consistent with a college‚Äôs earning performance being a
stronger driver of its demand than its learning performance. Though none of our measures
may fully isolate college value added, these findings suggest that learning measures may be
less related to other factors that affect student outcomes, which may not be observable in
all contexts. This is particularly relevant if learning outcomes are ultimately under greater
control on the part of colleges than earning results. In particular, earning measures, unlike
those based on learning, have a natural dynamic component in the years after students enter
the labor market. Throughout our analysis we have used earnings measured in the year
of each student‚Äôs graduation, but there are both conceptual and data-related reasons why
earnings might be measured later in a worker‚Äôs career.
To explore the potential implications of the timing of earning measurement, we use a different sample than in the above analysis that allows us to measure earnings later in workers‚Äô
careers. Specifically, we include 2003‚Äì2012 graduates with earnings observed in 2008‚Äì2012.
20

Table 6. Correlations with SES
(A)

(B)

(C)

Raw
means

Entrance
exam
residuals

Collegelevel
residuals

0.65‚àó‚àó‚àó
0.59‚àó‚àó‚àó
0.83‚àó‚àó‚àó
0.77‚àó‚àó‚àó

Field exit score
Reading exit score
Correlations
English exit score
Log earnings
Differences
from
earnings

Field exit score
Reading exit score
English exit score

0.36‚àó‚àó‚àó
0.16
0.75‚àó‚àó‚àó
0.72‚àó‚àó‚àó

0.04
0.08
0.20‚àó‚àó‚àó
0.39‚àó‚àó‚àó

‚àí0.36‚àó‚àó
‚àí0.56‚àó‚àó‚àó
0.03

‚àí0.12
‚àí0.18
0.07

‚àí0.35‚àó‚àó
‚àí0.31‚àó‚àó
‚àí0.19‚àó

Notes: The top panel displays coefficients from linear regressions of SES (defined by mother‚Äôs education)
measures on each of our learning and earning measures. All regressions have 157 observations with weights
equal to each college‚Äôs number of graduates. The bottom panel shows the difference between each of the
learning coefficients and the earnings coefficient. We exclude the xi and xc variables in calculating the
college-level residual measures for this table (see equation (4)). Stars indicate statistical significance with
robust standard errors.
* p < 0.10, ** p < 0.05, *** p < 0.01.

Table 7. Correlations with tuition

Field exit score
Reading exit score
Correlations
English exit score
Log earnings
Differences
from
earnings

Field exit score
Reading exit score
English exit score

(A)

(B)

(C)

(D)

Raw
means

Entrance
exam
residuals

Exam
+ SES
residuals

Collegelevel
residuals

0.32‚àó
0.24
0.59‚àó‚àó‚àó
0.67‚àó‚àó‚àó
‚àí0.36‚àó
‚àí0.44‚àó‚àó
‚àí0.08

0.16
‚àí0.05
0.63‚àó‚àó‚àó
0.67‚àó‚àó‚àó

0.24
0.10
0.54‚àó‚àó‚àó
0.60‚àó‚àó‚àó

‚àí0.52‚àó‚àó‚àó
‚àí0.72‚àó‚àó‚àó
‚àí0.04

‚àí0.36‚àó
‚àí0.50‚àó‚àó‚àó
‚àí0.06

0.02
0.02
‚àí0.03
0.27‚àó‚àó‚àó
‚àí0.26‚àó
‚àí0.25‚àó‚àó
‚àí0.31‚àó‚àó

Notes: The top panel displays coefficients from linear regressions of tuition (defined as in Table 2) measures
on each of our learning and earning measures. All regressions have 157 observations with weights equal to
each college‚Äôs number of graduates. The bottom panel shows the difference between each of the learning
coefficients and the earnings coefficient. Stars indicate statistical significance with robust standard errors.
* p < 0.10, ** p < 0.05, *** p < 0.01.

With this sample we can observe earnings between zero and eight years of potential experience, defined as earnings year minus graduation year.22 Note that this analysis relies on
cross-cohort earning comparisons, meaning that the sample differs across experience levels.
22

We can actually observe a ninth year of potential experience
using 2012 earnings for 2003 graduates, but
21
these ninth-year measures are noisy because they come from only a single cohort and year.

Tercile at experience 0:

Bottom

Middle

Top

1.0

Log earnings

0.5

0.0

‚àí0.5

‚àí1.0
0

1

2

3

4

5

6

7

8

Potential experience

Figure 5. Log earnings by potential experience
Notes: The sample includes 2003‚Äì2012 graduates with earnings measured at 0‚Äì8 years of potential experience,
defined as earnings year minus graduation year. Dots depict average log earnings at the 128 colleges in our
sample with at least ten earning observations for each experience level. Log earnings are demeaned by
graduation year and experience. We group colleges into three terciles based on experience zero earnings and
add horizontal spacing to improve visibility.

The earning measures analyzed above normalize measures to have a constant standard
deviation. Before computing such measures, we display the raw data in Figure 5. This
figure shows average log earnings at the 128 colleges that we observe at all experience levels,
where we demean earnings by graduation cohort and year. We group the 128 colleges into
three terciles of different shadings based on their average earnings at experience zero and
hold these terciles constant for all experiences levels.
Figure 5 shows that the spread in average earnings between the highest and lowest colleges
increases with worker experience, a result first documented by MacLeod et al. (2015). At
experience zero, nearly all colleges have average earnings within 30 percent of the mean, while
many colleges lie outside this range after eight years. Further, there is substantial mixing
of the terciles over time, such that some colleges with low initial earnings ultimately have
mean earnings above those of top tercile colleges. These two findings show that both the
magnitude and the ordering of differences in earnings across colleges can change substantially
depending on when one measures earnings.
Table 8 formalizes this point by showing how the correlation of earnings with initial measures of college productivity evolve with worker experience. For this table we calculate
22

Table 8. Correlations by potential experience
Panel A.
Raw means
Log
earnings
at exp. 0
Log
Log
Correlations Log
Log
Log

earnings
earnings
earnings
earnings
earnings

at
at
at
at
at

exp.
exp.
exp.
exp.
exp.

0
2
4
6
8

1.00‚àó‚àó‚àó
0.93‚àó‚àó‚àó
0.88‚àó‚àó‚àó
0.83‚àó‚àó‚àó
0.76‚àó‚àó‚àó

Log
Log
Log
Log

earnings
earnings
earnings
earnings

at
at
at
at

exp.
exp.
exp.
exp.

2
4
6
8

‚àí0.07‚àó
‚àí0.12‚àó‚àó‚àó
‚àí0.17‚àó‚àó‚àó
‚àí0.24‚àó‚àó‚àó

Differences
from
earnings
at exp. 0

Panel B.
College-level residuals

Field
exit score
0.44‚àó‚àó‚àó
0.63‚àó‚àó‚àó
0.68‚àó‚àó‚àó
0.70‚àó‚àó‚àó
0.69‚àó‚àó‚àó
0.20
0.25‚àó
0.26‚àó‚àó
0.26‚àó‚àó

Log
earnings
at exp. 0
1.00‚àó‚àó‚àó
0.92‚àó‚àó‚àó
0.85‚àó‚àó‚àó
0.78‚àó‚àó‚àó
0.67‚àó‚àó‚àó
‚àí0.08‚àó‚àó
‚àí0.15‚àó‚àó
‚àí0.22‚àó‚àó‚àó
‚àí0.33‚àó‚àó‚àó

Field
exit score
0.04
0.16‚àó
0.17‚àó
0.15‚àó
0.10
0.11
0.12
0.11
0.06

Notes: The top panel displays coefficients from linear regressions of earning measures at different experience
levels on experience zero earning measures and the field-specific learning measures. The sample is the same
as that for Figure 5. All regressions have 128 observations with weights equal to each college‚Äôs number of
graduates. The bottom panel shows the difference between each of the experience 2‚Äì8 coefficients and the
experience zero earnings coefficient. Stars indicate statistical significance with robust standard errors.
* p < 0.10, ** p < 0.05, *** p < 0.01.

earnings measures analogous to those above using the same students and colleges as in Figure 5. Panel A displays the raw mean measures (from equation (1)), and Panel B depicts
residuals from a regression on college mean entrance exam scores (equation (4)).23
The top panel of Table 8 shows the correlation of earnings measured at different experience
levels with earnings at experience zero and with our field-specific earnings measure from
above. The bottom panel shows the difference between the experience 2‚Äì8 correlations and
the experience zero correlations in the first row. The results show that the correlation
of earning measures with initial earnings declines substantially over time, and that this
holds for both the raw and residual methods. By contrast, the earning measures become
more correlated with the field-specific exit scores over time, though the differences are not
significant for the residual measures.
The main takeaway from Figure 5 and Table 8 is that one can arrive at very different
conclusions for a college‚Äôs earning productivity depending on when one measures earnings.
This highlights the fact that colleges do not have complete control over the earnings of their
23

We do not present individual entrance exam residual measures in Table 8 because we do not observe the
full vector of individual exam scores for all 2003‚Äî2012 graduates. For this reason, we also do not use a
first-step regression to net out individual characteristics in calculating the college-level measures (see Section
4.2.4).
23

graduates, which also depend on the post-schooling actions of workers and employers. This
leaves open the possibility that learning measures do a better job of isolating a college‚Äôs
contribution to students‚Äô human capital.
5.3. Learning and earning across majors. Our final set of results concern one way in
which colleges might be able to influence these productivity measures: their choice of which
majors to offer. To explore how our measures vary across majors, we repeat the four procedures described in Section 4.2, but instead of calculating productivity at the institution
level we do so at the institution/major level. In other words, we calculate separate learning and earning productivity measures for each major offered by each college.24 We then
sort the roughly 1,100 college/major pairs according to each measure and calculate each college/major‚Äôs percentile rank. This is analogous to the procedure used to calculate institution
ranks in Table 5.
Table 9 summarizes the resulting ranks using nine broader major ‚Äúareas‚Äù defined by the
Ministry of Education.25 The first column displays the proportion of all graduates in our
sample in each major area. More than half of all graduates are in majors related to business
and engineering, which are offered by almost all colleges in the country. Majors related to
fine arts and natural sciences are less popular, and are offered by only a small number of
colleges.
The other columns in Table 9 show the average ranks from the 1,100 college/major pairs
using different learning and earning measures. Panel A presents ranks based on the entrance exam residuals method, and Panel B displays ranks based on the college/major level
residual method. Using either method, the results show that some majors‚Äîsuch as those in
engineering, business, and law‚Äîreceive much higher ranks under the earning measures than
under the learning measures. Conversely, majors related to education, fine arts, and social
or natural sciences are much lower ranked using the earning measures.
Figure 6 elaborates on this result using a slightly more granular grouping of majors.
The horizontal axis displays the average rank in each major group using the field-specific
learning measure from Panel A of Table 9. The vertical axis depicts the average rank using
the earning measure from the same procedure. Major groups that lie below the 45 degree
line are ranked more highly on learning than on earning; these include many majors in social
and natural sciences majors. Major groups above the 45 degree line, including many related
to engineering and health, appear more favorable when rankings are based on earnings.
The results in Table 9 and Figure 6 suggest that the use of different productivity measures may create incentives for colleges to favor some majors over others. In particular, if
24

We include only institution/major pairs that have at least 20 graduates in our sample.
The Ministry‚Äôs categorization actually combines social sciences and law, but we split these major groups
because they have vastly different properties with respect to our productivity measures.
25

24

Table 9. Average institution/major rank by major area
Panel A. Entrance
exam residuals
Major area
Business/economics
Engineering
Law
Social sciences
Health
Education
Fine arts
Agronomy
Natural sciences

Panel B. College/major
level residuals

Prop. of
grads

Field
exit score

Log
earnings

Field
exit score

Log
earnings

0.35
0.29
0.14
0.14
0.07
0.06
0.05
0.02
0.02

0.50
0.51
0.48
0.55
0.52
0.55
0.50
0.52
0.75

0.53
0.60
0.81
0.41
0.66
0.27
0.46
0.35
0.62

0.53
0.45
0.43
0.51
0.54
0.57
0.41
0.47
0.55

0.59
0.59
0.75
0.33
0.68
0.36
0.27
0.37
0.50

Notes: This table includes all college/major pairs with at least 20 graduates in our sample, where majors are
defined by the program name at each college. The Ministry of Education records aggregate these majors into
the nine listed ‚Äúareas.‚Äù The first column shows the proportion of graduates from each major area, and the
remaining columns display percentile ranks of college/major pairs using the learning and earning measures in
the column header. For these we sort college/majors according to each measure, and then calculate average
ranks within the major areas. Averages are weighted by each college/major‚Äôs number of graduates.

policymakers primarily use earnings to measure performance, this could encourage college
administrators to shift resources away from more specialized majors.
6. Conclusion
Increasingly, policymakers are looking to provide information on the outcomes that different colleges produce for their graduates. In many ways this reflects a desire to extend school
accountability to higher education. Casual observation suggests this desire is particularly
prevalent in countries that have seen some combination of: significant growth in access to
college, growth of a substantial (and often relatively unregulated) private sector, and increasing amounts of student debt.26 As with school accountability in K-12 education‚Äîdespite
its much longer history‚Äîquestions remain as to the informational content and the ultimate
effects of initiatives in this area.
Our goal here has been to contribute by calculating, for the country of Colombia, systemwide measures of college productivity in terms of earning and learning. While we do not
claim that our measures isolate causal college value added, they allow for analyses beyond
those that have been previously feasible. Our findings suggest that measures of college
productivity on earning and learning are far from perfectly correlated.
26

For instance, the U.S., Chile, and Colombia fit some of these criteria.
25

1.00

Geology
Medicine
Administrative eng.
Chemistry

Law

Electrical eng.
Mechanical eng.
Chemical eng.

Log earnings rank

0.75

Nursing

Physics

Dentistry
Business admin.
Systems eng.
Public health
Communication
Psychology

0.50

Political science
Philosophy
Biology Anthropology

Optometry

0.25

Language arts

Agronomy

Education
Music

Physical education

Geography/history

0.00
0.00

0.25

0.50

0.75

1.00

Field exit score rank

Figure 6. Earning vs. field-specific learning ranks by major group
Notes: This figure plots percentile ranks for college/major pairs using the entrance exam residual earning
and field-specific learning measures. We calculate these ranks as in Panel A of Table 9, but we display
average ranks within a more granular categorization of majors into 51 groups defined by the Ministry of
Education. Averages are weighted by each college/major‚Äôs number of graduates.

A key implication of this is that the design of accountability systems will affect how these
portray different types of colleges, and potentially also how these colleges respond. For
instance, we find that in the case of Colombia, top private colleges generally perform better
under our earning measure, while selective public colleges appear more favorably under our
learning measure.
In addition, in the earnings dimension one can arrive at starkly different conclusions
regarding college‚Äô relative productivity depending on when one measures earnings. This is
problematic because the more chronologically removed the observation is from graduation,
the more factors extraneous to colleges‚Äîsuch as post-schooling human capital investment
decisions taken by employers and employees‚Äîwill have a chance to affect wages. This leaves
open the possibility that learning measures do a better job of isolating a college‚Äôs contribution
to students‚Äô human capital. Of course, tradeoffs abound, as shifting weight towards learning
measure may induce gaming similar to that which has been observed around ‚ÄúNo Child Left
Behind‚Äù and analogous K-12 accountability initiatives.
Finally, our results illustrate that the use of different productivity measures may create
incentives for colleges to favor some majors over others. For example, our findings suggest
that they might encourage institutions to shift resources away from more specialized majors
and towards areas such as business and engineering.
26

References
Altonji, J. G. and R. K. Mansfield (2014). Group-average observables as controls for sorting on unobservables when estimating group treatment effects: The case of school and
neighborhood effects. Mimeo, National Bureau of Economic Research Working Paper No.
20781.
Angrist, J., E. Bettinger, and M. Kremer (2006). Long-term consequences of secondary
school vouchers: Evidence from administrative records in colombia. American Economic
Review.
Arum, R. and J. Roksa (2011). Academically adrift: Limited learning on college campuses.
University of Chicago Press.
Chetty, R., J. N. Friedman, and J. Rockoff (2014). Measuring the impacts of teachers i:
Evaluating bias in teacher value-added estimates. American Economic Review 104 (9),
2593‚Äì2632.
Cooke, M. (1910). Academic and industrial efficiency.
Council for Aid to Education (2013). Does college matter? measuring critical-thinking outcomes using the CLA. Technical report. Available at http://cae.org/images/uploads/
pdf/Does_College_Matter.pdf in August 2016.
Dale, S. B. and A. B. Krueger (2014, November). Estimating the effects of college characteristics over the career using administrative earnings data. The Journal of Human
Resources 49 (2), 323‚Äì358.
Hastings, J., C. Neilson, and S. Zimmerman (2013). Are some degrees worth more than
others? evidence from college admission cutoffs in chile. Mimeo, National Bureau of
Economic Research Working Paper No. 19241.
Hoekstra, M. (2009). The effect of attending the flagship state university on earnings: A
discontinuity-based approach. Review of Economics and Statistics 91 (4), 717‚Äì724.
Hoxby, C. (2016). The productivity of u.s. postsecondary institutions. This volume.
Hoxby, C. and G. Bulman (2015). Computing the value added of american postsecondary
institutions. Technical report, Stanford University.
Kirkeboen, L., E. Leuven, and M. Mogstad (2016). Field of study, earnings, and self-selection.
The Quarterly Journal of Economics 131 (3), 1057‚Äì1111.
Klein, S., J. Steedle, and H. Kugelmass (2010). The lumina longitudinal study: Summary of
procedures and findings comparing the longitudinal and cross-sectional models. Unpublished manuscript.
MacLeod, W. B., E. Riehl, J. E. Saavedra, and M. Urquiola (2015). The big sort: College
reputation and labor market outcomes. Mimeo, National Bureau of Economic Research
Working Paper No. 21230.
27

Pascarella, E. T. and P. Terenzini (2005). How college affects students: A third decade of
research.
Saavedra, A. and J. E. Saavedra (2011). Do colleges cultivate critical thinking, problem
solving, writing and interpersonal skills? Economics of Education Review.
Saavedra, J. (2009). The learning and early labor market effects of college quality: A
regression discontinuity analysis. Mimeo, Harvard University.
Stange, K. (2012). Ability sorting and the importance of college quality to student achievement: Evidence from community colleges. Education Finance and Policy 7 (1), 74‚Äì105.
Sullivan, T., C. Mackie, W. F. Massy, and E. Sinha (2012). Improving measurement of
productivity in higher education. Technical report, The National Academies Press: Washington, D.C.

28

