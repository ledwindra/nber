NBER WORKING PAPER SERIES

PRINCIPAL COMPONENT ANALYSIS OF HIGH FREQUENCY DATA
Yacine AÃ¯t-Sahalia
Dacheng Xiu
Working Paper 21584
http://www.nber.org/papers/w21584

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2015

We thank Hristo Sendov for valuable discussions on eigenvalues and spectral functions. In addition,
we benefited much from comments by Torben Andersen, Tim Bollerslev, Oleg Bondarenko, Marine
Carrasco, Gary Chamberlain, Kirill Evdokimov, Jianqing Fan, Christian Hansen, Jerry Hausman, Jean
Jacod, Ilze Kalnina, Jia Li, Yingying Li, Oliver Linton, Nour Meddahi, Per Mykland, Ulrich MÃ¼ller,
Andrew Patton, Eric Renault, Jeffrey Russell, Neil Shephard, George Tauchen, Viktor Todorov, Ruey
Tsay, and Xinghua Zheng, as well as seminar and conference participants at Brown University, CEMFI,
Duke University, Harvard University, MIT, Northwestern University, Peking University, Princeton
University, Singapore Management University, University of Amsterdam, University of Chicago,
University of Illinois at Chicago, University of Tokyo, the 2015 North American Winter Meeting of
the Econometric Society, the CEME Young Econometricians Workshop at Cornell, the NBER-NSF
Time Series Conference in Vienna, the 10th International Symposium on Econometric Theory and
Applications, the 7th Annual SoFiE Conference, the 2015 Financial Econometrics Conference in Toulouse
and the 6th French Econometrics Conference. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
Â© 2015 by Yacine AÃ¯t-Sahalia and Dacheng Xiu. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
Â© notice, is given to the source.

Principal Component Analysis of High Frequency Data
Yacine AÃ¯t-Sahalia and Dacheng Xiu
NBER Working Paper No. 21584
September 2015
JEL No. C22,C55,C58,G01
ABSTRACT
We develop the necessary methodology to conduct principal component analysis at high frequency.
We construct estimators of realized eigenvalues, eigenvectors, and principal components and provide
the asymptotic distribution of these estimators. Empirically, we study the high frequency covariance
structure of the constituents of the S&P 100 Index using as little as one week of high frequency data
at a time. The explanatory power of the high frequency principal components varies over time. During
the recent financial crisis, the first principal component becomes increasingly dominant, explaining
up to 60% of the variation on its own, while the second principal component drives the common variation
of financial sector stocks.

Yacine AÃ¯t-Sahalia
Department of Economics
Bendheim Center for Finance
Princeton University
Princeton, NJ 08540
and NBER
yacine@princeton.edu
Dacheng Xiu
Booth School of Business
University of Chicago
5807 South Woodlaswn Avenue
Chicago, IL 60637
dachxiu@chicagobooth.edu

Principal Component Analysis of High Frequency Dataâˆ—
Yacine AÄ±Ìˆt-Sahaliaâ€ 

Dacheng Xiuâ€¡

Department of Economics

Booth School of Business

Princeton University and NBER

University of Chicago

This Version: September 16, 2015

Abstract
We develop the necessary methodology to conduct principal component analysis at high frequency.
We construct estimators of realized eigenvalues, eigenvectors, and principal components and provide the
asymptotic distribution of these estimators. Empirically, we study the high frequency covariance structure
of the constituents of the S&P 100 Index using as little as one week of high frequency data at a time.
The explanatory power of the high frequency principal components varies over time. During the recent
financial crisis, the first principal component becomes increasingly dominant, explaining up to 60% of the
variation on its own, while the second principal component drives the common variation of financial sector
stocks.
Keywords: ItoÌ‚ Semimartingale, High Frequency, Spectral Function, Eigenvalue, Eigenvector, Principal
Components, Three Factor Model.
JEL Codes: C13, C14, C55, C58.

1

Introduction

Principal component analysis (PCA) is one of the most popular techniques in multivariate statistics, providing
a window into any latent common structure in a large dataset. The central idea of PCA is to identify a small
number of common or principal components which effectively summarize a large part of the variation of the
data, and serve to reduce the dimensionality of the problem and achieve parsimony.
Classical PCA originated in Pearson (1901) and Hotelling (1933) and is widely used in macroeconomics
and finance among many other fields. One example is Litterman and Scheinkman (1991), who document a
âˆ— We

thank Hristo Sendov for valuable discussions on eigenvalues and spectral functions. In addition, we benefited much
from comments by Torben Andersen, Tim Bollerslev, Oleg Bondarenko, Marine Carrasco, Gary Chamberlain, Kirill Evdokimov,
Jianqing Fan, Christian Hansen, Jerry Hausman, Jean Jacod, Ilze Kalnina, Jia Li, Yingying Li, Oliver Linton, Nour Meddahi,
Per Mykland, Ulrich MuÌˆller, Andrew Patton, Eric Renault, Jeffrey Russell, Neil Shephard, George Tauchen, Viktor Todorov,
Ruey Tsay, and Xinghua Zheng, as well as seminar and conference participants at Brown University, CEMFI, Duke University,
Harvard University, MIT, Northwestern University, Peking University, Princeton University, Singapore Management University,
University of Amsterdam, University of Chicago, University of Illinois at Chicago, University of Tokyo, the 2015 North American
Winter Meeting of the Econometric Society, the CEME Young Econometricians Workshop at Cornell, the NBER-NSF Time
Series Conference in Vienna, the 10th International Symposium on Econometric Theory and Applications, the 7th Annual SoFiE
Conference, the 2015 Financial Econometrics Conference in Toulouse and the 6th French Econometrics Conference.
â€  Address: 26 Prospect Avenue, Princeton, NJ 08540, USA. E-mail address: yacine@princeton.edu.
â€¡ Address: 5807 S Woodlawn Avenue, Chicago, IL 60637, USA. E-mail address: dacheng.xiu@chicagobooth.edu. Xiu gratefully
acknowledges financial support from the IBM Faculty Scholar Fund at the University of Chicago Booth School of Business.

1

three-factor structure of the term structure of yields using PCA. PCA has also been applied to analyze the
dimension of volatility dynamics, e.g., Egloff, Leippold, and Wu (2010), suggesting a two-factor model of
volatility, capturing the long and short-term fluctuations of the volatility term structure. Another example
is the development of economic activity and inflation indices by Stock and Watson (1999) using PCA and a
factor model. A sentiment measure, reflecting the optimistic or pessimistic view of investors, was created by
Baker and Wurgler (2006) using the first principal component of a number of sentiment proxies, while a policy
uncertainty index was created by Baker, Bloom, and Davis (2013).
The estimation of the eigenvalues of the sample covariance matrix is the key step towards PCA. Classical
asymptotic results, starting with Anderson (1958) and Anderson (1963), show that eigenvalues of the sample
covariance matrix are consistent and asymptotically normal estimators of the population eigenvalues, at least
when the data follow a multivariate normal distribution. Even under normality, the asymptotic distribution
becomes rather involved when repeated eigenvalues are present. Waternaux (1976) showed that a similar
central limit result holds for simple eigenvalues as long as the distribution of the data has finite fourth moment,
while Tyler (1981) obtains the asymptotic distribution of eigenvectors under more general assumptions; see
also discussions in the books by Jolliffe (2002) and Jackson (2003).
The classical PCA approach to statistical inference of eigenvalues suffers from three main drawbacks. First
is the curse of dimensionality. For instance, it is well known that even the largest eigenvalue is no longer
consistently estimated when the cross-sectional dimension grows at the same rate as the sample size along
the time domain. Second, the asymptotic theory is essentially dependent on frequency domain analysis under
stationarity (and often additional) assumptions, see, e.g., Brillinger (2001). Third, the principal components
are linear combinations of the data, which fail to capture potentially nonlinear patterns therein.
These three drawbacks create difficulties when PCA is employed on asset returns data. A typical portfolio
may consist of dozens of stocks. For instance, a portfolio with 30 stocks has 465 parameters in their covariance
matrix, if no additional structure is imposed, while one with 100 stocks contain 5,050 parameters. As a
result, years of time series data are required for estimation, raising issues of survivorship bias, potential nonstationarity and parameter constancy. Moreover, asset returns are known to exhibit time-varying volatility
and heavy tails, leading to deviations from the assumptions required by the classical PCA asymptotic theory.
In addition, prices of derivatives, e.g., options, are often nonlinear functions of the underlying assetâ€™s price
and volatility. Ruling out nonlinear combinations disconnects their principal components from the underlying
factors that drive derivative returns.
These issues motivate the development in this paper of the tools necessary to conduct PCA for continuoustime stochastic processes using high frequency data and high frequency or â€œin-fillâ€ asymptotics, whereby the
number of observations grows within a fixed time window. This approach not only adds the important method
of PCA to the high frequency toolkit, but it addresses the three drawbacks mentioned above. First, the large
amount of intraday time series data vastly improve the time series vs. cross-sectional dimensionality trade-off.
And these are not redundant observations for the problem at hand. Unlike for expected returns, it is well
established theoretically that increasing the sampling frequency improves variance and covariance estimation,
at least until market microstructure concerns start biting. Yet market microstructure noise is not a serious
concern at the one minute sampling frequency we will employ empirically, for liquid stocks which typically
trade on infra-second time scales. As a result of the large increase in the time series dimension, it is plausible
to expect high frequency asymptotic analysis with the cross-sectional dimension fixed to serve as accurate
approximations.1 In fact, we will show both in simulations and empirically that PCA works quite well at high
1 At low frequency, in the case of large dimensional data, Geman (1980) and Bai, Silverstein, and Yin (1988) investigated
the strong law of the largest eigenvalue, when the cross-sectional dimension of the data grows at the same rate as the sample
size. Johnstone (2001) further analyzed the distribution of the largest eigenvalue using random matrix theory. These analyses

2

frequency for a portfolio of 100 stocks using as little as one week of one-minute returns data. Second, the
high frequency asymptotic framework enables nonparametric analysis of general stochastic processes, thereby
allowing strong dependence, non-stationarity, and heteroscedasticity due to stochastic volatility and jumps,
and freeing the analysis from the strong parametric assumptions that are required in a low frequency setting. In
effect, the existence of population moments becomes irrelevant and PCA becomes applicable at high frequency
for general ItoÌ‚ semimartingales. Third, our principal components are built upon instantaneous (or locally)
linear combinations of the stochastic processes, which thereby capture general nonlinear relationships by virtue
of ItoÌ‚â€™s lemma.
Within a continuous-time framework, we first develop the concept of â€œrealizedâ€ or high-frequency PCA for
data sampled from a stochastic process within a fixed time window. Realized PCA depends on realizations
of eigenvalues, which are stochastic processes, evolving over time. Our implementation of realized PCA is
designed to extend classical PCA from its low frequency setting into the high frequency one as seamlessly
as possible. Therefore, we start by estimating realized eigenvalues from the realized covariance matrix. One
technical challenge we must overcome is the fact that when an eigenvalue is simple, it is a smooth function
of the instantaneous covariance matrix. On the other hand, when it comes to a repeated eigenvalue, this
function is not differentiable, which hinders statistical inference, as the asymptotic theory requires at least
second-order differentiability. To tackle the issue of non-smoothness of repeated eigenvalues, we propose
an estimator constructed by averaging all repeated eigenvalues. Using the theory of spectral functions, we
show how to obtain differentiability of the proposed estimators, and the required derivatives. We therefore
construct an estimator of realized spectral functions, and develop the high frequency asymptotic theory for
this estimator. Estimation of eigenvalues, principal components and eigenvectors then arise as special cases of
this estimator, by selecting a specific spectral function. Aggregating local estimates results in an asymptotic
bias, due to the nonlinearity of the spectral functions. The bias is fortunately of higher order, which enables us
to construct a bias-corrected estimator, and show that the latter is consistent and achieves stable convergence
in law to a mixed normal distribution.
As far as asymptotic theory is concerned, our estimators are constructed by aggregating functions of instantaneous covariance estimates. Other examples of this general type arise in other high frequency contexts.
Jacod and Rosenbaum (2013) analyze such estimators with an application of integrated quarticity estimation.
Li and Xiu (2014) develop inference theory based on the generalized method of moments to investigate structural economic models which include the instantaneous volatility as an explanatory variable. Kalnina and
Xiu (2013) discuss estimation of the leverage effect measured by the integrated correlation with an additional
volatility instrument. Li, Todorov, and Tauchen (2013) discuss inference theory for volatility functional dependencies. An alternative inference theory in Mykland and Zhang (2009) is designed for a class of estimators
based on an aggregation of local estimates using a finite number of blocks.
Also related to the problem we study is the rank inference developed in Jacod, Lejay, and Talay (2008)
confirmed that the largest eigenvalue is no longer consistently estimated. Moreover, Johnstone and Lu (2009) prove that, for a
single factor model, the estimated eigenvector corresponding to the largest eigenvalue is also inconsistent unless the sample size
grows at a rate faster than the one at which the cross-sectional dimension increases. To resolve the inconsistency of the PCA
in this setting, different estimators of the covariance matrix have been proposed, using banding (Bickel and Levina (2008b)),
tapering (Cai and Zhou (2012)), and thresholding (Bickel and Levina (2008a)) methods, or imposing additional assumptions such
as sparsity. However, the sparsity requirement of the covariance matrix does not hold empirically or a large cross-section of stock
returns, see, e.g., Fan, Furger, and Xiu (2015). Alternatively, there is a new literature on methods that â€œsparsifyâ€ the PCA
directly, i.e., imposing the sparsity on eigenvectors, see, e.g., Jolliffe, Trendafilov, and Uddin (2003), Zou, Hastie, and Tibshirani
(2006), dâ€™Aspremont, Ghaoui, Jordan, and Lanckriet (2007), Johnstone and Lu (2009), and Amini and Wainwright (2009). None
of these methods are currently applicable to dependent data nor are central limit theorems available for these estimators in a
large dimensional setting.

3

and Jacod and Podolskij (2013), where the cross-sectional dimension is also fixed and the processes follow
continuous ItoÌ‚ semimartingales. Allowing for an increasing dimension but with an additional sparsity structure,
Wang and Zou (2010) consider the estimation of integrated covariance matrix with high-dimensional and high
frequency data in the presence of measurement errors. Tao, Wang, and Zhou (2013) investigate the minimax
rate of convergence for covariance matrix estimation in the same setting; see also Tao, Wang, and Chen (2013)
and Tao, Wang, Yao, and Zou (2011) for related work. Zheng and Li (2011) establish a Marcenko-Pastur type
theorem forstudy the spectral distribution of the integrated covariance matrix for a special class of diffusion
processes. In a similar setting, Heinrich and Podolskij (2014) study the spectral distribution of empirical
covariation matrices of Brownian integrals.
Since it is about PCA, this paper shares all the natural pros and cons of PCA, especially relative to factor
models. There is a large low frequency literature in economics and finance on factor analysis. While factor
analysis in the classical setting of low frequency and fixed dimension is a very useful tool, as in Ross (1976) for
instance, Chamberlain and Rothschild (1983) point out that when the dimension grows with the sample size,
PCA may be preferred due to its simplicity, and the fact that it may be employed under weaker assumptions,
such as a factor structure that is only approximate. In fact, PCA has been used extensively at low frequency
for analyzing factor models, to determine the number of factors, e.g. Bai and Ng (2002), or to construct and
use factors for forecasting, e.g. Stock and Watson (2002), with the asymptotic theory developed by Connor
and Korajczyk (1988), Stock and Watson (1998), and Bai (2003). The above factor models are static, as
opposed to the dynamic factor models discussed in Forni, Hallin, Lippi, and Reichlin (2000), Forni and Lippi
(2001), and Forni, Hallin, Lippi, and Reichlin (2004), in which discrete-time lagged values of the unobserved
factors may also affect the observed dependent variables. By contrast, this paper develops the theory for
high frequency PCA in a continuous-time setting. Our model consists of ItoÌ‚ semimartingales, and is fully
nonparametric without any assumptions on the existence of a factor structure. That said, similar techniques
as those developed here can in principle be developed in the future to estimate continuous-time models with
a factor structure but this is a distinct problem from PCA (just as it is in a low frequency setting).
The paper is organized as follows. Section 2 sets up the notation and the model. Section 3 provides the
estimators and the main asymptotic theory. Section 4 reports the results of Monte Carlo simulations. We
then implement the method to analyze by PCA the covariance structure of the S&P 100 stocks in Section
5, where we ask whether at high frequency cross-sectional patterns in stock returns are compatible with the
well-established low frequency evidence of a low-dimensional common factor structure (e.g., Fama and French
(1993)). We find that, excluding jump variation, three Brownian factors explain between 50 and 60% of
continuous variation of the stock returns, that their explanatory power varies over time and that during the
recent financial crisis, the first principal component becomes increasingly dominant, explaining up to over
60% of the variation on its own, capturing market-wide, systemic, risk. Despite the differences in methods,
time periods, and length of observation, these empirical findings at high frequency are surprisingly consistent
with the well-established low frequency Fama-French results of three common factors. Of course, the design
limitation of PCA is that it does lend itself easily to an identification of what the principal components are
in terms of economic factors. Nevertheless, we find evidence that the first principal component shares time
series characteristics with the overall market return, and, using biplots, we find that at the height of the crisis
the second principal component drives the common variation of financial sector stocks. Section 6 concludes.
Proofs are in the appendix.

4

2
2.1

The Setup
Standard Notations and Definitions

In what follows, Rd denotes the d-dimensional Euclidean space, and its subset R+
contains non-negative real
Pdd
k
vectors. Let ek be the unit vector in R+
,
with
the
kth
entry
equal
to
1,
1
=
k=1 e , and I be the identity
d
matrix. Md denotes the Euclidean space of all d Ã— d real-valued symmetric matrices. M+
d is a subset of
++
Md , which includes all positive-semidefinite matrices. We use Md to denote the set of all positive-definite
matrices. The Euclidean space Md is equipped with an inner product hA, Bi = Tr(AB). We use kÂ·k to denote
the Euclidean norm for vectors or matrices, and the superscript â€œ+â€ to denote the Moore-Penrose inverse of
a real-matrix, see e.g., Magnus and Neudecker (1999).
All vectors are column vectors. The transpose of any matrix A is denoted by A| . The ith row of A is written
as Ai,Â· , and the jth column of A is AÂ·,j . Aij denotes the (i, j)th entry of A. The operator diag : Md â†’ Rd
is defined as diag(A) = (A11 , A22 , . . . , Add )| . In addition, we define its inverse operator Diag, which maps a
vector x to a diagonal matrix.
Let f be a function from R+
d to R. The gradient of f is written as âˆ‚f , and its Hessian matrix is denoted
2
as âˆ‚ f . The derivative of a matrix function F : M+
d â†’ R is denoted by âˆ‚F âˆˆ Md , with each element written
as âˆ‚ij F , for 1 â‰¤ i, j â‰¤ d. Note that the derivative is defined in the usual sense, so that for any A âˆˆ M+
d,
âˆ‚ij A = Jij , where Jij is a single-entry matrix with the (i, j)th entry equal to 1. The Hessian matrix of F is
2
written as âˆ‚ 2 F , with each entry referred to as âˆ‚jk,lm
F , for 1 â‰¤ j, k, l, m â‰¤ d. We use âˆ‚ k to denote kth order
derivatives and Î´ i,j to denote the Kroneckerâ€™s delta function giving 1 if i = j or 0 otherwise. A function f is
Lipchitz if there exists a constant K such that |f (x + h) âˆ’ f (x)| â‰¤ K khk . In the proof, K is a generic constant
which may vary from line to line. A function with kth continuous derivatives is denoted a C k function.
u.c.p.
Data are sampled discretely every âˆ†n units of time. All limits are taken as âˆ†n â†’ 0. â€œ =â‡’ â€ denotes
p
Lâˆ’s
uniformly on compacts in probability, and â€œâˆ’â†’â€ denotes convergence in probability. We use â€œâˆ’â†’â€ to denote
stable convergence in law. We write an  bn if for some c â‰¥ 1, bn /c â‰¤ an â‰¤ cbn for all n. Finally, [Â·, Â·] denotes
the quadratic covariation between ItoÌ‚ semimartingales, and [Â·, Â·]c the continuous part thereof.

2.2

Eigenvalues and Eigenvectors

We now collect some preliminary results about eigenvalues and eigenvectors. For any vector x âˆˆ R+
d , xÌ„ denotes
the vector with the same entries as x, ordered in a non-increasing order. We use RÌ„+
to
denote
the
subset of
d
+
Rd containing vectors x satisfying x1 â‰¥ x2 â‰¥ . . . â‰¥ xd â‰¥ 0.
By convention, for any x âˆˆ RÌ„+
d , we can write:
x1 = . . . = xg1 > xg1 +1 = . . . = xg2 > . . . xgrâˆ’1 > xgrâˆ’1 +1 = . . . = xgr â‰¥ 0,

(1)

where gr = d, and r is the number of distinct element. {g1 , g2 , . . . , gr } depends on x. We then define a
corresponding partition of indices as Ij = {gjâˆ’1 + 1, gjâˆ’1 + 2, . . . , gj }, for j = 1, 2, . . . , r.
|
For any A âˆˆ M+
d , Î»(A) = (Î»1 (A), Î»2 (A), . . . , Î»d (A)) is the vector of its eigenvalues in a non-increasing
+
order. This notation also permits us to consider Î» as a mapping from M+
d to RÌ„d . An important result
establishing the continuity of Î» is, see, e.g., Tao (2012):
+
Lemma 1. Î» : M+
d â†’ RÌ„d is Lipchitz.

Associated with any eigenvalue Î»g of A âˆˆ M+
d , we denote its eigenvector as Î³ g , which satisfies AÎ³ g = Î»g Î³ g ,
and Î³ |g Î³ g = 1. The eigenvector, apart from its sign, is uniquely defined when Î»g is simple. In such a case,
without loss of generality we require the first non-zero element of the eigenvector to be positive. In the presence
5

of repeated eigenvalues, the eigenvector is determined up to an orthogonal transformation. In any case, we
can choose eigenvectors such that for any g 6= h, Î³ |g Î³ h = 0, see, e.g., Anderson (1958).
When Î»g is a simple root, we regard Î³ g as another vector-valued function of A. It turns out in this case,
both Î»g (Â·) and Î³ g (Â·) are infinitely smooth at A, see, e.g., Magnus and Neudecker (1999).
+
+
+
âˆ
Lemma 2. Suppose Î»g is a simple root of A âˆˆ M+
at A.
d , then Î»g : Md â†’ RÌ„ and Î³ g : Md â†’ Rd are C
Moreover, we have

âˆ‚jk Î»g (A) = Î³ gj (A)Î³ gk (A),

and

âˆ‚jk Î³ g (A) = (Î»g I âˆ’ A)+
Â·,j Î³ gk (A),

where Î³ gk is the kth entry of Î³ g . If in addition all the eigenvalues of A are simple, with (Î³ 1 , Î³ 2 , . . . , Î³ d ) being
the corresponding eigenvectors, then
2
âˆ‚jk,lm
Î³ gh = âˆ’

X
p6=g


1
Î³ Î³ Î³ Î³ Î³ âˆ’ Î³ pl Î³ pm Î³ ph Î³ pj Î³ gk
(Î»g âˆ’ Î»p )2 gl gm ph pj gk

+

XX

+

XX

+

XX

p6=g q6=p

p6=g q6=p

p6=g q6=g

1
Î³ Î³ Î³ Î³ Î³
(Î»g âˆ’ Î»p )(Î»p âˆ’ Î»q ) ql pm qh pj gk
1
Î³ Î³ Î³ Î³ Î³
(Î»g âˆ’ Î»p )(Î»p âˆ’ Î»q ) ql pm qj ph gk
1
Î³ Î³ Î³ Î³ Î³ .
(Î»g âˆ’ Î»p )(Î»g âˆ’ Î»q ) qk ql gm ph pj

In general, while the eigenvalue is always a continuous function, the eigenvector associated with a repeated
root is not necessarily continuous. In what follows, we only consider estimation of an eigenvector when it is
associated with a simple eigenvalue.

2.3

Dynamics of the Variable

The process we analyze is a general d-dimensional ItoÌ‚ semimartingale, defined on a filtered probability space
(â„¦, F, (Ft )tâ‰¥0 , P) with the following Grigelionis representation:
Z t
Z t
Xt = X0 +
bs ds +
Ïƒ s dWs + (Î´1kÎ´kâ‰¤1 ) âˆ— (Âµ âˆ’ Î½)t + (Î´1{kÎ´k>1} ) âˆ— Âµt ,
(2)
0

0

where W is a d-dimensional Brownian motion, Âµ is a Poisson random measure on R+ Ã—Rd with the compensator
Î½(dt, dx) = dt âŠ— Î½Ì„(dx), and Î½Ì„ is a Ïƒ-finite measure. More details on high frequency models and asymptotics
can be found in the book AÄ±Ìˆt-Sahalia and Jacod (2014).
The volatility process Ïƒ s is caÌ€dlaÌ€g, cs = (ÏƒÏƒ | )s âˆˆ M+
d , for any 0 â‰¤ s â‰¤ t. We denote d non-negative
eigenvalues of cs by Î»1,s â‰¥ Î»2,s â‰¥ . . . â‰¥ Î»d,s , summarized in a vector Î»s . As is discussed above, we sometimes
write Î»s = Î»(cs ), regarding Î»(Â·) as a function of cs . By Lemma 1, Î»(Â·) is a continuous function so that Î»(cs )
is caÌ€dlaÌ€g.

2.4

Principal Component Analysis at High Frequency

Similar to the classical PCA, the PCA procedure in this setting consists in searching repeatedly for instantaneous linear combinations of X, namely principal components, which maximize certain measure of variation,
while being orthogonal to the principal components already constructed, at any time between 0 and t. In
contrast to the classical setting, where one maximizes the variance of the combination (see, e.g., Anderson
(1958)), the criterion here is the continuous part of the quadratic variation.
6

Lemma 3. Suppose that X is a d-dimensional vector-valued process described in (2). Then there exists a
sequence of {Î»g,s , Î³ g,s }1â‰¤gâ‰¤d,0â‰¤sâ‰¤t , such that
cs Î³ g,s = Î»g,s Î³ g,s ,

Î³ |g,s Î³ g,s = 1,

and

Î³ |h,s cs Î³ g,s = 0,

where Î»1,s â‰¥ Î»2,s â‰¥ . . . â‰¥ Î»d,s â‰¥ 0. Moreover, for any caÌ€dlaÌ€g and vector-valued adapted process Î³ s , such that
Î³ |s Î³ s = 1, and Î³ |s cs Î³ h,s = 0, 1 â‰¤ h â‰¤ g âˆ’ 1,
c
Z u
Z u
Z u
|
|
Î³ sâˆ’ dXs ,
Î³ sâˆ’ dXs , for any 0 â‰¤ u â‰¤ t.
Î»g,s ds â‰¥
0

0

0

When Î»g,s is a simple root of cs between 0 and t, then Î³ g,s is an adapted caÌ€dlaÌ€g process, due to the
Rt
continuity of Î³ g (Â·) by Lemma 2, so that we can construct its corresponding principal component 0 Î³ |g,sâˆ’ dXs .
Rt
Given Lemma 3, we can define our parameters of interest: the realized eigenvalue, i.e. 0 Î»s ds; the realized
Rt |
principal components associated with some simple root Î»g , i.e. 0 Î³ g,sâˆ’ dXs . It may also be worth investigating
Rt
the average loading on the principal component, i.e. the realized eigenvector, which is 0 Î³ g,s ds. Since the
definitions of these quantities all rely on integrals, we call them integrated eigenvalues, integrated principal
components, and integrated eigenvectors.
Rt
The above analysis naturally leads us to consider inference for 0 Î»(cs )ds, for which the differentiability
of Î»(Â·) is critical: we start with the convergence of an estimator cÌ‚s to cs , from which we need in a delta
method sense to infer the convergence of the integrated eigenvalues estimator. A simple eigenvalue is C âˆ differentiable, but for repeated eigenvalues this is not necessarily the case. For this reason, in order to fully
address the estimation problem, we need to introduce spectral functions.

2.5

Spectral Functions

A real-valued function F defined on a subset of M+
d is called a spectral function (see, e.g., Friedland (1981)) if
+
|
for any orthogonal matrix O in Md and X in M+
d , F (X) = F (O XO). We describe sets in Rd and functions
+
from R+
d to R as symmetric if they are invariant under coordinate permutations. That is, for any symmetric
function f with an open symmetric domain in R+
d , we have f (x) = f (P x) for any permutation matrix P âˆˆ Md .
Associated with any spectral function F , we define a function f on R+
d , so that f (x) = F (Diag(x)). Since
permutation matrices are orthogonal, f is symmetric, and f â—¦ Î» = F , where â—¦ denotes function composition.
We will need to differentiate the spectral function F . We introduce a matrix function associated with the
corresponding symmetric function f of F , which, for any x âˆˆ RÌ„+
d in the form of (1), is given by
ï£±
ï£´
if p = q;
ï£² 0
2
2
Afp,q (x) =
âˆ‚pp
f (x) âˆ’ âˆ‚qq
f (x)
if p 6= q, and p, q âˆˆ Il ;
ï£´
ï£³
(âˆ‚p f (x) âˆ’ âˆ‚q f (x))/(xp âˆ’ xq ) otherwise.
for some l = {1, 2, . . . , r}.
The following lemma collects some known and useful results regarding the continuous differentiability and
convexity of spectral functions, which we will use below:
Lemma 4. The symmetric function f is twice continuously differentiable at a point Î»(A) âˆˆ RÌ„+
d if and only
if the spectral function F = f â—¦ Î» is twice continuously differentiable at the point A âˆˆ M+
.
The
gradient and
d
the Hessian matrix are given below:
âˆ‚jk (f â—¦ Î»)(A) =

d
X

Opj âˆ‚p f (Î»(A))Opk ,

p=1

7

2
âˆ‚jk,lm
(f â—¦ Î»)(A) =

d
X

d
X

2
âˆ‚pq
f (Î»(A))Opl Opm Oqj Oqk +

p,q=1

Afpq (Î»(A))Opl Opj Oqk Oqm ,

p,q=1

where O is any orthogonal matrix that satisfies A = O| Diag (Î»(A)) O. More generally, f is a C k function at
Î»(A) if and only if F is C k at A, for any k = 0, 1, . . . , âˆ. In addition, f is a convex function if and only if
F is convex.
Next, we note that both simple and repeated eigenvalues can be viewed as special cases of spectral functions.
Example 1 (A Simple Eigenvalue). Suppose the kth eigenvalue of A âˆˆ M+
d is simple, that is, Î»kâˆ’1 (A) >
Î»k (A) > Î»k+1 (A). Define for any x âˆˆ R+
,
d
f (x) = the kth largest entry in x = xÌ„k .
Apparently, f is a symmetric function, and it is C âˆ at any point y âˆˆ RÌ„+
d , with ykâˆ’1 > yk > yk+1 . Indeed,
k
l
âˆ‚f (y) = e , and âˆ‚ f (y) = 0 for l â‰¥ 2. By Lemma 4, Î»k (A) = (f â—¦ Î»)(A) is C âˆ at A.
Example 2 (Non-Simple Eigenvalues). Suppose the eigenvalues of A âˆˆ M+
d satisfy:
Î»glâˆ’1 (A) > Î»glâˆ’1 +1 (A) â‰¥ . . . â‰¥ Î»gl (A) > Î»gl+1 (A),
for some 1 â‰¤ glâˆ’1 < gl â‰¤ d. By convention, when gl = d, the last â€œ >â€ above is not used. Consider the
following function f , evaluated at x âˆˆ R+
d , which is given by
f (x) =

1
gl âˆ’ glâˆ’1

gl
X

xÌ„j .

j=glâˆ’1 +1

It is easy to verify that f is symmetric, and C âˆ at any point y âˆˆ RÌ„+
d that satisfies yglâˆ’1 > yglâˆ’1 +1 â‰¥ . . . â‰¥
Pgl
1
k
l
ygl > ygl+1 . Moreover, âˆ‚f (y) = gl âˆ’glâˆ’1 k=glâˆ’1 +1 e , and âˆ‚ f (y) = 0, for any l â‰¥ 2. As a result of Lemma
4, the corresponding spectral function,
1
F (A) = (f â—¦ Î»)(A) =
gl âˆ’ glâˆ’1

gl
X

Î»j (A),

j=glâˆ’1 +1

is C âˆ at A. In the special case where
Î»glâˆ’1 (A) > Î»glâˆ’1 +1 (A) = . . . = Î»gl (A) > Î»gl+1 (A),
i.e., there is a repeated eigenvalue, and F (A) = Î»glâˆ’1 +1 (A) = . . . = Î»gl (A). By contrast, Î»j (A) is not
differentiable, for any glâˆ’1 + 1 â‰¤ j â‰¤ gl .
Example 3 (Trace and Determinant). For any x âˆˆ R+
d , define
f1 (x) =

d
X

xj ,

and

j=1

f2 (x) =

d
Y

xj .

j=1

Both f1 and f2 are symmetric and C âˆ . Therefore, for any A âˆˆ M+
d , Tr(A) = (f1 â—¦ Î»)(A) and det(A) =
âˆ
(f2 â—¦ Î»)(A) are C at A.
The previous examples make it clear that the objects of interest, eigenvalues, are special cases of spectral
functions, whether they are simple or repeated. (We are also able to estimate the trace and determinant â€œfor
8

freeâ€.) Lemma 4 links the differentiability of a spectral function F, which is needed for statistical inference,
to that of its associated symmetric function f . The key advantage is that differentiability of f is easier to
establish.
Before turning to inference, we prove a useful result that characterizes the topology of the set of matrices
with special eigenvalue structures. The domain of spectral functions we consider will be confined to this set
in which these spectral functions are smooth.
Lemma 5. For any 1 â‰¤ g1 < g2 < . . . < gr â‰¤ d, the set

M(g1 , g2 , . . . , gr ) = A âˆˆ M++
| Î»gl (A) > Î»gl +1 (A), for any l = 1, 2, . . . , r âˆ’ 1
d

(3)

is dense and open in M++
d . In particular, the set of positive-definite matrices with distinct eigenvalues, i.e.,
M(1, 2, . . . , d), is dense and open in M++
d .
We conclude this section with some additional notations. First, we introduce a symmetric open subset of
RÌ„ /{0}, the image of M(g1 , g2 , . . . , gr ) under Î»(Â·):
+


D(g1 , g2 , . . . , gr ) = x âˆˆ R+
d /{0} | xÌ„gl > xÌ„gl +1 , for any l = 1, 2, . . . , r âˆ’ 1 .

(4)

We also introduce a subset of M(g1 , g2 , . . . , gr ), in which our spot covariance matrix ct will take values.
n
Mâˆ— (g1 , g2 , . . . , gr ) = A âˆˆ M++
|
d
o
Î»1 (A) = . . . = Î»g1 (A) > Î»g1 +1 (A) = . . . = Î»g2 (A) > . . . Î»grâˆ’1 (A) > Î»grâˆ’1 +1 (A) = . . . = Î»gr (A) .
Finally, we introduce the following set, which becomes relevant if we are only interested in a simple eigenvalue
Î»g :

M(g) = A âˆˆ M++
| Î»gâˆ’1 (A) > Î»g (A) > Î»g+1 (A) , and D(g) = Î»(M(g)).
d
By convention, we ignore the first (resp. second) inequality in the definition of M(g) when g = 1 (resp. g = d).

3

Estimators and Asymptotic Theory

3.1

Assumptions

We start with the standard assumption on the process X:2
Assumption 1. The drift terms bt is progressively measurable and locally bounded. The spot covariance matrix
ct = (ÏƒÏƒ | )t is an ItoÌ‚ semimartingale. Moreover, for any Î³ âˆˆ [0, 1), there is a sequence of stopping times (Ï„ n )
R
Î³
increasing to âˆ, and a deterministic function Î´Ì„ n such that Rd Î´Ì„ n (x) Î½Ì„ (dx) < âˆ and that kÎ´(Ï‰, t, x)k âˆ§ 1 â‰¤
Î´Ì„ n (x), for all (Ï‰, t, x) with t â‰¤ Ï„ n (Ï‰).
Rt
In view of the previous examples, our main theory is tailored to the statistical inference on 0 F (cs )ds.
Thanks to Lemma 4, we can make assumptions directly on f instead of F , which are much easier to verify.
Assumption 2. Suppose F is a vector-valued spectral function, and f is the corresponding vector-valued
Î¶
symmetric function such that F = f â—¦ Î». f is a continuous function, and satisfies kf (x)k â‰¤ K(1 + kxk ), for
some Î¶ > 0.
2 Using

the notation on Page 583 of Jacod and Protter (2011), Assumption 1 states that the process X satisfies Assumption
(H-1) and that ct satisfies Assumption (H-2).

9

In all the examples of Section 2.5, this assumption holds. By Lemma 1 and Assumption 2, F (cs ) is caÌ€dlaÌ€g,
Rt
and the integral 0 F (cs )ds is well-defined.
The above assumptions are sufficient to ensure the desired consistency of estimators we will propose.
Additional assumptions are required to establish their asymptotic distribution.
Assumption 3. There exists some open and convex set C, such that its closure CÂ¯ âŠ‚ M(g1 , g2 , . . . , gr ), where
1 â‰¤ g1 < g2 < . . . < gr â‰¤ d, and that for any 0 â‰¤ s â‰¤ t, cs âˆˆ C âˆ© Mâˆ— (g1 , g2 , . . . , gr ). Moreover, f is C 3 on
D(g1 , g2 , . . . , gr ).
Assumption 3, in particular, cs âˆˆ Mâˆ— (g1 , g2 , . . . , gr ), guarantees that different groups of eigenvalues do not
cross over within [0, t]. This condition is mainly used to deliver the joint central limit theorem for spectral
functions that depend on all eigenvalues, although it may not be necessary for some special cases, such as
det(Â·) and Tr(Â·), which are smooth everywhere. The convexity condition on C is in principle not difficult to
satisfy, given that M(g1 , g2 , . . . , gr ) can be embedded into some Euclidean space of real vectors. This condition
is imposed to ensure that the domain of the spectral function can be restricted to certain neighborhood of
{cs }0â‰¤sâ‰¤t , in which the function is smooth and the mean-value theorem can be applied. This assumption is
not needed if ct is continuous.
It is also worth mentioning that all eigenvalues of ct being distinct is a special case, which is perhaps
the most relevant scenario in practice, as is clear from Lemma 5. Even in this scenario, Assumption 3 is
required (with g1 , g2 , . . . , gr being chosen as 1, 2, . . . , d), because the eigenvalue function Î»(Â·) is not everywhere
differentiable.
Assumption 3 resembles the spacial localization assumption in Li, Todorov, and Tauchen (2014) and Li
and Xiu (2014), which is different from the polynomial growth conditions proposed by Jacod and Rosenbaum
(2013). The growth conditions are not satisfied in our setting, when the difference between two groups of
repeated eigenvalues approaches zero.
If we are only interested in the spectral function that depends on one simple eigenvalue, e.g., the largest
Rt
and simple integrated eigenvalue, 0 Î»1 (cs )ds, then it is only necessary to ensure that Î»1 (cs ) > Î»2 (cs ), for
0 â‰¤ s â‰¤ t, regardless of whether the remaining eigenvalues are simple or not. We thereby introduce the
following assumption for this scenario, which is much weaker than Assumption 3.
Assumption 4. There exists some open and convex set C, such that CÂ¯ âŠ‚ M(g), for some g = 1, 2, . . . , d, and
that for any 0 â‰¤ s â‰¤ t, cs âˆˆ C. Moreover, f is C 3 on D(g).

3.2

Realized Spectral Functions

We now turn to the construction of the estimators. To estimate the integrated spectral function, we start with
estimation of the spot covariance matrix. Suppose we have equidistant observations on X over the interval
[0, t], separated by a time interval âˆ†n . We form non-overlapping blocks of length kn âˆ†n . At each ikn âˆ†n , we
estimate cikn âˆ†n by
kn

|
1 X
b
cikn âˆ†n =
âˆ†nikn +j X âˆ†nikn +j X 1{kâˆ†n
,
(5)
ikn +j X kâ‰¤un }
kn âˆ†n j=1
n
where un = Î±âˆ†$
n , and âˆ†l X = Xlâˆ†n âˆ’ X(lâˆ’1)âˆ†n . Choices of Î± and $ are standard in the literature (see, e.g.,
AÄ±Ìˆt-Sahalia and Jacod (2014)) and are discussed below when implemented in simulations.
We then estimate eigenvalues of b
cikn âˆ†n by solving for the roots of |b
cikn âˆ†n âˆ’ Î»I| = 0. Using the notation
bik âˆ† . Almost surely, the eigenvalues stacked in Î»
bik âˆ† are distinct (see,
in Lemma 1, we have Î»(b
cikn âˆ†n ) = Î»
n n
n n

10

b1,ik âˆ† > Î»
b2,ik âˆ† > . . . > Î»
bd,ik âˆ† . Our proposed estimator of the
e.g., Okamoto (1973)) so that we have Î»
n n
n n
n n
integrated spectral function is then given by3
[t/(kn âˆ†n )]

X

V (âˆ†n , X; F ) = kn âˆ†n



bik âˆ† .
f Î»
n n

(6)

i=0

We start by establishing consistency of this estimator:
Î¶âˆ’1) 1
Theorem 1. Suppose Assumptions 1 and 2 hold. Also, either Î¶ â‰¤ 1, or Î¶ > 1 with $ âˆˆ [ 2Î¶âˆ’Î³
, 2 ) holds. Then
the estimator (6) is consistent. As kn â†’ âˆ and kn âˆ†n â†’ 0,

Z

u.c.p.

t

V (âˆ†n , X; F ) =â‡’

F (cs )ds.

(7)

0

Next, obtaining a central limit theorem for the estimator is more involved, as there is a second-order
asymptotic bias associated with the estimator (6), a complication that is also encountered in other situations
such as the quarticity estimator in Jacod and Rosenbaum (2013), the GMM estimator in Li and Xiu (2014),
or the leverage effect estimator by Kalnina and Xiu (2013). The bias here is characterized as follows:
$
Proposition 1. Suppose Assumptions 1, 2, and 3 hold. In addition, kn  âˆ†âˆ’Ï‚
n and un  âˆ†n for some
Î³ 1
1âˆ’Ï‚ 1
Ï‚ âˆˆ ( 2 , 2 ) and $ âˆˆ [ 2âˆ’Î³ , 2 ). As âˆ†n â†’ 0, we have



Z

kn V (âˆ†n , X; F ) âˆ’
0

t


p 1
F (cs )ds âˆ’â†’
2

d
X

Z

j,k,l,m=1

t
2
âˆ‚jk,lm
F (cs ) (cjl,s ckm,s + cjm,s ckl,s ) ds.

(8)

0

The characterization of the bias in (8) suggests a bias-corrected estimator as follows:
Ve (âˆ†n , X; F ) = kn âˆ†n

[t/(kn âˆ†n )] n

X

F (b
cikn âˆ†n )

(9)

i=0

âˆ’

1
2kn

d
X

o
2
âˆ‚jk,lm
F (b
cikn âˆ†n ) (b
cjl,ikn âˆ†n b
ckm,ikn âˆ†n + b
cjm,ikn âˆ†n b
ckl,ikn âˆ†n ) .

j,k,l,m=1

We then derive the asymptotic distribution of the bias-corrected estimator:
Î³ 1
$
Theorem 2. Suppose Assumptions 1, 2, and 3 hold. In addition, kn  âˆ†âˆ’Ï‚
n and un  âˆ†n for some Ï‚ âˆˆ ( 2 , 2 )
1âˆ’Ï‚ 1
and $ âˆˆ [ 2âˆ’r
, 2 ). As âˆ†n â†’ 0, we have

âˆš

1
âˆ†n



Z
Ve (âˆ†n , X; F ) âˆ’

t


Lâˆ’s
F (cs )ds âˆ’â†’ Wt ,

(10)

0

where W is a continuous process defined on an extension of the original probability space, which conditionally
on F, is continuous centered Gaussian martingale with its covariance given by
Z
E(Wp,t Wq,t |F) =

t

d
X

âˆ‚jk Fp (cs )âˆ‚lm Fq (cs ) (cjl,s ckm,s + cjm,s ckl,s ) ds.

(11)

0 j,k,l,m=1

Remark 1. As previously noted, in the case when the spectral function F only depends on a simple eigenvalue
Î»g , the same result holds under the weaker Assumption 4 instead of 3.
3 We

prefer this estimator to the alternative one using overlapping windows, because the overlapping implementation runs
much slower. In a finite sample, both estimators have a decent performance.

11

A feasible implementation of this distribution requires an estimator of the asymptotic variance, which we
construct as follows:
Proposition 2. The asymptotic variance of Ve (âˆ†n , X; F ) can be estimated consistently by:
[t/(kn âˆ†n )]

kn âˆ†n
p

âˆ’â†’

Z

t

X

d
X

i=0

j,k,l,m=1

d
X

âˆ‚jk Fp (b
cikn âˆ†n )âˆ‚lm Fq (b
cikn âˆ†n ) (b
cjl,ikn âˆ†n b
ckm,ikn âˆ†n + b
cjm,ikn âˆ†n b
ckl,ikn âˆ†n )

âˆ‚jk Fp (cs )âˆ‚lm Fq (cs ) (cjl,s ckm,s + cjm,s ckl,s ) ds.

(12)

0 j,k,l,m=1

3.3

Realized Eigenvalues

We next specialize the previous theorem to obtain a central limit theorem for the realized eigenvalue estimators.
With the structure of eigenvalues in mind, we use a particular vector-valued spectral function F Î» , tailor-made
to deliver the asymptotic theory we need for realized eigenvalues:
ï£¶|
ï£«
g1
g2
gr
X
X
X
1
1
1
Î»j (Â·),
Î»j (Â·), . . . ,
Î»j (Â·)ï£¸ .
(13)
F Î» (Â·) = ï£­
g1 j=1
g2 âˆ’ g1 j=g +1
gr âˆ’ grâˆ’1 j=g +1
1

râˆ’1

Apparently, if a group contains only one Î»j , then the corresponding entry of F Î» is equal to this single
eigenvalue; if within certain group, all eigenvalues are identical, then the corresponding entry of F Î» yields the
common eigenvalue of the group.
Î³ 1
1âˆ’Ï‚ 1
$
Corollary 1. Suppose kn  âˆ†âˆ’Ï‚
n and un  âˆ†n for some Ï‚ âˆˆ ( 2 , 2 ) and $ âˆˆ [ 2âˆ’r , 2 ).
(i) Under Assumption 1, the estimator of integrated eigenvalue vector given by
[t/(kn âˆ†n )]

X

V (âˆ†n , X; Î») = kn âˆ†n

Î»(b
cikn âˆ†n )

(14)

i=0

is consistent.
(ii) If Assumption 3 further holds, the estimator corresponding to the pth entry of F Î» given by (9) can be
written explicitly as:
Ve (âˆ†n , X; FpÎ» ) =

k n âˆ†n
gp âˆ’ gpâˆ’1

X

gp
X

i=0

h=gpâˆ’1 +1

[t/(kn âˆ†n )]





bh,ik âˆ† âˆ’ 1 Tr (Î»
bh,ik âˆ† I âˆ’ b
bh,ik âˆ†
Î»
cikn âˆ†n )+ b
cikn âˆ†n Î»
n n
n n
n n
kn


.


|
The joint central limit theorem for Ve (âˆ†n , X; F Î» ) = Ve (âˆ†n , X; F1Î» ), . . . , Ve (âˆ†n , X; FrÎ» ) is given by
âˆš

1
âˆ†n



Ve (âˆ†n , X; F Î» ) âˆ’

Z

t


Lâˆ’s
F Î» (cs )ds âˆ’â†’ WtÎ» ,

(15)

0

where W Î» is a continuous process defined on an extension of the original probability space, which conditionally
on F, is continuous centered Gaussian martingale with a diagonal covariance matrix given by
ï£« 2 Rt 2
ï£¶
g1 0 Î»g1 ,s ds
R
t 2
ï£¬
ï£·
2
ï£·
ï£¬
g2 âˆ’g1 0 Î»g2 ,s ds
Î»
Î» |
ï£¬
ï£·,
E(Wt (Wt ) |F) = ï£¬
(16)
..
ï£·
.
ï£¸
ï£­
Rt 2
2
gr âˆ’grâˆ’1 0 Î»gr ,s ds
12

where F Î» (cs ) = (Î»g1 ,s , Î»g2 ,s , . . . , Î»gr ,s )| .
(iii) Under Assumptions 1 and 4, our estimator with respect to the gth simple eigenvalue Î»g (Â·), is given by
[t/(kn âˆ†n )] 

X

Ve (âˆ†n , X; Î»g ) = kn âˆ†n

i=0



+
bg,ik âˆ† âˆ’ b
bg,ik âˆ† âˆ’ 1 Tr (Î»
bg,ik âˆ†
c
)
Î»
b
c
Î»
ik
âˆ†
ik
âˆ†
n n
n n
n n
n n
n n
kn


,

(17)

which satisfies:
âˆš

1
âˆ†n



Z
Ve (âˆ†n , X; Î»g ) âˆ’

t


Lâˆ’s
Î»
F Î» (cs )ds âˆ’â†’ Wt g ,

(18)

0

where W Î»g is a continuous process defined on an extension of the original probability space, which conditionally
Rt
on F, is a continuous centered Gaussian martingale with its variance 0 Î»2g,s ds.
Remark 2. Corollary 1 is the analogue in our context to the classical results on asymptotic theory for PCA
by Anderson (1963), who showed that
âˆš

d

b âˆ’ Î») â†’ N 0, 2Diag Î»2 , Î»2 , . . . , Î»2
n(Î»
1
2
d



.

(19)

b and Î» are the vectors of eigenvalues of the sample and population covariance matrices and Î» is simple.
where Î»
Note the similarity between (19) and (16) in the special case where the eigenvalues are simple (in which case
gi = i) and are constant instead of being stochastic. The classical setting requires the strong assumption that
the covariance matrix follows a Wishart Distribution. By contrast, our results are fully nonparametric, relying
instead on high frequency asymptotics.
Remark 3. If there are eigenvalues of cs that are equal to 0 for any 0 â‰¤ s â‰¤ t, then our estimator is superconsistent. This is an interesting case as the rank of the covariance matrix is determined by the number of
non-zero eigenvalues, see, e.g., the rank inference in Jacod, Lejay, and Talay (2008) and Jacod and Podolskij
(2013).

3.4

Realized Principal Components

As we have remarked before, when eigenvalues are simple, the corresponding eigenvectors are uniquely determined. Therefore, we can estimate the instantaneous eigenvectors together with eigenvalues, which eventually
leads us to construct the corresponding principal component:
Proposition 3. Suppose Assumptions 1 and 4 hold. In addition, Î³ g,s is a vector-valued function that corre$
sponds to the eigenvector of cs with respect to a simple root Î»g,s . Suppose kn  âˆ†âˆ’Ï‚
n and un  âˆ†n for some
1âˆ’Ï‚ 1
Ï‚ âˆˆ ( Î³2 , 12 ) and $ âˆˆ [ 2âˆ’r
, 2 ). Then we have as âˆ†n â†’ 0
[t/(kn âˆ†n )]âˆ’1

X

u.c.p

Î³
b|g,(iâˆ’1)kn âˆ†n (X(i+1)kn âˆ†n âˆ’ Xikn âˆ†n ) =â‡’

i=1

3.5

Z

t

Î³ |g,sâˆ’ dXs .

0

Realized Eigenvectors

So far we have constructed the principal components and estimated integrated eigenvalues. It remains to figure
out the loading of each entry of X on each principal component, i.e., the eigenvector. As the eigenvector is
stochastic, we estimate the integrated eigenvector. Apart from its sign, an eigenvector is uniquely identified if
its associated eigenvalue is simple. We determine the sign hence identify the eigenvector, by requiring a priori
certain entry of the eigenvector to be positive, e.g., the first non-zero entry.

13

Corollary 2. Suppose Assumptions 1 and 4 hold. In addition, Î³ g,s is a vector-valued function that corresponds
to the eigenvector of cs with respect to a simple root Î»g,s , for each s âˆˆ [0, t]. Then we have as âˆ†n â†’ 0
ï£¶
ï£¶
ï£«
ï£«
Z t
[t/(kn âˆ†n )]
X
X
bg,ik âˆ† Î»
bp,ik âˆ†
1
Î»
1 ï£­
Lâˆ’s
n n
n n
ï£¸âˆ’
ï£­Î³
âˆš
Î³ g,s dsï£¸ âˆ’â†’ WtÎ³ ,
Î³
b
k n âˆ†n
bg,ikn âˆ†n +
bg,ik âˆ† âˆ’ Î»
bp,ik âˆ† )2 g,ikn âˆ†n
2k
âˆ†n
n
(
Î»
0
n n
n n
i=0
p6=g
where W Î³ is a continuous process defined on an extension of the original probability space, which conditionally
on F, is a continuous centered Gaussian martingale with its covariance matrix given by
Z t
Î»g,s (Î»g,s I âˆ’ cs )+ cs (Î»g,s I âˆ’ cs )+ ds.
E(WtÎ³ (WtÎ³ )| |F) =
0

3.6

â€œPCAâ€ on the Integrated Covariance Matrix

One may wonder why we chose to estimate the integrated eigenvalues of the spot covariance matrix rather
than the eigenvalues of the integrated covariance matrix. Because eigenvalues are complicated functionals of
the covariance matrix, the two quantities are not much related, and it turns out that the latter procedure is
less informative than the former. For instance, the rank of the instantaneous covariance matrix is determined
by the number of non-zero instantaneous eigenvalues. The eigenstructure of the integrated covariance matrix
is rather opaque due to the aggregation of instantaneous covariances. Nevertheless, it is possible to construct
a â€œPCAâ€ procedure on the integrated covariance matrix, with the following results:
Rt
Proposition 4. Suppose the eigenvalues of the integrated covariance matrix 0 cs ds are given by
Î»1 = . . . = Î»g1 > Î»g1 +1 = . . . = Î»g2 > . . . Î»grâˆ’1 > Î»grâˆ’1 +1 = . . . = Î»gr > 0,

for 1 â‰¤ r â‰¤ d.

Then, we have, for $ âˆˆ [1/(4 âˆ’ 2Î³), 1/2),
ï£¶
ï£« ï£«
ï£¶
Z t

[t/âˆ†n ]
1 ï£­ Î»ï£­ X
Lâˆ’s
âˆš
F
(âˆ†ni X)(âˆ†ni X)| 1{kâˆ†n X kâ‰¤Î±âˆ†$ } ï£¸ âˆ’ F Î»
cs ds ï£¸ âˆ’â†’ WÌ„tÎ» ,
n
i
âˆ†n
0
i=0
where WÌ„ Î» is a continuous process defined on an extension of the original probability space, which conditionally
on F, is a continuous centered Gaussian martingale with its covariance matrix given by
Î»
Î»
E(WÌ„i,t
WÌ„j,t
|F) =

d
X

âˆ‚uv FiÎ»

1
gi âˆ’ giâˆ’1

 Z t

Z t

cs ds
(cuk,s cvl,s + cul,s cvk,s )ds âˆ‚kl FjÎ»
cs ds ,

0

u,v,k,l=1

âˆ‚uv FiÎ» (A) =

t

Z

gi
X

0

0

Oku Okv .

k=giâˆ’1 +1

where O is any orthogonal matrix that satisfies A = O| Diag (Î»(A)) O.
Similarly, we have:
Proposition 5. Suppose $ âˆˆ [1/(4âˆ’2Î³), 1/2). For the eigenvector Î³ g corresponding to some simple eigenvalue
Rt
Î»g of 0 cs ds,
ï£« ï£«
ï£¶
ï£¶
Z t

[t/âˆ†n ]
1 ï£­ ï£­ X
Lâˆ’s
âˆš
cs ds ï£¸ âˆ’â†’ WÌ„tÎ³ ,
Î³g
(âˆ†ni X)(âˆ†ni X)| 1{kâˆ†n X kâ‰¤Î±âˆ†$ } ï£¸ âˆ’ Î³ g
n
i
âˆ†n
0
i=0

14

where WÌ„ Î³ is a continuous process defined on an extension of the original probability space, which conditionally
on F, is a continuous centered Gaussian martingale with its covariance matrix given by
Î³ |
Î³
) |F) =
(WÌ„j,t
E(WÌ„i,t

d
X

t

Z
âˆ‚uv Î³ gi



 Z t
Z t
(cuk,s cvl,s + cul,s cvk,s )ds âˆ‚kl Î³ gj
cs ds ,
cs ds
0

0

u,v,k,l=1

âˆ‚kl Î³ gi (A) = (Î»g (A)I âˆ’

+
A)ik

0

Î³ gl (A).

The corresponding principal component is then defined as Î³ |g (Xt âˆ’ X0 ). However, this eigenvalue and the
principal component do not satisfy the fundamental relationship between the eigenvalue and the variance of
the principal component:

Z t

c
cs ds 6= Î³ |g (Xt âˆ’ X0 ), Î³ |g (Xt âˆ’ X0 ) ,
(20)
Î»g
0

which is a desired property of any sensible PCA procedure. This fact alone makes this second procedure much
less useful than the one we proposed above, based on integrated eigenvalues of the spot covariance. Another
feature that distinguishes this second â€œPCAâ€ procedure with both the classical one and our realized PCA is
Rt
that the asymptotic covariance matrix of eigenvalues of 0 cs ds is no longer diagonal.
An analogy of the relationship between the two PCA procedures is that between integrated quarticity
Rt 4
Rt
t 0 Ïƒ s ds and the squared integrated volatility ( 0 Ïƒ 2s ds)2 . If the covariance matrix is constant, the two
procedures are equivalent but not in general.

4

Simulation Evidence

We now investigate the small sample performance of the estimators in conditions that approximate the empirical setting of PCA for large stock portfolios. In order to generate data with a common factor structure,
we simulate a factor model in which a vector of log-stock prices X follows the continuous-time dynamics
dXt = Î² t dFt + dZt ,

(21)

where F is a collection of unknown factors, Î² is the matrix of factor loadings, and Z is a vector of idiosyncratic
components, which is orthogonal to F . This model is a special case of the general semimartingale model (2).
By construction, the continuous part of the quadratic variation has the following local structure:
[dX, dX]ct = Î² t [dF, dF ]ct Î² |t + [dZ, dZ]ct .
When the idiosyncratic components have smaller magnitude, the dominating eigenvalues of X are close to
the non-zero eigenvalues of Î² t [dF, dF ]ct Î² |t , by Weylâ€™s inequality. As a result, we should be able to detect the
number of common factors in F from observations on X. That said, our goal here is to conduct nonparametric
PCA rather than to estimate a parametric factor model, for which we need to resort to a large panel of X,
whose dimension increases with the sample size, see, e.g., Bai and Ng (2002), and so (21) is only employed
as the data-generating process on which PCA is employed without any knowledge of the underlying factor
structure.
Specifically, we simulate
dXi,t =

r
X

Î² ij,t dFj,t + dZi,t ,

F
dFj,t = Âµj dt + Ïƒ j,t dWj,t + dJj,t
,

j=1

15

Z
dZi,t = Î³ t dBi,t + dJi,t
,

where i = 1, 2, . . . , d, and j = 1, 2, . . . , r. In the simulations, one of the F s plays the role of the market
factor, so that its associated Î²s are positive. The correlation matrix of dW is denoted as ÏF . We allow for
time-varying Ïƒ j,t , Î³ t , and Î² ij,t , which evolve according to the following system of equations:
fj,t + dJ Ïƒ2 , dÎ³ 2 = Îº(Î¸ âˆ’ Î³ 2 )dt + Î·Î³ t dBÌ„t ,
dÏƒ 2j,t = Îºj (Î¸j âˆ’ Ïƒ 2j,t )dt + Î· j Ïƒ j,t dW
j,t
t
t
ï£±
p
ï£²ÎºÌƒ (Î¸Ìƒ âˆ’ Î² )dt + Î¾Ìƒ Î² dB
eij,t if the jth factor is the â€œmarketâ€,
j ij
ij,t
j
ij,t
dÎ² ij,t =
,
ï£³ÎºÌƒj (Î¸Ìƒij âˆ’ Î² )dt + Î¾Ìƒ dB
eij,t
otherwise.
ij,t
j
fj,Â· is Ïj , {J F }1â‰¤jâ‰¤r and {J Z }1â‰¤iâ‰¤d are driven by two Poisson
where the correlation between dWj,Â· and dW
j
i
F
Z
Processes with arrival rates Î» and Î» , respectively. Their jump sizes follow double exponential distributions
F
Z
Z
Ïƒ2
F
with means denoted by ÂµF
+ , Âµâˆ’ , Âµ+ , and Âµâˆ’ , respectively. {Jj }1â‰¤jâ‰¤r co-jumps with J , and their jump sizes
Ïƒ2
follow exponential distributions with the mean equal to Âµ . All the model parameters are given in Table 1.
They are chosen to be realistic given the empirical characteristics of the cross-section of stock returns and
their volatilities.
Anticipating our empirical application to the S&P 100 Index constituents, we simulate intraday returns
of d = 100 stocks at various frequencies and with horizons T spanning 1 week to 1 month. When r = 3, the
model implies 3 distinct eigenvalues reflecting the local factor structure of the simulated data. The remaining
population eigenvalues are identical, due to idiosyncratic variations. Throughout, we fix kn to be the closest
p
âˆ’1/2 p
log(d), with Î¸ = 0.5 and d is the dimension of X. Our choice of log(d)
divisors of [t/âˆ†n ] to Î¸âˆ†n
is motivated from the literature on high-dimensional covariance matrix estimation, although our asymptotic
design does not take into account an increasing dimensionality. Other choices of Î¸, e.g., 0.05 - 0.5, or functions
âˆš
of d, e.g., d log d, deliver the same results. To truncate off jumps from spot covariance estimates, we adopt
Rt
Rt
the usual procedure in the literature, i.e., choosing ui,n = 3( 0 cii,s ds/t)0.5 âˆ†0.47
n , for 1 â‰¤ i â‰¤ d, where 0 cii,s ds
can be estimated by, for instance, bipower variations.
We then apply the realized PCA procedure. We first examine the curse of dimensionality â€“ how increasing
number of stocks affects the estimation â€“ and how the sampling frequency affects the estimation. In light
of Corollary 1, we estimate 3 simple integrated eigenvalues as well as the average of the remaining identical
Rt
eigenvalues, denoted as 0 Î»is ds, i = 1, 2, 3, and 4. We report the mean and standard errors of the estimates
as well as the root-mean-square errors of the standardized estimates with d = 5, 10, 15, 20, 30, 50, and 100
stocks, respectively, using returns sampled every âˆ†n = 5 seconds, 1 minute and 5 minutes over one week and
one month horizons. The results, as shown from Tables 2 - 5, suggest that, as expected, the estimation is more
difficult as the dimensionality increases, but the large amount of high frequency data and in-fill asymptotic
techniques deliver very satisfactory finite sample approximations. The eigenvalues are accurately recovered.
The repeated eigenvalues are estimated with smaller biases and standard errors, due to the extra averaging
taken at the estimation stage. In Tables 6 - 7, we provide estimates for the first eigenvectors. Similar to the
estimation for eigenvalues, the estimates are very accurate, even with 100 stocks.
To further verify the accuracy of the asymptotic distribution in small samples as the sample size increases,
we provide in Figure 1 histograms of the standard estimates of integrated eigenvalues using 30 stocks with
5-second returns. Finally, we examine the finite sample accuracy of the integrated simple eigenvalues, in the
more challenging scenario with 100 stocks sampled every minute, which matches the setup of the empirical
analysis we will conduct below. To verify that the detection of 3 eigenvalues is not an artifact, we vary the
number of common factors in the data generating process from r = 2 to 4, by removing the third factor or
adding another factor that shares the same parameters as the third factor. The histograms are provided in
Figure 2. The results collectively show that the finite sample performance of the method is quite good even
for a 100-stock portfolio with one-minute returns and one-week horizon.
16

5

High-Frequency Principal Components in the S&P 100 Stocks

Given the encouraging Monte Carlo results, we now conduct PCA on intraday returns of S&P 100 Index
(OEX) constituents. We collect stock prices over the 2003 - 2012 period from the Trade and Quote (TAQ)
database of the New York Stock Exchange (NYSE).4 Due to entry and exit from the index, there are in total
158 tickers over this 10-year period. We conduct PCA on a weekly basis. One of the key advantages of the
large amount of high frequency data is that we are effectively able to create a ten-year long time series of
eigenvalues and principal components at the weekly frequency by collating the results obtained over each week
in the sample.
After removing those weeks during which the index constituents are switching, we are left with 482 weeks.
To clean the data, for each ticker on each trading day, we only keep the intraday prices from the single
exchange which has the largest number of transaction records. We provide in Figure 3 the quantiles of the
number of transactions between 9:35 a.m. EST and 3:55 p.m. EST, across 100 stocks each day. These stocks
have excellent liquidity over the sampling period. We thereby employ 1-minute subsamples for the most liquid
90 stocks in the index (for simplicity, we still refer to this 90-stock subsample as the S&P 100) in order to
address any potential concerns regarding microstructure noise and asynchronous trading.5 These stocks trade
multiple times per sampling interval and we compute returns using the latest prices recorded within each
sampling interval. Overnight returns are removed so that there is no concern of price changes due to dividend
distributions or stock splits. The 158 time series of cumulative returns are plotted in Figure 4.

5.1

Scree Plot

We report the time series average of the eigenvalue estimates against their order in Figure 5, a plot known in
classical PCA analysis as the â€œscree plotâ€ (see e.g. Jolliffe (2002)). This plot classically constitutes the main
graphical aid employed to determine the appropriate number of components in empirical applications. The
graph reveals that there are a handful of eigenvalue estimates, three, which are separated from the rest. The
fact that three factors on average explain the cross-sectional variation of stock returns is surprisingly consistent
with the well-established low frequency Fama-French common factor analysis, despite the large differences in
methods, time periods, sample frequencies and length of observation.
We also plot the estimates of the percentage variation explained by the first three integrated eigenvalues in
Figure 6, along with the average variation explained by the remaining eigenvalues. There are substantial time
variation of the first three eigenvalues, all of which are at peak around the recent financial crisis, indicating
an increased level of comovement, which in this context is the definition of systemic risk. The idiosyncratic
factors become relatively less important and even more dominated by the common factors during the crisis.
The first eigenvalue accounts for on average 30-40% of the total variations of the 90 constituents, capturing
the extent of the market variation in the sample. The second and third components together account for an
additional 15%-20% of the total variation. Therefore, there exists significant amount of remaining idiosyncratic
variation, beyond what can be explained by a three-common-factor model. The average variation explained
by the remaining 87 components are around 0.4%-0.6%, which corresponds to the idiosyncratic contributions
relative to the first three principal components.
4 While the constituents of the OEX Index change over time, we keep track of the changes to ensure that our choice of stocks
is always in line with the index constituents.
5 Estimators that are robust to both noise and asynchronicity are available for integrated covariance estimation in AÄ±Ìˆt-Sahalia,
Fan, and Xiu (2010), Christensen, Kinnebrock, and Podolskij (2010), Barndorff-Nielsen, Hansen, Lunde, and Shephard (2011),
and Shephard and Xiu (2012). Extending the current method to a noisy and asynchronous setting is theoretically interesting but
not empirically necessary for the present paper; it is left for future work.

17

Next, we compare the cumulative returns of the first three cumulative principal components. The time
series plots are shown in Figure 7. The empirical correlations among the three components are -0.020, 0.005,
and -0.074, respectively, which agrees with the design that the components are orthogonal. The first principal
component accounts for most of the common variation, and it shares the time series features of the overall
market return. This is further reinforced by the fact that the loadings of all the stocks on the first principal
component, although time-varying, remain positive throughout the sample period, which is not the case for
the additional principal components. It is worth pointing out however that these principal components are
not constrained to be portfolio returns, as their weights at each point in time are nowhere constrained to add
up to one. This means that the first principal component cannot be taken directly to be the market portfolio,
or more generally identified with additional Fama-French or additional mimicking portfolios.

5.2

Biplots

Although PCA is by design not suited to identifying the underlying economic factors corresponding to the
principal components, the time series evidence in Figure 7 suggests that the first principal component captures
the features of the overall market return, as already noted. Can we extract more information about the
economic nature of the principal components? For this purpose, we can use our results to produce biplots (see
Gabriel (1971)) and see which assets line up together in the basis of the principal components. A biplot gives
the relative position of the d variables on a plot where two principal components are the axes. The length of a
vector from the origin pointing towards the position of a variable on the biplot tends to reflect the magnitude
of the variation of the variable. Moreover, vectors associated with similar variables tend to point towards the
same direction, hence a biplot can be used for classification of the assets. While it is possible to compute
biplots for the spot eigenvectors, the biplots for the integrated eigenvectors are more stable.
We report two representative snapshots of the biplots of the first two integrated eigenvectors in Figures
8 (March 7-11, 2005, preceding the crisis) and 9 (March 10-14, 2008, during the crisis, although not at its
most extreme), in order to interpret the second principal component. We identify differently in the figures
the vectors that correspond to financial stocks (identified using the Global Industrial Classification Standard
(GICS) codes). The Federal Reserve made several important announcement during the week of March 10
-14, 2008 to address heightened liquidity pressures of the financial system, including the creation of the Term
Securities Lending Facility, and the approval of a financing arrangement between J.P. Morgan Chase and Bear
Stearns.
We find that the financial sector is clearly separated from the rest of the stocks during March 10-14, 2008,
in sharp contrast with the biplot for the week of March 7 - 11, 2005, during which no such separation occurs.
This suggests that PCA based on low-frequency data is likely to overlook these patterns. Interestingly, we can
also see from the March 2008 biplot that the Lehman Brothers (LEH) stands out, having the largest loadings
on both components. The corresponding scree plots for these two weeks in Figures 10 and 11 both suggest
the presence of at least three factors, but the eigenvalues in March 10-14, 2008 are much larger in magnitude,
consistently with a higher systematic component to asset returns. For each week, the figures report 95%
confidence intervals for the first three eigenvalues computed based on the distribution given in Corollary 1.

6

Conclusions

This paper develops the tools necessary to implement PCA at high frequency, constructing and estimating
realized eigenvalues, eigenvectors and principal components. This development complements the classical
PCA theory in a number of ways. Compared to its low frequency counterpart, PCA becomes feasible over
18

short windows of observation (of the order of one week), relatively large dimensions (90 in our application)
and further are free from the need to impose strong parametric assumptions on the distribution of the data,
applying instead to a broad class of semimartingales. The estimators perform well in simulations and reveal
that the joint dynamics of the S&P 100 stocks at high frequency are well explained by a three-factor model,
a result that is broadly consistent with the Fama-French factor model at low frequency, surprisingly so given
the large differences in time scale, sampling and returns horizon.
This paper represents a necessary first step to bring PCA tools to a high frequency setting. Although not
empirically relevant in the context of the analysis above of a portfolio of highly liquid stocks at the one-minute
frequency, the next steps in the development of the theory will likely include the incorporation of noise-robust
and asynchronicity-robust covariance estimation methods. We hope to pursue these extensions in future work.

19

References
AÄ±Ìˆt-Sahalia, Y., J. Fan, and D. Xiu (2010): â€œHigh-Frequency Covariance Estimates with Noisy and
Asynchronous Data,â€ Journal of the American Statistical Association, 105, 1504â€“1517.
AÄ±Ìˆt-Sahalia, Y., and J. Jacod (2014): High Frequency Financial Econometrics. Princeton University Press.
Amini, A. A., and M. J. Wainwright (2009): â€œHigh-Dimensional Analysis of Semidefinite Relaxations for
Sparse Principal Components,â€ Annals of Statistics, 37(5B), 2877â€“2921.
Anderson, T. W. (1958): An Introduction to Multivariate Statistical Analysis. Wiley, New York.
(1963): â€œAsymptotic Theory for Principal Component Analysis,â€ Annals of Mathematical Statistics,
34, 122â€“148.
Bai, J. (2003): â€œInferential Theory for Factor models of Large Dimensions,â€ Econometrica, 71, 135â€“171.
Bai, J., and S. Ng (2002): â€œDetermining the Number of Factors in Approximate Factor Models,â€ Econometrica, 70, 191â€“221.
Bai, Z., J. W. Silverstein, and Y. Q. Yin (1988): â€œA Note on the Largest Eigenvalue of a Large Dimensional Covariance Matrix,â€ Journal of Multivariate Analysis, 26, 166â€“168.
Baker, M., and J. Wurgler (2006): â€œInvestor Sentiment and the Cross-Section of Stock Returns,â€ The
Journal of Finance, 61(4), 1645â€“1680.
Baker, S. R., N. Bloom, and S. J. Davis (2013): â€œMeasuring Economic Policy Uncertainty,â€ Discussion
paper, Stanford University and University of Chicago.
Ball, J. M. (1984): â€œDifferentiability Properties of Symmetric and Isotropic Functions,â€ Duke Mathematical
Journal, 51, 699â€“728.
Barndorff-Nielsen, O. E., P. R. Hansen, A. Lunde, and N. Shephard (2011): â€œMultivariate Realised
Kernels: Consistent Positive Semi-Definite Estimators of the Covariation of Equity Prices with Noise and
Non-Synchronous Trading,â€ Journal of Econometrics, 162, 149â€“169.
Bickel, P. J., and E. Levina (2008a): â€œCovariance Regularization by Thresholding,â€ Annals of Statistics,
36(6), 2577â€“2604.
(2008b): â€œRegularized Estimation of Large Covariance Matrices,â€ Annals of Statistics, 36, 199â€“227.
Brillinger, D. R. (2001): Time Series: Data Analysis and Theory, Classics in Applied Mathematics (Book
36). SIAM: Society for Industrial and Applied Mathematics.
Cai, T. T., and H. H. Zhou (2012): â€œOptimal Rates of Convergence for Sparse Covariance Matrix Estimation,â€ Annals of Statistics, 40(5), 2389â€“2420.
Chamberlain, G., and M. Rothschild (1983): â€œArbitrage, Factor Structure, and Mean-Variance Analysis
on Large Asset Markets,â€ Econometrica, 51, 1281â€“1304.
Christensen, K., S. Kinnebrock, and M. Podolskij (2010): â€œPre-averaging estimators of the ex-post
covariance matrix in noisy diffusion models with non-synchronous data,â€ Journal of Econometrics, 159,
116â€“133.
20

Connor, G., and R. Korajczyk (1988): â€œRisk and Return in an Equilibrium APT: Application of a New
Test Methodology,â€ Journal of Financial Economics, 21, 255â€“289.
dâ€™Aspremont, A., L. E. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet (2007): â€œA Direct Formulation
for Sparse PCA Using Semidefinite Programming,â€ SIAM Review, 49(3), 434â€“448.
Davis, C. (1957): â€œAll Convex Invariant Functions of Hermitian Matrices,â€ Archiv der Mathematik, 8(4),
276â€“278.
Egloff, D., M. Leippold, and L. Wu (2010): â€œThe term structure of variance swap rates and optimal
variance swap investments,â€ Journal of Financial and Quantitative Analysis, 45, 1279â€“1310.
Fama, E. F., and K. R. French (1993): â€œCommon Risk Factors in the Returns on Stocks and Bonds,â€
Journal of Financial Economics, 33, 3â€“56.
Fan, J., A. Furger, and D. Xiu (2015): â€œIncorporating Global Industrial Classification Standard into
Portfolio Allocation: A Simple Factor-Based Large Covariance Matrix Estimator with High Frequency
Data,â€ Journal of Business and Economic Statistics, forthcoming.
Forni, M., M. Hallin, M. Lippi, and L. Reichlin (2000): â€œThe Generalized Dynamic-Factor Model:
Identification and Estimation,â€ The Review of Economics and Statistics, 82, 540â€“554.
(2004): â€œThe Generalized Dynamic Factor Model: Consistency and Rates,â€ Journal of Econometrics,
119(2), 231â€“255.
Forni, M., and M. Lippi (2001): â€œThe Generalized Dynamic Factor Model: Representation Theory,â€ Econometric Theory, 17, 1113â€“1141.
Friedland, S. (1981): â€œConvex Spectral Functions,â€ Linear and Multilinear Algebra, 9, 299â€“316.
Gabriel, K. (1971): â€œThe biplot graphic display of matrices with application to pricipal component analysis,â€
Biometrika, 58, 453â€“467.
Geman, S. (1980): â€œA Limit Theorem for the Norm of Random Matrices,â€ Annals of Probability, 8, 252â€“261.
Heinrich, C., and M. Podolskij (2014): â€œOn Spectral Distribution of High Dimensional Covaraition
Matrices,â€ Discussion paper, University of Aarhus.
Horn, R. A., and C. R. Johnson (2013): Matrix Analysis. Cambridge University Press, second edn.
Hotelling, H. (1933): â€œAnalysis of a Complex of Statistical Variables into Principal Components,â€ Journal
of Educational Psychology, 24, 417â€“441, 498â€“520.
Jackson, J. E. (2003): A Userâ€™s Guide to Principal Components. Wiley.
Jacod, J., A. Lejay, and D. Talay (2008): â€œEstimation of the Brownian dimension of a continuous ItoÌ‚
process,â€ Bernoulli, 14, 469â€“498.
Jacod, J., and M. Podolskij (2013): â€œA test for the rank of the volatility process: The Random Perturbation Approach,â€ Annals of Statistics, 41, 2391â€“2427.
Jacod, J., and P. Protter (2011): Discretization of Processes. Springer-Verlag.

21

Jacod, J., and M. Rosenbaum (2013): â€œQuarticity and Other Functionals of Volatility: Efficient Estimation,â€ Annals of Statistics, 41, 1462â€“1484.
Jacod, J., and A. N. Shiryaev (2003): Limit Theorems for Stochastic Processes. Springer-Verlag, second
edn.
Johnstone, I. M. (2001): â€œOn the distribution of the largest eigenvalue in principal components analysis,â€
Annals of Statistics, 29, 295â€“327.
Johnstone, I. M., and A. Y. Lu (2009): â€œOn Consistency and Sparsity for Principal Components Analysis
in High Dimensions,â€ Journal of the American Statistical Association, 104(486), 682â€“693.
Jolliffe, I. T. (2002): Principal Component Analysis. Springer-Verlag.
Jolliffe, I. T., N. T. Trendafilov, and M. Uddin (2003): â€œA Modified Principal Component Technique
Based on the LASSO,â€ Journal of Computational and Graphical Statistics, 12(3), 531â€“547.
Kalnina, I., and D. Xiu (2013): â€œModel-Free Leverage Effect Estimators at High Frequency,â€ Discussion
paper, UniversiteÌ de MontreÌal and University of Chicago.
Lewis, A. S. (1996a): â€œConvex Analysis on the Hermitian Matrices,â€ SIAM Journal of Optimizaiton, 6(1),
164â€“177.
(1996b): â€œDerivatives of Spectral Functions,â€ Mathematics of Operations Research, 21, 576â€“588.
Lewis, A. S., and H. S. Sendov (2001): â€œTwice Differentiable Spectral Functions,â€ SIAM Journal on
Matrix Analysis and Applications, 23, 368â€“386.
Li, J., V. Todorov, and G. Tauchen (2013): â€œInference Theory on Volatility Functional Dependencies,â€
Discussion paper, Duke University.
(2014): â€œAdaptive Estimation of Continuous-Time Regression Models using High-Frequency Data,â€
Discussion paper, Duke University.
Li, J., and D. Xiu (2014): â€œGeneralized Method of Integrated Moments for High-Frequency Data,â€ Discussion
paper, Duke University and The University of Chicago.
Litterman, R., and J. Scheinkman (1991): â€œCommon factors affecting bond returns,â€ Journal of Fixed
Income, June, 54â€“61.
Magnus, J. R., and H. Neudecker (1999): Matrix Differential Calculus with Applications in Statistics and
Economics. Wiley.
Mykland, P. A., and L. Zhang (2009): â€œInference for continuous semimartingales observed at high frequency,â€ Econometrica, 77, 1403â€“1445.
Okamoto, M. (1973): â€œDistinctness of the Eigenvalues of a Quadratic form in a Multivariate Sample,â€
Annals of Statistics, 1, 763â€“765.
Pearson, K. (1901): â€œOn Lines and Planes of Closest Fit to Systems of Points in Space,â€ Philosophical
Magazine, 2, 559â€“572.

22

Protter, P. (2004): Stochastic Integration and Differential Equations: A New Approach. Springer-Verlag,
second edn.
Rockafellar, R. T. (1997): Convex Analysis. Princeton University Press.
Ross, S. A. (1976): â€œThe Arbitrage Theory of Capital Asset Pricing,â€ Journal of Economic Theory, 13,
341â€“360.
Shephard, N., and D. Xiu (2012): â€œEconometric analysis of multivariate realized QML: Estimation of
the covariation of equity prices under asynchronous trading,â€ Discussion paper, University of Oxford and
University of Chicago.
SilhavyÌ, M. (2000): â€œDifferentiability Properties of Isotropic Functions,â€ Duke Mathematical Journal, 104,
367â€“373.
Stock, J. H., and M. W. Watson (1998): â€œDiffusion Indexes,â€ Discussion paper, NBER.
(1999): â€œForecasting Inflation,â€ Journal of Monetary Economics, 44, 293â€“335.
(2002): â€œForecasting using Principal Components from a Large Number of Predictors,â€ Journal of
American Statistical Association, 97, 1167â€“1179.
Sylvester, J. (1985): â€œOn the Differentiablity of O(n) Invaraint Functions of Symmetric Matrices,â€ Duke
Mathematical Journal, 52.
Tao, M., Y. Wang, and X. Chen (2013): â€œFast Convergence Rates in Estimating Large Volatility Matrices
Using High-Frequency Financial Data,â€ Econometric Theory, 29(4), 838â€“856.
Tao, M., Y. Wang, Q. Yao, and J. Zou (2011): â€œLarge Volatility Matrix Inference via Combining LowFrequency and High-Frequency Approaches,â€ Journal of the American Statistical Association, 106, 1025â€“
1040.
Tao, M., Y. Wang, and H. H. Zhou (2013): â€œOptimal Sparse Volatility Matrix Estimation for HighDimensional ItoÌ‚ Processes with Measurement Errors,â€ Annals of Statistics, 41(1), 1816â€“1864.
Tao, T. (2012): Topics in Random Matrix Theory. American Mathematical Society.
Tyler, D. E. (1981): â€œAsymptotic Inference for Eigenvectors,â€ Annals of Statistics, 9, 725â€“736.
Wang, Y., and J. Zou (2010): â€œVast volatility matrix estimation for high-frequency financial data,â€ Annals
of Statistics, 38, 943â€“978.
Waternaux, C. M. (1976): â€œAsymptotic Distribution of the Sample Roots for a Nonnormal Population,â€
Biometrika, 63, 639â€“645.
Zheng, X., and Y. Li (2011): â€œOn the Estimation of Integrated Covariance Matrices of High Dimensional
Diffusion Processes,â€ Annals of Statistics, 39, 3121â€“3151.
Zou, H., T. Hastie, and R. Tibshirani (2006): â€œSparse Principal Component Analysis,â€ Journal of
Computational and Graphical Statistics, 15(2), 265â€“286.

23

Appendix A
Appendix A.1

Mathematical Proofs
Proof of Lemma 1

Proof. By the classical Weyl inequalities in e.g., Horn and Johnson (2013) and Tao (2012), we have |Î»j (A +
) âˆ’ Î»j (A)| â‰¤ K kk, for all j = 1, 2, . . . , d, where A,  âˆˆ M+
d , and K is some constant. This establishes the
Lipchitz property.

Appendix A.2

Proof of Lemma 2

Proof. It is straightforward to show (by the implicit function theorem, see Magnus and Neudecker (1999)
Theorem 8.7) that any simple eigenvalue and its corresponding eigenvector, written as functions of A, Î»g (A)
and Î³ g (A), are C âˆ . To calculate their derivatives, note that AÎ³ g = Î»g Î³ g , hence we have (âˆ‚jk A)Î³ g +A(âˆ‚jk Î³ g ) =
(âˆ‚jk Î»g )Î³ g + Î»g (âˆ‚jk Î³ g ). Pre-multiplying Î³ |g on both sides yields âˆ‚jk Î»g = Î³ |g (âˆ‚jk A)Î³ g = Î³ gj Î³ gk . Rewrite it into
(Î»g I âˆ’ A)âˆ‚jk Î³ g = (âˆ‚jk A)Î³ g âˆ’ (âˆ‚jk Î»g )Î³ g , which leads to (Î»g I âˆ’ A)+ (Î»g I âˆ’ A)âˆ‚jk Î³ g = (Î»g I âˆ’ A)+ (âˆ‚jk A)Î³ g . As
a result, âˆ‚jk Î³ g = (Î»g I âˆ’ A)+ Jjk Î³ g .
In the case when all eigenvalues are simple, by direct calculation we have
âˆ‚jk Î³ gh =

X
p6=g

1
Î³ Î³ Î³ ,
Î»g âˆ’ Î»p ph pj gk

where we use the fact that (Î³ |1 , Î³ |2 , . . . , Î³ |d )| A(Î³ 1 , Î³ 2 , . . . , Î³ d ) = Diag(Î»(A)). Further,
2
âˆ‚jk,lm
Î³ gh = âˆ’

X

=âˆ’

X

p6=g

p6=g

X

1
1
(âˆ‚lm Î»g âˆ’ âˆ‚lm Î»p )Î³ ph Î³ pj Î³ gk +
âˆ‚lm Î³ ph Î³ pj Î³ gk
2
(Î»g âˆ’ Î»p )
Î»g âˆ’ Î»p
p6=g


1
Î³ gl Î³ gm Î³ ph Î³ pj Î³ gk âˆ’ Î³ pl Î³ pm Î³ ph Î³ pj Î³ gk
2
(Î»g âˆ’ Î»p )

+

XX

+

XX

+

XX

p6=g q6=p

p6=g q6=p

p6=g q6=g

1
Î³ Î³ Î³ Î³ Î³
(Î»g âˆ’ Î»p )(Î»p âˆ’ Î»q ) ql pm qh pj gk
1
Î³ Î³ Î³ Î³ Î³
(Î»g âˆ’ Î»p )(Î»p âˆ’ Î»q ) ql pm qj ph gk
1
Î³ Î³ Î³ Î³ Î³ ,
(Î»g âˆ’ Î»p )(Î»g âˆ’ Î»q ) qk ql gm ph pj

which concludes the proof.

Appendix A.3

Proof of Lemma 3

Proof. The proof is by induction. Consider, at first the following optimization problem:
Z u
max
Î³ |s cs Î³ s ds, s.t. Î³ |s Î³ s = 1, 0 â‰¤ s â‰¤ u â‰¤ t.
Î³s

0

Using a sequence of Lagrange multipliers Î»s , the problem can be written as solving
cs Î³ s = Î»s Î³ s ,

and Î³ |s Î³ s = 1, for any 0 â‰¤ s â‰¤ t.

Hence, the original problem is translated into eigenanalysis.

24

Suppose the eigenvalues of cs are ordered as in Î»1,s â‰¥ Î»2,s â‰¥ . . . â‰¥ Î»d,s . Note that Î³ |s cs Î³ s = Î»s , so that
Î»s = Î»1,s , and Î³ s = Î³ 1,s is one of the corresponding eigenvectors (if Î»1,s is not unique), and the maximal
Rt
variation is 0 Î»1,s ds.
Suppose that we have found Î³ 1,s , . . . , Î³ k,s , for 1 â‰¤ k < d and 0 â‰¤ s â‰¤ t, the (k + 1)th principal component
is defined by solving the following problem:
Z u
Î³ |s cs Î³ s ds, s.t. Î³ |s Î³ s = 1, and Î³ |j,s cs Î³ s = 0, for 1 â‰¤ j â‰¤ k, 0 â‰¤ u â‰¤ t.
max
Î³s

0

Using similar technique of Lagrange multipliers, Î»s , and Î½ 1,s , . . . , Î½ k,s , we find
cs Î³ s = Î»s Î³ s +

k
X

Î½ j,s cs Î³ j,s .

j=1

Multiplying on the left Î³ |l,s , for some 1 â‰¤ l â‰¤ k, we can show that Î½ l,s cs Î³ l,s = 0. Indeed,
0 = Î»l,s Î³ |l,s Î³ s = Î³ |l,s cs Î³ s = Î³ |l,s Î»s Î³ s +

k
X

Î½ j,s Î³ |l,s cs Î³ j,s = Î½ l,s Î»l,s .

j=1

Therefore, since l is an arbitrary number between 1 and k, we have cs Î³ s = Î»s Î³ s . Hence, Î»s = Î»k+1,s ,
Î³ s = Î³ k+1,s is one of the eigenvectors associated with the eigenvalue Î»k+1,s . This establishes the first part of
the theorem.
For any caÌ€dlaÌ€g and adapted process Î³ s ,
Z u
c Z t
Z u
Î³ |sâˆ’ dXs =
Î³ |sâˆ’ dXs ,
Î³ |s cs Î³ s ds.
0

0

0

Hence the statement follows from the gth-step optimization problem. Note that the validity of the integrals
above is warranted by the continuity of Î» given by Lemma 1.

Appendix A.4

Proof of Lemma 4

Proof. The first statement of the proof follows by immediate calculations from Theorem 1.1 in Lewis (1996b)
and Theorem 3.3 in Lewis and Sendov (2001). The second statement is discussed and proved in, e.g., Ball
(1984), Sylvester (1985), and SilhavyÌ (2000). Finally, the last statement on convexity is proved in Davis (1957)
and Lewis (1996a).

Appendix A.5

Proof of Lemma 5

Proof. Obviously, for any 1 â‰¤ g1 < g2 < . . . < gr â‰¤ d, the set defined in (4), D(g1 , g2 , . . . , gr ), is an open set in
P
R+
i6=j |xÌ„gi âˆ’ xÌ„gj |, which is a continuous and convex function. It is differentiable
d /{0}. Define f (x) = |xÌ„gr | +
at x if and only if x âˆˆ D(g1 , g2 , . . . , gr ). Therefore, by Lemma 4, f â—¦ Î» is convex, and it is differentiable at
A if and only if Î»(A) âˆˆ D(g1 , g2 , . . . , gr ), i.e., A âˆˆ M(g1 , g2 , . . . , gr ). On the other hand, a convex function
is almost everywhere differentiable, see Rockafellar (1997), which implies that M(g1 , g2 , . . . , gr ) is dense in
+
M++
d . Moreover, M(g1 , g2 , . . . , gr ) is the pre-image of the open set R /{0} under a continuous function h â—¦ Î»,
Q
where h(x) = i6=j |xÌ„gi âˆ’ xÌ„gj ||xÌ„gr |. Therefore, it is open.

25

Appendix A.6

Proof of Theorem 1

Proof. Note that
[t/(kn âˆ†n )]

X

V (âˆ†n , X; F ) = kn âˆ†n

[t/(kn âˆ†n )]


X
bik âˆ† = kn âˆ†n
f Î»
(f â—¦ Î»)(b
cikn âˆ†n ).
n n

i=0

i=0

By Assumption 2 and Lemma 4, f â—¦ Î» is a continuous vector-valued function. Moreover, for c âˆˆ M+
d,
Î¶
Î¶
kf â—¦ Î»(c)k â‰¤ K(1 + kÎ»(c)k ) â‰¤ K(1 + kck ). Below we prove this theorem for any spectral function F that is
Î¶
bounded by K(1 + kck ).
We start with a function F bounded by K everywhere. We extend the definition of b
c to the entire interval
[0, t] by letting:
b
cs = b
c(iâˆ’1)kn âˆ†n , for

(i âˆ’ 1)kn âˆ†n â‰¤ s < ikn âˆ†n .

Note that for any t > 0, we have
Z t
E V (âˆ†n , X; F ) âˆ’
F (cs )ds
0

Z

[t/(kn âˆ†n )]kn âˆ†n

Z

0

Z

t

E kF (b
cs ) âˆ’ F (cs )k ds +

c[t/(kn âˆ†n )]kn âˆ†n ) +
â‰¤kn âˆ†n E F (b

E kF (cs )k ds
[t/(kn âˆ†n )]kn âˆ†n

[t/(kn âˆ†n )]kn âˆ†n

â‰¤Kkn âˆ†n +

E kF (b
cs ) âˆ’ F (cs )k ds.
0
p

By the fact that b
cs âˆ’ cs âˆ’â†’ 0, it follows that E kF (b
cs ) âˆ’ F (cs )k â†’ 0, which is bounded uniformly in s and n
because F is bounded. Therefore, by the dominated convergence theorem, we obtain the desired convergence.
Next we show the convergence holds under the polynomial bound on F . Denote Ïˆ to be a C âˆ function on
R+ such that 1[1,âˆ) (x) â‰¤ Ïˆ(x) â‰¤ 1[1/2,âˆ] (x). Let Ïˆ Îµ (c) = Ïˆ(kck /Îµ), and Ïˆ 0Îµ (c) = 1 âˆ’ Ïˆ Îµ (c). Since the function
p Rt
F Â· Ïˆ 0Îµ is continuous and bounded, the above argument implies that V (âˆ†n , X; F Â· Ïˆ 0Îµ ) âˆ’â†’ 0 F Â· Ïˆ 0Îµ (cs )ds, for
Rt
Rt
any fixed Îµ. When  is large enough, we have 0 F Â· Ïˆ 0Îµ (cs )ds = 0 F (cs )ds by localization, since cs is locally
Î¶
bounded. On the other hand, F Â· Ïˆ Îµ (c) â‰¤ K kck 1{kckâ‰¥Îµ} , for Îµ > 1. So it remains to show that
ï£«
ï£¶
[t/(kn âˆ†n )]
X
Î¶
lim lim sup E ï£­kn âˆ†n
kb
cikn âˆ†n k 1{kbcikn âˆ†n k>Îµ} ï£¸ = 0.
Îµâ†’âˆ nâ†’âˆ

i=0

By (9.4.7) of Jacod and Protter (2011), there exists some sequence an going to 0, such that

 K
Î¶
E kb
cikn âˆ†n k 1{kbcikn âˆ†n k>Îµ} |Fikn âˆ†n â‰¤ Î¶ + Kan âˆ†n(1âˆ’Î¶+$(2Î¶âˆ’Î³)) ,
Îµ
which establishes the desired result.

Appendix A.7

Proof of Proposition 1

Proof. We divide the proof into several steps. To start, we need some additional notations. Let X 0 and c0
denote the continuous parts of the processes X and c, respectively. Also, we introduce b
c0ikn âˆ†n to denote the
estimator constructed similarly as in (5) with X replaced by X 0 and without truncation, namely
b
c0ikn âˆ†n =

kn
1 X
(âˆ†n
X 0 )(âˆ†nikn +j X 0 )| .
kn âˆ†n j=1 ikn +j

26

b0
In addition, Î»
c0ikn âˆ†n , and
ikn âˆ†n corresponds to the vector of eigenvalues of b
[t/(kn âˆ†n )]

X

V 0 (âˆ†n , X; F ) = kn âˆ†n

 0

b
f Î»
ikn âˆ†n .

i=0

We also define
1
kn âˆ†n
 
Î· ni = E

Z

(i+1)kn âˆ†n

cs ds,

cÌ„ikn âˆ†n =

ikn âˆ†n

sup

iâˆ†n â‰¤uâ‰¤iâˆ†n +kn âˆ†n

Î² nikn = b
c0ikn âˆ†n âˆ’ cikn âˆ†n ,

|biâˆ†n +u âˆ’ biâˆ†n |2 |Fiâˆ†n

Î±nl = (âˆ†nl X)(âˆ†nl X)| âˆ’ clâˆ†n âˆ†n ,

and

1/2
.

We first collect some known estimates in the next lemma:
Lemma 6. Under the assumptions of Proposition 1, we have


q
E sup kct+u âˆ’ ct k |Ft â‰¤ Ks1âˆ§q/2 , kE (ct+s âˆ’ ct |Ft )k â‰¤ Ks,

(A.1)

0â‰¤uâ‰¤s

E (âˆ†ni X)(âˆ†ni X)| 1{kâˆ†n X kâ‰¤un } âˆ’ (âˆ†ni X 0 )(âˆ†ni X 0 )| â‰¤ Kan âˆ†n(2âˆ’r)$+1 , for some an â†’ 0,
i
q
â‰¤ Kan âˆ†n(2qâˆ’r)$+1âˆ’q , for some q â‰¥ 1, and an â†’ 0,
cikn âˆ†n âˆ’ b
c0ikn âˆ†n
E b
p

c0ikn âˆ†n âˆ’ cÌ„ikn âˆ†n
E b
q
E (kÎ±ni k

|Fiâˆ†n ) â‰¤

â‰¤ Kknâˆ’p/2 , for some p â‰¥ 1,

(A.2)
(A.3)
(A.4)

Kâˆ†qn ,

for some q â‰¥ 0,


1/2
n
kE (Î±ni |Fiâˆ†n )k â‰¤ Kâˆ†3/2
âˆ†
+
Î·
n
n
i ,




jm kl
km
2
E Î±n,jk
Î±n,lm
âˆ’ cjl
â‰¤ Kâˆ†5/2
n ,
i
i
iâˆ†n ciâˆ†n + ciâˆ†n ciâˆ†n âˆ†n |Fiâˆ†n



n
E Î² nikn |Fikn âˆ†n â‰¤ Kâˆ†n 1/2 kn âˆ†1/2
n + Î·i ,



q
E Î² nikn |Fikn âˆ†n â‰¤ K knâˆ’q/2 + kn âˆ†n , for some q â‰¥ 2,

(A.5)
(A.6)
(A.7)
(A.8)
(A.9)

[t/âˆ†n ]

âˆ†n E

X

Î· ni â†’ 0.

(A.10)

i=1

Proof of Lemma 6. These estimates are given by Lemma A.2 in Li and Xiu (2014), (4.3), (4.8), (4.10), (4.11),
(4.12), (4.18), Lemmas 4.2 and 4.3 of Jacod and Rosenbaum (2013), and Lemma 13.2.6 of Jacod and Protter
(2011).
Now we return to the proof of Proposition 1.
1) We show that we can restrict the domain of function f to some compact set, where both the estimates
{b
cikn âˆ†n }i=0,1,2,...,[t/(kn âˆ†n )] and the sample path of {cs }sâˆˆ[0,t] take values. By (A.4), we have for p â‰¥ 1,
E b
c0ikn âˆ†n âˆ’ cÌ„ikn âˆ†n

p

â‰¤ Kknâˆ’p/2 .

Therefore, by the maximal inequality, we deduce, by picking p > 2/Ï‚ âˆ’ 2,
sup

E

0â‰¤iâ‰¤[t/(kn âˆ†n )]

b
c0ikn âˆ†n âˆ’ cÌ„ikn âˆ†n

p

âˆ’p/2âˆ’1
â‰¤ Kâˆ†âˆ’1
â†’ 0,
n kn

therefore, sup0â‰¤iâ‰¤[t/(kn âˆ†n )] b
c0ikn âˆ†n âˆ’ cÌ„ikn âˆ†n = op (1). Moreover, by (A.2) we have
E

sup
0â‰¤iâ‰¤[t/(kn âˆ†n )]

b
cikn âˆ†n âˆ’

b
c0ikn âˆ†n

1
â‰¤
k n âˆ†n

[t/âˆ†n ]âˆ’kn

X

E (âˆ†ni X)(âˆ†ni X)| 1{kâˆ†n X kâ‰¤un } âˆ’ (âˆ†ni X 0 )(âˆ†ni X 0 )|
i

i=0

27

â‰¤Kan âˆ†n(2âˆ’Î³)$âˆ’1+Ï‚ â†’ 0.
As a result, we have as âˆ†n â†’ 0,
sup

p

kb
cikn âˆ†n âˆ’ cÌ„ikn âˆ†n k âˆ’â†’ 0.

(A.11)

0â‰¤iâ‰¤[t/(kn âˆ†n )]

Note that by Assumption 3, for 0 â‰¤ s â‰¤ t, cs âˆˆ C âˆ© Mâˆ— (g1 , g2 , . . . , gr ), where C is a convex and open set.
Therefore, {cÌ„ikn âˆ†n }i=0,1,2,...,[t/(kn âˆ†n )] âˆˆ C by convexity. For n large enough, {b
cikn âˆ†n }i=0,1,2,...,[t/(kn âˆ†n )] âˆˆ
Â¯
C, with probability approaching 1, by (A.11). Since C âŠ‚ M(g1 , g2 , . . . , gr ), we can restrict the domain
Â¯ âŠ‚ D(g1 , g2 , . . . , gr ), in which f is C âˆ with bounded derivatives. Moreover,
of f to the compact set Î»(C)
because Î»gj (Â·), 1 â‰¤ j â‰¤ r are continuous functions, min1â‰¤jâ‰¤râˆ’1 (Î»gj (Â·) âˆ’ Î»gj+1 (Â·)) is hence continuous, so that
inf câˆˆC {min1â‰¤jâ‰¤râˆ’1 (Î»gj (c) âˆ’ Î»gj+1 (c))} â‰¥  > 0. It follows from Lemma 4 and Theorem 3.5 of SilhavyÌ (2000)
that F (Â·) is C âˆ with bounded derivatives on C.
2) Next, we have
[t/(kn âˆ†n )]

X

kV (âˆ†n , X; F ) âˆ’ V 0 (âˆ†n , X; F )k â‰¤ kn âˆ†n

F (b
cikn âˆ†n ) âˆ’ F (b
c0ikn âˆ†n )

i=0
[t/(kn âˆ†n )]

â‰¤ Kkn âˆ†n

X

b
cikn âˆ†n âˆ’ b
c0ikn âˆ†n .

i=0

By (A.3), we have
E

b
cikn âˆ†n âˆ’ b
c0ikn âˆ†n



â‰¤ Kan âˆ†n(2âˆ’Î³)$ ,

where an is some sequence going to 0, as n â†’ âˆ, which implies
V (âˆ†n , X; F ) âˆ’ V 0 (âˆ†n , X; F ) = Op (an âˆ†n(2âˆ’r)$ ).

(A.12)

As a result, given the conditions on $, we have
kn (V (âˆ†n , X; F ) âˆ’ V 0 (âˆ†n , X; F )) = op (1),
hence we can proceed with V 0 in the sequel.
3) Then we show for each 1 â‰¤ h â‰¤ d, we have
0

kn V (âˆ†n , X; Fh ) âˆ’ kn âˆ†n

[t/(kn âˆ†n )] 

X

Fh (cikn âˆ†n )

i=0

1
âˆ’
2kn

d
X

2
âˆ‚jk,lm
Fh (cikn âˆ†n ) (cjl,ikn âˆ†n ckm,ikn âˆ†n

+ cjm,ikn âˆ†n ckl,ikn âˆ†n )



!
= op (1).

j,k,l,m=1

where Fh is the hth entry of the vector-valued function F .
To prove it, we decompose the left hand side into 4 terms:
n
R1,h
=kn2 âˆ†n

[t/(kn âˆ†n )] 

X

Fh (cikn âˆ†n + Î² nikn ) âˆ’ Fh (cikn âˆ†n ) âˆ’

i=0

âˆ’

1
2

d
X

d
X

âˆ‚lm Fh (cikn âˆ†n )Î² n,lm
ikn

l,m=1


n,jk
2
âˆ‚jk,lm
Fh (cikn âˆ†n )Î² n,lm
ikn Î² ikn ,

(A.13)

j,k,l,m=1

28

[t/(kn âˆ†n )]

1
2

X

n
R2,h
=kn2 âˆ†n

i=0

d
X
j,k,l,m=1



1
n,jk
2
âˆ‚jk,lm
Fh (cikn âˆ†n ) Î² n,lm
Î²
âˆ’
(c
c
+
c
c
)
,
jl,ikn âˆ†n km,ikn âˆ†n
jm,ikn âˆ†n kl,ikn âˆ†n
ikn
ikn
kn
(A.14)

[t/(kn âˆ†n )]
n
R3,h
=kn âˆ†n

X

d
X

i=0

l,m=1

[t/(kn âˆ†n )]
n
R4,h
=kn

X

d
X

i=0

l,m=1

âˆ‚lm Fh (cikn âˆ†n )

kn
X

(clm,(ikn +u)âˆ†n âˆ’ clm,ikn âˆ†n ),

(A.15)

u=1

âˆ‚lm Fh (cikn âˆ†n )

kn
X

Î±n,lm
ikn +u ,

(A.16)

u=1

n
We first consider R1,h
. By (A.9), we have
[t/(kn âˆ†n )]

X

n
E(|R1,h
|) â‰¤ Kkn2 âˆ†n

[t/(kn âˆ†n )]

E Î² nikn

3

â‰¤ Kkn2 âˆ†n

i=0

â‰¤

Kkn2 âˆ†n

+

X

(knâˆ’3/2 + kn âˆ†n )

i=0

Kknâˆ’1/2

â†’ 0.

n
n
As to R2,h
, we denote the term inside the summation of R2,h
as Î½ nikn . So we have
[t/(kn âˆ†n )]

X

n
R2,h
= kn2 âˆ†n


Î½ nikn âˆ’ E(Î½ nikn |Fikn âˆ†n ) + E(Î½ nikn |Fikn âˆ†n ) .

i=0

By (A.8), we have
|E(Î½ nikn |Fikn âˆ†n )| â‰¤ K

p
âˆ†n .

On the other hand, by (A.9), we can derive
2
E Î½ nikn âˆ’ E(Î½ nikn |Fikn âˆ†n ) â‰¤ Kkn âˆ†n .
Then Doobâ€™s inequality implies that
ï£«

[s/(kn âˆ†n )]

E ï£­sup
sâ‰¤t

X

Î½ nikn

ï£¶

âˆ’ E(Î½ nikn |Fikn âˆ†n ) ï£¸ â‰¤ Kt.

i=0

As a result,
ï£«

n
E |R2,h
| â‰¤kn2 âˆ†n E ï£­sup
sâ‰¤t

â‰¤Kkn2 âˆ†n

ï£¶

[s/(kn âˆ†n )]

X

[t/(kn âˆ†n )]

Î½ nikn âˆ’ E(Î½ nikn |Fikn âˆ†n ) ï£¸ + kn2 âˆ†n


i=0

X

E(Î½ nikn |Fikn âˆ†n )

i=0

p
+ Kkn âˆ†n â†’ 0.

n
The proof for E(|R3,h
|) â†’ 0 is similar. Denote the term inside the summand as Î¾ nikn . By (A.1) and the
Cauchy-Schwarz inequality, we have

|E(Î¾ nikn |Fikn âˆ†n )| â‰¤ Kkn2 âˆ†n ,


E |Î¾ nikn |2 |Fikn âˆ†n â‰¤ Kkn3 âˆ†n .

By Doobâ€™s inequality again,
ï£«

[t/(kn âˆ†n )]
n
E |R3,h
| â‰¤ k n âˆ†n



X

E E Î¾ nikn |Fikn âˆ†n



+ k n âˆ†n ï£­

[t/(kn âˆ†n )]

X
i=0

i=0

29

ï£¶1/2

E |Î¾ nikn |2 ï£¸

â‰¤ Kkn2 âˆ†n â†’ 0.
âˆš
n
n
For R4,h
, it can be shown in the proof of Theorem 2 below that R4,h
= Op (kn âˆ†n ) = op (1).
4) Finally, it is sufficient to show that
ï£«
Z
[t/(kn âˆ†n )] Z (i+1)kn âˆ†n
X
ï£­
kn
Fh (cikn âˆ†n )ds âˆ’
ikn âˆ†n

i=0

!

(i+1)kn âˆ†n

Fh (cs )ds

Z

t

âˆ’

ikn âˆ†n

ï£¶
p
Fh (cs )dsï£¸ âˆ’â†’ 0,

[t/(kn âˆ†n )]kn âˆ†n

2
as the similar result holds if we replace Fh (cikn âˆ†n ) by âˆ‚jk,lm
Fh (cikn âˆ†n ) (cjl,ikn âˆ†n ckm,ikn âˆ†n + cjm,ikn âˆ†n ckl,ikn âˆ†n ).
Since Fh is bounded, the second term is bounded by Kkn2 âˆ†n â†’ 0. As to the first term, we notice that
Z (i+1)kn âˆ†n
Z (i+1)kn âˆ†n
Î¶ nikn =
Fh (cikn âˆ†n )ds âˆ’
Fh (cs )ds
ikn âˆ†n

ikn âˆ†n

is measurable with respect to F(i+1)kn âˆ†n , and that

|E Î¶ nikn |Fikn âˆ†n | â‰¤ K(kn âˆ†n )2 ,


E |Î¶ nikn |2 |Fikn âˆ†n â‰¤ K(kn âˆ†n )3 ,

so the same steps as in (2) and (3) yield the desired results.

Appendix A.8

Proof of Theorem 2

Proof. To start, we decompose
ï£«
ï£¶
[t/(kn âˆ†n )]
X
1 ï£­e
1
âˆš
V (âˆ†n , X; F ) âˆ’ kn âˆ†n
F (cikn âˆ†n )ï£¸ = âˆš
(R1n + R2n + R3n + R4n + R5n + R6n ) ,
âˆ†n
k
âˆ†
n
n
i=0

(A.17)


|
n
n
n
n
n
n
n
given by equations
and R4,h
, R3,h
, R2,h
where Rin = Ri,1
, Ri,2
, . . . , Ri,d
, for i = 1, 2, 3, 4, and 5, with R1,h
n
n
(A.13) - (A.16). In addition, R5,h and R6,h are given by
n
R5,h
=

k n âˆ†n
2

[t/(kn âˆ†n )]

X

d
X

i=0

j,k,l,m=1

2
âˆ’âˆ‚jk,lm
Fh (b
c0ikn âˆ†n )
n
R6,h
=

k n âˆ†n
2

[t/(kn âˆ†n )]

2
âˆ‚jk,lm
Fh (cikn âˆ†n ) (cjl,ikn âˆ†n ckm,ikn âˆ†n + cjm,ikn âˆ†n ckl,ikn âˆ†n )

b
c0jl,ikn âˆ†n b
c0km,ikn âˆ†n + b
c0jm,ikn âˆ†n b
c0kl,ikn âˆ†n

X

d
X

i=0

j,k,l,m=1



.

2
âˆ‚jk,lm
Fh (b
c0ikn âˆ†n ) b
c0jl,ikn âˆ†n b
c0km,ikn âˆ†n + b
c0jm,ikn âˆ†n b
c0kl,ikn âˆ†n

2
âˆ’âˆ‚jk,lm
Fh (b
cikn âˆ†n ) (b
cjl,ikn âˆ†n b
ckm,ikn âˆ†n




+b
cjm,ikn âˆ†n b
ckl,ikn âˆ†n ) + kn (V (âˆ†n , X; F ) âˆ’ V 0 (âˆ†n , X; F )) .

We have shown in the proof of Proposition 1 that Rin = Op (kn2 âˆ†n ), for i = 1, 2, 3. Therefore, these terms do
not contribute to the asymptotic variance of Ve 0 (âˆ†n , X; F ).
âˆš
n
Next, we show that R5,h
is also op (kn âˆ†n ). By (A.9) and the mean-value theorem, we have
[t/(kn âˆ†n )]
n
E|R5,h
| â‰¤Kkn âˆ†n

X

E cikn âˆ†n âˆ’ b
c0ikn âˆ†n â‰¤ K(knâˆ’1/2 + kn âˆ†n ) = op (kn

p

âˆ†n ).

i=0
n
As to R6,h
, by (A.12) and the mean-value theorem, we have
[t/(kn âˆ†n )]
n
E|R6,h
|

â‰¤Kkn âˆ†n

X

E b
cikn âˆ†n âˆ’ b
c0ikn âˆ†n = Op (an âˆ†n(2âˆ’r)$ ) = op (kn

i=0

30

p
âˆ†n ).

1âˆ’Ï‚
1
Hence, $ â‰¥ 2âˆ’Î³
> 4âˆ’2Î³
is sufficient to warrant the desired rate.
As a result, except for the term that is related to R4n , all the remainder terms on the right-hand side of
(A.17) vanish. We write R4n as
[t/(kn âˆ†n )]kn

R4n = kn

X

d
X

i=1

l,m=1

Ï‰ n,lm
Î±n,lm
,
i
i

where

Ï‰ n,lm
= âˆ‚lm F (c[(iâˆ’1)/kn ]kn âˆ†n ).
i

where Ï‰ n,lm
is a vector measurable with respect to F(iâˆ’1)âˆ†n , and kÏ‰ ni k â‰¤ K. To prove the stable convergence
i
result, we start with
1
âˆš E
âˆ†n

[t/(kn âˆ†n )]kn

X



Ï‰ n,lm
E
i

Î±n,lm
|Fiâˆ†n
i

i=0



1
â‰¤âˆš
âˆ†n

[t/(kn âˆ†n )]kn

X

p
n
Kâˆ†3/2
n ( âˆ†n + E (Î· i )) â†’ 0,

i=0

where we use (A.6) and (A.10). Moreover, by (A.5), we have
ï£«
ï£¶
[t/(kn âˆ†n )]kn


X
1 ï£­
4
4
kÏ‰ ni k E kÎ±ni k |Fiâˆ†n ï£¸ â‰¤ Kâˆ†n â†’ 0.
E
âˆ†2n
i=0


n
Also, similar to (4.18) in Jacod and Rosenbaum (2013), we have E Î±n,lm
âˆ†
N
|F
= 0, for any N that is
iâˆ†
n
i
i
an arbitrary bounded martingale orthogonal to W , which readily implies
âˆš

1
âˆ†n

[t/(kn âˆ†n )]kn

X


 p
Ï‰ n,lm
E Î±n,lm
âˆ†ni N |Fiâˆ†n âˆ’â†’ 0.
i
i

i=0

Finally, note that for any 1 â‰¤ p, q â‰¤ r, by (A.7),
1
âˆ†n

[t/(kn âˆ†n )]kn

X



n,lm
n,jk n,lm
|Ï‰ n,jk
Î±i |Fiâˆ†n âˆ’ (ciâˆ†n ,jl ciâˆ†n ,km + ciâˆ†n ,jm ciâˆ†n ,kl ) âˆ†2n â‰¤ Kâˆ†1/2
n ,
i,p Ï‰ i,q | E Î±i

i=0

which implies
1
âˆ†n
=
p

1
âˆ†n
Z t

âˆ’â†’

[t/(kn âˆ†n )]kn

X



n,lm
n,jk n,lm
Ï‰ n,jk
Ï‰
E
Î±
Î±
|F
iâˆ†
n
i,p
i,q
i
i

i=0
[t/(kn âˆ†n )]kn

X

n,lm
2
Ï‰ n,jk
i,p Ï‰ i,q (ciâˆ†n ,jl ciâˆ†n ,km + ciâˆ†n ,jm ciâˆ†n ,kl ) âˆ†n

i=0

âˆ‚jk Fp (cs )âˆ‚lm Fq (cs ) (cs,jl cs,km + cs,jm cs,kl ) .
0

Finally, by Theorem IX.7.28 of Jacod and Shiryaev (2003), we establish
1
Lâˆ’s
âˆš R4 âˆ’â†’ Wt ,
k n âˆ†n n
where Wt is conditional Gaussian on an extension of the probably space, with a covariance matrix
E (Wp,t Wq,t |F) =

d
X
j,k,l,m=1

Z

t

âˆ‚jk Fp (cs )âˆ‚lm Fq (cs ) (cs,jl cs,km + cs,jm cs,kl ) .
0

31

Appendix A.9

Proof of Proposition 2

Proof. As we have seen from the above proof, we have for any c âˆˆ C,
2

kâˆ‚jk Fp (c)âˆ‚lm Fq (c) (cjl ckm + cjm ckl )k â‰¤ K(1 + kck ),
which, combined with the same argument in the proof of Theorem 1, establishes the desired result.

Appendix A.10

Proof of Corollary 1

Proof. The first statement on consistency follows immediately from Theorem 1, as Assumption 2 holds with
Î¶ = 1. Next, we prove the central limit result. For any 1 â‰¤ p â‰¤ d, we define fpÎ» as,
fpÎ» (xÌ„)

1
=
gp âˆ’ gpâˆ’1

hence we have
âˆ‚fpÎ» (xÌ„)
Î»

and f is C

âˆ

âˆ‚jk FpÎ» (cs )

1
=
gp âˆ’ gpâˆ’1

gp
X

xÌ„j ,

j=gpâˆ’1 +1

gp
X

ev ,

âˆ‚ 2 fpÎ» (xÌ„) = 0,

v=gpâˆ’1 +1

and Lipchitz. By Lemma 4 we can derive

=

d
X

Ouj âˆ‚u fpÎ» (Î»(cs ))Ouk

u=1

gp
d
X
X
1
1
=
Ouj evu Ouk =
gp âˆ’ gpâˆ’1 u=1 v=g +1
gp âˆ’ gpâˆ’1
pâˆ’1

gp
X

Ovj Ovk .

v=gpâˆ’1 +1

Therefore, the asymptotic covariance matrix is given by
Z t X
d
âˆ‚jk FpÎ» (cs )âˆ‚lm FqÎ» (cs ) (cjl,s ckm,s + cjm,s ckl,s ) ds
0 j,k,l,m=1

1
1
=
gp âˆ’ gpâˆ’1 gq âˆ’ gqâˆ’1

Z

gq
X

gp
X

d
X

t

Ovj Ovk Oul Oum (cjl,s ckm,s + cjm,s ckl,s ) ds

0 j,k,l,m=1 v=g
pâˆ’1 +1 u=gqâˆ’1 +1

2
=
(gp âˆ’ gpâˆ’1 )(gq âˆ’ gqâˆ’1 )

Z

2
=
(gp âˆ’ gpâˆ’1 )(gq âˆ’ gqâˆ’1 )
Z t
2Î´ p,q
=
Î»2 ds.
(gp âˆ’ gpâˆ’1 ) 0 gp ,s

Z

t

d
X

gp
X

gq
X

Ovl Ovm Oul Oum Î»2v,s ds

0 l,m=1 v=g
pâˆ’1 +1 u=gqâˆ’1 +1
t

gp
X

gq
X

Î´ u,v Î»2v,s ds

0 v=g
pâˆ’1 +1 u=gqâˆ’1 +1

(A.18)

Next, we calculate the bias-correction term. Recall that the estimator is given by
FpÎ» (b
cikn âˆ†n ) =

1
gp âˆ’ gpâˆ’1

gp
X

bv,ik âˆ† ,
Î»
n n

v=gpâˆ’1 +1

bv,ik âˆ† is the corresponding eigenvalue of the sample covariance matrix b
where Î»
cikn âˆ†n . Although b
cikn âˆ†n and
n n
cikn âˆ†n may have different eigenstructure, it is easy to verify that the functional forms of the second order
derivative of FpÎ» evaluated at both points turn out to be the same, so here we only provide the calculations
based on b
cikn âˆ†n . Since almost surely, sample eigenvalues are simple, it implies from Lemma 4 that
2
âˆ‚jk,lm
FpÎ» (b
cikn âˆ†n ) =

d
X

fÎ»

bul O
buj O
bvk O
bvm
Auvp (Î»(b
cikn âˆ†n ))O

u,v=1

32

=

=

1
gp âˆ’ gpâˆ’1
1
gp âˆ’ gpâˆ’1

gp
X

d
X

h=gpâˆ’1 +1 u,v=1,u6=v
gp

ehu âˆ’ ehv
b O
b O
b O
b
O
bu,iâˆ† âˆ’ Î»
bv,iâˆ† ul uj vk vm
Î»
n

d
X

X

n

1

h=gpâˆ’1 +1 u=1,u6=h



bh,ik âˆ† âˆ’ Î»
bu,ik âˆ†
Î»
n n
n n


bul O
buj O
bhk O
bhm + O
bhl O
bhj O
buk O
bum ,
O

b is the orthogonal matrix such that Ob
b cik âˆ† O
b | = Diag(Î»(b
b on ikn âˆ†n
where O
cikn âˆ†n )). The dependence of O
n n
is omitted for brevity.
bh,ik âˆ† I âˆ’ b
To facilitate the implementation, we consider the matrix Î»
cikn âˆ†n . Note that
n n
bh,ik âˆ† âˆ’ Î»
b1,ik âˆ† , Î»
bh,ik âˆ† âˆ’ Î»
b2,ik âˆ† , . . . , Î»
bh,ik âˆ† âˆ’ Î»
bd,ik âˆ† ) = O(
bh,ik âˆ† I âˆ’ b
b Î»
b| ,
Diag(Î»
cikn âˆ†n )O
n n
n n
n n
n n
n n
n n
n n
hence we have


bh,ik âˆ† I âˆ’ b
Î»
cikn âˆ†n
n n

+

b | Diag
=O

1

1

!

1

,
, . . . , 0, . . .
bh,ik âˆ† âˆ’ Î»
bh,ik âˆ† âˆ’ Î»
bh,ik âˆ† âˆ’ Î»
b1,ik âˆ† Î»
b2,ik âˆ†
bd,ik âˆ†
Î»
Î»
n n
n n
n n
n n
n n
n n

As a result, we obtain


bh,ik âˆ† I âˆ’ b
Î»
cikn âˆ†n
n n

+

=

km

d
X

1

b
b
u=1,u6=p Î»h,ikn âˆ†n âˆ’ Î»u,ikn âˆ†n

buk O
bum ,
O

Therefore, we have
2
âˆ‚jk,lm
FpÎ» (b
cikn âˆ†n )

1
=
gp âˆ’ gpâˆ’1

gp
X


+

+
bh,ik âˆ† I âˆ’ b
bh,ik âˆ† I âˆ’ b
bhk Î»
bhm + O
bhj Î»
bhl .
O
cikn âˆ†n
O
cikn âˆ†n
O
n n
n n
jl

h=gpâˆ’1 +1

km

Now we can calculate the following term, which is used for bias-correction:
d
X

2
âˆ‚jk,lm
FpÎ» (b
ciâˆ†n ) (b
cjl,iâˆ†n b
ckm,iâˆ†n + b
cjm,iâˆ†n b
ckl,iâˆ†n )

j,k,l,m=1

1
=
gp âˆ’ gpâˆ’1

gp
X

d
X




+

+
b
b
b
b
b
b
Ohk Î»h,ikn âˆ†n I âˆ’ b
cikn âˆ†n
Ohm + Ohj Î»h,ikn âˆ†n I âˆ’ b
cikn âˆ†n
Ohl
jl

h=gpâˆ’1 +1 j,k,l,m=1

Â· (b
cjl,ikn âˆ†n b
ckm,ikn âˆ†n + b
cjm,ikn âˆ†n b
ckl,ikn âˆ†n )


gp

+
X
2
b
b
Î»h,ikn âˆ†n Tr Î»h,ikn âˆ†n I âˆ’ b
cikn âˆ†n
b
cikn âˆ†n .
=
gp âˆ’ gpâˆ’1

km

(A.19)

h=gpâˆ’1 +1

The last equality uses the following observation:6
bh,ik âˆ† I âˆ’ b
b | = 0,
(Î»
cikn âˆ†n )+ O
n n
h,Â·
which concludes the proof of (ii). The proof of (iii) uses the same calculations as above. Note that we can
apply Theorem 2 with only Assumption 4, because the spectral function here only depends on Î»g .
6 See

page 160 of Magnus and Neudecker (1999).

33

b
O.

Appendix A.11

Proof of Proposition 3

Proof. By Lemma 5 and the uniform convergence of b
cikn âˆ†n âˆ’ cÌ„ikn âˆ†n to 0 established above, we can restrict the
âˆ
domain of Î³ g (Â·) to the set C, in which it is C with bounded derivatives. By Theorem 21 of Protter (2004),
we have
[t/(kn âˆ†n )]âˆ’1

X

Î³ |g,ikn âˆ†n (X(i+1)kn âˆ†n

u.c.p

t

Z

âˆ’ Xikn âˆ†n ) =â‡’

Î³ g,sâˆ’ dXs .
0

i=1

Therefore, it remains to show that
[t/(kn âˆ†n )]âˆ’1 

X


u.c.p
Î³
b|g,(iâˆ’1)kn âˆ†n âˆ’ Î³ |g,ikn âˆ†n (X(i+1)kn âˆ†n âˆ’ Xikn âˆ†n ) =â‡’ 0.

i=1

Define a F(i+1)kn âˆ†n -measurable function:


b|g,(iâˆ’1)kn âˆ†n âˆ’ Î³ |g,ikn âˆ†n (X(i+1)kn âˆ†n âˆ’ Xikn âˆ†n ).
Î¾ ikn = Î³
By standard estimates in (A.1) with c replaced by X, (A.3), and (A.9),

E|E Î¾ ikn Fikn âˆ†n )| =E|b
Î³ g,(iâˆ’1)kn âˆ†n âˆ’ Î³ g,ikn âˆ†n ||E (X(i+1)kn âˆ†n âˆ’ Xikn âˆ†n )|Fikn âˆ†n |
â‰¤KE|b
c(iâˆ’1)kn âˆ†n âˆ’ cikn âˆ†n |(kn âˆ†n )


q
â‰¤K (kn âˆ†n )1/2 + an âˆ†n(2âˆ’Î³)$ + knâˆ’1 + kn âˆ†n (kn âˆ†n )
Moreover, we have by the same estimates above,

E |Î¾ ikn |2 |Fikn âˆ†n â‰¤ (kn âˆ†n + an âˆ†n(4âˆ’Î³)$âˆ’1 + knâˆ’1 + kn âˆ†n )kn âˆ†n .
Finally, using Doobâ€™s inequality, and measurability of Î¾ ikn , we obtain
ï£«
ï£¶
[s/(kn âˆ†n )]âˆ’1
X
E ï£­ sup |
Î¾ ikn |ï£¸
0â‰¤sâ‰¤t

i=1

ï£«

[t/(kn âˆ†n )]âˆ’1

â‰¤

X

E|E Î¾ ikn Fikn âˆ†n )| + ï£­

i=1

ï£¶1/2

[t/(kn âˆ†n )]âˆ’1

X

E |Î¾ ikn |2 |Fikn âˆ†n ï£¸


i=1



q
âˆ’1
(4âˆ’Î³)$âˆ’1
â‰¤K (kn âˆ†n )1/2 + an âˆ†(2âˆ’Î³)$
+
k
+
k
âˆ†
+ knâˆ’1 + kn âˆ†n )1/2
n
n n + K(kn âˆ†n + an âˆ†n
n
â†’0,
because (4 âˆ’ Î³)$ â‰¥ 1 under our assumptions on $ and Ï‚, which establishes the proof.

Appendix A.12

Proof of Corollary 2

Proof. The (p, q) entry of the asymptotic covariance matrix is given by
Z

t

d
X

âˆ‚jk Î³ gp,s âˆ‚lm Î³ gq,s (cjl,s ckm,s + cjm,s ckl,s ) ds

0 j,k,l,m=1

Z
=

t

d
X

0 j,k,l,m=1

+
(Î»g,s I âˆ’ cs )+
pj (Î»g,s I âˆ’ cs )ql Î³ gk,s Î³ gm,s (cjl,s ckm,s + cjm,s ckl,s ) ds

34

Z

t

=

d
X


2
+
(Î»g,s I âˆ’ cs )+
pj (Î»g,s I âˆ’ cs )ql Î»g,s cjl + Î»g,s Î³ gl,s Î³ gj,s ds

0 j,l=1

Z
=

t

Î»g,s (Î»g,s I âˆ’ cs )+ cs (Î»g,s I âˆ’ cs )+



0

p,q

ds,

Pd
where we use (Î»g,s I âˆ’ cs )+ Î³ g,s = 0, and k=1 Î³ gk,s ckm,s = Î»g,s Î³ gm,s . To calculate the asymptotic bias, we
bh and Î³
note that the b
cs has only simple eigenvalues almost surely. Denote Î»
bh as the corresponding eigenvalue
and eigenvector. We omit the dependence on time s to simplify the notations. By Lemma 2, we obtain
d
X

2
âˆ‚jk,lm
Î³
bgh (b
cjl b
ckm + b
cjm b
ckl )

j,k,l,m=1

ï£«
ï£¶
d
d
d
X
X
X
X
bp Î»
bg
bp Î»
bg
Î»
Î»
2
2
2
ï£­
Î³
bgl Î³
Î³
bpl Î³
=âˆ’2
bpm Î³
bph Î³
bgm +
bgm Î³
bph Î³
bpm +
Î³
bpm Î³
bgl Î³
bpl Î³
bph ï£¸
bg âˆ’ Î»
b p )2
bg âˆ’ Î»
b p )2
(
Î»
(
Î»
l,m=1
l,m=1
p6=g
l,m=1
p6=g
ï£¶
ï£«
d
d
X
X 2
XX
bp Î»
bg
Î»
ï£­
bgl Î³
bql Î³
bqh ï£¸
Î³
bql Î³
bpl Î³
bpm Î³
bgm Î³
bqh +
Î³
bpm Î³
+
bg âˆ’ Î»
bp )(Î»
bp âˆ’ Î»
bq )
(
Î»
l,m=1
l,m=1
p6=g q6=p
ï£¶
ï£«
d
d
X
XX
X 2
bq Î»
bg
Î»
ï£­
bgm Î³
bph Î³
bpm +
Î³
bqm Î³
bgl Î³
bql Î³
bpm Î³
bph ï£¸
+
Î³
bql Î³
bg âˆ’ Î»
bp )(Î»
bp âˆ’ Î»
bq )
(
Î»
l,m=1
p6=g q6=p
l,m=1
ï£¶
ï£«
d
d
XX
X
X
bp Î»
bq
Î»
2
ï£­
+
bgm Î³
bph +
Î³
bqm Î³
bpl Î³
bql Î³
bgm Î³
bph ï£¸
Î³
bpm Î³
bql Î³
bg âˆ’ Î»
bp )(Î»
bg âˆ’ Î»
bq )
(Î»
X

p6=g q6=g

=âˆ’

X
p6=g

l,m=1

l,m=1

bp Î»
bg
Î»
Î³
b .
bg âˆ’ Î»
bp )2 gh
(Î»

Since Î³ g (Â·) is a C âˆ function, it is straightforward using the proof of Theorem 2 that the desired CLT holds,
even though Î³ g (Â·) is not a spectral function.

Appendix A.13

Proof of Propositions 4 and 5

Proof. This follows by applying the â€œdeltaâ€ method, using Lemma 2 and Theorem 13.2.4 in Jacod and Protter
(2011).

35

Appendix B

j=1
j=2
j=3

Îºj
3
4
5
Î»F
1/t

Figures and Tables
Î¸j
0.05
0.04
0.03
ÂµF
+/âˆ’
âˆš
4 âˆ†

Î·j
0.3
0.4
0.3
Î»Z
2/t

Ïj
-0.6
-0.4
-0.25
ÂµZ
+/âˆ’
âˆš
6 âˆ†

Âµj
0.05
0.03
0.02
2
ÂµÏƒ
âˆš
âˆ†

ÎºÌƒj
1
2
3
ÏF
12
0.05
Îº
4

Î¸Ìƒi,j
U[0.25, 1.75]
N (0, 0.52 )
N (0, 0.52 )
ÏF
13
0.1
Î¸
0.3

Î¾Ìƒ j
0.5
0.6
0.7
ÏF
23
0.15
Î·
0.06

Table 1: Parameters in Monte Carlo Simulations
Note: In this table, we report the parameter values used in the simulations. The constant matrix Î¸Ìƒi,j is generated
randomly from the described distribution, and is fixed throughout all replications. The dimension of Xt is 100, whereas
the dimension of Ft is 3. âˆ† is the sampling frequency, and t is the length of the time window. The number of Monte
Carlo replications is 1,000.

36

# Stocks
5
10
15
20
30
50
100

True
0.4686
0.6489
0.8927
1.3044
2.1003
2.9863
6.6270

5
10
15
20
30
50
100

0.4879
0.6839
0.9397
1.3765
2.2157
3.1554
7.0000

1 Week, 5 Seconds
Bias
-0.0001
0.0001
-0.0004
0.0003
-0.0002
0.0002
-0.0004
1 Week, 5 Minutes
0.0107
0.0084
0.0128
0.0130
0.0210
0.0267
0.0552

Stdev
0.0075
0.0110
0.0149
0.0225
0.0356
0.0514
0.1141

True
0.4703
0.6552
0.8975
1.3148
2.1134
3.0104
6.6732

0.0452
0.0574
0.0724
0.1076
0.1670
0.2410
0.5314

0.5642
0.7143
1.0167
1.3882
2.2383
3.1518
6.9632

1 Week, 1 Minute
Bias
0.0007
-0.0014
-0.0011
-0.0018
-0.0033
-0.0054
-0.0127
1 Month, 5 Minutes
0.0006
-0.0024
-0.0024
-0.0041
-0.0073
-0.0125
-0.0270

Stdev
0.0152
0.0223
0.0296
0.0424
0.0688
0.1002
0.2179
0.0247
0.0258
0.0382
0.0503
0.0806
0.1155
0.2451

Table 2: Simulation Results: First Eigenvalue Estimation
Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the first integrated
eigenvalue. Column â€œTrueâ€ corresponds to the average of the true integrated eigenvalue; Column â€œBiasâ€ corresponds
to the mean of the estimation error; Column â€œStdevâ€ is the standard deviation of the estimation error.

# Stocks
5
10
15
20
30
50
100

True
0.3145
0.4268
0.5531
0.6517
0.9186
1.2993
2.3273

# Stocks
5
10
15
20
30
50
100

True
0.3255
0.4384
0.5751
0.6758
0.9586
1.3508
2.4312

1 Week, 5 Seconds
Bias
0.00004
-0.0001
-0.0003
-0.0008
-0.0015
-0.0017
-0.0041
1 Week, 5 Minutes
Bias
0.0020
0.0028
0.0014
0.0046
0.0017
-0.0022
-0.0084

Stdev
0.0051
0.0071
0.0086
0.0103
0.0144
0.0205
0.0361

True
0.3173
0.4289
0.5572
0.6556
0.9251
1.3080
2.3441

Stdev
0.0269
0.0379
0.0427
0.0535
0.0725
0.1046
0.1787

True
0.3639
0.4469
0.6524
0.7779
1.1365
1.5630
2.9148

1 Week, 1 Minute
Bias
0.0003
0.0006
0.0004
-0.0006
-0.0017
-0.0026
-0.0065
1 Month, 5 Minutes
Bias
-0.0003
-0.0003
-0.0017
-0.0021
-0.0036
-0.0064
-0.0136

Stdev
0.0110
0.0146
0.0188
0.0215
0.0305
0.0458
0.0766
Stdev
0.0189
0.0186
0.0257
0.0297
0.0438
0.0683
0.1113

Table 3: Simulation Results: Second Eigenvalue Estimation
Note: In this table, we report the summary statistics of 1000 Monte Carlo simulations for estimating the second
integrated eigenvalue. Column â€œTrueâ€ corresponds to the average of the true integrated eigenvalue; Column â€œBiasâ€
corresponds to the mean of the estimation error; Column â€œStdevâ€ is the standard deviation of the estimation error.

37

# Stocks
5
10
15
20
30
50
100

True
0.1338
0.2132
0.2941
0.3212
0.4825
0.6943
1.3808

# Stocks
5
10
15
20
30
50
100

True
0.1364
0.2199
0.3018
0.3324
0.4971
0.7216
1.4302

1 Week, 5 Seconds
Bias
0.0001
0.0001
0.0002
0.0001
0.0005
0.0001
0.0004
1 Week, 5 Minutes
Bias
0.0041
0.0033
0.0056
0.0037
0.0055
0.0028
-0.0016

Stdev
0.0022
0.0035
0.0046
0.0052
0.0078
0.0113
0.0221

True
0.1345
0.2149
0.2954
0.3242
0.4853
0.7016
1.3935

Stdev
0.0124
0.0183
0.0264
0.0294
0.0409
0.0577
0.1119

True
0.1404
0.2367
0.3396
0.3546
0.5686
0.8051
1.6278

1 Week, 1 Minute
Bias
0.0003
0.0002
0.0002
0.0000
0.0001
-0.0005
-0.0013
1 Month, 5 Minutes
Bias
0.0006
-0.0001
0.0009
0.0003
0.0006
-0.0010
-0.0028

Stdev
0.0044
0.0070
0.0093
0.0105
0.0153
0.0226
0.0438
Stdev
0.0054
0.0091
0.0131
0.0132
0.0211
0.0306
0.0612

Table 4: Simulation Results: Third Eigenvalue Estimation
Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the third
integrated eigenvalue. Column â€œTrueâ€ corresponds to the average of the true integrated eigenvalue; Column â€œBiasâ€
corresponds to the mean of the estimation error; Column â€œStdevâ€ is the standard deviation of the estimation error.

# Stocks
5
10
15
20
30
50
100

True
0.0596
0.0596
0.0596
0.0596
0.0596
0.0596
0.0596

# Stocks
5
10
15
20
30
50
100

True
0.0595
0.0595
0.0595
0.0595
0.0595
0.0595
0.0595

1 Week, 5 Seconds
Bias
0.0002
0.0001
0.0001
0.0001
0.0001
0.0001
0.0001
1 Week, 5 Minutes
Bias
0.0026
0.0029
0.0027
0.0028
0.0030
0.0029
0.0031

Stdev
0.0007
0.0004
0.0003
0.0002
0.0002
0.0001
0.0001

True
0.0597
0.0597
0.0597
0.0597
0.0597
0.0597
0.0597

Stdev
0.0041
0.0029
0.0024
0.0021
0.0020
0.0018
0.0016

True
0.0595
0.0595
0.0595
0.0595
0.0595
0.0595
0.0595

1 Week, 1 Minute
Bias
0.0005
0.0003
0.0004
0.0004
0.0004
0.0004
0.0004
1 Month, 5 Minutes
Bias
0.0006
0.0006
0.0006
0.0006
0.0007
0.0006
0.0006

Stdev
0.0015
0.0008
0.0006
0.0005
0.0004
0.0003
0.0002
Stdev
0.0017
0.0009
0.0007
0.0006
0.0005
0.0004
0.0003

Table 5: Simulation Results: Repeated Eigenvalue Estimation, Fourth and Beyond
Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the repeated
integrated eigenvalues. Column â€œTrueâ€ corresponds to the average of the true integrated eigenvalue; Column â€œBiasâ€
corresponds to the mean of the estimation error; Column â€œStdevâ€ is the standard deviation of the estimation error.

38

# Stocks

5

10

True
0.2457
0.8987
0.0566
0.0633
0.3110
0.0506
0.3444
0.4632
0.1422
0.4166
0.0864
0.3460
0.1268
0.3409
0.4262

1 Week, 5 Seconds
Bias
-0.0002
0.0028
0.0013
0.0001
0.0017
0.0004
0.0006
0.0011
0.0008
0.0004
0.0008
0.0008
0.0005
0.0005
0.0007

Stdev
0.0173
0.0261
0.0235
0.0119
0.0202
0.0049
0.0087
0.0129
0.0137
0.0220
0.0158
0.0088
0.0076
0.0174
0.0112

True
0.2475
0.8963
0.0536
0.0639
0.3086
0.0495
0.3452
0.4619
0.1422
0.4164
0.0863
0.3440
0.1259
0.3415
0.4271

1 Week, 1 Minute
Bias
0.0074
0.0174
-0.0018
0.0037
0.0042
0.0029
0.0080
0.0121
0.0094
0.0045
0.0089
0.0101
0.0066
0.0046
0.0099

Stdev
0.1559
0.2135
0.1281
0.0786
0.0645
0.0266
0.0691
0.0966
0.0990
0.1562
0.1117
0.0600
0.0451
0.1268
0.0942

Table 6: Simulation Results: Eigenvector Estimation
Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the integrated
eigenvector associated with the first eigenvalue. Column â€œTrueâ€ corresponds to the average of the true integrated
vector; Column â€œBiasâ€ corresponds to the mean of the estimation error; Column â€œStdevâ€ is the standard deviation of
the estimation error. The setup for these simulations is identical to that of Tables 2-5.

39

# Stocks

30

True
0.2587
0.1557
0.2364
0.0608
0.1398
0.2471
0.2250
0.2642
0.2004
0.0446
0.2311
0.2560
0.1979
0.2255
0.2185
0.1262
0.1916
0.0661
0.2118
0.0414
0.1007
0.0518
0.0550
0.2345
0.2003
0.1276
0.2813
0.0319
0.1417
0.1233

1 Week, 5 Seconds
Bias
âˆ’4 Ã— 10âˆ’6
3 Ã— 10âˆ’7
-0.00004
1 Ã— 10âˆ’6
-0.00001
âˆ’3 Ã— 10âˆ’6
0.00001
0.00002
0.00005
-0.00004
0.00005
0.00002
-0.00005
-0.00004
6 Ã— 10âˆ’7
-0.00006
0.00002
-0.00004
-0.00004
0.00006
-0.00004
-0.00006
-0.00001
-0.00005
0.00003
-0.00003
0.00002
-0.00005
0.00006
-0.00001

Stdev
0.0026
0.0015
0.0015
0.0028
0.0019
0.0017
0.0014
0.0027
0.0015
0.0022
0.0023
0.0021
0.0013
0.0019
0.0014
0.0015
0.0013
0.0025
0.0020
0.0013
0.0025
0.0017
0.0022
0.0013
0.0014
0.0025
0.0033
0.0030
0.0022
0.0024

True
0.2581
0.1558
0.2359
0.0608
0.1394
0.2476
0.2244
0.2647
0.2006
0.0447
0.2312
0.2565
0.1979
0.2247
0.2186
0.1263
0.1917
0.0661
0.2117
0.0414
0.1008
0.0517
0.0548
0.2349
0.2006
0.1275
0.2813
0.0319
0.1414
0.1235

1 Week, 1 Minute
Bias
0.00004
-0.0001
-0.00008
0.00009
0.00002
-0.00001
0.0001
0.00008
0.0002
0.0002
0.00001
-0.00002
0.00008
-0.0001
-0.00007
0.0002
-0.0001
0.0002
-0.00003
0.00008
-0.00006
-0.00003
0.0002
-0.0002
-0.0001
0.00008
0.0003
-0.00001
0.0002
0.0002

Stdev
0.0090
0.0054
0.0053
0.0096
0.0069
0.0059
0.0043
0.0094
0.0047
0.0075
0.0080
0.0075
0.0043
0.0066
0.0042
0.0051
0.0045
0.0088
0.0070
0.0045
0.0086
0.0058
0.0077
0.0043
0.0047
0.0085
0.0116
0.0106
0.0075
0.0085

Table 7: Simulation Results: Eigenvector Estimation
Note: In this table, we report the summary statistics of 1,000 Monte Carlo simulations for estimating the integrated
eigenvectors associated with the first eigenvalues. The column â€œTrueâ€ corresponds to the average of the true integrated
vector; â€œBiasâ€ corresponds to the mean of the estimation error; â€œStdevâ€ is the standard deviation of the estimation
error. The setup for these simulations is identical to that of Tables 2-5. To save space, we do not report the eigenvectors
in dimensions larger than 30.

40

0.4

0.4

0.2

0.2

0

-4

-2

0

2

0

4

-4

0.4

0.4

0.2

0.2

0

-4

-2

0

2

0

4

-4

-2

0

2

4

-2

0

2

4

Figure 1: Finite Sample Distribution of the Standardized Statistics.

Note: In this figure, we report the histograms of the 1,000 simulation results for estimating the first four integrated
eigenvalues using 5-second returns for 30 stocks over one week. The purpose of this figure is to validate the asymptotic
theory, hence the use of a short 5-second sampling interval. The smallest 27 eigenvalues are identical so that the fourth
eigenvalue is repeated. The solid lines plot the standard normal density; the dashed histograms report the distribution
of the estimates before bias correction; the solid histograms report the distribution of the estimates after bias correction
is applied and is to be compared to the asymptotic standard normal. Because the fourth eigenvalue is small, the dashed
histogram on the fourth subplot is out of the x-axis range, to the right.

41

0.4
0.2
0

0.4
0.2
0

-4

-4

-2

-2

0

0

2

2

4

4

0.4
0.2
0

0.4
0.2
0

0.4
0.2
0

-4

-4

-4

-2

-2

-2

0

0

0

2

0.4
0.2
0
4
-4

-2

0

2

4

2

0.4
0.2
0
4
-4

-2

0

2

4

2

0.4
0.2
0
4
-4

-2

0

2

4

0.4
0.2
0
-4

-2

0

2

4

Figure 2: Finite Sample Distribution of the Standardized Statistics.

Note: In this figure, we report the histograms of the integrated simple eigenvalues using weekly one-minute returns of
100 stocks, a setting that matches that of our empirical analysis below. Columns 1, 2, and 3 report the results with
r = 2, 3, and 4 common factors in the data generating process, respectively. The number of Monte Carlo replications
is 1,000.

42

Figure 3: The Liquidity of S&P 100 Index Components

Note: In this figure, we provide quartiles of the average time between successive transactions, or sampling frequencies,
for the 100 stocks S&P 100 Index constituents between 2003 and 2012 . We exclude the first and the last 5 minutes
from the 6.5 regular trading hours in the calculation, during which trading intensities are unusually high. The y-axis
reports the corresponding average sampling frequencies in seconds, computed from the high frequency transactions in
the sample. From these transactions, we construct a synchronous one-minute sample using the latest tick recorded
within that minute. The 10 least liquid stocks in the index are included in this figure but excluded from the empirical
analysis that follows.

43

Figure 4: Time Series of the Cumulative Returns of S&P 100 Index Components

Note: In this figure, we plot the time series of cumulative daily open-to-close returns of 158 S&P 100 Index Components
from 2003 to 2012. The thick black solid line plots the cumulative daily open-to-close S&P 100 Index returns. All
overnight returns are excluded. Companies that exited the index during the sample period, including bankruptcies,
are represented by a time series truncated at the time of delisting.

44

2.5

2

1.5

1

0.5

0

0

10

20

30

40

50

60

70

80

90

Figure 5: The Scree Graph

Note: In this figure, we report the time series average over the entire sample of the estimated eigenvalues against their
order, or â€œscree graphâ€. These integrated eigenvalues are estimated assuming that all of them are simple.

45

0.7
0.6

1st
2nd
3rd
4 Average

0.5
0.4
0.3
0.2
0.1
0
2003

2005

2007

2009

2011

2013

Figure 6: Time Series of Realized Eigenvalues

Note: In this figure, we plot the time series of the three largest realized eigenvalues, representing the percentage
variations explained by the corresponding principal components, using weekly 1-min returns of 90 most liquid S&P 100
Index constituents from 2003 to 2012. For comparison, we also plot the average of the remaining 87 eigenvalues.

46

3
2

PC1
PC2
PC3

1
0
-1
-2
-3
-4
-5
2003

2005

2007

2009

2011

2013

Figure 7: Time Series of Cumulative Realized Principal Components

Note: In this figure, we compare the cumulative returns of the first three principal components, using weekly 1-minute
returns of the S&P 100 Index constituents from 2003 to 2012.

47

Week: 03/07/2005 -- 03/11/2005

0.06

Non-Financial
Financial

0.04
0.02

Component 2

0
-0.02
-0.04
-0.06
-0.08
-0.1
-0.12

0

0.05

0.1

0.15

0.2

0.25

Component 1
Figure 8: Biplot of the Integrated Eigenvector: Pre-crisis Week

Note: This biplot shows the integrated eigenvector estimated during the week March 7-11, 2005. Each dot and the
vector joining it and the origin represents a firm. The x and y coordinates of a firm denote its average loading on the
first and second principal components, respectively. The dots and lines associated with firms in the financial sector are
colored in red and dashed, while non-financial firms are colored in blue and drawn as solid lines. The figures shows
little difference between the two groups during that week.

48

Week: 03/10/2008 -- 03/14/2008

0.15

Non-Financial
Financial

0.1

Component 2

0.05

0
AIG
-0.05
GS C
BAC
-0.1
JPM
-0.15

0

0.05

0.1

0.15

0.2

0.25

LEH
0.3

Component 1
Figure 9: Biplot of the Integrated Eigenvector: Crisis Week

Note: This biplot shows the integrated eigenvector estimated during the week March 10-14, 2008. Each dot and the
vector joining it and the origin represents a firm. The x and y coordinates of a firm denote its average loading on the
first and second principal components, respectively. The dots and lines associated with firms in the financial sector
are colored in red and dashed, with the following singled out: American International Group (AIG), Bank of America
(BOA), Citigroup (C), Goldman Sachs (GS), J.P. Morgan Chase (JPM), and Lehman Brothers (LEH). Non-financial
firms are colored in blue and drawn as solid lines. The figure shows a sharp distinction between the two groupsâ€™
dependence on the second principal component during that week.

49

Week: 03/07/2005 -- 03/11/2005

0.6

0.5

0.4

0.3

0.2

0.1

0

0

10

20

30

40

50

60

70

80

90

Figure 10: Scree Plot of the Integrated Eigenvalues: Pre-crisis Week

Note: This figure contains a scree plot of the integrated eigenvalues estimated during the week March 7-11, 2005.
The blue solid lines provide 95% confidence interval for the first three eigenvalues computed based on the results of
Corollary 1.

50

Week: 03/10/2008 -- 03/14/2008

8
7
6
5
4
3
2
1
0

0

10

20

30

40

50

60

70

80

90

Figure 11: Scree Plot of the Integrated Eigenvalues: Crisis Week

Note: This figure contains a scree plot of the integrated eigenvalues estimated during the week March 10-14, 2008.
The blue solid lines provide 95% confidence interval for the first three eigenvalues computed based on the results of
Corollary 1.

51

