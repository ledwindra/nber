{
    "id": 22180,
    "citation_title": "Bias against Novelty in Science: A Cautionary Tale for Users of Bibliometric Indicators",
    "citation_author": [
        "Jian Wang",
        "Reinhilde Veugelers",
        "Paula Stephan"
    ],
    "citation_publication_date": "2016-04-25",
    "issue_date": "2016-04-22",
    "revision_date": "2022-08-23",
    "topics": [
        "\n",
        "Health, Education, and Welfare",
        "\n",
        "Education",
        "\n",
        "Development and Growth",
        "\n",
        "Innovation and R&D",
        "\n"
    ],
    "program": [
        "\n",
        "Labor Studies",
        "\n",
        "Productivity, Innovation, and Entrepreneurship",
        "\n"
    ],
    "projects": null,
    "working_groups": [
        "\n",
        "Innovation Policy",
        "\n"
    ],
    "abstract": "\n\nResearch which explores unchartered waters has a high potential for major impact but also carries a higher uncertainty of having impact.  Such explorative research is often described as taking a novel approach.  This study examines the complex relationship between pursuing a novel approach and impact.  Viewing scientific research as a combinatorial process, we measure novelty in science by examining whether a published paper makes first time ever combinations of referenced journals, taking into account the difficulty of making such combinations.  We apply this newly developed measure of novelty to all Web of Science research articles published in 2001 across all scientific disciplines.  We find that highly novel papers, defined to be those that make more (distant) new combinations, deliver high gains to science:  they are more likely to be a top 1% highly cited paper in the long run, to inspire follow on highly cited research, and to be cited in a broader set of disciplines.  At the same time, novel research is also more risky, reflected by a higher variance in its citation performance.  In addition, we find that novel research is significantly more highly cited in \u201cforeign\u201d fields but not in its \u201chome\u201d field.  We also find strong evidence of delayed recognition of novel papers and that novel papers are less likely to be top cited when using a short time window.  Finally, novel papers typically are published in journals with a lower than expected Impact Factor.  These findings suggest that science policy, in particular funding decisions which rely on traditional bibliometric indicators based on short-term direct citation counts and Journal Impact Factors, may be biased against \u201chigh risk/high gain\u201d novel research.  The findings also caution against a mono-disciplinary approach in peer review to assess the true value of novel research.\n\n",
    "acknowledgement": "\nEarlier versions of this paper were presented at the Workshop on the Organization, Economics and Policy of Scientific Research, Turin; Institute for Research Information and Quality Assurance, Berlin; Max Planck Institute for Innovation and Competition, Munich; and the TIES seminar at the MIT Sloan School, Cambridge.  The authors thank seminar participants and in particular Pierre Azoulay, Christian Catalini, Paul David, Lee Fleming, Alfonso Gambardella, Dietmar Harhoff, Sybille Hinze, Stefan Hornbostel, Jacques Mairesse, Fabio Montobbio, Henry Sauermann, Daniel Sirtes, Scott Stern, Mark Veugelers, and John Walsh for helpful comments.  Financial support from KU Leuven (GOA/12/003) and the Research Foundation - Flanders (FWO, G.0825.12) is gratefully acknowledged.  J. Wang also gratefully acknowledges a postdoctoral fellowship from FWO.  Publication data are sourced from Thomson Reuters Web of Science Core Collection. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}