NBER WORKING PAPER SERIES

STICKY INFORMATION IN GENERAL EQUILIBRIUM
N. Gregory Mankiw
Ricardo Reis
Working Paper 12605
http://www.nber.org/papers/w12605

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
October 2006

This is an extended version of our paper with the same title published in the Journal of the European
Economic Association, April-May 2007. It includes a lengthy appendix laying out the model, solving
it, proving the propositions, and explaining the algorithms. All of the programs used are available
at our websites. We are grateful to Tiago Berriel for excellent research assistance, and to Ruchir Agarwal
and Mark Watson for useful comments. The views expressed herein are those of the author(s) and
do not necessarily reflect the views of the National Bureau of Economic Research.
© 2006 by N. Gregory Mankiw and Ricardo Reis. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.

Sticky Information in General Equilibrium
N. Gregory Mankiw and Ricardo Reis
NBER Working Paper No. 12605
October 2006
JEL No. E10,E30
ABSTRACT
This paper develops and analyzes a general-equilibrium model with sticky information. The only rigidity
in goods, labor, and financial markets is that agents are inattentive, sporadically updating their information
sets, when setting prices, wages, and consumption. After presenting the ingredients of such a model,
the paper develops an algorithm to solve this class of models and uses it to study the models dynamic
properties. It then estimates the parameters of the model using U.S. data on five key macroeconomic
time series. It finds that information stickiness is present in all markets, and is especially pronounced
for consumers and workers. Variance decompositions show that monetary policy and aggregate demand
shocks account for most of the variance of inflation, output, and hours.
N. Gregory Mankiw
Department of Economics
Littauer 223
Harvard University
Cambridge, MA 02138
and NBER
ngmankiw@fas.harvard.edu
Ricardo Reis
Princeton University
Department of Economics
324 Bendheim Hall
Princeton, NJ 08544-1021
and NBER
rreis@princeton.edu

1.

Introduction
Estimation and simulation of medium-sized macroeconometric models has increasingly

attracted the attention of economists who study monetary policy and the business cycle.1
This paper contributes to that eﬀort by focusing on a model in which sticky information is
the key imperfection that causes output to deviate from its long-run classical benchmark.
In this otherwise standard dynamic stochastic general equilibrium model, information is
updated sporadically by firms setting prices, workers setting wages, and consumers setting
the level of spending.
Solution and estimation of a general equilibrium model with sticky information raises
several thorny technical issues. We begin this paper outlining those issues and proposing
solutions. Our first contribution is methodological. It consists of two propositions that
provide an algorithm that eﬃciently solves medium-sized sticky-information models, derives
their impulse response functions, and calculates their likelihood in a few seconds.
We then proceed to estimate the model using five key time-series: inflation, output, hours
worked, wages, and an interest rate. The second contribution of this paper is to propose,
implement, and compare two estimation strategies for the model: maximum likelihood and
a Bayesian approach. The two strategies yield similar results.
We thus obtain estimates of how much information stickiness is needed to explain business cycle dynamics. We find that about a fifth of workers and consumers update their
information sets every quarter, so the mean information lag for both household members is
approximately five quarters. By contrast, firms are estimated to be much better informed
when setting prices: about two-thirds update their information set every quarter.
The model also produces an estimated variance decomposition, which shows how much
of the variation in each variable is attributable to each of the five shocks in the model. For
inflation, over 80 percent of the variance is attributable to the monetary policy shock. For
output growth and hours worked, the monetary policy shock is important, but so is the
shock to aggregate demand. The other three shocks–to productivity, the goods markup,
and the labor markup–are estimated to explain only a small fraction of the variance of
inflation, output growth, and hours worked.
2.

The model of the economy
1

See, for instance, Smets and Wouters (2003) and Levin, Onatksi, Williams and Williams (2006)

2

We study a general-equilibrium model with monopolistic competition and no capital
accumulation, familiar in the literature on monetary policy. We assume a continuum of
households with preferences that are additively separable and iso-elastic in consumption
and leisure. Households live forever and wish to maximize expected discounted utility while
being able to save and borrow by trading bonds between themselves. We think of households
as having two members: a worker and a consumer. The workers sell labor to firms in a set
of segmented markets for diﬀerent labor varieties, where each worker is the sole provider of
each variety. The consumers buy a continuum of varieties of goods from firms, which they
value according to a Dixit-Stiglitz aggregator. There is a continuum of firms, each selling one
variety of goods under monopolistic competition. Each firm operates a decreasing returns
to scale technology in total labor input, which is a Dixit-Stiglitz aggregate of the diﬀerent
varieties of labor. Finally, monetary policy follows a Taylor rule.
Less common is our assumption on information. There are three agents making decisions
in this economy: consumers, workers, and firms. We assume that each period, a fraction
δ of consumers, a fraction ω or workers, and a fraction λ of firms, randomly drawn from
their respective populations, obtain new information and calculate their optimal actions.
This assumption of sticky information can be justified by costs of acquiring, absorbing and
processing information (Reis, 2004, 2006) or by appealing to epidemiology (Carroll, 2002).2
We leave the detailed presentation of the model, the definition of an equilibrium and its
log-linearization to the appendix. Here, we discuss the 5 key reduced-form relations. The
first relation is the Phillips curve or aggregate supply curve:
¸
∙
∞
X
βν t
β(wt − pt ) + (1 − β)yt − at
j
−
.
(1 − λ) Et−j pt +
pt = λ
β + ν(1 − β)
(ν − 1)[β + ν(1 − β)]

(1)

j=0

The price level (pt ) depends on past expectations of: its current value, real marginal costs,
and desired markups.3 Marginal costs are higher: the higher are the real wages paid to
workers (wt − pt ), the more is produced (yt ) because of decreasing returns to scale (β < 1),
and the lower is aggregate productivity (at ). The desired markup falls with the elasticity
of substitution across goods varieties (ν t ), which we allow to vary randomly over time.
2

The optimal behavior of these inattentive agents and their interaction in markets raise some interesting
challenges. We discuss these in Mankiw and Reis (2006).
3
All variables with a t subscript refer to log-linearized values around their non-stochastic steady state.
Without any subscript are fixed parameters and steady state values.

3

Unexpected shocks to any of these three variables only raise prices by λ since only this
share of price-setters is aware of the news.
The second relation is the IS curve:
∞
X
n
(1 − δ)j Et−j (y∞
− θRt ) + gt ,
yt = δ

(2)

j=0

n = lim
where the long-run equilibrium output is y∞
i→∞ Et (yt+i ), and the long real interest
hP
i
∞
rate is Rt = Et
j=0 (it+j − ∆pt+1+j ) . Higher expected future output raises wealth

and increases spending, while higher expected interest rates encourage savings and lower
spending. The impact of interest rates on spending depends on the intertemporal elasticity

of substitution θ. We denote by gt aggregate demand shocks, which in the model correspond
to changes in government spending, but could also be modelled as changes in the desire for
leisure. The higher is δ, the larger the share of informed consumers that respond to shocks
immediately.
Next comes the wage curve:
∙
¸
∞
n − θR )
X
lt
ψ (y∞
ψγ t
γ(wt − pt )
t
j
+
+
−
(1 − ω) Et−j pt +
. (3)
wt = ω
γ+ψ
γ+ψ
θ(γ + ψ)
(γ + ψ)(γ − 1)
j=0

The five determinants of nominal wages are split into the five terms on the right-hand
side. First, nominal wages rise one-to-one with prices since workers care about real wages.
Second, the higher are real wages elsewhere in the economy the higher is demand for a
worker’s variety of labor so the higher the wage she will demand. Third, the more labor
is hired (lt ) the better it must be compensated since the marginal disutility of working
rises. Fourth, higher wealth discourages work through an income eﬀect, and higher interest
rates promote it by giving a larger return on saved earnings today. The product of ψ,
the Frisch elasticity of labor supply, and θ, the intertemporal elasticity of substitution,
determine the strength of this intertemporal labor supply eﬀect. Fifth and finally, if the
elasticity of substitution across labor varieties (γ t ) rises, workers’ desired markup falls so
they lower their wage demands. If many workers are informed (ω is high), wages are
instantly very responsive to changes in these determinants, whereas otherwise wages only
respond gradually over time.

4

The fourth relation is a standard production function:
yt = at + βlt ,

(4)

where β measures the extent of decreasing returns to scale from using more labor. The fifth
and final relation is the Taylor rule:
it = φy (yt − ytn ) + φp ∆pt − εt ,

(5)

where yt − ytn is the output gap, or the diﬀerence between actual output and its level if all
agents were attentive, and εt are policy disturbances.
These 5 equations give the equilibrium values for output, wages, prices, labor, and
nominal interest rates as a function of shocks to aggregate productivity growth, aggregate
demand, goods markups, labor markups, and monetary policy. We assume that each of
these shocks follows an autoregressive process of order 1 with coeﬃcients ρ∆a , ρg , ρν , ργ ,
g
γ
ν
ε
and ρε , and is subject to innovations e∆a
t , et , et , et , and et , that are independent and

normally distributed with standard deviations σ ∆a , σ g , σ ν , σ γ , and σ ε .
3.

Solving for the economy’s dynamics
Our model fits into the general class of linear rational expectations models for which

there are several ready-to-use solution algorithms. However, none of them is particularly
useful to solve the sticky-information model. The model involves both an infinite number of
past expectations of the present through sticky information, as well as present expectations
of variables at an infinite number of future dates through intertemporal smoothing. This
double infinity implies that the state-space of the model has an infinite dimension, which
current algorithms cannot handle.4
We have developed a general algorithm that can solve this and much larger generalequilibrium models with sticky information in a few seconds. It is based on the following
result, which comes from using a method of undetermined coeﬃcients and exploiting the
recursiveness of the model’s dynamics:
Proposition 1. Letting s ∈ S = {∆a, g, ν, γ, ε} denote the diﬀerent shocks, then pt =
4
Recently, Wang and Wei (2006) proposed an ingenious method to adapt existing algorithms to solve
sticky-information models. We leave a systematic comparison of their method with the one in this paper for
future research.

5

P

s∈S

P∞

s
n=0 p̂n (s)et−n

where p̂n (s) is a scalar measuring the impact of shock s at lag n on

the price level. The undetermined coeﬃcients solve the second-order diﬀerence equation:
An+1 p̂n+1 (s) − Bn p̂n (s) + φp p̂n−1 (s) = Cn (s) for n = 0, 1, 2, ...
with boundary conditions

:

(6)

p̂−1 = 0 and lim (p̂n − p̂n−1 ) = 0.
n→∞

The coeﬃcients An and Bn do not depend on the shock, while Cn (s) does; all depend on
the parameters and are given in the appendix.
The appendix describes our algorithm to solve this diﬀerence equation and finds, in corollary
1, the solution for the other variables in the model as a function of the price dynamics.
Figure 1 shows the responses of inflation, the output gap, and labor to one-standarddeviation shocks to monetary policy, aggregate productivity growth, and aggregate demand.5 In response to a monetary expansion, output and labor increase as the economy
enters a boom. Inflation rises gradually, following the hump-shaped pattern that has been
found in empirical work. Noticeably, inflation is more persistent than output, another robust feature of the data that many monetary models have trouble reproducing. The model
fits well the facts on how the economy responds to monetary policy shocks.
In response to a positive technological shock, inflation falls but converges rapidly to its
previous level. Interestingly positive productivity shocks in this economy lead to recessions,
just as in sticky price models (Gali, 1999). However, this is not a robust feature of the
sticky-information model: for diﬀerent parameter values, we can get a boom following
a technological improvement. Finally, a positive innovation to aggregate demand raises
inflation, output, and labor.
4.

Estimating the model
We use U.S. quarterly data from 1954:3 to 2006:1 for the non-farm business sector. We

measure wages using the total compensation per hour and labor input using total hours.
We divide output and hours by the total civilian non-institutional population and deflate
nominal variables using the implicit price deflator for the nonfarm business sector. Changes
in the log of this deflator are our measure of inflation, and the eﬀective federal funds rate
measures the nominal interest rate.
5

The parameters are set at the maximum-likelihood estimates in Table 1, described in the next section.

6

Using these data, we build series for de-meaned inflation, output growth, nominal interest rates, real wage growth, and hours. These are our observables, collected in the vector
xt =(∆pt , ∆yt , lt , it , ∆(wt − pt ))0 . The sticky-information general-equilibrium model implies
P
ε ∆a g ν γ 0
that xt = ∞
i=0 Φi et−i where et is the vector of shocks (et , et , et , et , et ) and the Φi are

5x5 matrices of coeﬃcients, found in proposition 1 and corollary 1. The question we ask in
this section is how to estimate the vector of parameters of the model using these data.
We estimate our model using both maximum likelihood and Bayesian methods.6 The

key input into these methods is the likelihood function, which in standard dynamic models
with a state-space solution can be evaluated using the Kalman filter. The solution of the
model using proposition 1 does not have a convenient state-space representation, so we use
instead the following result:
Proposition 2. Given a sample of data of length T , let X be the 5T ×1 vector that vertically
stacks the xt , and let Ω be the 5T ×5N matrix that vertically stacks [Φj−1 Φj−2 ... Φ0 Φ1
... ΦN ] from j = 1 to j = T . Finally, let Σ be a diagonal matrix with (σ 2ε , σ 2∆a , σ 2g , σ 2ν , σ 2γ )
in the diagonal and IN be an identity matrix of size N . The log-likelihood function is:
¯
¯
¡
¢−1
X
L = −2.5T ln(2π) − 0.5 ln ¯Ω(IN ⊗ Σ)Ω0 ¯ − 0.5X 0 Ω (IN ⊗ Σ) Ω0

(7)

The main diﬃculty with evaluating this expression is that inverting the large 5T ×5T matrix
Ω (IN ⊗ Σ) Ω0 is both slow and subject to potentially large numerical errors. The appendix
shows how to evaluate (7) without inverting this matrix by instead solving a recursive linear
system of equations. This provides an algorithm to evaluate the log-likelihood function
quickly and reliably.
Turning to estimation, we set the value of 9 out of the 20 parameters. Namely, we
set the intertemporal elasticity of substitution to 1 (the King-Plosser-Rebelo, 1988, utility
function) to guarantee that hours are stationary, the Frisch elasticity of labor supply to
4, and the labor share to 2/3. Using the production function, we can then measure the
aggregate productivity shocks exactly, and estimate that ρ∆a = .350 and σ ∆a = .010. We
set φy = 0.33 and φp = 1.24 to match Rudebusch’s (2002) estimates of the Taylor rule, and
using these we estimate that ρε = .918 and σ ε = .012.
We start our estimation of the model by finding the set of parameters that maximize
6

See An and Schorfheide (forthcoming) and Canova (forthcoming) for recent surveys on the estimation
of dynamic stochastic general equilibrium models.

7

the likelihood function. Table 1 presents the estimates. Curiously, we estimate a value
for the elasticity of substitution between goods that is higher and a value for the elasticity
of substitution between labor that is lower than what is typically assumed. The implied
price markup is only 3% and the implied wage markup is 31%, whereas usually these are
calibrated to values between 5% and 20%. A second feature to note is that most estimates
are quite precise, with tight confidence intervals.
Our main focus of interest are the measures of information stickiness. We estimate that
firms are relatively attentive, updating their information about every 4 months, whereas
consumers and workers are quite inattentive, only updating their plans about every 16
months. We test the null hypothesis that both members of a household, the consumer and
the worker, update their information at the same time. The likelihood ratio statistic is
.089, which has a p-value of .23 in the χ21 distribution. The data do not reject this plausible
hypothesis.7
Table 2 presents the variance decompositions associated with these estimates. Noticeably, the variance of inflation, output, and hours is almost entirely accounted for solely by
monetary and aggregate demand shocks. Shocks to productivity and price markups are relatively unimportant for these three variables, but play a role on the fluctuations of interest
rates and real wages. Wage markups are on average large, but their fluctuations explain
little of the variance of any of the variables.
Next we estimate our model using Bayesian methods instead. We see the main virtue
of these methods as allowing us, through the priors, to focus on an area of the parameter
space that we are particularly interested in. In our case, this area corresponds to the typical
calibrations of these models. For instance, we pick priors for the average substitutability of
goods and labor that imply average markups that are with 95% confidence between 6% and
21%, the values commonly assumed in the literature. For the parameters of inattentiveness,
we instead opt for a flat prior in order to impose as little as possible on the data. The
priors for the correlation and the variance of shocks are similar to those on the literature,
although they are more diﬀuse than usual.
Table 1 contains the results, which turn out to be similar to the maximum-likelihood
7

With δ = ω, the wage curve can be re-written instead as:


∞
[
ψ (yt − gt )
γ(wt − pt ) + lt − ψγ t /(γ − 1)
wt = δ
+
.
(1 − δ)j Et−j pt +
γ
+
ψ
θ(γ + ψ)
j=0

8

(8)

results. As expected, the diﬀerence between the markups on goods and labor is not as
extreme as before, as our prior heavily penalizes those extreme results. Also as expected,
our diﬀuse priors lead to wider credible sets. However, the estimates of inattentiveness are
relatively similar: consumers and workers update their information every 5 to 6 quarters,
whereas firms update every 1.5 quarters. Table 2 shows the variance decompositions using
these Bayesian estimates. These are similar to the maximum-likelihood conclusions, with
the exception of shocks to goods markups, which now account for a larger share of the
variance of all variables.
5.

Conclusion
In Mankiw and Reis (2002) we proposed a new way to model sluggish macroeconomic

adjustment. In this paper we have explored how this approach can be used in an empirical
dynamic stochastic general equilibrium model.
One lesson from our estimation (and also emphasized in Mankiw and Reis, 2006) is that
information stickiness is pervasive: it applies to firms, workers, and consumers. Some recent
research has estimated empirical dynamic stochastic general equilibrium models with sticky
information on the part of firms, assuming fully informed workers and consumers.8 Our
results suggest that these models were misspecified; this misspecification can potentially
explain reported poor fits of the model. Although more work is needed before reaching a
final verdict, we believe the assumption of sticky information remains a promising tool for
applied macroeconomists.

8
See Trabandt (2003) and Keen (2003) for early attempts to build DSGE models with sticky information
on the part of firms, and Andres et al (2005), Korenok and Swanson (2005), Kiley (2005) and Laforte (2005)
for estimations. Coibion (2006) finds that sticky information on the part of consumers is important to
explains inflation dynamics.

9

Appendix
This appendix sets out the model in the paper formally, solves it, proves the propositions,
and describes the algorithms that we used.
A.1.

The economic environment

The model is similar to the one in Mankiw and Reis (2006), but allows for shocks to
the elasticities of substitution between varieties of goods and labor. The reader is referred
to that article for a more detailed exposition and a description of the intuition behind our
assumptions and optimal behavior. Here, we are brief.
There are three types of agents: consumers, workers and firms, of which there is a continuum evenly distributed in the unit interval. Consumers and workers share a household
and strive to maximize the same utility function subject to the same budget constraint.
Within consumers, there are two members: a shopper, that decides the allocation of spending across the diﬀerent varieties and is always attentive, and a planner that decides total
expenditure and is often inattentive. Firms have two departments: a purchasing department that is always attentive and chooses how much of each variety of labor to hire, and a
sales department that is only sporadically attentive and sets the price of the firm’s output.
These agents meet in three sets of markets. In the labor market, workers sell their labor
to firms; in the goods market, firms sell their goods to consumers; and in the savings market
consumers trade bonds between themselves. Monetary and aggregate demand policy follow
exogenous rules and close the model.
To lay down the model formally, we start by describing the market clearing conditions
and policy processes, then set out the attentive agents’ problem, and finally write down the
inattentive agents’ problem.
Policy and market clearing. We assume that the government consumes a common fraction of each good in the economy. This is financed by lump-sum taxes that keep the budget
balanced at all dates. Therefore, the market clearing condition in the market for goods’
variety i is:
Gt

Z

1

Ct,j (i)dj = Yt,i ,

(9)

0

where 1 − 1/Gt is the fraction of output consumed by the government, Ct,j (i) is the consumption of variety i by agent j at time t, and Yt,i is the total production of good i at time
t. The fraction Gt is stochastic and shocks to it can be interpreted broadly as aggregate
10

demand shocks.9
The market clearing condition in each labor variety i is:
Z

Lt,i =

1

Nt,j (i)dj,

(10)

0

where Lt,i is the total labor supply of variety i at time t, and Nt,j (i) is the labor demand
by firm j of variety i at time t.
Monetary policy sets interest rates according to:
it ≡ log [Et (Πt+1 Pt /Pt+1 )] = φy log

µ

Yt
Ytn

¶

+ φp log

µ

Pt
Pt−1

¶

− εt .

(11)

The definition of the nominal interest rate follows the Fisher relation, whereas policy is
set according to a Taylor rule. The new notation is Pt for the price level, Πt+1 for the
gross real interest rate between t and t + 1, Ytn for the equilibrium level of output if all
are attentive, and εt to discretionary policy shocks. Note that a positive εt corresponds to
an expansionary shock. The coeﬃcient φy is positive reflecting a desire for stabilization,
and φp > 1 to respect the Taylor principle and lead to a determinate solution for inflation.
This rule by itself leaves the price level indeterminate, but we peg the initial price level at
P−1 = 1 ensuring determinacy.
Finally, we define total output and total labor as the aggregators across all varieties:10

Yt =

µZ

1

Yt,i

0

Lt =

µZ

0

ν̂ t −1
ν̂ t

1

γ̂ t −1
γ̂ t

Lt,i

t
¶ ν̂ ν̂−1
t
di
,

(12)

t
¶ γ̂ γ̂−1
t
di
.

(13)

Attentive agents. Consumer’s shopper j at date t solves:

min

{Ct,j (i)}i∈[0,1]

Z

1

Pt,i Ct,j (i)di s.t. Ct,j =

0

µZ

0

1

Ct,j (i)

ν̂ t −1
ν̂ t

t
¶ ν̂ ν̂−1
t
di
.

(14)

The price of each variety of goods is Pt,i , and the consumer values them according to a
Dixit-Stilitz utility function, with a stochastic elasticity of substitution ν̂ t . The standard
9

Shocks to the utility of leisure relative to consumption enter the model in a similar way to Gt .
U1
U1
Note that using instead the definitions Yt = 0 Yt,i di and Lt = 0 Lt,i di leads to the same results up to
a first-order approximation.
10

11

solution to this problem is:
−ν̂ t

Ct,j (i) = Ct,j (Pt (i)/Pt )

with Pt =

µZ

1

1−ν̂ t

Pt (i)

0

1
¶ 1−ν̂
t
di
,

(15)

so that Pt is the static price index. Summing over all consumers and using the market
clearing condition gives the total demand for variety i:
−ν̂ t

Yt,i = (Pt (i)/Pt )

C̄t Gt , where C̄t ≡

Z

1

Ct,j dj.

(16)

0

The purchasing department of firm j at date t minimizes expenditures given a DixitStiglitz production function that aggregates labor of diﬀerent varieties into a labor aggregate:
min

{Nt,j (i)}i∈[0,1]

Z

1

Wt,i Nt,j (i)di s.t. Nt,j =

0

µZ

1

Nt,j (i)

γ̂ t −1
γ̂ t

0

t
¶ γ̂ γ̂−1
t
di
.

(17)

Wt,i is the wage paid to labor variety i and γ̂ t is the stochastic elasticity of substitution
across labor varieties. The solution is:
−γ̂ t

Nt,j (i) = Nt,j (Wt (i)/Wt )

with Wt =

µZ

1

1−γ̂ t

Wt (i)

0

1
¶ 1−γ̂
t
di
,

(18)

where Wt is the static wage index. Summing over all firms and using the market clearing
condition, we obtain the demand for labor variety i:
−γ̂ t

Lt,i = (Wt (i)/Wt )

N̄t , where N̄t ≡

Z

1

Nt,j dj.

(19)

0

Inattentive agents. We start by considering the problem facing the pricing department
of a firm that last updated its information j periods ago. We assume that each period, a
randomly drawn fraction of firms λ updates their information, so there are λ(1 − λ)j firms
in this situation. They choose a nominal price to maximize expected real profits:
max Et−j
Pt,j

s.t.:

∙

Wt Nt,j
Pt,j Yt,j
−
Pt
Pt

β
and (16)
Yt,j = At Nt,j

¸

(20)
(21)

The first constraint is the production function, where β measures the degree of returns to

12

scale. We interpret this model with no capital accumulation as one in which there is a
fixed stock of capital, so β corresponds to the labor share. Aggregate productivity At is
stochastic. The second constraint is the demand for the firm’s product in (16). After some
rearranging, the first-order condition of this optimization problem is:
Pt,j =

Et−j [ν̂ t Wt Nt,j /Pt ]
.
Et−j [β(ν̂ t − 1)Yt,j /Pt ]

(22)

Next, consider the problem of an inattentive consumer’s planner. If she updates her
plan at date t, she chooses a plan for current and future consumption to solve:

V (At ) =
s.t.

:

max

{Ct+i,i }

At+1+i

(∞
X
i=0

1−1/θ

i

i

ξ (1 − δ)

Ct+i,i − 1
1 − 1/θ

+ ξδ

∞
X
i=0

i

i

)

ξ (1 − δ) Et [V (At+1+i )] ,

(23)

µ
¶
Wt+1+i,. Lt+1+i,. + Tt+1+i,.
for i=0,1,...
(24)
= Πt+1+i At+i − Ct+i,i +
Pt+1

and a no-Ponzi scheme condition. V (.) is the value function of the agent that depends on
her wealth At . The parameter ξ is the discount factor, while δ is the probability at each
date that the consumer updates her plan. The coeﬃcient θ is the intertemporal elasticity
of substitution so preferences are iso-elastic. Preferences are also additively separable in
consumption and leisure, but since the consumer does not control labor supply, the term in
leisure drops out of her problem. The budget constraint assumes that wages are received at
the beginning of the period so they earn interest before the next period. Finally Tt,. denote
both lump-sum taxes as well as the payments from an insurance contract that all agents
sign at the beginning of time that ensures that they all have the same wealth at the start
of each period. This is a standard assumption in these models to avoid tracking the wealth
distribution over time. The optimality conditions are:
−1/θ

ξ i (1 − δ)i Ct+i,i

= ξδ

∞
X
k=i

V 0 (At ) = ξδ

∞
X
k=0

We denote by Π̄t+i ,t+1+k =

t+k
Q

£
¤
ξ k (1 − δ)k Et V 0 (At+1+k ) Π̄t+i ,t+1+k for i=0,1,2,... (25)
£
¤
ξ k (1 − δ)k Et V 0 (At+1+k ) Π̄t ,t+1+k .

(26)

Πz+1 the compound return between t + i and t + 1 + k.

z=t+i
−1/θ

Combining (25) for i = 0 with (26) one learns that Ct,0

= V 0 (At ). Writing (26) recursively

and using these results one gets the first condition below. Condition (25) for i = j and (26)
13

for date t + j imply the second result:
−1/θ

Ct,0

−1/θ

Ct+j,j

h
i
−1/θ
= ξEt Rt+1 Ct+1,0 ,
h
i
−1/θ
= Et−j Ct+j,0 ,

(27)
(28)

Finally, we turn to workers. They solve a similar problem to consumers:

V̂ (At ) =

max

{Wt+i,i }

(

−

∞
X
i=0

ξ i (1 − ω)i Et

Ã

1+1/ψ

Lt+i,i + 1
1 + 1/ψ

!

+ ξω

s.t. (24) and (19)

∞
X
i=0

)
i
h
ξ i (1 − ω)i Et V̂ (At+1+i ) (29),
(30)

where V̂ (.) is the value function perceived by the worker, ω is the probability each period
that she will update her information, and ψ is the Frisch elasticity of labor supply in the isoelastic utility function. The worker faces as constraints the same budget as the consumer,
as well as the total demand for her services. The optimality conditions are:

ξω

∞
X
k=i

³
´
1+1/ψ
ξ i (1 − ω)i Et γ̂ t+i Lt+i,i /Wt+i,i =

¢
£
¡
¤
ξ k (1 − ω)k Et V 0 (At+1+k ) Π̄t+i ,t+1+k γ̂ t+i − 1 Lt+i,i /Pt+i for i=0,1,2,...
V̂ 0 (At ) = ξω

∞
X
k=0

h
i
ξ k (1 − ω)k Et V̂ 0 (At+1+k ) Π̄t ,t+1+k .

(31)
(32)

Combining (31) for i = 0 with (32) one learns that
1/ψ

Wt,0

Pt Lt,0
γ̂ t
× 0
=
.
γ̂ t − 1 V̂t (At )

(33)

Writing (32) recursively and using these results one gets the first condition below. Condition
(31) for i = j and (32) for date t + j imply the second result:
1/ψ

Lt,0 Pt
γ̂ t
×
γ̂ t − 1
Wt,0
Wt+i,i

Ã

1/ψ

Lt+1,0 Pt+1
γ̂ t+1
×
= ξEt Rt+1 ×
γ̂ t+1 − 1
Wt+1,0
³
´
1/ψ
Et γ̂ t+i Lt+i,i
³
´.
=
1/ψ−1
Et γ̂ t+i Lt+i,i Lt+i,0 /Wt+i,0

!

,

(34)

(35)

Monopolistically competitive equilibrium. A competitive equilibrium of this economy is

14

an allocation of total expenditures, consumption of varieties, labor supplied of the diﬀerent
varieties, and output produced of each variety such that consumers, workers and firms all
behave optimally, monetary policy follows the Taylor rule, and all markets clear.
A.2.

The log-linearized economy and shocks

We log-linearize the equilibrium conditions around the non-stochastic steady state.
Small caps denote the log-deviations of the respective large-cap variable from this steady
state, with the exceptions of: ν t and γ t which are the log-deviations of ν̂ t and γ̂ t , rt which
is the log-deviation of the short rate Et [Πt+1 ], and Rt which is the log-deviation of the long
rate limk→∞ Et [Π̄t ,t+1+k ].
Log-linearizing the market clearing conditions and policy rules, we get:
yt = gt + ct ,

(36)

it = φy (yt − ytn ) + φp ∆pt − εt ,

(37)

it = rt + Et (∆pt+1 ) ,
∞
X
(1 − δ)j ct,j ,
ct = δ

(38)
(39)

j=0

From the attentive agents’ section:
yt,j = yt − ν (pt,j − pt ) ,
∞
X
pt = λ
(1 − λ)j pt,j ,

(40)
(41)

j=0

lt,j = lt − γ(wt,j − wt ),
∞
X
wt = ω
(1 − ω)j wt,j

(42)
(43)

j=0

From the inattentive firm’s problem:
yt,j = at + βlt,j ,

(44)

pt,j = Et−j [wt − (yt,j − nt,j ) − ν t /(ν̄ − 1)]
¸
∙
β(wt − pt ) + (1 − β)yt − at − ν t β/(ν̄ − 1)
.
= Et−j pt +
β + ν̄(1 − β)

(45)

The second expression uses the two constraints in (21) to eliminate firm-specific variables.

15

From the inattentive consumer’s problem:
ct,0 = Et (ct+1,0 − θrt ) ,

(46)

ct,j = Et−j (ct,0 ) ,

(47)

and from the inattentive worker’s problem:
wt,0 − pt − lt,0 /ψ + γ t /(γ̄ − 1) = Et [−rt + wt+1,0 − pt+1 − lt+1,0 /ψ + γ t+1 /(γ̄ − 1)],
(48)
wt,j = Et−j (wt,0 ) .

(49)

There are five source of shocks in the model: monetary policy, aggregate productivity
growth, aggregate demand, goods markups, and labor markups. We assume that each
follows an independent AR(1):
εt = ρε εt−1 + eεt ,
gt = ρg gt−1 + egt ,

∆at = ρ∆a ∆at−1 + e∆a
t ,

ν t = ρν ν t−1 + eνt ,

γ t = ργ γ t−1 + eγt ,

(50)
(51)

where the shocks est ∼ N (0, σ 2s ) are i.i.d. over time, E[est est+k ] = 0 for k 6= 0, and independent
0

of each other, E[est est ] = 0 for s 6= s0 .
A.3.

The attentive equilibrium

The attentive equilibrium is the one that obtains when δ = ω = λ = 1, so all are
attentive. Following convention, we refer to variables in this equilibrium as being at their
“natural” levels and superscript them with n. Note however that this is not a Pareto optimal
equilibrium since there is monopoly power. Moreover, note that because the elasticities of
substitution change, markups change as well, so the “natural” levels or output or labor are
not a constant fraction of their Pareto optimal levels.
If all are attentive, all are identical, so: yt,j = yt , lt,j = lt , pt,j = pt , wt,j = wt , and

16

ct,j = ct . The model then collapses into the system of 6 equations:
ytn = gt + cnt ,
¡
¢
Et ∆pnt+1 = φp ∆pnt − rtn − εt ,
ytn = at + βltn ,

0 = β(wtn − pnt ) + (1 − β)ytn − at − ν t β/(ν̄ − 1),
¡
¢
cnt = −θrt + Et cnt+1 ,

zt = −rtn + Et (zt+1 ) , with zt ≡ wtn − pnt − ltn /ψ + γ t /(γ̄ − 1),

in 6 variables (ytn , cnt , ltn , pnt , rtn , wtn ). The solution for output is:
ytn = Ξa at + Ξg gt + Ξγ γ t + Ξν ν t , where:
Ξa ≡ (1 + 1/ψ)/ (1 + 1/ψ + β/θ − β) , Ξg ≡ (β/θ)/ (1 + 1/ψ + β/θ − β) ,
Ξγ ≡ (β/(γ − 1))/ (1 + 1/ψ + β/θ − β) , Ξν ≡ (β/(ν − 1))/ (1 + 1/ψ + β/θ − β) .
Using the solution for output, the solution for the other real variables follows: ltn = (ytn −
¡ n
¢
− ytn − gt+1 + gt .
at )/β, wtn − pnt = ytn − ltn − ν t β/(ν̄ − 1), cnt = ytn − gt , and θrtn = Et yt+1
Note that for hours to remain bounded, E(ltn ) = 0, requires the King-Plosser-Rebelo (1988)

restriction, θ = 1. In this case, output and real wages increase proportionally with aggregate
productivity and labor is independent of productivity shocks. This economy respects the
classical dichotomy as real variables are determined independently of monetary shocks.
Finally, the solution for inflation in the θ = 1 case is:
∆pnt =

(1 − β/(1 + 1/ψ))(1 − ρg )gt
(1 − ργ )γ t
ρa ∆at
+
−
φp − ρa
φp − ρg
(γ̄ − 1)(1 + 1/ψ)(φp − ργ )
(1 − ρν )ν t
εt
−
.
+
(ν̄ − 1)(1 + 1/ψ)(φp − ρν ) φp − ρε

(52)

Expansionary monetary policy, higher productivity growth, higher government spending,
¡
¢
and higher markups all raise inflation. Nominal interest rates are int = rtn + Et ∆pnt+1 ,
and the equilibrium is fully characterized.
A.4.

The sticky-information equilibrium

Starting with (41), replace yt,j using (40) and pt,j using (45) and rearrange to get the
17

AS curve in (1). Denoting by mct (real marginal costs) the fraction on the right-hand side,
it can be re-arranged to obtain a sticky-information Phillips curve:
∞

X
λmct
+λ
(1 − λ)j Et−1−j (∆pt + ∆mct ) .
∆pt =
1−λ

(53)

j=0

Next, starting with (46), iterate forward and take the limit as time goes to infinity. Then,
use the definition of the long rate Rt and the fact that complete insurance plus the fact that
£ n ¤
≡
eventually all become aware of shocks implies that limi→∞ Et (ct+i,0 ) = limi→∞ Et yt+i

yt∞ . Then, using this solution to replace for ct,0 in (47) and (39) gives an expression for
aggregate consumption. Replacing it in (36) and using the fact that gt is stationary gives
the IS curve in (2).

Very similar steps, using the expressions for wt,0 in (48), for wt,j in (49), the aggregator
for wt in (43) and replacing out lt,j using (42), gives the wage curve in (3). Aggregating
(44) over j gives the aggregate production function in (4). Finally, the expressions for the
nominal interest rate in (37) and (38) give the Taylor rule in (5).
These 5 equations together with the initial condition p−1 = 0 define an equilibrium in
the 5 variables (yt , pt , wt , lt , it ) as function of the five stochastic variables (εt , ∆at , gt , ν t , γ t )
A.5.

Proof of Proposition 1

Using a method of undetermined coeﬃcients, the solution for the generic variable zt ∈
Zt = {yt , pt , wt , lt , it } as a function of the innovations est−n for all non-negative n and
s ∈ S = {∆a, g, ν, γ, ε} is:
zt =

∞
XX

ẑn (s)est−n ,

(54)

s∈S n=0

where ẑn (s) is the undetermined coeﬃcient measuring the impact of shock s at lag n on
variable z. Define the following auxiliary parameters that will be useful:
Λn = λ

n
n
n
X
X
X
(1 − λ)i , ∆n = δ
(1 − δ)i , and Ωn = ω
(1 − ω)i ;
i=0

Ψn =

i=0

(55)

i=0

n
o
h
i
θ∆n [ψ + γ(1 − Ωn )] β+ν(1−β)
−
ν(1
−
β)
−
βψΩ
n
Λn
(1 − β)(γ + ψ)θ∆n + Ωn {θ∆n [1 − γ(1 − β)] + ψβ}

18

≡

Ψnum
n
.
Ψden
n

(56)

Focus first on the impact of monetary policy shocks. The AS in (1) implies that
∙

¸
β + ν(1 − β)
− ν(1 − β) p̂n (ε) = (1 − β)ŷn + β ŵn
Λn

(57)

for all n. The IS in equation (2) implies that:

ŷn = −∆n θ

∞
X

r̂n+i ,

(58)

i=0

for all n. The wage curve in (3) in turn implies that:
Ã

(γ + ψ − Ωn γ) ŵn = Ωn ψ p̂n + Ωn ŷn /β − ψ
for all n.

Using these three equations to substitute out

diﬀerencing (58) gives the two equations:

∞
X

!

r̂n+i .

i=0

P∞

i=0 r̂n+i

(59)

and ŵn and first-

ŷn = Ψn p̂n ,
ŷn+1
ŷn
θr̂n =
−
.
∆n+1 ∆n

(60)
(61)

where we used the definition of the parameter Ψn . Finally, using the Taylor rule: (5):
φp p̂n = φy ŷn+1 + (1 + φp )p̂n+1 − p̂n+2 − r̂n+1 − ρn+1
,
e

(62)

and replacing for ŷn and r̂n gives the solution in the proposition with:
An = 1 +

Ψn
, Bn = An + φy Ψn + φp , Cn (ε) = ρe
θ∆n

(63)

You can go through the exact same steps for the other four shocks. It should be evident
though that the only change is that there is a new term on the right-hand side of (60) which
we denote by Υn (s), and that the term in ρe no longer appears on the right-hand side of

19

(62). The solution will therefore have the same form, but now:

Cn (s) =

and

⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨

1
1−ρa

⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩

h
´
i
³
(1−ρn+2
)Υn+1 (a)
Υn (a)
a
(1 − ρn+1
−
)
φ
Υ
(a)
−
φ
Ξ
+
n
a
y
y
a
θ∆n
h
iθ∆n+1
(1−Υn+1 (g))ρg
1−Υn (g)
− θ∆n + φy (Υn (g) − Ξg ) ρng
θ∆n+1
h³
´
i
Υn+1 (γ)ρ
φy + θ∆1 n Υn (γ) − φy Ξγ − θ∆n+1 γ ρnγ
´
i
h³
(ν)ρν
ρnν
φy + θ∆1 n Υn (ν) − φy Ξν − Υn+1
θ∆n+1

Υn (s) =

⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩

θ∆n [γ + ψ + Ωn (1 − γ)] /Ψden
n

for s = a

βψΩn /Ψden
n

for s = g

βθψΩn ∆n /Ψden
n (γ − 1)

for s = γ

for s = a
for s = g

,

for s = γ
for s = ν
(64)

.

(65)

βθ∆n [ψ + γ(1 − Ωn )] /Ψden
n (ν − 1) for s = ν

Finally, turning to the boundary conditions, the first follows from the initial condition
that ensures the determinacy of the price level. The second condition follows from the fact
that as the time after a shocks goes to infinity, all become aware of the shock, inflation
approaches its natural level, and this in turn tends to zero since ∆pnt is stationary. This
concludes the proof.
A.6.

Algorithm to solve for the sticky-information equilibrium

In principle, solving the second-order diﬀerence equation should be easy. In practice, we
found that shooting algorithms (including multiple shooting alternatives) or the extended
path method were often unreliable. Small numerical imprecisions are compounded by both
of these algorithms leading them to quickly diverge away from the solution. As an alternative
we found that solving the system of linear equations implied by the diﬀerence equation and
the boundary conditions:
⎛

−B0

⎜
⎜
⎜ φp
⎜
⎜
⎜ ...
⎜
⎜
⎜ 0
⎜
⎜
⎜ 0
⎝
0

A1

...

0

0

−B1 ...

0

0

...

...

...

...

0

... −BN −2

0

...

φp

−BN−1

0

...

0

1

AN−1

0

⎞⎛

p̂0 (s)

⎟⎜
⎟⎜
0 ⎟ ⎜ p̂1 (s)
⎟⎜
⎟⎜
... ⎟ ⎜
...
⎟⎜
⎟⎜
⎜
0 ⎟
⎟ ⎜ p̂N−2 (s)
⎟⎜
⎜
AN ⎟
⎠ ⎝ p̂N−1 (s)
−1
p̂N (s)

⎞

⎛

C0 (s)

⎟ ⎜
⎟ ⎜
⎟ ⎜ C1 (s)
⎟ ⎜
⎟ ⎜
⎟ ⎜
...
⎟=⎜
⎟ ⎜
⎟ ⎜ CN −2 (s)
⎟ ⎜
⎟ ⎜
⎟ ⎜ CN −1 (s)
⎠ ⎝
0

⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠

(66)

was fast and reliable even for a very large N. Because the matrix of coeﬃcients is sparse
20

and recursive, this system poses no diﬃculties to most equation-solving programs. Note
that An and Bn are bounded and non-zero: tedious algebra shows that Ψn > 0 for all finite
n and limn→∞ Ψn = 0 and that as long as all parameters are finite, so is Ψn . Therefore,
An 6= 0, Bn 6= 0, limn→∞ An = 1, and limn→∞ Bn = 1 + φp . The system of equations is
therefore well-behaved. We have found that setting N = 1000, Matlab can find the solution
in less than 5 seconds, and that the ignored terms ẑn (s), for n > 1000, are typically lower
than 10−15 .
With a solution for prices, after tedious algebra, we can find:
Corollary 1. The coeﬃcients for output, real interest rates, nominal interest rates, real
wages and labor are:

ŷn (s) = Ψn p̂n (s) +

r̂n (s) =

⎧
⎪
⎪
⎪
⎨

n+1
Υa
)
n (1−ρa
1−ρa

for s = a

Υsn ρnj

for s = g, γ, ν

0
⎧
⎨

for s = ε

⎪
⎪
⎪
⎩

ŷn+1 (s)
ŷn
−
+
θ∆n+1
θ∆n ⎩

gn
θ∆n

−

gn+1
θ∆n+1

0

for s = g

(ŵn − p̂n )(s) = [1 + ν(1/β − 1)] (1/Λn − 1) p̂n (s) + (1 − 1/β)ŷn (s) +

ˆln (s) =

A.7.

ŷn (s)
−
⎩
β

(68)

for s = a, γ, ν, ε

ı̂n (s) = r̂n (s) + p̂n+1 (s) − p̂n (s)

⎧
⎨

(67)

1−ρn+1
a
β(1−ρa )

for s = a

0

for s = g, γ, ν, ε

⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩

(69)
1−ρn+1
a
β(1−ρa )
ρn
ν
ν−1

0

for s = a
for s = ν(70)
for s = g, γ, ε

Proof of Proposition 2

From the properties of the exogenous shocks: et v N (05 , Σ) and E(et e0t−j ) = 0 for any
j 6= 0. The properties of the normal distribution then imply that X v N (05T , Ω(IN ⊗Σ)Ω0 ).
The likelihood function follows from the density of the multivariate normal.
A.8.

Two algorithms to evaluate the log-likelihood function.

The formula for the log-likelihood function in proposition 2 involves inverting a 5T ×5T
matrix V = Ω (I5 ⊗ Σ) Ω0 , which is both slow and subject to potentially large numerical
errors. There are two approaches to getting around this problem.
21

(71)

The first approach is common in the estimation of ARMA models. A Choleski decomposition gives V = LL0 , where L is a lower triangular matrix. Defining LX̃ = X, we
can construct the X̃ vector easily since this is a recursive system of equations. Moreover,
P
th element in the diagonal of L. The
ln |V | = |LL0 | = |L|2 = 2 5T
j=1 ln(lj ), where lj is the j
first equality uses the Choleski decomposition, the second the properties that the determi-

nant of the product of two square matrices is equal to the product of the determinants and
that the determinant of a matrix is equal to the determinant of its transpose, and the third
equality uses the fact that for triangular matrices the determinant equals the product of
the elements in the diagonal and the properties of logarithms. The log-likelihood function
then becomes:
L = −2.5T ln(2π) −

5T
X
j=1

ln(lj ) − 0.5X̃ 0 X̃.

(72)

We have found that this algorithm takes less than 5 seconds to execute.
There is an alternative algorithm that is sometimes faster but only applicable if we have
a long data series. If T is large, then setting N = T leads to a negligible error in the solution
¡
¢
of the model. In this case, defining ΩX̂ = X, then X 0 (Ω (I5 ⊗ Σ) Ω0 )−1 X=X̂ 0 I5 ⊗ Σ−1 X̂.
P
P
Because Σ is diagonal with σ 2k as its kth element, this expression equals 5k=1 Tj=1 x̂25(j−1)+k /σ 2k .
P
Moreover, ln |V | = 2 ln |Ω| + ln (I5 ⊗ Σ) = 2 ln |Ω| + T 5k=1 ln(σ 2k ). Therefore, the loglikelihood function becomes:

L = −2.5T ln(2π) − ln |Ω| − 0.5T

5
X
k=1

ln(σ 2k ) − 0.5

5 X
T
X

x̂25(j−1)+k /σ 2k .

(73)

k=1 j=1

One diﬃculty with applying this algorithm is that it requires solving the linear system of
5T equations ΩX̂ = X. While in principle this could be diﬃcult and numerically imprecise,
we have typically found that Matlab is able to do it well.
A.9.

Estimation

The goal is to estimate the vector of parameters θ = (ν, γ, ρg , σ g , ρν , σ ν , ργ , σ γ , δ, ω,
λ). The parameter space is (1, +∞) for ν and γ, (−1, 1) for ρg , ρν , and ργ , (0, +∞) for σ g ,
σ ν , and σ γ and (0, 1] for δ, ω, and λ.
Maximum likelihood estimates come from maximizing L with respect to these parameters. We tried several algorithms (Matlab’s simplex-search algorithm, fminsearch, Matlab’s modified Newton-Raphson algorithm, fmincon, and Chris Sims’s alternative modified
22

Newton-Raphson algorithm, csminwel), and we started them from several points. While
there were several local maxima, especially close to the boundaries of the parameter space,
the clear global maximum found by all the algorithms is reported in table 1. For an estimate of the variance-covariance matrix of the estimates, we use the inverse of the Hessian:
¢−1
¡
.
− ∂ 2 L/∂θ∂θ0
Bayesian estimates come from postulating a prior density for the parameters f (θ), and

computing their posterior density using Bayes law: f (θ |X ) ∝ exp (L) f (θ). The priors
we use are described in the notes of table 1. There is no closed-form solution for the
posterior, which must be simulated numerically using Markov Chain Monte Carlo methods.
We developed two ways to do so.
The first uses the Gibbs sampler and exploits the fact that conditional on the remaining
parameters, the posterior for (σ 2g , σ 2ν , σ 2γ ) is known. Since the prior for each of these
parameters is an independent inverse-gamma distribution with parameter (τ k,1 , τ k,2 ), the
density of the prior is proportional to:
5
Y
¡ 2 ¢−0.5τ k,1 +1
exp(−0.5τ k,2 /σ 2k ).
σk

(74)

k=3

Multiplying by the likelihood in (73) shows that the posterior for (σ 2g , σ 2ν , σ 2γ ) conditional
on the other parameters is proportional to:
"
Ã
#
!
P
2
5
Y
)
τ k,2 τ k,1 + Tj=1 ŷ5(j−1)+k
¡ 2 ¢−0.5(τ k,1 +T )+1
exp −0.5
σk
/σ 2s .
τ k,1 + T

(75)

k=3

Therefore, the posterior is also proportional to the product of three independent inverse³
´
´
³
P
gamma distributions, with parameters τ k,1 + T, τ k,2 τ k,1 + Tj=1 x̂25(j−1)+k ) /(τ k,1 + T ) .

The density of the other parameters conditional on the variance does not have a known den-

sity, so it must be simulated using a Metropolis algorithm. The Gibbs sampler therefore
alternatively draws variances from the product of inverse gamma densities conditional on
the other parameters, and then uses a Metropolis step to draw these other parameters
conditional on the variances.
The second approach is to use a Metropolis random-walk algorithm for all 11 parameters.
The proposal function is a multivariate normal with mean equal to the last draw and
variance covariance matrix proportional to its maximum-likelihood estimate. A natural

23

starting point is the vector of maximum-likelihoods estimates.
One would expect that the first approach dominates the second. The pure Metropolis
algorithm must learn the shape of all of the distributions, whereas the Gibbs algorithm
exploits the knowledge that the density of the variances conditional on the other parameters
is a product of inverse gamma distributions. However, for our particular application, we
found that the pure Metropolis algorithm converged faster than the Gibbs algorithm. The
results in tables 1 and 2 were therefore generated using it. We started 5 Metropolis chains,
one at the maximum-likelihood value and the other 4 at overly dispersed draws from the
multivariate normal. Multiplying the maximum-likelihood variance-covariance matrix by
0.75 led to an acceptance rate of 25% for the Metropolis algorithm. Each chain was ran
for 50,000 draws and the first 30,000 were discarded. It took only about 3 days using in
two parallel Pentium 4, 3.2 Ghz computers to obtain these 250,000 draws–this confirms
the speed of the algorithms behind propositions 1 and 2. We monitored the scale reduction
factors proposed by Brooks and Gellman (1998). The largest of these factors was 1.010
supporting convergence, and plots of the between and within variances confirmed it. We
therefore proceeded to mix the draws from the 5 samples to obtain 100,000 independent
draws from the posterior density.

References
An, Sungbae, and Frank Schorfheide (forthcoming). “Bayesian Analysis of DSGE models.”
Econometric Reviews, forthcoming.
Brooks, Stephen P., and Andrew Gelman (1998) “General Methods for Monitoring Convergence of Iterative Simulations.” Journal of Computational and Graphical Statistics, 7
(4), 434-455.
Canova, Fabio (forthcoming). Applied Macroeconomic Research. Princeton University
Press: Princeton.
Coibion, Olivier (2006). “Inflation Inertia in Sticky Information Models.” Contributions to
Macroeconomics, 6 (1), article 1.
Gali, Jordi (1999). “Technology, Employment, and the Business Cycle: Do Technology
Shocks Explain Aggregate Fluctuations?” American Economic Review, 89 (1), 249271.
Kiley, Michael (2005). “A Quantitative Comparison of Sticky-Price and Sticky-Information
Models of Price Setting.” Working paper, Federal Reserve Board.
King, Robert G., Charles Plosser, and Sergio T. Rebelo (1988). “Production, Growth and
Business Cycles I. The Basic Neoclassical Model.” Journal of Monetary Economics,
21, 195-232.
Korenok, Oleg, and Norman R. Swanson (2005). “The Incremental Predictive Information
24

Associated with Using Theoretical New Keynesian DSGE Models vs. Simple Linear
Econometric Models.” Oxford Bulletin of Economics and Statistics, 67 (1), 905-930.
Laforte, Jean-Philippe (2005). “Pricing Models: A Bayesian DSGE approach for the US
Economy.” Working paper, Federal Reserve Board.
Levin, Andrew T., Alexei Onatski, John C. Williams and Noah Williams (2006). “Monetary
Policy Under Uncertainty in Micro-Founded Macroeconometric Models.” In NBER
Macroeconomics Annual 2005, edited by Mark Gertler and Kenneth Rogoﬀ, MIT
Press: Cambridge.
Mankiw, N. Gregory and Ricardo Reis (2002). “Sticky Information versus Sticky Prices:
A Proposal to Replace the New Keynesian Phillips Curve.” Quarterly Journal of
Economics, 117 (4), 1295-1328.
Mankiw, N. Gregory and Ricardo Reis (2006) “Pervasive Stickiness.” American Economic
Review, 96 (2), 164-169.
Nelson, Edward, Javier Andrés and David López-Salido (2005). “Sticky-Price Models and
the Natural Rate Hypothesis.” Journal of Monetary Economics, 52 (5), 1025-1053.
Reis, Ricardo (2004). “Inattentive Consumers.” NBER Working Paper No. 10883.
Reis, Ricardo (2006). “Inattentive Producers,” Review of Economic Studies, 73 (3), 793-821.
Rudebusch, Glenn D. (2002). “Term Structure Evidence on Interest Rate Smoothing and
Monetary Policy Inertia.” Journal of Monetary Economics, 49 (6), 1161-1187.
Smets, Frank and Raf Wouters (2003). “An Estimated Stochastic Dynamic General Equilibrium Model of the Euro Area.” Journal of the European Economic Association, 1
(5), 1123—1175.
Wang, Pengfei and Yie Wen (2006). “Solving Linear Diﬀerence Systems with Lagged Expectations by a Method of Undetermined Coeﬃcients.” FRB Saint Louis Working
Paper No. 2006-003c.

25

Figure 1: Impulse responses to one-standard-deviation shocks
Inflation

Output gap

Labor

0.015

0.02

0.008

0.015
0.01

0.006

0.01
0.004

0.005
0.005

0.002
0

0

10

20

30

40

0

-3

Technology shock

2

0

10

20

30

40

0

-3

x 10

0

0

-2

-2

-4

-4

-6

0

10

20

30

40

0

10

20

30

40

0

10

20

30

40

-3

x 10

0

x 10

-2
-4

0

10

20

30

40

-3

Aggregate demand shock

Monetary shock

0.01

1

-6

0

10

20

30

40

-8

-3

x 10

6

x 10

0.02
0.015

4
0.5

0.01
2

0

0

10

20

30

40

0

0.005

0

10

26

20

30

40

0

Table 1. Parameter estimates
Maximum likelihood
Estimate
ν
γ
ρg
σg
ρν
σν
ργ
σγ
δ
ω
λ

34.068
4.196
.938
.014
.630
1.819
.667
.187
.184
.195
.702

Standard
error
1.000
.626
.021
.002
.019
.252
.035
.047
.026
.011
.015

Prior
95% confidence
interval
[32.109 ; 36.027]
[2.970 ; 5.422]
[.897 ; .979]
[.010 ; .018]
[.593 ; .666]
[1.325 ; 2.313]
[.599 ; .735]
[.094 ; .279]
[.133 ; .234]
[.173 ; . 217]
[.673; .731]

Bayesian posterior

Density

Mean

1+G
1+G
B
IG1/2
B
IG1/2
B
IG1/2
U
U
U

11
11
.7
.222
.7
.222
.7
.222
.5
.5
.5

Standard
error
3.162
3.162
.224
.114
.224
.14
.224
.114
.289
.289
.289

95% coverage
set
[5.795 ; 18.085]
[5.795 ; 18.085]
[.198 ; .991]
[.107 ; .507]
[.198 ; .991]
[.107 ; .507]
[.198 ; .991]
[.107 ; .507]
[.025 ; .975]
[.025 ; .975]
[.025 ; .975]

Median
20.547
6.884
.950
.015
.676
1.289
.638
.347
.176
.210
.657

Standard
error
2.781
1.438
.022
.002
.023
.242
.043
.122
.027
.016
.023

95% coverage set
[15.554 ; 26.408]
[4.542 ; 10.245]
[.904 ; .988]
[.012 ; .019]
[.628 ; .719]
[.887 ; 1.838]
[.534 ; .701]
[.184 ; .674]
[.134 ; .242]
[.182 ; .244]
[.612 ; .703]

Notes: Sample size is 206. The calibrated coefficients are β=2/3, ψ=4, θ=1, φy=.33, φπ=1.24, ρε=.918, σε=.012, ρΔa=.350, σΔa=.010. Maximum
likelihood estimates come from using a modified Newton-Raphson search algorithm to maximize the log-likelihood, and the standard errors from
inverting the Hessian at the optimum. The confidence intervals come from the cumulative density function of the multivariate normal. For the prior
densities we used the gamma (G), the beta (B), the inverse gamma (IG) and the uniform (U) distributions, with parameters (10,1), (2.24,.96),
(2.02,.06) and (0,1) respectively. The posterior moments are based on 100,000 draws from the posterior, which come from mixing 5 independent
simulations of 50,000 draws, each with the first 30,000 draws discarded to ensure convergence.

Table 2. Variance decompositions
Maximum likelihood estimates and 95% confidence intervals
Shock
Aggregate
Aggregate
Variable
Monetary
productivity
demand
Inflation

Output
growth
Hours

Interest
rate
Wage
growth

Goods markup

Labor markup

.896

.028

.004

.070

.003

[.826 ; .937]

[.019 ; .041]

[.002 ; .009]

[.033 ; .130]

[.001 ; .006]

.247

.153

.436

.101

.064

[.171 ; .320]

[.115 ; .198]

[.257 ; .610]

[.040 ; .185]

[.024 ; .132]

.551

.032

.336

.041

.041

[.319 ; .663]

[.014 ; .067]

[.179 ; .612]

[.012 ; .084]

[.010 ; .116]

.506

.066

.017

.295

.117

[.352 ; .646]

[.048 ; .089]

[.008 ; .031]

[.163 ; .438]

[.053 ; .200]

.183

.262

.016

.479

.061

[.136 ; .244]

[.195 ; .360]

[.007 ; .033]

[.300 ; .605]

[.027 ; .099]

Goods markup

Labor markup

Bayesian median estimates and 95% credible sets

Variable
Inflation

Output
growth
Hours
Interest
rate
Wage
growth

Monetary

Aggregate
productivity

Shock
Aggregate
demand

.835

.028

.005

.129

.002

[.717 ; .906]

[.019 ; .040]

[.003 ; .010]

[.066 ; .242]

[.001 ; .004]

.213

.133

.446

.150

.048

[.155 ; .274]

[.101 ; .164]

[.309 ; .590]

[.080 ; .263]

[.021 ; .107]

.469

.028

.398

.063

.025

[.237 ; .603]

[.010 ; .057]

[.238 ; .703]

[.023 ; .144]

[.006 ; .076]

.408

.057

.016

.416

.093

[.251 ; .569]

[.039 ; .077]

[.010 ; .027]

[.266 ; .585]

[.043 ; .188]

.093

.192

.035

.663

.035

[.053 ; .146]

[.125 ; .271]

[.008 ; .027]

[.514 ; .786]

[.017 ; .070]

Notes: Maximum likelihood estimates come from using the MLE parameter estimates. The
confidence intervals are the 2.5% and 97.5% percentiles from 1,000 draws taken from a
multivariate normal distribution with mean and variance-covariance equal to their MLE
estimates. Bayesian estimates are the median, 2.5% and 97.5% percentiles cell-by-cell using
100,000 draws from the posterior density.

