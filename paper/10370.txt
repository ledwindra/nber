NBER WORKING PAPER SERIES

DO COLLEGE INSTRUCTORS MATTER?
THE EFFECTS OF ADJUNCTS AND GRADUATE ASSISTANTS
ON STUDENTS’ INTERESTS AND SUCCESS
Eric Bettinger
Bridget Terry Long
Working Paper 10370
http://www.nber.org/papers/w10370
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
March 2004

The authors thank the Ohio Board of Regents for their support during this research project.
Robert Sheehan and Andy Lechler provided invaluable help with the data. We also thank
participants at the NBER Summer Institute. Erin Riley, Suzan Akin, and Jason Weaver provided
excellent research assistance. The views expressed herein are those of the authors and not
necessarily those of the National Bureau of Economic Research.
©2004 by Eric Bettinger and Bridget Terry Long. All rights reserved. Short sections of text, not
to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.

Do College Instructors Matter? The Effects of Adjuncts and Graduate Assistants on
Students’ Interests and Success
Eric Bettinger and Bridget Terry Long
NBER Working Paper No. 10370
March 2004
JEL No. I2, H4
ABSTRACT
One of the most pronounced trends in higher education over the last decade has been the
increased reliance on instructors outside of the traditional full-time, Ph.D.-trained model. Nearly 43
percent of all teaching faculty were part-time in 1998, and at selective colleges, graduate assistant
instructors teach over 35 percent of introductory courses. Critics argue that these alternative
instructors, with less education and engagement within a university, are causing the quality of
education to deteriorate and may affect student interest in a subject. However, little research exists
to document these claims. This paper attempts to fill this void using a unique dataset of students at
public, four-year colleges in Ohio. The paper quantifies how adjunct and graduate assistant
instructors affect the likelihood of enrollment and success in subsequent courses. Because students
with alternative instructors may differ systematically from other students, the paper uses two
empirical strategies: course fixed effects and a value-added instructor model. The results suggest that
adjunct and graduate assistant instructors generally reduce subsequent interest in a subject relative
to full-time faculty members, but the effects are small and differ by discipline. Adjuncts and
graduate assistants negatively affect students in the humanities while positively affecting students
in some of the technical and professional fields.

Eric Bettinger
266 Peter B. Lewis Building
Dept of Economics
Case Western Reserve University
11119 Bellflower Rd.
Cleveland, OH 44106-7235
bettinger@cwru.edu
Bridget Terry Long
Harvard Graduate School of Education
Gutman Library 465
Appian Way
Cambridge, MA 02138
longbr@gse.harvard.edu

I.

Introduction
One of the most pronounced trends in higher education over the last two decades has been the

growing reliance on instructors outside of the traditional full-time, tenure-track model of faculty. In
particular, the percentage of courses taught by adjunct instructors, most often defined as part-time
faculty, increased by 30 percent at four-year colleges and universities from 1987 to 1999. The trends
were especially pronounced at public research and doctoral institutions during which time adjuncts
increased by 50 and 80 percent, respectively (NCES 1997, 2001). While adjuncts make up an
increasing proportion of new hires, they also are steadily replacing full-time positions. Between 1993
and 1998, 40 percent of all universities replaced full-time positions with part-time faculty (NCES,
2001). Given these trends, nearly 43 percent of all teaching faculty were part-time by 1998 (Chen,
2002). Additionally, graduate students often serve the role of college instructor. In 2000, 46 percent
of all graduate students in the nation had full responsibility for teaching at least one course and over 70
percent had at least some teaching responsibility (NCES, 2000).
Many have voiced concern about whether the growing use of adjunct instructors has affected
the quality of higher education. Because many adjuncts do not have Ph.D.s or other terminal degrees,
critics question whether they can provide the same quality of education as full-time faculty members.
In the humanities, for instance, the Modern Language Association (MLA) has asserted that university
quality has deteriorated with the increased usage of part-time faculty (MLA 2002, MLA 1985).
Moreover, some wonder whether adjuncts reduce student interest in a subject (National Institute of
Education 1984). Similar accusations have been made concerning graduate student instructors (Borjas,
2000; Norris, 1991). In addition to affecting instruction, adjuncts could impact the quality of student
advising and affect the distribution of other departmental tasks such as committee work. College
administrators have also complained about the additional time necessary to manage an adjunct and
graduate student workforce including the need to monitor teaching and find replacements due to
turnover.

1

On the other hand, there may be gains due to specialization from hiring adjuncts and graduate
assistants. Adjuncts typically focus on teaching, and therefore, may be better instructors than faculty
members who have to balance other job demands. Additionally, adjunct instructors may have
concurrent or previous industry experience outside of the university. This experience may enhance
their ability to teach or help students find eventual employment. Finally, adjunct and graduate
assistant instructors may also allow full-time, tenure-track faculty to more effectively focus on
research so that production within the department could increase.
The use of alternative types of instructors is partly explained by increasing pressure on
colleges to reduce costs (Leslie, 1998b). According to a 2001 report by the College and University
Professional Association for Human Resources (CUPA-HR), if universities compensated faculty solely
for teaching, then full-time faculty would average $2,674 per credit hour.1 In contrast, universities
paid adjunct faculty at the same institutions only $592 per credit hour. The attractiveness of adjuncts
as an inexpensive alternative to full-time, tenure-track faculty is even more pronounced considering
that 47 percent of universities do not offer benefits to part-time faculty (NCES, 2001). Under these
conditions, many adjuncts have expressed feelings of being treated as "second-class citizens," and this
could adversely affect their job performance (Leslie, 1995). Likewise, graduate assistants are an
inexpensive alternative to full-time faculty members, and the recent movement to unionize could
signal similar concerns.
The growing reliance on adjunct professors may signify higher education becoming more like
other parts of the American labor market. The end of mandatory retirement for faculty members in
January 1994 greatly increased the cost of tenure for colleges (Ehrenberg, 2002), and the use of
adjunct and graduate assistant instructors is one way to avoid this expense. The growing presence of
adjuncts may also reflect the desire of higher education to have a more flexible workforce. Part-time,
often temporary employment may help universities to screen for potential full-time faculty members
(Autor, 2000). Furthermore, temporary contracts allow colleges to be responsive to changes in the
1

The average salary of a full-time faculty member at a four-year public university was $58,828 in 2001, and the
average course load of full-time faculty was 22 semester hours.

2

demand and resources. For these reasons, proponents of the growing use of adjuncts argue that they
are essential to maintaining quality in a tight fiscal environment.
Surprisingly, however, little research exists to document any of the positive or negative claims
in relation to alternative instructors or the tradeoffs that are faced when using them. Whereas
researchers and policymakers continually debate measures of teacher quality and the effect of teacher
characteristics on student outcomes in primary and secondary school (e.g. Murnane et. al. 1991, Card
and Krueger 1998, Hoxby 2002, Temin 2002, Hanushek and Rivkin 2003), little is known about the
role of instructor quality in higher education or how to measure it. While several papers document the
growing trend in adjunct teaching (Burgan, Weisbuch, and Lowry 1999, Balch 1999) and others
describe the employment conditions of adjuncts (Gappa and Leslie 1993, Gappa 2000, NCES 2001),
there is little research on the impact of adjunct instructors. Similarly, only two studies could be found
on the effect of graduate assistant instructors on student outcomes and they rely on relatively small
samples with little information on student background. Moreover, none of the work compares the
relative effectiveness of different types of instructors.
One reason for the lack of research in these areas is the inability to link individual collegiate
outcomes to instructors' characteristics. While data exist on the experiences of college students (e.g.
Baccalaureate and Beyond and the National Education Longitudinal Study of 1988) and other data
survey faculty characteristics (e.g. the National Study of Postsecondary Faculty), one cannot link these
sources in meaningful ways to conduct analysis. Therefore, researchers are unable to identify the
characteristics of the faculty members that teach and advise students. However, this study attempts to
fill this gap using a unique longitudinal dataset.
This paper estimates the impact of adjunct instructors and graduate student instructors on
student outcomes by examining their effects on students' course-taking behavior, major choice, and the
completion of subsequent courses. The analysis is based on administrative and transcript data
available through a collaborative agreement with the Ohio Board of Regents (OBR). We track the
nearly 25,000 first-time freshman students at 12 public, four-year colleges in Ohio with information on

3

students’ course-taking behavior and performance as well as the characteristics of the corresponding
faculty member responsible for each course from when they enter in Fall 1998 to Spring 2002.2 The
OBR also provides basic information on each student’s background, high school performance, test
scores, and intended major. Due to the design of the data, we are also able to identify every course
that a student took even if he or she took courses at multiple campuses in Ohio.
To determine the impact of adjuncts and graduate students on student outcomes, we compare
the outcomes of students who had different types of instructors (i.e. adjunct, graduate student or fulltime professor) in their introduction to a particular subject. We employ two estimation strategies.
First, we use course fixed effects to compare students who took the same course but were assigned
different sections with different types of instructors. This strategy addresses biases related to the fact
that the likelihood of having an adjunct or graduate student instructor may be related to student
characteristics such as ability through course selection or their intended major. Additionally, similar to
Card and Kreuger (1998) and Rockoff (2004), we estimate fixed effects for individual faculty
members and then compare these estimated value-added coefficients to instructor characteristics.
The results measure the effects of college instructor quality on student outcomes while also
commenting on the tradeoffs between different types of labor in the production of higher education.
The findings suggest that, in general, adjunct and graduate assistant instructors reduce subsequent
interest in a subject relative to full-time, tenure-track faculty, but this effect is small and differs widely
by discipline. We find that adjuncts negatively affect students in the humanities and sciences while
positively affecting students in some of the professional fields, particularly in terms of success in
subsequent courses. In many cases, adjuncts under the age of 40 account for the estimated negative
effects suggesting that recent movements towards hiring young instructors, who are often
inexperienced and have not completed doctoral study, is negatively impacting students.

2

Due to data problems, we do not include is Shawnee State University. Shawnee is a small, non-selective college
representing less than 2 percent of the total enrollment at four-year, public colleges.

4

II.

LITERATURE REVIEW
In the K-12 literature, researchers routinely use and reevaluate measures of teacher quality.

For example, Hoxby (2000) measures what types of teacher characteristics districts value when they
are facing strong competitive pressures. To measure teacher quality, researchers often use
undergraduate college selectivity, subject matter expertise (measured by test scores and college
performance), the completion of advanced degrees, and experience. For example, Figlio and Rueben
(2001) use the test scores of education majors to gauge how tax limits affect the quality of new
teachers. Other studies directly link proxies for teacher quality to student outcomes. Ehrenberg and
Brewer (1994) found that students with teachers from more selective undergraduate institutions scored
higher on standardized tests after controlling for student background characteristics. This information
has been helpful in larger debates about the tradeoffs between different types of investments that could
be made in schools. Assuming higher-quality teachers are more expensive, schools often must choose
between increasing teacher quality (and thereby employing fewer teachers) or lowering class size.
In contrast, research about the connection between instructor characteristics and student
outcomes in higher education is virtually absent from the literature. The few studies that exist focus
on the effect of particular types of graduate assistants, rely on relatively small samples, and do not
have much information on student background. For example, Borjas (2000) analyzes the impact of
foreign teaching assistants on economics students’ performances at Harvard. Norris (1991) also
examines the effect of nonnative, English-speaking teaching assistants on students at the University of
Wisconsin. The literature also does not address issues related to the effect of the growing use of
adjunct instructors on students. Therefore, this paper addresses a considerable gap in the
postsecondary literature about the effects of different kinds of instructors.
While little is known about the impact of adjuncts on student outcomes, several papers
document the growing use of adjuncts. Foremost, David Leslie provides a wealth of information on
this trend in a series of articles. In The Growing Use of Part-Time Faculty (1998a), Leslie uses the
1993 National Survey of Postsecondary Faculty to quantify the increase. He finds that 42 percent of

5

teaching faculty members have part-time appointments only. Moreover, he finds that adjunct usage
varies by institution type and discipline. Research universities were least likely to employ adjuncts
while 60 percent of public, two-year faculties teach part-time. Other work provides further evidence
of the growing use of adjuncts. Burgan, Weisbuch, and Lowry (1999) find an increase in the use of
instructors on term contracts when analyzing a survey of non-tenure track faculty. Similarly, Balch
(1999) examines the increased use of part-time faculty as a trend that will continue to persist. Many
other papers discuss trends at particular institutions. For example, Jackson (1999) documents the
growth of temporary and part-time appointments at Maryland’s public colleges from 1981 to 1998.
Several reports examine the impact of adjuncts at particular institutions. For instance, Haeger
(1998) discusses the problems and solutions associated with adjunct instructors at Towson University.
However, due to a lack of data, researchers have not been able to perform large-scale analyses of the
impact of adjuncts on student outcomes. Instead, several have speculated about their effects. Leslie
(1998b) notes that adjuncts could affect education quality because fewer have Ph.D.s. In addition to
affecting instruction, Pisani and Stott (1998) argue that the use of adjuncts erodes the quality of student
advising, and others suggest that part-time faculty affect the distribution of other departmental tasks
such as committee work. The MLA (2003), the National Institute of Education (1984), and the
Education Commission of the States (Palmer 1998) have all issued reports or policy statements that
link the growing use of part-time professors to a decline in educational quality. Moreover, some
question the impact of adjuncts on student interest in a subject (National Institute of Education 1984).
On the other hand, Leslie and Gappa (1995) argue that part-time faculty could help broaden academic
programs by introducing real-world experiences into the classroom. Others have documented the
employment conditions and dissatisfaction of adjuncts (Gappa and Leslie 1993, Gappa 2000, and
Fulton 2000). Since many adjuncts have expressed feelings of being treated as "second-class citizens,"
Leslie (1995) questions how their treatment might affect the quality of education that adjuncts supply.
There is an additional literature on the role of graduate students in university teaching. Much
of this literature argues that graduate student teaching is essential to training future professors (e.g.

6

Smith 2001, Meyers and Prieto 2000, Knotts and Main 1999, Prieto and Altmaier 1994, Slevin 1992).
Yet while graduate student teaching may be an important aspect of training, the working conditions are
poor (McLeod and Schwarzbach 1993). Graduate students frequently complain about low wages,
large workloads, "poor working environments," and working without guidance (Koehnecke 1991).
These stresses coupled with the increased reliance on graduate student instructors have led to recent
efforts to strengthen graduate student teacher unions at universities across the nation (Mattson 2000,
Vaughn 1998, Sharnoff 1993).

III.

THEORETICAL FRAMEWORK: Production using Different Types of Instructors
Departments must make human resource decisions using faculty (Fk) to accomplish three

goals: teach the necessary classes (Tk), produce research (Rk), and provide service (Sk) to the
university in the form of committee work and advising students. Therefore, in a utility-maximizing
framework, department k does the following:
(1)

max Uk = α1 Tk (Fk) + α2 Rk (Fk) + α3 Sk (Fk)
s.t. Ck = ∑ Cjk ( Fjk ) ≤ Bk (Tk (Fk)) + Dk (Rk (Fk))

The relative importance of each goal is given by α1, α2, and α3 and may vary depending on whether the
institution is a research university or liberal arts college. Note that the budget of the department must
be less than the total amount spent on faculty members (Ck) and is related to student enrollment
through the production of teaching (Bk) and funds generated by research (Dk). Implicit in this
assumption is that students maximize their current consumption and future returns to education and
therefore seek to maximize gains from teaching. Student i’s maximization problem can be
characterized as:
(2)

max Ui = Ui ( ∑ Tk (Fk) ; other goods )
Suppose that there are two types of instructors: full-time faculty members and alternative

instructors (adjuncts and graduate assistants). While full-time faculty members engage in each of the
three tasks of a department, alternative instructors are only involved in teaching. However, the relative

7

proportion of full-time faculty members to alternative instructors may affect the research and service
functions of the department. For instance, the research productivity of full-time faculty members may
increase if the presence of alternative instructors allows them to specialize in research. However,
adjuncts and graduate assistants may not be required to help with advising and other types of service,
and therefore, as their numbers grow, there are fewer people left to handle these tasks.
Instructors provide two types of knowledge to students through their teaching. The first type,
academic (λ), is rooted in scholastic research while the second type, vocational (θ), develops from a
connection to industry and the labor market. In addition, each type of instructor has an optimal
amount of time (βFT and βALT) they wish to devote to teaching. The pool of faculty in a department
can be characterized as:
(3)

Fk = ∑ FFT, jk (λFT, θFT, βFT) + ∑ FALT, jk (λALT, θALT, βALT)
The instructor types may differ in their relative stock of each kind of knowledge. In terms of

academic knowledge, this may be true for several reasons. First, alternative instructors such as
adjuncts and graduate assistants often do not have terminal degrees and therefore may not be as
knowledgeable about a particular subject as full-time professors, the majority of whom have Ph.D.s.
Adjuncts are also not as involved in research, so to the extent that research influences teaching quality,
full-time faculty may be better teachers and provide more academic knowledge about a subject.
Additionally, because alternative instructors serve in a limited capacity, they may not have the same
knowledge about the university in comparison to full-time professors. As a result, they may not be as
effective in advising students about academic matters. For these reasons, we assume that λFT > λALT.
However, the relative size of β and θ is unclear and will depend on the type of alternative
instructor. In the case of adjuncts, one is likely to find βFT < βALT because adjuncts do not have
research or service requirements and can specialize in teaching. Moreover, because adjuncts are
judged according to their teaching, they may have greater incentives to do well at it.3 In terms of
vocational knowledge, older adjuncts may bring current or previous experience in industry, and
3

However, chairs note that it is difficult to monitor the quality of adjunct teaching and so this may not be the case.

8

consequently, they may have more practical information and provide better access to future
employment than full-time faculty members. Therefore, one could find θFT < θALT. On the other hand,
younger adjuncts, who might not have completed graduate work and could be very inexperienced, may
have little industry knowledge so the reverse could be true. In the case of graduate assistants, one
would expect to find βFT > βALT since graduate student instructors have other requirements such
coursework and research. Moreover, because they have not fully entered the discipline, θFT > θALT.
These differences in the amount of knowledge affect subsequent student and departmental
outcomes. First, the knowledge gained in an introductory class directly affects student success in
subsequent courses. Experiences with instructors may also affect future course-taking behavior. If
students choose their courses (and major) based on their knowledge and experiences in a given subject,
the mix of instructors they face early in a given discipline could influence these decisions. For
example, if a course produces additional knowledge that changes the subject in which a particular
student has a comparative advantage, then the student may change their major or choose a different set
of courses. Therefore, the size of ∂Tk / ∂FFT and ∂Tk / ∂FALT depends on the relative stock of each
kind of knowledge that each type of instructor has as well as the amount of effort they are able to put
towards teaching.
The effectiveness of different kinds of instructors is also likely to vary by discipline as it will
depend on the relative importance of academic and vocational knowledge in the department. For
example, in the humanities, which presumably favor academic knowledge, one may find that ∂Tk /
∂FFT > ∂Tk / ∂FALT while the relationship may be the opposite in professional fields such as business,
in which vocational knowledge is much more valued. Therefore, depending on the relative sizes of λ,
θ, and β and the importance of these factors in the discipline, one type of instructor might provide
better teaching outcomes (i.e. student enrollment) over another. It is important to note that even if
alternative instructors are not as effective as full-time faculty members, their use may still be rational.
As stated in equation (1), departments maximize their utility by choosing the optimal mix of

9

instructors to accomplish multiple goals, and adjuncts may have positive indirect effects on research
that justify a possible reduction in teaching outcomes.
In this paper, we measure teaching outcomes in three ways. The first two relate to student
enrollment: the number of credit hours taken in a subject and whether a student majors in a subject.
Thirdly, we examine the success rate of students in subsequent courses.4 Therefore, this paper
provides estimates of the relative differences between ∂Tk / ∂FFT and both ∂Tk / ∂FAdjunct and ∂Tk /
∂FGA by discipline.
Another way to think about this is that each instructor creates a certain value-added. In the
case of departments, instructors may increase student interest in a subject. The interest of student i in
subject k can be characterized as:
(4)

Iik = αi + βi Xi + ∑j VAjk + εi

s.t. VAjk = VAjk (λjk, θjk, βjk)

Interest in a subject is related to student background (Xi), including ability and interests prior to
entering college, and the value-added by each faculty member j that they take in subject k. This valueadded can be thought of as the faculty member’s fixed effect on student interest and is related to the
stock of knowledge the instructor has along with the effort they put into teaching. For the reasons
given above, VAFT and VAALT are likely to differ from each other and to vary by discipline. For the
analysis, we compare the magnitude of VAAdjunct and VAGA to VAFT.

IV.

EMPIRICAL FRAMEWORK: Course Fixed Effects and Faculty Value-Added Models
To identify the effects of adjuncts on students' course taking behavior, we focus on students'

experience by subject. Our unit of observation is student by subject (i.e. k observations per student
corresponding to the k subjects that each student takes classes in). We employ two strategies to
measure the impact of instructor type on students' course-taking behavior and subsequent success. The
first uses variation in instructors within a course. The second estimates the value-added by each
4

In previous versions of the paper, we also attempted to identify the effects of adjuncts on persistence and graduation.
While these are important outcomes, it is difficult to determine whether an adjunct in a single subject can realistically
contribute to such outcomes for a student. The results were difficult to interpret especially since many students who
are at-risk to drop-out take courses in departments with a greater presence of adjuncts.

10

instructor using faculty fixed effects and then links that value to student outcomes. Below we expand
on each strategy.
Estimating Instructor Effects Using Course Fixed Effects
In evaluating students’ experiences with different types of instructors, the key independent
variable is the proportion of the courses in subject k that student i took from adjuncts or graduate
assistants during the first semester student i was exposed to the topic. For example, if a student took
his or her first course in subject k from an adjunct professor, the variable would equal one. If the
student took the course from a full-time faculty member, the variable would equal zero. For those
cases where students take multiple courses in a given subject in the first semester of exposure, we set
the adjunct variable equal to the proportion of faculty that were adjuncts weighted by number of
semester credits for each course. We similarly define students' experiences with graduate student
instructors.
Our basic strategy will be to compare student i's outcomes in subject k to their experience with
adjuncts and graduate assistants:
(5)

yik = αik + βik Adjunctik + φik GradAsstik + γXik + λk + δi + εik

where λk represents fixed effects for the particular subject, δi controls for the semester the student was
first introduced to the topic, and Xik includes controls for student characteristics and the number of
credit hours students attempted in the first semester. Because we have multiple observations per
student, we always control for within-student correlation by clustering the standard errors.
The distribution of students across courses taught by adjuncts, graduate students, and full-time
faculty members may not be random. For example, if adjuncts or graduate students are more likely to
teach in particular majors or during evenings or weekends, then certain types of students will be more
likely to have them in courses (e.g. students with particular interests/abilities or who are more likely to
take evening courses). Additionally, students may choose courses based on the type of instructor. As
discussed above, students might prefer full-time professors if they perceive that they produce greater
knowledge or provide better advising than adjunct faculty, and the preferences for particular types of

11

instructors may be stronger within a student’s major. If students who take adjunct professors are
systematically different from other students, then our results will be biased by these traits.
To deal with the potential endogeneity of taking an adjunct or graduate assistant, we use course
fixed effects. This is an effective method in controlling for unobserved heterogeneity in students
course-taking behavior since it estimates the effect of adjuncts on students who take the same courses
but have different instructors due to multiple sections being offered or due to the fact that the course
was taken different years with different types of instructors. Essentially we are identifying off variation
in the assignment of instructor types between different sections and/or years of the same course.
Although we argue that course-fixed effects are an effective means of controlling for student sorting
across instructors, there may be some remaining selection issues if students systematically sort across
sections with a single course. We discuss this issue more fully below with the results.
To examine subsequent success in courses, we must also contend with nonrandom selection.
Since adjuncts and graduate students may affect student enrollment patterns, evidence on students'
success may have inherent selection biases – we never observe the potential success of students who
do not take additional classes. To estimate Equation 5, we must first control for the effects of adjuncts
and graduate assistants on the likelihood that students take additional courses. We estimate Heckman
selection models to control for this (Heckman 1979).
Instructor Value-Added Models
An alternative approach to using course-fixed effects is to estimate a value-added model for
faculty. This technique has gained popularity in hierarchal models of educational production. For
example, Card and Kreuger (1998) use this to illustrate the effects of different state's primary and
secondary school inputs on students' eventual earnings. Rockoff (2004) uses a similar approach to see
how student characteristics affect student test scores in New Jersey.
The value-added approach consists of two steps. First, we estimate the effect of a set of
dummy variables, one for each faculty member j, on the dependent variable.5 We save the coefficients
5

If a student took multiple courses in their first semester, we randomly choose one instructor.

12

on these faculty dummy variables and use them in the second stage as dependent variables. In the
second stage, we regress these measures of value-added against instructor characteristics. Our second
stage regression is:
(6)

(Value-added)jk = αjk + βjk Adjunctjk + φjk GradAsstjk + γZjk + λk + εjk

where λk represents fixed effects for the particular subject and Zjk includes controls for faculty age,
race, and gender. We also include campus fixed effects in these regressions. The course-fixed effect
models described in the previous section essentially weight faculty members effects by enrollment. By
contrast, the value-added approach weights faculty members equally and provides an estimate of the
average effect amongst faculty members.
Similar to the previous model, there could be student selection issues that bias these results. In
particular, one might worry that adjuncts and graduate assistants are not randomly assigned to courses.
They may be assigned to teach students where the potential for value-added is small (e.g. night classes
instead of honors classes) or in which the course inputs, such as class size, vary. However, because we
limit the sample to those that teach one of the first courses a student takes in a subject, these are all
instructors from introductory classes. It is unlikely that these beginning courses differ significantly,
particularly within department, the level of analysis in most of the paper. To check this, we matched
our data to information from the course catalog for one of the largest schools in our sample to see if
certain instructor types were more or less likely to teach at certain times of the day (i.e. day versus
night) or during certain parts of the week (i.e. weekday versus weekend). We found no relationship
between the likelihood of having an adjunct or graduate assistant and the time of day or week. For
these reasons, we do not believe this possible selection issue is significant.
The Data
This paper focuses on full-time, traditional (age 18 to 20), first-time freshman who entered
public, four-year colleges in Ohio during Fall 1998. The data are provided by the respective
institutions to the OBR and include information on student demographics, enrollment, credit hours
completed, and grade point averages. Furthermore, OBR has linked the student records to ACT and

13

SAT records. Most Ohio students take the ACT exam, and the ACT records include the highest test
score of the student and the most recent responses to the ACT survey, which includes important
student-reported information on high school preparation, performance and academic interests (most
notably, intended major in college). Summary statistics of the student sample can be found in Table 1.
The first column shows characteristics for the whole sample while the second column focuses on the
subsample of students who took the ACT exam. Since many of our covariates are only available for
these students, we will restrict our sample to these students throughout the paper.
Although this paper focuses on students and instructors in Ohio, the results should also have
external validity. Ohio is the sixth largest state in terms of college enrollment and seventh in terms of
population. The only states with greater numbers of students in public colleges are California, Texas,
New York, and Illinois (NCES, 2000). Moreover, Ohio reflects the complete spectrum of urban to
rural communities and labor markets that exist across the nation. Second, the array of public choices
in Ohio reflects the options students face in many other states. Another compelling reason to study
Ohio is that its college enrollment rates are similar to national patterns. The percentage of Ohio public
school students who graduate from high school and enter higher education the following fall are near
the national averages (Mortenson, 2002).
The most important sources of information for this project are the students’ transcripts, which
detail every course in which a student enrolls.6 For this paper, we drop all remedial or developmental
courses. From these data, we know the following information for each section of each course: topic
covered, how many hours the course was worth, the faculty member chiefly responsible for the course,
and whether the student passed or failed the course. For each faculty member, we observe whether the
faculty is full-time or part time, tenure or non-tenure track, the highest degree completed, and for most
of the sample, demographics such as age, race, and gender. Following the literature on adjunct
teaching, we refer to adjuncts as part-time faculty.

6

For schools on quarter rather than semester schedules, OBR converts the quarter hours to semester hours to
standardize the analysis.

14

Tables 2a and 2b summarize the characteristics of the instructors at public, four-year colleges
in Ohio. Table 2a summarizes the characteristics faculty who taught in courses with a total enrollment
of at least 20 students. The first column shows the raw mean while the second column weights the
means by student enrollment so to reflect the average faculty characteristics faced by a student. The
unit of observation is the instructor, so if a professor teaches multiple courses or multiple sections, he
or she is only listed once. About 51 percent of students have faculty members with Ph.D.s, 26 percent
have part-time instructors, and 13 percent take courses from graduate assistants. Table 2b limits the
faculty sample to those who taught initial courses in a subject. This sample represents all courses
regardless of enrollment. The average age of full-time faculty in introductory courses is similar to that
of part-time faculty. Adjunct instructors tend to include more women and minorities than the full-time
faculty. Finally, there are dramatic differences in the likelihood that a faculty member has a Ph.D.
across samples. Almost 90 percent of full-time faculty members have Ph.D.’s while less than 25
percent of part-time faculty have them.
As shown in Table 3, the use of alternative instructors differs by type of campus and
discipline.7 The table includes all professors including those teaching upper-division or small courses.
At selective institutions, the percentage of adjuncts teaching undergraduate level courses is about 22
percent. At non-selective institutions, this percentage is about 35 percent. The reverse pattern holds
for graduate students. Selective institutions rely more heavily (35 percent of course) as compared to
non-selective institutions (16 percent). The breakdown of faculties by school or department also
shows tremendous variation. Architecture has the highest percentage of adjuncts with about 56
percent of their faculty being part-time. Social Work and Public Administration similarly employ
adjuncts (46 percent of faculty). The faculties in the schools of Humanities and Business have a
similar proportion of adjuncts, about 32-34 percent. The Sciences have the lowest proportion of

7

Selective and non-selective institutions are distinguished by their admissions policies – non-selective colleges have
open admissions. The six selective colleges include Bowling Green State University, University of Cincinnati, Kent
State University, Miami University, Ohio State University, and Ohio University. Miami University and Ohio State
University are the top ranked public universities in Ohio with the former being categorized .as "highly selective" by
the Barron's Guide to College (Barrons 1997).

15

adjuncts with about 10 percent of the faculty being adjuncts. About 22 percent of the instructors in the
Social Sciences are adjuncts. The Social Sciences, Sciences, and Business have only limited reliance
on graduate students (2.4, 5.3, and 7.1 percent respectively) while Humanities relies heavily on
graduate student instructors (29 percent).
While the dataset provides a great deal of useful information, it also has several limitations.
First, we do not observe how many years a particular faculty member has been affiliated with a
particular university. In addition, we cannot track movements of faculty to other universities or their
professional activities at a particular university (including concurrent appointments at other
universities). Another limitation is that the data only include students attending Ohio public
universities. Students from Ohio that attend universities in other states, and students that attend private
schools in Ohio are excluded from the sample. Students who transfer from Ohio public institutions to
institutions located in other states are also indistinguishable in the data from students who dropout of
college. This potential bias, however, should be very small since the percentage of students who likely
transferred to private institutions or those outside of the state make up a small fraction of the total
number of observed dropouts.8 Furthermore, this data does a much better job at tracking students than
previous work.

V.

EMPIRICAL RESULTS

Estimates using Course Fixed Effects
Course fixed effects deal with the most egregious of selection issues, most notably, systematic
differences in the courses taken by students due to variation in ability levels and academic interests.
By looking at variation only within a course, we eliminate bias related to class selection. Table 4
demonstrates this fact by showing estimates of the relationship between student ability and intended

8

While we can not track students who transfer to private colleges or public out-of-state institutions, this is not likely
to be a large group. Using data from the Integrated Postsecondary Education Data System (IPEDS) and assuming that
transfer students are geographically representative of the freshman class, then one would expect around 650 Ohio
students to transfer to the out-of-state colleges. If one further assumes that all 650 transfer students just finished their
1st year, then about 4.3 percent of observed dropouts in our data are actually transfer students.

16

major and the types of instructors that individuals have. Without course effects, a one-point increase
in a student's ACT score decreases the likelihood that they have an adjunct instructor in a particular
course by 0.17 percentage points. Once we include course fixed effects, the magnitude of this estimate
drops by almost two-thirds, and no difference is found between those who had intended to major in the
subject and those that did not. The estimated effect appears to be driven by outcomes in the
humanities (-0.16 percentage points per ACT point). In the social sciences and sciences, the estimated
relationship is actually positive suggesting that students with higher test scores are slightly more likely
to be assigned adjuncts. The second panel focuses on the assignment of graduate student instructors.
Unfortunately, even with campus fixed effects we are not able to eliminate much of this relationship.
For each set of disciplines, we estimate a negative relationship between students' ACT scores and the
propensity to have a graduate student instructor.
Despite the observed differences in Table 4, any remaining selection after using course fixed
effects is unlikely to be a problem in our estimates. First, the estimated bias is very small. For
example, if a student's ACT score increases by 10 points – a dramatic increase for the ACT (maximum
score of 36), it only decreases the likelihood of having an adjunct by 0.6 percentage points. Another
way to put the magnitude in perspective is to compare the experiences of two students. Suppose one
has an ACT score of 18. This is one standard deviation below the sample mean and corresponds to the
34th percentile nationwide. The second student has an ACT score of 26 on the ACT – one standard
deviation above our sample average and at the 87th percentile on the test nationwide. These students
differ in ability rankings by almost 53 percentile points and yet the predicted difference in their
likelihood of having an adjunct is only 0.48 percentage points. This minute difference in the
likelihood of having an alternative instructor is likely to be unimportant and is only identified because
our sample is so large (234,143). It is also important to note that our subsequent models of the effects
of alternative instructors include controls for student background, including ACT score, and therefore,
we account for any observable ability sorting across sections.

17

Further analysis suggests that only two schools are driving the remaining selection found in the
estimates of Table 4: one for the assignment of adjuncts and another for the assignment of graduate
students. One possible explanation for this is the way some colleges handle registration. For example,
at several campuses, honors students are allowed to register for class before other students. Therefore,
they may choose to enroll in sections with professors whom they perceive to be better. However, at
most schools, this type of selection was likely tempered by the fact that course guides from the period
often did not list an instructor for each course. This was particularly true for introductory classes.
Therefore, students on most campuses did not have the information necessary to decide for or against
any particular professor. The exception appears to be for the two schools noted above. When we
exclude the former, the estimated effects of having an adjunct (to be shown below) do not change.
Likewise, when we exclude the latter school, the results of the paper do not change for having a
graduate assistant instructor. This provides additional support for the notion that selection is not
significantly biasing the results. To maximize our ability to estimate the impact of alternative
instructors by individual departments, we keep these two schools in the sample. However, to be
cautious, the models include campus fixed effects so that if either college systematically biases the
results in any way, this part of the estimate is removed.
Even if students with higher scores are differentially likely to have alternative instructors, it is
unclear in what the direction the bias might be. Our first two outcomes do not measure achievement,
and it is not clear if subsequent student interest is related to ability. It might be the case that students
with higher scores are more likely to correctly guess in which courses they are truly interested and so
are more likely to take future classes in the subject. On the other hand, because they are of higher
ability, they may be more willing to explore different subjects confident that they will do well
regardless.
The Impact on Subsequent Enrollment and Major Choice
Table 5a examines the relative impact of adjuncts and graduate assistants in comparison to
full-time faculty members. The outcome is the number of credit hours taken in a subject after the

18

initial semester. In specifications 1 and 2, which do not include course fixed effects, adjuncts are not
found to have a differential effect from full-time faculty members on the total semester hours taken
after the first semester. However, once comparing the student outcomes of those that had adjuncts to
those that had full-time instructors in the same course (columns 3 and 4), adjuncts appear to have a
statistically significant, negative effect on the number of subsequent courses students take in the
subject. The magnitude of the coefficient is not large – taking all of one's introductory courses from an
adjunct reduces future enrollment in those disciplines by about 0.10 credit hours. A negative effect is
also estimated for students who had graduate student instructors. Without course fixed effects, the
magnitude is much larger, but once this type of selection is accounted for, graduate student instructors
have a negative effect that is similar to that of adjuncts.
Table 5b estimates a similar specification focusing on students' choice of major. As before,
adjuncts negatively affect the likelihood that a student declares a major in the given subject. Students
who take adjuncts in their first courses are about 0.4 percentage points less likely to major in a subject
when compared to similar students who had full-time faculty members. Similarly, students who take
courses from graduate students are less likely to major in the subject. These measured effects are
robust to the inclusion of course fixed effects, but the coefficients are not statistically significant from
each other suggesting adjuncts and graduate students have a similar negative effect.
While the results in Tables 5a and 5b strongly suggest that adjunct and graduate assistant
teachers, on average, negatively affect student outcomes, there is substantial heterogeneity across
disciplines. As Table 3 illustrated, disciplines differ in their reliance of alternative instructors.
Moreover, as explained in the theoretical framework, adjuncts' outside experiences (i.e. vocational
knowledge) may have different effects on students in different disciplines. Table 6 disaggregates the
effects of adjuncts and graduate assistants by discipline. As shown, students who have adjunct
instructors in English, foreign languages, chemistry, biology, computer science, management, and
journalism take fewer subsequent credit hours in the subject than similar students who had full-time
faculty members for the same courses. Likewise, students who took courses from graduate students in

19

English, biology, chemistry, physics, management, and education were less likely to take additional
courses in the subject. In contrast, adjuncts had positive effects on student interest in psychology,
physics, and architecture.
While overall we found negative effects for adjuncts on the likelihood that students major in a
subject, when exploring the relationship by discipline, we only find statistically significant effects in
economics, English, philosophy, biology, chemistry, computer science, and journalism. Graduate
students are estimated to have negatively impacted major choice in English, history, philosophy,
foreign languages, and all of the hard science fields. As before, we find positive effects for adjuncts in
physics and psychology.
We can also test whether the estimated effects of adjuncts and graduate students statistically
differ. These results also appear in Table 6. We cannot reject the hypothesis that the estimated effects
are the same for most disciplines for both total subsequent hours attempted and major choice. In
psychology, physics, management, and education adjuncts perform significantly better than graduate
students at encouraging student interest. In journalism and computer science, graduate student
instructors tend to do better than adjuncts at getting students to take additional courses and possibly
majoring.
Adjunct Age and Effectiveness
One potential criticism of our estimation strategy is that we treat adjuncts as a homogeneous
group. Even within disciplines, there is some heterogeneity amongst adjuncts. Some adjuncts might
be young, inexperienced, "all-but-dissertation" (ABD) students. This group makes up the majority of
the recent increase in the use of adjuncts. On the other hand, adjuncts may be seasoned teachers or
experienced professionals who teach for enjoyment rather than primarily for income. While we do not
observe the specific work experience of an adjunct, we do observe their age, a proxy for experience.
In Table 7, we estimate the effect of adjuncts under and over the age of 40.9 In the left column we
show the results for those under the age of 40, a group likely made up of instructors without significant
9

The age cutoff was chosen because by this time most potential instructors should have completed their doctoral work
and/or had multiple years of experience either in business or higher education.

20

experience inside or outside of academe. In the middle column, we estimate the effect of adjuncts who
are over the age of 40 and likely to have some teaching or professional experience. As shown in Table
2b, they are more then twice as likely to have a Ph.D. or other terminal degree. To test whether the
estimates are statistically different from each other, in the right column we report the F-statistic from a
test on the equality of the coefficients in the first two columns.10
As before, we find negative effects in many disciplines; however, in most cases, the negative
effects are much larger for adjuncts under 40. For example, in economics, adjuncts under the age of
40 reduce the number of future hours by about 0.29 and reduce the likelihood of majoring in
economics by about 1.3 percentage points. Older adjuncts also exhibit negative effects, but the
magnitude is smaller and the estimates are statistically insignificant. However, when we perform an Ftest on whether these estimates are the same, we fail to reject the null of equality. While the younger
adjuncts almost always have a more negative effect, only in the case of accounting, management, and
finance are these estimates statistically different from each other. In these business fields, younger
adjuncts reduce both the number of hours and the likelihood of majoring in the subject by significant
amounts, both in magnitude and statistical significance. By contrast, older adjuncts do not show a
statistically significant effect; the point estimates are much smaller; and at least in finance, the
estimates are positive. Given that older adjuncts in business are more likely to bring in outside
knowledge or experience, this last finding suggests that experienced adjuncts are much more effective
than younger adjuncts, particularly in fields where this outside, vocational knowledge is likely to be
very important.
The Timing of Adjunct Effects
One of the key results thus far in the paper is that adjuncts tend to negatively affect student
enrollment patterns. One way we can examine this result is to test how the effects vary over time. If a
student has an adjunct professor during their first semester at school, it is reasonable to assume that the
majority of the effect of that adjunct should take place in the immediate future. We would be less
10

Although the estimates are not shown, the models continue to include graduate students. Because age information
is missing for over half of the adjunct sample, this group’s relative effect is also estimated but not shown.

21

convinced that adjuncts were truly exerting influence if the estimated effect occurred later in a
student's career. To test this idea, we focused only on students who took their first course in a given
subject in Fall of 1998. Using these students' transcripts, we then observed differences in the number
of hours that they took in the same subject over the next seven semesters. For this sample, the overall
effect of adjuncts on total subsequent hours taken was -0.23. The estimate was highly significant. We
then estimate the cumulative effect each semester.
The solid line in Figure 1 plots the proportion of the overall effect that had taken place by a
given semester. The dotted line represents what proportion would have taken place had the effect been
evenly distributed across semesters. The graph illustrates how the timing of the adjunct effect on
course-taking behavior works. For example, the cumulative effect on hours taken after four semesters
is -0.164, so by the end of the second year, 72 percent of the estimated overall effect (.164/.230) had
already occurred. Thirty-two percent of the overall effects of adjuncts had already taken place within
one semester of having the adjunct.
Figure 2 plots the effect of taking any course in a given semester after having an adjunct during
the 1st semester in the same subject. The dotted lines form a 95 percent confidence interval. In the
three semesters after a student takes an adjunct, there is an effect on the likelihood that students take
courses in that subject. The effect is significantly different than zero. After the 5th semester, the
estimated effect of taking any course in that subject is negative but indistinguishable from zero. Like
Figure 1, this graph is suggestive that the effects of adjuncts largely take place during the semesters
immediately after a student takes an adjunct.
The Effect on Performance in Subsequent Courses
Table 8 estimates the effects of having an adjunct on students' passage rates in subsequent
courses. Because we do not observe a student’s performance if he or she never enrolled in a
subsequent course, we use a two-step correction estimator following Heckman (1979). To estimate the
distribution of taking additional hours, we also include controls for whether the course was in each

22

student’s intended major, subject-level ACT scores, and high school grades in math and English. As
with the earlier models, course fixed effects are also included.11
With the exception of four disciplines, we find that adjuncts do not appear to have a significant
effect on the pass rate of students later in their academic careers. However, in computer science,
accounting, management, and architecture, adjunct instructors improve the likelihood students pass
future courses in comparison to full-time faculty members. Additionally, graduate student instructors
are found to increase the likelihood of success in economics and foreign languages.
The Value-Added of Different Instructor Types
Tables 9 and 10 show our results based on instructor fixed effects. In the first stage, we
regress the outcome (e.g. course-taking behavior or choice of major) on student characteristics and
instructor fixed effects. In the second stage, we regress the estimated value-added coefficients of each
instructor on a set of faculty characteristics. Table 9 shows the overall results. The baseline model in
columns 1 and 4 includes controls for gender, race, the subject in which the faculty member taught,
and campus. As shown, both adjunct and graduate student instructors appear to negatively impact
subsequent credit hours taken and the likelihood of choosing to major in the subject. For example,
adjuncts are estimated to reduce the number of subsequent credit hours by 0.49 while graduate student
instructors reduce the amount by about 0.67 hours. The F-stat on the equality of these coefficients is
only 1.17 so we cannot reject the hypothesis that these estimates are similar. When we evaluate the
effects of adjuncts and graduate students on major choice, we also find significant, negative effects. It
does appear, however, that the estimate effect of graduate students is statistically larger in magnitude
than that of adjuncts.
Using these value-added models, we are able to explore some of the reasons why alternative
instructors may have these negative effects in comparison to full-time faculty members. For example,
age, a proxy for experience, may be a factor as shown in Table 7. Therefore, the second models
11

There may be an additional bias in these specifications arising from the fact that some students have taken more
subsequent courses than others. We measure exposure to an adjunct by looking at the first course that a student takes
from a given department. If failing the 2nd course leads to students never taking an additional course, then our
estimates may be biased by "survivors."

23

(columns 2 and 5) control for the age of the instructor when available (we also include a dummy
variable for those whose age information is missing). However, the inclusion of age does not
substantially change the results. The third models for each outcome include the highest degree
completed by the instructor. Dummy variables are used for having a professional, masters, or
bachelor’s degree in comparison to having a Ph.D. Critics have pointed to differences in education as
one reason why alternative instructors may be bad for students. As shown, differences in education
explain much of the adjunct effect but very little of the graduate student effect. Both estimates remain
negative and statistically significant. Similarly, differences in age and education cannot account for
the negative effects of adjuncts and graduate students on major choice.
Tables 10a and 10b show the results when we disaggregate by discipline. Due to the fact that
the sample is much smaller (it is based on the number of instructors rather than students by course), the
estimates are less precise. Table 10a shows the results when we do and do not control for age and
education and examine the impact on subsequent credit hours taken. Similar to the results in Table 6,
adjuncts are found to have had a negative effect in English and computer science. We estimate
negative effects in the business fields, but these estimates are all statistically insignificant. Table 10b
estimates the effects of adjuncts and graduate students on major choice. For adjuncts, we only find
negative effects in English and history. For graduate students, we only find negative effects in
English, psychology, engineering, and journalism. Only in journalism does the estimate effect of
adjuncts seem to differ from that of graduate students.
VI.

CONCLUSIONS
Using a unique dataset with the ability to link student outcomes to faculty characteristics, this

is the first large-scale study on the impact of using alternative instructors in higher education. The
results in this paper suggest that with only a couple of exceptions, taking a course from an adjunct or
graduate student adversely affects the number of credit hours that students subsequently attempt in a

24

subject and the likelihood that a student majors in the subject. However, as we note throughout the
paper, the estimated effects tend to be small in magnitude. For example, suppose that an adjunct and a
full-time faculty member teach identical 50-person introductory courses. If we add up all of the credit
hours of all of the students from each of these sections, then the students of the adjunct are estimated
to take collectively only 5 credit hours fewer than the students from the full-time faculty member's
class. Given that the adjunct may cost only one-fifth the amount of a full-time faculty member, the
change in credit hours may not be considered substantial relative to the possible cost-savings.
Moreover, in most disciplines, neither adjuncts nor graduate assistants are found to negatively impact
future student performance.
The results, however, vary by discipline. We find that adjunct and graduate assistant
instructors had negative effects in the sciences and humanities, particularly in English. It is not
surprising, therefore, that organizations such as the Modern Language Association (MLA) have been
most vocal about the potentially negative effects of adjuncts. Interestingly, we find that young
instructors under the age of 40 explain much of the negative effect of adjuncts. In business-related
fields, we find statistically significant differences between adjuncts under the age of 40 and those over
the age of 40, with the latter group performing much better. Furthermore, adjuncts appear to improve
outcomes for students in more technical or professional fields (i.e. computer science, business, and
architecture) when examining the impact on pass rates in subsequent courses.
Due to the fact that adjuncts and graduate assistants tend to be assigned to students with lower
test scores, we must contend with bias in estimating the effects of adjuncts and graduate assistants on
student behavior. Using course fixed effects, we control for differences in class selection by exploiting
variation across sections within the same course. We also measure the "value-added" of individual
instructors and compare this amount to faculty characteristics. Each of these methods produces
statistically significant, negative effects on enrollments and major choice.
While the results of this paper are necessary to estimate the impact of alternative instructors on
a department, they are not sufficient to provide a full-scale cost-benefit analysis. The results suggest

25

that adjuncts and graduate assistants negatively impact enrollments but not student success in
subsequent courses. Future work is needed on the effects of adjuncts on research and service to fully
understand the tradeoffs between different kinds of instructors. Nonetheless, the paper makes an
important first step in calculating the effect of adjuncts on student course-taking behavior.

REFERENCES
Autor, David (2000) "Why do Temporary Help Firms Provide Free General Skills Training?" NBER
Working Paper Number 7637.
Balch, Pamela (1999) “Part-time faculty are here to stay.” Planning for Higher Education 27, 3, 32-41.
Baldwin, Roger G. Chronister, Jay L. (1996) “Full-Time Non-Tenure-Track Faculty.” NEA National
Education Association Higher Education Research Center Update, vol. 2, no. 5 (September).
Benjamin, Ernst (1998) “Variations in the Characteristics of Part-Time Faculty by General Fields of
Instruction and Research.” New Directions for Higher Education, no. 104.
Borjas, George (2000) “Foreign-Born Teaching Assistants and the Academic Performance of
Undergraduates.” NBER Working Paper No. 7635, April.
Brewster, David (2000) “The Use of Part-Time Faculty in the Community College.” Inquiry, vol. 5,
no. 1 (Spring), pp. 66-76.
Burgan, M., Weisbuch, R., and Lowry, S. (1999) “A profession in difficult times: The future of
faculty.” Liberal Education, vol. 85, no. 4, pp. 6-15.
Card, David and Alan Krueger (1998) “School Resources and Student Outcomes.” Annals of the
American Academy of Political & Social Science, vol. 559, no. 0, pp. 39-53.
Chen, Xianglei (2002) Teaching Undergraduates in U.S. Postsecondary Institutions: Fall 1998.
Washington: National Center for Education Statistics.
Chronister, Jay L. (1991) “Full-Time Non-Tenure-Track Faculty: Current Status, Future Prospects,
Remaining Research Questions.” ASHE Annual Meeting Paper, November.

26

Chronister, Jay L. Gansneder, Bruce M. Harper, Elizabeth. Baldwin, Roger G. (1997) “Full-Time NonTenure-Track Faculty: Gender Differences.” National Education Association Higher
Education Research Center Update, vol. 3, no. 5 (November).
College and University Professional Association for Human Resources (2001) National Faculty Salary
Survey. Executive Summary available at http://www.cupahr.org/cbsurvey/00-01ExecSum/0001NFSSExSum-Public.pdf.
Ehrenberg, Ronald G. (2000) Tuition Rising: Why Colleges cost so much. Cambridge: Harvard
University Press.
Ehrenberg, R. and D. Brewer (1994) “Do School and Teacher Characteristics Matter? Evidence from
the High School and Beyond.” Economics of Education Review, vol. 13, no. 1, pp. 1-17.
Figlio, David and Kim Rueben (2001) “Tax Limits and the Qualifications of New Teachers.” Journal
of Public Economics. vol. 80, no. 1, p 49-71.
Fulton, Richard (2000) “The Plight of Part-Timers in Higher Education.” Change, 32, 3, pp.38-43.
Gappa, Judith M. (2000) “The new faculty majority: Somewhat satisfied but not eligible for tenure.”
New Directions for Institutional Research, vol. 27, no. 1, pp. 77-86.
Haeger, John D. (1998) “Part-Time Faculty, Quality Programs, and Economic Realities.” New
Directions for Higher Education, vol. 26, no. 4 (Winter), pp. 81-88.
Hanushek, Eric and Steven G. Rivkin (2003) “Does Public School Competition Affect Teacher
Quality?” In Caroline M. Hoxby, ed. The Economics of School Choice. Chicago: The
University of Chicago Press.
Head, Ronald B. (2002) “The Role of Adjunct Faculty in the Community College.” Inquiry, vol. 7,
no. 1 (Spring), pp. 36-37.
Heckman, J. (1979) "Sample Selection Bias as a Specification Error." Econometrica 47:153-161.
Hoxby, Caroline (2002) “Would School Choice Change the Teaching Profession?” National Bureau of
Economic Research Working Paper No. 7866.

27

Jackson, Maureen (1999) Study of the employment status of faculty at Maryland public campuses.
Annapolis, MD: Maryland Higher Education Commission.
Jacobs, Lucy Cheser and Charles B. Friedman (1998) “Student Achievement Under Foreign Teaching
Associates Compared with Native Teaching Associates.” Journal of Higher Education, vol.
59, no. 5 (Sep/Oct), pp. 551-563.
Knotts, H. Gibbs and Eleanor C. Main (1999) "Teaching Ph.D. Students to Teach Political Science:
The Emory TATTO Program." PS: Political Science and Politics. Vol. 32, no. 3, pp. 607-10.
Koehnecke, Dianne Swenson (1991) "Boudnaries of Graduate Assistants: Bouncing off Boundaries."
Paper presented at the Annual Meeting of the Conference on College Composition and
Communication, Boston, MA, Mar 21-23.
Leslie, David W. (1998a) The Growing Use of Part-Time Faculty: Understanding Causes and Effects.
New Directions for Higher Education, no. 104.
Leslie, David W. (1998b) “Part-Time, Adjunct, and Temporary Faculty: The New Majority?” Report
of the Sloan Conference on Part-Time and Adjunct Faculty.
Leslie, David W. and Judith M. Gappa (1995) “The Part-Time Faculty Advantage.” Metropolitan
Universities: An International Forum, vol. 6, no.2, pp. 91-102.
Mattson, Kevin. (2000). "The Academic Labor Movement: Understanding its Origin and Current
Challenges. Social Policy. Vol 30, no. 4, pp. 4-10.
McLeod, Susan H. and Fred S. Schwarzbach (1993). "What about the TAs? Making the Wyoming
Resolution a Reality for Graduate Students." WPA: Writing Program Administration. Vol. 17.
no. 1-2, pp. 83-87.
Meyers, Steven A. and Loreto Prieto R. (2000) "Training in the Teaching of Psychology: What is
Done and Examining the Differences." Teaching of Psychology. Vol 27, no. 4, pp. 258-61.
Modern Language Association (2002). Ensuring the Quality of Undergraduate Programs in English
and Foreign Languages: MLA Recommendation on Staffing. Downloaded from
http://www.mla.org on June 12, 2003.

28

Murnane, Richard, Judith D. Singer, John B. Willett, James Kemple, and Randall Olsen (1991) Who
will teach? Policies that matter. Cambridge, MA: Harvard University Press.
National Center for Education Statistics (1997) Instructional Faculty and Staff in Higher Education
Institutions: Fall 1987 and Fall 1992. Working Paper No. 97-470.
----- (2000) 1999-2000 National Postsecondary Student Aid Study.
----- (2001) Institutional Policies and Practices: Results from the 1999 National Study of
Postsecondary Faculty, Institutional Survey. Working Paper No. 2001-201.
----- (2002) A Profile of Part-time Faculty: Fall 1998. Publication No. 2002-08.
Norris, Timothy (1991) “Nonnative English-speaking Teaching Assistants and Student Performance.”
Research in Higher Education, vol. 32, no. 4, 1991.
Palmer, James (1998). "Enhancing Faculty Productivity: A State Perspective." Education Commission
of the States Policy Paper.
Pisani, Anoush M. and Stott, Nathan (1998) “An investigation of part-time faculty commitment to
developmental advising.” Research in Higher Education, vol. 39, no. 2, pp. 121-42.
Prieto, Loreto R. and Elizabeth M. Altmaier (1994). "The Relationship of Prior Training and Previous
Teaching Experience to Self-Efficacy among Graduate Teaching Assistants." Research in
Higher Education. Vol. 35, no. 4, pp. 481-97.
Sharnoff, Elena (1993). "Neither Fish nor Fowl: Graduate Students, Unionization, and the Academy"
Talk presented at the Annual Meeting of the Modern Language Association, Toronto, Canada,
Dec 1993.
Slevin, James F. (1992) The Next Generation: Preparing Graduate Students for the Professional
Responsibilities of College Teachers. Association of American Colleges, Washington, D. C.
Smith, Kathleen S. (2001) "Pivotal Events in Graduate Teacher Preparation for a Faculty Career."
Journal of Graduate Teaching Assistant Development, vol. 8, no. 3, pp. 97-105.
Taylor, Gayle (2002) “Adjuncts: Fill-ins or Replacements?” Inquiry, vol. 7, no. 1, pp. 42-43.

29

Tompkins, Patrick (2002) “What We Talk About When We Talk About Faculty.” Inquiry, vol. 7, no.
1, pp. 44-46.
Temin, Peter (2002) “Teacher Quality and the Future of America.” Eastern Economic Journal, vol. 28,
no. 3. p 285-300.
Vaughn, Willliam (1998) "Apprentice or Employee? Graduate Students and Their Unions." Academe.
Vol. 84, no. 6, pp.43-49.
Watkins, Thomas G. (1995) “Instructional Costs at Master’s Institutions.” Journal of Economics, vol.
21 no. 2, pp. 71-76.

30

Table 1: Full-time, Traditional-aged Students at Four-year, Public Colleges in Ohio
All Students
Students with ACT Information
Background Characteristics
18.40
18.39
Age in 1998
(.546)
(.524)
Female
.539
.551
Black

.099

.090

Hispanic

.017

.015

Asian

.022

.021

White

.822

.843

Ohio Resident
Postsecondary Outcomes
In Remediation

.886

.999

.178

.178

In Remedial English

.091

.089

Total Credit Hours
(Fall98 – Spring02)

98.08
(40.08)

College GPA

2.70
(.880)
[25,320]

100.53
(38.36)
2.73
(.863)
[20,213]

Dropped Out by
Spring 2002

.307
(.461)
[32,216]

2.73
(.863)
[20,213]

Completed a 4yr Degree

.233

.232

Transferred Down

.063

.068

.800

1.00

ACT Score (36 max)

22.19
(4.26)
[25,762]

22.19
(4.26)

High School GPA

3.19
(.540)
[24,634]

3.19
(.540)
[24,634]

32,222

25,762

Ability and High School Measures
Took the ACT

Observations

Standard deviations are shown in the parentheses. The number of observations for variables with less than the total
observations is shown in brackets. Sample is restricted to full-time individuals age 18 to 20 who were first-time
students in Fall 1998.

31

Table 2a: All Faculty in Courses with over 20 Students
Raw Means

Weighted
by Enrollment

% with a Ph.D.

55.35

51.34

% Part-time Instructors

23.16

25.63

% Non-Tenure-Track Instructors

40.33

50.50

% Graduate Assistants

6.95

12.75

% Female

40.01
[5,401]

46.96
[5,401]

% Minority

22.21
[5,434]

18.46
[5,434]

Year Born

1955
(13.12)
[2,427]

1954
(14.43)
[2,427]

7,128

7,128

Observations

Notes: Restricted to active faculty teaching between 1998-2002 at the undergraduate level regardless of enrollment.

Table 2b: All Faculty Teaching a First Course in a Subject
Adjunct Instructors (Part-time)
Under
Over
No Age
Age 40
Age 40
Information

Full-time
Instructors

All

% with a Ph.D.

89.42

24.19

15.21

34.71

21.94

1.16

% Female

26.10

46.35

50.45

41.48

47.77

50.97

% Minority

19.36

19.43

21.03

14.67

21.94

37.88

Year Born

1950
(10.55)
[2,272]

1953
(12.85)
[1,340]

1966
(4.82)

1946
(9.57)

4,242

3,195

447

893

Observations

Graduate
Assistants

1970
(9.24)
[1,293]
1,855

3,712

Notes: Restricted to active faculty teaching between 1998-2002 at the undergraduate level regardless of enrollment.

32

Table 3: The Use of Adjuncts by Institution and Subject
Percent of Instructors that
are Adjuncts (part-time)
By University Campus
Selective Campuses
22.20
Non-selective Campuses

Percent of Instructors that
are Graduate Students
35.40

35.04

16.35

56.25

35.78

Mathematics and Statistics

38.64

28.99

Journalism and Communication

36.51

15.31

Computer Science

33.72

35.51

Humanities

32.27

29.46

Business

31.52

7.07

Education

23.44

36.92

Foreign Languages

22.18

8.13

Social Sciences

21.77

2.44

Engineering

18.75

7.45

Sciences

10.37

5.32

By School/Department
Architecture

Notes: Restricted to active faculty teaching between 1998 and 2002 at the undergraduate level regardless of
enrollment. Sample restricted to faculty teaching "first courses" in a subject. Selective institutions are defined as
having competitive, non-open admissions (Bowling Green State University, University of Cincinnati, Kent State
University, Miami University, Ohio State University, and Ohio University).

33

Table 4: Predictors of having an Alternative Instructor in 1st Semester of Subject
Dependent Variable: Proportion PT in 1st semester of Exposure to Subject
A. ADJUNCT INSTRUCTORS
All Subjects
Models with Course Fixed Effects
No Course
Fixed Effects

All
Subjects

Humanities

Social
Sciences

Sciences

Business

ACT Score

-.0017**
(.0002)

-.0006**
(.0002)

-.0016**
(.0005)

.0028**
(.0004)

.0008**
(.0004)

-.0010
(.0007)

In Pre-College Major

-.0325*
(.0238)

-.0213
(.0194)

.0770
(.1406)

-.0972
(.0604)

-.0189
(.0767)

-.0863*
(.0436)

(In Pre-College Major)
*ACT

.0005
(.0010)

.0003
(.0008)

-.0038
(.0057)

.0047*
(.0027)

.0011
(.0033)

.0031
(.0020)

235,143

235,143

47,697

65,224

43,226

21,153

25,255
.0922

25,255
.3383

24,222
.2603

23,668
.2823

21,522
.2883

8,865
.4400

Observations
(stud x subject)
Number of Students
R-Squared

B. GRADUATE ASSISTANT INSTRUCTORS
Models with Course Fixed Effects

All Subjects
No Course
Fixed Effects

All
Subjects

Humanities

Social
Sciences

Sciences

Business

-.0014**
(.0002)

-.0012**
(.0002)

-.0032**
(.0005)

-.0011**
(.0003)

-.0006**
(.0003)

-.0006*
(.0004)

In Pre-College Major

.0123
(.0175)

-.0115
(.0156)

.0548
(.1329)

.0230
(.0498)

.0311
(.0624)

.0039
(.0254)

(In Pre-College Major)
*ACT

-.0009
(.0008)

.0005
(.0007)

-.0026
(.0056)

-.0021
(.0023)

-.0012
(.0026)

-.0002
(.0011)

235,143

235,143

47,697

65,224

43,226

21,153

25,255
.1564

25,255
.4175

24,222
.3377

23,668
.3960

21,522
.2407

8,865
.2970

ACT Score

Observations
(stud x subject)
Number of Students
R-Squared

** Significant at the 5% level
* Significant at the 10% level
Notes: Full-time, traditional-age, first-time students who began at an Ohio university main campus during Fall 1998.
Standard errors are shown in parentheses and correct for correlation within observations of the same student.
Regressions include controls for gender, race, state of residence, and total credits taken in the semester. All models have
fixed effects for campus, department subject, and term. The departments not listed separately but included in “All
Subjects” are Computer Science, Communications, Math, Engineering, Architecture, Education, and Social Work.

34

Table 5a: Instructor Type on Subsequent Credit Hours Taken in Subject – All Subjects
Proportion Adjunct in 1st
Semester

Total Credit Hours Taken in Subject after 1st Exposure to Subject
(1)
(2)
(3)
(4)
-.0105
.0268
-.1030**
-.0857**
(.0393)
(.0384)
(.0408)
(.0404)

Proportion Grad Stud in
1st Semester

-.3658**
(.0431)

-.3342**
(.0425)

-.1618**
(.0487)

-.1552**
(.0484)

ACT Score

.1237**
(.0042)

.0980**
(.0040)

.0794**
(.0041)

.0622**
(.0039)

Dependent Variable

In Pre-College Major
(In Pre-College Major)
*ACT
Course Fixed Effects
N (students X subjects)
N (students)
R-Squared
F-Test: Adjunct =
Graduate Asst. (Prob>F)
** Significant at the 5% level
See the notes to the next table.

No
235,143
25,255
.1853
53.19
(.0000)

-5.8989**
(1.0250)

-6.2703**
(.9729)

.5987**
(.0452)

.4970**
(.0429)

No
Yes
235,143
235,143
25,255
25,255
.2149
.3343
56.58
1.26
(.0000)
(.2614)
* Significant at the 10% level

Yes
235,143
25,255
.3463
1.79
(.1814)

Table 5b: Instructor Type on the Likelihood of Majoring in a Subject – All Subjects
Dependent Variable

Chose to Major in the Subject
(2)
(3)
-.0036**
-.0038**
(.0011)
(.0012)

Proportion Adjunct in 1st
Semester

(1)
-.0048**
(.0011)

(4)
-.0032**
(.0012)

Proportion Grad Stud in
1st Semester

-.0071**
(.0012)

-.0061**
(.0012)

-.0058**
(.0015)

-.0055**
(.0015)

ACT Score

.0019**
(.0001)

.0014**
(.0001)

.0011**
(.0001)

.0009**
(.0001)

In Pre-College Major

.0566*
(.0294)

.0350
(.0293)

(In Pre-College Major)
*ACT

.0084**
(.0013)

.0060**
(.0013)

Course Fixed Effects
No
No
Yes
Yes
N (students X subjects)
235,143
235,143
235,143
235,143
N (students)
25,255
25,255
25,255
25,255
R-squared
.1646
.1985
.2866
.3010
F-Test: Adjunct =
2.91
3.67
1.77
2.47
Graduate Asst. (Prob>F)
(.0883)
(.0554)
(.1838)
(.1162)
** Significant at the 5% level
* Significant at the 10% level
Standard errors are shown in parentheses except in the last row in which they show p-values. Standard errors
correct for correlation within observations of the same student. Regressions include controls for gender, race, state
of residence, and total credits taken in the semester. Models also include fixed effects for campus, department, and
term.

35

Table 6: The Effects of Instructor Type by Subject
Dependent Var:

All Subjects

Credit Hours Taken after 1st Exposure
Graduate
F-Test
Adjuncts
Assistants
(Prob>F)
-.0857**
(.0404)

-.1552**
(.0484)

1.79
(.1814)

Chose to Major in the Subject
Graduate
F-Test
Adjuncts
Assistants
(Prob>F)
-.0032**
(.0012)

-.0055**
(.0015)

2.47
(.1162)

Observations
235,143
(25,255
studs)

Social Sciences and Humanities
-.0658
-.1598
.61
-.0056**
.0008
1.80
11,747
Economics
(.0763)
(.1171)
(.4339)
(.0023)
(.0048)
(.1802)
Political
.0300
.0007
.01
.0025
-.0032
.18
7,438
Science
(.2483)
(.2483)
(.9529)
(.0107)
(.0105)
(.6747)
.3705*
-.0318
3.40
.0133**
-.0040
5.76
17,274
Psychology
(.1979)
(.1280)
(.0654)
(.0061)
(.0045)
(.0164)
-.0364
-.0754
.08
.0028
.0016
.12
Sociology
14,418
(.0968)
(.1387)
(.7784)
(.0026)
(.0035)
(.7237)
-.3104**
-.3784**
.33
-.0119**
-.0126**
.05
English
22,482
(.1583)
(.1675)
(.5641)
(.0041)
(.0043)
(.8218)
-.1589
-.2025
.12
.0009
-.0078*
4.65
14,162
History
(.1175)
(.1393)
(.7330)
(.0036)
(.0041)
(.0310)
Philosophy &
-.0387
-.0268
.01
-.0035*
-.0063**
1.21
11,053
Religion
(.0969)
(.0010)
(.9227)
(.0018)
(.0023)
(.2709)
Foreign
-.4505**
-.0225
3.65
-.0019
-.0071**
2.59
11,106
Languages
(.2032)
(.2063)
(.0562)
(.0032)
(.0030)
(.1077)
Sciences and Technical Fields
-.8379**
-.4541**
1.25
-.0147*
-.0223**
.42
Biology
13,281
(.2938)
(.2116)
(.2638)
(.0096)
(.0078)
(.5147)
-.3661**
-.7736**
2.08
-.0072**
.0027
2.18
Chemistry
8,759
(.1786)
(.2649)
(.1495)
(.0034)
(.0065)
(.1399)
.3095**
-.3090*
9.07
.0116**
-.0053**
8.42
Physics
7,775
(.1434)
(.1567)
(.0026)
(.0047)
(.0025)
(.0037)
Computer
-.5010**
-.1181
3.41
-.0226**
.0007
4.41
7,452
Science
(.2261)
(.2481)
(.0657)
(.0082)
(.0082)
(.0357)
-.4800
-.7734
.03
-.0471
-.0970**
.85
Engineering
3,614
(1.2354)
(1.261)
(.8607)
(.0408)
(.0402)
(.3556)
Business and Professional Fields
.0348
.7440
.90
-.0108
.0278
1.19
5,544
Accounting
(.2333)
(.7335)
(.3416)
(.0125)
(.0348)
(.2745)
-.4446**
-.9660**
7.56
-.0120
.0525*
5.15
5,859
Management
(.2081)
(.1989)
(.0060)
(.0091)
(.0276)
(.0232)
-.2738
-.2189
.01
-.0051
.0150
.22
3,021
Finance
(.2493)
(.4919)
(.9055)
(.0157)
(.0439)
(.6410)
-.1589
-1.194**
3.40
.0195
-.0160
6.73
7,978
Education
(.5544)
(.5707)
(.0651)
(.0139)
(.0147)
(.0095)
3.7725**
.5783
2.02
.0147
.0197
.01
1,233
Architecture
(1.6590)
(1.423)
(.1560)
(.0418)
(.0339)
(.9297)
-2.2492**
.0126
9.73
-.0933**
-.0059
9.35
Journalism
5,312
(.3915)
(.6477)
(.0018)
(.0139)
(.0274)
(.0022)
** Significant at the 5% level
* Significant at the 10% level
Notes: Standard errors are shown in parentheses except in the F-test columns in which they show p-values. Standard
errors correct for correlation within observations of the same student. Regressions include controls for gender, race,
state of residence, and total credits taken in the semester. Models also include fixed effects for campus, term, and
course.

36

Table 7: Adjunct Instructors by Age on Subsequent Credit Hours Taken in a Subject
Chose to Major in the Subject
Credit Hours Taken after 1st Exposure
Observations
Adjuncts
Adjuncts
F-Test
Adjuncts
Adjuncts 40
F-Test
under 40
40 & Over (Prob>F)
under 40
& Over
(Prob>F)
-.0063**
-.0047**
0.38
235,143
-.3534**
-.1589**
5.22
All Subjects
(.0024)
(.0016)
(.5393)
(25,255 studs)
(.0755)
(.0531)
(.0223)
Social Sciences and Humanities
-.0127**
-.0035
2.68
-.2856*
-.0714
1.57
11,747
Economics
(.0053)
(.0028)
(.1017)
(.1574)
(.0836)
(.2102)
-.0419**
.0458**
13.01
Political
-.4097
.3963
2.64
7,438
(.0153)
(.0198)
(.0003)
Science
(.3239)
(.4161)
(.1042)
.2525
-.3099
2.63
.0026
-.0043
0.43
Psychology
17,274
(.2729)
(.2477)
(.1051)
(.0081)
(.0077)
(.5111)
.3289
-.1831*
4.81
.0151**
-.0005
4.80
Sociology
14,418
(.2302)
(.1051)
(.0283)
(.0071)
(.0027)
(.0285)
-.7175**
-.4427**
0.81
-.0164**
-.0206**
0.28
English
22,482
(.2971)
(.2172)
(.3683)
(.0075)
(.0056)
(.5974)
-.0047
.0064
2.44
-.1951
-.0238
0.57
14,162
History
(.0057)
(.0053)
(.1181)
(.1725)
(.1763)
(.4517)
Philosophy &
-.0381
.0937
0.31
-.0058**
-.0023
2.88
11,053
Religion
(.1912)
(.1295)
(.5799)
(.0017)
(.0022)
(.0895)
-.0057
.0014
1.01
Foreign
-.7310*
-.3803
0.56
11,106
(.0058)
(.0055)
(.3161)
Languages
(.3730)
(.3647)
(.4562)
Sciences and Technical Fields
-.0155*
-.0119
0.06
-1.022**
-.6738*
0.80
13,281
Biology
(.0106)
(.0118)
(.8042)
(.2573)
(.3583)
(.3707)
-.0047
-.0076**
0.22
-.8604*
-.2754*
1.45
8,759
Chemistry
(.0056)
(.0037)
(.6410)
(.4642)
(.1846)
(.2284)
.0012
.0111**
8.20
-.2930
.3229**
6.91
7,775
Physics
(.0023)
(.0049)
(.0042)
(.2367)
(.1505)
(.0086)
Computer
-.3953
-.6313**
1.28
-.0187*
-.0188*
0.00
7,452
Science
(.2683)
(.2822)
(.2577)
(.0103)
(.0108)
(.9932)
.4556
-2.7557
0.91
-.0328
-.0414
0.01
Engineering
3,614
(1.1421)
(3.2585)
(.3406)
(.0483)
(.0842)
(.9269)
Business and Professional Fields
-.0225
-.0225
0.00
-.9522**
.0122
8.56
5,544
Accounting
(.0246)
(.0141)
(.9990)
(.3424)
(.2157)
(.0034)
-.0803**
-.0045
13.67
-2.1037**
-.2417
12.96
5,859
Management
(.0178)
(.0111)
(.0002)
(.4809)
(.2568)
(.0003)
-.0489**
.0269
8.16
-.7908**
.5420
11.47
3,021
Finance
(.0193)
(.0246)
(.0043)
(.2700)
(.4057)
(.0007)
.0227
.0049
0.52
.1668
-1.0589
1.45
7,978
Education
(.0233)
(.0193)
(.4703)
(.9770)
(.8076)
(.2290)
6.2140*
.5210
2.26
.1068
-.0898
4.46
Architecture
1,233
(3.5312)
(2.4520)
(.1331)
(.0879)
(.0611)
(.0349)
-.0233
-.1040**
2.88
-2.6787**
-2.1784**
0.15
5,312
Journalism
(.0452)
(.0170)
(.0897)
(1.2162)
(.4965)
(.6997)
** Significant at the 5% level
* Significant at the 10% level
Notes: Standard errors are shown in parentheses except in the F-test columns in which they show p-values. Standard
errors correct for correlation within observations of the same student. Regressions include controls for gender, race,
state of residence, and total credits taken in the semester. Models also include fixed effects for courses, campus, and
term.
Dependent Var:

37

Table 8: Heckman Model Estimates on the Student Completion Rate of Future Courses
Adjuncts

Graduate
Assistants

F-Test
(Prob>F)

N
(truncated)

N
(students)

Social Sciences and Humanities
-.0199
.0675**
15.29
4,920
11,258
Economics
(.0119)
(.0219)
(.00)
Political
-.0172
-.0188
.00
4,990
7,107
Science
(.0274)
(.0210)
(.96)
-.0109
-.0155
.08
Psychology
9,325
16,450
(.0128)
(.0118)
(.78)
-.0055
.0063
.61
9,570
13,745
Sociology
(.0117)
(.0151)
(.44)
-.0222
-.0213
.00
English
4,397
21,416
(.0192)
(.0198)
(.96)
-.0066
-.0042
.04
History
5,585
13,487
(.0105)
(.0118)
(.84)
Philosophy &
.0018
-.0082
.12
7,365
10,562
Religion
(.0221)
(.0226)
(.73)
Foreign
.0080
.0208**
1.33
3,697
10,638
Languages
(.0097)
(.0105)
(.25)
Sciences and Technical Fields
.0229
-.0102
1.24
6,819
12,704
Biology
(.0232)
(.0190)
(.27)
.0191
-.0131
.55
Chemistry
3,336
8,443
(.0244)
(.0426)
(.46)
-.0240
-.0108
.24
Physics
3,814
7,466
(.0185)
(.0208)
(.62)
.0436**
.0207
.72
Computer
4,419
7,153
(.0194)
(.0289)
(.40)
Science
-.0495
.0363
3.52
Engineering
1,094
3,477
(.0350)
(.0304)
(.06)
Business and Professional Fields
.0315*
.0003
0.47
1,552
5,335
Accounting
(.0173)
(.0471)
(.49)
.0245**
.0288
0.22
Management
2,422
5,609
(.0095)
(.0430)
(.64)
.0029
-.0009
0.62
Finance
1,373
2,903
(.0123)
(.0309)
(.43)
.0094
-.0053
1.06
3,834
7,616
Education
(.0135)
(.0139)
(.30)
.0978*
.2453
.37
Architecture
723
1,182
(.0501)
(.2466)
(.54)
-.0148
-.0198
.05
Journalism
2,814
5,067
(.0143)
(.0201)
(.82)
** Significant at the 5% level
* Significant at the 10% level
Notes: Standard errors are shown in parentheses except in the F-test column in which they show p-values. Standard
errors correct for correlation within observations of the same student. Regressions include controls for gender, race,
state of residence, and total credits taken in the semester. Models also include fixed effects for campus, term, and
course.

38

Table 9: Impact of Instructors using Faculty Fixed Effects
Dependent Variable

Credit Hours Taken after 1st
Exposure
Baseline
Adding
Adding
Model
Age
Education

Chose to Major in
the Subject
Baseline
Adding
Adding
Model
Age
Education

(1)

(2)

(3)

(4)

(5)

(6)

Adjunct Instructors
(part-time)

-.4929**
(.1451)

-.4897**
(.1452)

-.3924**
(.1566)

-.0167**
(.0044)

-.0169**
(.0044)

-.0135**
(.0047)

Graduate Assistant
Instructors

-.6711**
(.1505)

-.6919**
(.1518)

-.6076**
(.1859)

-.0278**
(.0045)

-.0291**
(.0046)

-.0231**
(.0056)

-.0369**
(.0187)

-.0377**
(.0187)

-.0013**
(.0006)

-.0013**
(.0006)

Age
Bachelor’s Degree
(relative to Ph.D.)

-.1139
(.1954)

-.0106*
(.0059)

Master’s Degree
(relative to Ph.D.)

-.2852*
(.1535)

-.0069
(.0046)

Professional Degree
(relative to Ph.D.)

-.4977
(.5428)

-.0311*
(.0164)

Observations
11281
11281
11281
11281
11281
11281
R-Squared
.0795
.0802
.0806
.6411
.6413
.6415
F-Test: Adjunct =
1.17
1.49
1.45
4.99
4.99
5.96
Graduate Asst. (Prob>F) (.2791)
(.2219)
(.2291)
(.0256)
(.0256)
(.0147)
** Significant at the 5% level
* Significant at the 10% level
Notes: Standard errors are shown in parentheses except in the last row in which they show p-values. Standard errors
correct for correlation within observations of the same student. Each model includes controls for subject, campus,
gender, and race. When age is included, we also control for observations missing that information.

39

Table 10a: Impact of Instructors by Discipline using Faculty Fixed Effects
Dependent Variable: Credit Hours Taken after 1st Exposure
Baseline Model
Graduate
Adjuncts
Assistants
Social Sciences and Humanities
.2581
.0977
Economics
(.8458)
(1.0424)

F-Test
(Prob>F)

Controlling for Age and Education
Graduate
F-Test Observations
Adjuncts
Assistants (Prob>F)

0.02
(.8886)

.0743
(.9292)

-.1122
(1.1837)

0.02
(.8768)

303

Political Science

.6981
(.6083)

-.3495
(.5686)

2.09
(.1491)

.2999
(.6428)

-1.6629*
(.8462)

5.20
(.0232)

351

Psychology

-1.1935
(.7647)

-2.3946**
(.7581)

1.93
(.1655)

-.9835
(.8045)

-1.6968*
(1.0029)

0.54
(.4633)

493

Sociology

-.1735
(.5112)

-.1244
(.6112)

0.01
(.9376)

.0837
(.5706)

.2235
(.8547)

0.03
(.8529)

391

English

-.8537**
(.3917)

-1.0645**
(.4054)

0.28
(.5949)

-.7500*
(.4214)

-.6542
(.5028)

0.05
(.8300)

1424

History

-.5669
(.3518)

.2485
(.2929)

4.57
(.0330)

-.8209**
(.3788)

-.1626
(.4298)

2.37
(.1241)

635

Philosophy &
Religion

-.2148
(.3839)

.6882
(.4284)

3.38
(.0666)

-.3648
(.4484)

.5056
(.5311)

2.69
(.1015)

426

Foreign
.4294
Languages
(.4268)
Sciences and Technical Fields
.3421
Biology
(.8321)

.0263
(.4003)

0.99
(.3205)

.2987
(.4727)

-.2627
(.4880)

1.66
(.1985)

1132

-.7501
(.7018)

1.27
(.2600)

.9731
(.9114)

-.4610
(1.0556)

1.65
(.1994)

484

Chemistry

.7564
(.7019)

.5291
(.5947)

0.09
(.7644)

.9245
(.7352)

.5585
(.8454)

0.17
(.6801)

378

Physics

.2272
(.8400)

-.4009
(.7577)

0.50
(.4821)

-.1098
(.9340)

-.7602
(.9838)

0.45
(.5008)

309

-1.9525*
(1.0533)

-.3820
(1.1449)

1.94
(.1644)

-1.2151
(1.1420)

.1204
(1.2899)

1.34
(.2470)

380

-2.7359
(2.1768)

0.00
(.9518)

-2.2787
(1.7233)

-2.7527
(2.4103)

0.03
(.8565)

281

-.3532
(1.5973)

0.00
(.9667)

-.3059
(.9798)

-.8187
(1.8017)

0.09
(.7687)

175

Computer
Science

-2.5830
(1.6754)
Business and Professional Fields
-.4204
Accounting
(.8761)
Engineering

Management

-.3798
(.6866)

-.9246
(1.6222)

0.10
(.7492)

-.1345
(.7441)

-.6366
(1.6789)

0.08
(.7753)

234

Finance

.8957
(.9234)

1.8160
(1.6981)

0.28
(.5949)

.8113
(1.0842)

1.7266
(1.9678)

0.21
(.6440)

125

Education

-.7825
(.8647)

-1.6183
(.9847)

0.63
(.4286)

-.2489
(.9292)

-.8233
(1.1295)

0.27
(.6019)

829

Architecture

4.1705
(3.5286)

-1.2357
(5.8625)

0.77
(.3829)

4.1322
(3.7590)

-1.5616
(6.2280)

0.79
(.3761)

81

Journalism

.1489
(.9722)

-4.4486**
(1.4018)

9.47
(.0023)

.4548
(1.0752)

-3.7051**
(1.5231)

7.41
(.0069)

325

** Significant at the 5% level
* Significant at the 10% level
Notes: Standard errors are shown in parentheses except in the F-test column in which they show p-values. Standard
errors correct for correlation within observations of the same student. Each model includes controls for subject,
campus, gender, and race. Additionally, there are controls for age and the highest educational degree completed for
the right panel.

40

Table 10b: Impact of Instructors by Discipline using Faculty Fixed Effects
Dependent Variable: Chose to Major in the Subject
Baseline Model
Graduate
Adjuncts
Assistants
Social Sciences and Humanities
-.0136
-.0026
Economics
(.0251)
(.0307)

F-Test
(Prob>F)

Controlling for Age and Education
Graduate
F-Test Observations
Adjuncts
Assistants (Prob>F)

0.11
(.7427)

-.0140
(.0273)

.0030
(.0348)

0.23
(.6286)

303

Political Science

.0119
(.0230)

.0022
(.0218)

0.13
(.7232)

.0137
(.0244)

.0274
(.0320)

0.18
(.6695)

351

Psychology

-.0131
(.0238)

-.0786**
(.0222)

5.95
(.0151)

-.0096
(.0249)

-.0727**
(.0295)

4.44
(.0355)

493

Sociology

.0045
(.0151)

.0093
(.0181)

0.07
(.7935)

.0117
(.0168)

.0133
(.0249)

0.01
(.9404)

391

English

-.0494**
(.0120)

-.0580**
(.0124)

0.50
(.4781)

-.0480**
(.0130)

-.0496**
(.0155)

0.01
(.9123

1424

History

-.0261*
(.0134)

-.0229**
(.0113)

0.05
(.8255)

-.0329**
(.0146)

-.0249
(.0171)

0.23
(.6286)

635

Philosophy &
Religion

-.0025
(.0132)

.0104
(.0147)

0.57
(.4493)

.0004
(.0154)

.0056
(.0184)

0.08
(.7763)

426

Foreign
.0067
Languages
(.0113)
Sciences and Technical Fields
.0040
Biology
(.0241)

-.0108
(.0105)

2.65
(.1040)

-.0002
(.0125)

-.0173
(.0127)

2.22
(.1362)

1132

-.0335
(.0206)

1.79
(.1818)

.0148
(.0260)

.0089
(.0313)

0.03
(.8568)

484

Chemistry

.0126
(.0161)

.0047
(.0140)

0.21
(.6495)

.0121
(.0167)

.0087
(.0192)

0.03
(.8637)

378

Physics

.0192
(.0249)

-.0087
(.0224)

1.12
(.2914)

.0207
(.0272)

-.0090
(.0291)

1.05
(.3061)

309

Computer
Science

-.0098
(.0289)

-.0656**
(.0310)

3.27
(.0714)

.0116
(.0313)

-.0443
(.0350)

3.19
(.0751)

380

-.1281*
(.0651)

0.54
(.4614)

-.0730
(.0542)

-.1528**
(.0715)

1.02
(.3137)

281

-.0074
(.0661)

0.12
(.7264)

-.0136
(.0397)

-.0090
(.0739)

0.00
(.9483)

175

-.0712
(.0526)
Business and Professional Fields
-.0307
Accounting
(.0359)
Engineering

Management

-.0351
(.0293)

.0260
(.0692)

0.71
(.4019)

-.0219
(.0315)

.0174
(.0713)

0.28
(.5998)

234

Finance

.0763
(.0470)

.0572
(.0880)

0.05
(.8286)

.0746
(.0550)

.1009
(.1000)

0.07
(.7906)

125

Education

.0005
(.0235)

-.0342
(.0269)

1.48
(.2247)

.0223
(.0252)

-.0016
(.0307)

0.64
(.4222)

829

Architecture

.0165
(.0797)

-.0134
(.1304)

0.05
(.8280)

.0258
(.0859)

-.0090
(.1398)

0.06
(.8087)

81

Journalism

.0047
(.0341)

-.1858**
(.0484)

13.30
(.0003)

.0201
(.0377)

-.1484**
(.0522)

10.28
(.0015)

325

** Significant at the 5% level
* Significant at the 10% level
Notes: Standard errors are shown in parentheses except in the F-test column in which they show p-values. Standard
errors correct for correlation within observations of the same student. Each model includes controls for subject,
campus, gender, and race. Additionally, there are controls for age and the highest educational degree completed for
the right panel.

41

Figure 1: Proportion of Overall Effect of Adjuncts on Total Hours By Semester
1
0.9
0.8

Effect Size

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1st

2nd

3rd

4th

5th

6th

7th

8th

Sem ester
Adjunct Effect

Even Distribution

Figure 2: Effect of Adjuncts on Probability of Taking a Subsequent Course By Semester
0.06
0.04
0.02

Effect Size

0
-0.02

2nd

3rd

4th

5th

6th

7th

8th

-0.04
-0.06
-0.08
-0.1
-0.12
-0.14
Sem ester
Adjunct Effect

Upper Bound

Low er Bound

42

