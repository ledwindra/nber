                               NBER WORKING PAPER SERIES




    BAYESIAN AVERAGING, PREDICTION AND NONNESTED MODEL SELECTION

                                            Han Hong
                                           Bruce Preston

                                       Working Paper 14284
                               http://www.nber.org/papers/w14284


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    August 2008




We thank Raffaella Giacomini, Jin Hahn, Bernard Salanie, Barbara Rossi, Frank Schorfheide and Chris
Sims for comments. We also thank the NSF and the Sloan Foundation for generous research support.
The views expressed herein are those of the author(s) and do not necessarily reflect the views of the
National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

Â© 2008 by Han Hong and Bruce Preston. All rights reserved. Short sections of text, not to exceed two
paragraphs, may be quoted without explicit permission provided that full credit, including Â© notice,
is given to the source.
Bayesian Averaging, Prediction and Nonnested Model Selection
Han Hong and Bruce Preston
NBER Working Paper No. 14284
August 2008
JEL No. C14,C52

                                            ABSTRACT

This paper studies the asymptotic relationship between Bayesian model averaging and post-selection
frequentist predictors in both nested and nonnested models. We derive conditions under which their
difference is of a smaller order of magnitude than the inverse of the square root of the sample size
in large samples. This result depends crucially on the relation between posterior odds and frequentist
model selection criteria. Weak conditions are given under which consistent model selection is feasible,
regardless of whether models are nested or nonnested and regardless of whether models are correctly
specified or not, in the sense that they select the best model with the least number of parameters with
probability converging to 1. Under these conditions, Bayesian posterior odds and BICs are consistent
for selecting among nested models, but are not consistent for selecting among nonnested models.


Han Hong
Stanford University
Landau Economics Building
579 Serra Mall
Stanford, CA 94305
doubleh@stanford.edu

Bruce Preston
Department of Economics
Columbia University
420 West 118th Street
New York, NY 10027
and NBER
bp2121@columbia.edu
 Bayesian Averaging, Prediction and Nonnested Model Selection


                                       Han Hong and Bruce Preston1
                                       Previous version: January 2006
                                         This version: August 2008




                                                   Abstract

         This paper studies the asymptotic relationship between Bayesian model averaging and post-
         selection frequentist predictors in both nested and nonnested models. We derive conditions
         under which their difference is of a smaller order of magnitude than the inverse of the square
         root of the sample size in large samples. This result depends crucially on the relation between
         posterior odds and frequentist model selection criteria. Weak conditions are given under which
         consistent model selection is feasible, regardless of whether models are nested or nonnested and
         regardless of whether models are correctly specified or not, in the sense that they select the
         best model with the least number of parameters with probability converging to 1. Under these
         conditions, Bayesian posterior odds and BICs are consistent for selecting among nested models,
         but are not consistent for selecting among nonnested models.

         JEL Classification: C14; C52
         Keywords: Model selection criteria, Nonnested, Posterior odds, BIC




1       Introduction

Bayesian methods are becoming increasingly popular, both as a framework of model selection and
also as a tool of forecasting â€” see, among others, Fernandez-Villaverde and Rubio-Ramirez (2004a),
Schorfheide (2000), Stock and Watson (2001), Timmermann (2005), Clark and McCracken (2006)
and Wright (2003). These methods are often used to summarize statistical properties of data,
identify parameters of interest, and conduct policy evaluation. While empirical applications of
these methods are abundant, less is understood about their theoretical sampling properties. This
paper provides a starting point for understanding the relation between Bayesian forecast averaging
and frequentist model selection and prediction in a general framework that admits the possibility
of model misspecification.
    1
    Department of Economics, Stanford University and Department of Economics, Columbia University. We thank
Raffaella Giacomini, Jin Hahn, Bernard SalanieÌ, Barbara Rossi, Frank Schorfheide and Chris Sims for comments.
We also thank the NSF and the Sloan Foundation for generous research support. The usual caveat applies.
                                                                                                     2


We study the large sample properties of Bayesian prediction and model averaging for both nested
and nonnested models. We first show that, for a single model, the difference between Bayesian and
frequentist predictors are of smaller order of magnitude than the inverse of the square root of the
sample size in large samples, regardless of the expected loss function used in forming the Bayesian
predictors. This contrasts with the difference between
                                                      MLE    and Bayesian estimators, formed using
                                                        âˆš 
a variety of loss functions, which is of the order Op 1/ T .

For multiple models, we derive general conditions under which the Bayesian posterior odds place
asymptotically unit weight on the best model with the most parsimonious parameterization. Under
these conditions, the Bayesian average model forecast is equivalent to the frequentist post-selection
forecast up to a term that is of smaller order of magnitude than the inverse of the square root of
the sample size in large samples. These findings essentially combine Schwarzâ€™ original contribution
regarding BIC â€” that it is an asymptotic approximation to posterior odds â€” with the insights by
Sin and White (1996) who demonstrate the inconsistency of BIC for selecting among nonnested
models. The conditions we derive are weaker, more general, and allow for a much wider class of
models.

An immediate consequence of multiple model comparison is that Bayesian posterior odds compar-
ison is inconsistent for selecting between nonnested models. While this procedure will select one of
the best fitting models, it does not necessarily choose the most parsimonious model with probability
converging to 1 in large samples. Consistent selection among possibly nonnested models is feasible
using nonnested model selection criteria in the spirit of Sin and White (1996).

Empirical analyses frequently find that forecasts generated from averages of a number of models
typically perform better than forecasts of any one of the underlying models â€” see, for instance,
Stock and Watson (2001). Our theoretical findings suggest that this can be because the models
under consideration are close to each other and are all misspecified; so that the posterior weights are
non-degenerate among the set of models under comparison. Indeed, it is shown that for nonnested
models, posterior weights will be non-degenerate, as long as the models are sufficiently close to
each other. The case of nested models is more interesting. It turns out that when the two models
are sufficiently far from each other or sufficiently close from each other, the posterior weights will
be degenerate. However, when the two models are â€œjust close enoughâ€ but â€œnot too closeâ€, the
posterior weights can be non-degenerate, and, as a consequence, model averaging can outperform
each individual model.

Our results are of interest given the burgeoning use of Bayesian methods in the estimation of dy-
namic stochastic general equilibrium models in modern macroeconomics. See, inter alia, Fernandez-
                                                                                                    3


Villaverde and Rubio-Ramirez (2004a), Schorfheide (2000), Smets and Wouters (2002), Lubik and
Schorfheide (2003) and Justiniano and Preston (2004) for examples of estimation in both closed and
open economy settings. These papers all appeal to posterior odds ratios as a criterion for model
selection. By giving a classical interpretation to the posterior odds ratio, the present paper intends
to provide useful information regarding the conditions under which such selection procedures ensure
consistency. The analysis contributes to understanding the practical limitations of standard model
selection procedures given a finite amount of data.

The paper proceeds as follows. Section 2 describes model assumptions and derives their implications
on the large sample behavior of the likelihood function. Section 3 demonstrates the asymptotic
equivalence between Bayesian and frequentist predictors for a single model under weak conditions.
The rest of the paper generalizes this result to multiple models. Section 4 first derives weak
conditions under which the generalized posterior odds ratio is equivalent to BIC up to a term that is
asymptotically negligible, and under which alternative model selection criteria are feasible to select
consistently between both nested and nonnested models. Section 5 makes use of the asymptotic
equivalence between posterior odds ratio and BIC to derive the relation between Bayesian model
averaging and frequentist post-selection prediction. Finally, section 6 discusses the implications for
our results for non-likelihood-based models, and section 7 concludes.


2   Model assumptions and implications

For clarity of exposition, we identify a model with the likelihood function that is being used to
estimate model parameters. All results extend to general random distance functions that satisfy
the stochastic equicontinuity assumptions stated below.

A parameter Î² is often estimated by maximizing a random log-likelihood function QÌ‚ (Î²) associated
with some model f (yt , Î²) that depends on observed data yt and parameterized by the vector Î²:

                                   QÌ‚ (Î²) â‰¡ Q (yt , t = 1 . . . , T ; Î²) .

For example, under i.i.d sampling of the data, as in Vuong (1989) and Sin and White (1996), the
log-likelihood function takes the form of
                                                 T
                                                 X
                                      QÌ‚ (Î²) =         log f (yt ; Î²) ,
                                                 t=1

which minimizes the Kullback-Leibler distance between the parametric model and the data.

Under standard assumptions, the random objective function converges to a population limit when
the sample size increases without bound. It is assumed that there exists a function Q (Î²), uniquely
                                                                                                  4


maximized at Î²0 , which is the uniform limit of the random sample analog
                                           1                 p
                                     sup     QÌ‚ (Î²) âˆ’ Q (Î²) âˆ’â†’ 0.
                                     Î²âˆˆB   T

Typically, the following decomposition holds for QÌ‚ (Î²):
                               
                      QÌ‚ Î²Ì‚ = QÌ‚ Î²Ì‚ âˆ’ QÌ‚ (Î²0 ) + QÌ‚ (Î²0 ) âˆ’ T Q (Î²0 ) + T Q (Î²0 ) .
                              |      {z      } |          {z       } | {z }
                                                              (Qb)               (Qc)
                                       (Qa)


Under suitable regularity conditions, the following are true:
                                                    âˆš 
                    (Qa) = Op (1) ,      (Qb) = Op     T ,             (Qc) = O (T ) .

The regularity conditions under which the first equality holds are formally given below. They are
the same as those in Chernozhukov and Hong (2003). They do not require the objective function to
be smoothly differentiable, and permit complex nonlinear or simulation-based estimation methods.
In particular, conditions that require smoothness of the objective function are typically violated
in simulation-based estimation methods and in percentile-based non-smooth moment conditions.
Even for simulation-based estimation methods, it can be difficult for researchers to insure that the
simulated objective functions are smooth in model parameters.

ASSUMPTION 1 The true parameter vector Î²0 belongs to the interior of a compact convex
subset B of Rdim(Î²) .

ASSUMPTION 2 For any Î´ > 0, there exists  > 0, such that
                                                            
                                    1                  
              lim inf P      sup       QÌ‚ (Î²) âˆ’ QÌ‚ (Î²0 ) â‰¤ âˆ’ = 1.
                  T â†’âˆž    |Î²âˆ’Î²0 |â‰¥Î´ T

                                                                             p
ASSUMPTION 3 There exist quantities âˆ†T , JT , â„¦T , where JT â†’ âˆ’AÎ² , â„¦T = O (1),
                                       1
                                      âˆš â„¦âˆ’1/2
                                                  d
                                          T   âˆ†T âˆ’â†’ N (0, I) ,
                                        T
such that if we write
                                                                1
                RT (Î²) = QÌ‚ (Î²) âˆ’ QÌ‚ (Î²0 ) âˆ’ (Î² âˆ’ Î²0 )0 âˆ†T +      (Î² âˆ’ Î²0 )0 (T JT ) (Î² âˆ’ Î²0 )
                                                                2
then it holds that for any sequence of Î´T â†’ 0
                                                   RT (Î²)
                                     sup                        = op (1) .
                                  |Î²âˆ’Î²0 |â‰¤Î´T   1 + T |Î² âˆ’ Î²0 |2
                                                                                                     5

                                                 
THEOREM 1 Under assumptions (1), (2) and (3), QÌ‚ Î²Ì‚ âˆ’ QÌ‚ (Î²0 ) = Op (1).

                                     
The asymptotic distribution of QÌ‚ Î²Ì‚ âˆ’ QÌ‚ (Î²0 ) is also easy to derive in many situations. This
distribution is useful for model selection tests but is not directly used in model selection criteria
developed in section 4. In particular, satisfaction of the information matrix equality, â„¦T = âˆ’AÎ² +
op (1), is not necessary for our discussion of model selection criteria. This is especially relevant
under potential model misspecification.

                                       âˆš 
ASSUMPTION 4 QÌ‚ (Î²0 ) âˆ’ T Q (Î²0 ) = Op   T .


Assumption 4 typically follows from an application of the central limit theorem
                               1                      
                                                         d
                              âˆš    QÌ‚ (Î²0 ) âˆ’ T Q (Î²0 ) âˆ’â†’ N (0, Î£Q ) ,                           (2.1)
                                T
where
                                                                            
                                                     1 
                            Î£Q = lim V ar           âˆš    QÌ‚ (Î²0 ) âˆ’ T Q (Î²0 ) .
                                                      T

For example, in the case of the log-likelihood function for i.i.d. observations, where
                                 T
                                 X
                      QÌ‚ (Î²) =         log f (yt ; Î²)   and Q (Î²) = E log f (y; Î²) ,
                                 t=1

such convergence follows immediately from the central limit theorem: Î£ = V ar (log f (yt ; Î²)).

Beyond the likelihood setting, assumption 4 can in general be easily verified for extreme estimators
based on optimizating random objective functions. These include M estimator, generalized method
of moment estimators and their recent information-theoretic variants. It can be shown to hold for
generalized method of moment estimators regardless of whether the model is correctly specified or
misspecified.

The property of the final term (Qc) is immediate.


3   Bayesian predictive analysis

Consider the Bayesian inference problem of predicting yT +1 given a data set YT with observations
up to T . Typically we need to calculate the predictive density of yT +1 given YT , and for this
                                                                                                         6


purpose need to average over the posterior distribution of the model parameters Î² given the data
YT :
                                       Z
                       f (yT +1 |YT ) = f (yT +1 |YT , Î²) f (Î²|YT ) dÎ²

                                                                     eQÌ‚(Î²) Ï€ (Î²)
                                           Z
                                       =       f (yT +1 |YT , Î²) R                     dÎ².
                                                                     eQÌ‚(Î²) Ï€ (Î²) dÎ²
                                                        
We will assume that f (yT +1 |YT ; Î²) = f yT +1 |YÌ„T ; Î² , where YÌ„T is a finite dimensional subcompo-
nent of Y . For example, YÌ„T can be yT , the most recent observation in YT . It is well understood
                                                                                   
that the first-order randomness in the prediction is driven by f yT +1 |YÌ„T , Î² . The length of the
                                                                              
prediction interval comprises two parts: the first is due to f yT +1 |YÌ„T , Î² and the second is due to
the uncertainty from estimating Î². While the second part will decrease to zero as the sample size
T increases, the first part remains constant.

We are interested in the second-order uncertainty in the prediction that is due to the estimation
of the parameter Î², and therefore will consider a fixed value yÌ„ of the random component YÌ„T and
density
                                               Z
                           f (yT +1 |yÌ„, YT ) = f (yT +1 |yÌ„, Î²) f (Î²|YT ) dÎ²,              (3.2)

where yÌ„ can potentially differ from the observed realization of YÌ„T in the sample. For example, one
might consider out of sample predictions where yÌ„ does not take the realized value of YÌ„T .

Point predictions can be constructed as functionals of the posterior predictive density f (yT +1 |yÌ„, YT ).
For example, a mean prediction can be obtained by
                                             Z
                         E (yT +1 |yÌ„, YT ) = E (yT +1 |yÌ„, YT ; Î²) f (Î²|YT ) dÎ².

A median prediction is given by
                                                Z         x                                  
                                                                                           1
                    med fyT +1 (Â·|yÌ„, Y ) = inf x :            f (yT +1 |yÌ„, Y ) dyT +1   â‰¥     .
                                                                                            2

Under suitable regularity conditions, Î²Ì‚ is an asymptotic sufficient statistic for the random posterior
distribution and f (Î²|YT ) is approximately normal with mean Î²Ì‚ and variance âˆ’ T1 (AÎ² )âˆ’1 :
                                                                 
                                             A         1
                                  f (Î²|YT ) âˆ¼ N Î²Ì‚, âˆ’ (AÎ² )âˆ’1 .
                                                      T
This asymptotic approximation of the posterior distribution of Î² by a normal distribution is the
key element of the results formally developed in section 4, and forms the basis of the following
asymptotic approximation of the predictive distribution.
                                                                                                               7

                                                                               
The density f (yT +1 |yÌ„, YT ) satisfies the approximation, up to order op âˆš1T ,
                  Z                                                      0
                                             dim(Î²)          1   1
                     f (yT +1 |yÌ„, Î²) (2Ï€)âˆ’ 2 det (âˆ’T AÎ² ) 2 eâˆ’ 2 (Î²âˆ’Î²Ì‚ ) (âˆ’T AÎ² )(Î²âˆ’Î²Ì‚ ) dÎ²
                                                                          h0 (âˆ’AÎ² )h
                  Z                          
                                          h          dim(Î²)         1
               = f yT +1 |yÌ„, Î²Ì‚ + âˆš            (2Ï€)âˆ’ 2 det (âˆ’AÎ² ) 2 eâˆ’        2     dh.
                                           T

Researchers are most often interested in mean predictions and predictive intervals. Mean prediction
is convenient to analyze because of its linearity property. For instance,
                                           Z
                       E (yT +1 |yÌ„, YT ) = E (yT +1 |yÌ„, Î²) f (Î²|YT ) dÎ²
                                           Z                       
                                                                 h
                                          = E yT +1 |yÌ„, Î²Ì‚ + âˆš       f (h|Y ) dh.
                                                                  T
If E (yT +1 |yÌ„; Î²) is linear in Î², then it is easy to see from the fact that f (h|Y ) is approximately
normal with mean 0 and variance âˆ’AÎ²âˆ’1 that
                                                                            
                                                                        1
                               E (yT +1 |yÌ„, YT ) = E yT +1 |yÌ„; Î²Ì‚ + op âˆš     .
                                                                           T
Hence Bayesian mean prediction is asymptotically equivalent up to order                     âˆš1
                                                                                 with frequentist
                                                                                             T   
prediction, where the predictive density is formed using the extremum estimate Î²Ì‚, f yT +1 |yÌ„, Î²Ì‚ .

And even if E (yT +1 |yÌ„; Î²) is not linear in Î², as long as it is sufficiently
                                                                               smooth in Î², it is still possible
                                                                 h
to use a first-order Taylor expansion of E yT +1 |yÌ„; Î²Ì‚ + T around Î²Ì‚ to prove that the same result
                                                               âˆš

holds. Given the generic notation of yT +1 , these arguments also apply without change to more
general functions of yT +1 , in the sense that for a general function t (Â·) of yT +1 ,
                                                                                   
                                                                               1
                          E (t (yT +1 ) |yÌ„, YT ) = E t (yT +1 ) |yÌ„, Î²Ì‚ + op âˆš       .
                                                                                  T
These results can be generalized to nonlinear predictions that can be expressed as nonlinear func-
tions of the predictive density f (yT +1 |yÌ„, YT ). For example, median prediction and predictive in-
tervals can readily be constructed. In the following, we will formulate a general prediction as one
that is defined through minimizing posterior expected nonlinear and non-smooth loss functions.

A general class of nonlinear predictors Î»Ì‚ (yÌ„, YT ) can be defined as the solution to minimizing an
expected loss function Ï (Â·), which is assumed to be convex:

                         Î»Ì‚ (yÌ„, YT ) = arg min E (Ï (yT +1 , Î») |yÌ„, YT )
                                             Î»
                                                Z
                                      = arg min Ï (yT +1 , Î») f (yT +1 |yÌ„, YT ) dyT +1 ,
                                            Î»
                                                                                                     8


where the predictive density f (yT +1 |yÌ„, YT ) is defined in equation (3.2). For example, to construct
a one-sided Ï„ th predictive interval, we can take

                    Ï (yT +1 , Î») â‰¡ ÏÏ„ (yT +1 âˆ’ Î») = (Ï„ âˆ’ 1 (yT +1 â‰¤ Î»)) (yT +1 âˆ’ Î») .

The following focuses on this loss function.

We are interested in comparing Î»Ì‚ (yÌ„, YT ) to the nonlinear frequentist prediction, defined as
                                            Z                            
                      Î»Ìƒ (yÌ„, YT ) = arg min Ï (yT +1 , Î») f yT +1 |yÌ„, Î²Ì‚ dyT +1 .
                                          Î»

Also define the infeasible loss function, where the uncertainty from estimation of the unknown
parameters is absent, as
                                          Z
                           ÏÌ„ (yÌ„, Î»; Î²) = Ï (yT +1 , Î») f (yT +1 |yÌ„, Î²) dyT +1

                                       =E (Ï (yT +1 , Î») |yÌ„, Î²) .

Then the Bayesian predictor and the frequentist predictor can be written as
                                                Z
                          Î»Ì‚ (yÌ„, YT ) = arg min ÏÌ„ (yÌ„, Î»; Î²) f (Î²|YT ) dÎ²,
                                                Î»

and
                                                                      
                                    Î»Ìƒ (yÌ„, YT ) = arg min ÏÌ„ yÌ„, Î»; Î²Ì‚ .
                                                       Î»

The next theorem establishes the asymptotic relation between Î»Ì‚ (yÌ„, YT ) and Î»Ìƒ (yÌ„, YT ).


THEOREM 2 Under assumptions (1), (2) and (3), and assuming that for each yÌ„, ÏÌ„ (yÌ„, Î»; Î²)
is three times continuously differentiable in Î² and Î» in a small neighborhood of Î²0 and Î»0 â‰¡
arg minÎ» ÏÌ„ (yÌ„, Î», Î²0 ), with uniformly integrable derivatives, then
                                     âˆš                              p
                                       T Î»Ì‚ (yÌ„, YT ) âˆ’ Î»Ìƒ (yÌ„, YT ) âˆ’â†’ 0.


Note that the condition of this theorem only requires the integrated loss function ÏÌ„ (yÌ„, Î»; Î²) to
be smoothly differentiable in Î» and Î², and does not impose smoothness conditions directly on
Ï (yT +1 , Î»). The results cover predictive intervals, as long as the predictive density given Î² is
smoothly differentiable around the percentiles to be predicted.

Different loss functions can be used to construct a variety of Bayesian point estimators from the
posterior density. Unless the loss function is convex and symmetric around 0, the corresponding
                                                                                                      9


Bayes estimator
              âˆšis typically different from the frequentist maximum likelihood estimator at the
order of Op 1/ T . In contrast, we found that when different loss functions are used to define
different
    âˆš predictors,
                    Bayesian and frequentist predictors coincide with each other up to the order
op 1/ T . This is probably a more relevant result concerning loss functions because researchers
are typically more interested in using loss functions to define the properties of predictions, rather
than to define the estimator itself.


4     Posterior odds and consistent model selection

Bayesian averaging is a popular method for making predictions in the context of multiple models.
The weights used in Bayesian averaging are calculated through posterior odds ratios. To generalize
the results in the previous section to multiple models, it is important to understand first the relation
between posterior odds ratio calculation and consistent frequentist model selection criteria. This is
of independent interest given the increasing use of Bayesian methods in economics; particularly the
recent macroeconomics literature on estimation of dynamic stochastic general equilibrium models
which makes use of the posterior odds ratio for model selection and prediction.


4.1    Large sample properties of Bayes factors

The posterior distribution is an integral transformation of the model (f, Q), defined as
                                                 Z
                                                       0
                                    QÌ‚(Î²)
                                          Ï€ (Î²) / eQÌ‚(Î² ) Ï€ Î² 0 dÎ² 0 .
                                                               
                                   e

Associated with the posterior distribution is the Bayes factor
                                             Z
                                      PQ Ã— eQÌ‚(Î²) Ï€ (Î²) dÎ²,

where PQ is a prior probability weight of model (f, Q).

The following theorem connects the properties of the Bayes posterior distribution to the extremum
estimator and establishes the relation between the Bayes factor and the extremum estimator analog
of BICs. Under the regularity conditions stated in assumptions 1 to 3, the Bayes factor is asymp-
totically equivalent to using BIC as a model selection criterion. While the relation between Bayes
factor and BIC comes from the original contribution by Schwartz, the conditions in the following
theorem are considerably weaker and more general.


THEOREM 3 Under assumptions (1), (2) and (3), the Bayes factor satisfies the following rela-
                                                                                                          10


tion
                               Z
                      dim(Î²)                                p                 dim(Î²)
                  T      2         eQÌ‚(Î²)âˆ’QÌ‚(Î²Ì‚ ) Ï€ (Î²) dÎ² âˆ’â†’ Ï€ (Î²0 ) (2Ï€)       2     det (âˆ’AÎ² )âˆ’1/2 .


The formal details of the proof are relegated to an appendix. When QÌ‚ (Î²) is smoothly differentiable,
intuition can be gleaned by considering the expression:
                         Z                             Z
                      log e  QÌ‚(Î²)
                                   Ï€ (Î²) dÎ² âˆ’ QÌ‚ Î²Ì‚ = log eQÌ‚(Î²)âˆ’QÌ‚(Î²Ì‚ ) Ï€ (Î²) dÎ².
                                                                               
It can be approximated up to an op (1) term as follows. First, QÌ‚ (Î²) âˆ’ QÌ‚ Î²Ì‚ can be approximated
                                                    âˆš
by a quadratic function centered at Î²Ì‚, in a size 1/ T neighborhood around Î²Ì‚:
                                                                 
                                     1             0 âˆ‚ 2 QÌ‚T Î²Ì‚         
                        QÌ‚ (Î²) âˆ’ QÌ‚ Î²Ì‚ â‰ˆ       Î² âˆ’ Î²Ì‚                 Î² âˆ’ Î²Ì‚   .
                                            2               âˆ‚Î²âˆ‚Î² 0
                 âˆš
Second,
    pin this 1/ T size neighborhood, the prior density is approximately constant around Î²0 as
Ï€ Î²Ì‚ âˆ’â†’ Ï€ (Î²0 ). The impact of the prior density is negligible except for the value at Ï€ (Î²0 ). Out-
           âˆš
side the 1/ T size neighborhood around Î²Ì‚, the difference between eQÌ‚(Î²) and eQÌ‚(Î²Ì‚ ) is exponentially
small, and makes only asymptotically negligible construction to the overall integral.

The appendix proves formally the approximation:
                                                       0 âˆ‚ 2 QÌ‚T (Î²Ì‚ )
   Z                                      Z
                                             1
            Ï€ (Î²) dÎ² âˆ’ QÌ‚ Î²Ì‚ = log Ï€ (Î²0 ) e 2 (
                                                Î²âˆ’Î²Ì‚ ) âˆ‚Î²âˆ‚Î²       0 (Î²âˆ’Î²Ì‚ )
                          
      QÌ‚(Î²)
log e                                                                       dÎ² + op (1)
                                          Z            0
                                             1
                             = log Ï€ (Î²0 ) e 2 (Î²âˆ’Î²Ì‚ ) T AÎ² (Î²âˆ’Î²Ì‚ ) dÎ² + op (1)
                                                 dim(Î²)
                                                                            1
                                                                               
                             = log Ï€ (Î²0 ) (2Ï€) 2 det (âˆ’T AÎ² )âˆ’ 2 + op (1)
                                                       dim (Î²)           1            1
                                     = log Ï€ (Î²0 ) +           log (2Ï€) âˆ’ det (âˆ’AÎ² ) âˆ’ dim (Î²) log T + op (1) ,
                                       |                  2    {z        2        } 2
                                                                C (AÎ² ,Î² )

where C (AÎ² , Î²) can also depend on the prior weight on the model itself.


4.2    Consistent model selection

Now introduce a competing model, g (Y ; Î±), Î± âˆˆ Î› (generalizing the notations to multiple models
is immediate), where Î± is estimated by the random log-likelihood function:

                                           LÌ‚ (Î±) â‰¡ L (yt , t = 1, . . . , T ; Î±) .
                                                                                                  11


Similarly to model (f, Q), we assume that the decomposition of

                        LÌ‚ (Î±Ì‚) = LÌ‚ (Î±Ì‚) âˆ’ LÌ‚ (Î±0 ) + LÌ‚ (Î±0 ) âˆ’ T L (Î±0 ) + T L (Î±0 )
                                  |       {z      } |           {z       } | {z }
                                        (La)                  (Lb)

satisfies the stochastic order properties
                                                           âˆš 
                     (La) = Op (1) ,           (Lb) = Op     T ,        (Lc) = O (T ) .

We also assume that assumptions (1), (2), (3) and (4), and subsequently theorems 1, 2 and 3 hold
for model (g, L).

A conventional consistent model selection criterion takes the form of a comparison between
                        
                     QÌ‚ Î²Ì‚ âˆ’ dim (Î²) âˆ— CT       and     LÌ‚ (Î±Ì‚) âˆ’ dim (Î±) âˆ— CT ,

where CT is a sequence of constants that tends to âˆž as T goes to âˆž, at a rate to be prescribed
below, and dim () indexes the parametric dimension of the model. Î²Ì‚ and Î±Ì‚ are maximands of the
corresponding objective functions. The second term in the model selection criteria penalizes the
dimension of the parametric model. For instance, BIC takes CT = log T and AIC adopts CT = 2.

While the discussion so far has focused on parametric likelihood models, all results can be applied
without modification to other non-likelihood-based models, including method of moment models.
They are also applicable to non-log-likelihood random criterion functions that measure other no-
tions of distance between the model and the data generating process, such as the Cressie-Read
discrepancy statistic in generalized empirical likelihood methods.

For non-likelihood-based models, in addition to penalizing the parametric dimension of the model,
the dimension of the estimation procedure, such as the number of moment conditions could also be
penalized as in Andrews and Lu (2001). The goal of the latter penalization term is to preserve the
parsimony of the model and the informativeness of the estimation procedure, thereby improving
the precision of the model with a finite amount of data. To emphasize this possibility, we will adopt
a general notation dim (f, Q) and dim (g, L), rather than dim (Î²) and dim (Î±). The first argument
refers to the parametric dimension of the model, while the second argument refers to the dimension
of the information used in the inference procedure regarding model parameters.

For the specific case of the Bayesian (Schwartz) information criteria,
                                
                      BIC = QÌ‚ Î²Ì‚ âˆ’ LÌ‚ (Î±Ì‚) âˆ’ (dim (f, Q) âˆ’ dim (g, L)) Ã— log T

implicitly defining CT = log T . It now will be demonstrated that whether models
                                                                             are nested
or nonnested has important consequences for the asymptotic properties of QÌ‚ Î²Ì‚ âˆ’ LÌ‚ (Î±Ì‚) and
                                                                                                       12


consistency of the BIC as model selection criterion. The two cases of nested and nonnested models
are treated in turn.


4.2.1    Nested Models

There are two cases of interest: one model is better in the sense that Q (Î²0 ) 6= L (Î±0 ) and both
models are equally good in the sense that Q (Î²0 ) = L (Î±0 ). Consider the former. Without loss of
generality, let Q (Î²) be the larger nesting model and L (Î±) be the smaller model nested in Q (Î²).
When Î²0 6= Î±0 , it is typically the case that Q (Î²0 ) > L (Î±0 ). The goal of BIC is to select the correct
model, Q (Î²), with probability converging to 1. In this case, this will be true because

   BIC = (Qa) âˆ’ (La) + (Qb) âˆ’ (Lb) + T (Q (Î²0 ) âˆ’ L (Î±0 )) âˆ’ (dim (f, Q) âˆ’ dim (g, L)) Ã— log T,
                             âˆš
         |   {z    } |      {z    } |         {z         }
            Op (1)       Op ( T )           O(T )

will be dominated by +T (Q (Î²0 ) âˆ’ L (Î±0 )), which increases to +âˆž with probability converging to
1 as T â†’ âˆž. In other words, for any M > 0,

                                            P (BIC > M ) âˆ’â†’ 1.

Hence, BIC selects the correct model with probability converging to 1 if there is one correct model.
More generally, BIC selects one of the correct models with probability converging to 1.

Suppose now that both models are equally good, so that Q (Î²0 ) = L (Î±0 ). Because we are discussing
nested models, this also means that Î²0 = Î±0 (with the obvious abuse of the notation of equality
with different dimensions), and that QÌ‚ (Î²0 ) = LÌ‚ (Î±0 ) almost surely. In the case of likelihood models,
this means that f (Zt ; Î²0 ) = g (Zt ; Î±0 ) almost surely, since g (Â·) is just a subset of f (Â·). The true
model lies in the common subset of (Î²0 , Î±0 ) and has to be the same model.

In this case, the second term is identically equal to 0:

                     (Qb) âˆ’ (Lb) = QÌ‚ (Î²0 ) âˆ’ LÌ‚ (Î±0 ) âˆ’ (T Q (Î²0 ) âˆ’ T L (Î±0 )) â‰¡ 0.

Given that the last terms (Qc) and (Lc) disappear as a consequence of the equality Q (Î²0 ) = L (Î±0 ),
the BIC comparison is reduced to

                       BIC = (Qa) âˆ’ (La) âˆ’ (dim (f, Q) âˆ’ dim (g, L)) Ã— log T.
                             |   {z    }
                                   Op (1)

The second term, which is of order O (log T ), will dominate. So if dim (f, Q) > dim (g, L), BIC will
converge to âˆ’âˆž with probability converging to 1. In other words, for any M > 0,

                                        P (BIC < âˆ’M ) âˆ’â†’ 1.
                                                                                                    13


Hence, given two equivalent models, in the sense of Q (Î²0 ) = L (Î±0 ), the BIC will choose the most
parsimonious model (namely the one with the smallest dimension, either dim (f, Q) or (g, L)) with
probability converging to 1.

It is clear from the above arguments that, instead of using CT = log T , we can choose any sequence
of CT such that CT âˆ’â†’ âˆž and CT = o (T ).


4.2.2   Nonnested Models

Many model comparisons are performed among models that are not nested inside each other. A
leading example is the choice between a nonlinear model and its linearized version. For instance,
in an extensive literature on estimating consumption Euler equations, there has been debate on
the appropriateness of using log-linear versus non-linear Euler equations to estimate household
preference parameters. See, for instance, Carroll (2001), Paxson and Ludvigson (1999), and At-
tanasio and Low (forthcoming). More recently, Fernandez-Villaverde and Rubio-Ramirez (2003)
and Fernandez-Villaverde and Rubio-Ramirez (2004b) show that nonlinear filtering methods render
feasible the estimation of some classes of nonlinear dynamic stochastic general equilibrium models.
Being equipped with model selection criteria to compare multiple non-linear non-nested models is
clearly desirable.

In contrast to the case of nested models, the comparison of nonnested models imposes more stringent
requirements on CT for consistent model selection. Indeed, the further condition on CT that
     âˆš                                                                         âˆš
CT / T âˆ’â†’ âˆž is required in addition to CT = o (T ). As an example, CT = T log T will satisfy
both requirements, but CT = log T will not satisfy the second requirement. Letâ€™s call the model
                                                    âˆš
selection criteria N IC when we choose CT = log T T . To our knowledge these rate conditions are
first due to Sin and White (1996) in the context of smooth likelihood models.

Suppose that Q (Î²0 ) is greater than L (Î±0 ), implying model (f, Î²) is better than model (g, Î±).
Then, as before, N IC is dominated by T (Q (Î²0 ) âˆ’ L (Î±0 )), which increases to +âˆž with probability
                                                                                   âˆš
converging to 1. This is true regardless of whether we choose CT = log T or CT = T log T , since
both are of smaller order of magnitude than T . Hence the behavior of N IC when one model is
better than the other is essentially the same for both nested models and nonnested models.

However, when both models are equally good, so that Q (Î²0 ) = L (Î±0 ), NIC comprises the non-
vanishing term

                       (Qb) âˆ’ (Lb) = QÌ‚ (Î²0 ) âˆ’ LÌ‚ (Î±0 ) âˆ’ (T Q (Î²0 ) âˆ’ T L (Î±0 ))
                    âˆš
which is of order O( T ). In contrast to the nested case, it is no longer true that QÌ‚ (Î²0 ) â‰¡ LÌ‚ (Î±0 )
                                                                                                 14


with probability one when the two models are equally good. Hence the model selection criterion
takes the form

               N IC = (Qa) âˆ’ (La) + (Qb) âˆ’ (Lb) âˆ’ (dim (f, Q) âˆ’ dim (g, L)) Ã— CT .
                                          âˆš
                      |   {z    } |      {z    }
                         Op (1)       Op ( T )


                                                  âˆš
Hence, for choice of penalty function CT = log T T , or any other sequence that increases to âˆž
            âˆš
faster than T , the last term dominates. So if dim (f, Q) > dim (g, L), NIC converges to âˆ’âˆž with
probability converging to 1, or

                           P (N IC < âˆ’M ) âˆ’â†’ 1         for any   M > 0.

Thus, N IC will choose the most parsimonious model among the two models with probability
converging to 1.

In contrast, if the BIC had been used, where CT = log T , then the final term fails to dominate.
The second term, which is random, might instead dominate. It is immediate that model (f, Q)
and model (g, L) will both be selected with strictly positive probabilities. Such model selection
behavior does not have a clear interpretation when a unique ranking of models is desired. While an
analyst may be content with identifying just one of the best fitting models, regardless of whether
it is the most parsimonious or not, the definition of consistency we adopt from the literature seeks
to uniquely rank models given a model selection criterion and a particular set of assumptions. As
such, positive probability weights on multiple models fails our requirements.

While the properties of AIC and BIC for selecting among nested parametric models are well un-
derstood (e.g. see Sims (2001) and Gourieroux and Monfort (1995)), the following theorem serves
to summarize the above discussion about the NIC.


THEOREM 4 Suppose that assumptions (1), (2), (3) and (4) hold for models (f, Q) and (g, L).
Then
                                                                   âˆš
           N IC = QÌ‚ Î²Ì‚ âˆ’ LÌ‚ (Î±Ì‚) âˆ’ (dim (f, Q) âˆ’ dim (g, L)) Ã— log T T

has the following properties for (f, Q) and (g, L) either nested or nonnested:

  1. If Q (Î²0 ) > L (Î±0 ) then P (N IC > M ) âˆ’â†’ 1 for all M > 0;

  2. If Q (Î²0 ) = L (Î±0 ) and dim (f, Q) âˆ’ dim (g, L) > 0 then P (N IC > M ) âˆ’â†’ 1 for all M > 0.
                                                                                                    15


4.3   Posterior odds and BIC

The posterior odds ratio between the two models is defined as
                                            R QÌ‚(Î²)
                                       PQ     e      Ï€ (Î²) dÎ²
                                   log    Ã—R
                                       PL     eLÌ‚(Î±) Î³ (Î±) dÎ±

when the two models have prior probability weights PQ and PL = 1 âˆ’ PQ . Exploiting the approx-
imation in section 4.1, up to a term of order op (1), the log posterior odds ratio can be written
as
                                               
                          1          1                                              PQ
        QÌ‚ Î²Ì‚ âˆ’ LÌ‚ (Î±Ì‚) âˆ’     dim (Î²) âˆ’ dim (Î±) log T + C (AÎ² , Î²) âˆ’ C (AÎ± , Î±) + log
                            2          2                                              PL
implicitly defining a penalization term CT = log T . It is immediately clear that this expression is
asymptotically equivalent, up to a constant that is asymptotically negligible, to BIC. As discussed
previously, this choice of CT gives a consistent model selection criterion only when comparing
nested models. In the nonnested case, when the null hypothesis contends that two models are
asymptotically equivalent in fit and therefore misspecified, it fails to select the most parsimonious
model with probability converging to 1.

Fernandez-Villaverde and Rubio-Ramirez (2004a) also explore the large sample properties of the
posterior odds ratio in the case of parametric likelihood estimation. They demonstrate, assuming
there exists a unique asymptotically superior model in the sense of Q (Î²) > L (Î±), that the pos-
terior odds ratio will select model (f, Q) with probability converging to 1 as T goes to infinity.
Theorem 3 serves to generalize this finding. First, the results presented here are established under
very weak regularity conditions. Second, only by considering a null hypothesis that admits the
possible equivalence of the two models, in the sense that Q (Î²) = L (Î±), can a complete classical
interpretation properly be given to the posterior odds ratio as a model selection criterion. Statis-
tically distinguishing models is fundamental to classical hypothesis testing. Given the absence of
prior information in classical estimation it is necessary to entertain the possibility that two or more
models are equally good in a testing framework. The results of this paper are therefore seen to be
couched naturally in the classical paradigm.


4.4   Multiple Models

The previous theorem extends directly to the case of multiple models. Suppose there are a total
of M models, of which k of them are equally good, in the sense of having the same limit objective
function with magnitude Q (Î²0 ), but the other M âˆ’ k models have lower valued limit objective
functions. Then with probability converging to 1, both the BICs and the NICs for the k good
                                                                                                     16


models will be infinitely larger than those for the M âˆ’ k inferior models. In other words, with
probability converging to 1, none of the M âˆ’ k inferior models will ever be selected by either BIC
or NIC comparison.

However, the behavior of BIC and NIC can be very different among the k best models depending
on whether they are nested or nonnested. If the k best models are all nested inside each other,
both BICs and NICs will select the most parsimonious model with probability converging to 1. In
contrast, if there exist two models that are not nested inside each other, then BIC will put random
weights on at least two models. But NIC will still choose the most parsimonious model among all
the k best models.


5   Bayesian model averaging and post-selection prediction

The results in the previous section can now be applied to study prediction using the average of
two or more models. It is well known that the frequentist asymptotic distribution properties of
post-selection estimation and prediction are not affected by the model selection step, as long as the
model selection criterion is consistent. Here we demonstrate conditions under which the property
of consistent model selection is possessed by the Bayesian prediction procedure (in the sense of
asymptotically placing unitary probability weight on a single model).

With two models (f, Q) and (g, L), we can write the predictive density as

                                   BFQ                               BFL
          f (yT +1 |yÌ„, YT ) =             f (yT +1 |yÌ„, YT , Q) +           f (yT +1 |yÌ„, YT , L)
                                 BFQ + BFL                         BFQ + BFL

where the posterior probability weights on each model are defined as
                            Z                                   Z
                                QÌ‚(Î²)
                 BFQ = PQ e           Ï€ (Î²) dÎ² and BFL = PL eLÌ‚(Î±) Î³ (Î±) dÎ±.


In the case of likelihood models eQÌ‚(Î²) = f (YT |Î²) and eLÌ‚(Î±) = f (YT |Î±). In particular, note

                                            f (YT ) = BFQ + BFL

is the marginal density of the data YT . The model specific predictive densities are respectively,

                                                         eQÌ‚(Î²) Ï€ (Î²) f (yT +1 |yÌ„, Î²) dÎ²
                                                     R
                           f (yT +1 |yÌ„, YT , Q) =            R
                                                                 eQÌ‚(Î²) Ï€ (Î²) dÎ²
                                                     Z
                                                =         f (yT +1 |yÌ„, Î²) f (Î²|YT , Q) dÎ²
                                                                                                              17

                              R
and f (yT +1 |yÌ„, YT , L) =       f (yT +1 |yÌ„, Î±) f (Î±|YT , L) dÎ±. As before, a general class of predictions can
be defined by
                                                    Z
                         Î»Ì‚ (yÌ„, YT ) = arg min         Ï (yT +1 , Î») f (yT +1 |yÌ„, YT ) dyT +1 .
                                                Î»

The following theorem establishes the asymptotic properties of the Bayesian predictor formed by
averaging the two models.

THEOREM 5 Suppose the assumptions stated in Theorem 2 hold for both models (f, Q) and
(g, L). Also assume that one of the following two conditions holds:

   1. Q (Î²0 ) > L (Î±0 ), (f, Q) and (g, L) can be either nested or nonnested.

   2. Q (Î²0 ) = L (Î±0 ) but (f, Q) is nested inside (L, g). In addition, dim (Î±) âˆ’ dim (Î²) > 1.
       âˆš                          p                                             
Then T Î»Ì‚ (yÌ„, YT ) âˆ’ Î»Ìƒ (yÌ„, YT ) âˆ’â†’ 0, where Î»Ìƒ (yÌ„, YT ) = arg minÎ» ÏÌ„ yÌ„, Î»; Î²Ì‚ is formed from
(f, Q).

It is clear from previous discussion that if Q (Î²0 ) = L (Î±0 ) and (f, Q) and (g, L) are not nested,
then the weight on neither BFQ nor BFL will converge to 1, and the behavior of f (yT +1 |yÌ„, YT ) will
be a random average between the two models.

These theoretical results contrast with the conventional wisdom regarding forecasting: that av-
erages (or some combination) of available forecasting models perform better than any one model
taken in isolation. Recent studies by Stock and Watson (2001), Stock and Watson (2003) and
Wright (2003) adduce evidence consistent with this view. The former papers show that a range
of combination forecasts outperform benchmark autoregressive models when forecasting output
growth of seven industrialized economies and similarly that the predictive content of asset prices
in forecasting inflation and output growth when models are averaged improves upon univariate
benchmark models. The latter paper shows that Bayesian model averaging can improve upon the
random walk model of exchange rate forecasts. Understanding the fundamental cause of this dis-
junction between theoretical and empirical findings is left for future work. However, we offer the
following remarks based on our findings


5.1   Models that are locally close

For forecasts generated from averages of a number of models to perform better than forecasts of
any one of the underlying models, the Bayesian posterior weights have to be asymptotically non-
degenerate among at least two of the models that are being averaged. From the previous analysis,
                                                                                                    18


we have shown that this is possible among nonnested models when two of the models are â€œequally
goodâ€ (Q (Î²0 ) = L (Î±0 ).
Our analysis also suggests that among nested models, unless their dimensions are essentially the
same (dim (f, Q) = dim (g, L)), it is most likely that posterior weights will concentrate on one of
the models, either the â€œbestâ€ model or the most parsimonious model. It turns out that it is still
possible for the posterior weights to be non-degenerate among nested models when the two models
are â€œjust close enoughâ€ but â€œnot too closeâ€. To see this, suppose that Q (Î²0 ) âˆ’ L (Î±0 ) = h (T ),
where h (T ) is a function of the sample size T . Recall the decomposition for nested models:

                   (Qa) âˆ’ (La) + T (Q (Î²0 ) âˆ’ L (Î±0 )) âˆ’ (dim (f, Q) âˆ’ dim (g, L)) Ã— CT .
                   |   {z    } |          {z         }
                      Op (1)           T Ã—O(h(T ))

 As long as T Ã—h (T ) is approximately the same order as CT , the penalization term will not dominate
the second term, and it is then possible for posterior weights to be non-degenerate if the last two
terms happen to approximately offset each other.


6   Generalized nonnested model selection criteria

While results in the previous sections are stated with the parametric log-likelihood function in
mind, they apply without modification to a wide class of random non-likelihood-based objective
functions, assuming direct comparison between Q (Î²) and L (Î±) is interpretable. Such interpretation
is possible, for example, when both are constructed as information-theoretic empirical likelihoods
corresponding to a different set of model and moment conditions.

In the case where QÌ‚ (Î²) takes the form of a quadratic norm, such as the GMM estimator, it can be
similarly shown that when Q (Î²0 ) 6= 0, or when the GMM model is misspecified, (2.1) continues to
hold. When Q (Î²0 ) = 0, or when the GMM model is correctly specified, QÌ‚ (Î²0 ) âˆ’ T Q (Î²0 ) = QÌ‚ (Î²0 )
typically converges in distribution to the negative of the quadratic norm of a normal distribution,
a special case of which is the Ï‡2 distribution when an optimal weighting    âˆš  matrix is being used.
Regardless, QÌ‚ (Î²0 ) âˆ’ T Q (Î²0 ) = Op (1) implies QÌ‚ (Î²0 ) âˆ’ T Q (Î²0 ) = Op   T , so that the statement
           âˆš 
(Qb) = Op      T is valid in both cases.

For non-likelihood Q and L, the integral transformations
                              Z                                   Z
                                QÌ‚(Î² 0 )    0                           0
                                               0
                 QÌ‚(Î²)                               LÌ‚(Î±)
                                                           Î³ (Î±) / eLÌ‚(Î± ) Î³ Î±0 dÎ±0 ,
                                                                               
                e      Ï€ (Î²) / e         Ï€ Î² dÎ² and e

can be properly interpreted as distributions. Chernozhukov and Hong (2003) show that estimation
can proceed using simulation methods from Bayesian statistics, such as Markov Chain Monte
Carlo methods, using various location measures to identify the parameters of interest. These
                                                                                                    19


so-called Laplace-type estimators are defined analogously to Bayesian estimators but use general
statistical criterion functions in place of the parametric likelihood function. By considering integral
transformations of these statistical functions to give quasi-posterior distributions, this approach
provides a useful alternative consistent estimation method to handle intensive computation of
many classical extremum estimators such as the GMM estimators of Hansen (1982), Powell (1986)â€™s
censored median regression, nonlinear IV regression such as Berry, Levinsohn, and Pakes (1995)
and instrumental quantile regression as in Chernozhukov and Hansen (2005).

The following provides two specific examples of generalized posterior odds ratios constructed from
non-likelihood random distance functions. They implicitly deliver a penalization term for the
dimension of the information used in the estimation procedure, though all penalize the parametric
dimension of the model.

It is worth underscoring that while penalization for parameterization is a natural choice for para-
metric likelihood models, in general it is not obvious this is necessarily the most desirable form of
penalty function outside the likelihood framework. In the context of generalized Bayesian inference,
there may be grounds to consider alternative penalty functions that also penalize the dimension
of the estimation procedure. For instance, Andrews and Lu (2001) and Hong, Preston, and Shum
(2003) consider such penalty functions that involve both the number of parameters and the number
of moment conditions. The use of additional moments in GMM and GEL contexts is desirable on
efficiency grounds.

Andrews and Lu (2001) proposed consistent model and moment selection criteria for GMM es-
timation. Interestingly, such selection criteria, which award the addition of moment conditions
and penalize the addition of parameters, can not be achieved using a generalized Bayes factor
constructed from the GMM objective function:
                                                                              T
                                                                            1X
                  QÌ‚ (Î²) = âˆ’T gT (Î²)0 WT gT (Î²) ,      where     gT (Î²) =       m (yt , Î²) ,
                                                                            T
                                                                              t=1

                                                             p
for m (yt , Î²) a vector of moment conditions and WT âˆ’â†’ W positive definite.

With the objective function QÌ‚ (Î²) and associated prior PQ , the volume
                         Z
                                                                   1
                     PQ eQÌ‚(Î²) Ï€ (Î²) dÎ² = eQÌ‚(Î²Ì‚ ) eC (AÎ² ,Î² ) T âˆ’ 2 dim(Î²) Ã— eop (1) ,

only shrinks at a rate related to the number of parameters and not the number of moment condi-
tions. Generalized Bayes factors using the quadratic GMM objective functions do not encompass
the model selection criteria proposed by Andrews and Lu (2001). This is not surprising given the
                                                                                                     20


lack of likelihood interpretation of conventional two-step GMM estimators based on a quadratic
norm of the moments.

The recent literature on generalized empirical likelihood (GEL) estimators proposes an information-
theoretic alternative to efficient two-step method of moment estimators that minimizes a likelihood
distance between the data and the model moments. A GEL estimator is defined as the saddle point
of a GEL function,
                                          
                                     Î²Ì‚, Î»Ì‚ = arg max arg min QÌ‚ (Î², Î») .
                                                    Î²âˆˆB         Î»âˆˆÎ›

For example, in the case of exponential tilting
                                       T
                                       X
                                             Ï Î»0 m (yt , Î²)             Ï (x) = ex .
                                                            
                         QÌ‚ (Î², Î») =                             where
                                       t=1

Given the connection between GEL and parametric likelihood models, it is interesting to ask
whether one can define a generalized Bayes factor based on the GELs that mimics the model
and moment selection criteria of Andrews and Lu (2001) and others. We define such a GEL Bayes
factor as
                                       Z
                                                   1
                             GELBF =      R                    Ï€ (Î²) dÎ²
                                            eâˆ’QÌ‚(Î²,Î») Ï† (Î») dÎ»
where Ï† (Î») is a prior density on the lagrange multiplier Î». Intuitively, a large volume of the integral
                                          Z
                                            eâˆ’QÌ‚(Î²,Î») Ï† (Î») dÎ»

indicates that Î» tends to be large, and therefore that the GMM model (f, Q) is more likely to
be incorrect, or misspecified. Hence, we use its inverse to indicate the strength of the moments
involved in the GMM model. The asymptotic equivalence between this GEL Bayes factor and a
model and moment selection criteria is given in the following proposition.

Proposition 1 Suppose assumptions (1), (2) and (3) hold,     assume also that there are interior
points in the parameter space Î»0 and Î²0 such that T QÌ‚ Î²Ì‚, Î»Ì‚ âˆ’ Q (Î²0 , Î»0 ) = Op (1), where Q (Î², Î»)
is the uniform probably limit of QÌ‚ (Î², Î») /T . Then
                                          1
                  log GELBF = QÌ‚ Î²Ì‚, Î»Ì‚ + (dim (Î») âˆ’ dim (Î²)) log T + Op (1) .
                                               2

As a consequence of this proposition, asymptotically the GEL Bayes factor puts all weight on models
with the best fit Q (Î²0 , Î»0 ), and among models with equal best fit, the one with the largest number of
over-identifying moment conditions dim (Î») âˆ’ dim (Î²). Model selection behaviors are undetermined
among models with equal fit and equal number of overidentifying moment conditions.
                                                                                                   21


7   Conclusion

This paper exploits the connections between Bayesian and classical predictions. For predictions
based on minimization of a general class of nonlinear loss functions, we demonstrated conditions
under which the asymptotic distribution properties of prediction intervals are not affected by model
averaging and the posterior odds place asymptotically unitary probability weight on a single model.
This establishes an analogue to the well-known classical result: that asymptotical distribution
properties of post-selection estimation and prediction are not affected by first-stage model selection
so long as the model selection criterion is consistent.

Of course, when confronted with multiple misspecified models, it is clear that there is no unique
approach to model selection. While the number of parameters might appropriately capture model
parsimony in the nested case, this may not necessarily be true in the nonnested case. There may
be better ways to capture model complexity in this instance. Exploring possible criteria is left for
future work.

This paper should not be interpreted as claiming that model selection criteria can be used to the
exclusion of standard measures of model fit. For example, there are a number of single model
specification tests available â€” see, among others, Newey (1985), Fan (1994) and Hansen (1982) â€”
which should be applied in assessing model fit. Even the best model chosen by model comparison
methods may be rejected by such specification tests indicating that all models are far from the
true data generating process (see Sims (2003)). Indeed, as emphasized by Gourieroux and Monfort
(1995), hypothesis testing procedures may lead to the simultaneous acceptance or rejection of two
nonnested hypothesis. The former may reflect a lack a data while the latter may suggest the testing
framework is misspecified.

It is also worth noting Bayesian inference often advocates the use of more general classes of loss
function for model evaluation â€“ see Schorfheide (2000) for a recent discussion and promotion of such
an approach. For instance, one potential criticism of ranking models based only on statistical fit and
parameter parsimony, is that such criterion could well give rise to perverse policy recommendations
if the selected model fails to capture important components for the transmission mechanism in the
case of monetary policy. However, this will in general be true for any proposed criterion for ranking
models, whether decision theoretic or purely statistical, and only serves to emphasize that analysis
of this kind is never meant to supersede sound economic reasoning.

What this paper has attempted to treat carefully is whether models can be statistically distin-
guished, given a plausible set of models, finite sample and a particular set of assumptions. It
remains as further work to analyze the statistical properties of general classes of decision theoretic
                                                                                                22


approaches to model selection and whether said approaches resolve the inconsistency of posterior
odds ratios under our assumptions.


References
Andrews, D. (1994): â€œEmpirical Process Methods in Econometrics,â€ in Handbook of Econometrics, Vol.
 4, ed. by R. Engle, and D. McFadden, pp. 2248â€“2292. North Holland.

Andrews, D., and B. Lu (2001): â€œConsistent model and moment selection procedures for GMM estimation
 with application to dynamic panel data models,â€ Journal of Econometrics, 101, 123â€“164.

Attanasio, O., and H. Low (forthcoming): â€œEstimating Euler Equations,â€ Review of Economic Dynamics.

Berry, S., J. Levinsohn, and A. Pakes (1995): â€œAutomobile Prices in Market Equilibrium,â€ Economet-
  rica, 63, 841â€“890.

Carroll, C. D. (2001): â€œDeath to the Log-Linearized Consumption Euler Equation! (And Very Poor
  Health to the Second-Order Approximation),â€ Advances in Macroeconomics.

Chernozhukov, V., and C. Hansen (2005): â€œAn IV Model of Quantile Treatment Effects,â€ Econometrica,
  73(1), 245â€“261.

Chernozhukov, V., and H. Hong (2003): â€œA MCMC Approach to Classical Estimation,â€ Journal of
  Econometrics, 115(2), 293â€“346.

Clark, T. E., and M. W. McCracken (2006): â€œCombining Forecasts from Nested Models,â€ working
  paper, Federal Reserve Bank of Kansas.

Fan, Y. (1994): â€œTesting the Goodness of Fit of a Parametric Density Function by Kernel Me thod,â€
  Econometric Theory, 10, 316â€“356.

Fernandez-Villaverde, J., and J. F. Rubio-Ramirez (2003): â€œEstimating Nonlinear Dynamic Equi-
  librium Economies: Linear versus Nonlinear Likelihood,â€ unpublished, University of Pennsylvannia.

       (2004a): â€œComparing Dynamic Equilibrium Models to Data: A Bayesian Approach,â€ Journal of
  Econometrics, 123, 153â€“187.

         (2004b): â€œEstimating Nonlinear Dynamic Equilibrium Economies: A Likelihood Approach,â€ Uni-
  versity of Pennsylvannia, PIER Working Paper 04-001.

Gourieroux, C., and A. Monfort (1995): Statistics and Econometric Models. Cambridge University
 Press, Cambridge, UK.

Hansen, L. (1982): â€œLarge Sample Properties of Generalized Method of Moments Estimators,â€ Economet-
 rica, 50(4), 1029â€“1054.
                                                                                                      23


Hong, H., B. Preston, and M. Shum (2003): â€œGeneralized Empirical Likelihood-Based Model Selection
 Criteria for Moment Condition Models,â€ Econometric Theory, 19, 923â€“943.

Justiniano, A., and B. Preston (2004): â€œNew Open Economy Macroeconomics amd Imperfect Pass-
  through: An Emprical Analysis,â€ unpublished, Columbia University and International Monetary Fund.

Lubik, T. A., and F. Schorfheide (2003): â€œDo Central Banks Respond to Exchange Rate Movements?
  A Structural Investigation,â€ unpublished, Johns Hopkins University and University of Pennsylvania.

Newey, W. (1985): â€œMaximum Likelihood Specification Testing and Conditional Moment Tests,â€ Econo-
  metrica, pp. 1047â€“1070.

Newey, W., and D. McFadden (1994): â€œLarge Sample Estimation and Hypothesis Testing,â€ in Handbook
  of Econometrics, Vol. 4, ed. by R. Engle, and D. McFadden, pp. 2113â€“2241. North Holland.

Pakes, A., and D. Pollard (1989): â€œSimulation and the Asymptotics of Optimization Estimators,â€
  Econometrica, 57(5), 1027â€“1057.

Paxson, C. H., and S. Ludvigson (1999): â€œApproximation Bias in Euler Equation Estimation,â€ NBER
  Working Paper No. T0236.

Pollard, D. (1991): â€œAsymptotics for Least Absolute Deviation Regression Estimator,â€ Econometric
  Theory, 7, 186â€“199.

Powell, J. L. (1986): â€œCensored Regression Quantiles,â€ Journal of Econometrics, 32, 143â€“155.

Schorfheide, F. (2000): â€œLoss Function-Based Evaluation of DSGE Models,â€ Journal of Applied Econo-
  metrics, 15, 645â€“670.

Sims, C. (2001): â€œTime Series Regression, Schwartz Criterion,â€ Lecture Note, Princeton University.

         (2003): â€œProbability Models for Monetary Policy Decisions,â€ unpublished, Princeton University.

Sin, C. Y., and H. White (1996): â€œInformation Criteria for Selecting possibly misspecified parametric
  models,â€ Journal of Econometrics, 71, 207â€“225.

Smets, F., and R. Wouters (2002): â€œAn Estimated Dynamics Stochastic General Equilibrium Model of
  the Economy,â€ National Bank of Belgium, Working Paper No. 35.

Stock, J. H., and M. W. Watson (2001): â€œForecasting Output and Inflation: The Role of Asset Prices,â€
  NBER Working Paper 8180.

         (2003): â€œCombination Forecasts of Output Growth in a Seven-country Data Set,â€ unpublished,
  Princeton University.

Timmermann, A. (2005): â€œForecast Combinations,â€ in Handbook of Economic Forecasting, ed. by C. W. G.
  Graham Elliott, and A. Timmermann. North Holland.
                                                                                                        24


Vuong, Q. (1989): â€œLikelihood-ratio tests for model selection and non-nested hypotheses,â€ Econometrica,
 pp. 307â€“333.

Wright, J. H. (2003): â€œBayesian Model Averaging and Exchange Rate Forecasts,â€ Board of Governers of
 the Federal Reserve System, International Finance Discussion Papers, No. 779.



A    Proof of Theorem 1

It has been shown (e.g. Pakes and Pollard (1989), Newey and McFadden (1994) and Andrews
(1994)) that under assumptions 1, 2 and 3,
                                   âˆš               âˆ†T
                                    T Î²Ì‚ âˆ’ Î²0 = âˆ’Aâˆ’1
                                                  Î² âˆš + op (1) .
                                                      T
Combined with assumption 3, this implies that
                       âˆš        0 âˆ†
                                        T 1âˆš        0    âˆš        
       QÌ‚ Î²Ì‚ âˆ’ QÌ‚ (Î²0 ) = T Î²Ì‚ âˆ’ Î²0 âˆš âˆ’     T Î²Ì‚ âˆ’ Î²0 (JT ) T Î²Ì‚ âˆ’ Î²0 + op (1) ,
                                        T 2
               
because of RT Î²Ì‚ = op (1), we can write

                                                 1 âˆ†T 0   âˆ†
                               QÌ‚ Î²Ì‚ âˆ’ QÌ‚ (Î²0 ) = âˆ’ âˆš Aâˆ’1 Î² âˆš T + op (1) .
                                                   2 T       T
                                                          âˆ†T
The conclusion follows from the assumptions that          âˆš
                                                            T
                                                                = Op (1) and that âˆ’AÎ² is positive definite.



B    Proof of Theorem 3
                                                 âˆš           
Define Î²ÌƒT = Î²0 âˆ’ T1 Aâˆ’1
                      Î²  âˆ†  T , and define  h  =  T  Î²  âˆ’ Î²ÌƒT   . Then through a change of variables, we
can write
                               Z                  Z     â€œ         â€          
                       dim(Î²)
                                  QÌ‚(Î²)               QÌ‚ âˆšh +Î²ÌƒT       h
                     T    2      e      Ï€ (Î²) dÎ² = e       T       Ï€ âˆš + Î²ÌƒT dh.
                                                                        T
Chernozhukov and Hong (2003) has shown that (equation A5 of p326) under the same set of
assumptions:
                    Z     â€œ       â€                               
                        QÌ‚ âˆšh +Î²ÌƒT âˆ’QÌ‚(Î²0 )+ 2T
                                              1
                                                âˆ†0T Aâˆ’1
                                                     Î² âˆ†T
                                                             h
                      e      T                            Ï€ âˆš + Î²ÌƒT dh
                                                              T
                           p               dim(Î²)
                         âˆ’â†’ Ï€ (Î²0 ) (2Ï€)      2     det (âˆ’AÎ² )âˆ’1/2 .
                                                                                                                  25


Therefore the proof for theorem 3 will be completed if one can show that
                                             1 0 âˆ’1
                                                             
                                                                 p
                            QÌ‚ Î²Ì‚ âˆ’ QÌ‚ (Î²0 ) âˆ’    âˆ†T AÎ² âˆ†T âˆ’â†’ 0,
                                               2T
where Î²Ì‚ is the conventional M estimator, defined as (see Pakes and Pollard (1989)),
                                                            
                                QÌ‚ Î²Ì‚ = inf QÌ‚ (Î²) + op T âˆ’1/2 .
                                                    Î²

This is indeed the conclusion of Theorem 3.                                                                       


C        Proof of Theorem 2
                          âˆš                                                                                âˆš
Define ÏˆÌ‚ (yÌ„, YT ) =      T Î»Ì‚ (yÌ„, YT ) âˆ’ Î»Ìƒ (yÌ„, YT ) so that Î»Ì‚ (yÌ„, YT ) = Î»Ìƒ (yÌ„, YT ) + ÏˆÌ‚ (yÌ„, YT ) / T . By
definition, ÏˆÌ‚ (yÌ„, YT ) minimizes the following loss function with respect to Ïˆ,
                                 Z                             
                                                           Ïˆ
                                    ÏÌ„ yÌ„, Î»Ìƒ (yÌ„, YT ) + âˆš ; Î² f (Î²|YT ) dÎ².
                                                            T
                     âˆš        
Also define h â‰¡ T Î² âˆ’ Î²Ì‚ as the localized parameter space around Î²Ì‚. The implied density for
localized parameter h is given by
                                                1 dim(Î²)
                                                                       
                                                                    h
                                 Î¾ (h) = âˆš                  f Î²Ì‚ + âˆš |YT .
                                                 T                   T
Then ÏˆÌ‚ (yÌ„, YT ) also minimizes the equivalent loss function of
                                       Z                         
                                                     Ïˆ         h
                            QT (Ïˆ) = ÏÌ„ yÌ„, Î»Ìƒ + âˆš ; Î²Ì‚ + âˆš         Î¾ (h) dh
                                                      T         T
where we are using the shorthand notations Î»Ì‚ = Î»Ì‚ (yÌ„, YT ) and Î»Ìƒ = Î»Ìƒ (yÌ„, YT ). For a given Ïˆ, we are
interested in the asymptotic behavior of QT (Ïˆ) as T â†’ âˆž. Define
                                         Z                    
                                    Ïˆ                         h                           
             QÌ„T (Ïˆ) = ÏÌ„ yÌ„, Î»Ìƒ + âˆš ; Î²Ì‚ + ÏÌ„ yÌ„, Î»Ìƒ; Î²Ì‚ + âˆš      Î¾ (h) dh âˆ’ ÏÌ„ yÌ„, Î»Ìƒ; Î²Ì‚ .       (C.3)
                                     T                         T
Essentially, QÌ„T (Ïˆ) is a first order approximation to QT (Ïˆ). Under the assumptions stated in
Theorem 2, it can be shown that for each Ïˆ, 2
                                                       p
                                     T QT (Ïˆ) âˆ’ QÌ„T (Ïˆ) âˆ’â†’ 0.
                                                                                      â€œ        â€
    2
        As T â†’ âˆž, Î¾ (h) converges in a strong total variation norm in probability to Ï† h; âˆ’Aâˆ’1
                                                                                            Î²   , the multivariate
normal density with mean 0 and variance âˆ’Aâˆ’1
                                          Î² . In fact, the proof of Theorem 3 shows that
                                           Z
                                             hÎ± Î¾ (h) dh = Op (1) ,
                                                                                         `         Â´
for all Î± â‰¥ 0. This, combined with the stated assumption that the differentiability of ÏÌ„ YÌ„ , Î»; Î² with respect to Î»
and Î², implies the stated convergence in probability.
                                                                                                           26


Because QT (Ïˆ) and QÌ„T (Ïˆ) are both convex in Ïˆ, and since QÌ„T (Ïˆ) is uniquely minimized at Ïˆ â‰¡ 0,
the convexity lemma (e.g. Pollard (1991)) is used to deliver the desired result that
                                  âˆš                            p
                             ÏˆÌ‚ = T Î»Ì‚ (yÌ„, YT ) âˆ’ Î»Ìƒ (yÌ„, YT ) âˆ’â†’ 0.


An alternative proof can be based on a standard Taylor expansion of the first order conditions that
define Î»Ì‚ (yÌ„, YT ) and Î»Ìƒ (yÌ„, YT ). This is straightforward but the notations will be more complicated.
End of proof of theorem 2.                                                                             


D     Proof of Theorem 5

It is clear from Theorem 3 and its following discussions that under either one of the stated condi-
tions,
                                             BFQ     p
                                   wQ â‰¡              â†’1           as   T â†’ âˆž.
                                           BFQ + BFL

It is because from Theorem 3, for constants

                                 CQ = PQ Ï€ (Î²0 ) (2Ï€)dim(Î²)/2 det (âˆ’AÎ² )âˆ’1/2

and

                                 CL = PL Ï€ (Î±0 ) (2Ï€)dim(Î±)/2 det (âˆ’AÎ± )âˆ’1/2

we can write

                                              CL eLÌ‚(Î±Ì‚) T âˆ’ dim(Î±)/2 (1 + op (1))
               1 âˆ’ wQ =
                          CQ eQÌ‚(Î²Ì‚ ) T âˆ’ dim(Î²)/2 (1 + op (1)) + CL eLÌ‚(Î±Ì‚) T âˆ’ dim(Î±)/2 (1 + op (1))
                                                              dÎ² âˆ’dÎ±
                                        CL LÌ‚(Î±Ì‚)âˆ’QÌ‚(Î²Ì‚ )
                           (1 + op (1)) C Q
                                            e             T 2
                     =                                       dÎ² âˆ’dÎ±
                                                                     .
                                           CL LÌ‚(Î±Ì‚)âˆ’QÌ‚(Î²Ì‚ )
                       (1 + op (1)) 1 + CQ e                 T 2

           p
While wQ â†’ 1 under the stated conditions, the specific rate of convergence depends on the specific
condition stated in Theorem 3. Under condition 1, It is clear that âˆƒ Î´ > 0 such that with probability
converging to 1, for all T large enough,

                                                 1 âˆ’ wQ < eâˆ’T Î´ .                                        (D.4)
                                                                                                        27

                                                                     
On the other hand, when condition 2 holds, we know that LÌ‚ (Î±Ì‚) âˆ’ QÌ‚ Î²Ì‚ converges to a quadratic
norm of a normal distribution. It can then be shown that
                                            dÎ± âˆ’dÎ²
                                                                 d   CL
                                                                        exp Ï‡Ì„2 ,
                                                                               
                                        T      2     (1 âˆ’ wQ ) âˆ’â†’                                    (D.5)
                                                                     CQ

where Ï‡Ì„2 is typically distributed as the quadratic form of a random vector such as described in
Vuong (1989).

Using the definition of ÏÌ„ (yÌ„, Î»; Î²), and define ÏÌ„ (yÌ„, Î»; Î±) similarly, we can write Î»Ì‚ (yÌ„, YT ) as the
minimizer with respect to Î» of
                  Z                                            Z
              wQ ÏÌ„ (yÌ„, Î»; Î²) f (Î²|YT , Q) dÎ² + (1 âˆ’ wQ ) ÏÌ„ (yÌ„, Î»; Î±) f (Î±|YT , L) dÎ±.

                                         âˆš                                          âˆš       
As before, define ÏˆÌ‚ (yÌ„, YT ) =          T Î»Ì‚ (yÌ„, YT ) âˆ’ Î»Ìƒ (yÌ„, YT ) and define h â‰¡ T Î² âˆ’ Î²Ì‚ .    Then
ÏˆÌ‚ (yÌ„, YT ) equivalently minimizes, with respect to Ïˆ,

                                       QT (Ïˆ) =wQ Q1T (Ïˆ) + (1 âˆ’ wQ ) Q2T (Ïˆ)

where, with Î»Ìƒ â‰¡ Î»Ìƒ (yÌ„, YT ),
                                               Z                             
                                                                  Ïˆ        h
                                 Q1T (Ïˆ) =           ÏÌ„ yÌ„, Î»Ìƒ + âˆš ; Î²Ì‚ + âˆš     Î¾ (h) dh,
                                                                   T        T
and
                                               Z                     
                                                                  Ïˆ
                                 Q2T   (Ïˆ) =         ÏÌ„ yÌ„, Î»Ìƒ + âˆš ; Î± f (Î±|YT , L) dÎ±.
                                                                   T

Now recall the definition of QÌ„T (Ïˆ) in equation (C.3) in the proof of theorem 2. Also define
                                                                         
                            QÌƒT (Ïˆ) = wQ QÌ„T (Ïˆ) + (1 âˆ’ wQ ) ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚ .

We are going to show that with this definition
                                                    p
                                  T QT (Ïˆ) âˆ’ QÌƒT (Ïˆ) âˆ’â†’ 0.                                           (D.6)

If (D.6) holds, it then follows again from the convexity lemma of Pollard (1991) and the fact that
QÌƒT (Ïˆ) is uniquely optimized at Ïˆ = 0 that
                                           âˆš                             p
                             ÏˆÌ‚ (yÌ„, YT ) = T Î»Ì‚ (yÌ„, YT ) âˆ’ Î»Ìƒ (yÌ„, YT ) âˆ’â†’ 0.
                                                                                                               28


Finally, we will verify (D.6). With the definition of QÌƒT (Ïˆ), we can write
                                                                                   
      T QT (Ïˆ) âˆ’ QÌƒT (Ïˆ) = T wQ Q1T (Ïˆ) âˆ’ QÌ„T (Ïˆ) + T (1 âˆ’ wQ ) Q2T (Ïˆ) âˆ’ ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚ .
                                                        

             p                                                                           p
Because wQ â†’ 1, it follows from the proof of Theorem 2 that wQ T Q1T (Ïˆ) âˆ’ QÌ„T (Ïˆ) â†’ 0. As the
sample size increases, f (Î±|YT , L) tends to concentrate on Î±Ì‚, therefore it can also be shown that
                                                             p
                                      Q2T (Ïˆ) âˆ’ ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚ â†’ 0.

Now if either condition 1 holds or if condition 2 holds and dÎ± âˆ’ dÎ² > 2, then because of (D.4) and
                    p
(D.5), T (1 âˆ’ wQ ) âˆ’â†’ 0 and the second term vanishes in probability. Finally, in the last case where
dÎ± = dÎ² + 2 under condition 2, we know from equation (D.5) that
                                               CL   d
                                                   exp Ï‡Ì„2 = Op (1) .
                                                              
                                   T (1 âˆ’ wQ ) âˆ’â†’                                               (D.7)
                                               CQ
                                                            
Hence it is also true that T (1 âˆ’ wQ ) Q2T (Ïˆ) âˆ’ ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚ = op (1). Therefore (D.6) holds.    

Remark: It also follows from the same arguments as above that the results of Theorem 5 does not
hold when dÎ± = dÎ² + 1. In fact, in this case, we can redefine
                                                                               Ïˆ 
                                                               âˆ‚ 
               QÌƒT (Ïˆ) = wQ QÌ„T (Ïˆ) + (1 âˆ’ wQ ) ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚ +    ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚ âˆš    .
                                                                âˆ‚Î»                 T
We can then follow the same logic as before to show that
                                                      p
                                  T QT (Ïˆ) âˆ’ QÌƒT (Ïˆ) âˆ’â†’ 0.

Note that in the definition of
                             QÌ„T (Ïˆ) in equation
                                                 (C.3) of Theorem 2, using a second order Taylor
                                      Ïˆ
expansion we can replace ÏÌ„ yÌ„, Î»Ìƒ + âˆšT ; Î²Ì‚ by
                                                                       
                                        1 01                         1
                                         Ïˆ ÏÌ„Î»Î» yÌ„, Î»Ìƒ; Î²Ì‚ Ïˆ + op         .
                                        T 2                            T
As such we can write
                         Z                                                                          
                                              h                                                  
        T QÌƒT (Ïˆ) âˆ’ wQ       ÏÌ„ yÌ„, Î»Ìƒ; Î²Ì‚ + âˆš     Î¾ (h) dh âˆ’ ÏÌ„ yÌ„, Î»Ìƒ; Î²Ì‚     âˆ’ (1 âˆ’ wQ ) ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚
                                               T
         1                âˆš                âˆ‚              
       = Ïˆ 0 ÏÌ„Î»Î» Î»Ìƒ; Î²Ì‚ Ïˆ + T (1 âˆ’ wQ )        ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚ Ïˆ + op (1) .
         2                                   âˆ‚Î»
                                                                           p
                                                            âˆ‚                   âˆ‚
It follows from both (D.7) and the convergence of âˆ‚Î»          ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚ âˆ’â†’ âˆ‚Î» ÏÌ„ (yÌ„, Î»0 ; Î±0 ) that

                      âˆš                 âˆ‚              
                                                          d CL          âˆ‚ 0
                          T (1 âˆ’ wQ )      ÏÌ„ yÌ„, Î»Ìƒ; Î±Ì‚ âˆ’â†’    exp Ï‡Ì„2       ÏÌ„ (yÌ„, Î»0 ; Î±0 )0
                                        âˆ‚Î»                  CQ          âˆ‚Î»
                                                                                                    29


where Î»0 = arg minÎ» ÏÌ„ (yÌ„, Î»; Î²0 ). Hence again with convexity arguments for uniform convergence
we can show that
                           d           1                     CL              âˆ‚
             ÏˆÌ‚ (yÌ„, YT ) âˆ’â†’ arg min Ïˆ 0 ÏÌ„Î»Î» (Î»0 ; Î²0 ) Ïˆ +     exp Ï‡Ì„2           ÏÌ„ (yÌ„, Î»0 ; Î±0 )0 Ïˆ
                                   Ïˆ 2                       CQ                âˆ‚Î»
                                                  CL          âˆ‚
                            = âˆ’ ÏÌ„Î»Î» (Î»0 ; Î²0 )âˆ’1    exp Ï‡Ì„2      ÏÌ„ (yÌ„, Î»0 ; Î±0 )0 .
                                                  CQ          âˆ‚Î»
                                       âˆš                          
In other words, if dÎ² = dÎ± âˆ’ 1, T Î»Ì‚ (yÌ„, YT ) âˆ’ Î»Ìƒ (yÌ„, YT ) converges in distribution to a non
degenerate random variable.
