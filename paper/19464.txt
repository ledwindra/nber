NBER WORKING PAPER SERIES

INSTRUCTION TIME, CLASSROOM QUALITY, AND ACADEMIC ACHIEVEMENT
Steven G. Rivkin
Jeffrey C. Schiman
Working Paper 19464
http://www.nber.org/papers/w19464

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2013

We thank Marcus Casey, Robert Kaestner, Cuiping Long, Darren Lubotsky, Ben Ost, Houston Stokes,
Javaeria Qureshi and participants at the UIC economics research lunch and the National Institute for
Educational Evaluation in the Ministry of Education of Spain for helpful comments. Rivkin thanks
the US Department of Education, Institute for Education Sciences for financial support. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau
of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2013 by Steven G. Rivkin and Jeffrey C. Schiman. All rights reserved. Short sections of text, not
to exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.

Instruction Time, Classroom Quality, and Academic Achievement
Steven G. Rivkin and Jeffrey C. Schiman
NBER Working Paper No. 19464
September 2013
JEL No. I21,I24,I25,I28
ABSTRACT
Many countries, American jurisdictions and charter schools have recently embraced longer school
days or more time devoted to core academic classes. Recent research generally supports the notion
that additional time raises achievement, though difficulties isolating an exogenous source of variation
raise questions about the strength of much of the evidence. Moreover, it seems likely that the magnitude
of any causal link between achievement and instruction time depends upon the quality of instruction,
the classroom environment, and the rate at which students translate classroom time into added knowledge.
In this paper we use panel data methods to investigate the pattern of instruction time effects in the
2009 Programme for International Student Assessment (PISA) data. The empirical analysis shows
that achievement increases with instruction time and that the increase varies by both amount of time
and classroom environment. These results indicate that school circumstances are important determinants
of the likely benefits and desirability of increased instruction time.

Steven G. Rivkin
Department of Economics
University of Illinois at Chicago
601 South Morgan UH725 M/C144
Chicago, IL 60607
and NBER
sgrivkin@uic.edu
Jeffrey C. Schiman
University of Illinois at Chicago
jschim2@uic.edu

I. Introduction
The belief that increased time on task raises output would go unchallenged in
most settings, but public schooling is an exception. Arguments of extensive inefficiencies
that dampen the return to additional time or spending are widespread. Nonetheless, many
countries and American jurisdictions have recently embraced longer school days or more
time devoted to core academic classes. The conceptual appeal is clear: additional time
allows teachers to “cover more material and examine topics in greater depth and in
greater detail, individualize and differentiate instruction, and answer students’ questions”
(Farbman 2012).
Many point to KIPP Academy schools for evidence of the benefits of extended
time in class. Through a longer school day and Saturday school, instruction time averages
around 1,700 hours per year in KIPP schools, roughly 60% more than the US average,
and evidence suggests that KIPP students significantly outperform similar students in
regular public schools (Farbman 2011).1 Of course KIPP academy schools differ along
other dimensions as well so it is difficult to isolate the specific mechanisms that account
for KIPP’s apparent success.2
Recent research on instruction time generally supports the notion that additional
time raises achievement, though difficulties isolating an exogenous source of variation
raise questions about the strength of much of the evidence.3 To illustrate the empirical
difficulty consider the difference between academic and vocational secondary schooling.

1

See http://www.kipp.org/our-approach for more details about how KIPP academy schools operate.
In a recent paper, Angrist et al. (2010) attempt to isolate the contributions of various factors to the
educational success of KIPP students.
3
Recent work includes (Coates 2003; Gijselaers and Schmidt 1995; Kuehn and Landeras 2012; Lavy
2010; Lavy 2012; Mandel and Süssmuth 2011; Marcotte 2007; Marcotte and Hemelt 2008; Roland G.
Fryer 2011; and Wiermann 2005). Lavy (2010) emphasizes the identification problem and adopts an
empirical approach that provides a foundation for our work in this paper.
2

2

Academic schools typically spend more time in mathematics and language arts
instruction and boast higher achievement than vocational schools, but instruction time
differences across sectors does not provide a valid source of identification because of the
positive selection into academic schools. Alternatively, instruction-time variation
resulting from the desire to supplement the education of lower achievers would tend to
produce downward biased estimates. Thus it is not possible to determine a priori if the
simple correlation between achievement and instruction time overstates or understates the
causal relationship.
Moreover, it seems likely that the magnitude of any causal link between
achievement and instruction time depends upon the quality of instruction, the classroom
environment, and the rate at which students translate classroom time into added
knowledge. For example, expanded instruction time in response to poor mathematics
achievement may have little impact if an ineffective curriculum, inadequate teacher
subject matter knowledge, or disruptive behavior led to the low achievement in the first
place. Furthermore, even if existing class time is effective, there may be decreasing
marginal benefits to additional minutes if the quality of instruction, classroom
environment, or student effort diminishes with time.
In this paper we focus on such potential heterogeneity and investigate the pattern
of instruction time effects using the 2009 Programme for International Student
Assessment (PISA) data. We are particularly interested in the mediating effects of teacher
quality and the classroom environment and the character of any diminishing returns. In
order to overcome biases introduced by the non-random allocation of instruction time and

3

unobserved differences in school quality, we build on the work of Lavy (2010) and use
within-school variation across subjects or grades to identify the effects.
The appeal of a focus on within school variation across subjects is the fact that
students taking both mathematics and language arts (for example) bring the same general
skills and experience as well as the same school environment for each subject. Therefore
neither student heterogeneity in general ability and work habits nor general school quality
will contaminate estimates of instruction-time effects identified from average instructiontime differences between subjects. This leaves only subject specific factors related to
instruction time as potential confounding factors, and the focus on school average rather
than individual instruction time differences mitigates problems introduced by
consideration of subject-specific skills when determining course placement.
The possibility that school quality or school average ability differences by subject
could exist and either precipitate or result from instruction-time differences remains, and
therefore we also consider grade differences within subject and school as an alternative
source of variation. This comparison is complicated by the fact that 9th graders have one
fewer year of schooling than 10th graders, and clearly it is not the same students in both
grades. Nonetheless as we discuss below, as long as skill and instruction-time differences
across cohorts are not related, the within-subject variation produces lower bound
estimates.
The empirical analysis shows that achievement increases with instruction time
and that the increase varies by both amount of time and classroom environment. First,
there is evidence of diminishing returns, though the rate of decrease appears to be quite
gradual. Second, there is evidence that better classroom environment as indexed by

4

responses to questions about student behavior and student-teacher interactions also
appears to raise the benefit of additional instruction time. These results indicate that
school circumstances are important determinants of the likely benefits and desirability of
increased instruction time.

II. Data
The data come from the 2009 Programme for International Student Assessment
(PISA), a survey and assessment administered to fifteen year old students around the
world. At least 150 schools are randomly selected in each country based on a stratified
sample design, and within each school, 35 students that are 15 years old are sampled at
random. Each student is assessed in mathematics, science, and language arts and then
answers a set of questions about family background, school environment, home
environment, and study habits.4 The PISA test focuses on knowledge applications and is
thought to be highly informative about the quality of preparation for higher education and
the labor market. A representative from each school also provides information on staff,
environment, and pedagogical and human resource practices.
We focus on mathematics and language arts separately from science and language
arts because the quality of mathematics education likely affects performance on the
science examination. Because our research design identifies instruction time effects on
the basis of between subject differences in test scores and instruction time, this potential
4

Each student is assigned five achievement measures for each subject called plausible values. To estimate
regressions using plausible values, one must estimate separate regressions with each of the five plausible
values and then average across the estimates. Estimating separately by plausible value may give different
results in smaller samples (e.g. samples less than 6,000), but in samples larger than 6,000, practically
speaking, the estimates will be very similar (Adams and Wu 2002). Here we present estimates based on the
first plausible value through the estimates are insensitive to choice of plausible value. Upon request, tables
are available from the authors.

5

spillover is especially problematic. We expect there to be little mathematics and language
arts spillover and little science to language arts spillover at the high school level. In the
appendix, we provide results based on within school and grade science and language arts
differences.
The PISA test was administered in 2000, 2003, 2006, and 2009, and we use the
2009 wave because of the richness of information on instruction time and availability of
measures about classroom environment and instructional quality. In 2009, students are
asked the number of mathematics, science, and language arts classes attended per week
and the length in minutes of an average class. This potentially permits us to separately
identify the effects of additional classes per week and minutes per class, but in reality
there is little variation in average class length across subjects. To calculate average
instruction time for each subject, grade, and school, we multiply the average weekly
number of classes by the length of an average class.
The variability and range of responses to the instruction time questions does raise
concerns about data quality and measurement that may not be fully addressed by
aggregation. Non-trivial numbers of students report more than ten classes per week in a
subject or class lengths of over two hours. In order to mitigate errors in variables bias, we
exclude information on classes per week or average length of classes from the school
average calculations if reported number of classes exceeds ten or class length exceeds
two hours.5 These restriction set to missing approximately 1% of student reported
information on instruction time.

5

We test the sensitivity of our estimates to different restrictions on weekly number of classes and average
class lengths. In general, the estimates are insensitive to how we restrict the data. Upon request, tables are
available from the authors.

6

By comparison, the 2006 data used in Lavy (2010) (at the time of his writing,
2009 data was not yet available) report instruction time categories only. In 2006, students
responded to weekly time spent in each subject in five intervals: “no time; less than 2
hours a week; 2 or more but less than 4 hours a week; 4 or more but less than 6 hours a
week; and 6 or more hours a week.” A clear disadvantage of this taxonomy is the absence
of detailed information on numbers of classes and minutes. In addition, the taxonomy
produces instructional time distributions that differ substantially from those for 2000 and
for 2009. While the majority of weekly instruction time would fall in the 2 to < 4 hour
category based on survey responses in 2000 and 2009, the distribution is more evenly
split between 2 to < 4 hour and the 4 to < 6 hour categories in 2006 (not shown), raising
concerns about the accuracy of student responses.
We use factor analysis to generate the indexes of classroom environment and
teacher quality based on a series of questions listed in Table 6 along with the factor
loadings. The index of the quality of the classroom environment comes from a series of
questions to school representatives that ask to “what extent the learning of students is
hindered by the following phenomenon.” The phenomena include disruption, other
aspects of student behavior, student-teacher interactions and other aspects of teacher
behavior. Respondents could check “not at all, some of the time, most of the time, or all
of the time.” A higher value on the index of quality reflects a classroom environment that
is more conducive to learning.
The absence of direct measures of instruction quality leads us to focus on
measures of the shortage of qualified teachers for subjects and the quality of teacher
personnel practices. Such questions are not ideal instruments to measure the quality of

7

instruction, as they are likely to provide fairly noisy information about teacher
performance. Moreover, personnel practices are likely to be related to other aspects of
school operations such as the processes through which curricula are chosen and budgets
determined. Therefore the index of instruction quality may perhaps be interpreted more
accurately as a measure of the quality of management practices.
PISA test booklets are designed so that not every student takes both a
mathematics and language arts component. Instead, each student is randomly assigned
one of twenty-one test booklets, fourteen of which contain both mathematics and
language arts components.6 Those who do not take a math or language arts component
have their scores for these subjects imputed by the PISA test makers based on the
available assessments.7 Because our research design relies on information across subjects,
prior to aggregation we drop students who do not take both math and language arts. This
restriction drops approximately 30 percent of the sample. We also limit the analysis
sample to the same set of observations used in all regressions, which drops approximately
3,060 school-by-grade-by-subject observations.
The main sample used in this analysis includes 47,580 school-by-grade-by-subject
observations for 9th and 10th grade students in 16,154 schools in 72 countries. We focus
on these two grades in order to minimize complications introduced by grade retention and
to avoid cells with small numbers of students. Some components of the analysis restrict
the sample to only schools with both 9th and 10th grades.

6

A recent working paper by Borghans and Schils (2012) discusses in more detail the variation in PISA test
booklets in the 2006 data.
7
The variable “bookid” denotes which subjects are contained in each student’s test booklet. We drop those
with booklet ID 2, 4, 6, 13, 22, 24, and 26 because these booklets do not contain both math and language
arts components. The subject clusters and book IDs are described at
www.oecd.org/dataoecd/15/31/48580826.xls on Table 2.2

8

III. Empirical Model
This section describes the empirical framework used to investigate the effects of
instructional time on achievement. Conceptually, the empirical framework must address
potential biases introduced by confounding student and school factors and the likelihood
that the benefits of additional time vary by both the quality of instruction and classroom
environment. The association between instruction time on the one hand and both student
and school factors results from the fact that instruction time is determined by family
selection of schools, assignment of students to schools and courses of study, and systemic
rules about school operations. Academically oriented students are much more likely to
attend academic high schools that devote more class time to mathematics and language
arts. Schools may assign higher achievers in a subject to courses that meet more often, or
schools may assign struggling students to additional remedial sections. Governments
concerned about poor performance in mathematics may mandate a minimum amount of
instructional time, or governments with a strong commitment to mathematics may
mandate more class time along with higher salaries and stronger teacher training. Finally,
the analysis must consider possible endogenous family responses to realized school
quality, as additional instructional time outside of school can substitute for lower or less
productive school instruction time (Todd and Wolpin, 2003). Two things quickly become
clear: instruction time is likely to be related to a number of factors that may themselves
be determinants of achievement, and the direction of those relationships and therefore the
direction of any bias from unobserved factors is ambiguous.

9

By comparison, potential heterogeneity in returns to additional instruction time is
likely to be more predictable along at least two important dimensions. First, diminishing
returns to additional time are likely to set in at some point due to fatigue. Second,
extending the class time taught by an ineffective teacher is likely to yield little return, as
is extending time in a classroom plagued by disruption or poor relations between students
and teachers. It is these two dimensions that we explore in the empirical analysis.
IIIa. Baseline Empirical Model
Identification of the effect of instruction time on achievement requires exogenous
variation that is not related to unobserved differences in students and schools. Existing
research shows that available variables explain little of the variation in the quality of
instruction and student skill, and therefore it is necessary to account for unobserved
student and school factors. Fortunately, as Lavy (2010) points out, the testing of students
in multiple subjects enables the use of panel data methods that account for differences in
school and teacher quality, school climate, and student ability that span both subjects.
The instruction time effect can be identified by the difference in time devoted to
mathematics relative to language arts for each student, and all between student and school
differences in the allocation of instruction time can be ignored.
A potential problem with comparisons between class time in language arts and
mathematics is the purposeful placement of students into courses on the basis of subject
specific skills and interest. Weaker mathematics students are more likely to be placed in
lower level mathematics courses that could meet less frequently, and the data may not
contain information on that could be used to control for underlying math skill. Therefore
following Lavy (2010) we aggregate instruction time and test scores for each student to

10

the school-by-grade-by-subject level. Such aggregation eliminates the potential
confounding influence of within school variation in the mathematics-reading skill
differential.
In order to highlight the key assumptions underlying the various fixed effects
structures, we begin with a simple specification that ignores heterogeneity in the return to
class time. Equation (1) models achievement A in subject k in grade g in school s in
country c as a function of minutes per week of instruction M and a series of error
components that capture interactions among country, school, grade and subject. Note that
country is fully subsumed by school and is included as an interaction with grade and
subject to highlight the potential importance of country policies and practices regarding
curriculum, accountability, funding, and other factors.
(1)
The school-by-grade fixed effect ( ) accounts for differences in average ability,
level of disruption, and school quality that are common across subjects for students in a
particular cohort, grade, school, and country. Therefore all subject invariant differences
in academic skills and school quality at each grade level are removed which controls for
the primary confounding factors. Only within school and grade instruction-time
differences among subjects remain for identification. Note, importantly, that the schoolby-grade fixed effects fully account for a range of subject invariant influences including
national minimum school starting and leaving ages, school funding and governance
structures, and family background.
The school-by-grade fixed effect does not account for differences among subjects
in either instruction time or various other factors that could influence achievement. These

11

include, but are not limited to, national curricula, the quality of instruction in one subject
in relation to the other, and subject-specific student skills. The country-by-grade-bysubject fixed effect ( ) captures some such influences including national curricula, but it
does not capture subject-specific abilities or instructional quality specific to schools that
are related to instruction time. Our estimate of instruction time effects would be biased
upward if the difference between school-by-grade average mathematics and language arts
instruction time is positively related to the difference in average abilities in mathematics
and language arts, as would be the case if analytically skilled students attended schools
that devoted more time to mathematics instruction. A similar upward bias would arise if
the instruction difference was positively related to the difference in the quality of
mathematics versus language arts instruction. Of course, a negative relationship between
instruction time in a subject on the one hand and ability or instructional quality on the
other would introduce a negative bias.
It is not clear whether confounding subject-specific factors introduce bias.
Nonetheless, the availability of multiple grades per school enables an alternative
approach that accounts directly for school-by-subject factors. Rather than identifying
effects on the basis of within school-by-grade instruction-time differences across
subjects, effects can be identified on the basis of instruction-time differences across
grades for the same subjects. Essentially this amounts to including a school-by-subject
fixed effect into Equation (1) and excluding the school-by-grade fixed effect.
If instruction time is significantly related to achievement, a larger instruction time
difference in mathematics courses between 9th and 10th grade should be associated with a
larger difference in test scores. As opposed to the school-by-grade fixed effect

12

specifications, the school-by-subject specification accounts for subject-specific
differences among schools in both school quality and average student skills. However,
this advantage is potentially offset by the fact that 9th and 10th grade mathematics and
language arts scores are produced by different students who are not in the same grade
cohort. Importantly, the strict exogeneity assumption does not require equality in average
ability or the quality of instruction across grades but only that any differences are not
related to differences in grade-average instruction time. As long as the course schedule
for a subject and grade is not responsive to grade differences in student or teacher skills
one would not expect performance-induced changes in the course schedule to occur and
introduce bias.
However, inadequate treatment of learning dynamics can introduce correlation
between lagged instruction time and the error which violates the strict exogeneity
assumption. Unless learning fully depreciates each year, a better 9th grade education will
raise achievement in 10th grade as well as 9th grade. Therefore additional 9th grade
instruction time will tend to increase 10th grade achievement. As Meghir and Rivkin
(2011) illustrate, fixed effect estimates that are based on achievement differences across
grades will tend to introduce a downward bias in models that compare achievement in the
respective grades and do not account for prior achievement. Even though our analysis
does not compare achievement of the same student in successive grades, persistence in
the structure of instruction time across cohorts would still attenuate estimates based on
instructional time differences between 9th and 10th grade. Importantly, the direction of
bias introduced by this specification error is unambiguously toward zero, meaning that

13

the school-subject fixed effect estimates are likely to provide a lower bound of the true
average instruction time effect.
Persistence in the structure of instruction time across grades also complicates the
interpretation of the estimates from the school by grade specifications discussed above.
Specifically, identification based on mathematics and language arts instruction time
differences in 10th grade, for example, does not produce an unbiased estimate of the
effects of instruction time on achievement in 10th grade if the difference in 10th grade is
correlated with the difference in 9th grade. Rather the estimate would capture the effect of
instruction time in 10th grade plus persistent effects from differences in previous grades.
The magnitude and direction of the bias would depend upon the correlation between
instruction time differences in 9th and 10th grades. In this sample the correlation equals
0.42, indicating that the school by grade fixed effect estimates will tend to overstate the
effect of instruction time in a grade. Thus the true effect likely lies between the estimates
produced by the school by grade and those produced by the school by subject
specifications.
A final complication arises from the possibility that parents respond to school
inputs when determining family education inputs (Todd and Wolpin 2003). The direction
of bias that would arise from an endogenous family response is unclear. For example, if
parents judge the school to lack instruction time in a particular subject, they may
compensate by studying more with their child at home. Assuming that more parental help
is positively related to student achievement and negatively related to classroom
instruction time, failing to account for the endogenous parental response would tend to
bias downward the estimated effect of instructional time.

14

As an informal specification test, we include subject-specific measures of out-ofschool instruction time. The 2009 wave of PISA asks “How many hours do you typically
spend per week attending <out-of-school-time lessons> in the following subjects (at
school, at home or somewhere else)?” The student can respond “do not attend; less than
2 hours; 2 to 4 hours; 4 to 6 hours; or 6 or more hours.” We aggregate student responses
to these questions to the school-by-grade-by-subject level for the same reason we
aggregate instruction time. Appendix Table A3 shows that the inclusion of this variable
has little effect on the in-school instruction time estimate, providing evidence that any
such parental behavior may not introduce bias in this framework.
IIIb. Dimensions of Heterogeneity
We explore the possibility that there are diminishing returns to instruction time
and that the effect varies by classroom environment and the quality of instruction.
Instruction time varies by total number of minutes per week and by the division of that
time into classes, and it may matter if the 180 minutes per week are divided into four 45minute classes or three 60-minute classes. The information on number of classes per
week and minutes per class potentially enables the identification of diminishing returns
along each of these dimensions. In reality, there is very little variation across subjects in
the length of classes at a school, and therefore we focus on total minutes and use
quadratic and higher-order terms to investigate the presence and magnitude of
diminishing returns.
Identification of heterogeneity by classroom environment and the quality of
instruction requires measures of each. The surveys lack direct measures of student
behavior, and a growing body of evidence highlights the weakness of observed

15

characteristics as measures of teacher quality. School administrators do answer a series
of questions about student behavior and student-teacher relations, and we use the
responses to construct an index of classroom environment. Administrators also respond to
questions about teacher evaluation and support and whether there are teacher shortages in
specific subjects, and we use these responses to construct an index of instruction quality.
Specifically, the indexes come from separate factor analyses that take he ordinal
character of the responses into consideration. These indexes do not vary within schools,
but they can be interacted with instruction time to produce information on heterogeneity
in the return to additional instruction time by classroom environment and the quality of
instruction.
Table 6 lists the variables used to generate each factor and the factor loadings.
Single combinations of factors explain 89% and 99% of the variance in instructional and
classroom quality, respectively. The factor weightings illustrate the importance of the
teacher shortage indicators in the construction of the quality of instruction factor and the
high correlation of all student behavior and student-teacher interaction variables in the
construction of the classroom environment index. Thus it is not possible to separately
identify the effects of disruption, the quality of student-teacher interactions, student
attendance, or disrespectful behavior toward teachers or peers.

IV. Results
We report a series of estimates that characterize the relationship between
achievement and instruction time using the fixed effect specification described in the
previous section. Because a school’s class length tends not to vary across subject or

16

grade, we present results for both the number of classes and total minutes per week in
most tables. The initial set of results report the average effect of instructional time on
achievement. Subsequently we explore the existence of non-linear effects of both minutes
and classes, and this section concludes with the results of the investigation of potential
heterogeneity in the effects of instructional time by classroom environment and the
quality of instruction. Prior to presenting the fixed effect results we describe the withinschool variation across subjects in class time and achievement used to identify the
estimates.

IVa. Instructional Time Differences Between Subjects
In Table 1, we describe the joint-distribution of instruction time in mathematics
and language arts for both total weekly minutes and the number of classes. Although the
diagonal elements have the highest frequencies a substantial share of schools report
instructional time disparities between subjects. Consider first the top panel on weekly
minutes. Among students reporting language arts minutes between 200 and 219, only
slightly more than half report mathematics minutes that fall in the same category. Among
those with other than 200 to 219 minutes of mathematics instruction time, the majority
spends more time in mathematics than language arts classes. Not surprisingly, at higher
levels of language arts instructional time a larger share of students spend less as opposed
to more time in mathematics classes.
A similar pattern holds for classes per week, the primary source of within-school
instructional time variation. Students that attend four language arts classes per week are
more likely to attend five or more mathematics classes than fewer than four. However,

17

students that attend five language arts classes per week are less likely to attend six or
more mathematics classes than fewer than five.
Table 1 documents the existence of adequate within-school instructional time
variation to identify effects, and we now describe patterns of test score differences to
examine whether the raw test score data are consistent with the belief that longer classes
raise achievement. Table 2 reports differences in average test score (mathematics minus
language arts) by the joint distribution of mathematics and language arts instructional
time based on both minutes and classes per week. This table has the same structure as
Table 1 but replaces the cell shares with the average score differences.
A finding that entries above the diagonal (where instructional time for math
exceeds instructional time for language arts) tend to be more positive than entries along
the diagonal (where there is little or no difference between subjects) which in turn tend to
be more positive than entries below the diagonal (where instructional time for language
arts exceeds that for mathematics) would be consistent with a positive effect of
instructional time, and the pattern in Table 2 provides support for such an effect. In the
top panel there are only three negative entries above the diagonal, and Table 1 shows that
these are three of the smallest of the above-diagonal cells. In contrast, there are ten
negative entries below the diagonal including three of the six largest entries. Finally,
entries along the diagonal tend to fall in between those above and those below.
IVb. Baseline Estimates
This section begins with results from the basic models that estimate the average
effect of instructional time and then moves to results from models with a more flexible
parameterization of the relationship between achievement and time. All tables report

18

coefficients from specifications with school-by-grade fixed effects and specifications
with school-by-subject fixed effects as well as robust standard errors clustered by school.
The main sample includes 47,580 school-grade-subject cells, and roughly two thirds of
the sample contains schools with both 9th and 10th grade. Therefore the remaining one
third does not contribute to the identification of the estimates based on the school-bysubject fixed effect specification.
Table 3 reports estimates of the relationship between achievement and
instructional time as measured by both weekly minutes and the number of classes for
specifications without fixed effects, with school-by-grade fixed effects, and with schoolby-subject fixed effects. The two panels share a similar pattern of highly significant
estimates that decline by more than 60 percent with the inclusion of school-by-grade
fixed effects and another 30 percent when school-by-subject effects replace school-bygrade effects.
The smaller estimates from the specifications with school by subject fixed effects
are consistent with the issues raised in the previous section. Factors that could contribute
to the observed pattern include subject specific skills that are positively related to
instruction time and not accounted for in the specifications with school by grade fixed
effects, correlation between instruction-time differences in the current and prior grades
that inflate estimates from the school by grade fixed effect specifications, attenuation bias
in the specifications with school by subject fixed effects introduced by violation of the
assumption that 9th grade instructional time has no effect on 10th grade achievement, or
larger measurement error-induced attenuation bias in the models with school by subject
fixed effects.

19

Although there is little direct evidence exists on subject-specific skills, available
information suggests that the contribution of the other factors likely varies. On the one
hand, an analysis of residual variances in Appendix Table A2 find little or no evidence in
support of larger measurement error-induce attenuation bias in the school by subject fixed
effect specifications. On the other hand, available evidence does support the belief that
the effects of instruction time in prior years contribute to the observed pattern. First,
Jacob, Lefgren, and Sims (2008), Rothstein (2010), and Kain and Staiger (2008) find that
at least a portion of the knowledge acquired in a grade persists into the future. Second,
the correlation between school average instruction-time differences in ninth and tenth
grades equals 0.42 in the PISA data. Such persistence in effects and correlation in
instruction time differences leads the effects of instruction time in prior years to inflate
the school by grade fixed effect estimates and to attenuate the school by subject fixed
effect estimates as discussed in Section III.
Note that the instruction-time coefficient remains positive in the fully saturated
specification with both school by grade and school by subject fixed effects (not reported),
though the estimate is much smaller and quite imprecise. Unfortunately, the final column
in Appendix Table A2 shows that less than three percent of the variation in the schoolaverage instruction-time difference remains, consistent with the notion that there is
inadequate variation in actual instruction time to generate a precise, unbiased estimate not
attenuated by measurement error.
The instructional time measure provides another dimension over which
differences in magnitudes arise, as the magnitude of the effect is generally larger in the
regressions based on classes as opposed to weekly minutes. Consider the average class

20

length of roughly 50 minutes. The school-by-grade fixed-effect coefficient indicates that
the addition of one class per week would raise achievement by roughly 2 points on
average, while the addition of 50 minutes per week would raise achievement by roughly
1.25 points on average. Note, however, that this difference becomes much smaller in the
school-by-subject specifications. One interpretation is that the return to additional time
diminishes more quickly when classes are lengthened than when schools increase the
number of classes per week, though the lack of within school variation in class length
precludes a direct test of this hypothesis.
In Figure 1, we present additional evidence on the relationship between
instruction time and achievement. Each figure scatters the mean residuals from two
separate regressions where instruction time and achievement, respectively, are regressed
on country-by-school-by-grade effects and school-by-grade (left panels) or school-bysubject (right panels) fixed effects. In all the figures, the relationship between study time
and achievement is positive and strong, though as expected the slope is less steep in
regressions with school-by-subject fixed effects.
We now investigate the possibility of diminishing returns to additional minutes.
Table 4 reports results from the three specifications with weekly minutes entered as a
quadratic, and the results in both fixed effect specifications strongly support the
hypothesis of diminishing returns. Importantly, the return to additional minutes
diminishes quite slowly, becoming negative at over 500 minutes per week in both
specifications, a number that exceeds the 95th percentile.
Table 5 reports results from fixed effect specifications that group weekly minutes
and classes into seven and five categories respectively. Although both specifications

21

produce a generally positive relationship between achievement and minutes, there are
some inconsistencies. For example, in column (2) with school-by-subject fixed effects,
the highest category in both the minutes and classes specification the estimate is negative.
However, this category likely contains substantial error in measurement. Attendance in
greater than six classes per week may reflect efforts to remediate low performance or
may result from reporting error, and a number of observations that report weekly minutes
above 280 (80 minutes per day if students attend school six days per week) may also
suffer from reporting error.
IVc. Heterogeneity by classroom environment and the quality of instruction
The notion that the return to additional time depends crucially on the quality of
the learning environment fits with the emphasis on the role of disruption in education
production presented in Lazear (2001) and more general consideration of the quality of
teachers and schools. In this section we investigate the possibility of variation in the
return to instruction time by reported student and teacher behavior.
Because the instructional and classroom quality indexes do not vary within
schools, the direct effects on achievement cannot be identified. However, we can interact
these measures with the instructional time variables in order to investigate heterogeneity
in the returns to instruction time along these dimensions. The instructional quality index
ranges from 1.1 to 4.2 with a mean of 3.51, and the classroom environment measure
ranges from 1.2 to 4.7 with a mean of 3.45.
The results in Table 7 provide some support for the hypothesis that the return to
additional instructional time increases with the quality of the classroom environment and
no support for the hypothesis that the return increases with the quality of instruction. In

22

the school by grade fixed effect specifications the coefficients on the classroom
environment interactions are positive and significant at the 1 percent level. As expected,
the magnitude and significance is smaller in the school by subject specifications, where
the still positive coefficients are slightly less than half as large as those from the school
by grade fixed effect specifications. The small and insignificant coefficients on the
interaction with the quality of instruction is consistent with the notion that the quality of
instruction does not affect the return to additional class time, but we believe that the more
compelling interpretation is that our indirect measures fail to capture salient differences
in teacher effectiveness.
We evaluate the return to instruction time at the 25th, 50th, and 75th percentiles of
classroom quality using the school by grade fixed effect estimates and the distribution of
the classroom environment index. These suggest that an additional hour of weekly
instruction time raises achievement by more than twice as much at the 75th percentile
(0.025 standard deviations) than at the 25th percentile (0.011 standard deviations).
Schools with behaviors that place them in the lower tail in terms of classroom
environment therefore are likely to realize little or no benefit from increases in
instructional time. The fact that these survey questions provide noisy information about
the quality of the classroom environment including the degree of disruption raise the
possibility of much greater heterogeneity along this dimension as well as by the quality of
instruction.
The model of education production in Lazear (2001) suggests the possibility of a
nonlinear relationship between the level of disruption and learning, and we now examine
a more flexible specification that includes interactions with indicators for quartile of the

23

instructional quality and classroom environment distributions (the lowest quartile
interactions are excluded). Again there is little or no evidence of heterogeneity along our
measure of the quality of instruction. The school by grade fixed effect specification
produces a monotonically increasing return to additional instructional time as the quality
of the classroom environment increases, while the school by subject fixed effect
specification suggests that only the bottom quartile schools fail to receive the benefit
accruing to all others. An additional school by subject fixed effect regression that
interacts instruction time with an indicator for not being in the bottom classroom
environment quartile produces an interaction coefficient of 0.027 that approaches
significance at the 10 percent level (the standard error equals 0.018). This provides
additional evidence that it is the schools with poor classroom environments that realize
little or no benefit from additional instruction time.

V. Conclusions and Policy Implications
Instructional time has become an important element in school reform discussions,
as many advocate for increases in time devoted to mathematics and reading instruction. A
shortage of compelling empirical evidence has hindered the decision-making process, and
a primary goal of this paper is to build on the contributions of recent work and provide
additional information. The analysis uses panel data methods made possible by the
richness of the PISA data, and the fixed effects models accounted for student and school
heterogeneity including differences by subject in some specifications.
The empirical analysis provides strong evidence in favor of the notion that
additional time raises achievement using a series of specifications and measures of

24

instructional time. Given the character of the deficiencies of the two fixed effects models,
the results suggest that the effect is positive and modest in magnitude on average.
Although instructional time is found to exhibit diminishing returns, the rate of decrease
appears to be quite gradual.
Perhaps most important, the benefit of additional instructional time appears to
vary with the quality of the classroom environment. The results produced by both
specifications show that schools with low quality classroom environments likely realize
little or no benefit from additional instruction time. On the one hand, it does not appear
that schools can compensate for poor environments with additional time. If anything,
additional time might be expected to degrade further the quality of the classroom
environment as it becomes more difficult for students to sit and listen. On the other hand,
there would appear to be substantial complementarities between policies that improve the
classroom environment such as the strict discipline demanded in KIPP Academy schools
and those that expand instruction time. Thus these results are consistent with the large
benefits found for attendance at KIPP Academy charter schools.
In contrast, the estimates provide little or no evidence of a relationship between
the return to additional instruction time and the quality of instruction. Yet given the
absence of direct measures of teacher quality, class size, and other established
determinants of the quality of instruction, this finding may simply reflect the weakness of
the quality of instruction measure. Additional research is called for to gain a better
understanding of heterogeneity by the quality of instruction.

25

Bibliography
Adams, Ray, and Margaret Wu. 2002. “PISA 2000 Technical Report”. OECD.
http://www.oecd.org/edu/preschoolandschool/programmeforinternationalstudentasses
smentpisa/33688233.pdf.
Angrist, Joshua D., Susan M. Dynarski, Thomas J. Kane, Parag A. Pathak, and
Christopher R. Walters. 2010. “Inputs and Impacts in Charter Schools: KIPP Lynn.”
American Economic Review 100 (2): 239–43.
Chetty, Raj, John N. Friedman, and Jonah E. Rockoff. 2011. “The Long-Term Impacts of
Teachers: Teacher Value-Added and Student Outcomes in Adulthood”. Working
Paper 17699. National Bureau of Economic Research.
http://www.nber.org/papers/w17699.
Coates, Dennis. 2003. “Education Production Functions Using Instructional Time as an
Input.” Education Economics 11 (3): 273–292.
Farbman, David. 2011. Learning Time in America: Trends to Reform the American
School Calendar. http://www.eric.ed.gov/PDFS/ED521518.pdf.
Farbman, David. 2012. “The Case for Improving and Expanding Time in School: A
Review of Key Research and Practice.”
http://www.timeandlearning.org/files/CaseforMoreTime_1.pdf.
Gijselaers, Wim H., and Henk G. Schmidt. 1995. “Effects of Quantity of Instruction on
Time Spent on Learning and Achievement.” Educational Research and Evaluation 1
(2): 183–201. doi:10.1080/1380361950010204.
Jacob, Brian A., Lars Lefgren, and David Sims. 2008. “The Persistence of TeacherInduced Learning Gains”. NBER Working Paper 14065. National Bureau of
Economic Research, Inc. http://ideas.repec.org/p/nbr/nberwo/14065.html.
Kane, Thomas J., and Douglas O. Staiger. 2008. “Estimating Teacher Impacts on Student
Achievement: An Experimental Evaluation”. NBER Working Paper 14607. National
Bureau of Economic Research, Inc. http://ideas.repec.org/p/nbr/nberwo/14607.html.
Herbst, Mikolaj, Daniel Munich, Steven Rivkin, and Jeffrey Schiman. 2012.
Understanding the Divergent Trends in PISA Test Results for Poland and the Czech
Republic. Working Paper.
Kuehn, Zoe, and Pedro Landeras. 2012. Study Time and Scholarly Achievement in PISA.
Working Paper. FEDEA. http://ideas.repec.org/p/fda/fdaddt/2012-02.html.

26

Lavy, Victor. 2010. Do Differences in Schools’ Instruction Time Explain International
Achievement Gaps? Evidence from Developed and Developing Countries. NBER
Working Paper. National Bureau of Economic Research, Inc.
http://ideas.repec.org/p/nbr/nberwo/16227.html.
Lavy, Victor. 2012. “Expanding School Resources and Increasing Time on Task: Effects
of a Policy Experiment in Israel on Student Academic Achievement and Behavior”.
Working Paper 18369. National Bureau of Economic Research.
http://www.nber.org/papers/w18369.
Lazear, Edward P. 2001. “Educational Production.” The Quarterly Journal of Economics
116 (3) (August 1): 777–803. doi:10.1162/00335530152466232.
Mandel, Philipp, and Bernd Süssmuth. 2011. Total Instructional Time Exposure and
Student Achievement: An Extreme Bounds Analysis Based on German State-Level
Variation. CESifo Working Paper Series. CESifo Group Munich.
http://ideas.repec.org/p/ces/ceswps/_3580.html.
Marcotte, Dave E. 2007. “Schooling and Test Scores: A Mother-natural Experiment.”
Economics of Education Review 26 (5): 629–640.
Marcotte, Dave E., and Steven W. Hemelt. 2008. “Unscheduled School Closings and
Student Performance.” Education Finance and Policy 3 (3) (July 1): 316–338.
doi:10.1162/edfp.2008.3.3.316.
Meghir, Costas, and Steven Rivkin. 2011. “Chapter 1 - Econometric Methods for
Research in Education.” In Handbook of the Economics of Education, ed. Stephen
Machin and Ludger Woessmann Eric A. Hanushek, Volume 3:1–87. Elsevier.
http://www.sciencedirect.com/science/article/pii/B9780444534293000016.
National Center on Time & Learning. Why Time Matters.
http://timeandlearning.org/?q=why-time-matters.
Roland G. Fryer, Jr. 2011. Injecting Successful Charter School Strategies into Traditional
Public Schools: Early Results from an Experiment in Houston. NBER Working
Paper. National Bureau of Economic Research, Inc.
http://ideas.repec.org/p/nbr/nberwo/17494.html.
Rothstein, Jesse. 2010. “Teacher Quality in Educational Production: Tracking, Decay,
and Student Achievement.” The Quarterly Journal of Economics 125 (1): 175–214.
Todd, Petra E., and Kenneth I. Wolpin. 2003. “On the Specification and Estimation of the
Production Function for Cognitive Achievement*.” The Economic Journal 113 (485):
F3–F33. doi:10.1111/1468-0297.00097.

27

Wiermann, Christian. 2005. Class Size, Instruction Time and Central Exit Examinations :
Disentangling the Relative Contributions to Scholastic Achievement. Working Papers
of the Research Group Heterogenous Labor. Research Group Heterogeneous Labor,
University of Konstanz/ZEW Mannheim.
http://ideas.repec.org/p/knz/hetero/0504.html.

28

Table 1: Joint Distribution of Mathematics and Language Arts Instructional Minutes and Classes Based on
Student Level Data
1. Minutes per week (proportion of math total)
Language Arts
0-99
100-179
180-199
200-219
220-239
240-279
280+

0-99
0.41
0.32
0.15
0.02
0.03
0.03
0.05

100-179
0.04
0.58
0.11
0.14
0.05
0.05
0.03

180-199
0.02
0.13
0.48
0.01
0.18
0.14
0.04

Total

9,102

45,251

42,250

Mathematics
200-219
0.00
0.18
0.01
0.57
0.01
0.16
0.07

2. Classes per week (proportion of math total)
Mathematics
Language Arts
0-2
3
4
0-2
0.51
0.12
0.03
3
0.22
0.49
0.15
4
0.18
0.31
0.53
5
0.06
0.07
0.21
6+
0.03
0.02
0.08
Total

20,867

44,914

88,115

220-239
0.02
0.08
0.14
0.01
0.47
0.23
0.06

240-279
0.01
0.08
0.08
0.16
0.06
0.49
0.13

280+
0.02
0.06
0.04
0.05
0.03
0.16
0.64

Total
8,220
50,051
37,238
36,695
30,976
53,403
39,445

34,082

35,384

51,455

38,504

256,028

5
0.02
0.06
0.21
0.56
0.15

6+
0.03
0.04
0.10
0.20
0.63

Total
21,347
45,877
84,279
73,743
44,135

77,017

38,468

269,381

Notes: The joint distributions are calculated at the student level. The sample starts with only students that have a math and language arts
component in their PISA exam (304,070 students). We then merge the student sample by country, school, grade, and subject to the
analysis sample and drop observations not used in the regressions.

29

Table 2: Mathematics minus language arts score difference by instructional time in mathematics and
language arts
1. Minutes per week
Language Arts
0-99
100-179
180-199
200-219
220-239
240-279
280+

Mathematics
200-219
-2.8
2.1
-1.4
1.4
2
-8.7
-1.1

0-99
1.4
2.8
-1.7
3.7
15.9
2.8
-5.6

100-179
7.4
0
1.2
-6.4
6.3
-9
-11.9

180-199
-6.3
8.2
0.8
-5.8
1.6
-1.3
-3.6

0-2
-2.8
1
-3.9
-10
-16.2

3
3.8
-0.9
-1.4
-5
1.6

Mathematics
4
-3.4
4.7
0.7
-8.8
-0.8

220-239
15.4
7.2
4.1
4.7
2.8
-17.4
3.8

240-279
7.3
0.1
2.5
1.3
6
-1.1
5.6

280+
7.1
0.4
2.4
13.3
10.9
7.9
4.2

2. Classes per week
Language Arts
0-2
3
4
5
6+

5
2.4
3.1
0.7
-1
0.8

6+
9.4
5.7
9.3
5.1
10.7

30

Table 3: Estimated Effects of Weekly Instructional Minutes and Classes per Week on
Achievement
(1)
(2)
(3)
Panel A:
Weekly Minutes of Instruction
0.068***
0.025***
0.017**
(0.008)
(0.006)
(0.008)
Panel B:
Weekly Number of Class
Periods

School-by-grade fixed effect
School-by-subject fixed effect
Sample Size
# of Schools

4.946***

2.078***

1.153**

(0.497)

(0.346)

(0.541)

N
N

Y
N

N
Y

47,580
16,154

47,580
16,154

47,580
16,154

Notes: The dependent variable in all regressions is stacked school-by-grade-by-subject average test scores
based on PV1MATH and PV1READ. Estimates are insensitive to choice of plausible value. A consistent
sample of schools is used for all regressions in the following tables. All regressions also include a country-bygrade-by-subject effect. Prior to aggregation to the country-school-grade-subject level, the sample is limited
to students who had both math and language arts components in their 2009 PISA exam. The sample consists
of 47,580 school-by-grade-by-subject observations from 16,154 schools.
Robust Standard errors clustered by school are in parentheses.
*** Significant at the 1 percent level.; ** Significant at the 5 percent level; * Significant at the 10 percent
level.

31

Figure 1: Estimated Effect of Instructional Minutes and Classes Per Week on Standard Deviations of Achievement
Panel A: Weekly Minutes of Instruction
Panel A: School-by-Grade Fixed Effect

0
-.02

-.02

-.01

-.01

0

SD for Test Scores

.01

.01

.02

.02

Panel B: School-by-Subject Fixed Effect

-1

-.5
0
.5
SD for Weekly Minutes of Instruction

1

-1

-.5
0
.5
SD for Weekly Minutes of Instruction

1

Panel B: Weekly Number of Classes
Panel B: School-by-Subject Fixed Effect

0

-.02

-.01

-.01

0

SD for Test Scores

.01

.01

.02

Panel A: School-by-Grade Fixed Effect

-1

-.5

0
SD for Weekly Classes

.5

1

-1

-.5

0
SD for Weekly Classes

.5

1

Notes: Similar to Chetty, Friedman, and Rockoff (2011), this figure presents the regression estimates non-parametrically. To plot
each figure, we regress both instruction time and test scores in standard deviations on country-by-grade-by-subject fixed effects as
well as school-by-grade or school-by-subject fixed effects. After both regressions, we calculate residuals, group them based on the
instruction time residuals, and scatter the grouped residuals against each other.

32

Table 4: Estimated Effects of Weekly Minutes Using a Quadratic Specification
(1)
(2)
(3)
Weekly Minutes of
0.3410***
0.0701***
0.0973***
Instruction
(0.0197)
(0.0131)
(0.0209)
Weekly Minutes of
Instruction Squared
School-by-grade
fixed effect
School-by-subject
fixed effect
Sample Size
# of Schools

-0.00043***

-0.00007***

-0.00012***

(0.00003)
N

(0.00002)
Y

(0.00003)
N

N

N

Y

47580
16154

47580
16154

47580
16154

Notes: Robust Standard errors clustered by school are in parentheses.

33

Table 5. Estimated Effects of Weekly Instructional Minutes and Classes per Week on
Achievement from Regressions Using Instructional Time Categories
(1)
(2)
Panel A: Average Shares of Students in each
Minutes Per Week Category (relative to 200-219)

0 to 99

-3.5
(2.5)

-20.6***
(3.9)

100 to 179

-2.2
(1.3)

-6.6***
(2.1)

180 to 199

4.5***
(1.6)

-5.2*
(2.8)

220 to 239

1.7
(1.7)

0.8
(2.9)

240 to 279

5.9***
(1.3)

0.7
(2.4)

280+

4.4***
(1.6)

-2.7
(2.7)

0 to 2

-8.2***
(1.7)

-16.7***
(2.8)

3

-2.9***
(1.0)

-2.9*
(1.6)

5

2.1**
(0.9)

1.6
(1.8)

6 to 10

3.9***
(1.3)

-4.1*
(2.3)

School-by-grade fixed effect
School-by-subject fixed effect
Sample Size
# of Schools

Y
N
47580
16154

N
Y
47580
16154

Panel B: Average Shares in each Weekly
Classes Category (relative to 4)

Notes: Robust Standard errors clustered by school are in parentheses.

34

Table 6. Factor Analysis of Questions on Student and Teacher Behavior

Principal Discusses Job with Teachers
Principal Makes Suggestions to Teachers
Teachers are Observed by an Authority Figure
No Lack of Science Teachers
No Lack of Mathematics Teachers
No Lack of Language Arts Teachers
No Lack of Teachers in other subjects

Instructional Quality
Factor Loadings
Scoring Coefficients
0.0114
-0.0006
-0.0629
-0.0150
-0.0958
-0.0146
0.8811
0.2962
0.8929
0.3272
0.8618
0.2599
0.7863
0.1649

Lack of Student Absences
Lack of Student Disruption
Lack of Student Skipping
Students Respect Teachers
Lack of Student Drug Use
Lack of Student Bullying

Classroom Quality
Factor Loadings
Scoring Coefficients
0.7056
0.1633
0.7571
0.1824
0.7870
0. 2510
0.8021
0.2397
0.7011
0.1527
0.7284
0.1898

Notes: Each school representative responds to a series of questions about the school and classroom climate (Q11, Q17, Q23,
and Q26). We use responses to these questions in our factor analysis.
The eigenvalue for the teacher quality factor is 2.948 and the proportion of variance it explains is 83%. We predict only the
first factor and call it classroom hindrances. The eigenvalue for the classroom quality factor is 3.356 and the proportion of
variance it explains is 99%. Higher values on both scales reflect higher quality. Given the ordered categorical nature of the
variables, we use a Polychoric correlation matrix to conduct the factor analysis.

35

Table 7. Estimated Effects of Instructional Time, by Teacher and Classroom Quality
(1)
(2)
(3)
(4)
Panel A:
Weekly Minutes of Instruction

(5)

(6)

-0.001
(0.024)

-0.000
(0.033)

-0.049*
(0.025)

-0.013
(0.037)

-0.054*
(0.031)

-0.021
(0.043)

Weekly Minutes*Instruction Quality

0.008
(0.007)

0.005
(0.010)

-

-

0.002
(0.007)

0.003
(0.010)

Weekly Minutes*Classroom Quality

-

-

0.021***
(0.007)

0.009
(0.011)

0.021***
(0.007)

0.008
(0.011)

0.806
(1.442)

1.952
(2.055)

-2.050
(1.395)

-0.228
(2.362)

-2.051
(1.707)

0.701
(2.644)

Weekly Classes*Instruction Quality

0.364
(0.396)

-0.230
(0.588)

-

-

0.001
(0.423)

-0.373
(0.632)

Weekly Classes*Classroom Quality

-

-

1.178***
(0.393)

0.407
(0.674)

1.178***
(0.419)

0.514
(0.719)

Y
N
47580
16154

N
Y
47580
16154

Y
N
47580
16154

N
Y
47580
16154

Y
N
47580
16154

N
Y
47580
16154

Panel B:
Weekly Number of Class Periods

School-by-grade fixed effect
School-by-subject fixed effect
Sample Size
# of Schools

36

Table 8. Estimated Effects of Instructional Time, by quartile of Teacher and Classroom Quality

Weekly Minutes of
Instruction
Weekly Minutes*2nd Quality
Quartile
Weekly Minutes*3rd Quality
Quartile
Weekly Minutes*4th Quality
Quartile
Weekly Minutes*(2nd
through 4th Quality Quartile)

School-by-grade fixed effect
School-by-subject fixed
effect
Sample Size
# of Schools

Instructional Quality
(1)
(2)
(3)
0.023**
0.023
0.020**

(4)
0.016

(5)
-0.004

Classroom Quality
(6)
(7)
-0.002
-0.004

(8)
-0.002

(0.010)

(0.015)

(0.010)

(0.014)

(0.010)

(0.016)

(0.010)

(0.016)

-0.004

-0.011

-

-

0.021

0.027

-

-

(0.013)

(0.021)

-

-

(0.014)

(0.022)

-

-

0.004

-0.012

-

-

0.039***

0.030

-

-

(0.013)

(0.021)

-

-

(0.013)

(0.022)

-

-

0.012

0.008

-

-

0.049***

0.021

-

-

(0.015)

(0.023)

-

-

(0.014)

(0.023)

-

-

-

-

0.007
(0.011)

0.002
(0.017)

-

-

0.037
(0.011)***

0.027
(0.018)

Y
N

N
Y

Y
N

N
Y

Y
N

N
Y

Y
N

N
Y

47,580
16,154

47,580
16,154

47,580
16,154

47,580
16,154

47,580
16,154

47,580
16,154

47,580
16,154

47,580
16,154

Notes: The regression includes the main effects of instruction time, the quality quartiles, and interactions of instruction time and quality quartiles. We omit the lowest quality
category as the base category. Because the quality quartile main effects do not vary within a school, they are captured perfectly by the fixed effects.

37

Appendix Table A1: Descriptive Statistics
Math

Language Arts
Mean
SD
457.28
79.34

Mean
457.14

SD
83.82

Average Weekly Number of Classes

4.35

1.16

4.43

1.22

Average Length in Minutes of an
Average Class

51.76

12.15

51.46

11.99

Average minutes per week

221.55

69.23

223.83

69.71

Average Instruction Quality

3.51

0.76

3.51

0.76

Average Classroom Quality

3.45

0.73

3.45

0.73

Average Test Score

# of Schools

16,154

Notes: To calculate weekly minutes of instruction, we multiply the school-by-grade-by-subject average
number of weekly classes attended by the length of an average class (ST28Q01*ST29Q01 and
ST28Q02*ST29Q02). Prior to aggregation to the grade-by-school-by-subject level, students who reported
having more than 10 classes per week or average class lengths greater than 120 minutes were set to missing.
Total number of observations is 48,528 and each represents a country-by-school-by-grade-by-subject average
value. In all analyses that follow, standard errors will be clustered on school of which there are 16,452.

Appendix Table A2: Percent of Variation in Instruction Time Measures Explained by the Fixed Effects
Average weekly minutes
Average Weekly Classes
School-by-grade fixed effects
School-by-subject fixed effects
Subject-by-grade-by-country effects

0%
0%

43%
50%

88%
90%

87%
91%

97%
98%

N
N
N

N
N
Y

Y
N
Y

N
Y
Y

Y
Y
Y

Notes: Average weekly minutes, average weekly classes, and average minutes per class are used as dependent variables. The independent
variables used in each regression are indicated in the table. The percent indicates
from each regression.

38

Weekly Minutes of
Instruction

Appendix Table A3: Out-of-school study
(1)
(2)
0.025***
0.030***
0.014*
(0.006)

(0.006)

(4)

2.078***

2.231***

0.846

(0.346)

(0.348)

(0.540)

(0.008)

Weekly Number of
Class Periods

Out-of-school
Lessons (rel. to 2 to
< 4 Hours)
None

(3)

0.525
(1.361)

13.726***
(1.848)

0.604
(1.362)

13.723***
(1.848)

<2 Hours per week

-3.270**
(1.438)

-3.053
(2.126)

-3.182**
(1.437)

-3.044
(2.127)

4 to <6 Hours per
week

1.091

-2.468

0.983

-2.423

(1.752)

(2.522)

(1.752)

(2.523)

0.921
(2.163)

-5.696**
(2.887)

0.607
(2.164)

-5.642*
(2.888)

Y

Y

N

Y

Y

N

N

N

Y

N

N

Y

47,580
16,154

46,136
16,038

46,136
16,038

47,580
16,154

46,136
16,038

46,136
16,038

6+ Hours per week

School-by-grade
fixed effect
School-by-subject
fixed effect
Sample Size
# of Schools

* p<0.10, ** p<0.05, *** p<0.01

39

Appendix Table A4: Estimated Effects of Weekly Instructional Minutes and Classes per
Week on Achievement in Science and Language Arts
(1)
(2)
(3)
Panel A:
Weekly Minutes of
0.091***
0.013***
0.034***
Instruction
(0.006)
(0.003)
(0.007)
Panel B:
Weekly Number of
Class Periods

School-by-grade
fixed effect
School-by-subject
fixed effect
Sample Size
# of Schools

5.917***

0.778***

2.337***

(0.341)

(0.158)

(0.388)

N

Y

N

N

N

Y

47,786
16,301

47,786
16,301

47,786
16,301

40

Appendix Table A5. Estimated Effects of Instructional Time on Achievement in Science and Language Arts, by Teacher and
Classroom Quality
(1)
(2)
(3)
(4)
(5)
(6)
Panel A:
Weekly Minutes of Instruction
0.023**
0.028
-0.007
0.017
0.005
0.016
(0.011)
(0.028)
(0.011)
(0.027)
(0.014)
(0.034)
Weekly Minutes*Teacher Quality

-0.003
(0.003)

0.002
(0.008)

Weekly Minutes*Classroom Quality

Panel B:
Weekly Number of Class Periods

Weekly Classes*Teacher Quality

1.347**
(0.534)

2.313
(1.423)

-0.160
(0.146)

0.007
(0.397)

Weekly Classes*Classroom Quality

School-by-grade fixed effect
School-by-subject fixed effect
Sample Size
# of Schools

Y
N
47786
16301

N
Y
47786
16301

-0.004
(0.003)

0.000
(0.008)

0.006*
(0.003)

0.005
(0.008)

0.007**
(0.003)

0.005
(0.008)

-0.238
(0.559)

1.689
(1.513)

0.424
(0.668)

1.819
(1.723)

-0.253*
(0.153)

-0.054
(0.431)

0.289*
(0.153)

0.188
(0.427)

0.357**
(0.161)

0.205
(0.463)

Y
N
47786
16301

N
Y
47786
16301

Y
N
47786
16301

N
Y
47786
16301

Notes: Robust Standard errors clustered by school are in parentheses.
* p<0.10, ** p<0.05, *** p<0.01

41

