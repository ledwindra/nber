NBER WORKING PAPER SERIES

USING A SATISFICING MODEL OF EXPERIMENTER DECISION-MAKING TO
GUIDE FINITE-SAMPLE INFERENCE FOR COMPROMISED EXPERIMENTS
Ganesh Karapakula
James J. Heckman
Working Paper 27738
http://www.nber.org/papers/w27738

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
August 2020

We thank Juan Pantano and Azeem Shaikh for comments on early drafts of this paper. We thank
Cheryl Polk and Lawrence Schweinhart of the HighScope Educational Research Foundation for
their assistance in data acquisition, sharing historical documentation, and their longstanding
partnership with the Center for the Economics of Human Development. This research was
supported in part by: the Buffett Early Childhood Fund; NIH Grants R01AG042390,
R01AG05334301, and R37HD065072; and the American Bar Foundation. The views expressed
in this paper are solely those of the authors and do not necessarily represent those of the funders,
the official views of the National Institutes of Health, nor the views of the National Bureau of
Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2020 by Ganesh Karapakula and James J. Heckman. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.

Using a Satisficing Model of Experimenter Decision-Making to Guide Finite-Sample Inference
for Compromised Experiments
Ganesh Karapakula and James J. Heckman
NBER Working Paper No. 27738
August 2020
JEL No. C01,C4,I21
ABSTRACT
This paper presents a simple decision-theoretic economic approach for analyzing social
experiments with compromised random assignment protocols that are only partially documented.
We model administratively constrained experimenters who satisfice in seeking covariate balance.
We develop design-based small-sample hypothesis tests that use worst-case (least favorable)
randomization null distributions. Our approach accommodates a variety of compromised
experiments, including imperfectly documented re-randomization designs. To make our analysis
concrete, we focus much of our discussion on the influential Perry Preschool Project. We
reexamine previous estimates of program effectiveness using our methods. The choice of how to
model reassignment vitally affects inference.

Ganesh Karapakula
Yale University
New Haven, CT 06511
United States
ganesh.karapakula@yale.edu
James J. Heckman
Center for the Economics of
Human Development
University of Chicago
1126 East 59th Street
Chicago, IL 60637
and IZA
and also NBER
jjh@uchicago.edu

A data appendix is available at
http://www.nber.org/data-appendix/w27738
A Link to online appendix is available at
http://cehd.uchicago.edu/inference-for-compromised-experiments

1

Introduction

This paper develops a finite-sample, design-based approach for analyzing data from compromised
social experiments using a satisficing model of experimenter behavior. Compromises can take
many forms, including exchanges or transfers of subjects across the experimental groups based
on post-randomization considerations that are not fully documented. For specificity, we motivate
our approach drawing on the influential Perry Preschool Project, an experimental high-quality
preschool program targeted toward disadvantaged African-American children in the 1960s.1
Previous studies of the Perry program report substantial treatment effects on numerous outcomes.2 These studies have greatly influenced discussions about the benefits of early childhood
programs.3 However, critics of the Perry program question the validity of these conclusions. They
point to the small sample size of the experiment—just over a hundred observations. They also
mention incomplete knowledge of, and compromises in, the randomization protocol used to form
control and treatment groups. Problems with attrition and non-response are also cited. Previous research (Heckman et al., 2010a; Heckman et al., 2020) addresses some of these concerns.4 We offer
an alternative approach that models experimenter decision-making in conducting the experiment.
The Perry randomization protocol was a multi-stage process. Its main compromised feature
is shared by many randomized controlled trials: undocumented re-randomization. This involves
reassignment of treatment status after initial random assignment in order to improve balance between experimental groups with respect to baseline covariates, but without a pre-specified, fully
documented reassignment plan.
This practice occurs often. Bruhn and McKenzie (2009) survey 25 leading researchers using
randomized experiments and report a typical response:
“[Experimenters] regressed variables like education on assignment to treatment, and
then re-did the assignment if these coefficients were ‘too big.’”
Some 52% admit to “subjectively deciding whether to redraw” and 15% admit to “using a statistical
rule to decide whether to redraw” the treatment assignment vector in at least one of the experiments
they conducted.5 The authors conclude that

1 See

Schweinhart et al. (1993, 1985, 2005). Heckman et al. (2010a) describe the program in detail. See also
Appendix A.
2 See, e.g., Heckman et al. (2010a) and Heckman et al. (2020).
3 See Obama (2013).
4 We compare in detail the approaches of Heckman et al. (2010a) and Heckman et al. (2020) with our methods in
Section 4.
5 These percentages are calculated by weighting each survey respondent by the number of experiments in which
the respondent had participated.

3

“this reveals common use of methods to improve baseline balance, including several
rerandomization methods not discussed in print.”
The approach developed in this paper applies to experiments conducted in such a subjective
and incompletely documented manner. If rerandomization criteria are specified and adhered to
before carrying out final treatment assignment, there exist simpler methods for conducting valid
inference.6 We supplement the literature by considering the case where the reassignment rule is
only partially documented. We build on and complement the analysis of Heckman et al. (2020)
with an explicit model of experimenter behavior.
We model experimenters as decision-makers who satisfice in seeking to achieve covariate balance with a “suitable” metric. Implicit decision rules underlie all covariate balancing procedures.
The decision-makers forming the experimental groups do not necessarily have a precise rule in
mind and satisfice in the sense of Simon (1955). Even if experimenters have a specific rule in
mind, it may not be carefully documented.
This paper proceeds in the following way. Section 2 illustrates the class of problems addressed
in this paper by reexamining the reassignment protocols of an influential compromised smallsample social experiment, to which we apply our methods both here and in a more extensive
analysis (Heckman and Karapakula, 2020). Section 3 presents a satisficing model of experimenter
behavior consistent with the available information on it from published and informal accounts.
We partially identify the set of randomization protocols consistent with our model. We consider
the generality of our approach by discussing the class of experiments to which our model applies.
In Section 4, we first discuss hypotheses of interest and conventional testing procedures used in
the literature. We then construct worst-case randomization tests using stochastic approximations
of least favorable randomization null distributions. We also compare our approach with that of
Heckman et al. (2010a) and Heckman et al. (2020). Section 5 presents our test statistics and
uses our methodology to reexamine the inference reported by Heckman et al. (2020). Section 6
concludes.

2

The Motivating Problem

To give specificity to our analysis we draw on a prototypical social experiment, the Perry Preschool
Project, which was conducted in the early 1960s. The original sample for the experiment consisted
6 See,

e.g., Morgan and Rubin (2012, 2015) and Li et al. (2018). Morgan and Rubin (2012) state that they “only
advocate rerandomization if the decision to rerandomize or not is based on a pre-specified criterion.” Their inferential
methods require knowledge of such pre-specified criteria. Although rerandomization methods have the property that
they reduce variance of the null distribution asymptotically in certain settings (Li et al., 2018; Morgan and Rubin,
2012, 2015), this property is not guaranteed in the finite-sample setting we consider.

4

of 128 children. Five of these children were dropped from the study due to extraneous reasons.7
Starting at age 3, treatment in the following two years included preschool for 2.5 hours per day
on weekdays during the academic year. The program also offered 1.5-hour weekly home visits
by the Perry teachers to promote parental engagement with the child.8 For more details on the
background and eligibility criteria of the program, see Heckman et al. (2010a) and Appendix A.

2.1

Randomization Protocol

Understanding the randomization protocol is essential for constructing valid frequentist inference
for any experiment. As Bruhn and McKenzie (2009) emphasize, many experimental studies in
economics do not report the complete set of rules (e.g., balancing criteria) used to form experimental samples. They conduct hypothesis tests that ignore the randomization protocols actually
used. In analyzing the Perry data, this issue is salient. Reports vary about the procedure used and
the exact rules followed in creating experimental samples. We discuss the various descriptions of
the randomization protocols. While the core descriptions of the procedure followed are broadly
consistent across texts, some of the details provided are vague and inconsistent, even those by the
same authors. We account for this ambiguity in designing and interpreting our hypothesis tests.
While the details are Perry-specific, the general principles involved are not.
Before the initiation of the randomization procedure by the Perry staffers in each of the last four
Perry cohorts, any younger siblings of participants enrolled in previous waves are separated from
children of freshly recruited families, whom we term “singletons” (Schweinhart, 2013; Schweinhart et al., 1985). As Schweinhart et al. (1985) explain,
“[A]ny siblings [are] assigned to the same group [either treatment or control] as their
older siblings in order to maintain the independence of the groups.”
By construction this does not apply to the very first cohort.
7 According to Schweinhart et al. (2005), “4 children did not complete the preschool program because they moved
away and 1 child died [in a fire accident] shortly after the study began.” We are missing the following data (on some
of these children) that are necessary for inference procedures. We do not know the mother’s working status at baseline
of a subject in wave 0 (who has a sibling in wave 1) among the five children who dropped out of the original sample
of 128 for extraneous reasons. We also do not know the gender of a subject in wave 1. (We use the Perry convention
that wave 0 is the first wave and wave 4 is the last one.) The baseline information on these subjects is important in
our formal model of the randomization protocol. We do not make assumptions regarding the mother’s working status
at baseline of the subject in wave 0 and the gender of the other subject in wave 1. We run our testing procedures for
each of the possible values of the variables. While we use the data on the five dropped children in our simulations of
the randomization protocol for our worst-case tests, we treat the five participants as ignorable in our estimation of the
treatment effects. Thus, our effective sample for estimation and inference is the core sample of 123 children.
8 Those in the treatment group of the first entry cohort (wave 0) were provided the intervention for only one year,
starting at age 4, and thus constitute an exception. Our estimates of treatment effects pool all five cohorts, even though
the lower program intensity in the first cohort might in principle attenuate the magnitudes of the effects downward.

5

The singletons from new families are then randomized into the two experimental groups as
follows. Weikart et al. (1978) detail the second step of the randomization protocol:
“First, all [singletons] are rank-ordered according to Stanford–Binet [IQ] scores.
Next, they are sorted (odd / even) into two groups.”
Singletons are then divided into two groups, one comprising those with even IQ ranks and another
with odd IQ ranks. The latter group has one additional person if the singletons are odd in number;
otherwise, the sizes of the two groups are equal.
In the third step, children are exchanged between the two groups to balance the vector of means
of an index of socioeconomic status (SES), the proportions of boys and girls, and the proportion of children with working mothers, in addition to mean IQ (Schweinhart et al., 1993; Weikart
et al., 1964). The exact balancing criteria and the number of exchanges are not specified, and
the exchanges are not necessarily restricted to those between consecutively ranked IQ pairs,9 as
is sometimes assumed, e.g., in Heckman et al. (2020). After the first three steps, there are two
undesignated groups that differ in number by at most one, and the two groups are balanced with
respect to mean IQ, mean SES, percentage of boys, and the proportion of children with working
mothers, in a manner acceptable to the staffers, using balancing rules that are undocumented.
All sources agree that in the fourth step a toss of a fair coin decides assignment of the two
groups to treatment and control conditions. The fifth step concerns children with working mothers
who are placed in the treatment group after the fourth step. In the fifth step, some of these children
are transferred to the control group.10 Although there is no consistent account of the number of
transfers, the sources describe the fifth step as involving one-way transfers of some children of
working mothers from the treatment group to the control group.11 Weikart et al. (1978) provide
reasons for the transfers: “no funds were available [to provide all working mothers with logistical
support, and] special arrangements could not always be made.” We interpret this statement as
implying that special arrangements could be made for at least some working mothers to enable
their children to attend preschool and participate in home visits if placed in the treatment group.
The constraints facing program administrators in doing so likely vary across cohorts. We assume
that the Perry staffers are impartial as to which working mothers get special arrangements.
9 See Appendix B. According to Schweinhart et al. (1993), “[The staffers] exchanged several similarly ranked pair

members so the two groups would be matched on [the baseline variables].” Even though the phrase “similarly ranked
pair members” might suggest consecutively ranked members, this is not necessarily the case. In Appendix B, we use
Perry data from wave 4 to demonstrate that the exchanges were not necessarily between consecutively ranked pairs.
10 See Schweinhart et al. (1993, 1985); Schweinhart and Weikart (1980); Zigler and Weikart (1993).
11 This is also manifested in the observed data. For example, as explained later in Section 3.2, the number of
singletons in wave 2 is 22, with 12 in the control group and 10 in the treatment group. If there were exchanges
between the initial experimental groups instead of one-way transfers to the control group, there would have been 11
singletons in both the control and treatment groups instead of 12 and 10, respectively.

6

Table 1 summarizes the randomization protocol. The main sources of ambiguity are in boldface: (a) the undocumented balancing criteria and rules used to satisfactorily balance the two
undesignated groups with respect to the mean levels of baseline variables in the third step; and (b)
the nature of constraints on the provision of special home visitation arrangements for children of
working mothers in the fifth step.
Table 1: Schematic of the Actual Randomization Protocol

1. Recruit participants and separate any younger siblings of participants enrolled in previous
waves from singletons (children of freshly recruited families)
↓
2. Rank singletons by IQ and split into two groups based on whether the rank is even or odd
↓
3. Exchange singletons between the two groups to satisfactorily balance the mean levels of
a vector of IQ, SES, gender, and mother’s working status
↓
4. Toss a fair coin to determine which of the two groups becomes the initial treatment group
↓
5. Transfer some children of working mothers from the treatment group to the control group
impartially if special arrangements for home visits can be made for only a limited number
↓
6. Assign any eligible younger siblings to the same group as their enrolled older siblings

3

Modeling the Randomization Protocol and Bounding the
Unknown Parameters

Since no precise description of the full Perry randomization protocol exists, we do not know who
was exchanged in the third step and who was transferred in the fifth step, making a standard
bounding analysis intractable. To address this problem, we assume that experimenters satisfice12
in seeking “balance” in the baseline covariate means of treatment and control groups, while facing
capacity constraints on special home visits for children of working mothers.
Using this model, we bound the level of covariate balance deemed acceptable by the experimenters at the end of the first three stages of the protocol. We also bound the number of possible
transfers at the fifth stage of the assignment procedure. Our model and the identified bounds are
12 See

Simon (1955), an early paper in behavioral economics.

7

used to construct worst-case randomization tests using least favorable null distributions (for treatment effects). While the details differ, the approach readily generalizes to the class of compromised
re-randomization designs discussed by Bruhn and McKenzie (2009).

3.1

Formalizing the Randomization Protocol

We first model the Perry randomization protocol and later discuss its generalizability. Let Sc be the
set of unique identifiers of participants in cohort13 c ∈ {0, 1, 2, 3, 4} with no elder siblings already
enrolled in the Perry Preschool Project. The cardinality of the set of singletons is |Sc |.14 The
participants in the set Sc are ranked according to their IQs by the Perry staffers, using an undocumented method to break any ties. The participants with odd and even ranks are then split into two
undesignated groups, with d|Sc |/2e and b|Sc |/2c members, respectively.15 Staffers exchange participants between the two groups until the mean levels of four variables (Stanford–Binet IQ, index
of socioeconomic status, gender, and mother’s working status) are balanced to their satisfaction.16
The exact metric the staffers used to determine satisfactory covariate balance is not documented.
We assume that they use Hotelling’s two-sample t-squared statistic τc2 , which is equivalent
to the Mahalanobis distance metric often used in matching.17 However, for each cohort’s initial
groups (partially identified in Section 3.2), the Hotelling statistic and raw mean differences do not
correspond to their possible minimum values and are sometimes far away from them.18 Thus,
13 Each

of the cohorts corresponds to one of the five waves (labeled 0 through 4) of study participants recruited
from the fall seasons of 1962 through 1965. Waves 0 and 1 were randomized in the fall of 1962, while the waves 2,
3, and 4 were randomized in the fall of 1963, 1964, and 1965, respectively. We follow the labeling convention for the
cohorts by the Perry analysts who designate the first cohort as “0.”
14 Note that the other participants in cohort c who are not singletons have older siblings already enrolled in the
Perry experiment in a previous wave. The non-singletons are not randomized but rather assigned to the same treatment
status as their elder siblings already enrolled in the study.
15 Note that d·e ≡ ceil(·) is the ceiling function and b·c ≡ floor(·) is the floor function. They assign the least upper
integer bound and greatest lower integer bound to the argument in the function, respectively.
16 An exchange means a swap between two participants belonging to different undesignated groups. Since the Perry
experiment did not use a matched pair design, an exchange or swap is not restricted to occur between participants with
consecutive IQ ranks. Exchanges between participants with non-consecutive IQ ranks can occur. See Appendix B.
17 The Hotelling’s multivariate two-sample t-squared statistic τ 2 maps a partition (A, B) of S (such that |A| =
c
c
d|Sc |/2e and B = Sc \ A) to R≥0 and is given by τc2 (A, B) = (Z̄A − Z̄B )0 (|A|−1 Σ̂A + |B|−1 Σ̂B )−1 (Z̄A − Z̄B ) ,
P
where Z̄A = |A|−1 i∈A Zi , with Zi as the vector containing the i-th participant’s IQ, index of socioeconomic status,
P
P
gender, and mother’s working status, Z̄B = |B|−1 i∈B Zi , and Σ̂A = (|A| − 1)−1 i∈A (Zi − Z̄A )(Zi − Z̄A )0 , while
P
Σ̂B = (|B| − 1)−1 i∈B (Zi − Z̄B )(Zi − Z̄B )0 . We use this metric for dimensionality reduction and computational
feasibility. Chung and Romano (2016) show, without assuming normality, that the permutation distribution of τc2
is asymptotically chi-squared. If adequate computational power were available, we could also incorporate into our
model the raw mean differences in the four variables, their studentized versions, or other measures of mean differences
between two groups. Of course, it is possible that the Perry staffers were just eyeballing mean differences and did not
use any formal metric.
18 For cohort 0, the proportion of possible group formations with a lower Hotelling statistic is at least 29.24%. The
corresponding numbers for cohorts 1, 2, 3, and 4 are 64.51%, 14.79%, 9.76%, 75.56%, respectively. Similarly, the raw
mean differences in baseline covariates for the initial groups also do not correspond to their minimum possible values.

8

it appears in terms of this model that program officials were satisficing rather than optimizing
(minimizing covariate imbalance) in constructing the two groups.
This process results in a partition (A∗c , Bc∗ ) of the set Sc chosen uniformly from
Uc (δc ) = {(A, B) : A ⊂ Sc , B = Sc \ A, |A| = d|Sc |/2e, τc2 (A, B) ≤ δc },

(1)

where δc is a satisficing threshold that captures how stringent or lax the Perry staffers were in trying
to balance the mean levels of the two groups.19 Note that the above set is invariant to the choice
of any strictly increasing transformation of the Hotelling statistic and the corresponding satisficing
(0)
threshold. Define Di,c as an indicator of whether participant i ∈ Sc belongs to A∗c . In other words,
(0)

Di,c = I{i ∈ A∗c }.
In the next stage, the Perry staffers flip a fair coin to determine whether A∗c or Bc∗ becomes the
preliminary treatment group. Let Qc be an indicator of whether the coin flip results in a head. If
Qc = 1, then Bc∗ becomes the treatment group. If Qc = 0, then A∗c becomes the treatment group.
(1)
Let Di,c denote membership in the preliminary treatment group. Thus
(1)

(0)

(0)

Di,c = Qc (1 − Di,c ) + (1 − Qc ) Di,c .

(2)

In the next step, some children of working mothers initially placed in the treatment group are
transferred to the control group.20 To model this process, we introduce additional notation. Define
Mi as an indicator of whether participant i’s mother was working at baseline. Cohorts 0 and 1
were both randomized in the fall of 1962, while each of the remaining cohorts were randomized
in successive years from 1963 through 1965. For cohorts 2 and higher, i.e., for c ∈ {2, 3, 4}, let
mc be the number of children of working mothers initially placed in the treatment group: mc =
P
(1)
i∈Sc Mi Di,c . For the entry cohorts, let m0,1 be the number of children of working mothers
19 The satisficing threshold δ is the maximum level of covariate imbalance that satisficed Perry staffers. The threshc
old δc is unknown to the analyst but can be partially identified, as explained later. We assume a uniform probability
over Uc for the choice of the partition (A∗c , Bc∗ ) for the purpose of keeping the model simple and computationally
feasible. In general, we might suspect the following: given two partitions of Sc with the same level of Hotelling’s
statistic, there might have been a higher probability mass on the partition closer to the initial grouping based on odd
and even IQ ranks. In addition, the staffers might have also preferred not to make additional exchanges if they expected relatively insignificant reductions in covariate imbalance. In other words, the probability that the Perry staffers
chose a particular partition (A∗c , Bc∗ ) could have depended on their preferences over substitution between two things:
similarity of (A∗c , Bc∗ ) to the initial IQ rank-based grouping; and the level of covariate imbalance (as measured by
Hotelling’s statistic) resulting from the partition (A∗c , Bc∗ ). However, there is no unique way to formalize this notion.
Such a general model may not even be computationally feasible.
20 The Perry teachers conducted special home visits for working mothers at times other than weekday afternoons,
when they visited the homes of non-working mothers. Because of logistical and financial constraints, the teachers
were able to visit the homes of only a limited number of working mothers at times other than weekday afternoons.
Thus, the children of working mothers in the preliminary treatment group for whom these special arrangements could
not be made were transferred to the control group.

9

P
P
(1)
initially placed in the treatment group for cohorts 0 and 1, that is, m0,1 = c∈{0,1} i∈Sc Mi Di,c .
Define ηc as a parameter indicating the maximum number of children of working mothers in
cohort c ∈ {2, 3, 4} for whom special arrangements could be made to enable special home visits.21
We define η0,1 to be the parameter indicating the maximum number of children of working mothers
in the pooled cohorts 0 and 1 for whom special home visitation arrangements could be made,
averting their transfer to the control group if placed in the initial treatment group.22
Special arrangements are made for min(η0,1 , m0,1 ) children of working mothers in the entry
cohorts and for min(ηc , mc ) such children in each cohort c ∈ {2, 3, 4} to enable special home
visits, as opposed to weekday home visits for children of non-working mothers. If there are any
remaining children with working mothers in the initial treatment group, they are transferred to
the control group, potentially increasing covariate imbalance.23 We assume that the Perry staffers
impartially choose (with equal probability) the children for whom the special accommodations are
made.24 To formalize this assumption, let Vi,c be a binary indicator for whether the participant
i ∈ Sc was placed the initial treatment group, had a working mother, and remained in the treatment
(1)
group through special accommodations for home visits. The vector (Vi,c : i ∈ Sc , Mi Di,c = 1)
is assumed to be drawn uniformly from the set {v ∈ {0, 1}mc : ||v||1 = min(ηc , mc )} for all
c ∈ {2, 3, 4}. Since the two entry cohorts face a common capacity constraint with respect to
(1)
special home visitation accommodations, the vector (Vi,c : i ∈ S0 ∪ S1 , Mi Di,c = 1) is assumed
to be drawn uniformly from the set {v ∈ {0, 1}m0,1 : ||v||1 = min(η0,1 , m0,1 )}. In addition,
(1)

Vi,c = 0 for a participant i ∈ Sc if Mi Di,c = 0 for all c ∈ {0, 1, 2, 3, 4}.25 In this notation, the
(2)

participant’s final treatment status Di,c is given by
(2)

(1)

(1)

(1)

Di,c = Mi Di,c Vi,c + (1 − Mi Di,c ) Di,c .

(3)

S
Any Perry subjects with identifiers not in 4c=0 Sc receive the same treatment status as their
elder siblings already enrolled in the Perry study. Thus, the final treatment status Di of the i-th
21 Thus,

ηc can be thought of as slots available for special visits to the homes of working mothers. Equivalently, it
is the number of children of working mothers who would remain in the final treatment group if all of them were placed
in the preliminary treatment group.
22 Since cohorts 0 and 1 had a common set of teachers, they share the number of slots available for the special home
visits. Thus, we pool these two cohorts while defining m0,1 and η0,1 . However, cohorts 2 through 5 have separate
parameters for the slots available for special home visits.
23 It is possible that the Perry staffers engaged in another round of satisficing at this step. In principle, this could
be incorporated into our model but would increase its dimensionality. Since the published accounts do not mention
another round of balancing, we do not add this feature to our model to keep it computationally feasible.
24 We are implicitly assuming that all working mothers would be able to send their children to preschool and
participate in weekly home visits if special arrangements could be made for them. A model allowing for heterogeneity
in availability of working mothers (for special arrangements) does not appear to be computationally feasible.
25 In other words, V = 0 for the participants who were either initially placed in the control group or placed in the
i,c
initial treatment group but have non-working mothers.

10

S
(2)
subject is given by Di = Di,c if i ∈ c Sc . Otherwise, if participant i is not from a freshly
recruited family, the assignment is given by Di = Dh , where the h-th subject is the i-th subject’s
S
eldest sibling enrolled in the Perry study, if i ∈ I \ c Sc , where I is the set of all Perry subjects.

3.2

Partially Identifying Satisficing Thresholds and Capacity Constraints

Using the Perry data, we now demonstrate how we can partially identify the satisficing thresholds
δc and the special home visitation capacity constraints ηc using the last three cohorts as examples.
We then present a general framework for partially identifying these parameters.
Example 1: Wave 2 (A Case with One Transfer in the Last Stage)
Wave 2

Di = 0

Di = 1

Total

Mi = 0

9

7

16

Mi = 1

3

3

6

Total

12

10

22

This example discusses the steps for bounding the parameters δ2 and η2 in wave 2. Shown above
is a contingency table of mother’s working status Mi and final treatment status Di for participants
i ∈ S2 in cohort 2 with no elder siblings already enrolled in the Perry study. There are 22 such
participants in total. Since there are an even number of participants, each of the initial two undesignated groups (as well as the initial treatment and control groups in the next stage) would have
been d|S2 |/2e = b|S2 |/2c = 11 in size. However, we observe only 10 members in the final treatment group but 12 members in the final control group. This implies that there must have been
one transfer from the initial treatment group to the control group. Thus, one of the 3 children of
working mothers in the final control group was in the initial treatment group. However, we do
not know exactly which one of these children was transferred, so there are 3 possibilities for the
2 , τ 2 , τ 2 be the Hotelling two-sample statistics for these three
initial treatment group. Let τ2,1
2,2 2,3
possibilities. One of these Hotelling statistics was the actual level of covariate imbalance between
the initial treatment and control groups, and this level of imbalance is assumed to be within the
2 , τ 2 , τ 2 }. In
satisficing threshold δ2 of the Perry staffers (by construction). Thus, δ2 ≥ min{τ2,1
2,2 2,3
addition, m2 = 4, since there must have been 4 children of working mothers in the initial treatment
group, consisting of the 3 participants who remain in the final treatment group and the 1 participant who was transferred to the control group. Since 3 of the initial 4 participants remained in the
final treatment group, min(η2 , m2 ) = min(η2 , 4) = 3, implying that η2 = 3, the only solution that
satisfies the equality. We next present two other examples.

11

Example 2: Wave 3 (A Case with 1 or 2 Transfers in the Last Stage)
Wave 3

Di = 0

Di = 1

Total

Mi = 0

7

9

16

Mi = 1

5

0

5

Total

12

9

21

In this example, we show a contingency table of Mi and Di for the 21 participants i ∈ S3 in
cohort 3. The sizes of the larger and smaller undesignated groups would have been d|S3 |/2e = 11
and b|S3 |/2c = 10, respectively. However, either of these two groups could have been the initial
treatment group. Since there are 12 members in the final control group and 9 in the final treatment
group, there are two possible cases: if the initial treatment group had 10 members, there would have
been 10 − 9 = 1 transfer; but if it had 11 members, there would have been 11 − 9 = 2 transfers.
Since the number of transfers involving children of working mothers is either 1 or 2, the number


of possibilities for the initial treatment group is 51 + 52 = 5 + 10 = 15, as all the 5 children
2 , . . . , τ2
of working mothers in this cohort are in the control group. Let τ3,1
3,15 be the Hotelling
2
2
statistics for those 15 possibilities. Then, δ3 ≥ min{τ3,1 , . . . , τ3,15 }. In addition, m3 ∈ {1, 2},
since m3 is the sum of the number of transfers (either 1 or 2) and the number of remaining children
in the final treatment group (0 in this cohort). As no working mother remained in the treatment
group, min(η3 , m3 ) = 0, implying that η3 = 0, which is the only number consistent with this
equality. Thus, the Perry staffers were unable to provide special home visitation accommodations
for any of the participants in this cohort.
Example 3: Wave 4 (A Case with No Transfers in the Last Stage)
Wave 4

Di = 0

Di = 1

Total

Mi = 0

5

10

15

Mi = 1

4

0

4

Total

9

10

19

In this example, we show a contingency table of Mi and Di for the 19 participants i ∈ S4 in
cohort 4. The sizes of the larger and smaller undesignated groups would have been d|S3 |/2e =
10 and b|S3 |/2c = 9. These coincide with the final sizes of the treatment and control groups,
respectively. Accordingly, we can conclude that the observed final treatment group was indeed
the initial treatment group for this cohort. Otherwise, the control group would have had at least
2 be the Hotelling statistic for the observed partition of S based on the final
10 members. Let τ4,1
4
2
treatment status. Then, δ4 ≥ τ4,1 . In addition, note that there are no children of working mothers
in the final treatment group, which was also the initial treatment group, and so m4 = 0. Since
12

min(η4 , m4 ) = min(η4 , 0) = 0 and there are 4 members with working mothers in total, it follows
that η4 ∈ {0, 1, 2, 3, 4}, because any of these values satisfies the equality. Thus, the observed data
for cohort 4 is not helpful in bounding η4 .
Partial Identification of the Satisficing Thresholds and Capacity Constraints in General
We now present a general characterization of how to partially identify the satisficing thresholds
and capacity constraints on special home visits.
Wave c

Di = 0

Di = 1

Total

Mi = 0

ω0,0

ω0,1

ω0,∗

Mi = 1

ω1,0

ω1,1

ω1,∗

Total

ω∗,0

ω∗,1

|Sc |

The above contingency table shows that there are ωm,d participants with (Mi , Di ) = (m, d) ∈
{0, 1}2 among the set of participants Sc in cohort c.26 The total number of children with nonworking mothers is ω0,∗ = ω0,0 + ω0,1 and that of working mothers is ω1,∗ = ω1,0 + ω1,1 . The
total number of participants in the final control group is ω∗,0 = ω0,0 + ω1,0 and that in the final
treatment group is ω∗,1 = ω0,1 + ω1,1 . The partial identification of the satisficing thresholds and
capacity constraints would vary depending on whether |Sc | is even or odd and also depending on
whether ω∗,1 = d|Sc |/2e or ω∗,1 < d|Sc |/2e. We discuss each of these cases separately.
First, consider the case where |Sc | is even or odd and ω∗,1 = d|Sc |/2e. In this case, since the
size of the final treatment group remains the same as that of the initial treatment group, there must
have been no transfers of children with working mothers from the treatment group to the control
group. Since the final treatment group is the same as the initial one, we can bound the satisficing
2 , where τ 2 is the Hotelling statistic for the partition of S based
threshold as follows: δc ≥ τc,1
c
c,1
on the final treatment status. In addition, since there are no transfers, the number of children of
working mothers in the initial treatment group mc equals ω1,1 . Since min(ηc , ω1,1 ) = ω1,1 , it
follows that ηc ∈ {ω1,1 , . . . , ω1,∗ }, i.e., the number of slots available for special home visits must
be at least the number ω1,1 observed in the data.
Second, consider the case where |Sc | is even and ω∗,1 < d|Sc |/2e. As in Example 1, in this case
it is clear that the number of transfers in the final stage must have been χc = d|Sc |/2e−ω∗,1 , which
is a positive number. The χc transferred children must be among the ω1,0 members with working

mothers in the final control group. Thus, there are ωχ1,0
possibilities for the initial treatment
c
group. Let ϑδc be the set containing the Hotelling statistics for those possibilities. Then, δc ≥
min ϑδc . In addition, there must have been mc = ω1,1 + χc children with working mothers in the
26 Note

that ωm,d ≡ ωm,d,c for all (m, d) ∈ {0, 1}2 but we suppress the subscript c for simplicity.

13

initial treatment group. It remains to determine which values of ηc are consistent with the equality
min(ηc , ω1,1 + χc ) = ω1,1 . Since χc > 0, it follows that ηc = ω1,1 .
Third, consider the case where |Sc | is odd and ω∗,1 < d|Sc |/2e. As in Example 2, in this
case there are two possibilities for the number χc of transfers in the final stage. Specifically, χc ∈
{b|Sc |/2c−ω∗,1 , d|Sc |/2e−ω∗,1 }. These χc transferred children must be among the ω1,0 members


ω1,0
ω1,0
with working mothers in the final control group. Thus, there are b|S |/2c−ω
+ d|S |/2e−ω
∗,1

c

c

∗,1

possibilities for the initial treatment group. Let ϑδc be the set containing the Hotelling statistics for
those possibilities. Then, δc ≥ min ϑδc . The number mc of children with working mothers initially
η
assigned treatment is either equal to ω1,1 +b|Sc |/2c−ω∗,1 or equal to ω1,1 +d|Sc |/2e−ω∗,1 . Let ϑc
be the set of values of ηc consistent with the equality min(ηc , mc ) = ω1,1 . If mc = ω1,1 +d|Sc |/2e−
ω∗,1 , then ηc = ω1,1 , since d|Sc |/2e > ω∗,1 . However, if mc = ω1,1 +b|Sc |/2c−ω∗,1 , there are two
sub-cases: if b|Sc |/2c > ω∗,1 , then ηc = ω1,1 ; but if b|Sc |/2c = ω∗,1 , then ηc ∈ {ω1,1 . . . , ω1,∗ }.
η
Therefore, the special home visiting slots can be partially identified as follows: ηc ∈ ϑc , where
η
η
ϑc = {ω1,1 . . . , ω1,∗ } if b|Sc |/2c = ω∗,1 , and ϑc = {ω1,1 } if b|Sc |/2c > ω∗,1 .
This general characterization of the partial identification of satisficing thresholds δc applies to
all cohorts c ∈ {0, 1, 2, 3, 4} but that of the special home visiting capacity constraints ηc applies
only to cohorts c ∈ {2, 3, 4}. However, similar reasoning can be used to partially identify the
capacity constraint η0,1 for pooled cohorts 0 and 1.27

3.3

Applicability of Our Approach to Other Compromised Experiments

Our approach can be applied to many of the studies that Bruhn and McKenzie (2009) criticize,
especially experiments using undocumented re-randomization. All of these experiments have the
feature that some criterion determines “satisfactory balance.” For example, Bruhn and McKenzie
(2009) quote a survey response that says, “[experimenters] regressed variables like education on
assignment to treatment, and then re-did the assignment if these coefficients were ‘too big.’” With
appropriate modifications, our model of satisficing thresholds directly applies to experiments conducted in such a subjective and incompletely documented manner. Suitable adjustments include
replacing Hotelling’s statistic in our model with studentized regression coefficients (selected by
pretesting or otherwise) or other metrics actually used to measure covariate imbalance between the
27 Specifically,

η0,1 ∈ {η ∈ {0, . . . ,

0,1

P

0,1

Mi } : min(η, χ0 + χ1 + ω1,1 ) = ω1,1 , χ0 ∈ C0 , χ1 ∈ C1 }, where
0,1
ω1,1 = i∈S0 ∪S1 Mi Di and Cc = {d|Sc |/2e − ω∗,1,c , max{0, b|Sc |/2c − ω∗,1,c }} for c ∈ {0, 1}. In our application,
η0,1 ∈ {3}. Since we do not make assumptions on the missing mother’s working status at baseline for a subject in
wave 0 and the missing gender of another subject in wave 1 (among the five who dropped out of the initial sample of
128 for extraneous reasons), our partial identification of δ0 and δ1 depends on the values in the partially identified set
for the missing variables. Since we do not make assumptions on the two missing binary variables, this is a strength of
our analysis, despite quadrupling the computational cost. We also use known information that there was at least one
transfer in wave 0 (Weikart et al., 1964) to narrow the partially identified set for that cohort.
i∈S0 ∪S1

P

14

treatment and control groups. Our methods for partially identifying the underlying randomization
rules can be used when the subjective satisficing thresholds are not documented. Even though we
only use one balancing criterion (Hotelling’s statistic) for dimensionality reduction in our definition of Uc (·), it can be trivially modified to accommodate multiple balancing criteria. In addition,
if the experiment has strata instead of cohorts, the c’s in our model would correspond to strata.
If an experiment does not have transfers after forming the intermediate treatment and control
groups, then there are no capacity constraints, i.e., the ηc ’s play no role. However, in some social
experiments, post-randomization transfer of some participants from the control to the treatment
group can occur if additional funding for the intervention becomes available. For example, waitlist control groups are used in some clinical studies. While this is the reverse of what occurred in
the Perry experiment, our model (with appropriate modifications) can be readily applied. Overall,
our approach can be adapted to analyze a variety of compromised experiments across multiple
disciplines.

4

Hypotheses of Interest and Inference

The conventional way to analyze randomized experiments is to posit a null hypothesis that the average effect of treatment is zero and to proceed testing it with large-sample methods using asymptotic
or bootstrap distributions. Given the relatively small size of many experimental samples, reliance
on large sample methods can be problematic.28
In some settings permutation tests can be used to test the null hypothesis that the outcomes in
the control group have the same distribution as those in the treatment group without relying on
large-sample theory. Permutation tests exploit the property that treatment and control labels within
the same strata are exchangeable under the null hypothesis of a common outcome distribution. If
randomization of the treatment status did not involve explicit stratification on baseline covariates,
permutation tests need to make restrictive assumptions on the strata within which treatment and
control labels are exchangeable. This approach is used by Heckman et al. (2010a).29 They assume
that conditioning on covariates solves the problem of post-random assignment reallocation but
without any explicit model for why it is effective in doing so.30
This paper uses knowledge of the randomization protocol to draw inferences about treatment
effects. Once a precise null hypothesis is specified, we can determine the distribution of estimates
28 In

a set of 53 studies of randomized controlled trials published in some leading economics journals, Young
(2019) also finds that experimental results obtained using asymptotic theory are misleading, relative to results based
on randomization tests.
29 However, unless the permutation method reflects the method used for random assignment of the treatment, permutation tests do not in general allow us to test hypotheses about counterfactual outcomes of the individual Perry
participants.
30 In practice, their approach relies on large-sample methods in using regression analysis to condition on covariates.

15

generated by the randomization scheme and assess the statistical significance of the observed treatment effects.
In this section, we first formulate our hypotheses of interest. We then discuss conventional
inferential procedures. Finally, we introduce worst-case (least favorable) randomization tests and
discuss how to conduct them using stochastic approximations, and then we compare our methods
with alternative approaches for inference with imperfect randomization.

4.1

Hypotheses of Interest

Let Y 1 be the treated outcome, Y 0 be the untreated outcome, Z represent background variables, and
F be their joint distribution at the population level. The conventional approach tests the hypothesis
HC of equality of means, i.e.,
HC : EF [Y 1 − Y 0 ] = 0,
(4)
assuming that the realizations (Yi1 , Yi0 , Zi ) of those variables for individual i are distributed according to F for all i ∈ P, where P is the set of experimental subjects. Because each participant in our
sample is assigned to either the treatment group or the control group, we only observe either Yi1 or
Yi0 for each i ∈ P. The hypothesis HC is traditionally tested by applying large-sample methods
to the observed data (Yi , Di , Zi )i∈P , where Di is the treatment status, Yi = Di Yi1 + (1 − Di ) Yi0 ,
and Zi is the vector of pre-program covariates. It is of interest to conduct tests about the missing
counterfactual outcomes within our sample, even though tests of population-level parameters are
more commonly employed.
Instead of appealing to some hypothetical long-run sampling experiment to conduct inference,
we seek knowledge of the sample in hand. One hypothesis of interest is whether the average
treatment effect within the sample is zero, i.e.,
HN :


1 X 1
Yi − Yi0 = 0, 31
NP

(5)

i∈P

where NP = |P|. A special case of HN is the sharp null hypothesis of no treatment effects
whatsoever for each participant:
HF : τi ≡ Yi1 − Yi0 = 0

(6)

for all i ∈ P,32 Fisher’s (1925; 1935) null hypothesis. It involves a joint test of zero individual
31 This

is attributed to Neyman (1923).

32 While this formulation states that each individual treatment effect τ
i

is zero, the analyst may fix each τi at a desired
value for hypothesis testing. Such a hypothesis is often called sharp because it specifies one set of counterfactual
outcomes for the participants.

16

treatment effects and is trivially equivalent to HN if there is no heterogeneity in the treatment
effects. The advantage of Fisher’s hypothesis HF is that it provides a testable model in which
all the counterfactual outcomes are specified.33 Such hypothesis testing can be conducted using
our knowledge of the randomization protocol without relying on large-sample theory. With all the
counterfactual outcomes specified, we can learn about the randomization distribution of treatment
effects, and we can gauge the extent to which the observed data can be rationalized using the
specified null model.34
Hypothesis HN nests the sharp null hypothesis HF . In general there are many configurations
of the individual treatment effects that are all consistent with HN . Thus, to test HN using only
limited knowledge of the randomization protocol, we would need to test each one of all the sharp
null hypotheses like HF that imply HN .35 However, a non-rejection of HF implies non-rejection
of HN , and so testing other sharp null hypotheses may not be necessary if we are unable to reject
HF . Of course, a rejection of HF would not imply a rejection of HN . The latter is a very
conservative criterion. We next discuss conventional hypothesis testing procedures.

4.2

Conventional Hypothesis Testing Procedures

For tests of population-level parameters such as HC in equation (4), the most commonly reported
measure of statistical significance is the asymptotic p-value. For completely randomized experiments, it can be interpreted as the p-value based on a large-sample approximation of the distribution of an estimator, say difference-in-means, over all possible randomizations under the null
hypothesis HN (Neyman, 1923). Li et al. (2018) derive an asymptotic theory of the differencein-means estimator in experiments involving rerandomization with a pre-specified balancing rule
using the Mahalanobis distance, for which the asymptotic distribution of the estimator is a linear
combination of normal and truncated normal variables. Resampling methods are also widely used
to quantify statistical uncertainty. For example, the bootstrap standard error is reported in many
research papers with an associated bootstrap p-value.
Permutation tests are often used when researchers are interested in testing whether treatment
and control groups have a common outcome distribution without relying on large-sample theory.
33 Note

that we observe either Yi1 or Yi0 for each participant i ∈ P. Thus, under the null model (6), the other
counterfactual outcome can be imputed according to the fact that Yi1 = Yi0 . In general, if τi is hypothesized to be equal
to a number τi◦ , the counterfactual outcomes (Yi1 , Yi0 ) under the null model are equal to (Yi + τi◦ , Yi ) if Di = 0 and is
equal to (Yi , Yi − τi◦ ) if Di = 1 for all i ∈ P.
34 See Athey and Imbens (2017) and Abadie et al. (2020) for background on this topic. Also, note that our randomization tests are conditional tests that exploit random variation in the treatment status but fix the other observed data.
See Lehmann (1993).
35 When the outcomes under consideration are binary and the experiment involves a completely randomized design,
there are are strategies to test the weak null hypothesis in a computationally feasible way (see, e.g., Li and Ding, 2016;
Rigdon and Hudgens, 2015).

17

Such tests rely on the property that the treatment and control labels are exchangable within each
stratum of the experiment under the null hypothesis of a common distribution. In their permutation
tests, Heckman et al. (2010a) use strata defined by wave, gender, and indicator for above-median
socioeconomic status, assuming that experimental labels within each stratum are exchangeable.
To compare their permutation procedures with the methods developed in this paper, we use a
simplified version of their permutation tests using block permutations within cohorts of eldest
participant-siblings (whose treatment statuses determine that of their younger participant-siblings).
In the Perry context, Heckman et al. (2020) develop an extension of permutation tests to account for imperfect randomization. In this paper, we offer an alternative design-based approach to
conduct inference for a broader class of compromised experiments. We first present our approach
and then compare it with theirs.

4.3

Worst-Case Randomization Tests

This paper advocates and uses worst-case approximate randomization tests to analyze the Perry
data. Fisher’s sharp null hypothesis HF specifies all the counterfactual outcomes, which are imputed according to the hypothesis using the observed data. If we knew the exact randomization
protocol of the Perry experiment, we could measure where the observed test statistic falls along its
exact randomization distribution, i.e., the distribution of the test statistic over all possible treatment
status vectors that could have been hypothetically generated by the randomization protocol. The
more extreme the observed test statistic falls along the null distribution, the more incompatible the
observed data would be with the sharp null hypothesis. However, for Perry and many other social experiments, the exact randomization protocol is unknown: even in our stylized model of the
randomization protocol, the satisficing thresholds and capacity constraints are only partially identified. To account for this ambiguity, we could in theory conduct randomization tests36 over the
set of all possible randomization protocols. Thus, we could conduct the worst-case randomization
test using the least favorable distribution among all the possible randomization distributions. This
results in the following worst-case p-value that serves as an upper bound for the true randomization
p-value:
(7)
pw = sup PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)},
γ ∗ ∈Ξ

where Ξ is the partially identified set for γ = (δ0 , . . . , δ4 , η0,1 , η2 , η3 , η4 ), the vector of true values
of parameters (satisficing thresholds and capacity constraints), PΛγ ∗ represents probability under
the probability space Λγ ∗ of randomizations generated by the protocol parameterized by γ ∗ , D̃γ ∗
represents a random treatment status vector defined on the probability space Λγ ∗ , D denotes the
36 These tests are not strictly exact because our model simplifies the actual randomization procedure and can at best

be considered a useful approximation of the true model of the protocol.

18

observed treatment status vector, and T(·) is the chosen test statistic such that T(·) maps a treatment status vector to a real number measuring the magnitude of the outcome difference between
the treatment and control groups. Since the sharp null hypothesis specifies counterfactual outcomes, the data (Yi0 , Yi1 , Zi )i∈P are fixed according to HF , and the only random variation in the
above construction comes from the randomization protocol. The sample space Ωγ ∗ of the uniform
probability space Λγ ∗ , on which the random treatment status vector D̃γ ∗ is defined, is given by
4

Ωγ ∗ =

Uc (δc∗ )
×
c=0

!
× ΩQ,Vγ ∗ ,

(8)

×4

where U(δ0∗ , . . . , δ4∗ ) ≡ c=0 Uc (δc∗ ) is the Cartesian product of the sets of admissible partitions of
Sc (in the initial step of the protocol) across all cohorts c ∈ {0, . . . , 4}, and ΩQ,Vγ ∗ is the Cartesian
product of the sample spaces for all other random variables, characterizing the outcomes Qc of the
coin flips and vectors of variables Vi,c used for determining which children of working mothers are
transferred from the treatment to control group in the last step across all cohorts c ∈ {0, . . . , 4},
used in the randomization protocol parameterized by γ ∗ .37 Using this notation we establish the
following proposition for any α ∈ (0, 1):
Proposition 1. Under the model of randomization protocol in Section 3, the hypothesis test that
rejects the sharp null hypothesis whenever pw ≤ α controls the Type I error rate at level α.
Proof. Let pγ ∗ (D) ≡ PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)} for all γ ∗ ∈ Ξ, let pw (D) ≡ supγ ∗ ∈Ξ pγ ∗ (D)
represent the worst-case p-value, and let ψα (D) ≡ I{pw (D) ≤ α} be the test for a given D, a
realization of the random treatment status vector defined on the probability space Λγ , where γ
is the true value of the model parameter. Since pγ (D) ≤ pw (D) by construction, it follows that
EΛγ [ψα (D)] = EΛγ [I{pw (D) ≤ α}] ≤ EΛγ [I{pγ (D) ≤ α}] = PΛγ {pγ (D) ≤ α} ≤ α under HF .
This is a trivial extension of the simple standard argument used to show the finite-sample validity
of randomization tests (see Lehmann and Romano, 2005). The proposition can be equivalently
stated in terms of a critical value for the test statistic, as in Heckman et al. (2020).

Although it would be ideal to compute the exact value of pw , it is computationally not feasible.
As is common practice in computing permutation and randomization p-values (see Lehmann and
Romano, 2005), we resort to stochastic approximations. Even so, there are two challenges in
estimating the worst-case p-value. First, approximating the probability PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)}
for a given value γ ∗ ∈ Ξ is computationally demanding. Second, estimating pw based on such tail
probability estimates for a finite number of points on Ξ is also challenging. We tackle these two
challenges sequentially and discuss how we handle some forms of stochastic approximation errors.

×

×

Mc
{v
m=1


ΩQ,Vγ ∗ = {0, 1}5 ×
c∈{(0,1),2,3,4}
P
P
S
i∈S0 S1 Mi and Mc =
i∈Sc Mi for all c ∈ {2, 3, 4}.
37 Specifically,

19


∈ {0, 1}m : ||v||1 = min(ηc∗ , m)} , where M0,1 =

4.3.1

Approximating Tail Probabilities of Randomization Distributions

The first challenge is to approximate PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)} for a given value γ ∗ in the partially
∗ , η ∗ , η ∗ , η ∗ ) ∈ Ξ. Our approach is to break up the
identified set, i.e., for γ ∗ = (δ0∗ , . . . , δ4∗ , η0,1
2 3 4
sample space of Λγ ∗ into two parts, compute the tail probability (measuring how extreme the
observed test statistic is in its randomization null distribution) for each of these two parts, and then
use the law of total probability and Monte Carlo methods to get the desired final result. To do so,
†
we introduce additional notation. Let δc be the lower bound of the partially identified set for the
†
true value of the satisficing threshold δc for c ∈ {0, . . . , 4}. Then, for any given value δc∗ ≥ δc ,
observe that
Uc (δc∗ ) = Xc ∪ Yc (δc∗ ),
(9)
where
†

Xc = {(A, B) ∈ Uc (∞) : τc2 (A, B) ≤ δc }

(10)

and
†

Yc (δc∗ ) = {(A, B) ∈ Uc (∞) : δc < τc2 (A, B) ≤ δc∗ },

(11)

for all c ∈ {0, . . . , 4}. Thus, we can use Uc (∞), which is the set with an infinite satisficing threshold such that all allowed partitions of Sc are satisfactory, to construct Xc , Yc (δc∗ ), and Uc (δc∗ ). The
†
set Xc has elements with Hotelling statistics below the lower bound δc of the partially identified
set for the satisficing threshold. The other set Yc (δc∗ ) = Uc (δc∗ ) \ Xc has elements with Hotelling
†
e X = 4 Xc be the Cartesian product of the sets Xc across
statistics between δc and δc∗ . Let Ω
c=0
4
Y
∗
∗
X
e X . Both Ω
e X and Ω
e Y∗ can be
e
e
cohorts, and let Ωγ ∗ = U(δ0 , . . . , δ4 ) \ Ω = { c=0 Uc (δc∗ )} \ Ω
γ
constructed using U(∞, . . . , ∞) by discarding elements in their respective complements. Since
e X remains constant. Notice that
the sets Xc do not depend on the values δc∗ , the set Ω

×

×

eX ∪ Ω
e Y∗ ) × ΩQ,V ∗ = (Ω
e X × ΩQ,V ∗ ) ∪ (Ω
e Y∗ × ΩQ,V ∗ ).
Ωγ ∗ = (Ω
γ
γ
γ
γ
γ

(12)

Y
X
eX
Let ΛX
γ ∗ and Λγ ∗ be the uniform probability spaces over the sample spaces Ωγ ∗ ≡ Ω × ΩQ,Vγ ∗
e Y∗ × ΩQ,V ∗ , respectively. In addition, let
and ΩY∗ ≡ Ω
γ

γ

x(γ ∗ )

γ

e X × ΩQ,V ∗ |
e X | · | ΩQ,V ∗ |
|Ω
|Ω
eX |
|Ω
γ
γ
=
,
=
=
≡
eX ∪ Ω
e Y∗ | · | ΩQ,V ∗ |
eX ∪ Ω
e Y∗ |
| Ωγ ∗ |
| Ωγ ∗ |
|Ω
|
Ω
γ
γ
γ
| ΩX
γ∗ |

(13)

which is the proportion of elements in the sample space Ωγ ∗ belonging to ΩX
γ ∗ . Note that the last
∗
X
e and Ω
e Y∗ constructed
equality above implies that x(γ ) can be simply computed with the sets Ω
γ

20

using U(∞, . . . , ∞).38 Then, by the law of total probability, we have that
Y
∗
PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)} = x(γ ∗ )·PΛX {T(D̃X
γ ∗ ) ≥ T(D)}+y(γ )·PΛY∗ {T(D̃γ ∗ ) ≥ T(D)}, (14)
∗
γ
γ

Y
where D̃X
γ ∗ and D̃γ ∗ represent random treatment status vectors defined on the probability spaces
Y
∗
∗
ΛX
γ ∗ and Λγ ∗ , respectively, and y(γ ) = 1 − x(γ ). Since the sample spaces in the model are large,
we use Monte Carlo draws from the probability spaces through rejection sampling to stochastically
approximate the tail probability PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)}.39,40 Our approach provides a feasible
way to estimate PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)} for points γ ∗ in Ξ efficiently using rejection sampling.

4.3.2

Estimating and Bounding the Worst-Case Tail Probability

The second challenge is to estimate or bound the worst-case tail probability pw . Taking the supremum of tail probabilities over points in the set Ξ may seem intractable, since Ξ is the Cartesian
product of a finite set and a non-compact set.41 However, we exploit the fact that Uc (ν) = Uc (∞)
for all ν ≥ ∆c , where ∆c = max{τc2 (A, B) : (A, B) ∈ Uc (∞)}, since Uc (∞) is a finite set, for
all c ∈ {0, · · · , 4}. Let Ξ◦ be the compact subset of Ξ given by
†

Ξ◦ = {γ̃ ≡ (δ̃0 , . . . , δ̃4 , η̃0,1 , η̃2 , η̃3 , η̃4 ) ∈ Ξ : δc ≤ δ̃c ≤ ∆c ∀c}.

(15)

It then follows that
pw ≡ sup PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)} = max PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)}.
∗
◦
∗
γ ∈Ξ

γ ∈Ξ

(16)

×

4

= c=0 Uc (∞), a very large set, to approximate x(γ ∗ ).
39 We use 400 Monte Carlo draws from ΛX to approximate P
X
γ∗
ΛX∗ {T(D̃γ ∗ ) ≥ T(D)}. This is effectively importance

38 We use 500,000 Monte Carlo draws from U(∞, . . . , ∞)

γ

∞ = (∞, . . . , ∞, η ∗ , η ∗ , η ∗ , η ∗ ), and
sampling. In addition, we use 2600 Monte Carlo draws from ΛY
γ ∞ , where γ
0,1 2 3 4

use rejection sampling to draw random samples from ΛY
{T(D̃Y
γ ∗ for approximating PΛY
γ ∗ ) ≥ T(D)}. It takes much
γ∗
∗
longer to compute these tail probabilities than to compute x(γ ). Limited computational power restricted the number
of Monte Carlo draws.
40 Since the randomly sampled treatment status vectors are i.i.d. and uniformly distributed on corresponding sample
spaces, for a given γ ∗ the associated p-value stochastic approximations can be used to construct valid tests. For details,
see Section 4 of Romano (1989), Section 3.2 of Romano and Wolf (2005), or Section 15.2.1 of Lehmann and Romano
(2005). Although this holds when γ ∗ is taken as given, our main object of interest is the worst-case p-value in equation
(7). Since it is infeasible to compute a p-value for each γ ∗ ∈ Ξ, we also resort to stochastic approximations of the
supremum in equation (7). In Section 4.3.2, we discuss how we account for uncertainty in the stochastic approximation
of the worst-case p-value.

×

4

×

4

Ξ = c=0 [δc† , ∞) × ϑη0,1 × c=2 ϑηc , where δc† is the lower bound for the satisficing threshold δc ,
η
and ϑc is the finite partially identified set for the capacity constraint ηc .
41 Specifically,

21

Thus, it suffices to estimate the worst-case tail probability over the set Ξ◦ , which is compact.42 We
use stochastic approximations for this purpose as well. It is computationally infeasible to compute
a p-value for each of the points in the set Ξ◦ and take the maximum of those p-values. To deal with
S
this challenge, we first write Ξ◦ = Ll=1 Ξ◦l , where Ξ◦1 , . . . , Ξ◦L are disjoint hyper-rectangles that
form a partition of the set Ξ◦ . In our application, L = 20, and each hyper-rectangle represents the
partially identified set for (δ0 , . . . , δ4 ) at fixed values of (η0,1 , η2 , η3 , η4 ).43 Then, note that
pw = max{p1w , . . . , pLw },

(17)

plw = max◦ PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)}

(18)

where
γ ∗ ∈Ξl

for l ∈ {1, . . . , L}. We approximate plw for each l ∈ {1, . . . , L} using the p-values pl(1) , . . . , pl(S)
arranged in descending order for S = 900 uniformly sampled random points on the set Ξ◦l .44
We estimate plw for each l ∈ {1, . . . , L} using the maximum order statistic p̃lM :
p̃lM = max

1≤s≤S

pl(s) = pl(1) ,

(19)

which converges almost surely to plw as S → ∞. However, this estimate may have stochastic
approximation error. One way to deal with stochastic approximation-related uncertainty in p̃lM
is by constructing a confidence band for plw . To do so, we construct an upper bound based on
de Haan’s (1981) 90% asymptotic confidence band for the true maximum using the S randomly
sampled p-values. The upper confidence bound p̃lD is given by
l ,
p̃lD = pl(1) + (pl(1) − pl(2) ) · KdH

42 In

(20)

fact, we can further simplify the worst-case tail probability. Let Γc = {τc2 (A, B) : (A, B) ∈ Uc (∞)}, which
is a finite set, for all c ∈ {0, . . . , 4}, and let ΞΓ = {γ̃ ≡ (δ̃0 , . . . , δ̃4 , η̃0,1 , η̃2 , η̃3 , η̃4 ) ∈ Ξ◦ : δ̃c ∈ Γc ∀c}, which is also
a finite set. Then, we have that pw = maxγ ∗ ∈ΞΓ PΛγ ∗ {T(D̃γ ∗ ) ≥ T(D)}. However, even though the set ΞΓ is finite,
its size is too large in practice, making stochastic approximations still necessary.
43 Note that in our application, η , η , and η are point-identified while η is partially identified to be in the set
0,1 2
3
4
{0, . . . , 4}. Thus, (η0,1 , η2 , η3 , η4 ) has 5 possible values. In addition, since we do not know the mother’s working
status at baseline for a subject in wave 0 and the gender of a subject in wave 1 (both of whom are among the 5
participants who dropped out of the study for extraneous reasons), there are 4 possible configurations of the two
missing binary variables. Thus, in total there are L = 5 × 4 = 20 hyper-rectangles that make up Ξ◦ .
44 To ensure that we are covering Ξ◦ and its edges well when sampling the random points, we use a normalization.
We use the distribution Fτc2 of Hotelling statistics on Uc (∞) to normalize δc so that Fτc2 (δc ) ∈ [Fτc2 (δc† ), 1], a compact
set, for all c ∈ {0, . . . , 4}. Thus, γ and Ξ◦l are monotonically transformed accordingly in practice. We can do this
because Uc (∞) is a finite set and Uc (δc ) ≡ {(A, B) : A ⊂ Sc , B = Sc \ A, |A| = d|Sc |/2e, τc2 (A, B) ≤ δc } is
equivalent to the set {(A, B) : A ⊂ Sc , B = Sc \ A, |A| = d|Sc |/2e, Fτc2 (τc2 (A, B)) ≤ Fτc2 (δc )}.

22

l is a factor provided by de Haan (1981) for the 90% asymptotic confidence bound.45
where KdH
Thus, the 90% confidence interval for plw is given by [p̃lM , p̃lD ]. Finally, the true worst-case p-value
pw can be approximated by the worst-case maximum (max.) p-value p̃M given by

p̃M = max{p̃1M , . . . , p̃LM },

(21)

and its upper confidence bound is given by the worst-case de Haan p-value p̃D as follows:
p̃D = max{p̃1D , . . . , p̃LD },

(22)

which provides at least 90% coverage as S → ∞. Of course, these stochastic approximations
affect the exact finite-sample validity of the resulting hypothesis tests, but the validity of these
approximations can be arbitrarily increased with adequate additional computational power. This is
an issue common to most resampling methods in statistics (see Lehmann and Romano, 2005).
In the previous discussion, the test statistic T(·) used to compute the worst-case tail probability
is left general. There is reason to suspect that the choice of the test statistic matters, as shown for
permutation tests by Chung and Romano (2013, 2016). Wu and Ding (2020) show that using studentized test statistics in certain randomization tests can control type I error asymptotically under
certain weak null hypotheses while preserving finite-sample validity under sharp null hypotheses.
Their theory ignores covariates and is limited to completely randomized factorial experiments and
stratified or clustered experiments. However, they conjecture that “the strategy [of using studentized test statistics to make randomization tests asymptotically robust under weak null hypotheses
while retaining their finite-sample validity under sharp null hypotheses may also be] applicable
for experiments with general treatment assignment mechanisms” (Wu and Ding, 2020). While
we do not attempt to prove or disprove their conjecture in the Perry experimental setting, we take
it seriously given their results for certain randomization tests along with Chung and Romano’s
(2013, 2016) results for permutation tests. Thus, we provide worst-case p-values using both the
nonstudentized and studentized test statistics.
4.3.3

Multiple Testing

Since PΛγ {pw ≤ α} ≤ α under HF for any α ∈ (0, 1) by Proposition 1, Holm (1979) tests
of multiple hypotheses based on the worst-case p-values would also have finite-sample validity.
Multiplicity-adjusted p-values can be computed as follows. Let ρ(1) , . . . , ρ(K) be the associated
i−1
√
l
l
0.9αdH − 1
, where αdH
= − ln( S)/ ln[(pl(3) − pl √ )/(pl(2) − pl(3) )], based on
( S)
de Haan’s (1981) result. In the context of estimating the minimum of a function over a compact set using order
statistics, de Haan (1981) proposes construction of a confidence band for the minimum. We apply this result without
loss of generality in our context (estimation of the maximum rather than the minimum).
45 Specifically,

l
KdH
=

h

23

single worst-case p-values arranged in ascending order. Then, the Holm stepdown p-values adjusted for multiple testing are given by %(k) = maxj≤k min(1, (K − j + 1) ρ(j) ) for k ∈ {1, . . . , K}.
However, these adjusted p-values can be even more conservative because they assume least favorable dependence structure between the single worst-case p-values (Romano et al., 2010), making
this the “worst-case” of the “worst-case.” However, slightly less conservative multiple hypothesis
tests are available in the literature (see Romano and Shaikh, 2010; Romano and Wolf, 2005). Since
it is unclear how much improvement in terms of power they provide relative to Holm tests in our
context, we do not discuss the more computationally involved stepdown procedures in this paper.

4.4

Comparing Methods for Inference with Imperfect Randomization

Our approach complements that of Heckman et al. (2020), who improve on the methodology of
Heckman et al. (2010a) by (i) exploiting a symmetry generated by the Perry randomization protocol, (ii) using finite-sample inference that accounts for imperfect randomization, and (iii) making
transfers in the fifth step of the randomization protocol depend on a binary variable indicating
whether the mother is available for home visits, assuming program infrastructure is available to
support it. It is only partially observed in their model. We also exploit the symmetry: Qc represents
the result of a fair coin flip to determine which of the two initially undesignated groups becomes
the intended treatment group. However, we model other features of the protocol differently.
Heckman et al. (2020) model the reassignment of children of some working women by introducing a partially observed binary variable Ui that equals 1 if the mother of participant i was
unavailable for home visits and 0 otherwise. It is known only for children of non-working mothers,
for whom Ui = 0, and for the children of working mothers in the final treatment group, who also
have Ui = 0. For children of working mothers in the control group, Ui is not known and could be
either 0 or 1. To deal with this difficulty, Heckman et al. (2020) construct two permutation tests.
The first test sets Ui to 0 for all children of working mothers in the final control group and conducts
a generalized permutation test accordingly. The second test: (i) samples a vector of Ui from the
space of possibilities for Ui ; (ii) conducts a generalized permutation test given the sampled vector
of Ui and obtains the corresponding permutation p-value; and (iii) repeats steps (i) and (ii) until
the space of possibilities is exhausted. It then takes the maximum p-value among the computed
p-values. Our worst-case inferential methods are similar in spirit. However, there are three key
differences between our approach and theirs.
First, Heckman et al. (2020) interpret Ui as a fixed trait of mothers regardless of the (random)
circumstances facing program administrators. However, whether or not a working mother and her
child are visited at home (through special arrangements, e.g., on a weekend) depends, at least in
part, on the availability and capacity constraints of the Perry staff. While Ui = 0 for non-working

24

mothers in both papers, we do not view Ui as a fixed binary trait of working mothers. Consistent
with our review of the randomization protocol, we assume that children of working mothers are
able to participate in the program if special arrangements, such as weekend home visits, are made
for them. In our model, there are capacity constraints for making special arrangements, so only
a limited number of slots are available.46 In their model, if Ui = 1 for a working mother, her
child would always be placed in the control group, because she would not accept any special
accommodations even if provided by the Perry staff. Unlike the Vi,c variable that determines postrandomization transfers in our model, the Ui characteristic in their model is allowed to be related
to potential outcomes, but this is a consequence of its interpretation as a fixed trait of mothers
independent of the capacities of program administrators.
Second, their procedure assumes that “some participants were exchanged between the treatment and control groups in order to ‘balance’ gender and socio-economic status score while keeping Stanford-Binet IQ score roughly constant.”47 However, as shown in Appendix B, Perry data
from wave 4 reveal that the exchanges were not necessarily between consecutively ranked IQ pairs.
Our approach accommodates this feature while also making more explicit the notion of balance.
Third, on a more minor note, we incorporate the five children (out of the original 128) who
dropped out of the study due to extraneous reasons, since those five children were also a part of
the initial randomization protocol. Our approach can also more readily be applied than that of
Heckman et al. (2020) to a variety of compromised experiments, including many discussed by
Bruhn and McKenzie (2009). We next demonstrate that there are important differences between
inferences obtained from our procedure and theirs.

5

Reanalysis of Heckman et al. (2020)

This section uses the methods developed in this paper to reconsider the conclusions reached by
Heckman et al. (2020) on the Perry participants through age 40. In a companion paper, we apply
these methods to analyze fresh samples of participants through age 55 and their adult children
(Heckman and Karapakula, 2020). Analyzing the new wave of data in this paper would raise a
variety of new issues about sampling procedures better left for another occasion.
We first list our estimators of treatment effects. Using the corresponding test statistics, we then
apply our worst-case inferential methods to reanalyze the results in Heckman et al. (2020).
46 Our

model is limited in the sense that it does not allow for heterogeneity among working mothers in their availability for special arrangements. We assume that the Perry administrators choose with equal probability which working
mothers get special arrangements.
47 This is Step 40 in their paper. Accordingly, their tests involve “permuting treatment status among those families
with the same observed and unobserved characteristics (defined by the characteristics of the eldest child in the case
of families with multiple children).” In practice, they discretize socioeconomic status (SES) into a binary indicator of
above-median SES.

25

5.1

Estimators and Test Statistics for Hypothesis Testing

A variety of test statistics and estimators can be used in our approach and that of Heckman et al.
(2020). Our empirical work focuses conventional ones often used in practice. Let Di represent the
treatment status of participant i, and let Zi be the vector of baseline variables.48 In addition, let
Yi denote the observed outcome of interest of participant i in a relevant subsample P containing
NP = |P| participants, and let Yid be the counterfactual outcome of participant i when his or her
treatment status Di is fixed at d ∈ {0, 1}. In switching regression notation (Quandt, 1958, 1972),
Yi = Di Yi1 + (1 − Di ) Yi0 .

(23)

The average treatment effect τ̄ in the subsample P given by
τ̄ =


1 X 1
Yi − Yi0
NP

(24)

i∈P

and is conventionally estimated by a difference-in-means (DIM) estimator that takes raw mean
differences between non-attrited treated and control observations. However, the randomization
procedures used in Perry and other similar experiments only justify conditional independence:
(Yi1 , Yi0 ) ⊥⊥ Di | Zi . Exploiting this property and controlling for Zi in a regression of Yi on Di and
Zi using complete case observations, we obtain the ordinary least squares (OLS) estimator.49 It
would be desirable to relax linearity, but the Perry sample size makes this impractical.
All of these estimators assume that non-response is determined at random or at random conditional on observed covariates. Let Ri be an indicator of whether Yi is missing. It could depend
on the treatment status Di and the pre-program covariates Zi . The augmented inverse probability weighting (AIPW) estimator allows for this possibility by using the weaker assumption that
Yi ⊥⊥ Ri | Di , Zi . The AIPW estimator of the treatment effect is

1 X 1
π̂i − π̂i0 ,
NP

(25)


I{Ri = 1, Di = d}  d
d
+
Yi − Ŷi .
λ̂di φ̂di

(26)

Π̂AIPW =

i∈P

where
π̂id

=

Ŷid

In this expression, Ŷid is the ordinary least squares (projection) estimator of the conditional expec48 In

the Perry context, it consists of the four pre-program covariates used during the randomization phase, i.e.,
Stanford-Binet IQ, index of socioeconomic status, gender, and mother’s working status.
49 Both OLS and DIM estimators can be studentized using their cluster-robust asymptotic standard errors, allowing
for correlation between error terms of the participant-siblings in the Perry experiment.

26

tation E[Yi | Zi , Di = d, Ri = 1] for d ∈ {0, 1}, φ̂di is an estimator of Pr(Di = d | Zi ), the i-th participant’s propensity of being in experimental state d, and λ̂di is an estimator of Pr(R1i = 1 | Zi , Di = d),
which is the propensity of having a non-missing outcome after fixing the treatment status Di , for
d ∈ {0, 1}. Propensity scores are often estimated using logits. The AIPW estimator adjusts the outcome based on pre-program covariates and corrects for covariate imbalance and various forms of
non-response.50 It is asymptotically unbiased and has a double robustness property: the estimator
is robust to misspecification of either the propensity score models or the models for counterfactual
outcomes, but not both.51 For this reason, the AIPW estimator is sometimes preferred over the
DIM and OLS estimators.52 We use the studentized version of the AIPW estimate as our main test
statistic in our empirical analysis.53
We could use a local average treatment effect (LATE) estimator, and other standard estimation
methods dealing with imperfect compliance, if we knew each observation’s initial treatment status.
However, in the Perry example, we do not know which members were transferred from the initial
treatment group to the control group in the last step of the randomization protocol. Given this
problem, we do not present LATE estimates.54
50 The

AIPW estimator also assumes conditional independence of the counterfactual outcomes and the treatment
status, i.e., (Yi1 , Yi0 ) ⊥
⊥ Di | Zi , which is valid because of the random assignment of the treatment status conditional on
pre-program variables. Note that the propensity score model used in the AIPW estimator is a direct consequence of the
law of conditional probability: Pr(Ri = 1, Di = d | Zi ) = Pr(R1i = 1 | Zi , Di = d) Pr(Di = d | Zi ) for d ∈ {0, 1}. In the
econometrics literature, the AIPW estimator is better known as a type of efficient influence function (EIF) estimator
(Cattaneo, 2010). The estimator given by equation (25) can be studentized using the empirical sandwich standard
error under the assumption that the propensity score and regression models are both correctly specified (Lunceford
and Davidian, 2004). For studentization,
we use a cluster-robust version of this asymptotic standard error, given by
P
P
the following formula: N1P [ j∈J ( i∈Fj π̂i1 − π̂i0 − Π̂AIPW )2 ]1/2 [|J|/(|J| − 1)]1/2 , where Fj represents a cluster of
participant-siblings in the set J of clusters. Our studentized test statistics are based on the asymptotic standard error
mainly for computational ease, but studentization based on the bootstrap standard error would be superior in theory.
51 See Robins et al. (1994), Lunceford and Davidian (2004), and Kang and Schafer (2007). The double-robustness
property (consistency despite certain forms of misspecification) is easier to understand by rewriting equation (26) as
follows: π̂id = Yid + (λ̂di φ̂di )−1 (I{Ri = 1, Di = d} − λ̂di φ̂di )(Yid − Ŷid ) for d ∈ {0, 1}. If the propensity score models
are correctly specified, the average value of λ̂di φ̂di consistently estimates the probability that I{Ri = 1, Di = d} = 1,
in which case the sample average of the whole second term in the rewritten expression for π̂id tends to zero. If, on
the other hand, the counterfactual outcome model is correctly specified, then the average value of Ŷid consistently
estimates the expectation of Yid , again in which case the sample average of the whole second term in the rewritten
expression for π̂id converges to zero. Thus, the AIPW estimator remains consistent for the average treatment effect if
either the propensity score models or the counterfactual outcome models are misspecified but not both.
52 However, we present estimates from all of these procedures in the appendix as a form of sensitivity analysis. The
AIPW estimator can become unstable if both the propensity score models and the counterfactual outcome models are
misspecified (Kang and Schafer, 2007). Thus, we do not solely rely on the AIPW estimator but use it in conjunction
with the DIM and OLS estimators.
53 Since AIPW clearly has an asymptotic justification, it is not strictly a small-sample procedure from an estimation
perspective. Nevertheless, we can conduct inference using its finite-sample worst-case randomization null distribution
using our design-based methods.
54 In theory, we could bound the LATE estimate by considering all possible values for each observation’s initial
treatment status, and then we could use the LATE bound as a test statistic for inference. However, this is very demanding computationally and thus not feasible in practice.

27

5.2

Empirical Analysis

We first conduct a head-to-head comparison of Heckman et al.’s (2020) methods and ours using
the same outcomes they analyze. Additionally, to compare the impact of using mean differences
versus AIPW test statistics in the conventional inferential approaches and our design-based worstcase inference, we extend the outcomes they study and analyze data on violent crime.
Tables 2 and 3 below report our reanalyses of Heckman et al. (2020), analyzing each outcome
one at a time using the doubly robust attrition-adjusted AIPW estimator. Tables 4 and 5 provide
stepdown p-values for the outcomes based on multiple testing. Extended versions of these tables
are presented in Appendices E through H using alternative test statistics for inference.55
In Tables 6 and 7, we reproduce Heckman et al.’s (2020) results and provide a side-by-side
comparison of their inferences with our own. The most stringent (max-U) single p-values they
report for the effects on the California Achievement Test (CAT) reading, arithmetic, language, mechanics, and spelling scores at age 14 in the male sample using the studentized DIM test statistic
are 0.036, 0.086, 0.012, 0.023, 0.012, respectively, which are lower than the asymptotic p-values
we report in Table 2. After adjusting for multiple testing, their adjusted max-U p-values are no
more than 0.086, based on which they conclude that these effects are statistically significant. In
contrast, using our approach, the worst-case maximum (single) p-values using studentized DIM
test statistic are 0.171, 0.119, 0.075, 0.054, 0.123, respectively. As shown in our Table 2, using
the studentized AIPW test statistic, our worst-case maximum p-values are 0.349, 0.291, 0.177,
0.133, 0.273, respectively,56 implying that the effects on the CAT scores for males are not statistically significant. Of course, the stepdown p-values for these outcomes shown in Table 4 are also
insignificant. Our inference for the female sample is qualitatively similar to theirs. As shown in
Table 3, the block of CAT scores for females is statistically significant at the 10% level. However,
the multiplicity-adjusted stepdown worst-case de Haan p-values in Table 5 are 0.12 or larger.
Table 4 reports stepdown p-values for male outcomes. No estimated effect (after age 5) remains statistically significant at the 10% level after adjusting for multiple hypothesis testing using
the worst-case maximum or worst-case de Haan p-values. However, in Table 5, which presents
stepdown analysis of female outcomes, the treatment effects on post-program outcomes (related to
CAT scores and education) are statistically significant at the 10% level using our worst-case maximum p-value. Nevertheless, all of these effects on female outcomes, except for one (high school
graduation), disappear when worst-case de Haan p-values are used.
55 In these appendices,

for each outcome we include the conventional p-values (i.e., asymptotic, bootstrap, and permutation p-values) and design-based p-values (i.e., worst-case maximum and worst-case de Haan p-values) associated
with each of the DIM, OLS, and AIPW estimators of treatment effects. We also include permutation and worst-case
p-values based on both nonstudentized and studentized test statistics. In addition, we include stepdown versions of the
worst-case p-values.
56 The corresponding worst-case de Haan (single) p-values are 0.382, 0.322, 0.210, 0.147, 0.302, respectively.

28

Table 2: Reanalysis of Male Outcomes in Heckman et al. (2020) Using Single Tests
Variable

Age

Untreated
mean

Treated
mean

AIPW
estimate

Asymptotic Bootstrap Permutation Worst-case Worst-case
p-value
p-value
p-value
max. p
de Haan p

Stanford–Binet IQ

4

83.077

94.909

8.988

0.0000

0.0000

0.0004

0.0049

0.0056

Stanford–Binet IQ

5

84.793

95.400

9.167

0.0000

0.0002

0.0004

0.0071

0.0071

Stanford–Binet IQ

6

85.821

91.485

3.056

0.0557

0.0512

0.0712

0.0872

0.1229

Stanford–Binet IQ

7

87.711

91.121

1.576

0.2040

0.2143

0.2104

0.2002

0.2198

Stanford–Binet IQ

8

89.054

88.333

−3.829

0.0512

0.0719

0.0556

0.1461

0.2396

Stanford–Binet IQ

9

89.026

88.394

−4.167

0.0398

0.0577

0.0472

0.1289

0.1457

Stanford–Binet IQ

10

86.026

83.697

−4.722

0.0225

0.0412

0.0292

0.0707

0.1022

CAT reading score

14

9.000

13.926

1.815

0.2957

0.3221

0.3112

0.3488

0.3823

CAT arithmetic score

14

8.107

16.000

3.095

0.2410

0.2629

0.2608

0.2909

0.3216

CAT language score

14

6.536

14.333

5.029

0.0815

0.0995

0.1076

0.1771

0.2098

CAT mechanics score

14

6.964

15.556

5.979

0.0538

0.0638

0.0712

0.1333

0.1467

CAT spelling score

14

11.536

18.519

3.171

0.2652

0.2865

0.2600

0.2734

0.3016

High school graduate

19

0.513

0.485

0.015

0.4550

0.4540

0.4868

0.5607

0.6036

Vocational training

40

0.333

0.394

0.071

0.2762

0.2886

0.2932

0.3984

0.4353

Highest grade completed

19

11.282

11.364

0.087

0.3902

0.3901

0.4240

0.4589

0.6118

Grade point average

19

1.794

1.814

−0.035

0.4366

0.4336

0.4328

0.5414

0.6563

Total non-juvenile arrests

40

11.718

7.455

−3.895

0.0461

0.0368

0.0668

0.1019

0.1795

Total crime cost

40

775.901

424.679

−313.263

0.1376

0.1361

0.1764

0.2060

0.2329

Total charges

40

13.385

9.000

−4.132

0.0678

0.0579

0.0920

0.1242

0.1789

Non-victimless charges

40

3.077

1.485

−1.444

0.0274

0.0238

0.0372

0.0755

0.1332

Currently employed

19

0.410

0.545

0.147

0.1263

0.1315

0.1292

0.2989

0.3351

Unemployed last year

19

0.128

0.242

0.102

0.1817

0.1827

0.2148

0.3050

0.4203

Jobless months (past 2 yrs)

19

3.816

5.281

1.367

0.2572

0.2501

0.2928

0.3371

0.4217

Currently employed

27

0.564

0.600

0.089

0.2156

0.2259

0.2452

0.3348

0.3703

Unemployed last year

27

0.308

0.242

−0.081

0.2238

0.2190

0.2388

0.3460

0.3776

Jobless months (past 2 yrs)

27

8.795

5.133

−3.868

0.0438

0.0430

0.0588

0.1193

0.2030

Currently employed

40

0.500

0.700

0.266

0.0089

0.0075

0.0204

0.0444

0.0640

Unemployed last year

40

0.462

0.364

−0.143

0.0843

0.0957

0.0912

0.1629

0.1671

Jobless months (past 2 yrs)

40

10.750

7.233

−4.758

0.0154

0.0200

0.0188

0.0632

0.0722

Note: This table reports p-values for single hypothesis tests of treatment effects on various outcomes of male participants at the given ages. The
inferences are based on the studentized AIPW test statistic.

29

Table 3: Reanalysis of Female Outcomes in Heckman et al. (2020) Using Single Tests
Variable

Age

Untreated
mean

Treated
mean

AIPW
estimate

Asymptotic Bootstrap Permutation Worst-case Worst-case
p-value
p-value
p-value
max. p
de Haan p

Stanford–Binet IQ

4

83.692

96.360

13.425

0.0000

0.0000

0.0004

0.0034

0.0040

Stanford–Binet IQ

5

81.650

94.316

14.157

0.0008

0.0006

0.0064

0.0273

0.0382

Stanford–Binet IQ

6

87.160

90.913

5.271

0.0365

0.0281

0.0636

0.0820

0.0959

Stanford–Binet IQ

7

86.000

92.520

7.347

0.0313

0.0154

0.0564

0.0858

0.1232

Stanford–Binet IQ

8

83.600

87.840

4.669

0.1144

0.0896

0.1704

0.2040

0.2141

Stanford–Binet IQ

9

83.043

86.739

4.809

0.0633

0.0679

0.1128

0.1992

0.2628

Stanford–Binet IQ

10

81.789

86.750

6.480

0.0277

0.0323

0.0596

0.1600

0.1976

CAT reading score

14

8.444

16.500

7.345

0.0130

0.0128

0.0268

0.0573

0.0935

CAT arithmetic score

14

6.889

11.818

6.227

0.0102

0.0138

0.0284

0.0544

0.0710

CAT language score

14

7.833

19.455

11.923

0.0009

0.0013

0.0044

0.0178

0.0232

CAT mechanics score

14

8.833

20.636

12.425

0.0014

0.0015

0.0072

0.0208

0.0269

CAT spelling score

14

10.722

29.500

18.270

0.0017

0.0042

0.0064

0.0180

0.0253

High school graduate

19

0.231

0.840

0.570

0.0000

0.0000

0.0004

0.0051

0.0075

Vocational training

40

0.077

0.240

0.183

0.0286

0.0494

0.0420

0.1165

0.2231

Highest grade completed

19

10.750

11.760

1.202

0.0023

0.0106

0.0120

0.0284

0.0500

Grade point average

19

1.527

2.415

0.958

0.0000

0.0155

0.0004

0.0112

0.0381

Total non-juvenile arrests

40

4.423

2.160

−1.938

0.0657

0.0795

0.0880

0.1566

0.1925

Total crime cost

40

293.497

22.165

−246.242

0.1475

0.1227

0.2436

0.2615

0.3036

Total charges

40

4.923

2.240

−2.309

0.0439

0.0528

0.0580

0.1366

0.1407

Non-victimless charges

40

0.308

0.040

−0.249

0.0365

0.0263

0.0612

0.0853

0.1201

Currently employed

19

0.154

0.440

0.297

0.0054

0.0048

0.0152

0.0585

0.0619

Unemployed last year

19

0.577

0.240

−0.354

0.0029

0.0033

0.0104

0.0313

0.0377

Jobless months (past 2 yrs)

19

10.421

5.217

−4.197

0.0723

0.1386

0.1140

0.2020

0.2780

Currently employed

27

0.545

0.800

0.215

0.0471

0.0604

0.0648

0.0960

0.1237

Unemployed last year

27

0.542

0.250

−0.269

0.0523

0.0457

0.0728

0.1663

0.2242

Jobless months (past 2 yrs)

27

10.455

6.240

−1.298

0.3328

0.3449

0.2916

0.4526

0.7373

Currently employed

40

0.818

0.833

−0.016

0.4536

0.4586

0.4912

0.6550

0.6802

Unemployed last year

40

0.409

0.160

−0.194

0.0807

0.1079

0.1324

0.1934

0.2386

Jobless months (past 2 yrs)

40

5.045

4.000

0.057

0.4910

0.4927

0.4700

0.6114

0.6379

Note: This table reports p-values for single hypothesis tests of treatment effects on various outcomes of female participants at the given ages. The
inferences are based on the studentized AIPW test statistic.

30

Table 4: Reanalysis of Male Outcomes in Heckman et al. (2020) Using Stepdown Tests
Age

Untreated
mean

Treated
mean

AIPW
estimate

Stanford–Binet IQ

4

83.077

94.909

8.988

0.0001

0.0002

0.0028

0.0346

0.0394

Stanford–Binet IQ

5

84.793

95.400

9.167

0.0003

0.0012

0.0028

0.0425

0.0425

Stanford–Binet IQ

6

85.821

91.485

3.056

0.1593

0.2058

0.1888

0.3534

0.5112

Stanford–Binet IQ

7

87.711

91.121

1.576

0.2040

0.2143

0.2104

0.3866

0.5112

Stanford–Binet IQ

8

89.054

88.333

−3.829

0.1593

0.2058

0.1888

0.3866

0.5112

Stanford–Binet IQ

9

89.026

88.394

−4.167

0.1593

0.2058

0.1888

0.3866

0.5112

Stanford–Binet IQ

10

86.026

83.697

−4.722

0.1126

0.2058

0.1460

0.3534

0.5112

CAT reading score

14

9.000

13.926

1.815

0.7229

0.7886

0.7800

0.8202

0.9047

CAT arithmetic score

14

8.107

16.000

3.095

0.7229

0.7886

0.7800

0.8202

0.9047

CAT language score

14

6.536

14.333

5.029

0.3260

0.3980

0.4304

0.7084

0.8390

CAT mechanics score

14

6.964

15.556

5.979

0.2690

0.3190

0.3560

0.6667

0.7335

CAT spelling score

14

11.536

18.519

3.171

0.7229

0.7886

0.7800

0.8202

0.9047

High school graduate

19

0.513

0.485

0.015

1.0000

1.0000

1.0000

1.0000

1.0000

Vocational training

40

0.333

0.394

0.071

1.0000

1.0000

1.0000

1.0000

1.0000

Highest grade completed

19

11.282

11.364

0.087

1.0000

1.0000

1.0000

1.0000

1.0000

Grade point average

19

1.794

1.814

−0.035

1.0000

1.0000

1.0000

1.0000

1.0000

Total non-juvenile arrests

40

11.718

7.455

−3.895

0.1384

0.1103

0.2004

0.3057

0.5366

Total crime cost

40

775.901

424.679

−313.263

0.1384

0.1361

0.2004

0.3057

0.5366

Total charges

40

13.385

9.000

−4.132

0.1384

0.1158

0.2004

0.3057

0.5366

Non-victimless charges

40

3.077

1.485

−1.444

0.1096

0.0952

0.1488

0.3020

0.5327

Variable

Asymptotic Bootstrap Permutation Worst-case Worst-case
p-value
p-value
p-value
max. p
de Haan p

Currently employed

19

0.410

0.545

0.147

0.3790

0.3946

0.3876

0.8968

1.0000

Unemployed last year

19

0.128

0.242

0.102

0.3790

0.3946

0.4296

0.8968

1.0000

Jobless months (past 2 yrs)

19

3.816

5.281

1.367

0.3790

0.3946

0.4296

0.8968

1.0000

Currently employed

27

0.564

0.600

0.089

0.4313

0.4380

0.4776

0.6697

0.7405

Unemployed last year

27

0.308

0.242

−0.081

0.4313

0.4380

0.4776

0.6697

0.7405

Jobless months (past 2 yrs)

27

8.795

5.133

−3.868

0.1313

0.1290

0.1764

0.3580

0.6089

Currently employed

40

0.500

0.700

0.266

0.0268

0.0225

0.0564

0.1333

0.1920

Unemployed last year

40

0.462

0.364

−0.143

0.0843

0.0957

0.0912

0.1629

0.1920

Jobless months (past 2 yrs)

40

10.750

7.233

−4.758

0.0309

0.0399

0.0564

0.1333

0.1920

Note: This table reports Holm stepdown p-values for multiple hypothesis tests of treatment effects on various outcomes of male participants at the
given ages. The inferences are based on the studentized AIPW test statistic. The blocks used for multiple testing are indicated above using divider
lines.

31

Table 5: Reanalysis of Female Outcomes in Heckman et al. (2020) Using Stepdown Tests
Age

Untreated
mean

Treated
mean

AIPW
estimate

Stanford–Binet IQ

4

83.692

96.360

13.425

0.0000

0.0000

0.0028

Stanford–Binet IQ

5

81.650

94.316

14.157

0.0046

0.0035

Stanford–Binet IQ

6

87.160

90.913

5.271

0.1387

0.1125

Stanford–Binet IQ

7

86.000

92.520

7.347

0.1387

Variable

Asymptotic Bootstrap Permutation Worst-case Worst-case
p-value
p-value
p-value
max. p
de Haan p
0.0239

0.0278

0.0384

0.1637

0.2290

0.2820

0.4099

0.4796

0.0771

0.2820

0.4099

0.4929

Stanford–Binet IQ

8

83.600

87.840

4.669

0.1387

0.1359

0.2820

0.4801

0.5928

Stanford–Binet IQ

9

83.043

86.739

4.809

0.1387

0.1359

0.2820

0.4801

0.5928

Stanford–Binet IQ

10

81.789

86.750

6.480

0.1387

0.1125

0.2820

0.4801

0.5928

CAT reading score

14

8.444

16.500

7.345

0.0205

0.0255

0.0536

0.1088

0.1421

CAT arithmetic score

14

6.889

11.818

6.227

0.0205

0.0255

0.0536

0.1088

0.1421

CAT language score

14

7.833

19.455

11.923

0.0043

0.0064

0.0220

0.0889

0.1162

CAT mechanics score

14

8.833

20.636

12.425

0.0056

0.0064

0.0256

0.0889

0.1162

CAT spelling score

14

10.722

29.500

18.270

0.0056

0.0127

0.0256

0.0889

0.1162

High school graduate

19

0.231

0.840

0.570

0.0000

0.0000

0.0016

0.0202

0.0299

Vocational training

40

0.077

0.240

0.183

0.0286

0.0494

0.0420

0.1165

0.2231

Highest grade completed

19

10.750

11.760

1.202

0.0046

0.0318

0.0240

0.0567

0.1142

Grade point average

19

1.527

2.415

0.958

0.0000

0.0318

0.0016

0.0336

0.1142

Total non-juvenile arrests

40

4.423

2.160

−1.938

0.1461

0.1589

0.2320

0.4098

0.4803

Total crime cost

40

293.497

22.165

−246.242

0.1475

0.1589

0.2436

0.4098

0.4803

Total charges

40

4.923

2.240

−2.309

0.1461

0.1585

0.2320

0.4098

0.4803

Non-victimless charges

40

0.308

0.040

−0.249

0.1461

0.1051

0.2320

0.3413

0.4803

Currently employed

19

0.154

0.440

0.297

0.0107

0.0099

0.0312

0.1171

0.1237

Unemployed last year

19

0.577

0.240

−0.354

0.0088

0.0099

0.0312

0.0939

0.1132

Jobless months (past 2 yrs)

19

10.421

5.217

−4.197

0.0723

0.1386

0.1140

0.2020

0.2780

Currently employed

27

0.545

0.800

0.215

0.1412

0.1371

0.1944

0.2879

0.3712

Unemployed last year

27

0.542

0.250

−0.269

0.1412

0.1371

0.1944

0.3325

0.4485

Jobless months (past 2 yrs)

27

10.455

6.240

−1.298

0.3328

0.3449

0.2916

0.4526

0.7373

Currently employed

40

0.818

0.833

−0.016

0.9072

0.9173

0.9400

1.0000

1.0000

Unemployed last year

40

0.409

0.160

−0.194

0.2421

0.3237

0.3972

0.5803

0.7157

Jobless months (past 2 yrs)

40

5.045

4.000

0.057

0.9072

0.9173

0.9400

1.0000

1.0000

Note: This table reports Holm stepdown p-values for multiple hypothesis tests of treatment effects on various outcomes of female participants at the
given ages. The inferences are based on the studentized AIPW test statistic. The blocks used for multiple testing are indicated above using divider
lines.

32

Tables 2 through 5 use the studentized AIPW test statistic for inference. Heckman et al. (2020)
use the studentized DIM test statistic instead. Tables 6 and 7 compare their inferences with ours
using the same test statistic. The effects for males on post-program outcomes remain statistically
insignificant at the 10% level using stepdown worst-case de Haan p-values, whereas treatment
effects on CAT scores are statistically significant in Heckman et al.’s (2020) analysis.
Table 6: Comparing Heckman et al.’s (2020) DIM-Based Inference with Ours for Male Sample
Heckman et al.’s (2020) p-values
Variable

Age

U=0
p-value
(unadj.)

U=0
p-value
(adjusted)

Max-U
p-value
(unadj.)

Max-U
p-value
(adjusted)

Worst-case p-values using our method
Worst-case Worst-case Worst-case Worst-case
max. p
max. p
de Haan p de Haan p
(unadjusted) (adjusted) (unadjusted) (adjusted)

Stanford–Binet IQ

4

0.001

0.001

0.008

0.008

0.0048

0.0333

0.0053

0.0368

Stanford–Binet IQ

5

0.022

0.691

0.077

0.800

0.0076

0.0456

0.0095

0.0572

Stanford–Binet IQ

6

0.033

0.034

0.094

0.102

0.0249

0.1247

0.0306

0.1531

Stanford–Binet IQ

7

0.103

0.172

0.247

0.374

0.0782

0.3128

0.1185

0.4741

Stanford–Binet IQ

8

0.599

0.691

0.733

0.800

0.5480

1.0000

0.5743

1.0000

Stanford–Binet IQ

9

0.450

0.548

0.631

0.680

0.5580

1.0000

0.6044

1.0000

Stanford–Binet IQ

10

0.684

0.691

0.790

0.800

0.2568

0.7704

0.3509

1.0000

CAT reading score

14

0.017

0.035

0.036

0.086

0.1711

0.3572

0.1776

0.5135

CAT arithmetic score

14

0.032

0.035

0.086

0.086

0.1191

0.3572

0.1555

0.5135

CAT language score

14

0.001

0.004

0.012

0.027

0.0748

0.2994

0.1284

0.5135

CAT mechanics score

14

0.006

0.007

0.023

0.035

0.0535

0.2673

0.0681

0.3407

CAT spelling score

14

0.003

0.035

0.012

0.086

0.1225

0.3572

0.1503

0.5135

High school graduate

19

0.614

0.674

0.704

0.716

0.6322

1.0000

0.6718

1.0000

Vocational training

40

0.341

0.567

0.547

0.608

0.3674

1.0000

0.4281

1.0000

Highest grade completed

19

0.383

0.622

0.410

0.669

0.3659

1.0000

0.4031

1.0000

Grade point average

19

0.457

0.674

0.567

0.716

0.5133

1.0000

0.5493

1.0000

Total non-juvenile arrests

40

0.036

0.038

0.100

0.115

0.0701

0.2493

0.0896

0.3249

Total crime cost

40

0.037

0.049

0.042

0.143

0.1695

0.2493

0.2133

0.3249

Total charges

40

0.049

0.049

0.143

0.143

0.0946

0.2493

0.1038

0.3249

Non-victimless charges

40

0.025

0.037

0.063

0.091

0.0623

0.2493

0.0812

0.3249

Currently employed

19

0.050

0.164

0.224

0.290

0.2809

0.7459

0.3864

1.0000

Unemployed last year

19

0.901

0.901

0.922

0.922

0.2486

0.7459

0.3673

1.0000

Jobless months (past 2 yrs)

19

0.821

0.849

0.873

0.890

0.3291

0.7459

0.4019

1.0000

Currently employed

27

0.268

0.295

0.485

0.512

0.3629

0.6294

0.4130

0.7278

Unemployed last year

27

0.235

0.295

0.360

0.512

0.3147

0.6294

0.3639

0.7278

Jobless months (past 2 yrs)

27

0.020

0.020

0.036

0.051

0.0965

0.2894

0.1265

0.3795

Currently employed

40

0.103

0.116

0.130

0.146

0.0566

0.1697

0.0853

0.2559

Unemployed last year

40

0.154

0.154

0.216

0.216

0.1684

0.1697

0.2494

0.2559

Jobless months (past 2 yrs)

40

0.064

0.116

0.070

0.146

0.0693

0.1697

0.0957

0.2559

Note: This table compares inferences reported by Heckman et al. (2020) with the inferences obtained using our worst-case tests. The first two
columns list the blocks of outcomes analyzed by Heckman et al. (2020). The next four columns reproduce their zero-U (U = 0) p-values and
max-U p-values before and after adjusting for multiplicity of hypotheses. Since all of their tests are based on studentized DIM estimate, we report
our inferences (using the studentized DIM test statistic) side by side for comparison. The last four columns report our worst-case maximum p-values
and worst-case de Haan p-values before and after adjusting for multiplicity of hypotheses. The unadjusted p-values refer to single p-values that are
unadjusted for multiplicity of hypotheses. The adjusted p-values refer to stepdown p-values after adjusting for multiple testing.

33

The contrast between Tables 5 and 7 reveals the importance of the choice of test statistic. In
Table 7, several effects on female outcomes are statistically significant at the 10% level using
stepdown worst-case de Haan p-values based on the studentized DIM test statistic. However, in
Table 5, using the studentized AIPW test statistic, only one effect (on high school graduation) after
age 5 is statistically significant based on the worst-case de Haan p-values.
Table 7: Comparing Heckman et al.’s (2020) DIM-Based Inference with Ours for Female Sample
Heckman et al.’s (2020) p-values
Variable

Age

U=0
p-value
(unadj.)

U=0
p-value
(adjusted)

Max-U
p-value
(unadj.)

Max-U
p-value
(adjusted)

Worst-case p-values using our method
Worst-case Worst-case Worst-case Worst-case
max. p
max. p
de Haan p de Haan p
(unadjusted) (adjusted) (unadjusted) (adjusted)

Stanford–Binet IQ

4

0.008

0.008

0.020

0.020

0.0052

0.0362

0.0066

0.0460

Stanford–Binet IQ

5

0.012

0.203

0.014

0.354

0.0183

0.1095

0.0500

0.2999

Stanford–Binet IQ

6

0.094

0.164

0.160

0.346

0.1397

0.5589

0.1476

0.5904

Stanford–Binet IQ

7

0.133

0.137

0.191

0.222

0.0734

0.3671

0.0978

0.4890

Stanford–Binet IQ

8

0.152

0.164

0.339

0.346

0.1487

0.5589

0.2143

0.5904

Stanford–Binet IQ

9

0.203

0.203

0.354

0.354

0.2134

0.5589

0.2272

0.5904

Stanford–Binet IQ

10

0.203

0.203

0.267

0.354

0.1398

0.5589

0.1748

0.5904

CAT reading score

14

0.078

0.082

0.136

0.167

0.0413

0.0825

0.0771

0.1542

CAT arithmetic score

14

0.035

0.082

0.074

0.167

0.1000

0.1000

0.1357

0.1542

CAT language score

14

0.008

0.070

0.020

0.144

0.0111

0.0514

0.0235

0.0941

CAT mechanics score

14

0.047

0.082

0.097

0.167

0.0116

0.0514

0.0287

0.0941

CAT spelling score

14

0.043

0.082

0.082

0.167

0.0103

0.0514

0.0115

0.0577

High school graduate

19

0.008

0.008

0.020

0.020

0.0050

0.0200

0.0075

0.0299

Vocational training

40

0.078

0.078

0.144

0.144

0.1233

0.1233

0.1489

0.1489

Highest grade completed

19

0.070

0.070

0.113

0.113

0.0249

0.0497

0.0474

0.0948

Grade point average

19

0.039

0.039

0.082

0.082

0.0079

0.0237

0.0094

0.0299

Total non-juvenile arrests

40

0.020

0.133

0.121

0.158

0.1150

0.2641

0.1337

0.3160

Total crime cost

40

0.024

0.133

0.082

0.158

0.0660

0.2641

0.0790

0.3160

Total charges

40

0.020

0.067

0.043

0.090

0.0979

0.2641

0.1382

0.3160

Non-victimless charges

40

0.125

0.133

0.158

0.158

0.0693

0.2641

0.0803

0.3160

Currently employed

19

0.008

0.031

0.035

0.090

0.0518

0.1200

0.0705

0.1742

Unemployed last year

19

0.024

0.031

0.074

0.090

0.0400

0.1200

0.0581

0.1742

Jobless months (past 2 yrs)

19

0.125

0.125

0.206

0.206

0.0807

0.1200

0.1044

0.1742

Currently employed

27

0.110

0.149

0.175

0.198

0.0712

0.2137

0.0867

0.2600

Unemployed last year

27

0.078

0.149

0.128

0.175

0.1026

0.2137

0.1325

0.2650

Jobless months (past 2 yrs)

27

0.110

0.149

0.166

0.198

0.1881

0.2137

0.2449

0.2650

Currently employed

40

0.442

0.442

0.567

0.567

0.4786

0.8773

0.5336

0.9701

Unemployed last year

40

0.047

0.070

0.113

0.160

0.0825

0.2475

0.1250

0.3750

Jobless months (past 2 yrs)

40

0.352

0.367

0.540

0.540

0.4386

0.8773

0.4850

0.9701

Note: This table compares inferences reported by Heckman et al. (2020) with the inferences obtained using our worst-case tests. The first two
columns list the blocks of outcomes analyzed by Heckman et al. (2020). The next four columns reproduce their zero-U (U = 0) p-values and
max-U p-values before and after adjusting for multiplicity of hypotheses. Since all of their tests are based on studentized DIM estimate, we report
our inferences (using the studentized DIM test statistic) side by side for comparison. The last four columns report our worst-case maximum p-values
and worst-case de Haan p-values before and after adjusting for multiplicity of hypotheses. The unadjusted p-values refer to single p-values that are
unadjusted for multiplicity of hypotheses. The adjusted p-values refer to stepdown p-values after adjusting for multiple testing.

34

Heckman et al. (2020) do not analyze the Perry treatment effects on convictions for violent
crime, which are substantial and play an important role in cost-benefit analyses of early childhood
programs (see Heckman et al., 2010b). Using administrative data on the criminal activity of participants, we illustrate their importance and, at the same time, the importance of long-term follow-up.
Tables 8, 9, and 10 provide estimates and measures of statistical significance of treatments effects
in the pooled sample (of all participants) on cumulative convictions for violent misdemeanors and
felonies at various ages. Appendix D presents expanded versions of these tables reporting inference for various estimators and test statistics for the pooled sample as well as the male and female
subsamples. As shown in Table 9, the AIPW estimates of the treatment effect on cumulative violent misdemeanor convictions are below −0.5 at ages 30 and 40. The treatment effects on violent
misdemeanor convictions are statistically significant at the 2% and 10% levels before and after
multiple hypothesis testing, respectively, regardless of the method used for inference on the pooled
sample (see Appendix D).
Table 8: DIM-Based Single Hypothesis Tests on Cumulative Convictions for Violent Crime
Type

Age

Untreated
mean

Treated
mean

DIM
estimate

Asymptotic Bootstrap Permutation Worst-case Worst-case
p-value
p-value
p-value
max. p
de Haan p

Misdemeanor

30

0.5231

0.0517

−0.4714

0.0109

0.0021

0.0036

0.0122

0.0154

Misdemeanor

40

0.6825

0.0877

−0.5948

0.0033

0.0005

0.0004

0.0053

0.0065

Felony

30

0.2846

0.1897

−0.0950

0.2301

0.2263

0.2624

0.4057

0.5318

Felony

40

0.4762

0.1930

−0.2832

0.0333

0.0332

0.0384

0.0708

0.0900

Note: This table reports p-values for single hypothesis tests of treatment effects on cumulative misdemeanor and felony convictions for violent crime
at ages 30 and 40, using the pooled sample of participants. The inferences are based on the studentized DIM (difference-in-means) test statistic.

Table 9: AIPW-Based Single Hypothesis Tests on Cumulative Convictions for Violent Crime
Type

Age

Untreated
mean

Treated
mean

AIPW
estimate

Asymptotic Bootstrap Permutation Worst-case Worst-case
p-value
p-value
p-value
max. p
de Haan p

Misdemeanor

30

0.5231

0.0517

−0.5300

0.0064

0.0020

0.0024

0.0099

0.0154

Misdemeanor

40

0.6825

0.0877

−0.6491

0.0021

0.0010

0.0008

0.0133

0.0181

Felony

30

0.2846

0.1897

−0.0561

0.3174

0.3217

0.3488

0.4820

0.5078

Felony

40

0.4762

0.1930

−0.2052

0.0664

0.0778

0.0708

0.1543

0.1754

Note: This table reports p-values for single hypothesis tests of treatment effects on cumulative misdemeanor and felony convictions for violent
crime at ages 30 and 40, using the pooled sample of participants. The inferences are based on the studentized AIPW test statistic.

The choice of inferential method becomes more important in analyzing treatment effects on cumulative convictions for felonies. At age 30, there are no statistically significant treatment effects.
At age 40, as shown in Table 9, the magnitude of the treatment effect is higher at about −0.21,
which represents more than a four-tenths reduction in the control mean. However, using simple
difference-in-means estimates and conventional p-values can be misleading. Using conventional
p-values, the effect at age 40 appears to be statistically significant at the 10% level, as shown in Ta35

Table 10: AIPW-Based Multiple Hypothesis Tests on Cumulative Convictions for Violent Crime
Type

Age

Untreated
mean

Treated
mean

AIPW
estimate

Asymptotic Bootstrap Permutation Worst-case Worst-case
p-value
p-value
p-value
max. p
de Haan p

Misdemeanor

30

0.5231

0.0517

−0.5300

0.0192

0.0059

0.0072

0.0394

0.0617

Misdemeanor

40

0.6825

0.0877

−0.6491

0.0085

0.0039

0.0032

0.0398

0.0617

Felony

30

0.2846

0.1897

−0.0561

0.3174

0.3217

0.3488

0.4820

0.5078

Felony

40

0.4762

0.1930

−0.2052

0.1327

0.1556

0.1416

0.3086

0.3507

Note: This table reports Holm stepdown p-values for multiple hypothesis tests of treatment effects on cumulative misdemeanor and felony convictions for violent crime at ages 30 and 40, using the pooled sample of participants. The inferences are based on the studentized AIPW test statistic.
All the above four variables, which represent cumulative crime outcomes at different ages, are treated as a block for multiple testing.

ble 8. However, the design-based worst-case p-values, especially those associated with the AIPW
estimate, are much higher. The worst-case de Haan p-values for the studentized DIM and AIPW
estimates are about 0.090 and 0.175, respectively.
The four variables at ages 30 and 40 considered in Tables 8 and 9 are conceptually related, since
they are cumulative crime outcomes measured at different ages. To account for this, we treat these
outcomes as a single block of variables and conduct multiple hypothesis testing using the more
conservative Holm stepdown procedure, producing results in Table 10. After multiple testing, the
effects on cumulative convictions for violent misdemeanors remain statistically significant at the
10% level at both ages 30 and 40, whereas the the effects on violent felonies are insignificant at
both ages.
These analyses, and those in the appendices, show that use of small-sample inference and the
method used to account for compromised randomization matter in analyzing the data. Failure to
account for either can give a very positive spin to the Perry program. Accounting for it qualifies
such conclusions. We have not, however, established the superiority of our approach. We have
established that a very cautious design-based approach produces conservative inference, which by
itself is not surprising. Our reanalysis of Heckman et al. (2020) is very conservative. Nonetheless,
a few conclusions survive. We test Fisher’s sharp null hypothesis HF of no treatment effect for
each participant. It may in fact be the case that there are treatment effects for many participants
and yet we do not reject the sharp null hypothesis because of our worst-case approach.

6

Conclusion

In this paper, we develop and apply a design-based finite-sample inferential method for analyzing
social experiments with compromised randomization. Compromises come in many forms. They
include incompletely documented re-randomization procedures used to improve baseline covariate
balance between treatment and control groups. They also include reassignment of treatment status
due to administrative constraints.
36

We build a behavioral model of satisficing experimenters who seek balance in baseline covariates across treatments and controls and who provide readers of their reports qualitative, and sometimes conflicting, summaries of the actual experimental protocols used. We model the randomization protocol as only partially known to the user of experimental data. The empirical researcher
recognizes and tries to account for the guiding principles experimenters used in the reassignment of
treatment status for balancing baseline covariates while operating under administrative constraints.
We show how to partially identify model parameters and construct worst-case (least favorable)
randomization tests over a set of possibilities for the actual treatment assignment mechanism.
Our analysis of the Perry program serves as a proof-of-concept of the usefulness of our worstcase finite-sample testing approaches, which are applicable to other compromised experiments,
such as those discussed by Bruhn and McKenzie (2009). Our approach is more portable than that
of Heckman et al. (2020), which utilizes very specific features of the Perry randomization protocol. Application of our procedures result in conservative finite-sample inferences. It is remarkable
that when we apply our worst-case methods to the latest wave of Perry data on the participants
at late midlife and their adult children (Heckman and Karapakula, 2020), we find many statistically significant policy-relevant beneficial treatment effects that survive application of worst-case
inferential procedures.

37

References
Abadie, A., S. Athey, G. W. Imbens, and J. M. Wooldridge (2020). Sampling-based versus designbased uncertainty in regression analysis. Econometrica 88(1), 265–296.
Athey, S. and G. W. Imbens (2017). The econometrics of randomized experiments. In Handbook
of Economic Field Experiments, Volume 1, pp. 73–140. Amsterdam: Elsevier.
Bruhn, M. and D. McKenzie (2009). In pursuit of balance: Randomization in practice in development field experiments. American Economic Journal: Applied Economics 1(4), 200–232.
Cattaneo, M. D. (2010). Efficient semiparametric estimation of multi-valued treatment effects
under ignorability. Journal of Econometrics 155(2), 138–154.
Chung, E. and J. P. Romano (2013). Exact and asymptotically robust permutation tests. The Annals
of Statistics 41(2), 484–507.
Chung, E. and J. P. Romano (2016). Multivariate and multiple permutation tests. Journal of
Econometrics 193(1), 76–91.
de Haan, L. (1981). Estimation of the minimum of a function using order statistics. Journal of the
American Statistical Association 76(374), 467–469.
Fisher, R. A. (1925). Statistical methods for research workers. Edinburgh: Oliver and Boyd.
Fisher, R. A. (1935). The design of experiments. Edinburgh: Oliver and Boyd.
Heckman, J. J. and G. Karapakula (2020). Intergenerational impacts of a program designed to
promote the social mobility of disadvantaged African Americans. Unpublished manuscript, The
University of Chicago.
Heckman, J. J., S. H. Moon, R. Pinto, P. A. Savelyev, and A. Yavitz (2010a). Analyzing social experiments as implemented: A reexamination of the evidence from the HighScope Perry
Preschool Program. Quantitative Economics 1(1), 1–46.
Heckman, J. J., S. H. Moon, R. Pinto, P. A. Savelyev, and A. Yavitz (2010b). The rate of return to
the Highscope Perry Preschool Program. Journal of Public Economics 94(1-2), 114–128.
Heckman, J. J., R. Pinto, and A. M. Shaikh (2020). Inference with imperfect randomization: The
case of the Perry Preschool Program. Unpublished manuscript, The University of Chicago.
Holm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of
Statistics, 65–70.
Kang, J. D. Y. and J. L. Schafer (2007). Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data. Statistical Science 22(4),
523–539.
Lehmann, E. L. (1993). The Fisher, Neyman-Pearson theories of testing hypotheses: One theory
or two? Journal of the American Statistical Association 88(424), 1242–1249.
38

Lehmann, E. L. and J. P. Romano (2005). Testing statistical hypotheses. New York: Springer.
Li, X. and P. Ding (2016). Exact confidence intervals for the average causal effect on a binary
outcome. Statistics in Medicine 35(6), 957–960.
Li, X., P. Ding, and D. B. Rubin (2018). Asymptotic theory of rerandomization in treatment–
control experiments. Proceedings of the National Academy of Sciences 115(37), 9157–9162.
Lunceford, J. K. and M. Davidian (2004). Stratification and weighting via the propensity score
in estimation of causal treatment effects: A comparative study. Statistics in Medicine 23(19),
2937–2960.
Morgan, K. L. and D. B. Rubin (2012). Rerandomization to improve covariate balance in experiments. The Annals of Statistics 40(2), 1263–1282.
Morgan, K. L. and D. B. Rubin (2015). Rerandomization to balance tiers of covariates. Journal of
the American Statistical Association 110(512), 1412–1421.
Neyman, J. S. (1923). Próba uzasadnienia zastosowań rachunku prawdopodobieństwa do doswiadczeń polowych (On the application of probability theory to agricultural experiments: Essay on
principles). Roczniki Nauk Rolniczych (Annals of Agricultural Sciences) 10, 1–51. Reprinted in
Statistical Science 5(4), 465–472, as a translation by D. M. Dabrowska and T. P. Speed (1990)
from section 9 (29–42) of the original Polish article.
Obama, B. (2013). The 2013 State of the Union Address. Washington, DC: The White House
Office of the Press Secretary.
Quandt, R. E. (1958). The estimation of the parameters of a linear regression system obeying two
separate regimes. Journal of the American Statistical Association 53(284), 873–880.
Quandt, R. E. (1972). A new approach to estimating switching regressions. Journal of the American Statistical Association 67(338), 306–310.
Rigdon, J. and M. G. Hudgens (2015). Randomization inference for treatment effects on a binary
outcome. Statistics in Medicine 34(6), 924–935.
Robins, J. M., A. Rotnitzky, and L. P. Zhao (1994). Estimation of regression coefficients
when some regressors are not always observed. Journal of the American Statistical Association 89(427), 846–866.
Romano, J. P. (1989). Bootstrap and randomization tests of some nonparametric hypotheses. The
Annals of Statistics 17(1), 141–159.
Romano, J. P. and A. M. Shaikh (2010). Inference for the identified set in partially identified
econometric models. Econometrica 78(1), 169–211.
Romano, J. P., A. M. Shaikh, and M. Wolf (2010). Hypothesis testing in econometrics. Annual
Review of Economics 2(1), 75–104.

39

Romano, J. P. and M. Wolf (2005). Exact and approximate stepdown methods for multiple hypothesis testing. Journal of the American Statistical Association 100(469), 94–108.
Schweinhart, L. J. (2013). Long-term follow-up of a preschool experiment. Journal of Experimental Criminology 9(4), 389–409.
Schweinhart, L. J., H. V. Barnes, D. P. Weikart, W. Barnett, and A. Epstein (1993). Significant benefits: The High/Scope Perry Preschool Study through age 27 (Monographs of the High/Scope
Educational Research Foundation, 10). Ypsilanti, MI: HighScope Educational Research Foundation.
Schweinhart, L. J., J. R. Berrueta-Clement, W. S. Barnett, A. S. Epstein, and D. P. Weikart (1985).
The promise of early childhood education. The Phi Delta Kappan 66(8), 548–553.
Schweinhart, L. J., J. Montie, Z. Xiang, W. S. Barnett, C. R. Belfield, and M. Nores (2005).
Lifetime effects: The High/Scope Perry Preschool Study through age 40 (Monographs of the
High/Scope Educational Research Foundation, 14). Ypsilanti, MI: HighScope Educational Research Foundation.
Schweinhart, L. J. and D. P. Weikart (1980). Young Children Grow Up: The Effects of the Perry
Preschool Program on Youths Through Age 15. Ypsilanti, MI: HighScope Educational Research
Foundation.
Simon, H. A. (1955). A behavioral model of rational choice. The Quarterly Journal of Economics 69(1), 99–118.
Weikart, D. P., J. T. Bond, and J. T. McNeil (1978). The Ypsilanti Perry Preschool Project:
Preschool years and longitudinal results through fourth grade. Number 3. Ypsilanti, MI: HighScope Educational Research Foundation.
Weikart, D. P., C. K. Kamii, and N. L. Radin (1964). Perry Preschool Project progress report.
Technical report, Ypsilanti Public Schools.
Wu, J. and P. Ding (2020). Randomization tests for weak null hypotheses in randomized experiments. Journal of the American Statistical Association, in press.
Young, A. (2019). Channeling Fisher: Randomization tests and the statistical insignificance of
seemingly significant experimental results. The Quarterly Journal of Economics 134(2), 557–
598.
Zigler, E. and D. P. Weikart (1993). Reply to Spitz’s comments. American Psychologist 48(8),
915–916.

40

