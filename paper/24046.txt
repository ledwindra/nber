NBER WORKING PAPER SERIES

FAST AND SLOW LEARNING FROM REVIEWS
Daron Acemoglu
Ali Makhdoumi
Azarakhsh Malekian
Asuman Ozdaglar
Working Paper 24046
http://www.nber.org/papers/w24046

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
November 2017

We thank participants at several seminars and conferences for useful suggestions and comments.
We gratefully acknowledge financial support from the Toulouse Network with Information
Technology and Army Research Office, ARO MURI W911NF-12-1-0509. The views expressed
herein are those of the authors and do not necessarily reflect the views of the National Bureau of
Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2017 by Daron Acemoglu, Ali Makhdoumi, Azarakhsh Malekian, and Asuman Ozdaglar. All
rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without
explicit permission provided that full credit, including © notice, is given to the source.

Fast and Slow Learning From Reviews
Daron Acemoglu, Ali Makhdoumi, Azarakhsh Malekian, and Asuman Ozdaglar
NBER Working Paper No. 24046
November 2017
JEL No. C72,D83,L15
ABSTRACT
This paper develops a model of Bayesian learning from online reviews, and investigates the
conditions for asymptotic learning of the quality of a product and the speed of learning under
different rating systems. A rating system provides information about reviews left by previous
customers. A sequence of potential customers decide whether to join the platform. After joining
and observing the ratings of the product, and conditional on her ex ante valuation, a customer
decides whether to purchase or not. If she purchases, the true quality of the product, her ex ante
valuation, an ex post idiosyncratic preference term and the price of the product determine her
overall satisfaction. Given the rating system of the platform, she decides to leave a review as a
function of her overall satisfaction. We study learning dynamics under two classes of rating
systems: full history, where customers see the full history of reviews, and summary statistics,
where the platform reports some summary statistics of past reviews. In both cases, learning
dynamics are complicated by a selection effect — the types of users who purchase the good and
thus their overall satisfaction and reviews depend on the information that they have available at
the time of their purchase. We provide conditions for asymptotic learning under both full history
and summary statistics, and show how the selection effect becomes more difficult to correct for
with summary statistics. Conditional on asymptotic learning, the speed (rate) of learning is
always exponential and is governed by similar forces under both types of rating systems, though
the exact rates differ. Using this characterization, we provide the rate of learning under several
different types of rating systems. We show that providing more information does not always lead
to faster learning, but strictly finer rating systems always do. We also illustrate how different
rating systems, with the same distribution of preferences, can lead to very fast or very slow
speeds of learning.
Daron Acemoglu
Department of Economics, E52-446
MIT
77 Massachusetts Avenue
Cambridge, MA 02139
and CIFAR
and also NBER
daron@mit.edu
Ali Makhdoumi
Electrical Engineering and Computer Science
Massachusetts Institute of Technology
77 Massachusetts Ave, 32-D640
Cambridge, MA 02139
makhdoum@mit.edu

Azarakhsh Malekian
Rotman School of Management
University of Toronto
and LIDS, MIT
azarakhsh.malekian@rotman.utoronto.ca
Asuman Ozdaglar
Dept of Electrical Engineering
and Computer Science
Massachusetts Institute of Technology
77 Massachusetts Ave, E40-130
Cambridge, MA 02139
asuman@mit.edu

1

Introduction

The fraction of consumer purchases performed online has reached 8.9%, and now accounts for
$100 billion. Amazon, alone, now has over 70 million prime customers.1 As important as the
lower costs of online transactions are the potential benefits of online information sharing. For
instance, many platforms such as Amazon, eBay or Airbnb encourage users to provide reviews
and present to potential new customers/users summaries of reviews by past users. It would not
be an exaggeration to say that the success of many of those platforms depends in large part on the
informativeness of their reviews, since otherwise users would not know which sellers to trust and
may often be unable to gather enough information about some of the product choices available
to them. Despite the centrality of such online reviews, there is relatively little work investigating
the specific challenges such systems face and their efficacy in aggregating dispersed information
of diverse users.
We construct a simple benchmark model of Bayesian learning from past reviews. For simplicity, we consider an online platform selling a single product of unknown quality, which is either
high or low, and assume that the platform also provides a rating system, consisting of options for
reviews for users and a rule for aggregating these reviews. Examples of rating systems include:
full history systems (where the reviews of all past users, and their exact sequence, are presented to
current potential users), and more realistically, summary statistics systems (where some summary
statistics of the reviews of past users are shown to potential new users). The options for reviews
include different scores (“like” vs. “dislike” or number of stars) from which users can choose.
Potential users know the rating system of the online platform and decide whether to enter
the platform. Upon entry, they observe their taste parameter, which determines how likely they
are to like the product in question, obtain information about the product according to the rating
system, and decide whether to purchase it or not. Following purchase, they experience their utility
from the product, which also depends on the realization of an additional ex post idiosyncratic
preference parameter, and may decide to leave a review. Whether they leave a review and which
score they choose depends on their ex post utility relative to some pre-specified thresholds.2 Using
this model, we investigate how well (and how rapidly) the information of users is aggregated by
various rating systems.
Our analysis has three main contributions. First, we identify a new challenge to learning,
which we call the selection effect, likely to be particularly relevant in the context of online reviews.
Because users know part of their preferences before purchase, those making a purchase are “selected” according to their taste parameters. Moreover, this selection becomes more pronounced
when the information about the product is not very favorable — e.g., only those very biased towards a particular type of book or service would consider buying it if past purchasers have very
negative assessments of its quality. The selection effect is different from the difficulty faced by
1

US Census Bureau News, U.S. Department of Commerce.
In much of our analysis, we take these thresholds as given, and return to a discussion of how they are determined
at the end.
2

1

models of observational learning, such as Bikhchandani et al. [1992], Welch [1992] and Banerjee
[1992], which we discuss in greater detail below. In observational learning models, agents may not
be able to learn the underlying state because of “herding” — the possibility that the informative
signals of past users cease to affect their behavior because they are themselves following the information of others. In contrast, here, herding issues do not arise because users base their reviews
on their own experience, and the information they receive while making their purchase does not
directly affect this experience. Instead, the challenge for users is to disentangle past users’ preferences concerning the product from their information relayed through reviews. We clarify the
conditions under which asymptotic learning — which enables users to obtain very accurate estimates of the underlying quality from sufficiently many reviews — occurs, and highlight the role
of Bayesian updating in undoing the implications of the selection effect. In particular, with naive
agents who do not recognize the presence of the selection effect, we show that asymptotic learning
typically fails.
Second, in addition to conditions for asymptotic learning, we investigate the speed of learning.
In particular, both in the full history and summary statistics cases, we show that learning is exponentially fast and provide a tight characterization of the speed (or the rate) of learning as a
function of the Kullback-Leibler (KL) divergence between the probability distribution of reviews
conditional on high vs. low quality.3 In addition, because of the aforementioned selection effect,
these probability distributions depend on the “public belief”, which summarizes the assessment
of past users concerning the quality of the product. The exact form of this dependence varies between the full history and the summary statistics cases, highlighting how the difficulty of dealing
with the selection problem depends on the exact information structures.
Our results on the speed of learning are not just of methodological interest. A rating system
that aggregates the dispersed information of past users accurately but extremely slowly would
not be very useful to an online platform that rely on potential users having this information in
real time. In fact, we show that the revenues of the platform are higher under a rating system
with higher learning speed, which confirms that the platform’s incentives are in fact aligned with
accelerating learning.
Third, we also provide a number of results on what types of rating systems lead to faster aggregation of information. We show that, in general, providing more information to users does
not guarantee faster learning. In fact, some summary statistics may lead to more rapid learning
than allowing users to see the full history of past reviews. Nevertheless, we also show that “strict
refinements” of the review system (which increase the granularity of the information provided by
the rating system) lead to faster learning. Examples of such strict refinements include a change in
the rating system so that different types of reviews that were aggregated previously are now disaggregated, and replacing averages of past reviews with more detailed information about fractions
3

The role of KL divergence in this context is intuitive. Asymptotically, the problem of each individual is similar to
a binary hypothesis testing problem (Cover and Thomas [2012, Chapter 11]), though with one important complication:
the observations are not conditionally independent, which we overcome by using a different line of proof and show
that the speed of learning is still related to KL divergence.

2

of past customers leaving different reviews. We also illustrate that, for a given set of underlying
preference and information parameters, different rating systems can lead to very slow or very fast
learning.
Throughout we carry out the analysis separately for the full history case, which enables us to
use the martingale convergence theorem and related tools, and for rating systems with summary
statistics, which necessitate a very different and novel approach.4 Asymptotic learning takes place
under very mild assumptions with full history, and requires somewhat more restrictive, but still
reasonable, assumptions when the rating system provides summary statistics. This difference is
because having access to the full history of reviews enables users to undo the implications of the
selection effect mentioned above, while the selection effect becomes more challenging when users
have access only to summary statistics. Perhaps surprisingly, conditional on asymptotic learning,
the speed of learning is given by a similar logic and similar expressions in the two cases, even
though the exact speed of learning could differ quite significantly in these scenarios.
Our work is related to several literatures. First, as already mentioned, we build on, but are
very distinct from, the Bayesian observational learning literature pioneered by Bikhchandani et al.
[1992], Welch [1992] and Banerjee [1992], where each agent sees the actions of some or all past
agents. Smith and Sørensen [2000] provide a comprehensive analysis of such models when individuals observe all past actions (see also Banerjee and Fudenberg [2004]), while Acemoglu et al.
[2011], Lobel and Sadler [2015a], Lobel and Sadler [2015b], Mossel et al. [2014] and Mossel et al.
[2015], among others, study Bayesian observational learning when agents observe a subset of past
actions determined according to a stochastic network.5 Our main innovation relative to this literature is the selection effect (which emerges as the main barrier to learning in our model), and our
characterization of the speed of learning.
We are not aware of any comprehensive study of the speed of learning in Bayesian models of
observational learning. The partial exceptions are Acemoglu et al. [2009], which studies the speed
of learning in the baseline observational learning model when each agent observes the previous
action and when each agent observes one randomly drawn action from the past; Hann-Caruthers
et al. [2017], which compares the speed of learning in the baseline observational learning model
when each agent observes the previous action versus the case in which they also observe past
signals; Harel et al. [2014], which studies the speed of learning in a setting where finitely many
agents repeatedly observe each other’s actions; and Vives [1993], Vives [1995], and Amador and
Weill [2012], which focus on the speed of learning in rational expectations equilibria, where agents
learn from prices.
Our paper is also related to a few works on review and rating systems. Most relevant is Ifrach
et al. [2014], which studies a setting similar to the full history version of our model with pricing,
4

Namely, we construct two distributions, one majorized by the distribution of our summary statistics conditional on
high quality, and one majorizing the distribution of our summary statistics conditional on low quality, and show that
these two distributions are asymptotically separated.
5
See also Tay et al. [2008] and Drakopoulos et al. [2013] for distributed detection models, where some of the same
issues as in the baseline observational learning models may prevent learning.

3

and investigates the implications of the pricing strategy of the seller on learning.6 Their model provides guidelines for pricing in this setting, but it neither features the selection effect nor considers
summary statistics nor characterizes the speed of learning. Less directly related but still relevant
are: Che and Horner [2015], which studies the optimal review system to encourage experimentation by early users; Horner and Lambert [2016], which investigates the trade-off between the informational role of reviews and their impact on the seller’s effort on quality; and Garg and Johari
[2017], which studies the implications of pairwise comparisons on online reputation building.7
The rest of the paper is organized as follows. In Section 2, we introduce our model. In Section
3 we formulate both customers’ and platform’s problems and define an equilibrium. Sections 4
and 5 provide conditions for asymptotic learning and characterize the speed of learning under
full history and summary statistics, respectively. Section 6 presents several results on the effects
of the design of rating systems on the speed of learning. Section 7 concludes, while we present the
proofs of all of our results in the Appendix.

2

Environment

We consider a platform that is selling a product to a set of customers/users. The true quality
of the product is unknown to both the customers and the platform. The platform has a rating
system, which collects reviews from previous customers and provides a rating of the product
(which could be a summary statistic of these reviews or their entire history) for future potential
customers.8 New customers decide whether to join the platform. Upon joining, they observe
their ex ante valuation and the information from the rating system of the platform, and decide
whether to purchase the product. After the purchase decision, their utility from this product,
which depends on true quality and an additional ex post idiosyncratic preference term, is realized,
and they decide whether and what review to leave.

2.1

Customer Utility and Decisions

We assume that the true quality of the product is binary, corresponding to low or high quality, and
denote it by Q∗ ∈ {0, 1}. A sequence of potential customers decides whether to join the platform
to collect additional information about the product and potentially to purchase it. In particular,
one customer arrives at each time and is denoted by the time of her arrival, t ∈ N. Customer t
faces an entry cost drawn independently from a continuous distribution Fc . After observing this
6
Crapis et al. [2016], Vaccari et al. [2016], and Besbes and Scarsini [2015] study various non-Bayesian models of
learning from reviews.
7
There is also a small empirical literature on reviews, including Blake et al. [2016], Talwar et al. [2007], Li and
Hitt [2008], Lafky [2014], Mayzlin et al. [2014], and Chua and Banerjee [2016]. Though the motivations and the exact
strategies of users when they leave reviews are complex, the evidence is broadly consistent with the notion that reviews
are informative about the underlying quality of the product or provider and also reflect various idiosyncratic factors.
8
Throughout, we refer to the scores or other information left by customers as “review,” and to the aggregate of these
reviews provided by the platform as “rating”.

4

cost (and with the full knowledge of the rating system being used by the platform and the price of
the product), the customer decides jt ∈ {0, 1}, designating whether she has joined the platform.
The utility of customer t from purchasing the product is
θt + ζt + Q − p,
where p is the price of the product; θt is an ex ante idiosyncratic preference (valuation) term,
drawn independently for each customer from a continuous distribution Fθ ; and ζt is an ex post
idiosyncratic preference term, drawn also independently from a different continuous distribution
Fζ . Note that this is the “gross” utility of the customer, since we have not subtracted the cost of
joining the platform ct , which is already sunk at this point (the “net” utility can be obtained by
subtracting ct from this expression). The “gross” utility from not purchasing the good is normalized to zero.9
After entry into the platform (jt = 1), the customer observes her ex ante valuation θt and
the rating of the product provided by the platform based on past reviews. After receiving this
information, she decides whether to purchase the product, which is denoted by bt ∈ {0, 1}.
If she decides to purchase (i.e., bt = 1), she experiences her full utility (or equivalently, she
observes the true quality of the product and her ex post preference term ζt ). At this point, the
consumer decides whether to leave a review of the product and what review to leave. We assume
that the rating system of the platform allows one of −K1 , . . . , 0, . . . , K2 reviews, with 0 interpreted
as leaving no review. For most of the paper, we assume that all customers have thresholds denoted
by λ−K1 ≤ · · · ≤ λ−1 ≤ λ1 ≤ · · · ≤ λK2 , and their reviews will be determined by the location of
their utility relative to these thresholds. In particular, customer t chooses review rt such that



−K1 ,


rt = i,



K ,
2

θt + ζt + Q − p ≤ λ−K1
λi−1 ≤ θt + ζt + Q − p ≤ λi , − K1 < i < K2 ,

(1)

θt + ζt + Q − p ≥ λK2 .

In what follows, we will often refer to the most favorable review K2 , as “like,” and sometimes
refer to the least favorable review −K1 , as “dislike”.
We also note that the idiosyncratic preference terms, θt and ζt , are customer t’s private information.
Throughout our analysis, we assume that the thresholds for reviews, λ−K1 , . . . , λK2 are fixed,
same across customers, and publicly known. But this assumption is only for notational convet
nience. We could alternatively assume that these thresholds are given by λ−K1 + ν−K
, . . . , λ1 +
1
t , where the ν terms are iid draws across individuals and thresholds, and are not
ν1t , . . . , λK2 + νK
2

observed by other agents. It is straightforward to see that in this case, the analysis applies without
9

The important restriction implied by this form of customer utility is that there is a common “quality” that all
customers care about. The additive structure also implies that they all care about it with similar intensity; this feature
can be relaxed without changing our main results.

5

any modifications with the ex post idiosyncratic preference term ζt , being replaced by the sum of
ζt and −ν terms. We return to a discussion of endogenous thresholds at the end of the paper.
We adopt the following “richness” assumption which guarantees that all reviews have a nonzero probability.
Assumption 1 (Richness). The random variables, c, θ and ζ, have continuous cumulative density functions and the support of ζ is sufficiently wide to ensure that all possible reviews have positive probability;
i.e.,
ζ̄ + θ − p > λK2 and ζ + θ̄ + 1 − p < λ−K1 ,
where the support of θ is denoted by [θ, θ̄] and the support of ζ is denoted by [ζ, ζ̄].

2.2

The Rating System

The platform observes purchase decision bt as well as review decision rt . We let ht ∈ {N } ∪
{−K1 , . . . , K2 }, where ht = N designates “no purchase” by customer t, i.e., bt = 0. We call ht
the action at time t. The history available to platform at time t is Ht = {h1 , . . . , ht−1 } and by
convention H1 = ∅. The platform has a rating system denoted by Ω, which at time t maps the
history of reviews by customers into a rating—which could be the same as the history itself or as
summary statistic of the reviews. We denote the rating available for customer t by Ωt . Examples
of Ωt include: (i) full history Ωt = Ht , and (ii) fraction of “likes”, i.e., fraction of K2 reviews among
all customers, and (iii) fraction of “likes” among “likes” and “dislikes”, i.e., fraction of K2 reviews
among −K1 and K2 reviews.

3

Customers’ and Platform’s Problems

We next introduce the optimization problems of customers and the platform. Our key assumptions are that both sets of agents maximize their utility and are Bayesian. This essentially amounts
to looking for a perfect Bayesian equilibrium of the dynamic incomplete information game between
the platform and the customers: all agents maximize their expected utility, and their expectations
are given by Bayes’ rule whenever possible (i.e., whenever the event in question has nonzero
probability). Though the extent of strategic interactions in our setup is limited, customers still
need to reason through the review decisions of previous users in order to form their beliefs about
the underlying quality of the product.

3.1

Customers’ Problem

We break the customers’problem into an entry, a purchase, and a review decision as illustrated in
Figure 1.
Entry Decision: before joining the platform, a customer only knows the price p, the cost ct , and the
rating system Ω (not the realization of Ωt ). In particular, she does not know the number of people

6

jt = 0

bt = 0

p
jt = 1
Rating system
ct

p
Ωt

bt = 1

θt

p
Q
θt
ζt

rt

Figure 1: Entry, purchase and review decisions: the entry decision, jt , is based on price, the rating
system, and the cost of entry according to Eq. (2). The purchase decision, bt , is based on price,
the realization of the rating of the product, and the private ex ante valuation according to Eq. (3).
The review decision, rt , is based on price, the true quality of the product, the ex ante and ex post
valuations according to Eq. (1).
who have joined the platform before her and has an improper uniform prior on this number.10
Therefore, in a perfect Bayesian equilibrium customer t’s entry decision is given by
jt = arg max 1{j = 1} (Et,Ωt ,Q,θt ,ζt [1{bt = 1} (θt + ζt + Q − p)] − ct ) ,
j∈{0,1}

(2)

where Et,Ωt ,Q,θt ,ζt is the Bayesian expectation over all random variables.
Purchase Decision: after joining the platform, customer t observes the realized rating Ωt . Based on
Ωt , she forms a belief regarding the quality of the product as
qt , P [Q = 1|Ωt ] .
When customer t joins the platform, she also observes her ex ante valuation θt . Therefore, in a
perfect Bayesian equilibrium, customer t’s purchase decision is given by
bt = arg max 1{b = 1} (EQ,ζt [θt + ζt + Q − p | Ωt ]) = arg max 1{b = 1} (θt + E [ζ] + qt − p) .
b∈{0,1}

b∈{0,1}

(3)
This maximization problem makes the selection effect, described in the Introduction, apparent. If qt
is very low, only agents with high θt will choose bt = 1.
Review Decision: after a customer purchases the product, she observes the true quality Q∗ ∈ {0, 1}
as well as her ex post valuation from the product ζt based on which she leaves a review as given
in Eq. (1).

3.2

The Platform’s Problem

The platform does not know the true quality of the product and we also assume that it does not
control the price (which is set by a seller). Its only decision is the rating system, Ω. Its revenues
10
We can relax this assumption, which plays no role in our analysis of learning and the speed of learning (at that
point the customer does know her place in the sequence). This assumption is only used in our analysis of the platform’s
maximization problem in subsection 6.6.

7

come from advertisements or other benefits that it derives from potential customers joining the
platform. We assume, in particular, that it receives a fixed revenue for each customer that decides
to join. Therefore, the platform’s problem is to maximize the expected number of customers who
join the platform.
In Section 6, we show that the platform’s revenue is maximized by choosing a rating system
with the highest speed of learning. Intuitively, this is because a rating system that ensures faster
learning provides greater expected utility to potential customers, encouraging them to join the
platform. Before establishing this result, we show that under mild assumptions, the relevant set
of rating systems guarantee asymptotic learning and we characterize their speed of learning. We
separately study learning dynamics for a rating system that provides the full history of reviews
and rating systems that provide summary statistics of past reviews.

4

Full History

With full history, the rating system provides the entire history of previous actions, i.e, Ωt = Ht .11
We show that under Assumption 1, as long as potential customers do not stop purchasing the
product, this type of rating system guarantees asymptotic learning (almost sure convergence of
beliefs to the true quality of the product). We then characterize the speed (rate) of learning.
We refer to the probability that the underlying quality is high given the history of reviews up
to time t as public belief at time t, and denote it by qt .12 Clearly,
qt = P [Q = 1 | Ht ] .
Customer t, who receives this information from the rating system, can compute this public belief
by Bayes’ rule. In fact, given full history, each customer can compute (and thus knows) the public
belief at any time s ≤ t.
We denote by lt the likelihood ratio of this belief, i.e.,
lt ,

qt
P [Q = 1 | Ht ]
=
.
1 − qt
P [Q = 0 | Ht ]

Therefore, the likelihood ratio at time t is
lt =

t−1
qt
P [Q = 1|Ht ]
P [Ht | Q = 1] Y P [hs | Q = 1, Hs ]
=
=
=
.
1 − qt
P [Q = 0|Ht ]
P [Ht | Q = 0]
P [hs | Q = 0, Hs ]
s=1

Note that lt is a sufficient statistic of Ht for (estimating) Q.
11
In particular, we are assuming that the history also includes the “no purchases” and “no reviews”. This assumption
is adopted to simplify the notation in our baseline analysis. We show at the end of the section that all of our results
translate to the case in which the history only includes information about those who have left reviews.
12
We follow the observational learning literature in referring to qt as the “public belief”. This emphasizes that under
full history, all customers s ≥ t can accurately infer this belief.

8

4.1

Learning Dynamics

We next show how the public belief evolves for customer t + 1. Customer t + 1 observes ht (and
not θt and ζt ) based on which she updates her belief as
lt+1 = lt ×

P [ht |Q = 1, lt ]
,
P [ht |Q = 0, lt ]

where using Eq. (1) and Eq. (3) we have



P [qt + θt + E[ζ] − p < 0] ,




P [q + θ + E[ζ] − p ≥ 0, θ + ζ + Q − p ≤ λ
t
t
t
t
−K1 ] ,
P [ht |Q, lt ] =


P [qt + θt + E[ζ] − p ≥ 0, λi−1 ≤ θt + ζt + Q − p ≤ λi ] ,




P [q + θ + E[ζ] − p ≥ 0, θ + ζ + Q − p ≥ λ ] ,
t
t
t
t
K2

ht = N
ht = −K1
−K1 < i < K2 , ht = i,
ht = K2 .
(4)

Therefore, the dynamics of public belief is captured by the following random process
lt+1 = lt ×

P [h|Q = 1, lt ]
,
P [h|Q = 0, lt ]

w.p. P [h|Q = Q∗ , lt ] , h ∈ {N } ∪ {−K1 , . . . , K2 },

where Q∗ is the true quality of the product.

4.2

Asymptotic Learning

We next show necessary and sufficient conditions for learning the true quality. We recall that θ̄ is
the upper support of the distribution of θ.
Theorem 1. Suppose Assumption 1 holds.
(a) If θ̄ + E[ζ] − p ≥ 0, then there is asymptotic learning, i.e., qt → Q∗ almost surely.
(b) If θ̄ + E[ζ] − p < 0, then there is a positive probability with which purchases stop and asymptotic
learning does not occur.
The proofs of all our results are provided in the Appendix.
The theorem shows that the condition θ̄ + E[ζ] − p ≥ 0 is sufficient for asymptotic learning.
When this condition does not hold, with positive probability all potential customers stop buying
the product and asymptotic learning fails. This can be seen by noting that at the time of the
purchase, the most positive assessment of expected quality will be from customer with the highest
ex ante valuation θ̄, and is θ̄ + E[ζ] + q − p, where q is the public belief at the time of purchase.
When θ̄ + E[ζ] − p < 0, there exists a sufficiently low value of q such that θ̄ + E[ζ] + q − p < 0,
implying that once beliefs reach this pessimistic level, even customers with the most positive ex
ante valuation stop purchasing, and thus beliefs remain stuck at q. Conversely, however, when
condition θ̄ + E[ζ] − p ≥ 0 holds, even for very pessimistic beliefs about the underlying quality,
9

some customers purchase the product and this generates sufficient information for asymptotic
learning.
Since our interest is in the speed of learning, in what follows, we impose:
Assumption 2. θ̄ + E[ζ] − p ≥ 0.
It is also instructive to revisit the difficulty faced by the agents in their updating decisions. As
explained in the Introduction, this difficulty has its roots in the selection effect — the fact that the
unobserved ex ante valuations of those who have previously purchased the good are correlated
with their public belief. One way of seeing the role of the selection effect is to consider a “naive”
learning rule, which simply considers the averages of past reviews, without attempting to correct
for the fact that the set of customers leaving reviews was determined by the information available
to them. Formally, we define a q̄-naive learning rule as an updating rule that aggregates past
reviews holding the public belief fixed at some level q̄. The naivety of this rule comes from the
fact that it does not take into account how the ex ante valuations of past customers change with
the public belief at the time. The evolution of beliefs under a q̄-naive learning rule is given by
t−1
Y
qt
P [hs | q = q̄, Q = 1]
=
.
1 − qt
P [hs | q = q̄, Q = 0]
s=1

The next proposition shows that asymptotic learning fails with naive learning rules.
Proposition 1. Suppose Assumptions 1 and 2 hold. Provided that

Eh∼P[·|q=0,Q=0]


P [h | q = q̄, Q = 0]
< 1,
P [h | q = q̄, Q = 1]

(5)

there is no asymptotic learning with a q̄-naive learning rule when Q∗ = 0, and provided that

Eh∼P[·|q=1,Q=1]


P [h | q = q̄, Q = 1]
< 1,
P [h | q = q̄, Q = 0]

(6)

there is no asymptotic learning with a q̄-naive learning rule when Q∗ = 1.
This proposition thus shows that under conditions (5) and (6), naive customers, who do not
recognize the selection effect, will not learn the underlying quality. To understand these conditions, let us consider the case where Q∗ = 1 so that the relevant condition is (6). The term in
square brackets is the likelihood ratio of history h under Q = 1 relative to Q = 0 holding the
public belief at q = q̄. The expectation of this likelihood ratio formed under correct public beliefs
(that is, under q = 1) is always greater than 1, and this ensures asymptotic learning with Bayesian
agents. However, when q = q̄ < 1, this expectation can be less than 1. To see when this will be the
case, consider a rating system with only two review options, “like” and “dislike”. Then condition
(6) is equivalent to P [h = K2 | q = 1, Q = 1] < P [h = K2 | q = q̄, Q = 0]. This condition is satisfied
if there is a sufficiently strong selection effect — if the entry of customers with relatively unfavor10

able ex ante types when q = 1 makes a “like” (K2 ) review less likely under Q = 1 than it would be
with q = q̄ and Q = 0.
In contrast to this result, recall that Theorem 1 has shown that Bayesian updating always leads
to asymptotic learning with full history. This is because with access to the full history of reviews,
Bayesian agents can form a correct assessment of the public beliefs of all past users and thus undo
the selection effect. For example, in the case with the two review options, the relevant comparison
for Bayesian agents is P [h = K2 | q = 1, Q = 1] vs. P [h = K2 | q = 1, Q = 0], and because the former is always greater, asymptotic learning is guaranteed. Put differently, Bayesian agents successfully filter out the correlation between reviews at time t and the distribution of ex ante valuations
of potential customers who would have purchased at time t, and this removes the selection effect
and ensures asymptotic learning. We will see in the next section that, without access to the full
history of reviews, dealing with the selection effect becomes more challenging, though Bayesian
customers will still be able to learn the underlying quality under some additional, relatively weak
conditions.
We should also note at this point that the intuition for asymptotic learning in Theorem 1 is very
different than in the baseline model of observational learning (in particular, Smith and Sørensen
[2000] and Acemoglu et al. [2011]). In these prior works learning occurs if the signals are unbounded, meaning that there is a positive probability of a very informative signal. This ensures
that “herding,” where all agents follow the action suggested by the public belief, disregarding
their own information, is not possible. In contrast, there is no possibility of herding here, because
agents leave reviews after experiencing the true quality. Instead, as we have just discussed, the
present challenge is to filter out the selection effect, which Bayesian agents can always do when
they have access to full history.

4.3

Speed of Learning

We next characterize the speed of learning under the full history rating system, i.e., Ωt = Ht for all
t. For this purpose, we introduce the Kullback-Leibler (KL) divergence between two distributions.
Definition 1 (KL divergence). Suppose µ and ν are two distributions on a measurable space X .
Kullback-Leibler divergence is defined as
 h  i
Eµ log dµ ,
dν
D(µ||ν) ,
∞,

µν
o.w.,

where µ  ν means µ is absolutely continuous with respect to ν.13 In particular, if µ and ν are two
13

A measure µ is absolutely continuous with respect to another measure ν if µ(E) = 0 for every set with ν(E) = 0.
Also, throughout, log refers to logarithm with base e.

11

distributions on a finite set X , KL divergence becomes
D(µ||ν) ,

X


µ(x) log

x∈X

µ(x)
ν(x)


.

The next theorem characterizes the rate of learning under full history.
Theorem 2. Suppose Assumptions 1 and 2 hold. Then learning is exponentially fast, i.e., almost surely qt
converges exponentially fast to Q∗ . Moreover, for Q∗ = 0, we almost surely have
1
log qt = −D (P [·|q = 0, Q = 0] ||P [·|q = 0, Q = 1]) ,
t→∞ t
lim

and for Q∗ = 1, we almost surely have
lim

t→∞

1
log(1 − qt ) = −D (P [·|q = 1, Q = 1] ||P [·|q = 1, Q = 0]) .
t

In what follows we use rate of learning and speed of learning interchangeably to refer to the
exponent with which belief converges to the true quality.
This theorem establishes that learning under full history is exponentially fast, and moreover,
its exact rate is governed by the KL divergence between the probability distribution of possible
actions (i.e., h ∈ {N } ∪ {−K1 , . . . , K2 }) when the underlying quality is Q∗ and the public belief
is asymptotically equal to q = Q∗ and the probably distribution of observing the same sequence
when the underlying quality is 1 − Q∗ . There are three components to the intuition for this result.
First, the fact that the learning is exponentially fast follows from the ability of customers to overcome the selection effect, which then enables them to combine the information of the t signals (for
customer t + 1) they are receiving from the reviews of past customers.
Second, the exact speed of learning is given by KL divergence. This is also intuitive. We can
think of the problem of distinguishing Q = 0 from Q = 1 as a binary hypothesis testing problem. The best error exponent for a binary hypothesis testing problem from independently-drawn
samples is given by the KL divergence between the probability distributions of these samples
conditional on the two hypotheses (e.g., Cover and Thomas [2012, Theorem 11.8.3]). Technically,
in our case the sequence of reviews does not correspond to an independently-drawn sample because of the selection effect — the current belief affects the distribution of types that will purchase
and thus the probability distribution of reviews. Nevertheless, as q → Q∗ almost surely, we can
bound the effects of this dependence and still derive the KL divergence as the measure of the distance between the two relevant probability distributions determining the speed of learning. More
specifically, note that when Q∗ = 0,
1
1
log lt = lim
t→∞ t
t→∞ t
lim

X

t
X

h∈{N }∪{−K1 ,...,K2 } s=1

12


1{hs = h} log

P [h | qs , Q = 1]
P [h | qs , Q = 0]


.

If we could apply the strong law of large numbers with qs = 0, this would imply
1
lim log lt =
t→∞ t

X


P [h | q = 0, Q = 0] log

h∈{N }∪{−K1 ,...,K2 }

P [h | q = 0, Q = 1]
P [h | q = 0, Q = 0]



= −D (P [·|q = 0, Q = 0] ||P [·|q = 0, Q = 1]) .
Though we cannot directly apply the strong law of large numbers, we can bound the departure
from independence by sandwiching lt between two stochastic processes with independent increments, both converging at the rate stated in the theorem.
Finally, it is also worth noting that both probability distributions in the KL divergence condition on q = Q∗ . This is because, under full history, each customer correctly reasons about the
public belief of previous users, which is converging to Q∗ . It is this feature of the full history rating
system that enables the effective filtering of selection effect, and will contrast with our analysis of
the case of summary statistics, which we present in the next section.

4.4

When Customers Observe a Subset of Previous Actions

We end this section by showing that the results presented so far directly generalize to the case in
which the history available to potential customers only includes a subset of actions. For instance,
the history available to potential customers may only include reviews (and no information about
those who have not purchased the product or have not left a review). Formally, we consider a
subset of all possible actions {N } ∪ {K1 , . . . , K2 } denoted by T , and denote a partition of T by
T1 , . . . , Tm , i.e., T = ∪m
i=1 Ti and Ti ∩ Tj = ∅, i 6= j ∈ [m]. We also use the notation τ to denote
the subsequence of time t at which users observe an action in T (e.g., if customers observe past
reviews, τ corresponds to the number of reviews so far). In other words, the history observed by
the customer acting after τ − 1 is {h̃1 , . . . , h̃τ −1 }, where h̃s = i if the s-th action in T belongs to Ti .
Customers do not observe any other event, and have uniform priors over the times at which there
have been unobservable actions. The next proposition generalizes Theorem 2 to this case.
Proposition 2. Suppose that Assumptions 1 and 2 hold, and that customers only observe actions in T and
have uniform priors over the times at which actions that are not in T have taken place. Then, there is again
asymptotic learning. Moreover, for Q∗ = 0, we almost surely have
lim

τ →∞

1
log qτ = −D (P [· | h ∈ T, q = 0, Q = 0] ||P [· | h ∈ T, q = 0, Q = 1]) ,
τ

and for Q∗ = 1, we almost surely have
1
log(1 − qτ ) = −D (P [· | h ∈ T, q = 1, Q = 1] ||P [· | h ∈ T, q = 1, Q = 0]) .
τ →∞ τ
lim

To understand this result, consider a customer at time t that observes a history with τ actions in
T . This customer does not know the number of past users who have joined the platform between
13

the most recent action in T and her time. However, she knows exactly the belief of any user who
has taken an action in T because she observes the same history as this user, and she can filter out
the selection effect and aggregate information of all past customers who have taken an action in T ,
which is sufficient to ensure asymptotic learning. The speed of learning is modified in view of the
fact that the information of customers who are not taking an action in T is not being transmitted,
but is still given by a similar KL divergence term.

5

Summary Statistics

In this section, we focus on more realistic rating systems where the platform provides summary
statistics of reviews by past customers. We then establish asymptotic learning in this case under an
additional assumption (which is needed because filtering out the selection effect is more difficult
without access to full history), and subsequently characterize the speed of learning in this case.

5.1

Learning Dynamics

We now focus on rating systems in which there exists a vector of summary statistics S, providing
various summaries of past reviews. As in our analysis in subsection 4.4, with summary statistics
customers will not see the full history, and we again find it convenient to index them not by the
time of their arrival, but by the number of actions that have taken place so far (i.e., actions that
are in the set T defined in subsection 4.4). In our leading case, this will be the number of reviews
left so far, denoted by τ . We also assume that customers know their τ (and not their exact place in
the original sequence, and as in subsection 4.4, have uniform priors over their exact place in the
sequence). Therefore, for consumer indexed by τ , the rating system takes the form Ωτ = Sτ , and
reports information from the first τ − 1 reviews.
To define the summary statistics, we consider a partitioning of all review options {−K1 , . . . , K2 }\
{0} (recall that 0 represents “no review”) denoted by T = ∪m
i=1 Ti . A summary statistic is a vector
Sτ ∈ Rm with its i-th entry representing fraction of those reviews in Ti that has occurred among
all reviews, i.e.,14

τ
1X
Sτ (i) =
1{hs ∈ Ti },
τ

i ∈ [m].

s=1

The following are examples of rating systems with summary statistics:
• The fractions of each one of K1 + K2 reviews are reported. In this case, Sτ ∈ Rm where
m = K1 + K2 , and T = {T1 , . . . , Tm } is T1 = {−K1 }, . . . , TK1 = {−1}, TK1 +1 = {1}, . . . , and
TK1 +K2 = {K2 }.
• “Likes” among all reviews meaning that the rating system reports only the fraction of reviews that give the highest score, h = K2 , out of the available K1 + K2 options. This rating
P
With this definition, for any τ ≥ 0, we have m
i=1 Sτ (i) = 1. Therefore, one of the entries of vector Sτ is redundant.
This is chosen to simplify the notation used in the analysis and does not change any of our results.
14

14

system is represented by Sτ ∈ R2 where
Sτ (1) =

τ
1X
1{hs = K2 },
τ

and

Sτ (2) = 1 − Sτ (1).

(7)

s=1

We will also discuss other, albeit perhaps less realistic, rating systems which provide information about fractions of reviews among all purchases (including the customers who have purchased the good in question, but have left no review in the denominator) and among all customers who have joined the platform. For example, when we focus on all purchases, with a slight
abuse of notation, we will use τ to denote the number of purchases, and we consider partitions
of {−K1 , . . . , K2 } (rather than partitions of {−K1 , . . . , K2 }\{0}). A particularly simple version of
such a rating system would be one that reports “likes” among all purchases which can also be
represented by Eq. (7), but with τ now corresponding to the number of purchases.
Similar to our analysis of the full history case, we denote the expectation of the underlying
quality Q, conditional on the information available from the rating system Sτ , by qτ . In other
words, qτ = P [Q = 1 | Sτ ]. The likelihood ratio implied by this belief is then
lτ =

qτ
P [Q = 1 | Sτ ]
P [Sτ | Q = 1]
=
=
.
1 − qτ
P [Q = 0 | Sτ ]
P [Sτ | Q = 0]

(8)

Crucially, however, unlike the full history case, qτ , is no longer a public belief — future customers
who do not have access to Sτ neither observe nor can compute qτ . For this reason, the likelihood
ratio lτ is no longer a martingale, and we need to develop a different approach for analyzing the
dynamics of beliefs and their asymptotic properties.15
With this objective, let us write the governing stochastic process for Sτ given the true quality
Q∗

as
Sτ +1 =

1
τ
Sτ +
Yτ +1 ,
τ +1
τ +1

∀τ ≥ 0,

(9)

where Yτ +1 ∈ Rm , and
Yτ +1 = ei ,

w.p. P [h ∈ Ti | h ∈ T, qτ , Q∗ ] ,

where ei ∈ Rm is the i-th canonical basis (i.e., ei = (0, . . . , 0, 1, 0, . . . , 0), only with its i-th entry
being equal to 1).
We should note that the same challenge that was discussed in the previous section is present
here, i.e., in finding posterior probabilities in Eq. (8), Bayesian customers must take into account
that there have been some unobserved actions that do not belong to T .
15

In Lemma 1 in the Appendix, we show that, given uniform priors on unobserved actions, Eq. (8) holds exactly when
users apply Bayes’ rule fully recognizing that there may have been other customers whose actions are not recorded in
the summary statistics (e.g., because they have not purchased or have not left reviews).

15

5.2

Asymptotic Learning

Let us use G1 (i, q) (respectively, G0 (i, q)) to denote the probability of Yτ = ei given belief q and
Q = 1 (respectively, Q = 0). Therefore,
G1 (i, q) = P [h ∈ Ti | h ∈ T, q, Q = 1] ,

i ∈ [m],

G0 (i, q) = P [h ∈ Ti | h ∈ T, q, Q = 0] ,

i ∈ [m],

(10)

where [m] denotes the set {1, . . . , m}. Since T1 , . . . , Tm form a partition of T , for any q, we have
Pm
Pm
i=1 G1 (i, q) =
i=1 G0 (i, q) = 1.
As explained in the previous section, the selection effect becomes more difficult to handle
without full history. Recall that with full history, a customer knows the public belief at each point
in the past, and thus can work out the average ex ante valuation of those who purchased at that
point, enabling her to correct for the selection effect. This is no longer possible with a rating
system that provides summary statistics, because a customer does not know the belief of previous
customers. Nevertheless, we will show that asymptotic learning still obtains provided that the
following additional assumption is imposed.
Assumption 3 (Separation). A rating system (T1 , . . . , Tm ) has the separation property if there exists i ∈ [m] such that ranges of G1 (i, ·) and G0 (i, ·) are separated, i.e., either
min G1 (i, q) > max G0 (i, q), or

q∈[0,1]

q∈[0,1]

min G0 (i, q) > max G1 (i, q).

q∈[0,1]

q∈[0,1]

The separation property ensures that, whatever the value of the belief q, the asymptotic distribution of at least one dimension of the summary statistics has support when the underlying
quality is high that is everywhere above (or below) the support when the underlying quality is
low. This separation property enables customers to correctly identify the underlying quality of
the product despite the selection effect.
Theorem 3. Suppose Assumptions 1, 2, and 3 hold. Then, there is asymptotic learning, i.e., qτ → Q∗
almost surely.
Intuitively, given a summary statistic Sτ , customers form their beliefs based on the likelihood
of Sτ given Q = 0 and Q = 1. If the probability distributions of summary statistics given Q = 0
and Q = 1 both assign positive probabilities to the same events, then conditional on the realization
of these events (e.g., certain fractions of reviews), it would be impossible for customers to learn
the underlying state almost surely. Conversely, consider the case in which these two probability
distributions are separated. Specifically, without loss of generality, suppose that for some i ∈ [m],
G1 (i, ·) is above G0 (i, ·). As τ grows, the marginal distributions of Sτ (i) conditional on Q = 0
and Q = 1 overlap with smaller and smaller probabilities, and depending on the value of Sτ (i) as
τ → ∞, customers can learn the underlying state almost surely.

16

0.35

0.8

Probability

0.6

G1 (1, q)

0.3

G0 (1, q)

0.25

Probability

0.7

0.5
0.4
0.3

Distribution of likes: Q∗ = 1
Distribution of likes: Q∗ = 0

0.2
0.15
0.1

0.2
0.05

0.1
0
0

0
0

0.2

0.4

0.6

0.8

0.1

0.2

0.3

0.4

Sτ (1)
τ :

1

Belief q

0.5

0.6

0.7

0.8

0.9

1

fraction of likes

(a)

(b)

Figure 2: (a) G1 (1, q) and G0 (1, q) as a function of q ∈ [0, 1] (b) distribution of fraction of “likes ”for
Q = 1 and Q = 0 for τ = 100.
0.07

1

0.06

Distribution of fraction of likes: Q∗ = 1
Distribution of fraction of likes: Q∗ = 0

0.8

Probability

Probability

0.05

0.6

G1 (1, q)

0.4

G0 (1, q)

0.04
0.03
0.02

0.2

0.01
0

0
0

0.2

0.4

0.6

0.8

0

1

0.1

0.2

0.3

0.4

Sτ (1)
τ :

Belief q

(a)

0.5

0.6

0.7

0.8

0.9

1

fraction of likes

(b)

Figure 3: (a) G1 (1, q) and G0 (1, q) as a function of q ∈ [0, 1] (b) distribution of fraction of “likes ”for
Q = 1 and Q = 0 for τ = 100.
We will next show via an example that if the “separation assumption” does not hold, then
asymptotic learning fails.
Example 1. We consider a rating system that reports the fraction of “likes” among all reviews,
i.e., T = {−K1 , . . . , K2 }\{0}, T1 = {K2 }, and T2 = T \ T1 . The probability distributions of this
summary statistics conditional on the underlying state Q and belief q, G1 (1, q) and G0 (1, q) are
shown in Figure 2a, which illustrates that these distributions are not separated (and neither are
G1 (2, q) = 1 − G1 (1, q) and G0 (2, q) = 1 − G0 (2, q)). In the Appendix, we show that asymptotic
learning does not occur in this example: given the lack of separation, the likelihood ratio of Sτ
given belief q and Q = 0 vs. Q = 1 remains bounded away from 0 or ∞ as shown in Figure 2b.
In contrast, G1 (1, q) and G0 (1, q) in Figure 3a are separated and as illustrated in Figure 3b, the
distribution of fraction of “likes” among all reviews given Q = 0 and Q = 1 are also separated,
and consequently, there is asymptotic learning both when Q = 0 and when Q = 1.

5.3

Speed of Learning

We next characterize the asymptotic speed of learning under rating systems with summary statistics. As we have seen, ensuring asymptotic learning requires additional conditions under summary statistics. However, the next theorem shows that conditional on asymptotic learning, the
speed of learning is governed by a related KL divergence.

17

Theorem 4. For a given rating system (T1 , . . . , Tm ), suppose Assumptions 1, 2, and 3 hold. Then, for
Q∗ = 0, we almost surely have
1
log qτ = −D (P [· | h ∈ T, q = 0, Q = 0] ||P [· | h ∈ T, q = 1, Q = 1])
τ →∞ τ
lim

and for Q∗ = 1, we almost surely have
lim

τ →∞

1
log(1 − qτ ) = −D (P [· | h ∈ T, q = 1, Q = 1] ||P [· | h ∈ T, q = 0, Q = 0]) .
τ

The intuition for why KL divergence determines the speed of learning is similar to the full
history case. There is a major difference worth noting, however. While under full history, the
probability distribution under the alternative hypothesis Q = 1 − Q∗ , was still conditioned on
q = Q∗ , it is now conditioned on q = 1 − Q∗ . This again reflects the role of the selection effect.
Because under full history, customers know the public belief at each point, they can fully correct
for the selection effect, and this enables them to condition on the correct public belief. Here, this
is not possible. In particular, the implicit assumption has to be that if Q = 1 − Q∗ , then the belief
of all other customers, which is not observed, would have also converged to q = 1 − Q∗ . This
difference explains the exact form of the KL divergence in the previous theorem.

6

Comparison of Rating Systems

In this section, we compare the speed of learning under full history and summary statistics; we
show how varying certain properties of the rating system affects the speed of learning; we provide
examples of fast and slow learning; and finally, we show how the platform has a natural incentive
for choosing rating systems that ensure fast learning.

6.1

Full History versus Summary Statistics

In this subsection, we show that the speed of learning could be greater under rating systems with
either full history or summary statistics. For simplicity, we focus on a one-dimensional summary
statistics, i.e., those that report the fraction of one type of review among either all reviews or
all customers. Whether asymptotic learning under full history is faster will turn out to depend
on whether the probability of positive reviews under a rating system with summary statistics
exhibits negative selection or positive selection. Negative selection is what we have focused on so far.
It corresponds to the case where as the public belief becomes more favorable to Q = 1, the ex ante
valuations of customers become more “negatively selected” (more likely to be negative). Positive
selection, conversely, corresponds to the case where as the public belief becomes more favorable to
Q = 1, the ex ante valuations of customers become more “positively selected”. Though negative
selection is the more natural case (because with more favorable reviews about the quality of the
product, customers with lower ex ante valuations also purchase it), we illustrate that positive

18

selection can emerge in some rating systems. The next two corollaries show that with negative
selection, rating systems with summary statistics lead to slower learning than full history, but
with positive selection, they may entail faster learning. For simplicity, in these two corollaries, we
focus on rating systems with only three review options: “dislike” (h = −K1 ), “no review” (h = 0),
and “like” (h = K2 ), and compare the speed of learning under full history and summary statistics
when customers have access to these three review options.
Corollary 1 (negative selection). Suppose Assumptions 1, 2, and 3 hold and Q∗ = 0 (the case where
Q∗ = 1 is analogous). Consider a rating system that reports the fraction of “likes among reviews”, i.e.,
P
Sτ (1) = τ1 ts=1 1{hs = K2 } with τ corresponding to the number of reviews. Then
P [h = K2 | h ∈ {−K1 , K2 }, q = 1, Q = 1] < P [h = K2 | h ∈ {−K1 , K2 }, q = 0, Q = 1] ,

(11)

and the speed of learning under this rating system with summary statistics is slower than under full history.
The rating systems considered in Corollary 1 exhibit negative selection precisely because
P [h = K2 | h ∈ {−K1 , K2 }, q, Q] is decreasing in q (as well as being increasing in Q). This reflects
the fact that as q increases, the marginal type that purchases has lower θ, and is thus less likely
to gain a very high overall satisfaction leading to a positive (“like”) review. We next discuss why
learning is slower in this case with summary statistics than under full history. Suppose Q∗ = 0,
and recall that with summary statistics the inference problem of customers is to distinguish the
probability distribution of reviews conditional on q = 1 and Q = 1 (the “alternative hypothesis”)
from the probability distribution of reviews conditional on q = 0 and Q = 0 (the “null hypothesis”). In contrast, with full history they must distinguish the probability distributions conditional
on q = 0 and Q = 1 and conditional on q = 0 and Q = 0 (recall Theorem 2). But then inequality
(11), which follows from the fact that with negative selection P [h = K2 | h ∈ {−K1 , K2 }, q, Q] is
decreasing in q, implies that this inference is more difficult with summary statistics than with full
history. This comparison is at the root of the faster rate of learning under full history.
The next corollary, however, shows that the opposite result is possible if we consider a rating
system with “likes among all (potential) customers”, which induces positive selection.
Corollary 2 (positive selection). Suppose Assumptions 1, 2, and 3 hold and Q∗ = 0 (the case where
Q∗ = 1 is analogous). Consider a rating system that reports the fraction of “likes among all customers”,
P
i.e., Sτ (1) = τ1 τs=1 1{hs = K2 } with τ now corresponding to the number of customers who joined the
platform. Then
P [h = K2 | h ∈ {−K1 , K2 }, q = 1, Q = 1] > P [h = K2 | h ∈ {−K1 , K2 }, q = 0, Q = 1] ,

(12)

and the speed of learning under this rating system with summary statistics is faster than under full history.
The intuition for this corollary is similar to that of Corollary 1, with the only difference that
now there is positive selection. This is because as q increases, there are more purchases, and
19

more purchases always increase the number of more “likes” (or more generally, of more positive
reviews). But then positive selection implies that the gap between the probability distributions
of reviews conditional on q = 0 and Q = 0 and conditional on q = 0 and Q = 1 is less than the
gap between those conditional on q = 0 and Q = 0 and conditional on q = 1 and Q = 1. This
means that learning is slower under full history. The simplest intuition is that positive selection
heightens the difference between the probability distributions implied by the null and alternative
hypotheses.16

6.2

The Effects of More Refined Rating Systems

In this subsection, we show that more refined rating systems lead to faster learning, both under full
history and under summary statistics. Given two rating systems Ω and Ω0 with the same review
options, we say that Ω0 is more refined than Ω if the latter reports ratings in a partition T that is
(strictly) coarser than the former’s partition T 0 . For example, this will be the case when Ω reports
the fraction of customers that have left one of the most positive two review options, while Ω0
separately reports the fractions of customers that have left each one of those two review options.
The next proposition is the main result of this subsection.
Theorem 5. Consider a rating system with either full history or with summary statistics, and suppose that
Assumptions 1 and 2 hold, and in the case of summary statistics, suppose also that Assumption 3 holds.
Then the speed of learning is always faster under a more refined rating system.
Intuitively, a more refined rating system always gives more information, and this makes it
easier for customers to distinguish between the probability distribution of reviews induced under
the true state of nature and the alternative. Importantly, however, this result does not imply that
changing the rating system by increasing the number of review options always leads to faster
learning. This is because, when the platform changes the review options in the rating system, the
thresholds of users might also change (for this reason, the comparison between a more and less
refined rating system in Theorem 5 kept the number of review options the same, but modified
the reporting strategy of the platform). The next example shows that if increasing the number of
review options changes the thresholds, it can lead to slower learning.
Example 2. Consider a rating system with full history. Assume ζ is uniform over [−2, 2], θ is
uniform over [−1, 1], and p = 0. We consider the following two rating systems:
1. there are two review options “like” and “dislike”, with thresholds λK2 = λK1 = 0;
2. there are three review options “like”, “no review”, and “dislike”, with thresholds λK1 =
−1/2 and λK2 = 1/2.
16
It is also straightforward to show that the set of parameters that ensures that the conditions of both Corollaries
1 and 2 is nonempty. For example, we show in Appendix that they are satisfied when we take both ζ and θ to have
uniform distributions with mean zero and large enough supports.

20

We can then use Theorem 1 to characterize the speed of learning in these two cases. We see
that even though the second rating system has more review options, it leads to slower learning.
This result does not contradict Theorem 5 because the partition of the second rating system, T 0 , is
not strictly finer than that of the first rating system’s partition T .

6.3

Average Summary Statistics

Several online platforms report an average score of past reviews rather than depicting detailed
fractions of reviews that fall in different categories. In this subsection, we show that this type of
average summary statistics leads to slower learning than their detailed counterpart. More formally, consider a rating system with K ≥ 1 review options:



1,


hτ = i,



K,

θτ + E[ζτ ] + qτ − p ≥ 0, θτ + ζτ + Q − p ≤ λ1
θτ + E[ζτ ] + qτ − p ≥ 0, λi−1 ≤ θτ + ζτ + Q − p ≤ λi , 2 ≤ i ≤ K − 1
θτ + E[ζτ ] + qτ − p ≥ 0, θτ + ζτ + Q − p ≥ λK−1 .

A rating system is said to have an average summary statistics if it reports the number
τ
1X
Sτ =
hs ,
τ
s=1

where τ is the number of reviews. Similar to Eq. (10), for Q ∈ {0, 1}, let us define


P [θτ + ζτ + Q − p ≤ λ1 | θ + E [ζ] + q − p ≥ 0] ,


GQ (i, q) = P [λi−1 ≤ θτ + ζτ + Q − p ≤ λi | θ + E [ζ] + q − p ≥ 0] ,



P [θ + ζ + Q − p ≥ λ
| θ + E [ζ] + q − p ≥ 0] ,
τ

τ

K−1

i = 1,
2 ≤ i ≤ K − 1, ,
i = K.

Again, similar to Eq. (9), given true quality Q∗ ∈ {0, 1}, the governing stochastic process for Sτ
becomes
Sτ +1 =

1
τ
Sτ +
Yτ +1 ,
τ +1
τ +1

∀τ ≥ 0,

where
Yτ +1 = i,

w.p. GQ∗ (i, qτ ) , i ∈ [K].

To distinguish the rating systems we have analyzed so far (which report fractions of users that
have left various different reviews) from those with average statistics, we refer to them as “rating
systems with vector summary statistics”.
Theorem 6. Suppose Assumptions 1 and 2 hold, and for all i = 2, . . . , K,

PK

j=i G1 (j, q) and

PK

j=i G0 (j, q)

are separated, i.e.,
min
q

K
X
j=i

G1 (j, q) > max
q

K
X
j=i

21

G0 (j, q),

i = 2, . . . , K.

(13)

Then asymptotic learning occurs under average summary statistics, i.e., qτ → Q∗ almost surely, but the
speed of learning is slower under average statistics than with vector summary statistics.
Theorem 6 establishes that the speed of learning for rating systems with average summary
statistics is always slower than the comparable rating system with vector summary statistics. The
intuition is similar to Theorem 5. Note also that we did not impose Assumption 3, since condition
(13), which ensures separation with average summary statistics, is sufficient for Assumption 3
(to see this we can take i = K in condition (13)). This point is illustrated in greater detail in the
next example, which shows that asymptotic learning might fail under average summary statistics,
while it applies with a rating system with summary statistics that reports fractions of different
reviews.
Example 3. We consider a rating system with three review options: “like” (3), “neutral” (2), and
“dislike” (1), where the probability of “like” and “dislike” as a function of belief are shown in
Figure 4.
1. Average summary statistic: the probability of average of reviews at τ = 100 is plotted in
Figure 5a. The probability for Q = 0 and Q = 1 have overlap, showing that with positive
probability, learning does not occur. Note that the probabilities plotted in Figure 5b do not
satisfy the assumptions of Theorem 6 because for i = 2, we have
min G1 (2, q) + G1 (3, q) = 1 − max G1 (1, q) < 1 − min G0 (1, q) = max G0 (2, q) + G0 (3, q).
q

q

q

q

2. Vector summary statistics: the probability of “like” at τ = 100 is plotted in Figure 5b. The
probability for Q = 0 and Q = 1 are separated, showing that by considering only the fraction
of “likes”, we can identify whether Q = 0 or Q = 1, hence learning occurs.
This implies that for the same distributions of preference parameters (i.e., θ and ζ), asymptotic
learning fails for the rating system with average summary statistic, while there continues to be
asymptotic learning for the rating system with the same review options but with vector summary
statistics.

6.4

Effects of Targeted Information

In addition to the overall ratings of a product, platforms such as Amazon offer information about
the reviews of groups of customers with certain characteristics. For instance, in reviews of a book
at the intersection of climate science and economics, Amazon separately shows reviews among
customers who are interested in economics as well as reviews among customers who are interested
in climate science. We refer to this type of additional information as “targeted information”.
Our next result shows that targeted information always increases the speed of learning.

22

1
1
Q∗ = 1
0.8

Q∗ = 0

Probability

Probability

0.8
0.6
0.4
0.2

Q∗ = 1
Q∗ = 0

0.6
0.4
0.2

0

0
0

0.2

0.4

0.6

0.8

1

0

0.2

Belief: q

0.4

0.6

0.8

1

Belief: q

(a)

(b)

Figure 4: (a) Probability of “like” given belief for Q∗ = 0 and Q∗ = 1 which are separated. (b)
Probability of “dislike” given belief for Q∗ = 0 and Q∗ = 1 which are not separated.
0.25

0.06
Prob. of weighted rating: Q∗ = 1

0.2

0.05

Q∗ = 1

0.04

Q∗ = 0

Probability

Probability

Prob. of weighted rating: Q∗ = 0

0.15

0.1

0.05

0.03
0.02
0.01
0

0
1

1.5

2
2.5
Normalized Weighted Ratings

0

3

0.2

0.4

0.6

0.8

1

Fraction of likes

(a)

(b)

Figure 5: (a) Probability distribution of normalized average ratings for Q∗ = 0 and Q∗ = 1 which
are not separated (for τ = 100). (b) Probability distribution of fraction of “likes” for Q∗ = 0 and
Q∗ = 1 which are separated (for τ = 100).
Theorem 7. Consider a rating system with either full history or with summary statistics, and suppose that
Assumptions 1 and 2 hold, and in the case of summary statistics, suppose also that Assumption 3 holds.
Then the speed of learning is always faster with additional targeted information.
The intuition for faster learning with targeted information is related to the selection effect. Recall that the selection effect makes it more difficult for customers to learn from observed reviews.
Targeted information increases customers’ ability to distinguish the true quality from its opposite, because with additional information they can more successfully filter out the selection effect.
Note, however, that for this result of faster learning, the targeted information has to be provided
in addition to the baseline. If the platform reports only information about the reviews of the subgroup of customers (and suppresses any information about reviews from other subgroups), this
can slow down learning relative to the baseline without targeted information.

6.5

Fast and Slow Learning from Reviews

We have seen so far that the speed of rating system for both summary statistics and full history is
given by the KL divergence between the probability distribution of reviews under the true quality
and its opposite. This implies, for example, that for a rating system with summary statistics,
learning is fast when the ratio of probability of a review given (q = Q∗ , Q = Q∗ ) and (q = 1 −
Q∗ , Q = 1 − Q∗ ) is high. We next illustrate how the speed of learning depends on the parameters
23

0.02

Speed of Learning for Q∗ = 1
Speed of learning

0.015

Speed of Learning for Q∗ = 0
0.01

0.005

0
5

10

15

20

25

30

ζ̄

Figure 6: The speed of learning as a function of the support of ζ for Example 4 with p = 0,
θ̄ = −θ = 2, and λK2 = −λ−K1 = 1.
of the rating system and the distribution of preferences.
In the first example, we show that increasing the support of ex post preference parameter, ζ,
can significantly decrease the speed of learning.
Example 4. Suppose θ and ζ have symmetric uniform distributions with diffuse supports such that

p ∈ −θ̄ + 1, θ̄ ,

ζ̄ ≥ max{λK2 + 1, −λ−K1 − p + θ̄ + 1}.

From Theorem 2, the speed of learning with full history can be computed as shown in Figure 6.
We can see that the speed of learning for both Q∗ = 0 and Q∗ = 1 significantly decrease as the
support of ζ widens (i.e., as ζ̄ increases). The characterization of the speed of learning in Theorem
2 provides the intuition for this result: when ζ has a diffuse support, the likelihood of each review
under both (q = Q∗ , Q = Q∗ ) or (q = Q∗ , Q = 1 − Q∗ ) is similar, and thus the likelihood ratio of
each review is close to 1, leading to a low KL divergence. The differences in the speed of learning
can be quite substantial. For example, Figure 6 shows that when ζ̄ = 5, the speed of learning
for Q∗ = 1 is 0.02, which means that the “half life” of convergence to the true state of nature is
50 periods (meaning that starting from any belief half of the distance to q = 1 will be close in
50 periods). In contrast, when ζ̄ is 20, the speed of learning changes to 0.001, and the half life of
learning the underlying state is 1000.
The next example shows that a refinement of the review options in the rating system can significantly increase the speed of learning.
Example 5. Suppose that θ has a uniform distribution over [−2, 2], p = 0, and ζ has the following
symmetric bimodal distribution

1



24 ,



 1 + 4 z + b + 1 ,
4
fζ (z) = 24

1


+ 4 −z − b + 14 ,

24



1,
24

z ≤ −b −
−b −

1
4

1
4

< z ≤ −b,

−b < z ≤ −b + 41 ,
−b +

1
4

< z ≤ 0,

over the support [−6, 6]. This distribution implies that the likelihood of a realization of ζ is very
24

θ

θ

ζ
Q=1

ζ
Q=0

Figure 7: The joint distribution of (θ, ζ) and the probabilities of the review options “like” and
“love” for two cases under Q = 0 and Q = 1 for b = 3 for Example 5. The double shaded areas
correspond to the “love” option, and the shaded areas correspond to the “like” option.
high around b and −b. We next compute the speed of learning of rating system with full history
and the following review options:
• Case 1: “dislike”, “no review”, and “like” and the thresholds are 1 and −1.
• Case 2: “dislike”, “no review”, “like”, and “love” and the thresholds are 1, −1, and 5.
The joint distribution of (θ, ζ) and the probabilities of observing the “like” and “love” reviews
under Q = 0 and Q = 1 are depicted in Figure 7. The speed of learning for these two cases as
a function of parameter b are, in turn, plotted in Figure 8. In Figure 8, when b = 5, the speed of
learning without the “love” option is approximately 0.001, which means that the half time over
learning the truth would be 1000 periods. In contrast, with the “love” option present, the speed
of learning changes to 0.2, equivalent to the half time for learning the truth be 5. The speed of
learning of the first case is slow because for each review, the ratio of the probability of observing
that review when Q = 0 to the probability of observing it when Q = 1 is close to 1. In fact, as b
increases, this ratio becomes closer to 1 and the speed of learning declines further (towards zero).
The speed of learning in the presence of the “love” option is shown in Figure 8, and is significantly
higher. This is because the probability of observing the “love” review is very close to zero when
Q = 0, and is positive when Q = 1. This leads to a large value of KL divergence and thus to
relatively fast learning.17

6.6

The Choice of Rating System

In this subsection, we show that the speed of learning is highly relevant for platforms. In fact, platforms themselves have an incentive to choose rating systems with faster learning speed. Recall, in
17

Theorem 5 implies that for a given distribution for parameters θ and ζ, the rating system with the highest speed
of learning will be the one with the most refined rating system, which means reporting the exact utility of previous
customers (i.e., θt + ζt + Q − p). For instance, when Q∗ = 0 and under full history, the highest rate of learning is
D (P [θ + ζ − p | θ + E[ζ] − p ≥ 0] ||P [θ + ζ − p + 1 | θ + E[ζ] − p ≥ 0]), which is 0.55 in Example 5.

25

0.25

Speed of learning

0.2

0.15

Rating system with “love” review
Rating system without “love” review

0.1

0.05

0
3

3.5

4

4.5

5

Parameter: b

Figure 8: The speed of learning as a function of parameter b for Example 5. As b increases, the
speed of learning of the rating system without the “love” option becomes very small, while the
speed of learning of the rating system with the “love” option remains large.
particular, that in our model the platform’s revenues come from advertisements or other benefits
that it derives from potential customers joining the platform. For example, the platform may be
receiving a fixed revenue for each customer that joins. Consequently, we can write the platform’s
problem as maximizing the expected number of customers that join the platform. We next show
that this problem is equivalent to choosing a rating system with a higher speed of learning. We
say rating system ΩIt has a higher learning speed than rating system ΩII
t , if
1
1
log qtI ≤ lim log qtII , given Q∗ = 0,
t→∞ t
t→∞ t


1
1
lim log 1 − qtI ≤ lim log 1 − qtII , given Q∗ = 1,
t→∞ t
t→∞ t




where qtI = P Q = 1 | ΩIt and qtII = P Q = 1 | ΩII
t .
lim

Theorem 8. Given two rating systems I and II, the one with higher learning speed has a higher revenue
for the platform.
The main result in this theorem is intuitive: with a rating system that guarantees a faster learning speed, potential customers know that once they join the platform, they are less likely to make
both type I and type II errors in their purchase decisions. This increases their ex ante utility from
joining, and thus increases the expected revenues of the platform. To see why the probabilities
of type I and type II errors go down, let us consider separately the case of low and high quality. Suppose Q∗ = 0. Then the problem for potential customers is to incorrectly buy the product thinking that it is high-quality. In particular, if θ + ζ − p ∈ (−1, 0), then with probability
Pt,Ωt [qt ≥ p − θ − ζ | Q = 0], they purchase the product and suffer a loss of θ + ζ − p. Yet, this
probability of incorrectly purchasing the product becomes smaller when qt is closer to Q∗ = 0.
Conversely, suppose that Q∗ = 1. Then the problem for potential customers is to fail the purchase
of the product when they should be buying. In particular, if θ + ζ − p ∈ (−1, 0), then with probability Pt,Ωt [qt ≤ p − θ − ζ | Q = 1] customers do not purchase the product and forgo a potential
utility gain of 1 + θ + ζ − p. The probability of such a mistake becomes smaller when qt is closer
to Q∗ = 1. Thus in both cases a faster learning speed implies smaller expected probabilities of
mistakes, encouraging customers to join the platform.
26

7

Concluding Remarks

As the amount of goods and services sold online continues to increase rapidly, rating systems that
provide information on both the quality of various products that are difficult to inspect online
and the reputation of distant sellers and service providers have also become critical. Despite their
importance, properties and efficacy of online rating systems have attracted only limited attention
in the recent literature.
In this paper, we developed a model of Bayesian learning from online reviews, and investigated the conditions for asymptotic learning of the quality of a product and the speed of learning
under different rating systems. A rating system provides information about reviews left by previous customers. Prominent examples of such rating systems are those that provide average scores
or the distribution of numerical ratings left by past customers.
In our model, a sequence of potential customers decide whether to join a platform and contemplate the purchase of a product with unknown quality, which is assumed to be binary, corresponding to either high or low quality. After joining and observing the ratings of the product,
customers observe their ex ante valuation for the product, and then decide whether or not to purchase it. If they purchase, the true quality of the product, their ex ante valuation for the product
(which are heterogeneous), an ex post idiosyncratic preference term and the price of the product
determine their overall satisfaction. Finally, given the rating system of the platform, a customer
decides to leave a review as a function of her overall satisfaction. In most of our analysis, we took
the thresholds that determine which type of review (from the set of available reviews) a customer
will leave to be exogenous.
In our formal analysis, we distinguished learning dynamics under two classes of rating systems: full history, where customers see the full history of reviews, and summary statistics, where the
platform reports some summary statistics of past reviews. We argued that in both cases, learning
dynamics are complicated by a selection effect — the types of customers who purchase the good
and thus their overall satisfaction and reviews depend on the information that they have available at the time of their purchase. For example, only those who have a relatively high ex ante
valuation for the product would purchase it after seeing fairly negative reviews of its underlying
quality. But this implies that this selected group of purchasers are much more likely to leave positive reviews than a random customer. Unless corrected, this selection effect would imply that the
information of past customers about the quality of the product cannot be aggregated accurately.
Our formal analysis provides conditions for asymptotic learning under both full history and
summary statistics. These conditions encapsulate how customers deal with the selection problem under different rating systems. Under full history, beliefs about the underlying quality of the
product follow a martingale, meaning in particular that each customer can accurately estimate the
beliefs at the time that another past customer has made her purchase. As a result, it is feasible to
correct for the selection effect, which enables asymptotic learning to take place under relatively
mild conditions. Conditional on asymptotic learning, we characterized the speed (rate) of learning, which is crucial for the success of online rating systems — it is not practically relevant to
27

have a rating system that aggregates past information but does so extremely slowly. We show that
under full history, learning is exponentially fast, and its exact rate is given by the KL divergence
between the probability distribution of reviews under the true quality and its opposite. The emergence of the KL divergence as the measure of distance between these two probability distributions
is intuitive, because asymptotically, the problem of estimating the quality of the product resembles a binary hypothesis testing problem, though with sample observations that are correlated
with each other because of the selection effect.
We also show how the selection effect becomes more difficult to filter out with summary statistics, because now customers do not know what type of information past users had when they
made their decisions. Nevertheless, we provide conditions for asymptotic learning in this case,
and demonstrate that conditional on asymptotic learning, the rate of learning is still exponential
and its exact rate, though different than the full history case, is still given by a KL divergence term.
The last part of the paper studied how different properties of the rating system influence the
rate of learning. We showed that providing more information does not always lead to faster learning. In particular, despite its greater success in filtering out the selection effect, having access to
full history can lead to slower learning in cases where the selection effect, paradoxically, heightens the gap between the probability distributions of reviews that will arise under the true quality
and its opposite. Nevertheless, we also established several comparative statics that increase the
speed of learning from reasonable modifications of a rating system. First, allowing for strictly
finer ratings leads to faster learning, provided that this does not shift the thresholds of customers
(which would otherwise make the two information structures non-nested). Second, we showed
that moving from a rating system that provides averages of previous reviews to one that provides
fractions of users that have left different reviews always increases the rate of learning. Third, we
demonstrated that providing more information about the characteristics of reviewers (for example, supplementing existing summary statistics with summary statistics among users with similar
profiles to the current customer) also always increases the rate of learning. We also showed that
different rating systems can lead to very fast or very slow speeds of learning.
In all these cases, it is also worth noting that the platform’s incentives are often aligned with
increasing the speed of learning of customers. We demonstrated this by showing that, in our
baseline model, the revenues of the platform are higher under a rating system that leads to a
faster rate of learning.
We view our paper as a first step in a comprehensive theoretical analysis of learning from
online rating systems. Several simplifying assumptions enabled us to obtain sharp results, but
need to be relaxed in future work to gain a more holistic understanding of the performance of
such systems. We briefly discuss some of the important ones here.
First, our analysis focused on Bayesian learning. We believe that the intuition that comes from
Bayesian learning is often relevant even if agents are not fully Bayesian. Nevertheless, it is important to also study how the presence of (some) non-Bayesian agents modifies these conclusions.
The selection effect implies that non-Bayesians will not be able aggregate information from past

28

customers accurately. But in our baseline model, the presence of some non-Bayesians does not
change the ability of Bayesian agents to learn.
Second, and more fundamentally, throughout we took the thresholds agents use for mapping
from their overall satisfaction into reviews as fixed and exogenous. As commented before, making
these thresholds random (with independently distributed increments) has no effect on our analysis. But it would be more challenging to endogenize these thresholds, which would necessitate a
systematic theory of why customers leave reviews and how these motives change as the choices of
the platform, including the rating system, change. One possibility would be an expressive motive
(e.g., they may wish to complain if their overall satisfaction is low). With an expressive motive,
endogenizing these thresholds is not difficult, though it is unclear whether as the rating system
is modified, these expressive motives change as well. The alternative is a strategic motive, where
customers leave reviews in order to influence the behavior of future customers. This is an exciting
and challenging area for research, in part because with a strategic motive, the way in which customers leave reviews will change as the rating system is modified. This of course complicates the
comparisons of the speed of learning across different rating systems.
Third, one could allow for a richer preference structure. This could include moving away from
a setting in which there is a common quality. For example, a high-quality book about global warming is likely to yield high utility for many customers, but to may also lead to low satisfaction to
those who do not believe in man-made climate change). It could also move in the direction of imperfect information about satisfaction, which would make users still rely on the information they
have received from the rating system in choosing their reviews, thus introducing issues related to
herding as studied in the previous literature.
Finally, an exciting research area is to close the gap between the theoretical models of learning
from online reviews and the burgeoning empirical literature on behavior in online markets.

Appendix: Proofs
Proof of Theorem 1
At time t, the likelihood ratio is a random variable defined as
Z(· | lt ) ,

P [·|Q = 1, lt ]
,
P [·|Q = 0, lt ]

where Z(· | lt ) = Z(h | lt ) with probability P [h|Q = Q∗ , lt ]. We will use this random variable in
the proof of this theorem as well as the proof of Theorem 2.
Part (a): If θ̄ + E[ζ] − p ≥ 0 holds, then customers almost surely learn the true quality.
Without loss of generality, we assume Q∗ = 0 and then show qt → 0 almost surely. The proof
for Q∗ = 1 is similar and can be obtained by analyzing

P[Q=0|Ωt ]
P[Q=1|Ωt ] .

We first show that lt forms

a martingale and thus converges to a limiting random variable and then show that the limiting
random variable must be 0 with probability 1.
29

Note that the random variables Z(· | lt ) are all mean 1 (conditioned on the history). This is
because we have
E [Z(· | lt+1 ) | Z(· | l1 ), . . . , Z(· | lt )] =

X

P [h | Q = 0, lt ]

h

P [h | Q = 1, lt ] X
=
P [h | Q = 1, lt ] = 1.
P [h | Q = 0, lt ]
h

This guarantees that lt forms a martingale. Since lt ≥ 0, from the martingale convergence theorem
(Chapter 5, Durrett [2010]) we conclude that lt → l∞ almost surely. We next show that the limiting
random variable l∞ is 0 almost surely.
Given any history (or equivalently its sufficient statistic lt ), we have
P [h = K2 |Q = 1, lt ]
P [qt + θ + E[ζ] − p ≥ 0, θ + ζ + 1 − p ≥ λK2 ]
=
P [h = K2 |Q = 0, lt ]
P [qt + θ + E[ζ] − p ≥ 0, θ + ζ − p ≥ λK2 ]
P [qt + θ + E[ζ] − p ≥ 0, θ + ζ − p − λK2 ∈ [−1, 0)]
=1+
P [qt + θ + E[ζ] − p ≥ 0, θ + ζ − p ≥ λK2 ]
P [θ + E[ζ] − p ≥ 0, θ + ζ − p − λK2 ∈ [−1, 0)]
≥1+
,
P [1 + θ + E[ζ] − p ≥ 0, θ + ζ − p ≥ λK2 ]

Z(h = K2 | lt ) =

where the inequality follows by substituting qt = 0 in numerator and qt = 1 in the denominator.
Using condition θ̄ + E[ζ] − p ≥ 0 of part (a) and Assumption 1, we have
,

P [θ + E[ζ] − p ≥ 0, θ + ζ − p − λK2 ∈ [−1, 0)]
> 0.
P [1 + θ + E[ζ] − p ≥ 0, θ + ζ − p ≥ λK2 ]

Therefore, irrespective of the belief (or corresponding likelihood ratio l), whenever h = K2 , the
random variable Z(· | l) is strictly larger than 1, i.e.,
min Z(h = K2 | l) − 1 ≥  > 0.
l

(14)

Again, using Assumption 1 and θ̄ + E[ζ] − p ≥ 0, for any belief probability of h = K2 is positive,
i.e.,
η , min P [q + θ + E[ζ] − p ≥ 0, θ + ζ − p ≥ λK2 ] > 0.
q∈[0,1]

With these definitions for  and η, for all t and lt , we have
P [|Z(· | lt ) − 1| ≥  | lt ] = P [h = K2 | lt ] ≥ η.
We next show that l∞ = 0 with probability 1. Using Eq. (14), for an arbitrary δ > 0, we can write
P [|lt+1 − lt | ≥ δ] = E [1{ |lt (Z(· | lt ) − 1) | ≥ δ}] ≥ E [1{lt ≥ δ}1{ |Z(· | lt ) − 1 | ≥ }]
= Elt [1{lt ≥ δ}E [1{ |Z(· | lt ) − 1 | ≥ } | lt ]]
= Elt [1{lt ≥ δ}P [ |Z(· | lt ) − 1 | ≥  | lt ]] ≥ ηElt [1{lt ≥ δ}] = ηP [lt > δ] .

(15)

Since lt → l∞ almost surely, we have P [|lt+1 − lt | ≥ δ] → 0 which along with Eq. (15) leads to
30

P [lt > δ] → 0. This shows that lt → 0 in probability, which together with lt → l∞ almost surely,
establishes that P[l∞ = 0] = 1.18
Finally, note that since lt =

qt
1−qt ,

from lt → 0, we have qt → 0 almost surely.

Part (b): If θ̄ + E[ζ] − p < 0, then with positive probability customers do not learn the true quality.
We break the proof into two steps. In the first step we show that with a positive probability in
finite time qt becomes very small. In the second step we show that once this happens no purchase
takes place and learning stops.
Step 1: There exists a finite t such that with positive probability we have qt < min

n

p−θ̄−E[ζ]
,1
2

o
.

First note that from Assumption 1 and using a similar argument to that of Eq. (14), we have
P[θ + E[ζ] + q − p ≥ 0, θ + ζ + 1 − p ≤ λ−K1 ]
< 1.
P[θ + E[ζ] + q − p ≥ 0, θ + ζ − p ≤ λ−K1 ]
q∈[0,1]
max

Therefore, we obtain




lim 

t→∞

q≥min

nmax

p−θ̄−E[ζ]
,1
2

o


 t
P [θ + E[ζ] + q − p ≥ 0, θ + ζ + 1 − p ≤ λ−K1 ] 
= 0.
P [θ + E[ζ] + q − p ≥ 0, θ + ζ − p ≤ λ−K1 ]

We let T0 be the smallest number such that for all t ≥ T0 , we have




o
n max
p−θ̄−E[ζ]
,1
min
2

o
n

p−θ̄−E[ζ]
 t
,
1
min
2
P [θ + E[ζ] + q − p ≥ 0, θ + ζ + 1 − p ≤ λ−K1 ] 
n
o.
<
p−θ̄−E[ζ]
P [θ + E[ζ] + q − p ≥ 0, θ + ζ − p ≤ λ−K1 ]
1 − min
,
1
2

We also let
T0


ρ,
q≥min

nmin

p−θ̄−E[ζ]
,1
2

o (P [θ

+ E[ζ] + q − p ≥ 0, θ + ζ − p ≤ λ−K1 ])

> 0.

We next show that with probability at least ρ there exists a time t ∈ [1, T0 ] such that

qt ≤ min


p − θ̄ − E[ζ]
,1 .
2

n
o
We let E be the event that in the interval [1, T0 ], the belief goes below min p−θ̄−E[ζ]
,
1
and E1 be
n
o2
the event that in the interval [1, T0 − 1], the belief goes below min p−θ̄−E[ζ]
, 1 . We can write
2
P [E] = P [E | E1 ] P [E1 ] + P [E | E1c ] P [E1c ] = P [E1 ] + P [E1c ] ×






p − θ̄ − E[ζ]
p − θ̄ − E[ζ]
P qT0 ≤ min
, 1 qt ≥ min
, 1 , 1 ≤ t ≤ T0 − 1 ≥ P [E1 ] + ρP [E1c ] ≥ ρ.
2
2
18

To see this, first note that from lt → l∞ almost surely, we have lt → l∞ in probability and the result follows by
noting that for a sequence of random variables {Xn }, if Xn → X and Xn → Y in probability, then X = Y almost
surely.

31

Therefore, with probability ρ > 0, there exists t∗ ∈ [1, T0 ] such that qt∗ < min

n

p−θ̄−E[ζ]
,1
2

o
.

Step 2: For all t ≥ t∗ , we have qt = qt∗ and the limiting belief becomes qt∗ 6= Q∗ .
We show this claim by induction on t. It evidently holds for t = t∗ . Since

qt∗ − p + θ̄ + E[ζ] ≤ min


p − θ̄ − E[ζ]
p − θ̄ − E[ζ]
, 1 − p + θ̄ + E[ζ] = −
< 0,
2
2

purchase does not happen at time t∗ . Now that purchase does not happen at time t∗ , the belief at
time t∗ + 1 is the same as qt∗ because
P [h = N | Q = 1, lt∗ ]
P [θ + E[ζ] + qt∗ − p < 0]
=
= 1.
P [h = N | Q = 0, lt∗ ]
P [θ + E[ζ] + qt∗ − p < 0]
Therefore, no purchase occurs at time t∗ + 1. By repeating this argument no purchase occurs for
any t ≥ t∗ and qt = qt∗ . Finally, note that qt∗ is away from Q∗ = 0 (and similarly from Q∗ = 1)
because using Assumption 1 if purchase occurs, then probability of any review is non-zero. Hence,
the likelihood ratio at each time is multiplied by a non-zero number (bounded number) and hence
cannot become 0 in finite time.

Proof of Proposition 1
We show the first part of proposition as the proof of the second part follows from an identical
argument. We first show that Eq. (5) leads to




Eh∼P[·|q=0,Q=0] log

P [h | q = q̄, Q = 1]
P [h | q = q̄, Q = 0]


> 0.

(16)

This is because using inequality log x ≤ x − 1 yields


 


P [h | q = q̄, Q = 1]
P [h | q = q̄, Q = 0]
Eh∼P[·|q=0,Q=0] log
= Eh∼P[·|q=0,Q=0] − log
P [h | q = q̄, Q = 0]
P [h | q = q̄, Q = 1]


P [h | q = q̄, Q = 0]
≥ Eh∼P[·|q=0,Q=0] 1 −
> 0,
P [h | q = q̄, Q = 1]
where we used Eq. (5) in the last inequality. Using Eq. (16) and the fact that probability of actions
are continuous (Assumption 1), we can find  > 0 such that for all qh ≤ , h ∈ {N }∪{−K1 , . . . , K2 },


X

P [h | qh , Q = 0] log

h∈{−K1 ,...,K2 }∪{N }

P [h | q = q̄, Q = 1]
P [h | q = q̄, Q = 0]


> 0.

If qt → 0 almost surely, then there exists N such that for all t ≥ N we have qt ≤ . Since
qt
qN
=
1 − qt
1 − qN

t
Y
P [hs | q = q̄, Q = 1]
,
P [hs | q = q̄, Q = 0]

s=N +1

32

(17)

using strong law of large numbers, we obtain
1
lim
t→∞ t
≥

Pts=N 1{hs =h} !
P [h | q = q̄, Q = 1]
log
P [h | q = q̄, Q = 0]
h∈{−K1 ,...,K2 }∪{N }




X
P [h | q = q̄, Q = 1]
min P [h | q, Q = 0] log
.
q≤
P [h | q = q̄, Q = 0]


X

h∈{−K1 ,...,K2 }∪{N }

Therefore, taking limit of both sides of Eq. (17) as t → ∞ we obtain
1
lim log
t→∞ t



qt
1 − qt



X

≥



h∈{−K1 ,...,K2 }∪{N }




P [h | q = q̄, Q = 1]
min P [h | q, Q = 0] log
> 0.
q≤
P [h | q = q̄, Q = 0]

This is a contradiction because the left hand side is negative (qt → 0) and the right hand side is
positive.

Proof of Theorem 2
We prove the theorem for Q∗ = 0 as the proof for Q∗ = 1 can be obtained by a similar argument.
We re-index the set of actions and denote it by A = {1, . . . , |A|}. Based on the sequence lt , we
define a coupled new sequence ¯lt that is larger than lt and updates with iid increments. Since
qt → 0 almost surely (equivalently lt → 0 almost surely), for any  we can choose N such that
lt ≤  for t ≥ N . Using Assumption 1, for any a ∈ A, letting a = arg maxl∈[0,] Z(a | l) leads to
Z(a | lt ) ≤ Z(a | l = a ),

∀a ∈ A, t ≥ N.

(18)

Without loss of generality we suppose

0 ≤ Z(a = 1 | l = 1 ) ≤ · · · ≤ Z a = |A| | l = |A| .

(19)

Using Assumption 1 once again, we can choose p1 , . . . , p|A| such that

max |pa − P [a|l = 0, Q = 0] | ≤ ,
a∈A

X

pa log (Z(a | l = a )) < 0,

(20)

a∈A

and
m
X

P [a|lt , Q = 0] ≥

a=1

m
X

pa ,

a=1

We next define the sequence ¯lt for t ≥ N .
• For t = N : we let ¯lN = lN .

33

∀m ∈ A,

for t ≥ N.

(21)

• For t > N : we will define {¯lt } which couples with {lt } through a sequence of uniform iid
random variables {σt }. Recall that lt+1 = lt Z(a | lt ) with probability P [a | lt , Q = 0]. We let
lt+1 = lt Z(a | lt ),

a−1
X

if σt ∈

¯lt+1 = ¯lt Z(ā | l = a ),

i=1
ā−1
X

if σt ∈

P [i | lt , Q = 0] ,

a
X

#
P [i | lt , Q = 0]

i=1

pi ,

i=1

ā
X

#
pi .

i=1

Note that by definition, the sequence {lt } defined above evolves as the likelihood ratio of the
public belief. This is because we have
"
Pσt σt ∈

a−1
X

P [i|lt , Q] ,

i=1

a
X

##
P [i|lt , Q]

= P [a|lt , Q] .

i=1

Using strong law of large numbers (Durrett [2010, Theorem 2.3.5]) because {σt } is an iid sequence, we almost surely have
(
|A|
t−1 X
X
1
1
1 σs ∈
lim log ¯lt = lim
t→∞ t
t→∞ t
s=1 a=1
X
=
pa log (Z (a | a )) .

a−1
X

pi ,

i=1

a
X

#)
pi

(log (Z (a | l = a )))

i=1

a∈A

P
Using Eq. (20), this establishes that ¯lt converges linearly to zero with learning speed − a∈A pa log Z(a |
l = a ). We next show that ¯lt is larger thanlt . We show this by induction on t. It evidently
holds for
i
Pa
Pa−1
t = N . Suppose at time t, we have σt ∈
i=1 P [i | lt , Q = 0] , i.e., at = a.
i=1 P [i | lt , Q = 0] ,
Using Eq. (21), this leads to āt ≥ a. Therefore, we have
¯lt+1 = Z(āt | l = ā )¯lt ≥ Z(āt | l = ā )lt
t
t

By induction hypothesis

≥ Z(a | l = a )lt

By āt ≥ a and Eq. (19)

≥ Z(a | lt )lt

By Eq. (18)

= lt+1 .

By definition of {lt }.

This shows that almost surely
X
1
log lt ≤
pa log (Z(a | l = a )) .
t→∞ t
lim

a∈A

Since this inequality holds for all , letting  → 0 (and consequently a → 0 and pa → P [a|l = 0, Q = 0]

34

for all a ∈ A) leads to
X
X
1
log lt ≤ lim
pa log (Z(a | l = a )) =
P [a | l = 0, Q = 0] log (Z(a | l = 0))
t→∞ t
→0
lim

a∈A

a∈A

= −D (P [·|q = 0, Q = 0] ||P [·|q = 0, Q = 1]) .

(22)

Similarly, by reversing the inequalities in Eq. (18) and Eq. (21), we can construct another sequence
lt with independent increments which is smaller than lt , leading to
1
log lt ≥ −D (P [·|l = 0, Q = 0] ||P [·|l = 0, Q = 1]) .
t→∞ t
lim

(23)

Putting Eq. (22) and Eq. (23) together leads to
lim

t→∞

1
log lt = −D (P [·|l = 0, Q = 0] ||P [·|l = 0, Q = 1]) .
t

Finally, we have limt→∞ 1t log qt = limt→∞ 1t log lt , which completes the proof.

Proof of Proposition 2
We present the proof for Q∗ = 0 (the proof for Q∗ = 1 follows from the same argument). Recall
that τ is the sequence of time at which there was an action in T and H̃τ = {h̃1 , . . . , h̃τ −1 } denotes
the available history after τ − 1 actions in T . Similar to the proof of Theorem 1, we define random
P[·|h̃∈T,Q=1,lτ ]
variable Z̃(· | lτ ) , P ·|h̃∈T,Q=0,l . We first show that lτ +1 = lτ Z̃(· | lτ ), and then we establish lτ
[
τ]
is a martingale. Note that between τ -th action in T and τ + 1-th action in T , each customer has
observed the same history, and therefore has the same belief hqτ . Hence, each
i has taken an action h̃
that does not belong to T , independently with probability P h̃ 6∈ T | qτ , Q . Moreover, customers
do not know how many others have joined the platform before them, and instead, by assumption,
have a uniform prior over this number. We let Hτ :τ +1 denote the history of actions in between
τ -th and τ + 1-th actions in T . We next present a recursive characterization of the likelihood ratio
corresponding to belief {qτ }∞
τ =0 , exploiting the uniform (improper) prior assumption. We have
lτ +1

P
P[H̃τ , Hτ :τ +1 , h̃τ |Q = 1]
P[H̃τ , h̃τ |Q = 1]
H
=
= P τ :τ +1
P[H̃τ , h̃τ |Q = 0]
Hτ :τ +1 P[H̃τ , Hτ :τ +1 , h̃τ |Q = 0]
P
P[H̃τ | Q = 1]P[Hτ :τ +1 |H̃τ , Q = 1]P[h̃τ |H̃τ , Hτ :τ +1 , Q = 1]
H
= P τ :τ +1
Hτ :τ +1 P[H̃τ | Q = 0]P[Hτ :τ +1 |H̃τ , Q = 0]P[h̃τ |H̃τ , Hτ :τ +1 , Q = 0]
P
P[Hτ :τ +1 |H̃τ , Q = 1]P[h̃τ |H̃τ , Hτ :τ +1 , Q = 1]
H
= lτ × P τ :τ +1
.
Hτ :τ +1 P[Hτ :τ +1 |H̃τ , Q = 0]P[h̃τ |H̃τ , Hτ :τ +1 , Q = 0]

(24)

We next consider P[Hτ :τ +1 |H̃τ , Q]. The randomness in Hτ :τ +1 comes from two sources: (i) the
number of customers who joined the platform between τ -th and τ + 1-th actions in T , and (ii) the

35

actions they took, which depend on their θ and ζ. We let U denote the first random variable. By
assumption, this random variable is a uniform (improper) over all integers. We also note that all
customers in this time interval have observed the same history (i.e., H̃τ ), formed the same belief
(i.e., qτ ), and took an action independently with probability P[h 6∈ T | qτ , Q]. Therefore, we have
P[Hτ :τ +1 |H̃τ , Q] =

∞
X

P[Hτ :τ +1 |U = u, H̃τ , Q] =

u=0

∞
X

P[h 6∈ T | H̃τ , Q]u .

(25)

u=0

Invoking Eq. (25) in Eq. (24) leads to
h
i
u
P
[h
∈
6
T
|
q
,
Q
=
1]
P
h̃
|
q
,
Q
=
1
τ
τ
τ
u=0
h
i = lτ
= lτ P
∞
u
P
[h
∈
6
T
|
q
,
Q
=
0]
P
h̃
|
q
,
Q
=
0
τ
τ
τ
u=0
h
i


P h̃τ | h̃τ ∈ T, qτ , Q = 1
i = lτ Z̃ h̃τ | lτ .
= lτ h
P h̃τ | h̃τ ∈ T, qτ , Q = 0
P∞

lτ +1

P[h̃τ |qτ ,Q=1]
1−P[h6∈T |qτ ,Q=1]
P[h̃τ |qτ ,Q=0]
1−P[h6∈T |qτ ,Q=0]

Also note that, given Q∗ = 0, Z̃(· | lτ ) has mean 1, which implies that {lτ }∞
τ =1 forms a nonnegative
martingale. Then, from the martingale convergence theorem, we can conclude that lτ → l∞ almost
surely. The rest of the proof is identical to that of Theorem 1 and Theorem 2.

Proof of Theorem 3
A summary statistic Sτ is a vector (i1 /τ, . . . , im /τ ) such that

Pm

j=1 ij

= τ and ij ≥ 0, j ∈ [m]. For

any summary statistics, which equivalently can be expressed as (i1 , . . . , im ) at time τ , we let
p1 (i1 , . . . , im , τ ) , P [Sτ = (i1 , . . . , im ) | Q = 1] ,

p0 (i1 , . . . , im , τ ) , P [Sτ = (i1 , . . . , im ) | Q = 0] .

We first show a recursive relation that determines the evolution of the values of p1 (i1 , . . . , im , τ )
and p0 (i1 , . . . , im , τ ). Note that crucially, the evolution of sequences {p1 (·, τ )} and {p0 (·, τ )} are
not independent of each other as both of them depend on q (·, τ ) which in turn depends on both
of the sequences. The next lemma shows that despite this dependence and the potential updating
that Bayesian individuals may engage in view of the fact that there may have been customers who
have taken actions that do not belong to T , these probabilities satisfy an intuitive recursion under
our assumption of uniform priors over the number of past people who joined the platform.

36

Lemma 1. The sequences {p1 (i1 , . . . , im , τ )} and {p0 (i1 , . . . , im , τ )} satisfy
p1 (i1 , . . . , im , τ )
=

m
X

p1 (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im , τ − 1)G1 (j, q (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im , τ − 1)) ,

j=1

p0 (i1 , . . . , im , τ )
=

m
X

p0 (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im , τ − 1)G0 (j, q (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im , τ − 1)) ,

j=1

q(i1 , . . . , im , τ ) =

p1 (i1 , . . . , im , τ )
.
p1 (i1 , . . . , im , t) + p0 (i1 , . . . , im , τ )

(26)

Proof. For τ = 1, the belief without any observation is 1/2. Therefore, we have


1
p1 (ei , 1) = G1 i,
,
2



1
p0 (ei , 1) = G0 i,
,
2

i ∈ [m].

Similar to the proof of Proposition 2, we let random variable Hτ −1:τ denote the history of actions
in between τ − 1-th and τ -th actions in T . Given customers have uniform prior on the number
of people who joined the platform denoted by U , and the fact that all customers in this interval
observe the same history and hence form the same belief, we can write
P[Hτ −1:τ |Sτ −1 , Q] =

∞
X

P[Hτ −1:τ |U = u, Sτ −1 , Q] =

u=0

∞
X

P[h 6∈ T | Sτ −1 , Q]u .

(27)

u=0

Using Eq. (27), we next show the update rule for τ ≥ 2. We have
p1 (i1 , . . . , im , τ ) = P [Sτ (1) = i1 , . . . , Sτ (m) = im | Q = 1]
=

m
X
X

P [Sτ −1 = (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im ) | Q = 1] P [Hτ −1:τ | Q = 1, Sτ −1 = (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im )]

j=1 Hτ −1:τ

× P [hτ ∈ Tj | Hτ −1:τ , Q = 1, Sτ −1 = (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im )]
=

m
X

P [Sτ −1 = (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im ) | Q = 1]

j=1

×
=

∞
X
u=0
m
X

P [h 6∈ T | q (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im ) , Q = 1]u P [h ∈ Tj | q (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im ) , Q = 1]
P [Sτ −1 = (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im ) | Q = 1] ×

j=1

=

m
X

P [h ∈ Tj | q (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im ) , Q = 1]
1 − P [h 6∈ T | q (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im ) , Q = 1]

P [Sτ −1 = (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im ) | Q = 1] × P [h ∈ Tj | h ∈ T, q(i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im ), Q = 1]

j=1

=

m
X

p1 (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im , τ − 1) × G1 (j, q (i1 , . . . , ij−1 , ij − 1, ij+1 , . . . , im , τ − 1)) .

j=1

We can write a similar recursion for p0 (i1 , . . . , im , τ ). The proof concludes by using Bayes’ rule to

37

find q(i1 , . . . , im , τ ) as in Eq. (26).
We break the rest of the proof into three steps. Using Assumption 3, there exists i ∈ [m] such
that the range of G1 (i, ·) and G0 (i, ·) are separated. Without loss of generality, we suppose the
range of G1 (i, ·) is above the range of G0 (i, ·), i.e., G1 = minq G1 (i, q) > maxq G0 (i, q) = G0 .
Step 1: The summary statistic i with Q = 1 majorizes a binomial random variable and the summary statistic i with Q = 0 is majorized by a binomial random variable. Formally, we have:19
Sτ (i)  Binomial(G1 , τ ),

Q = 1,

Sτ (i)  Binomial(G0 , τ ),

Q = 0.

Equivalently, we have
X

S1 (l, τ ) , P [Sτ (i) ≤ l | Q = 1] =

l
X

kj ,j∈[m]\{i} ki =1

X

S0 (l, τ ) , P [Sτ (i) ≤ l | Q = 0] =

l
X

kj ,j∈[m]\{i} ki =1

l  
X
τ
p1 (k1 , . . . , km , τ ) ≤
Gj1 (1 − G1 )τ −j ,
j
j=0

l  
X
τ
p0 (k1 , . . . , km , τ ) ≥
Gj0 (1 − G0 )τ −j . (28)
j
j=0

Proof of Step 1: we prove the claim for Q = 1 as the proof for Q = 0 is identical. The proof idea is
as follows. Given Q = 1, consider the random process Sτ (i). Lemma 1 shows that the distribution
of Sτ (i) satisfies a recursive relation. We consider another random process, whose distribution
satisfies a similar recursion with the difference that G1 (i, qτ ) is replaced by its minimum over
all q, i.e., G1 . We show that the distribution of this process at τ is the same as the distribution
of Binomial(G1 , τ ). Moreover, because this process is recursively defined using the minimum of
G1 (i, q) over q, it follows that Sτ with Q = 1 majorizes Binomial(G1 , τ ). We next show the formal
proof. First, note that from definition in Eq. (28) we have
S1 (l, τ ) − S1 (l, τ − 1) =

X

l
X

p1 (k1 , . . . , km , τ ) −

kj ,j∈[m]\{i} ki =1

X

=

l X
m
X

X

l
X

p1 (k1 , . . . , km , τ − 1)

kj ,j∈[m]\{i} ki =1

p1 (k1 , . . . , kj−1 , kj − 1, kj+1 , . . . , km , τ − 1)G1 (j, q (k1 , . . . , kj−1 , kj − 1, kj+1 , . . . , km , τ − 1))

kj ,j∈[m]\{i} ki =1 j=1

X

−

l
X

p1 (k1 , . . . , km , τ − 1)

kj ,j∈[m]\{i} ki =1

X

=

l
X

p1 (k1 , . . . , km , τ − 1)

X

G1 (j, q (k1 , . . . , km , τ − 1))

j=1

kj ,j∈[m]\{i} ki =1

−

m
X

p1 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ − 1)G1 (i, q(k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ − 1))

kj ,j∈[m]\{i}

X

−

l
X

p1 (k1 , . . . , km , τ − 1)

kj ,j∈[m]\{i} ki =1

=−

X

p1 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ − 1)G1 (i, q(k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ − 1)),

(29)

kj ,j∈[m]\{i}
19
We say random variable X majorizes random variable Y and write X  Y if X first-order stochastically dominates
Y , i.e., for any a we have P[X ≥ a] ≥ P[Y ≥ a].

38

where the second equality follows from Lemma 1 and the last equality follows from

Pm

i=1 G1 (i, q)

=

1. We define the sequence {S̃1 (l, τ )}l,τ recursively as
S̃1 (l, τ ) = S̃1 (l − 1, τ − 1)G1 + S̃1 (l, τ − 1)(1 − G1 ),

0 ≤ l ≤ τ , τ ≥ 0.

(30)

We will next show by induction on τ that S̃1 (l, τ ) ≥ S1 (l, τ ). For τ = 1, we have


1
S̃1 (0, 1) = 1 − G1 ≥ 1 − G1 i,
= S1 (0, 1),
2

S̃1 (1, 1) = 1 = S1 (1, 1).

We now suppose that the induction hypothesis holds for τ − 1 and prove it for τ . We have
S̃1 (l, τ ) = S̃1 (l − 1, τ − 1)G1 + S̃1 (l, τ − 1)(1 − G1 ) ≥ S1 (l − 1, τ − 1)G1 + S1 (l, τ − 1)(1 − G1 )
X
≥ S1 (l, τ − 1) − G1
p1 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ − 1)
kj ,j∈[m]\{i}

X

≥ S1 (l, τ − 1) −

p1 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ − 1)G1 (i, q(k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ − 1))

kj ,j∈[m]\{i}

= S1 (l, τ ),

(31)

where the first inequality follows from induction hypothesis, the second inequality follows from
definition in Eq. (28), the third inequality follows from using G1 = minq G1 (i, q), and the last
equality follows from using Eq. (29). We next show by induction on τ that the recursive definition
of {S̃1 (l, τ )} given in Eq. (30) leads to
S̃1 (l, τ ) =

l  
X
τ
Gj1 (1 − G1 )τ −j .
j
j=0

This evidently holds for τ = 1 as we have S̃1 (0, 1) = 1 − G1 and S̃1 (1, 1) = 1. We now suppose it
holds for τ − 1 and prove it for τ . We have
S̃1 (l, τ ) = S̃1 (l − 1, τ − 1)G1 + S̃1 (l, τ − 1)(1 − G1 )


l−1 
l 
X
X
τ −1
τ −1
j
τ −1−j
= G1
G1 (1 − G1 )
+ (1 − G1 )
Gj1 (1 − G1 )τ −1−j
j
j
j=0

=

l
X
j=0

j=0


 
 X
 
l
τ −1
τ −1
j
j
τ −j
τ −j τ
G1 (1 − G1 )
+
=
G1 (1 − G1 )
.
j
j−1
j

(32)

j=0

Putting Eq. (31) and Eq. (32) together, completes the proof of Step 1.
Step 2: Letting ∆ = G1 − G0 , for any  > 0, we have
1
2
P[qτ >  | Q = 0] ≤ e−τ ∆ /2 ,


∀τ ≥ 1, and

1
2
P[1 − qτ >  | Q = 1] ≤ e−τ ∆ /2 ,


39

∀τ ≥ 1. (33)

Proof of Step 2: Using Step 1, for γ =
S1 (γτ , τ ) ≤

γτ  
X
τ

j

j=0

Gj1 (1

G1 +G0
,
2

τ −j

− G1 )

we have G1 −

∆
2

= G0 +

∆
2

= γ. This leads to



 
∆
2
= P Binomial(G1 , τ ) ≤ G1 −
τ ≤ e−τ ∆ /2 ,
2

where we used Chernoff-Hoeffding’s inequality in the last inequality. We also have
S0 (γτ , τ ) ≥

γτ  
X
τ
j=0

j

Gj0 (1

− G0 )



 
∆
2
= P Binomial(G0 , τ ) ≤ G0 +
τ ≥ 1 − e−τ ∆ /2 .
2

τ −j

We show the claim for Q∗ = 0 as the proof for Q∗ = 1 is identical. We have
P[qτ >  | Q = 0] =

X

p0 (k1 , . . . , km , τ )1{q(k1 , . . . , km , τ ) > }

kj ,j∈[m]

=

τγ
X

X


p0 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )1

l=0 kj ,j∈[m]\{i}

+

τ
X



X

p0 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )1

l=τ γ kj ,j∈[m]\{i}

≤

τγ
X

X


p0 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )1

l=0 kj ,j∈[m]\{i}

+ (1 − S0 (γτ , τ ))
≤

τγ
X

X


p0 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )1

l=0 kj ,j∈[m]\{i}

+ e−τ ∆
≤

τγ
X

X




q(k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )
>
1 − q(k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )
1−

q(k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )

>
1 − q(k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )
1−





Using 1{·} ≤ 1

p1 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )

>
p0 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )
1−

2 /2

l=0 kj ,j∈[m]\{i}

≤

q(k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )

>
1 − q(k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ )
1−

Using Eq. (33)
1−
2
p1 (k1 , . . . , ki−1 , l, ki+1 , . . . , km , τ ) + e−τ ∆ /2


1 −  −τ ∆2 /2
1
2
2
e
+ e−τ ∆ /2 = e−τ ∆ /2 .



Using Eq. (33)

This completes the proof of Step 2.
Step 3: We have qτ → Q∗ almost surely.
Proof of Step 3: We show the proof for Q∗ = 0 as the proof for Q∗ = 1 is identical. The idea is to use
Borell-Cantelli Lemma together with the exponential tail bound obtained in Step 2 (see Etemadi
[1981] and Korchevsky and Petrov [2010] for similar arguments). For any m, τ ∈ N, we let
(1)
Aτ m


,

1
qτ ≥
m

40



Using Step 2, for any m ∈ N we have
 1 
∞
∞
X
X
2
( )
e−τ ∆ /2 < ∞.
P Aτ m ≤ m
τ =1

τ =1

Therefore, using Borell-Cantelli lemma leads to


h
i
\ [ (1/m)
 = 0.
P i.o. A(1/m)
= P
Ak
τ
τ ∈N k≥τ

Finally, we can write

P



∞
C 
[ \ [ (1/m)
X


lim qτ = 0
=P
Ak
≤
P[i.o. A(1/m)
] = 0,
τ

τ →∞

m∈N τ ∈N k≥τ

m=1

where we used union bound for the inequality. This completes the proof.

Proof of the Claim in Example 1
We show that asymptotic learning does not occur in the example. If there were asymptotic learning, then given Q = 0,

Sτ (1)
τ

would be concentrated around G0 (1, q = 0) and given Q = 1,

Sτ (1)
τ

would be concentrated around G1 (1, q = 1). However, as we will show next, (Sτ (1) | Q = 1) majorizes (Sτ (1) | Q = 0), establishing that G1 (1, q = 1) ≥ G0 (1, q = 0) which is contradiction. We
will next establish this majorization relationship. Recall that
S1 (l, τ ) , P [Sτ (1) ≤ l | Q = 1] ,

S0 (l, τ ) , P [Sτ (1) ≤ l | Q = 0] .

We will show by induction on τ that for all l, we have S1 (l, τ ) ≤ S0 (l, τ ). This holds for τ = 1, as
S1 (0, 1) = 1 − G1 (1, q = 21 ) ≤ 1 − G0 (1, q = 12 ) = S0 (0, 1) and S1 (1, 1) = S0 (1, 1) = 1. We next show
that if this claim holds for τ − 1, then it holds for τ as well. We can write
S1 (l, τ ) = S1 (l, τ − 1) − G1 (q (l, τ − 1 − l, τ − 1)) p1 (l, τ − 1 − l, τ − 1)
= S1 (l, τ − 1) (1 − G1 (q (l, τ − 1 − l, τ − 1))) + G1 (q (l, τ − 1 − l, τ − 1)) S1 (l − 1, τ − 1)
≤ S0 (l, τ − 1) (1 − G1 (q (l, τ − 1 − l, τ − 1))) + G1 (q (l, τ − 1 − l, τ − 1)) S0 (l − 1, τ − 1)
= S0 (l, τ − 1) − G1 (q (l, τ − 1 − l, τ − 1)) p0 (l, τ − 1 − l, τ − 1)
≤ S0 (l, τ − 1) − G0 (q (l, τ − 1 − l, τ − 1)) p0 (l, τ − 1 − l, τ − 1) = S0 (l, τ ),
where the first inequality follows from induction hypothesis and the second inequality follows
from G1 (q) ≥ G0 (q) for all q ∈ [0, 1].

41

Proof of Theorem 4
We show the theorem for Q∗ = 0 as the proof for Q∗ = 1 is identical. First note that using the
assumptions we can invoke Theorem 3 to show qτ → Q∗ almost surely.
Given Q = 0, since qτ → 0 almost surely, we have that

Sτ (i)
τ

→ G0 (i, 0) almost surely for all i ∈ [m].

Also, given Q = 1, since qτ → 1 almost surely, we have that

Sτ (i)
τ

→ G1 (i, 1) almost surely for all

i ∈ [m]. Therefore, for any  there exists large enough N such that for τ ≥ N we have
qτ ≤ ,

Sτ (i)
− G0 (i, 0) ≤ , ∀i ∈ [m], Q∗ = 0
τ
Sτ (i)
Q∗ = 1,
− G1 (i, 1) ≤ , ∀i ∈ [m], Q∗ = 1.
τ

Q∗ = 0

qτ ≥ 1 − ,
We let

G0 (i, 0) = min P [h ∈ Ti | h ∈ T, q, Q = 0] ,

Ḡ1 (i, 1) = max P [h ∈ Ti | h ∈ T, q, Q = 1] .

q≤

q≥1−

With these definitions, suppose that at time τ ≥ N we have Sτ (i) = Nτ (i) for i ∈ [m]. We let Hτ
be the set of possible histories that are consistent with Nτ (1), . . . , Nτ (m). We obtain
P

P

qτ
Hτ ∈Hτ P[Hτ | Q = 1]
Hτ ∈Hτ P[HN | Q = 1]P[HN +1:τ | HN , Q = 1]
log
= log P
= log P
1 − qτ
Hτ ∈Hτ P[Hτ | Q = 0]
Hτ ∈Hτ P[HN | Q = 0]P[HN +1:τ | HN , Q = 0]
P


(maxHN P[HN | Q = 1]) Hτ ∈Hτ P[HN +1:τ | HN , Q = 1]
P
≤ log
(minHN P[HN | Q = 0]) Hτ ∈Hτ P[HN +1:τ | HN , Q = 0]

Ñτ (i) 
Qm
(maxHN P[HN | Q = 1]) |Hτ | i=1 Ḡ1 (i, 1)

≤ log 
Qm
N̂τ (i)
(minHN P[HN | Q = 0]) |Hτ | i=1 (G0 (i, 0))

Ñτ (i) 
Qm
(maxHN P[HN | Q = 1]) i=1 Ḡ1 (i, 1)
,
= log 
(34)
Q
N̂τ (i)
(G
(i,
0))
(minHN P[HN | Q = 0]) m
0
i=1
where Ñτ (i) is Nτ (i) minus the number of i’s in arg maxHN P[HN | Q = 1] and N̂τ (i) is Nτ (i)
minus the number of i’s in arg minHN P[HN | Q = 0]. Using Assumptions 1 and 2, we have
0 < Ḡ1 (i, 1) ≤ 1 and 0 < G0 (i, 0) ≤ 1, which leads to
Ñ (i)
Ḡ1 (i, 1) τ
(G0 (i, 0))N̂τ (i)


≤

Ḡ1 (i, 1)
G0 (i, 0)

Nτ (i) 

1
Ḡ1 (i, 1)G0 (i, 0)

N
,

i ∈ [m].

(35)

Putting Eq. (34) and Eq. (35) together, we obtain
qτ
log
≤ log
1 − qτ

m

maxHN P[HN | Q = 1] Y
minHN P[HN | Q = 0]



i=1

42

Ḡ1 (i, 1)
G0 (i, 0)

Nτ (i) 

1
Ḡ1 (i, 1)G0 (i, 0)

N !
.

(36)

Since Q∗ = 0, we almost surely have

Nτ (i)
τ

→ G0 (i, 0), which yields

m

X Nτ (i)
1
qτ
≤ lim
log
log
τ →∞
τ →∞ τ
1 − qτ
τ



lim

i=1

Ḡ1 (i, 1)
G0 (i, 0)


=

m
X


G0 (i, 0) log

i=1

Ḡ1 (i, 1)
G0 (i, 0)


.

Since this inequality holds for any , by letting  → 0 we have G0 (i, 0) → G0 (i, 0) and Ḡ1 (i, 1) →
G1 (i, 1), which leads to
m

X
1
qτ
≤
lim log
G0 (i, 0) log
τ →∞ τ
1 − qτ



i=1

G1 (i, 1)
G0 (i, 0)


=−

m
X


G0 (i, 0) log

i=1

G0 (i, 0)
G1 (i, 1)


.

(37)

Similarly, by letting
Ḡ0 (i, 0) = max P [h ∈ Ti | h ∈ T, q, Q = 0] ,

G1 (i, 1)

q≤

= min P [h ∈ Ti | h ∈ T, q, Q = 1] ,
q≥1−

we obtain
m

X Nτ (i)
1
qτ
log
log
≥ lim
τ →∞ τ
τ →∞
1 − qτ
τ



lim

i=1

G1 (i, 1)
Ḡ0 (i, 0)


=

m
X


G0 (i, 0) log

i=1

G1 (i, 1)
Ḡ0 (i, 0)


.

Again, by letting  → 0 we have Ḡ0 (i, 0) → G0 (i, 0) and G1 (i, 1) → G1 (i, 1), which yields
m

X
1
qτ
lim log
≥−
G0 (i, 0) log
τ →∞ τ
1 − qτ
i=1



G0 (i, 0)
G1 (i, 1)


.

(38)

Therefore, combining Eq. (37) and Eq. (38), we obtain
lim

τ →∞

1
qτ
log
= −D (P [· | h ∈ T, q = 0, Q = 0] ||P [· | h ∈ T, q = 1, Q = 1]) ,
τ
1 − qτ

which completes the proof.

Proof of Corollary 1
We show the corollary for Q∗ = 0 as the proof for Q∗ = 1 is identical. In this proof we let Like to
denote h = K2 and Review to denote h ∈ {K2 , −K1 }. We break the proof into two steps. We first
show negative selection, i.e.,
P [Like | Review, q = 1, Q = 0] < P [Like | Review, q = 0, Q = 0] ,
P [Like | Review, q = 1, Q = 1] < P [Like | Review, q = 0, Q = 1] .

43

We then compare the speed of the full history with summary statistics.
Step 1: We show G0 (1, q) defined as
G0 (1, q) = P [Like | Review, q, Q = 0] ,
is decreasing in q. An identical proof shows G1 (1, q) is decreasing in q as well. We let λ−1 and λ1
denote the thresholds for “like”, “no review”, and “dislike” . We have
P [θ + ζ − p ≥ λ1 , θ + E[ζ] + q − p ≥ 0]
P [θ + E[ζ] + q − p ≥ 0, θ + ζ − p 6∈ (λ−1 , λ1 )]
P [θ + ζ − p ≥ λ1 , θ + E[ζ] + q − p ≥ 0]
=
.
P [θ + ζ − p ≥ λ1 , θ + E[ζ] + q − p ≥ 0] + P [θ + ζ − p ≤ λ−1 , θ + E[ζ] + q − p ≥ 0]

G0 (1, q) =

To show G0 (1, q) is decreasing, it is sufficient to show
P [θ + ζ − p ≥ λ1 , θ + E[ζ] + q − p ≥ 0]
P [θ + ζ − p ≤ λ−1 , θ + E[ζ] + q − p ≥ 0]
is decreasing in q. This in turn is equal to
R∞

p−q−E[ζ] fθ (x) (1 − Fζ (p + λ1 − x)) dx
R∞
,
p−q−E[ζ] fθ (x) (Fζ (p + λ−1 − x)) dx

where we used the independence of θ and ζ. Taking derivative of this term with respect to q yields


R∞
(1
−
F
(λ
+
q
+
E
[ζ]))
1
ζ
p−q−E[ζ] fθ (x) (Fζ (p + λ−1 − x)) dx

fθ (p − q − E[ζ]) × 
2
R
∞
f
(x)
(F
(p
+
λ
−
x))
dx
−1
ζ
p−q−E[ζ] θ
R∞

−



(Fζ (λ−1 + q + E [ζ])) p−q−E[ζ] fθ (x) (1 − Fζ (p + λ1 − x)) dx 
,
R
2
∞
f
(x)
(F
(p
+
λ
−
x))
dx
−1
θ
ζ
p−q−E[ζ]

which is non-positive because
Z

∞

(1 − Fζ (λ1 + q + E [ζ]))

fθ (x) (Fζ (p + λ−1 − x)) dx
p−q−E[ζ]
Z ∞

− (Fζ (λ−1 + q + E [ζ]))

fθ (x) (1 − Fζ (p + λ1 − x)) dx
p−q−E[ζ]
Z ∞

≤ (1 − Fζ (λ1 + q + E [ζ]))
fθ (x) (Fζ (λ−1 + q + E[ζ])) dx
p−q−E[ζ]
Z ∞
− (Fζ (λ−1 + q + E [ζ]))
fθ (x) (1 − Fζ (λ1 + q + E[ζ])) dx = 0,
p−q−E[ζ]

where we used the fact that cumulative density function is non-decreasing for the inequality.
Step 2: We next compare the speed of learning with summary statistics and full history. We show
44

the proof for Q∗ = 0 as the proof for Q∗ = 1 is identical. We let
G00 = P [Like | Review, q = 0, Q = 0] ,

G01 = P [Like | Review, q = 0, Q = 1] ,

G11 = P [Like | Review, q = 1, Q = 1] .
Subtracting the speed of learning for full history (Theorem 2) from the speed of learning for summary statistics (Theorem 4), leads to

G00 log

G11
G01




+ G00 log

G11
G01


.

(39)

We next show given G00 ≤ G11 ≤ G01 , Eq. (39) is non-negative. Using − log(1 + x) ≥ −x, we have







G01
G11
1 − G11
1 − G01
G00 log
+ (1 − G00 ) log
≥ G00 1 −
+ (1 − G00 ) 1 −
G01
1 − G01
G11
1 − G11


G00 G01 (1 − G00 )(1 − G01 )
(G01 − G11 )(G11 − G00 )
+
≥ 0.
=1−
=
G11
1 − G11
G11 (1 − G11 )


Proof of Corollary 2
We will show the corollary for Q∗ = 0 as the proof for Q∗ = 1 is identical. We let λ−1 and λ1 denote
the thresholds for “like”, “no review”, and “dislike” . First note that P [Like | q, Q] is increasing in
both q and Q because we have
P [Like | q, Q] = P [θ + q + E[ζ] − p ≥ 0, θ + ζ − p + Q ≥ λ1 ] .
We next show that the speed of learning with summary statistics is faster than the speed of learning with full history. We let
G00 = P [Like|q = 0, Q = 0] , G01 = P [Like|q = 0, Q = 1] , G11 = P [Like|q = 1, Q = 1] .
Subtracting the speed of learning for full history (Theorem 2) from the speed of learning for summary statistics (Theorem 4), leads to

G00 log

G01
G11




+ (1 − G00 ) log

1 − G01
1 − G11


.

We next show that given G00 ≤ G01 ≤ G11 , this expression is non-negative. Using − log(1 + x) ≥
−x, we obtain









G01
1 − G01
G11
1 − G11
G00 log
+ (1 − G00 ) log
≥ G00 1 −
+ (1 − G00 ) 1 −
G11
1 − G11
G01
1 − G01


G00 G11 (1 − G00 )(1 − G11 )
(G11 − G01 )(G01 − G00 )
=1−
+
=
≥ 0.
G01
1 − G01
G01 (1 − G01 )

45

This completes the proof.

When Corollaries 1 and 2 Hold Simultaneously
Suppose Q∗ = 0. For uniform θ and ζ with ζ = −ζ̄ (and hence E[ζ] = 0) and θ = −θ̄, we have
Z
P [Like | q = 0, Q = 1] = P [θ + ζ + 1 − p ≥ λ1 , θ − p ≥ 0] =
p

1
(ζ̄ − ζ)(θ̄ − θ)

Z

1
=
(ζ̄ − ζ)(θ̄ − θ)



=

θ̄

1
P [ζ ≥ λ1 + p − 1 − x] dx
θ̄ − θ

θ̄


ζ̄ − λ1 − p + 1 + x dx

using ζ̄ ≥ max{λ1 + 1, θ̄ + 1 − p − λ1 }

p

θ̄ − p





1
1
ζ̄ − λ1 − p + 1 + θ̄
2
2



1
using θ̄ ≥ max{p + , 1 − p}.
2

(40)

Similarly, we have
1
P [Like | q = 1, Q = 0] =
(ζ̄ − ζ)(θ̄ − θ)





1
1 1
θ̄ − p + 1 ζ̄ − λ1 − p − + θ̄
.
2
2 2


(41)

Comparing Eq. (40) and Eq. (41), given θ̄ − p > ζ̄ − λ1 − 12 , we obtain
P [Like | q = 0, Q = 1] > P [Like | q = 1, Q = 0] ,
which guarantees separation assumption. We next show the assumptions of Corollary 2 hold.
Using θ̄ ≥ max{p, 1 − p} and ζ̄ ≥ max{λ1 + 1, θ̄ + 1 − p − λ1 }, we can write
P [θ + ζ + 1 − p ≥ λ1 , θ − p + 1 ≥ 0]
P [Like | Review, q = 1, Q = 1] =
P [θ − p + 1 ≥ 0, θ + ζ + 1 − p 6∈ (λ−1 , λ1 )]


1
1
p 1 1
.
=
ζ̄ − λ1 − + + θ̄ 
λ
2 2 2
ζ̄ − ζ
1 − 1 −λ−1

(42)

ζ̄−ζ

Similarly, we have
1
P [Like | Review, q = 0, Q = 0] =
ζ̄ − ζ



p 1
1
.
ζ̄ − λ1 − + θ̄ 
λ1 −λ−1
2 2
1−

(43)

ζ̄−ζ

Comparing Eq. (42) and Eq. (43), we obtain
P [Like | Review, q = 1, Q = 1] > P [Like | Review, q = 0, Q = 0] .
Therefore, the conditions of both Corollary 1 and Corollary 2 hold which concludes the proof.

46

Proof of Theorem 5
Full history: We show the proof for Q∗ = 0 as the proof of Q∗ = 1 is similar. Suppose the subset
Ti ⊆ T is partitioned into Til and Tih . Therefore, with a more refined rating system the speed of
learning changes by



P [h ∈ Til | q = 0, Q = 0]
P [h ∈ Til | q = 0, Q = 0] log
P [h ∈ Til | q = 0, Q = 1]


P [h ∈ Tih | q = 0, Q = 0]
+ P [h ∈ Tih | q = 0, Q = 0] log
P [h ∈ Tih | q = 0, Q = 1]


P [h ∈ Ti | q = 0, Q = 0]
− P [h ∈ Ti | q = 0, Q = 0] log
.
P [h ∈ Ti | q = 0, Q = 1]

(44)

Since P [h ∈ Ti | q = 0, Q] = P [h ∈ Til | q = 0, Q] + P [h ∈ Tih | q = 0, Q], we can use the following
inequality
x log

x
y
x+y
+ y log 0 ≥ (x + y) log 0
,
0
x
y
x + y0

∀x, x0 , y, y 0 ∈ (0, 1)

(45)

to conclude that Eq. (44) is non-negative. Finally, note that Eq. (45) holds because of Jensen’s
inequality. This completes the proof.
Summary statistics: The proof follows from Theorem 4 and a similar argument to full history.

Proof of Theorem 6
We break the proof into two parts. In the first part we show that learning occurs and in the second
part we find the speed of learning.
Part 1 (Learning): The proof is similar to that of Theorem 3. We break the proof into three steps.
First, we show a lemma to pin down the evolution of beliefs and probability of the summary
statistic given Q = 0 and Q = 1. Second, we show that the probabilities are separated. Third, we
show the exponential tail bound for the beliefs which in turn shows almost sure convergence.
Step 1: Letting
pQ (i, τ ) , P[Sτ = i | Q],

Q ∈ {0, 1},

the following lemma establishes a recursive relation which pinpoints the values of p1 (i, τ ) and
p0 (i, τ ).
Lemma 2. The sequences {p1 (i, τ )} and {p0 (i, τ )} satisfy
pQ (i, τ ) =

K
X

pQ (i − j, τ − 1)GQ (j, q (i − j, τ − 1)) ,

j=1

where q (i, τ ) =

p1 (i,τ )
p1 (i,τ )+p0 (i,τ ) .

Proof. The proof is similar to that of Lemma 1.
47

(46)

Step 2: Given the separation assumption, we can find numbers G1 (j) and G0 (j) for j ∈ [K] such
that

K
X

G1 (j) = min
q

j=i

K
X

G1 (j, q) > max
q

j=i

K
X

G0 (j, q) =

j=i

K
X

G0 (j),

2 ≤ i ≤ K.

j=i

We show that the probability of average summary statistics with Q = 1 majorizes a multinomial
random variable with alphabet 1, . . . , K, where the probability of letter i ∈ [K] is G1 (i). Similarly
the probability of summary statistics i with Q = 0 is majorized by a multinomial random variable.
Formally, we have
Sτ  Multinomial (1, . . . , K; G1 (1), . . . , G1 (K); τ ) ,

Q = 1,

Sτ  Multinomial (1, . . . , K; G0 (1), . . . , G0 (K); τ ) ,

Q = 0.

Equivalently, letting
S1 (l, τ ) , P [Sτ ≤ l | Q = 1] =

l
X

p1 (j, τ ),

S0 (l, τ ) , P [Sτ ≤ l | Q = 0] =

j=1

l
X

p0 (j, τ ),

j=1

we have
S1 (l, τ ) ≤ S̃1 (l, τ ) , P [Multinomial (1, . . . , K; G1 (1), . . . , G1 (K); τ ) ≤ l]
S0 (l, τ ) ≥ S̃0 (l, τ ) , P [Multinomial (1, . . . , K; G0 (1), . . . , G0 (K); τ ) ≤ l]
Proof of Step 2: We prove the claim for Q = 1 as the proof for Q = 0 is identical. The proof follows
by induction on τ . If it holds for τ − 1, then using Lemma 2 we can write
S1 (l, τ ) =

l X
K
X

pQ (i − j, τ − 1)G1 (j, q (i − j, τ − 1))

i=1 j=1

≤ S1 (l − 1, τ − 1) −

K−1
X

p1 (l − j, τ − 1)

j=1

=

K
X

K
X

G1 (r) ,

r=j+1

S1 (l − j, τ − 1)G1 (j) ≤

j=1

K
X

S̃1 (l − j, τ − 1)G1 (j) = S̃1 (l, τ ).

j=1

The proof of Step 1 finishes by noting that for τ = 1 we have

1
≤ G1 (1) = S̃1 (1, 1), S1 (K, 1) = 1 = S̃1 (K, 1),
S1 (1, 1) = G1 1,
2

 X
i
i
X
1
S1 (i, 1) =
G1 j,
≤
G1 (j) = S̃1 (1, 1), 2 ≤ i ≤ K − 1.
2


j=1

j=1

48

(47)

Step 3: Letting ∆ =



P

K
j=1 jG1 (j)

∆2
2
P[qτ >  | Q = 0] ≤ e−τ 2K 2 ,


−



P

K
j=1 jG0 (j)

∀τ ≥ 1, and

, for any  > 0, we have

∆2
2
P[1 − qτ >  | Q = 1] ≤ e−τ 2K 2 ,


∀τ ≥ 1. (48)

Moreover, we have qτ → Q∗ almost surely.
Proof of Step 3: First note that ∆ > 0, as we have
K
X

jG1 (j) =

j=1
1
2

Letting γ =

P

K
j=1 jG1 (j)

K X
K
X

G1 (i) >

j=1 i=j

+

K X
K
X

G0 (i) =

j=1 i=j

K
X

jG0 (j).

j=1


jG
(j)
and using Step 2 along with Chernoff-Hoeffding’s
0
j=1

PK

bound, we have



∆2
∆
S1 (γτ , τ ) ≤ S̃1 (γτ , τ ) = P Multinomial (1, . . . , K; G1 (1), . . . , G1 (K); τ ) ≤ τ
≤ 2e−τ 2K 2 .
2
We also have

∆2

S0 (γτ , τ ) ≥ 1 − 2e−τ 2K 2 .

(49)

We show the claim for Q∗ = 0 as the proof for Q∗ = 1 is identical. We have
Kτ
X

Kτ
X




q(i, τ )

P[qτ >  | Q = 0] =
p0 (i, τ )1{q(i, τ ) > } =
p0 (i, τ )1
>
1 − q(i, τ )
1−
i=0
i=0
 X



τγ
Kτ
X


q(i, τ )
q(i, τ )
=
>
+
>
p0 (i, τ )1
p0 (i, τ )1
1 − q(i, τ )
1−
1 − q(i, τ )
1−
i=0
i=τ γ


τγ
X

q(i, τ )
≤
>
+ (1 − S0 (γτ , τ ))
Using 1{·} ≤ 1
p0 (i, τ )1
1 − q(i, τ )
1−
i=0


τγ
X
∆2
q(i, τ )

≤
Using Eq. (48)
p0 (i, τ )1
>
+ 2e−τ 2K 2
1 − q(i, τ )
1−
i=0

≤

τγ
X
1−
i=1



∆2

p1 (i, τ ) + 2e−τ 2K 2 ≤

∆2
∆2
1 −  −τ ∆22
2
2e 2K + 2e−τ 2K 2 = e−τ 2K 2 .



Using Eq. (48)

This completes the proof of the first part. The second part follows from using Borell-Cantelli
Lemma together with the exponential tail bound, similar to that of Theorem 3.
Part 2 (Speed of Learning): We show this part for Q∗ = 0 as the proof for Q∗ = 1 is identical. We
will use Sanov’s Theorem (Cover and Thomas [2012, Chapter 11]) stated below:
Let Q be a probability distribution on finite set X = {1, . . . , K}. Also, let E be a (closed) set of

49

probability distribution on X , then we have





!



 Y
K
X


1
τ
ni 

lim log 
Q(i)
D(P ||Q),
 = − Pmin
τ →∞ τ
∈E
n1 , . . . , n K
 Pn1 ,...,nK

i=1


K
i=1 ni =τ
nK
n1
( τ ,..., τ )∈E
Similar to the proof of Theorem 4, letting Hτ indicate the set of all histories for which the average
P
of ratings is E0 , K
j=1 jG0 (j, 0), we obtain

P

1
qτ
Hτ ∈Hτ P [Hτ | Q = 1]
= lim log P
τ →∞ τ
1 − qτ
Hτ ∈Hτ P [Hτ | Q = 0]

P
 Q K
τ
nj
n1 ,...,nK
G
(j,
1)
j=1 1
n1 ,...,nK
PK


 PK j=1 nj =τ



1
j=1 jnj =τ E0




= lim log  P

Q
K
τ
τ →∞ τ
nj 


n1 ,...,nK
G
(j,
0)
j=1 0
n1 ,...,nK
PK


nj =τ

1
lim log
τ →∞ τ



PK j=1
j=1 jnj =τ E0

=−

min

a1 ,...,aK
PK
aj =1
PKj=1
j=1 jaj =E0

D ([a1 , . . . , aK ] || [G1 (1, 1), . . . , G1 (K, 1)]) ,

where we used Sanov’s Theorem with E = {(a1 , . . . , aK ) :

PK

j=1 aj

= 1,

PK

j=1 jaj

= E0 } to obtain

the limit. Finally, note that this speed of learning is smaller (not larger) than
D ([G0 (1, 0), . . . , G0 (K, 0)] || [G1 (1, 1), . . . , G1 (K, 1)]) ,
which is the speed of learning for vector summary statistics.

Proof of Theorem 7
To study the effects of targeted rating systems we consider m types of customers where the val(i)

uations of customers of type i ∈ [m] are drawn iid from distributions fθ

(i)

and fζ

for θ and ζ,

respectively. We also let pi denote the probability that a new customer belongs to group i for
1 ≤ i ≤ m. Similar to Eq. (4), for any i ∈ [m], we let P(i) [· | qt , Q] denote the probability of different
(i)

(i)

actions when the distribution of θ and ζ are fθ and fζ . Using these notations, we next show the
proof of this Theorem for Q∗ = 0 (the proof for Q∗ = 1 follows from a similar argument).
Full history: Learning the true quality follows from Theorem 1. We next characterize the speed of
learning. We have m×(K2 +K1 +1) total possible actions, i.e., {1, . . . , m}×({−K1 , . . . , K2 } ∪ {N }).

50

Therefore, Theorem 2 shows that learning speed is
m
X

X

(i)

pi P

i=1 h∈{−K1 ,...,K2 }∪{N }
m

X
=
pi D P(i) [· | q =
i=1

[h | q = 0, Q = 0] log

pi P(i) [h | q = 0, Q = 0]
pi P (i) (h | q = 0, Q = 1)

!


0, Q = 0] ||P(i) [· | q = 0, Q = 1] .

We next show that learning speed is higher than learning speed without types. Using convexity
of KL-divergence and Jensen’s inequality, we obtain
D (P [· | q = 0, Q = 0] ||P [· | q = 0, Q = 1]) = D

m
X

pi P

(i)

[· | q = 0, Q = 0] ||

i=1

≤

m
X

m
X

!
pi P

(i)

[· | q = 0, Q = 1]

i=1




(i)
(i)
pi D P [· | q = 0, Q = 0] ||P [· | q = 0, Q = 1] ,

i=1

which completes the proof of this part.
Summary statistics: Learning the true quality follows from Theorem 3. Using Theorem 4, for each
type i ∈ [m] the speed of learning is given by
lim

1

ti →∞ ti

log lti = −D (P [· | h ∈ T, q = 0, Q = 0] ||P [· | h ∈ T, q = 1, Q = 1]) ,

(50)

where ti denotes the sequence of customers that belong to type i. Each customer that joins the
platforms observes the summary statistics of all of m rating systems and forms a belief, which
Q
ti
using Bayes’ rule can be written as lt = m
i=1 lti . Using Eq. (50) along with t → pi almost surely
(using SLLN), we obtain
m

X
1
lim log lt = −
pi D (P [· | h ∈ T, q = 0, Q = 0] ||P [· | h ∈ T, q = 1, Q = 1]) .
t→∞ t
i=1

Again, using Jensen’s inequality completes the proof.

Proof of Theorem 8
We break the proof into two steps. In the first step we reformulate the platform’s revenue and
highlight that the speed of learning is a key component of platform’s revenue. In the second step
we show that increasing speed of learning, increases platform’s revenue.

51

Step 1: Maximizing the expected revenue of platform is equivalent to maximizing

Eθ,ζ



1
1
− p + θ + ζ 1 {θ + ζ − p ≥ 0} + (1 − p + θ + ζ) 1 {θ + ζ − p ∈ (−1, 0)}
2
2

1
+ Eθ,ζ [(−(p − θ − ζ)Pt,Ωt [qt ≥ p − θ − ζ | Q = 0]) 1 {θ + ζ − p ∈ (−1, 0)}]
2
1
+ Eθ,ζ [(−(1 − (p − θ − ζ))Pt,Ωt [qt ≤ p − θ − ζ | Q = 1]) 1 {θ + ζ − p ∈ (−1, 0)}] .
2

(51)

The platform’s expected revenue from customer t is proportional to P[jt = 1] (recall that platform gains a fixed revenue from each customer that joins the platform). Therefore, the platform’s
problem is to maximize P[jt = 1]. Using Eq. (2), this probability can be written as
Pc [jt = 1] = P [EQ,θt ,ζt ,Ωt ,t [1{bt = 1} (θt + ζt + Q − p)] − ct ≥ 0]
= Fc (EQ,θt ,ζt ,Ωt ,t [(θt + ζt + Q − p)1{bt = 1}]) .
Since Fc (·) is non-decreasing, the platform’s problem is to maximize
EQ,θt ,ζt ,Ωt ,t [(θt + ζt + Q − p)1{bt = 1}] .
Using Eq. (3), we have
EQ,θt ,ζt ,Ωt ,t [(θt + ζt + Q − p) 1{bt = 1}] = EQ,θt ,ζt ,Ωt ,t [(θt + ζt + Q − p) 1{qt > p − θt − E [ζ]}]
1
= Eθ,ζ [− (p − θ − ζ) Pt,Ωt [qt > p − E [ζ] − θ | Q = 0]]
2
1
+ Eθ,ζ [(1 − (p − θ − ζ)) Pt,Ωt [qt > p − ζ − E [θ] | Q = 1]]
2
1
1
= (1 − p + Eθ,ζ [θ + ζ]) + Eθ,ζ [− (p − θ − ζ) Pt,Ωt [qt ≥ p − θ − ζ | Q = 0]]
2
2
1
(52)
+ Eθ,ζ [− (1 − (p − θ − ζ)) Pt,Ωt [qt ≤ p − θ − ζ | Q = 1]] .
2
We can further write
EQ,θt ,ζt ,Ωt ,t [(θt + ζt + Q − p) 1{bt = 1}]
1
1
(1 − p + Eθ,ζ [θ + ζ]) + Pθ,ζ [p − θ − ζ ≤ 0] Eθ,ζ [− (p − θ − ζ) | p − θ − ζ ≤ 0]
2
2
1
+ Pθ,ζ [p − θ − ζ ≥ 1] Eθ,ζ [− (1 − (p − θ − ζ)) | p − θ − ζ ≥ 1]
2
1
+ Pθ,ζ [p − θ − ζ ∈ [0, 1]] Eθ,ζ [− (p − θ − ζ) Pt,Ωt [qt ≥ p − θ − ζ | Q = 0] | p − θ − ζ ∈ [0, 1]]
2
1
+ Pθ,ζ [p − θ − ζ ∈ [0, 1]] Eθ,ζ [− (1 − (p − θ − ζ)) Pt,Ωt [qt ≤ p − θ − ζ | Q = 1] | p − θ − ζ ∈ [0, 1]]
2 


1
1
= Eθ,ζ
− p + θ + ζ 1 {θ + ζ − p ≥ 0} (1 − p + θ + ζ) 1 {θ + ζ − p ∈ (−1, 0)}
2
2
1
+ Eθ,ζ [(−(p − θ − ζ)Pt,Ωt [qt ≥ p − θ − ζ | Q = 0]) 1 {θ + ζ − p ∈ (−1, 0)}]
2
1
+ Eθ,ζ [(−(1 − (p − θ − ζ))Pt,Ωt [qt ≤ p − θ − ζ | Q = 1]) 1 {θ + ζ − p ∈ (−1, 0)}] .
2
=

52

This proves the first step.
Step 2: If system I has a higher speed than system II, then the platform’s revenue from using
system I is higher.
For the second step, note that the terms that depend on the rating system are
1
Pθ,ζ [p − θ − ζ ∈ [0, 1]] Eθ,ζ [− (p − θ − ζ) Pt,Ωt [qt ≥ p − θ − ζ | Q = 0] | p − θ − ζ ∈ [0, 1]]
2
1
+ Pθ,ζ [p − θ − ζ ∈ [0, 1]] Eθ,ζ [− (1 − (p − θ − ζ)) Pt,Ωt [qt ≤ p − θ − ζ | Q = 1] | p − θ − ζ ∈ [0, 1]] .
2
Also, note that if ΩIt has a higher learning speed than ΩII
t , then since customers have an improper
uniform prior on the number of previous customers (before joining the platform), we obtain
 I

II
Pt,ΩIt ,ΩII
q
≤
q
|
Q
=
0
= 1,
t
t
t

 I

II
Pt,ΩIt ,ΩII
q
≥
q
|
Q
=
1
= 1.
t
t
t

This leads to


Pt,ΩIt qtI ≥ p − θ − ζ | Q = 0 ≤ Pt,ΩII
t


Pt,ΩIt qtI ≤ p − θ − ζ | Q = 1 ≤ Pt,ΩII
t


qtII ≥ p − θ − ζ | Q = 0 ,
 II

qt ≤ p − θ − ζ | Q = 1 .


Since p − θ − ζ ∈ [0, 1], we have − (p − θ − ζ) ≤ 0 and − (1 − (p − θ − ζ)) ≤ 0, which shows the
revenue of the rating system ΩIt is higher than the revenue of the rating system with ΩII
t .

References
D. Acemoglu, M. Dahleh, I. Lobel, and A. Ozdaglar (2009), Rate of convergence of learning in
social networks. In Proceedings of the American Control Conference, pages 2825–2830.
D. Acemoglu, M. A. Dahleh, I. Lobel, and A. Ozdaglar (2011), Bayesian learning in social networks. The Review of Economic Studies, 78(4):1201–1236.
M. Amador and P.-O. Weill (2012), Learning from private and public observations of others?
actions. Journal of Economic Theory, 147(3):910–940.
A. Banerjee and D. Fudenberg (2004), Word-of-mouth learning. Games and Economic Behavior, 46
(1):1–22.
A. V. Banerjee (1992), A simple model of herd behavior. The Quarterly Journal of Economics, 107(3):
797–817.
O. Besbes and M. Scarsini (2015). On information distortions in online ratings. Columbia Business
School Research Paper.
S. Bikhchandani, D. Hirshleifer, and I. Welch (1992), A theory of fads, fashion, custom, and cultural
change as informational cascades. Journal of political Economy, 100(5):992–1026.
53

T. Blake, C. Nosko, and S. Tadelis (2016), Returns to consumer search: Evidence from ebay. In
Proceedings of the 2016 ACM Conference on Economics and Computation, pages 531–545.
Y.-K. Che and J. Horner (2015). Optimal design for social learning.
A. Y. Chua and S. Banerjee (2016), Helpfulness of user-generated reviews as a function of review
sentiment, product type and information quality. Computers in Human Behavior, 54:547–554.
T. M. Cover and J. A. Thomas (2012). Elements of information theory. John Wiley & Sons.
D. Crapis, B. Ifrach, C. Maglaras, and M. Scarsini (2016), Monopoly pricing in the presence of
social learning. Management Science.
K. Drakopoulos, A. Ozdaglar, and J. N. Tsitsiklis (2013), On learning with finite memory. IEEE
Transactions on Information Theory, 59(10):6859–6872.
R. Durrett (2010), Probability: theory and examples. Cambridge university press.
N. Etemadi (1981), An elementary proof of the strong law of large numbers. Zeitschrift für
Wahrscheinlichkeitstheorie und verwandte Gebiete, 55(1):119–122.
N. Garg and R. Johari (2017), Pairwise comparisons for online reputation systems.
W. Hann-Caruthers, V. V. Martynov, and O. Tamuz (2017), The speed of sequential asymptotic
learning.
M. Harel, E. Mossel, P. Strack, and O. Tamuz (2014), When more information reduces the speed of
learning. Available at SSRN 2541707.
J. Horner and N. S. Lambert (2016), Motivational ratings.
B. Ifrach, C. Maglaras, and M. Scarsini (2014), Bayesian social learning with consumer reviews.
ACM SIGMETRICS Performance Evaluation Review, 41(4):28–28.
V. Korchevsky and V. Petrov (2010), On the strong law of large numbers for sequences of dependent random variables. Vestnik St. Petersburg University: Mathematics, 43(3):143–147.
J. Lafky (2014), Why do people rate? theory and evidence on online ratings. Games and Economic
Behavior, 87:554–570.
X. Li and L. M. Hitt (2008), Self-selection and information role of online product reviews. Information Systems Research, 19(4):456–474.
I. Lobel and E. Sadler (2015a), Information diffusion in networks through social learning. Theoretical Economics, 10(3):807–851.
I. Lobel and E. Sadler (2015b), Preferences, homophily, and social learning. Operations Research, 64
(3):564–584.
54

D. Mayzlin, Y. Dover, and J. Chevalier (2014), Promotional reviews: An empirical investigation of
online review manipulation. The American Economic Review, 104(8):2421–2455.
E. Mossel, A. Sly, and O. Tamuz (2014), Asymptotic learning on bayesian social networks. Probability Theory and Related Fields, 158(1-2):127–157.
E. Mossel, A. Sly, and O. Tamuz (2015), Strategic learning and the topology of social networks.
Econometrica, 83(5):1755–1794.
L. Smith and P. Sørensen (2000), Pathological outcomes of observational learning. Econometrica,
68(2):371–398.
A. Talwar, R. Jurca, and B. Faltings (2007), Understanding user behavior in online feedback reporting. In Proceedings of the 8th ACM conference on Electronic commerce, pages 134–142.
W. P. Tay, J. N. Tsitsiklis, and M. Z. Win (2008), On the subexponential decay of detection error
probabilities in long tandems. IEEE Transactions on Information Theory, 54(10):4767–4771.
S. Vaccari, M. Scarsini, and C. Maglaras (2016), Social learning in a competitive market with
consumer reviews.
X. Vives (1993), How fast do rational agents learn? The Review of Economic Studies, 60(2):329–347.
X. Vives (1995), The speed of information revelation in a financial market mechanism. Journal of
Economic Theory, 1(67):178–204.
I. Welch (1992), Sequential sales, learning, and cascades. The Journal of finance, 47(2):695–732.

55

