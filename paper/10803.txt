NBER WORKING PAPER SERIES

A REVEALED PREFERENCE RANKING OF
U.S. COLLEGES AND UNIVERSITIES
Christopher Avery
Mark Glickman
Caroline Hoxby
Andrew Metrick
Working Paper 10803
http://www.nber.org/papers/w10803
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2004

The authors' affiliations are, respectively, John F. Kennedy School of Government at Harvard University;
Department of Health Services at the Boston University School of Public Health, Department of Economics
at Harvard University, and Department of Finance of The Wharton School at the University of Pennsylvania.
We thank Bruce Sacerdote, Joel Waldfogel and seminar participants at Columbia, Wharton, Yale, the
University of Texas at Austin, University of California Santa Cruz, Harvard, and the National Bureau of
Economic Research for helpful comments. We thank Andrew Fairbanks and Jim Barker, who helped to
design and implement the College Admissions Project survey. We also thank Michael Behnke, Larry Momo,
Jay Matthews, and the 510 high school counselors who made the survey happen. We are grateful for the aid
of many hard-working and perspicacious research assistants: Joshua Barro, James Carmichael, Rohit
Chandwani, Michael Cuthbert, Suzanne Ko, Ilyana Kuziemko, Michael McNabb, Kathryn Markham, Emily
Oster, Chris Park, Jenna Robins, Aaron Roth, Maria Shim, Catherine So, Rania Succar, Michael Thakur,
Kenneth Wang, and Jill Zitnik. Scott Resnick deserves very special thanks. The first version of this paper
appeared in October 2002. The views expressed herein are those of the author(s) and not necessarily those
of the National Bureau of Economic Research.
©2004 by Christopher Avery, Mark Glickman, Caroline Hoxby, and Andrew Metrick. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.

A Revealed Preference Ranking of U.S. Colleges and Universities
Christopher Avery, Mark Glickman, Caroline Hoxby, and Andrew Metrick
NBER Working Paper No. 10803
September 2004
JEL No. I2, C11, C25
ABSTRACT
We show how to construct a ranking of U.S. undergraduate programs based on students' revealed
preferences. We construct examples of national and regional rankings, using hand-collected data on
3,240 high- achieving students. Our statistical model extends models used for ranking players in
tournaments, such as chess or tennis. When a student makes his matriculation decision among
colleges that have admitted him, he chooses which college "wins" in head-to-head competition. The
model exploits the information contained in thousands of these wins and losses. Our method
produces a ranking that would be difficult for a college to manipulate. In contrast, it is easy to
manipulate the matriculation rate and the admission rate, which are the common measures of
preference that receive substantial weight in highly publicized college rating systems. If our ranking
were used in place of these measures, the pressure on colleges to practice strategic admissions would
be relieved.
Christopher Avery
Harvard University
John F. Kennedy School of Government
79 John F. Kennedy Street
Cambridge, MA 02138
and NBER
christopher_avery@ksg.harvard.edu
Mark Glickman
Department of Health Services
School of Public Health
Boston University
mg@math.bu.edu

Caroline Hoxby
Department of Economics
Harvard University
Cambridge, MA 02138
and NBER
choxby@harvard.edu
Andrew Metrick
The Wharton School
University of Pennsylvania
3620 Locust Walk
Philadelphia, PA 19104
and NBER
metrick@wharton.upenn.edu

Executive Summary
We show how to construct a ranking of U.S. undergraduate programs based on how desirable
students find them. We call this the revealed preference ranking of colleges. We construct
examples of national and regional rankings, using data we collected on the college
applications, admissions, and matriculation of 3,240 high- achieving students.
Our statistical model extends models used for ranking players in tournaments, such as chess or
tennis. When a student decides to matriculate at one college, among those that have admitted
him, he effectively decides which college "won" in head-to-head competition. The model
efficiently combines the information contained in thousands of these wins and losses.
Our method produces a ranking that would be very difficult for a college to manipulate. In
contrast, colleges can easily manipulate the matriculation rate and the admission rate, which
are the crude proxies commonly used to measure colleges' desirability. Because there is a
strong demand for measures of colleges' desirability, colleges are forced to advertise their
matriculation and admissions rates. Moreover, college guides like U.S. News are forced to give
substantial weight to the matriculation and admissions rates. These crude proxies are not only
misleading; they induce colleges to engage in distorted conduct that decreases the colleges' real
selectivity while increasing the colleges' apparent desirability. So long as colleges are judged
based on their crude admissions and matriculation rates, they are unlikely to eliminate
strategic admissions or roll back early decision programs, which are the key methods of
manipulating the proxies. Many college administrators correctly perceive that they are in a
bad equilibrium. Yet, so long as the crude proxies are used, the bad equilibrium is likely to
persist. If our ranking method were used, the pressure on colleges to practice strategic
admissions would be relieved.
We rank more than 100 colleges in the national ranking, and we show how each college is
likely to fare in a head-to-head match up against specific rival colleges. We also show regional
rankings and demonstrate that they combine up to generate a truly national ranking among
colleges that are highly preferred. We explain how to think about niche colleges, such as
California Institute of Technology, whose applicants are self-selected to an unusual degree; and
we propose useful sub-rankings for certain types of colleges.

1

I. Why a Revealed Preference Ranking?
In this study, we show how to construct a ranking of U.S. undergraduate programs
based on students' revealed preferences –that is, the colleges students prefer when they can
choose among them. The result is a ranking of colleges based on their desirability. We develop
a statistical model that logically extends models used for ranking players in tournaments, such
as chess and tennis. When a student makes his matriculation decision among colleges that
have admitted him, he chooses which college "wins" in head-to-head competition. The model
exploits the information contained in thousands of these wins and losses.
We construct an example of our ranking using data from a survey of 3,240 highly
meritorious students that was specifically conducted for this study. Because we do not have a
fully representative sample of college applicants, we rank only about a hundred undergraduate
programs and our ranking is an example, not definitive. Nevertheless, we can show that our
ranking has advantages. In particular, it is less manipulable than crude measures of revealed
preference, such as the admissions rate and matriculation rate. A ranking constructed
according to our method would be a good substitute for the preference indicators that receive
substantial weight in formulas of high publicized college rating systems, like that of U.S. News
and World Report. Many colleges currently feel compelled to engage in strategic admissions
behavior in order to maximize their published college ratings. Use of our ranking method
would relieve this pressure.
Rankings based on students' revealed preference measure a college's desirability in
students' eyes. Such desirability may reflect a college's quality, but it is unlikely to be identical
to quality. Indeed, the notion of what constitutes quality in a college is likely to differ from
person to person. Faculty, parents, policy makers, and students may all assign different
weights to colleges' characteristics. Why then construct a revealed preference ranking at all,
which merely shows the value that students (in combination with their parents) put on
colleges?
The primary reason that we are motivated to construct a revealed preference ranking is
a practical one. Parents and students demand revealed preference information and college

2

guides feel obliged to offer them some. The two measures of preference used by college guides
are the crude matriculation rate and crude admissions rate. One objection to these measures is
that they are inefficiently coarse. Our revealed preference ranking efficiently aggregates the
information contained in individual students' decisions. Another serious objection to these
measures is that colleges can manipulate them, though at a cost. Colleges do not necessarily
want to manipulate their matriculation rate and admissions rate; they feel compelled to do so.
A college that does not manipulate these rates, when its competitors do, loses ground in highly
publicized college ratings. Such lost ground will eventually have real effects on the college's
ability to recruit students, attract donations, and so on.1 In short, U.S. colleges are in a bad
equilibrium: colleges manipulate the rates even though they would all be better off if no
college manipulated the rates. If a revealed preference ranking like ours were used, colleges
would find it extremely hard to "defect" and the bad equilibrium would not arise. All parties
(including the college guides) should be pleased to have a measure of revealed preference that
limits or even eliminates manipulation.
We have attempted to justify constructing a good indicator of revealed preference by
pointing out that one is demanded. But, why do students and their parents demand such
measures? There are a few possible answers.
First, students believe and act as though their peers matter. This may be because peer
quality affects the level of teaching that is offered. Alternatively, students may learn directly
from their peers. If such channels for peer effects are important, then it is reasonable for
students to care about whether they are surrounded by peers with high college aptitude.
Students will want to see a revealed preference ranking because it will show them which
colleges can offer the highest concentration of desirable peers. A more preferred college wins
more often in matriculation tournaments. Thus, it can afford to be more selective and can offer
peers with higher aptitude.
Second, students–especially the high achieving students on whom we focus–are not

1

See evidence on the real effects of the ratings, see Ehrenberg and M onks (1999).

3

ignorant about college quality. They gather information about colleges' quality from
publications, older siblings, friends who are attending college, college counselors, and their
own visits to colleges. A student may place the greatest weight on his own observations of
quality, but he will also put some weight on the observations of other students, simply because
his own sample of observations is too small to be representative. A revealed preference
ranking efficiently aggregates observations about quality from thousands of students. There
are parallels to other industries. For instance, people judge restaurant and hotel quality based
partly on their own experiences, but they also want to know about other people's experiences.
This is why there is a demand for guides like Zagat's, which aggregate people's observations
about hotels and restaurants.
Third, it has long been hypothesized that specific colleges' degrees serve as signals of a
student's aptitude, which is hard for future employers to observe directly [Spence, 1974]. In
equilibrium, a college's degree signals the aptitude of the students who actually attend it. For
instance, there will be an equilibrium only if a Princeton degree signals aptitude that is
consistent with the actual distribution of aptitude among Princeton students. This is another
reason for students to care about the ability of their peers and, thus, their college's tendency to
attract students.2
In Section II of the paper, we further discuss the weaknesses of using the matriculation
rate and the admissions rate as measures of revealed preference, and show how these
measures can easily be manipulated. In Section III, we present our statistical model of college
choice as a multiple comparison problem. We show how to account for the potentially
confounding effects of tuition discounts, financial aid, and other factors that might make a
college "win" when it would lose on the basis of its intrinsic desirability.
The data for our study was hand-collected in a survey of 510 high schools, with surveys

2

W e do not know, however, that such signaling is actually important. Students may be able to
use indicators other than their college degrees to inform future employers about their abilities. For
instance, a student whose abilities much exceed those of his college classmates could reveal his very high
grades, his leadership, his ability to win national fellowships, and so on.

4

returned for 3,240 students. Section IV describes the survey methodology and provides
summary statistics for the sample. These data are used to estimate the model, with the results
discussed in Section V. Section VI concludes the paper.

II. The Manipulability of Various Measures of Revealed Preference
One of the two common measures of revealed preference is the matriculation rate–the
share of accepted students who matriculate at a college:
(1)
There are several methods by which a college can manipulate its matriculation rate. The
reason that most methods work is that the matriculation rate is just an aggregate statistic and
has no way of taking account of the composition of the pool of admittees (higher or lower
merit?) and or of which students within the pool of admittees are matriculating (those with the
best alternative offers or those with worst alternative offers?).
An early decision program is the most dramatic means by which a college can
manipulate its matriculation rate. Every early decision admittee has a 100 percent probability
of matriculating, so –mechanically– the more students whom a college admits under its early
decision program, the higher is its matriculation rate. (It is important to distinguish between
early decision, in which a student commits to matriculate if admitted, and early action, in which
a student is admitted early but can apply to numerous other colleges and turn down the early
admission offer.) An early decision program is not without costs for the college. As Avery,
Fairbanks, and Zeckhauser (2003) show, college lowers their admissions standards for early
decision applicants in order to induce them to pre-commit to matriculating and pre-commit to
having no alternative offers when it comes to negotiating over financial aid. As a college
admits more and more of its class under early decision, its actual admissions standards fall and
students will therefore experience less meritorious peers. Yet, by the standard of the
matriculation rate, the college's desirability will have risen.
Another method by which a college can manipulate its matriculation rate is deliberately

5

not admitting students who are likely to be admitted by close competitors or colleges that are
often more highly preferred. A college administrator may say to himself, "My college will
ultimately fail to attract good applicants unless I raise its matriculation rate. I can achieve this
with a strategic policy that denies admission to students who seem likely to be accepted by
colleges more desirable than mine. By systemically denying them admission, my college will
of course lose of its some most desirable students (because some percentage of the highly
desirable students would have matriculated). However, it is worthwhile to sacrifice the actual
desirability of my college class in order to appear more desirable on a flawed indicator." To
make this strategy concrete, suppose that Princeton wanted to raise its matriculation rate. It
could decide to admit only students who were very likely to fall just short of the admissions
thresholds for Harvard, Yale, Stanford, MIT, and other close competitors. The students
admitted would thus have no colleges in their "menus" that were close competitors to
Princeton, and they would be likely to matriculate. Students who attend Princeton would
almost certainly prefer that the university not pursue such a policy because it would reduce the
peer quality of their fellow students. Yet, by the standard of the matriculation rate, Princeton's
measured appeal would rise just as its actual appeal fell.
We have not arbitrarily selected Princeton for our example. It is by no means alone in
appearing to practice somewhat strategic admissions (for other examples, see "Glass Floor:
How Colleges Reject the Top Applicants and Boost Their Status," 2001), but it makes for a
particularly clear example in our data.3 Consider Figure 1, which shows admissions at
Harvard, MIT, and Princeton. If a college is not practicing strategic admissions, then the
probability that a student is admitted ought to rise monotonically in his or her merit. In
contrast, a college that is strategic will have non-monotonic admissions probabilities. A
student's probability of admission will first rise in his or her merit and then fall as his or her
merit moves into the range in which the strategic college faces stiff competition. In other

3

As described below, we have the most ample data on the colleges that are the most selective.
Princeton provides the clearest example among the top several such colleges.

6

words, the college will avoid admitting students in the range in which it is likely to lose in a
matriculation tournament. Finally, if the student's merit is high enough, a strategic college will
probably admit the student even if the competition will be stiff. This is because the prospective
gains from enrolling a "star" will more than make up for the prospective losses from a higher
Figure 1

admissions rate and lower matriculation rate. (Recall that the crude admissions rate and
matriculation rate do not record who is admitted or matriculates.)
Although we realize that it is not a definitive measure of a student's merit, for the sake
of these purely illustrative figures, we use a student's combined SAT I score, measured in
national percentiles. This measure is at least readily understood and reasonably continuous. It
is also wholly unrelated to our ranking method.
Examine MIT admissions in Figure 1. The probability of a student's being admitted

7

rises steeply and monotonically in his or her combined SAT score, suggesting that MIT is not
engaging in strategic admissions. Now examine Harvard admissions in Figure 1. The line has
a flat region that suggests that the probability of a student's being admitted is about 10 percent
regardless of where his SAT scores in the range between the 93rd and the 98th percentiles.
Above the 98th percentile, a student's probability of admissions rises steeply. Finally, consider
Princeton admissions in Figure 1. At Princeton, the admissions probability rises to 20 percent
at the 93 percentile, then falls to 10 percent at the 98 percentile (precisely the region where
competition is toughest), and then rises again for students with SAT scores in the top 2
percentiles.
In short, it appears that Princeton practices more strategic admissions than MIT or
Harvard. When we see the revealed preference ranking later in the paper, we will see that
Figure 1 makes sense because Harvard and MIT could benefit less from strategic admissions
than Princeton could. While Figure 1 is not definitive, it provides suggestive evidence that
even a highly prestigious school may practice potentially costly strategic admissions. Such
behavior is potentially costly to the actual quality of an admissions class, with no clear benefit
beyond a higher reported matriculation rate.
The second of the two common proxies for revealed preference is the admission
rate–that is, the share of applicants who are admitted by a college:
(2)
There a several methods by which a college can manipulate its admissions rate. The reason
that most methods work is that the admissions rate is just an aggregate statistic. It does not
account for the composition of the pool of applicants (are they high or low merit?). It does not
account for which applicants a college admits.
In forming a class of a given size, a college can admit fewer students if its matriculation
rate is higher. Therefore, the methods discussed above for manipulating the matriculation rate
are also methods for manipulating the admissions rate. For instance, if a college makes heavy
use of an early decision program, it only needs to admit only slightly more students than the

8

number that it actually wishes to enroll. This is because the early decision admittees are precommitted to enrolling. The technique of not admitting applicants who are likely to be
admitted by close competitors also allows a college to publish a lower (better) admissions rate.
In addition, colleges can manipulate their admissions rate by encouraging applications
from students who have little chance of actually gaining admission. A college can advertise
less stringent criteria than it actually applies. By doing so, it encourages marginal students to
apply, increases its number of applications, decreases its admissions rate, and raises its
apparent desirability, even though its real desirability has not changed. For instance, this is
how Toor (2000) described her job as an admissions officer at Duke University: "The job of
admissions officers is to recruit, to boost application numbers. The more applications, the lower
the admit rate, the higher the institutional ranking. Increasing application numbers is usually
the No. 1 mandate of the recruiting season. Partly, that means trying to get the very best
students to apply. But it also means trying to persuade those regular, old Bright Well-Rounded
Kids (B.W.R.K.'s, in admissionese) to apply -- so that the college can reject them and bolster its
selectivity rating."
In short, the two conventional measures are manipulable by colleges, though at a cost.
If the goal of college admissions is to admit the optimal class, then colleges must systemically
deviate from this goal in order to manipulate their matriculation and admissions rates.
Colleges must sacrifice actual desirability for apparent desirability. Even if all colleges prefer
not to manipulate the crude rates, each college will lose if it refrains from manipulation when
other colleges do not refrain.
How might colleges escape this bad equilibrium? If the measure of revealed preference
is not manipulable (or manipulable only by very complex, costly means), then all parties could
be better off. In the next section, we formally describe the statistical method we use to create a
revealed preference ranking of colleges. Here, we can give some intuition into why a ranking
based on our method is not prey to simple forms of manipulation. For this exercise, it may be
helpful for readers to think of some sport or game familiar to them.
Our method is based on "wins" and "losses" in thousands of "tournaments" in which

9

students are choosing the college at which to matriculate. Under this method, a college's
ranking vis-a-vis a competitor is based on its record of wins and losses. Colleges that rarely
compete directly in tournaments (because they are of very different selectivity) are ranked
using the win/loss records of intermediate colleges that link them through series of
tournaments: A routinely competes with B, B routinely competes with C, C routinely competes
with D, etc. Given our methods, there is no easy way for a college to artificially boost its
ranking with no true change in its appeal to students. For instance, recall the example in which
Princeton alters its acceptance decisions in order to avoid match-ups with Harvard, Yale,
Stanford and so on. We would be unable to rank Princeton rank vis-a-vis its close competitors
because its match-ups would always be against less selective colleges. That is, our estimates
would reflect the fact that Princeton was not admitting the highly meritorious students for
whom it should have been competing. We would see that, while it was consistently "winning,"
it was winning only among students who failed to get admitted to close competitors.
Readers might also find it helpful if we stated what a college would need to do if it
were to manipulate our ranking successfully. None of the crude methods of manipulation
described above would work. A college would need to do something more subtle. Return to
the Princeton example, for concreteness. Princeton would need to find students in its applicant
pool who were likely to attend Princeton even if admitted to Harvard, Yale, MIT, Stanford, and
so on. Such students would have to exist exogenously; they could not be "created" by
Princeton's giving them extra aid to induce them to matriculate. (Giving them extra aid would
not work because we can observe and account for aid.) Moreover, Princeton would have to
identify these students using characteristics not observable to other colleges. If the trait that
Princeton used to pick out likely matriculators was observable (such as being a Princeton
alumnus' child), then this trait could be used as a control in any revealed preference ranking, as
we will do below with some characteristics collected in our study. Without an early decision
program to bind students or "secret" traits that distinguished its likely matriculators, a college
could not identify students whose matriculation tournaments it would win.

10

III. The Model
A. The Desirability of Colleges
The exercise of ranking colleges is necessarily predicated on the notion that there are
latent indices of desirability on which college can be ranked. In the language of econometrics,
the exercise is based on the assumption there are latent variables that indicate the desirability
of each college (perhaps on multiple dimensions). Our measure of desirability encompasses
all characteristics of a school, including (perceived) educational quality, campus location, and
tuition. We do not claim to know how latent desirability is constructed. We simply assert that,
to the extent that students act in accordance with it, we can construct rankings.
We suspect that latent desirability is well defined on a national basis for the most
academically elite colleges in the United States. We also suspect that latent desirability is
defined on a national basis for the most elite specialized colleges in the United States:
engineering schools, music schools, and so on. We would not be surprised to find, however,
that once we move below the most academically elite colleges, latent desirability is only welldefined within regions of the country and perhaps within other dimensions. If we had a very
large, random sample of all college applicants, we could construct rankings within regions and
specialties and show where they joined up to become a national ranking. Given that the data
we use for our exercise is focused on high achieving students who do not apply much outside
the group of the most academically elite colleges, we will start by constructing a national
ranking of such colleges. We will rank only those that the data suggest have a national draw.
Subsequently, we construct regional rankings and discuss specialized rankings. Until then,
however, we encourage the reader to think of a college's latent desirability as being
unidimensional.
For our exercise, is it necessary that all students identically perceive a college's
desirability? No. We will allow students' perception of a college's desirability to be distributed
around a mean level. Indeed, if there were no such distributions, all students would make
identical matriculation decisions when offered the same choices. We know that this does not

11

occur. What we need for our exercise is a pattern of wins and losses that would arise if colleges
had latent desirabilities that were perceived with idiosyncratic noise added in.
Finally, note that our exercise does not impose the existence of latent desirability; our
method simply will not work if widespread agreement on desirability does not exist. To see
this, suppose that there were no uniformity in how students perceived colleges' desirability.
Each student would act as though he had been randomly assigned a ranking of colleges, where
his ranking was independent of all other students' rankings. We would find no pattern in the
"wins" and "losses" because it would be random whether a college won or lost in head-to-head
competition for a student. Overall, we can afford to be agnostic about how students develop
preferences over colleges. Our data will only reveal such preferences to the extent that they are
systematic.
The problem of ranking colleges can be framed as a collection of multiple comparisons.
Comparison data come from competitions in which alternatives are compared and an outcome
indicates that one alternative has been preferred over the others. Many sports and games fall
into this framework because players are compared via competition, and the winner of a
competition is deemed the "preferred alternative.'' Also, marketing applications, including
experiments in which consumers choose among products or services, are well-suited to
multiple comparison models. An important problem addressed by multiple comparison
models is how to rank objects when direct comparisons do not take place. For example, in the
context of a "Swiss system'' chess tournament, every competitor competes against only a few
other individuals rather than against every other competitor. That is, player A competes
against B, and B competes against C, but A does not compete against C. Yet, an inference is
still desired for the comparison of A versus C. In the context of college choice, every college
does not compete directly with every other college, though the goal is to draw conclusions
about all colleges' desirability.
B. Matriculation Tournaments as a Multiple Comparison Problem
To understand how college choice can be viewed as a multiple comparison problem,
suppose that a collection of students has been admitted to a set of schools. Each schools’

12

desirability is modeled as a latent distribution of values. Each student effectively holds a
tournament among the collection of schools that have admitted him; in our model this
tournament is played by taking one draw from each school’s distribution. The school with the
highest draw has "won" the multi-player tournament, and the student matriculates at that
school. Assuming that there are no confounding variables, a reasonable inference is that the
school that wins the multi-player tournament is preferred to the other schools in that
competition. By aggregating the information from all students' tournaments, inferences about
the desirability of schools can be constructed.
David (1988) surveys the rich body of work on multiple comparison modeling, which
mainly focuses on paired comparison models, where each tournament contains only two
players. While no one has previously attempted to rank colleges using comparison models,
there are abundant applications for divining chess ability from tournament data-- see, for
example, Zermelo (1929), Good (1955), Elo (1978) and Glickman (1993, 1999, 2001).4
We build on the Bradley-Terry (1952) and the Luce (1959) models in which the
distribution of desirability is an extreme value distribution. The assumption of an extreme
value distribution for potentially observed desirability leads to a logit model. The main
alternative to the assumption of an extreme value distribution for potentially observed
desirability is a normal distribution. This leads to a class of models studied by Thurstone
(1927) and Mosteller (1951) in the context of paired comparisons. When analyzing paired
comparison data in practice, it makes almost no difference whether one assumes that the
distribution of potentially observed desirability is extreme value or normal (see Stern, 1992).
Models based on extreme value distributions tend to be more tractable and computationally
efficient, which guides our choice.
It is worth noting that the extreme-value or normal distribution of potential
desirabilities is a probabilistic assumption about the merit of an individual school, not an

4

Statistical comparison models have also been used to study which college characteristics
students like and which student characteristics colleges like. See, for example, M anski and W ise (1983),
Long (2003), Avery and Hoxby (2004).

13

assumption about the distribution of mean desirabilities across schools. Because college
comparison data can provide strong information about the relative desirabilities of colleges,
any assumptions made about the distribution of mean desirabilities should be weak. Our
modeling approach allows for the possibility, for example, that a small number of schools are
estimated to have mean desirabilities substantially greater than the remaining schools
considered.
C. The Matriculation Model
Assuming that each college's potentially observed desirability follows an extreme value
distribution with the same scale and shape, the relevant parameter is the location parameter of
the distribution. The latent variable is:
= the desirability parameter of college i,
where we index colleges with i=1,2,...,I.
Students prefer colleges with higher desirability, among those in their choice set.
Suppose that student j is admitted to a set of colleges
indicator variable

consisting of

schools. Let the

tell us which college the student chooses:

(3)

The result of the multi-player competition among the

colleges that admitted student j is

assumed to follow a multinomial distribution:
(4)
where

is the probability that student j chooses to matriculate at college i among his

college choices.5 We assume Luce's choice model, of the form:
(5)

5

For expositional convenience, we have reindexed the colleges in student j set
can be written 1,..., .

, so that they

14

This model can be rewritten as a conditional logit model, sometimes called McFadden's choice
model.
The

s include all characteristics that do not vary within each college: such

characteristics include average perceptions about the quality of the education and the average
cost of attendance. For some characteristics, we can measure variation across applicants:
tuition, room fees, board fees, grants to the student, the subsidy value of loans to the student,
the subsidy value of the work-study it offers the student, the cost associated with its distance
from the student's home, its being in-state, its being in-region, and its being the alma mater of
one or more of the student's parents. We add these characteristics to the model to gain extra
explanatory power.6
Let the vector

be the K characteristics that can vary among

admittees and that are faced by admittee j who is considering whether to matriculate at college
i. Note well that each characteristic is de-meaned so that we obtain the college's desirability at
its average level in the data. It is not possible to separately identify the effect of these average
characteristics from the

for each school. We treat

as a vector of covariates which are

allowed to enter the model linearly. Specifically, the probabilities for the matriculation model
become:
(6)

In fitting the model, not only are the

inferred, but so are the

, which are the effects of the

characteristics on matriculation.
D. Self-Selection and the Application Decision
We estimate the

from matriculation decisions of admitted applicants. Of course, to be

admitted, one must first apply, so our underlying data for each school is not a random sample
of all students, but rather of all students conditional on their application to that school. This is a

6

In practice, the covariates have a trivial effect on the rankings. Estimates of the model without
covariates are available from the authors.

15

self-selected group, and we expect a group of applicants to find a school more desirable than
an otherwise identical set of non-applicants. Such self-selection does not induce any bias if the
applicant pool for every school is shifted equivalently: that is, if we are estimating a

for every

school based on the applicant pool, but the equivalent parameter for all students (applicants
and non-applicants) is

-

, and the

is the same for all schools, i. Since our ratings are

unique only up to a constant, such a shift would not change the interpretation of our results.
Self-selection would cause a problem if the applicant pools are induced differently
across schools. This would appear to be a major issue only for “niche schools” that attract
applications from only the most interested students. Any speciality school could fall into this
category, with engineering schools, school with a religious affiliation, or single-sex schools
being the most likely. These schools might attract applicant pools with stronger preferences –
because students who are lukewarm about the speciality don’t bother to apply – and
effectively have a higher

, leading the estimated

to be biased upward.

The ideal way to handle these selection issues would be to explicitly model the
application decision, but this is not feasible without many further assumptions. With
thousands of schools to choose from, even artificial constraints on the number of applications
leads to a complex combinatorial problem. In this case, the modeler – like the applicants
themselves – is forced to use shortcuts and assumptions. Since these assumptions would
ultimately drive the extent of selection bias, it seems more straightforward to acknowledge this
potential bias and discuss its implications where it is appropriate. Thus, we proceed under the
baseline assumption that the

are identical across schools. In Section V, we discuss the

implications of deviations from this assumption, point out specific schools for which these
deviations may make a difference, and propose a practical method for dealing with them.
While we do not believe that self-selection is a major issue for the
greater concern for inference on the

estimates, it is of

coefficients For instance, suppose that price sensitivity

is heterogeneous and students who are especially price sensitive seek out colleges that offer
them substantial discounts. We might overestimate the effects of prices because the variation
in the data comes disproportionately from price-sensitive students. For this reason, we will not

16

give strong interpretations to the coefficients on these characteristics. It is still useful to include
these characteristics in the regression, especially because they may proxy for otherwise
unobservable variables.
E. Model Fitting
The complexity of our model lends itself naturally to fitting the model in the Bayesian
framework. We fit our model by computing the posterior distribution of model parameters
followed by summarizing important features of the distribution. The posterior distribution of
parameters is proportional to the product of the likelihood function with a prior distribution.
The likelihood can be written as a product of multinomial logit probabilities derived from
equation (6). We assume a locally uniform but proper prior distribution that factors into
independent densities. The prior distribution consists of the following components:

(7)

where

indexes the kth element of the vector delta.
We summarize estimated college desirability by computing the posterior modes of the

. These were carried out using a Newton-Raphson algorithm for multinomial logit models,
as implemented in Stata. The posterior modes are presented in Tables 3, 5 and 6. We also fit
the model using Markov chain Monte Carlo(MCMC) simulation from the posterior distribution
to infer more complex quantities of interest. For example, to answer questions like "is there a
meaningful distinction in desirability between the college ranked 15th and the college ranked
20th?" we cannot simply rely on comparing posterior modes. Instead, MCMC produces
simulated values from the posterior distribution of model parameters. Thus, using MCMC
simulation, pairs of values can be generated from the posterior distribution of (
the probability that
pairs in which

is greater than

is greater than

,

), and

can be evaluated by computing the proportion of

. An answer like 95 percent or more – analogous to a 95%

17

significance test – tells us that the colleges are substantially more distinct than an answer like
55 percent.
The MCMC algorithm proceeds as follows. Initial values of all parameters are set to the
prior mean values (though the initial values can be set arbitrarily). Then values are simulated
from the conditional posterior distributions of each model parameter. This process is repeated
until the distributions of values for individual parameters stabilize. The values simulated
beyond this point can be viewed as coming from the posterior distribution. A recent example
of MCMC methods applied to paired comparison models is Glickman (2001).
We implemented the MCMC algorithm using the program BUGS (Spiegelhalter et al.,
1996). For each model, a burn-in period of 10,000 iterations was run, and parameter summaries
were based on every 5th iteration of a subsequent 30,000 iterations. Based on trace plots from
our data analyses, 10,000 iterations was sufficient to reach the stationary distribution. Every
5th iteration was sampled to reduce the autocorrelation in successive parameter draws. This
process produced 6000 values per parameter on which to calculate parameter summaries. In
Table 4, which shows pairwise match-ups for each college we rank, we display summaries
based on MCMC draws from the posterior distribution.

IV. Data
To construct an example of our revealed preference ranking, we use from the College
Admissions Project survey, in which we surveyed high school seniors in the college graduating
class of 2004.7 We designed the survey to gather data on students with very high college
aptitude who are likely to gain admission to the colleges with a national or broad regional
draw that are most appropriate for ranking. While such students are represented in surveys
that attempt to be nationally representative, such as the National Educational Longitudinal
Survey, they are a very small share of the population of American students. As a result, the
number of such students is so small in typical surveys that their behavior cannot be analyzed,

7

See Avery and Hoxby [2000] for additional detail.

18

even if the survey contains a large number of students. By focusing on students with very
strong academic credentials, we end up with a sufficient number of tournaments among
colleges with a national draw to construct a revealed preference ranking among them.
We reemphasize that we use the College Admissions Project data to construct an
example of a revealed preference ranking. If we had had much greater resources, we would
have surveyed a more fully representative sample of students in the United States. With more
data, our national ranking would be more definitive, and we would be able to rank many more
colleges (most of them in regional or specialized rankings, not the national ranking). At the
end of this section, we describe the cut-offs we used to determine which colleges we could
reasonably rank.
A. Survey Design
In order to find students who were appropriate candidates for the survey, we worked
with counselors from 510 high schools around the United States. The high schools that were
selected had a record of sending several students to selective colleges each year, and they were
identified using published guides to secondary schools, such as Peterson's and the experience
of admissions experts. Each counselor selected ten students at random from the top of his
senior class as measured by grade point average. Counselors at public schools selected
students at random from the top 10% of the senior class, while counselors at private schools
(which tend to be smaller and have higher mean college aptitude) selected students at random
from the top 20% of the senior class.8 The counselors distributed the surveys to students,
collected the completed surveys, and returned them to us for coding.9 Students were tracked

8

The counselors were given detailed instructions for random sampling from the top 20, 30, 40, or
50 students in the senior class depending on the size of the school. For example, a counselor from a
public school with 157 students was asked to select 10 students at random from the top 20 students in the
senior class, with the suggestion that the counselor select students ranked #1, 3, 5, 7, 9, 11, 13, 15, 17, and
19.
9

The exception was the parent survey, which parents mailed directly to us in an addressed,
postage-paid envelope so that they would not have to give possibly sensitive financial information to the
high school counselor.

19

using a randomly assigned number; we never learned the names of the students who
participated.
Survey participants completed two questionnaires over the course of the academic year.
The first questionnaire was administered in January 2000. It asked for information on the
student's background and college applications; the majority of these questions were taken
directly from the Common Application, which is accepted by many colleges in place of their
proprietary application forms. Each student listed up to ten colleges where he had applied, his
test scores, and race. In addition, each student listed the colleges and graduate schools (if any)
attended by each parent and the colleges (if any) attended by older siblings along with their
expected graduation dates.
The second questionnaire was administered in May 2000 and asked for information
about the student's admission outcomes, financial aid offers, scholarship offers, and
matriculation decision. Each student listed their financial aid packages with the amounts
offered in three categories: grants, loans, and Work Study. We obtained detailed information
on grants and scholarships. On a third questionnaire distributed to a parent of each survey
participant, we collected information on parents' income range (see Table 1 for the income
categories.)
We matched the survey to colleges' administrative data on tuition, room and board,
location, and other college characteristics. In all cases, the ultimate source for the
administrative data was the college itself and the data were for the 2000-01 school year, which
corresponds to the survey participants' freshmen year.10
The College Admissions Project survey produced a response rate of approximately 65%,
including full information for 3,240 students from 396 high schools.11 The final sample contains

10

11

See Avery and Hoxby [2004] for a complete description of administrative data sources.

The most comm on reasons for failure to return the survey were changes of high school
administration, an illness contracted by the counselor, and other administrative problems that were
unrelated to the college admissions outcomes of students who had been selected to participate.

20

students from 43 states plus the District of Columbia.12 Although the sample was constructed
to include students from every region of the country, it is intentionally representative of
applicants to highly selective colleges and therefore non-representative of American high
school students as a whole. Regions and states that produce a disproportionate share of the
students who apply to selective colleges are given a weight in the sample that is approximately
proportionate to their weight at very selective colleges, not their weight in the population of
American high school students. Of course, all of the students in the sample have very strong
academic records.
Because the students are drawn from schools that send several students to selective
colleges each year (though not necessarily to very selective colleges), the students in the sample
are probably slightly better informed than the typical high aptitude applicant. However, in
other work [Avery and Hoxby, 2004], we have found that students who make it into the sample
act very much like one another when they make college decisions, regardless of whether they
come from more or less advantaged backgrounds. This suggests that a revealed preference
ranking based on our sample may reflect slightly more information than one based on the
typical applicant, but the difference in the information embodied in the ranking is probably
small.
B. Sample Statistics
The summary statistics shown in Tables 1 and 2 demonstrate show the students in the
sample are high achieving. The average (combined verbal and math) SAT score among
participants was 1357, which put the average student in the sample at the 90th percentile of all
SAT takers.13 About 5 percent of the students won a National Merit Scholarship; 20 percent of
them won a portable outside scholarship; and 46 percent of them won a merit-based grant from

12

The states missing from the sample are Alaska, Delaware, Iowa, M ississippi, North Dakota,
South Dakota, and W est Virginia.
13

W e converted American College Test (ACT) scores to SAT scores using the cross-walk
provided by The College Board. We converted all college admissions scores into national percentile
scores using the national distribution of SAT scores for the freshman class of 2000-01.

21

at least one college. 45 percent of the students attended private school, and their parents'
income averaged $119,929 in 1999.14 However, 76 percent of the sample had incomes below the
cut-off where a family is considered for aid by selective private colleges, and 59 percent of the
students applied for need-based financial aid.15
Table 1
Description of the Students in the College Admission Project Data
Variable

Mean

Std. Dev. Minimum Maximum

Male

0.41

0.49

0.00

1.00

White, non-Hispanic

0.73

0.44

0.00

1.00

Black, non-Hispanic

0.04

0.18

0.00

1.00

Asian

0.16

0.36

0.00

1.00

Hispanic

0.04

0.19

0.00

1.00

Native American

0.00

0.03

0.00

1.00

Other race/ethnicity

0.04

0.19

0.00

1.00

Parents are married

0.83

0.38

0.00

1.00

Sibling(s) enrolled in college

0.23

0.42

0.00

1.00

119,929

65,518

9,186

240,000

27,653

16,524

0

120,000

Applied for financial aid?

0.59

0.49

0.00

1.00

National Merit Scholarship winner

0.05

0.22

0.00

1.00

Student's combined SAT score

1357

139

780

1600

Student's SAT score, in national percentiles

90.4

12.3

12.0

100.0

Median SAT score at most selective college
to which student was admitted

86.4

10.4

33.5

98.0

Median SAT score at least selective college
to which student was admitted

73.8

14.6

14.3

97.0

Student's high school was private

0.45

0.50

0.00

1.00

Parents' income
Expected family contribution

14

See Avery and Hoxby [2004] for descriptions of how the aid variables were hand checked and
how some parents' income was estimated based on their Expected Family Contribution, a federal
financial aid measure.
15

The cut-off was approximately $160,000, but the actual cut-off depends on family
circumstances.

22

83 percent of the student's parents were currently married, and 23 percent of the
students had at least one sibling currently enrolled in college. The racial composition of the
survey participants was 73 percent white, 16 percent Asian, 3.5 percent black, and 3.8 percent
Hispanic.
Looking at Table 2, which shows descriptive statistics on the colleges where the
students applied, were admitted, and matriculated; we can see that the survey participants
applied to a range of colleges that included "safety schools" (the mean college to which a
student applied had a median SAT score 8.5 percentiles below the student's own). However,
the participants also made ambitious applications: 47.5 percent of them applied to at least one
Ivy League college.
Table 2
Description of the Colleges in the College Admission Project Data
Colleges at Which Students
Applied

Were Admitted

Matricalated

Mean

Std.
Dev.

Mean

Std.
Dev.

Mean

Std.
Dev.

Matriculated at this college

0.28

0.45

0.18

0.39

1.00

0.00

Admitted to this college

1.00

0.00

0.66

0.47

1.00

0.00

Grants from this college

2720

5870

1778

4933

4029

7051

Loans from this college

641

2282

413

1856

1020

2722

Work study amount from this college

172

593

111

483

296

768

Father is an alumnus of this college

0.04

0.20

0.03

0.17

0.07

0.25

Mother is an alumna of this college

0.03

0.17

0.02

0.14

0.04

0.19

Sibling attended or attends this college

0.05

0.21

0.04

0.19

0.08

0.28

0.3325

0.4711

0.2631

0.4403

0.2843

0.4512

Variable

College is public
College's median SAT score, in
percentiles

80.5947 12.5188 83.8816 12.0390 83.4215 12.5494

In-state tuition

16435

9594

18181

9199

17432

9513

Out-of-state tuition

19294

6191

20498

5891

19841

6371

Tuition that applies to this student

17671

8492

19277

7965

18340

8599

College is in-state

0.3270

0.4691

0.2666

0.4422

0.3368

0.4727

597

809

673

873

576

827

Distance between student's high school
and this college, in miles

23

We can see that the students made logical application decisions. The mean college to
which they applied had a median SAT score at the 83rd percentile; the mean college to which
they were admitted had median SAT score at the 81st percentile. This small difference suggests
that the students aimed a little high in their applications, a procedure that is optimal. 66
percent of the colleges to which students were admitted were private, and their mean tuition
was $17,671. Notice that we show the colleges' in-state tuition, out-of-state tuition, and the
tuition that actually applies to the students in the sample (in-state or out-of-state as
appropriate).
The final column of Table 2 shows descriptive statistics for the colleges at which the
students matriculated. They are more selective, on average, than the colleges to which the
students were admitted: their median SAT score is at the 83.4th percentile, as opposed to the
81st percentile median SAT score of the colleges to which students were admitted. This makes
sense because it implies that students included "safety schools" in their choice sets, but often
did not matriculate at them. One measure of the unusual college aptitude of the survey
participants is the list of colleges at which the largest numbers of participants enrolled.
Seventeen institutions enrolled at least 50 students from the sample: Harvard, Yale, University
of Pennsylvania, Stanford, Brown, Cornell, University of Virginia, Columbia, University of
California–Berkeley, Northwestern, Princeton, Duke, University of Illinois, New York
University, University of Michigan, Dartmouth, and Georgetown.

V. Results
We show a college in the national ranking if it was not a military academy and if, in our
sample, it competed in matriculation tournaments in at least six of the nine regions of the U.S.
106 colleges met these criteria. The mean college shown in the national ranking competed in
73 matriculation tournaments, and the median college competed in 57. Admittedly, the six
region cut-off is somewhat arbitrary, but we show regional rankings after showing the national
rankings. The regional rankings pick up extra colleges. Please note that if a small college fails
to appear in the rankings, one should not conclude that its ranking is below 106 or that it does

24

not have a national draw. For a small college, our sample might fail to pick up enough
applicants to include the college in the national ranking, even if its draw is national in
character.
A. National Ranking
Table 3 presents the revealed preference ranking of colleges and universities with a
national draw. For each college, we present its mean desirability among students. Keep in
mind that Table 3 shows only an example of our ranking method. With more data, we would
produce a more definitive ranking. The rankings are on an arbitrary numerical scale of value,
Elo points, which are used in chess and other game rankings. The conversion multiplies the

s

by 173 and then adds whatever number gives 2800 points to the highest ranked college.16 The
following table contains rule-of-thumb relationships between point differences and probability
of winning:
400

.919

300

.853

200

.758

100

.637

50

.569

0

.500

-50

.431

-100

.363

-200

.242

-300

.147

-400

.081

Note that Elo point differences tell us only about the college versus its mean competitor.
Put another way, attaching standard errors to the estimates in Table 3 would not be very useful

16

W e choose 2800 as the maximum number because this is approximately the rating for the
highest-rated chess player in the world. W e use the Elo scale because of its familiarity. In addition to
serving as the main rating system for chess and many other board games, the Elo scale has also been used
in a wide variety of sports. A partial list includes soccer (www.eloratings.net/), college football
(www.usatoday.com/sports/sagarin/fbt01.htm), cricket (www.ultra-sports.com/Cricket/UC4/
UC4abselo.html), and racquetball (www.eqp.com/pubs/rb/PlayerRankByELO.asp).

25

because they would not reliably indicate whether any two colleges' rankings were statistically
distinct. This is because the statistical significance of the difference between any two colleges'
ranks depends on the overlap between their two groups of admittees. For this reason, it is best
to use Table 4 for head-to-head comparisons between colleges.
Table 4 summarizes the results of posterior draws: the Bayesian analogue to a set of
paired significance tests. For instance, in 96 percent of the draws from the posterior
distribution, Harvard's ranking was higher than Yale's, and for 95 percent of the draws
Harvard was higher than Cal Tech. For all other colleges, Harvard's ranking was higher in at
least 99 percent of the draws. Put another way, we are 96 percent confident that Harvard's
ranking is higher than Yale's, 95 percent confident it is higher than Cal Tech, and at least 99
percent confident that it is higher than that of other colleges. For Yale, in turn, we are 88
percent confident that its ranking is higher than Stanford's, 78 percent confident that its
ranking is higher than Cal Tech's, and 91 percent confident that its ranking is higher than
MIT's.
Table 4 helps us to understand the results for Cal Tech, which are somewhat
problematic. Because students self-select into applying to Cal Tech based on an orientation
toward math and science, Cal Tech's pool of admittees overlaps only slightly with that of most
other institutions, except for MIT, with which Cal Tech has substantial overlap. MIT, on the
other hand, does have substantial overlap with other top schools. Unlike the other institutions
in the top twenty, Cal Tech appears to draw a more focused group of applicants. In Section
III.D, we discussed how such self-selection might bias inference for some speciality schools,
with the possibility of some upward bias in the

estimate.

All of the top twenty are private institutions and four-fifths are universities (the
exceptions being Amherst, Wellesley, Williams, and Swarthmore). The next twenty institutions

26

rank
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43

Table 3
A Revealed Preference Ranking of Colleges
College Name
Harvard
Yale
Stanford
Cal Tech
M IT
Princeton
Brown
Columbia
Amherst
Dartmouth
W ellesley
U Penn
U Notre Dame
Swarthmore
Cornell
Georgetown
Rice
W illiams
Duke
U Virginia
Northwestern
Pom ona
Berkeley
Georgia Tech
M iddlebury
W esleyan
U Chicago
Johns Hopkins
USC
Furman
UNC
Barnard
Oberlin
Carleton
Vanderbilt
UCLA
Davidson
U Texas
NYU
Tufts
W ashington & Lee
U M ichigan
Vassar

Elo pts
2800
2738
2694
2632
2624
2608
2433
2392
2363
2357
2346
2325
2279
2270
2236
2218
2214
2213
2209
2197
2136
2132
2115
2115
2114
2111
2104
2096
2072
2061
2045
2034
2027
2022
2016
2012
2010
2008
1992
1986
1983
1978
1978

27

rank
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86

Table 3
A Revealed Preference Ranking of Colleges
College Name
Grinnell
U Illinois
Carnegie Mellon
U M aryland
W illiam & M ary
Bowdoin
W ake Forest
Claremont
M acalester
Colgate
Smith
U M iami
Haverford
M t Holyoke
Connecticut College
Bates
Kenyon
Emory
W ashington U
Occidental
Bryn M awr
SM U
Lehigh
Holy Cross
Reed College
RPI
Florida State
Colby
UCSB
GW U
Fordham
Sarah Lawrence
Bucknell
Catholic U
U Colorado
U W isconsin
Arizona State
W heaton (Il)
Rose Hulman
UCSC
Boston U
UCSD
Tulane

Elo pts
1977
1974
1957
1956
1954
1953
1940
1936
1926
1925
1921
1914
1910
1909
1906
1903
1903
1888
1887
1883
1871
1860
1858
1839
1837
1835
1834
1820
1818
1798
1796
1788
1784
1784
1784
1780
1774
1750
1745
1736
1736
1732
1727

28

rank
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105

Table 3
A Revealed Preference Ranking of Colleges
College Name
U Richmond
CW RU
Trinity College
Colorado College
Indiana U
Penn State
American U
Hamilton
U W ashington
U Rochester
Lewis & Clark
W heaton (M A)
Clark
Skidmore
Purdue
Colorado State
Syracuse
Scripps
Loyola U
Tuition (In Thousands, In-state or Out-of-state, W hichever Applies)

Elo pts
1714
1704
1703
1698
1689
1686
1681
1674
1629
1619
1593
1564
1551
1548
1525
1513
1506
1479
1221
-6.443
(3.129)
Grants (In Thousands)
28.156
(2.104)
Loans (In Thousands)
12.629
(3.018)
W ork-study
3.023
(13.091)
Indicator: Is Dad's College
70.458
(29.450)
Indicator: Is M om's College
34.432
(24.797)
Indicator: Is a Sibling's College
94.743
(25.290)
Indicator: College in Home State
25.646
(38.033)
Indicator: College in Home Region
15.191
(20.533)
Distance from Home (Hundreds of M iles)
4.276
(2.137)
Notes: Estimates based on equation (6) converted into Elo points (see text). Standard errors are in
parentheses.

29
Table 4: Share of Draws in W hich College in the Row is Ranked Higher than the College Various Places Below It
Num ber of Places Below
College

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

Harvard

96

100

95

100

100

100

100

100

100

100

100

100

100

100

100

100

100

100

100

100

Yale

88

78

91

95

100

100

100

100

100

100

100

100

100

100

100

100

100

100

100

100

Stanford

58

62

76

100

100

100

100

100

100

100

100

100

100

100

100

100

100

100

100

100

Cal Tech

51

57

89

94

96

96

96

98

98

99

100

100

100

100

100

100

100

100

100

100

M IT

63

99

100

100

100

99

100

100

100

100

100

100

100

100

100

100

100

100

100

100

Princeton

96

99

99

99

99

100

100

100

100

100

100

100

100

100

100

100

100

100

100

100

Brown

80

87

90

88

97

96

98

100

100

100

100

100

100

100

100

100

100

100

100

100

Columbia

65

66

72

80

85

92

99

99

99

98

100

100

100

100

100

100

100

100

100

100

Amherst

50

59

62

74

85

92

95

97

95

98

97

100

99

100

100

99

98

100

100

100

Dartmouth

60

65

76

86

95

97

98

96

99

98

100

100

100

100

100

99

100

100

100

99

W ellesley

50

64

75

82

85

93

90

92

90

98

98

98

99

97

94

99

99

99

97

100

U Penn

68

81

94

96

97

94

99

98

100

99

100

100

100

98

100

100

100

99

100

100

Notre Dame

65

73

78

89

84

88

85

97

96

98

98

95

91

99

99

99

96

100

99

100

Swarthmore

53

60

78

73

74

68

90

90

91

93

87

80

95

94

95

91

97

98

98

98

Cornell

61

82

75

81

73

97

95

98

97

94

85

99

99

99

94

100

99

100

99

100

Georgetown

77

68

71

62

93

91

95

95

89

78

97

97

98

91

99

99

99

99

100

100

Rice

45

38

31

62

68

64

75

60

49

75

75

76

77

86

87

89

89

92

93

92

W illiams

46

39

67

73

69

79

66

56

78

79

79

80

89

89

90

91

94

93

94

94

Duke

40

81

82

84

89

77

63

91

91

92

86

97

96

97

96

99

99

98

99

100

U Virginia

88

87

90

92

83

70

95

94

96

89

98

98

98

98

100

100

99

99

100

100

Northwestern

62

54

71

49

35

72

72

74

74

88

88

90

90

96

96

93

95

99

98

90

Pom ona

40

58

39

28

55

56

55

64

74

75

78

79

83

84

84

85

89

89

83

88

Berkeley

69

47

33

69

69

72

73

86

88

89

89

95

98

93

95

99

98

90

99

96

Georgia Tech

31

22

45

47

45

59

67

70

71

74

78

78

81

82

85

86

79

84

82

80

M iddlebury

37

67

69

69

72

84

85

88

87

92

92

92

92

96

96

89

96

94

90

97

W esleyan

78

79

80

79

90

90

91

92

95

96

94

95

98

98

92

97

97

93

98

98

UChicago

52

52

63

74

76

79

80

85

86

86

87

93

93

83

92

90

85

95

95

91

Johns Hopkins

49

62

72

74

77

78

83

84

84

85

91

91

81

91

88

83

93

94

89

95

30
Table 4: Share of Draws in W hich College in the Row is Ranked Higher than the College Various Places Below It
Num ber of Places Below
College

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

USC

62

73

76

79

80

86

89

85

87

94

93

82

94

90

84

96

96

91

96

93

Furman

54

56

59

61

62

61

68

66

68

70

68

67

70

69

70

76

67

78

74

82

UNC

54

58

61

63

61

70

68

73

75

69

71

71

71

76

82

70

84

79

88

72

Barnard

54

58

59

56

66

64

68

70

65

66

68

68

71

78

66

81

75

84

70

84

Oberlin

54

54

51

64

59

62

66

63

61

63

65

66

74

62

77

72

81

68

82

65

Carleton

49

47

58

55

57

60

59

55

58

62

60

68

55

72

66

75

64

77

59

65

Vanderbilt

47

61

57

61

64

60

59

61

63

65

74

60

79

70

82

65

81

64

68

81

UCLA

64

61

65

68

62

62

64

66

70

77

63

81

74

85

68

85

67

70

83

87

Davidson

46

45

49

50

44

48

53

49

58

46

64

57

66

57

69

50

56

67

74

60

U Texas

50

55

55

49

53

58

55

65

51

70

63

74

61

75

55

61

74

79

65

66

NYU

56

56

48

54

58

56

68

50

73

65

77

61

79

56

62

76

82

66

69

66

Tufts

51

42

48

54

49

61

44

68

60

71

58

73

50

57

71

78

62

63

61

83

W ash & Lee

44

47

52

48

56

45

62

56

64

55

67

48

54

65

71

60

58

57

75

80

U M ichigan

54

59

58

70

52

74

66

79

62

79

58

63

77

83

68

70

67

87

96

98

Vassar

56

51

62

47

68

61

71

58

73

51

58

71

78

63

64

62

82

89

90

79

Grinnell

44

53

42

59

53

61

53

64

46

53

62

69

57

55

55

72

77

76

68

68

U Illinois

64

45

70

61

74

58

75

51

59

74

80

64

65

62

85

94

96

81

76

70

Carnegie Mell

34

57

50

61

51

65

40

49

64

71

55

53

52

75

85

85

71

69

63

76

U M aryland

72

64

75

61

76

55

62

74

80

65

67

64

85

93

95

82

78

72

86

79

W illiam M ary

44

53

46

58

33

43

56

64

49

46

45

69

76

76

64

63

57

69

63

72

Bowdoin

59

51

62

41

49

62

68

54

52

51

73

80

80

69

68

62

74

68

76

79

W ake Forest

44

55

29

40

54

62

46

42

42

67

74

74

62

62

55

68

62

71

75

84

Claremont

59

43

49

58

64

52

51

50

68

72

71

63

64

59

68

63

71

74

80

74

M acalester

28

37

50

57

43

38

38

61

67

65

56

58

51

61

56

67

68

78

69

77

Colgate

58

71

77

62

63

61

82

89

90

78

75

69

83

76

82

87

93

89

95

96

Smith

62

69

55

54

52

73

78

77

68

69

62

73

68

75

78

85

80

85

88

60

U M iami

58

44

40

40

61

67

65

57

59

52

62

57

66

68

78

69

76

81

49

63

Haverford

37

32

32

53

57

56

49

52

45

54

49

59

61

71

62

69

72

42

56

66

31
Table 4: Share of Draws in W hich College in the Row is Ranked Higher than the College Various Places Below It
Num ber of Places Below
College

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

M t Holyoke

48

47

67

72

70

62

64

57

67

62

70

72

80

74

80

83

55

67

77

68

Connecticut C.

48

72

80

79

68

66

60

73

67

74

80

86

81

87

90

58

71

85

71

83

Bates

72

79

79

68

66

61

72

67

74

78

86

80

86

88

58

70

83

71

82

90

Kenyon

53

51

44

48

42

49

46

56

56

68

58

64

70

39

53

64

56

61

73

76

Emory

46

40

45

38

46

41

55

54

68

56

65

72

34

50

62

55

61

76

76

62

W ash. U

43

47

41

49

44

57

59

73

59

70

77

36

53

67

57

65

81

80

65

75

Occidental

53

46

55

50

61

62

73

64

72

76

42

57

69

60

67

79

80

68

76

81

Bryn M awr

44

52

48

56

57

66

58

63

67

41

53

63

57

60

70

72

62

70

71

78

SM U

58

54

63

63

72

64

70

74

47

60

69

62

67

76

78

69

76

78

85

88

Lehigh

46

57

58

68

58

65

71

38

53

65

57

62

75

77

64

73

77

87

90

89

Holy Cross

60

60

70

62

67

72

42

56

67

59

65

76

77

66

74

77

85

89

88

87

Reed College

49

58

49

54

59

33

46

54

51

51

61

66

55

64

64

71

77

77

77

78

RPI

62

50

57

63

32

47

57

52

54

68

71

58

69

71

81

87

85

83

85

85

Florida State

38

43

49

24

37

45

43

41

53

60

47

59

58

67

74

74

74

76

75

68

Colby

57

64

31

47

57

51

54

68

71

57

68

70

81

87

86

84

86

86

79

88

UCSB

59

25

42

52

48

47

63

68

53

65

68

81

88

85

83

85

85

77

88

82

GW U

22

37

44

43

41

56

62

48

61

60

73

81

80

77

80

80

70

82

75

76

Fordham

63

73

64

72

81

82

71

78

82

89

92

91

90

91

91

87

93

90

89

89

Sarah Lawr.

59

53

56

66

70

60

68

69

76

81

81

81

82

81

76

83

79

79

80

91

Bucknell

47

46

60

65

51

63

63

74

80

80

78

81

81

73

82

76

78

78

92

94

Catholic U

51

60

64

54

63

62

67

73

74

73

74

73

69

75

70

72

73

84

87

78

U Colorado

64

69

54

66

68

80

86

84

83

84

84

77

86

81

81

81

94

97

87

57

U W isconsin

58

44

57

54

67

75

75

73

76

76

67

79

70

72

72

90

93

80

48

82

Arizona St

39

51

45

53

63

63

64

65

64

57

66

58

61

63

80

85

71

43

74

83

W heaton (IL)

60

59

67

73

74

73

75

74

69

76

69

72

73

86

89

79

53

82

88

87

Rose Hulman

46

52

59

60

61

62

61

56

62

55

58

60

75

79

68

42

72

78

77

48

UCSC

61

71

69

70

71

71

63

73

65

67

69

87

91

77

45

80

89

86

53

94

Boston U

64

63

63

66

65

56

68

55

61

62

85

91

73

40

77

87

84

45

95

81

32
Table 4: Share of Draws in W hich College in the Row is Ranked Higher than the College Various Places Below It
Num ber of Places Below
College

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

Ucsd

51

53

55

53

46

55

43

49

52

77

83

65

34

69

79

78

37

89

74

89

Tulane

52

53

52

45

54

42

48

52

73

80

63

33

68

77

76

37

87

73

87

61

U Richmond

50

49

44

51

41

46

49

68

74

59

33

65

73

73

36

82

69

83

60

CW RU

47

43

50

39

45

49

68

75

60

32

65

74

72

35

84

70

85

59

61

Trinity Coll

44

52

40

46

51

72

78

62

33

66

76

74

36

85

71

86

Colorado Coll

58

48

53

56

75

80

67

37

69

78

77

41

86

74

87

64

Indiana U

38

45

48

71

77

60

32

66

75

74

34

85

70

86

60

Penn State

56

59

81

87

69

37

73

83

81

42

92

77

91

66

American U

54

75

80

65

34

69

78

76

39

88

74

87

63

Hamilton

69

75

61

33

65

73

72

36

82

69

84

60

47

U W ashington

57

44

21

51

57

59

21

70

55

74

U Rochester

38

17

46

51

52

17

64

50

70

43

Lewis&Clark

26

55

61

61

28

72

59

75

51

W heaton (M a)

77

82

82

56

87

79

88

72

Clark

55

56

25

64

53

68

47

Skidmore

52

18

62

49

67

43

Purdue

19

60

48

64

41

Colorado St

88

78

90

69

35

Syracuse

39

58

Scripps

66

44

Loyola U

31

33

are, however, a mix of public and private, small and large, colleges and universities. They are
also more geographically diverse. They include private schools from middle and southern
states: University of Chicago, Furman, Carleton, Davidson, Northwestern, Oberlin, Vanderbilt.
There are also several public universities: UC - Berkeley, UCLA, Georgia Tech, U Texas, North
Carolina.
The colleges ranked from 41 to 106 include a good number of states' "flagship"
universities, numerous liberal arts colleges, several private universities, and a few more
institutes of technology.17 As a rule, the lower one goes in the revealed preference ranking, the
less distinct is a college's desirability from that of its immediate neighbors in the ranking.
Among the top ten colleges, we generally enjoy confidence of about 75 percent that a college is
ranked higher than the college listed one below it. To achieve the same level of confidence for
colleges ranked eleven to twenty, we need to compare a college with one that is about four
places below it. To achieve 75 percent confidence with the colleges ranked twenty to 30, we
need to compare a college with one that is about six places below it. In short, our confidence
about the exact rank order falls with colleges' measured desirability. There are two reasons
why our confidence falls. First, there may be less consensus among students about colleges'
desirability as we move from the best known colleges to those with less wide reputations.
Second, owing to the nature of our sample, our data are thickest for the most selective colleges.
We did a simple test to determine whether data thickness was primarily responsible for the fall
off in confidence: we randomly selected only 20 observations per college. With these data, we
found that about two-thirds of the drop-off in confidence disappeared. That is, if our data
were equally representative for all colleges, our confidence about the exact rank order would
probably fall only about one third as fast as it does.

17

The students in our sam ple who had a Florida resident as a parent were the first cohort to
receive Florida A-Plus Scholarships, which allowed them to attend public universities in Florida for free.
The initiation of the scholarships generated considerable excitement and may have raised the ranking of
public universities in Florida, such as Florida State, among students in our sample.

34

B. Comparing Measures of Revealed Preference
For the colleges that are in the top twenty based on revealed preference, Table 5 shows
what their rankings would be if they were based on, respectively, the admissions and
matriculation rates. We use crude admissions and matriculation rates from the College
Board's Standard Research Complication, the same data as form the "Common Data Set"
published on colleges' websites and used by college guides like U.S. News.
Table 5
A Comparison of the Revealed Preference Ranking of Colleges
and Rankings Based on the Crude Admissions and Matriculation Rates
National Rank Based On:
Revealed Preference (based on Admissions Rate Matriculation Rate
Matriculation Tournaments)
Harvard
1
4
139
Yale
2
12
309
Stanford
3
7
297
Cal Tech
4
9
854
MIT
5
13
422
Princeton
6
5
266
Brown
7
14
561
Columbia
8
6
438
Amherst
9
19
916
Dartmouth
10
20
647
Wellesley
11
23
492
U Penn
12
104
794
U Notre Dame
13
58
459
Swarthmore
14
28
1016
Cornell
15
45
649
Georgetown
16
22
703
Rice
17
25
996
Williams
18
29
797
Duke
19
32
859
U Virginia
20
76
630
Notes: Left-hand column shows rank based on Table 3. The admissions and matriculation
rates are based on the Common Data Set, used by most college guidebooks.
Looking at Table 5, we observe that most of the top twenty colleges based on revealed
preference are not in the top twenty based on the admissions and matriculation rates. Indeed,

35

the admissions rate puts 10 of them outside the top twenty and the matriculation rate puts all
of them outside the top 100. Clearly, there are many colleges with low admissions rates or high
matriculation rates that are not perceived to be highly desirable. Apart from convenience, we
are unable to frame an argument for why the crude rates have any advantage over the
procedures for revealing preference that we outline in this paper.
C. Regional Rankings
Table 6 shows the rankings we obtain if we estimate the matriculation model separately
for students from each of the nine census divisions of the U.S. The nine divisions are:
Division 1: Connecticut, Massachusetts, Maine, New Hampshire, Rhode Island, Vermont;
Division 2: New Jersey, New York, Pennsylvania;
Division 3: Illinois, Indiana, Michigan, Ohio, Wisconsin;
Division 4: Kansas, Minnesota, Missouri, Nebraska;
Division 5: D.C., Florida, Georgia, Maryland, North Carolina, South Carolina, Virginia;
Division 6: Alabama, Kentucky, Tennessee;
Division 7: Arkansas, Louisiana, Oklahoma, Texas;
Division 8: Arizona, Colorado, Idaho, Montana, New Mexico, Nevada, Utah, Wyoming;
Division 9: California, Hawaii, Oregon, Washington.
We make no great claims for these regional rankings because the sample for each region
is small. Rather, we show Table 6 so that the reader can see how the regional rankings
combine to form a truly national ranking at the top. Because the regional samples are small,
some schools do not get ranked in some regions, and thus we have left spaces where Elo points
suggest that a school ranked in other regions is missing. For instance, in division 6 (Alabama,
Tennessee, Kentucky), neither Cal Tech nor Stanford is ranked. Because the regional samples
are small, we merely group schools outside of the top 30 (see note below the table).
Looking at Table 6, the most noteworthy thing is the great consistency of the ranking of
the top ten institutions. Each region reproduces the national ranking, with a couple of
exceptions. In region 7 and 9, Stanford is ranked above MIT, whereas MIT is usually ranked
higher. Also, Amherst and Dartmouth often trade places in the rankings. Among the

36

Region 1:
CT, MA, ME,
NH, RI, VT
1 Harvard
2 Cal Tech
3 Yale
4 MIT
5 Stanford
6 Princeton
7 Brown
8 Columbia
9 Dartmouth
10 Amherst
11 Wellesley
12 Notre Dame
13 U Penn
14 Swarthmore
15 Rice
16 Cornell
17 Georgetown
18 Duke
19 Williams
20 U Virginia
21 Wesleyan
22 Harvey Mudd
23 Northwestern
24 Pomona
25 U Chicago
26 Middlebury
27 Johns Hopkins
28 USC
29 Berkeley
30 Georgia Tech

Region 2:
NJ, NY, PA
Harvard
Cal Tech
Yale
MIT
Princeton
Stanford
Brown
Columbia
Dartmouth
Amherst
Wellesley
Notre Dame
U Penn
Cooper Union
Swarthmore
Cornell
Georgetown
Rice
Duke
Williams
U Virginia
Harvey Mudd
Wesleyan
Northwestern
Pomona
U Chicago
Middlebury
Berkeley
Johns Hopkins
Georgia Tech

Table 6: An Example of Regional Preference Rankings of Colleges
Ranking among Students From:
Region 3:
Region 4:
Region 5:
Region 6:
Region 7:
IL, IN, MI, OH, KS, MN, MO,
DC, FL, GA,
AL, KY, TN
AR, LA, OK, TX
WI
NE
MD, NC, SC, VA
Harvard
Harvard
Harvard
Harvard
Harvard
Cal Tech
Cal Tech
Cal Tech
Cal Tech
Yale
Yale
Yale
Yale
Yale
MIT
MIT
MIT
MIT
Stanford
Stanford
Princeton
Stanford
MIT
Princeton
Stanford
Princeton
Princeton
Princeton
Brown
Brown
Brown
Brown
Brown
Columbia
Amherst
Columbia
Columbia
Columbia
Amherst
Dartmouth
Dartmouth
Dartmouth
Dartmouth
Dartmouth
Notre Dame
Amherst
Wellesley
Amherst
Wellesley
U Penn
Notre Dame
U Penn
Wellesley
U Penn
Swarthmore
Wellesley
Amherst
U Penn
Notre Dame
Williams
U Penn
Duke
Notre Dame
Swarthmore
Cornell
Swarthmore
Swarthmore
Cornell
Cornell
Duke
Cornell
Cornell
Rice
Duke
Georgetown
Duke
Georgia Tech Duke
Rice
U Virginia
Georgetown
Williams
Williams
Williams
Rice
Rice
Georgetown
Georgetown
Georgetown
Wesleyan
Williams
Rice
U Virginia
U Virginia
USC
Harvey Mudd U Virginia
Wesleyan
Wesleyan
Northwestern
U Virginia
Wesleyan
Northwestern
Harvey Mudd U Chicago
Wesleyan
Claremont
Berkeley
Northwestern
Pomona
Northwestern
Northwestern Georgia Tech
Pomona
Georgia Tech
Pomona
Fordham
USC
Middlebury
Johns Hopkins Georgia Tech
Berkeley
U Chicago
Johns Hopkins U Texas
Berkeley
USC
Johns Hopkins
Berkeley
UNC
Middlebury
Pomona
Pomona
USC
Vanderbilt
U Chicago
U Chicago
Middlebury
U Chicago
Carleton
Johns Hopkins UNC
U Texas
U Texas
Oberlin
USC
Vanderbilt
UNC

Region 8:
AZ, CO, ID, MT,
NM, NV, UT, WY
Harvard
Cal Tech
Yale
Stanford
Princeton
Brigham Young
Brown
Columbia
Dartmouth
U Penn
Amherst
Notre Dame
Williams
Swarthmore
Cornell
Duke
Rice
U Virginia
Georgetown
Wesleyan
Pomona
Middlebury
Berkeley
Northwestern
USC
U Chicago
Georgia Tech
UNC
Johns Hopkins
Oberlin

Region 9:
CA, HI, OR,
WA
Harvard
Cal Tech
Yale
Stanford
MIT
Princeton
Brown
Columbia
Dartmouth
Amherst
U Penn
Wellesley
Notre Dame
Cornell
Swarthmore
Georgetown
Duke
Rice
Cooper Union
Williams
U Virginia
Harvey Mudd
Wesleyan
Pomona
Berkeley
Northwestern
Johns Hopkins
USC
U Chicago
Middlebury

37
Notes for Table 6
Next 30 colleges, for each region:
Region 1 (CT, MA, ME, NH, RI, VT): UNC, Oberlin, Vanderbilt, U Florida, Barnard, Carleton, Furman, George Mason, Davidson, U Michigan, UCLA, NYU,
Tufts, Claremont Mckenna, U Illinois, Vassar, Washington and Lee, Grinnell, Pitzer, Carnegie Mellon, U Maryland, Wake Forest, Kenyon, Bowdoin, William and
Mary, Colgate, SMU, Macalester, U Miami.
Region 2 (NJ, NY,PA): USC, U Texas, UNC, Carleton, Barnard, Vanderbilt, Oberlin, Davidson, Washington and Lee, UCLA, NYU, Tufts, U Michigan, U Florida,
Furman, Vassar, Grinnell, U Illinois, St. John's, Bowdoin, U Maryland, Kenyon, William and Mary, Carnegie Mellon, Wake Forest, Claremont Mckenna, Smith,
Colgate, Pitzer, Macalester.
Region 3 (IL, IN, MI, OH, WI): UNC, Claremont Mckenna, Fordham, Carleton, USC, Vanderbilt, Oberlin, Davidson, Barnard, UCLA, U Illinois, SMU,
Washington and Lee, Bradley, U Florida, U Michigan, Tufts, Vassar, NYU, Grinnell, U Missouri, Wake Forest, Bowdoin, Carnegie Mellon, Illinois Wesleyan, U
Oregon, Haverford, Macalester, Smith, William and Mary.
Region 4 (KS, MN, MO, NE): Washington and Lee, Vassar, Davidson, Tufts, Furman, Bowdoin, Colgate, Grinnell, U Michigan, New York, Rhodes, U Illinois,
SMU, Haverford, Macalester, Kenyon, Wake Forest, U Missouri, Connecticut College, U Maryland, Carnegie Mellon, Bradley, Sarah Lawrence, Lehigh,
Washington U., Bates, Bucknell, College of William and Mary, U Miami, Colby.
Region 5 (DC, FL, GA, MD, NC, SC, VA): UNC, U Texas, U Florida, Fordham, Barnard, Vanderbilt, Carleton, UCLA, Davidson, Oberlin, U Michigan, Tufts,
Vassar, U Maryland, Furman, U Illinois, Washington and Lee, NYU, Grinnell, U. of the South, Bowdoin, Kenyon, Carnegie Mellon, William and Mary, Wake
Forest, Macalester, Smith, U Miami, Colgate, Haverford.
Region 6 (AL, KY, TN): Furman, Johns Hopkins, Middlebury, UCLA, U Texas, Barnard, Davidson, U the South, Wake Forest, SMU, Carleton, Oberlin, U
Michigan, U Illinois, Texas A&M, NYU, Rhodes, Vassar, Occidental, Smith, Clemson, Kenyon, Carnegie Mellon, Bowdoin, William and Mary, Bates, U Miami,
Washington and Lee, Washington U., Haverford.
Region 7 (AR, LA, OK, TX): Furman, Oberlin, Carleton, UCLA, Rhodes, Vanderbilt, Barnard, Davidson, Fordham, U Michigan, Washington and Lee, Tufts, NYU,
Wake Forest, U Illinois, Bowdoin, Vassar, Carnegie Mellon, Colgate, Smith, U Maryland, SMU, Macalester, Haverford, Washington U., Connecticut College,
Emory, Mount Holyoke, Bucknell, Bryn Mawr.
Region 8 (AZ, CO, ID, MT, NM, NV, UT, WY): Barnard, Claremont Mckenna, Carleton, Vanderbilt, UCLA, NYU, Wake Forest, Tufts, Macalester, Washington
and Lee, U Michigan, Bowdoin, U Oregon, Vassar, Colgate, U Miami, Mount Holyoke, Carnegie Mellon, Grinnell, Haverford, William and Mary, Emory, U
Missouri, Whitman, U Colorado, Washington U., Santa Clara, U. Arizona, UCSB, Occidental.
Region 9 (CA, HI, OR, WA): U Texas, SMU, UNC, UCLA, Carleton, Barnard, Oberlin, Davidson, Vanderbilt, NYU, Washington and Lee, Tufts, U Illinois, U
Michigan, U Oregon, Pitzer, Vassar, Bowdoin, Carnegie Mellon, Grinnell, Smith, Wake Forest, Macalester, Fordham, St. John's, Claremont Mckenna, William and
Mary, Haverford, Emory, Whitman.

38

institutions ranked 11 to 30, there is considerable consistency overall, and nearly all of the
changes in rank order appear to be noise, probably due to the small regional samples. The
overall impression is one of consistency: the national ranking is truly national, at least at the
top.
Regionalism is more evident in the colleges ranked 31 to 60, which are shown in the
notes below Table 6. While much of the variation in the ranking is noise at this point, owing to
the small regional samples, it is notable that Southern colleges do better in the South (U. of the
South, Clemson, and Rhodes are the most obvious), Midwestern colleges do better in the
Midwest (Bradley is the most obvious), and Western colleges do better in the West (Whitman,
Santa Clara, Occidental, and Pitzer are the most obvious). In addition, flagship state
universities are likely to show up in their region, even if not in distant regions (U Oregon, U
Colorado, and U Arizona are the most obvious). However, for the colleges ranked 31 to 60, the
overwhelming impression is that the regional rankings are not very regional. The regional
favorites never represent more than ten percent of the 30, and most of the colleges that appear
show up in every region.
Perhaps the single most interesting college in Table 6 is Brigham Young, which appears
in the top 10, between Princeton and Brown, in region 8 (which contains Utah). We have
checked and determined that, if we were to compute a Utah-specific ranking, Brigham Young
would rank even higher. The dramatic appearance of Brigham Young in the top 10 almost
certainly occurs because the college is particularly desirable in the eyes of Mormon students.18
We cannot verify this conjecture because we did not ask students about their religion, but this
leads us back to our general point about latent desirability and self-selection into applicant
pools. The reason that Brigham Young wins so many tournaments with Utah students is that it
is truly more desirable to them. Similarly, the reason that a bit of regionalism appears is that
University of the South, say, is truly more desirable to Southerners. This is not a problem we

18

The reason that Brigham Young does not appear in the national ranking is that, in our sample,
it competes in fewer than six regions.

39

need to "fix" in the national ranking. It is simply an indicator that, with sufficient data, it
would be reasonable to compute sub-rankings for identifiable groups of students with welldefined tastes. We know now that these rankings will tend to join up at the top. A benefit of
computing sub-rankings is that some colleges' performance in the national rankings depends
on the fact that they are especially popular with a well-defined set of students who self-select
into applying (think of Cal Tech). Self-selection does not appear to be an important concern
with our national ranking, except perhaps for the engineering schools. However, we speculate
that it would be appropriate to construct sub-rankings once we got much outside of this group.

VI. Conclusions
In this paper, we show how students' college choice behavior can be used to generate
revealed preference rankings of American colleges and universities. Using a data set on the
college application and matriculation choices of highly meritorious American students, we
construct examples of a national revealed preference ranking and regional revealed preference
rankings. Our procedure generates a revealed preference ranking which would be very
difficult for a college to manipulate with strategic admissions behavior.
Given the strong demand for measures of revealed preference among parents and
students, it is clear that colleges will be forced to provide some such information and college
guides like U.S. News will be forced to give substantial weight to such information. In the
absence of a revealed preference ranking method such as ours, colleges and college guides use
two flawed, manipulable proxies: the crude admissions rate and crude matriculation rate.
These proxies are not only misleading; they induce colleges to engage in distorted conduct that
decreases the colleges' real selectivity while increasing the colleges' apparent desirability, as
measured by the proxies. So long as colleges are judged based on the crude admissions and
matriculation rates, it is unlikely that all colleges will eliminate strategic admissions or roll back
early decision programs, which are key means for manipulating the proxies. Many college
administrators correctly perceive that they are in a bad equilibrium. Yet, so long as colleges'
find it advantageous to use early decision and other costly admissions strategies, the bad

40

equilibrium is likely to persist.
Gathering our data was a moderately costly undertaking for researchers, but the cost
would be a trivial share of the revenues associated with college guides. Moreover, at least
some of the data are already compiled by organizations like The College Board and the ACT, so
that gathering a highly representative sample should be very feasible. If a revealed preference
ranking constructed using our procedure were used in place of manipulable indicators like the
crude admissions rate and crude matriculation rate, much of the pressure on colleges to
manipulate admissions would be relieved. In addition, students and parents would be
informed by significantly more accurate measures of revealed preference. We close by
reminding readers that measures of revealed preference are just that: measures of desirability
based on students and families making college choices. They do not necessarily correspond to
educational quality.

41

References
Avery, Christopher, Andrew Fairbanks, and Richard Zeckhauser. The Early Admissions Game:
Joining the Elite. Cambridge, MA: Harvard University Press, 2003.
Avery, Christopher, and Caroline M. Hoxby. The College Admissions Project: Counselor Report.
Cambridge, MA: The College Admissions Project, 2000.
Avery, Christopher, and Caroline M. Hoxby. "Do and Should Financial Aid Packages Affect
Students' College Choices?" in Caroline M. Hoxby, ed. College Choice: The Economics of
Where to Go, When to Go, and How to Pay for It. Chicago: University of Chicago Press,
2004.
David, Herbert. The Method of Paired Comparisons. Oxford: Oxford University Press, 1988.
Ehrenberg, Ronald G., and James W. Monks. "The Impact of US News and World Report
College Rankings on Admissions Outcomes and Pricing Decisions at Selective Private
Institutions." National Bureau of Economic Research Working Paper Number 7227,
1999.
Elo, Arpad E. The Rating of Chessplayers, Past and Present. London: Batsford, 1978.
Glickman, Mark E. "Paired Comparison Models with Time Varying Parameters," Doctoral
thesis, Harvard University Dept of Statistics, 1993.
Glickman, Mark E. "Parameter Estimation in Large Dynamic Paired Comparison Experiments."
Applied Statistics, 48 (1999), pp. 377-394.
Glickman, Mark E. "Dynamic Paired Comparison Models with Stochastic Variances," Journal of
Applied Statistics, 28 (2001), pp. 673-689.
Good, Irving J. "On the Marking of Chess Players," Mathematical Gazette, 39 (1955), pp. 292-296.
Long, Bridget T. "Does the Format of a Financial Aid Program Matter? The Effect of State
In-Kind Tuition Subsidies," National Bureau of Economic Research Working Paper
Number 9720, 2003.
Luce , R. Duncan. Individual Choice Behavior. Wiley: New York, 1959.
Thurstone, L.L. "A Law of Comparative Judgment," Psychological Review, 34 (1927), pp. 273-286.

42

Manski, Charles F., and David A. Wise. College Choice in America. Cambridge: Harvard
University Press, 1983.
Mosteller, Frederick. "Remarks on the Method of Paired Comparisons. I. The Least Squares
Solution Assuming Equal Standard Deviations and Equal Correlations," Psychometrika,
16 (1951), pp. 3-9.
Spence, Michael. "Education as a Signal," Chapter 2 in Market Signaling. Cambridge: Harvard
University Press, 1974.
Spiegelhalter, DJ, A. Thomas, N.G. Best, and W.R. Gilks WR. BUGS: Bayesian Inference Using
Gibbs Sampling, version 0.6, 1996.
Stern, Hal. "Are All Linear Paired Comparison Models Empirically Equivalent?" Mathematical
Social Sciences, 23 (1992), pp. 103-117.
Toor, Rachel. "Pushy Parents and Other Tales of the Admissions Game," Chronicle of Higher
Education, October 6 2000, p. B18.
Wall Street Journal. "Glass Floor: How Colleges Reject the Top Applicants and Boost Their
Status," Wall Street Journal, May 29, 2001.
Zermelo, Ernst. "Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der
Wahrscheinlichkeitsrechnung," Math. Zeit., 29 (1929), pp. 436-460.

