NBER WORKING PAPER SERIES

EMPIRICAL INDUSTRIAL ORGANIZATION:
A PROGRESS REPORT
Liran Einav
Jonathan D. Levin
Working Paper 15786
http://www.nber.org/papers/w15786

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
February 2010

This paper was prepared for the Journal of Economics Perspectives. We thank Ran Abramitzky, David
Autor, Tim Bresnahan, Amy Finkelstein, Phil Haile, Ali Hortacsu, Chad Jones, Richard Levin, Aviv
Nevo, Ariel Pakes, and Mike Whinston for useful comments and discussions, and especially Tim Taylor
for helping us incorporate many of the comments into the final draft. We gratefully acknowledge
research support from the National Science Foundation and the Stanford Institute for Economic Policy
Research. The views expressed herein are those of the authors and do not necessarily reflect the views
of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2010 by Liran Einav and Jonathan D. Levin. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.

Empirical Industrial Organization: A Progress Report
Liran Einav and Jonathan D. Levin
NBER Working Paper No. 15786
February 2010
JEL No. C8,L0
ABSTRACT
The field of Industrial Organization has made dramatic advances over the last few decades in developing
empirical methods for analyzing imperfect competition and the organization of markets. We describe
the motivation for these developments and some of the successes. We also discuss the relative emphasis
that applied work in the field has placed on economic theory relative to statistical research design,
and the possibility that a focus on methodological innovation has crowded out applications. We offer
some suggestions about how the field may progress in coming years.

Liran Einav
Stanford University
Department of Economics
579 Serra Mall
Stanford, CA 94305-6072
and NBER
leinav@stanford.edu
Jonathan D. Levin
Stanford University
Department of Economics
579 Serra Mall
Stanford, CA 94305-6072
and NBER
jdlevin@stanford.edu

Introduction
The field of industrial organization has made dramatic advances over the last few decades
in developing empirical methods for analyzing imperfect competition and the
organization of markets. These new methods have diffused widely: into merger reviews
and antitrust litigation, regulatory decision-making, price-setting by retailers, the design
of auctions and marketplaces, and into neighboring fields in economics, marketing and
engineering. Increasing access to firm-level data and in some cases the ability to
cooperate with firms or governments in experimental research designs is offering new
settings and opportunities to apply these ideas in empirical work.

This essay begins with a sketch of how the field has evolved to its current state, in
particular how the field’s emphasis has shifted over time from attempts to relate broad
aggregate measures across industries toward more focused studies of individual
industries. The second and primary part of the essay describes several active areas of
inquiry. While in some sense a survey, our goal is not to be comprehensive but to
highlight key ideas and illustrate how modern research has approached different
problems. We also discuss some of the broader impacts of this research and places where
research efforts have been more or less successful. The last section steps back to offer a
broader perspective. We address some current debates about research emphasis in the
field, and more broadly about empirical methods, and offer some thoughts on where
future research might go.

What Got Us Here?
Industrial organization is concerned with the structure of industries in the economy and
the behavior of firms and individuals in these industries. The field has historically
focused on how markets depart from idealized conditions of perfect competition, whether
because of scale economies, transaction costs, strategic behavior or other factors. From
an empirical perspective, this leads to questions about how competition plays out in
different markets, and how it relates to industry structure. Not surprisingly, these

2

questions overlap with public policy issues, such as the appropriate antitrust stance
toward concentrated industries or the design of regulatory mechanisms for industries with
scale economies.

Modern research in industrial organization has evolved largely in response to two
challenges that plagued the field into the 1970s. The first was a lack of compelling
theoretical models for studying imperfectly competitive markets. This problem was
largely reversed by the game theory revolution of the 1980s, which permitted much
sharper modeling and analysis of problems such as product differentiation, network
effects, barriers to entry, pricing strategies, and the effect of asymmetric information in
product markets (Tirole, 1988). Indeed once this line of research began to advance,
models proliferated so rapidly that some prominent members of the field were lamenting
their over-abundance (Fisher, 1989; Peltzman, 1991)! 1

The second challenge for empirical work was a lack of good data and convincing
empirical strategies for evaluating hypotheses about competition or industry structure.
The most active strand of empirical research into the 1980s consisted of cross-industry
regression analyses that attempted to link market structure across industries with market
outcomes. This agenda can be traced back at least to Bain (1951, 1956), and is sometimes
referred to as the “Structure-Conduct-Performance” paradigm. In a typical study, a
researcher might use data on a cross-section of industries to regress an outcome measure
such as accounting profit on an industry concentration measure such as the combined
market share of the four largest firms. Researchers of course recognized that
concentration might depend on many of the same factors that influenced profitability,
creating an endogeneity problem. More sophisticated studies might try to “fix” the
problem by assuming that industry concentration derived from “exogenous” barriers to
entry, for instance technological scale economies or fixed costs such as advertising or
research and development.
1

The central issue at the time was that many of these models seemed to be sensitive to hard-to-assess
assumptions about consumer preferences, asymmetric information, and the ability of firms to make
strategic commitments. This sensitivity turns out to have had an impact on subsequent empirical research,
in that it made researchers appreciate the potential importance of even small details in the underlying
strategic environment.

3

While the broad question of what makes industries more or less competitive is an
important one, the cross-industry approach had notable limitations. Many studies relied
on data consisting of a small number of industry aggregates, with empirical measures that
proxied imperfectly for the relevant economic quantities. For instance, a common
concern was that accounting measures of profits available in the data might not be an
accurate gauge of true economic returns. Even more troublesome, most studies in this
literature lacked convincing strategies for identification. What is needed to identify the
type of causal effect described above is exogenous variation in industry concentration.
While cross-industry differences in advertising-to-sales ratios or capital intensity may
correlate with industry concentration, it is hard to argue that they are unrelated to factors
affecting firm profits, and hence qualify as useful instrumental variables. This problem
was well summarized by Schmalensee (1989, p. 954), who began his definitive review of
the cross-industry literature by acknowledging that “essentially all variables that have
been employed in such studies are logically endogenous.”

Both the concerns about cross-industry regression models and the development of clearer
theoretical foundations for analyzing imperfect competition set the stage for a dramatic
shift in the 1980s toward what Bresnahan (1989) coined the “New Empirical Industrial
Organization”. Underlying this approach was the idea that individual industries are
sufficiently distinct, and industry details sufficiently important, that cross-industry
variation was often going to be problematic as a source of identification. Instead, the new
wave of research set out to understand the institutional details of particular industries and
to use this knowledge to test specific hypotheses about consumer or firm behavior, or
estimate models that could be used for counterfactual analysis, such as what would
happen following a merger or regulatory change.

The current state of the field reflects this transition. Today, most of the influential
research in empirical industrial organization looks extensively to economic theory for
guidance, especially in modeling firm behavior. Studies frequently focus on a single
industry or market, with careful attention paid to the institutional specifics, measurement

4

of key variables, and econometric identification issues. Advocates of this approach argue
that when successful, it combines the conceptual clarity of economic theory with
convincing empirical measurement, and that the focus on individual industries offers the
best opportunity to understand the competitive mechanisms at work.

Of course, industry studies also have their drawbacks. They may lead to narrow analyses
and sometimes leave researchers reluctant to generalize their findings to other contexts
where the institutions may be different. As a result, the broader take-aways often tend to
be either qualitative insights — for example that it is possible for entry to have a
substantial effect on prices, or for asymmetric information to matter significantly in
market outcomes — or empirical methods that can be applied in multiple settings. In
addition, critics sometimes argue that the econometric models used in some studies,
which often rely heavily on equilibrium assumptions, can lead to non-transparent
analyses that obscure the link between the estimated parameters and the underlying
variation in the data. We return to these points in the last section.

Where Has The Action Been? A Brief Tour
In this section, we describe a few active areas of empirical research in industrial
organization. We start with the problem of estimating consumer demand, which is a key
input for almost any study of market competition. We then discuss models of short-run
price competition, both in traditional markets where firms post prices and in bidding
markets where firms compete in auctions. Finally, we turn to the problem of longer-run
competition, where entry, exit and investment decisions can shape market structure and
the competitive landscape.

In each case, we discuss some of the empirical methods that have been developed, and a
few applications that usefully illustrate specific points. One point to emphasize is that
although there is no recipe for empirical research in industrial organization, many papers
share common themes. Modern research in the field often begins with a theory of market
equilibrium, which can be more or less explicit depending on the application. Often

5

researchers rely explicitly on assumptions about optimization or equilibrium to draw
inferences about underlying parameters. In other cases, the theory is used to derive
testable predictions about the data or to interpret estimates of causal effects. Careful
attention is paid to the institutional context and market details. One broad lesson is that
looking across industries these details can be very important.

Estimating Demand in Imperfectly Competitive Markets
Many studies of imperfect competition begin by describing consumer behavior, often by
estimating a model of consumer demand for the products of the relevant industry.
Consumer demand may be interesting in its own right --- to understand what consumers
value and how they substitute between products as prices or product offerings change, to
assess the welfare effects of mergers or new products, or to measure how information or
advertising affect consumer decisions. Demand elasticities are also a critical input for
identifying the degree of market power that firms can exercise. Many of the focal
advances in this area have come in the form of novel econometric models that in
principle can be applied broadly across markets, as well as in non-market environments.

The typical situation in most industries is that consumers face a choice of products that
vary along different dimensions. Product differentiation bestows firms with a degree of
market power. For instance, one factor weighing into Apple’s pricing of the iPhone is that
some consumers just prefer the iPhone to comparable phones made by Palm or Nokia.
Moreover, consumers who value the iPhone interface may be different from those who
value the Blackberry’s ability to synch with corporate email servers. The problem
becomes more subtle when we recognize that Apple sells several iPhone versions, and
that consumers may be able to delay their purchase and wait for prices to fall. This means
that Apple’s pricing incentives will depend not just on a single demand elasticity with
respect to a single Apple price, but on a set of elasticities that describe how customers
will shift their purchasing as Apple changes its pricing schedule.

6

Assessing the demand for differentiated products, therefore, calls for an analysis that
relates the prices of a set of competing products, and perhaps other strategic variables
such as advertising, to the quantities sold of those products. This task poses an immediate
challenge. Even a simple linear demand system for an industry with n products will have
n2 price coefficients, and estimating these coefficients will require distinct variation in
each of the n prices. The more products, the greater the identification challenge,
particularly because many attractive sources of variation such as cost shocks may
simultaneously affect all prices. Other idiosyncratic sources may affect just a few prices,
leaving the effect of other prices not identified.

Work on demand modeling, therefore, has centered on the trade-off between allowing
flexible substitution patterns and the lack of variation in typical data that allows such
substitution patterns to be flexibly identified. One strategy is to divide products into
segments and estimate a model that restricts substitution patterns across segments but
allows flexibility within segments. Hausman (1997) applies this approach in studying the
demand for ready-to-eat cereals, dividing products into adult, family and kids’ cereals.
Another strategy is to describe products in terms of a relatively small number of
characteristics, and assume that consumers trade off price and other characteristics in a
way that is uniform across products. Berry, Levinsohn and Pakes (1995) take this
approach in modeling the demand for autos by describing cars in terms of their size,
horsepower, and gas mileage, as well as a quality characteristic that is not captured by the
three observable attributes.

These techniques have been highly influential. They are commonly used in antitrust
work, both in merger reviews to define the scope of the market and as an input to
simulations, and in antitrust ligation to determine damages from excessively high prices.
In recent years, these techniques have diffused into neighboring fields in economics,
including trade, education, housing, health, and environmental economics. Because they
allow researchers to map demand estimates to welfare, they also have been influential in
debates over adjusting the consumer price index to account for new goods and
substitution bias.

7

In practice, these models of consumer demand commonly are estimated using either
individual choice data or aggregate data on prices and market shares. Estimation requires
variation that will identify the demand response to price and product changes. A standard
concern is that because prices in a market tend to respond to demand shifts, there can be a
simultaneity problem in trying to infer how purchasing decisions respond to price
changes. This is the imperfect competition analogue to the textbook demand and supply
simultaneity problem. Just as in the textbook setting, researchers frequently adopt an
instrumental variables strategy.

In part because it is hard to find independent movement of many product prices, some of
the most popular identification strategies rely on restrictions across equations in the
demand system. One such approach is to use a product’s price in other markets as an
instrumental variable, under the theory that cross-market correlation in the price of a
given product, conditional on observed demand characteristics, will be due to common
cost factors rather than unobserved features of demand. An alternative is to instrument for
prices using the non-price characteristics of competing products, which proxy for the
degree of competition.

Neither is a perfect solution, so the source of price variation and its power has to be
evaluated in each application. Take the latter approach involving the non-price
characteristics of competing products. In the case of the iPhone, the relevant variation
might arise from competing firms introducing touch screens, creating additional
competition for Apple and leading it to reduce the iPhone price. While the story is not
implausible, caution is clearly warranted. For instance, if competitors were themselves
responding to the surprisingly high demand for iPhones, the exclusion restriction required
for a valid instrument would be violated.

Our own view is that many applications of these methods --- while they are often very
careful in clarifying the statistical conditions under which their identification strategy is
valid --- tend to be rather thin in explaining the precise source of identifying variation, in

8

arguing why the required statistical condition is likely to hold, or in providing first stage
regressions and other diagnostics.

An important development in this regard is the increased availability of proprietary data
on consumer behavior and the opportunity to engage in experiments sanctioned by firms.
For example, a recent paper by Lewis and Reiley (2009) examines a large-scale
experiment that randomized the exposure of consumers to internet advertising from a
large retailer and tracked subsequent purchasing behavior. Although the focus is not on
price elasticities, their study demonstrates the data-gathering capabilities of new
technologies and the increased willingness of firms to cooperate in academic research.
These trends are likely to create many opportunities for future research.

One point to recognize, however, is that experimental measurement is not a panacea. In a
typical demand setting, while experimental measurements of the consumer response to a
specific change in prices or information are informative, they are only a starting point.
Explicitly modeling consumer choices provides a way to link different behavioral
responses and connect them to underlying parameters of interest, 2 as well as a way to
translate estimated behavioral responses into statements about consumer welfare. So
while we imagine that better and richer data will allow researchers to obtain more precise
and convincing estimates of consumer responses to prices, such improved estimates will
complement rather than substitute the demand techniques described above. We return to
these points in the final section.

Market Power and Price Competition
A long-standing and central problem in industrial organization is the extent to which
market outcomes reflect the exercise of market power or some form of implicit or explicit
2

One example of this is Hendel and Nevo (2006), who point out that short-run estimates of demand
elasticities can be misleading if consumers respond to sales by stockpiling non-perishables. Instead, they
develop a model that relates estimated short-run elasticities to long-run elasticities, which are more relevant
for the price effect of mergers. Another example is Cohen and Einav (2007), who show how data on
insurance choices can be used to estimate risk preferences under the assumption that consumers maximize
expected utility from wealth.

9

collusion. Porter’s (1983) study of nineteenth century railroad cartels, Bresnahan’s (1987)
study of the automobile industry and Nevo’s (2001) study of the breakfast cereal industry
are classic analyses of market power. The approaches taken in these and related papers
provide the basic toolkit for marrying demand and supply in an equilibrium analysis of
imperfectly competitive markets.

Underlying each of these studies is a model of market equilibrium. The most common
model involves Bertrand-Nash price competition with differentiated products. In
equilibrium, each firm sets its price to equal marginal cost plus a mark-up that depends
on the semi-elasticity of the firm’s demand curve, taking the prices of competing
products as given. These equilibrium conditions allow a researcher who has obtained
estimates of consumer demand and firm costs to compute equilibrium prices, or test
hypotheses about non-cooperative pricing behavior, perhaps against alternative
behavioral assumptions such as collusive pricing.

In a typical study, a researcher might start with prices and quantities from a cross-section
or panel of markets. Often the first step is to estimate market demand, for instance using
one of the strategies described in the previous section. On the supply side, researchers
sometimes have access to accounting data on costs, although as emphasized by
Bresnahan (1989), accounting practices generally are not geared toward reporting the
economic notion of marginal cost that the theory implies should be relevant. An
alternative is to infer costs from observed prices by relying on an assumption of profit
maximization (Rosse, 1970). To do this, the researcher computes the optimal markup
using estimated demand elasticities, and subtracts it from the observed price to obtain an
estimate of marginal cost. This approach relies on the strong assumption that firms are
setting profit-maximizing prices, but it is often employed due to its elegance and the fact
that it does not require direct cost data. Our view is that at times it can be a bit too
seductive, in the sense that researchers sometimes shun available cost data on the grounds
that it is imperfect even when it could be quite informative or at least complementary. A
few studies, including Nevo (2001) and Hortacsu and Puller (2008), combine direct and

10

indirect cost measurement to cross-check their analyses and test the hypothesis of
equilibrium pricing.

The basic framework of imperfect competition that we have just described provides a
starting point for a wide variety of analyses. Many of the early industry studies focused
on distinguishing the unilateral exercise of market power from collusive alternatives, or
measuring the price and welfare impacts of mergers, taxes, or new goods. More recent
work has incorporated and studied the effects of search costs, price discrimination, retail
sales and consumer stockpiling, adverse selection effects, and the use of non-price
strategies to attract customers. As we now illustrate, this work can be highly diverse,
linked by the conceptual framework of imperfect competition but not necessarily
constrained to “traditional” product markets.

One example is a recent paper by Ellison and Ellison (2009), which analyzes how
internet price comparison engines affect competition among a set of internet retailers.
They use carefully documented price variation to show that the price ranking provided by
the search engine leads to dramatic consumer price sensitivity. This sensitivity gives
firms an incentive to engage in “obfuscation” to manipulate consumer search and in this
way to relax what would otherwise be cut-throat price competition. A second example is
the work by Gentzkow and Shapiro (2009), who construct a measure of newspaper
“slant”, and estimate consumer demand for it using variation in the distribution areas of
local newspapers. They then use the estimated demand as an input in a model of optimal
choice of “slant” by newspaper owners. A final example comes from our own work on
consumer credit markets (Einav, Jenkins and Levin, 2008). In that work, we start by
using discrete shifts in pricing to estimate credit demand, in this case for subprime auto
loans. We then incorporate the demand estimates into a model of loan pricing in order to
study screening and credit terms as set by a lender with market power.

A key point to emphasize about these applications is that while the use of detailed data
and relatively transparent identification are important, the results themselves are most
interesting when viewed through the lens of a particular theoretical model. As with any

11

empirical study, the exact results apply to a particular setting at a particular time, and we
have already argued that across the broader economy there is a great deal of
heterogeneity in the way markets and industries operate. Thus, what makes the analyses
meaningful beyond a narrow context is that they illuminate theoretical mechanisms —
manipulation of consumer search, catering to ideological preferences, or pricing under
asymmetric information — that will also operate in other markets at other times.

While we have focused on academic studies, one notable effect of the methods described
above has been to shift the standards for empirical evidence in merger reviews and
antitrust litigation. Thirty years ago, it was common for antitrust arguments to rest on
simple summary measures of industry structure such as concentration ratios and
Herfindahl-Hirschman indices. Nowadays, the Department of Justice and the Federal
Trade Commission, which are tasked with reviewing proposed mergers, commonly
undertake sophisticated econometric studies to define industry boundaries and assess the
likelihood of price increases or collusive behavior following a merger. These exercises
often draw on academic research, and in turn have motivated the development of new
empirical models.

Competition in Auction Markets
The section above described models of imperfect competition that apply most directly to
traditional consumer markets where firms post prices. Many intermediate goods markets
are bidding markets. For example, firms and governments often solicit bids to supply
goods or services, or use auctions to sell resources in limited supply. Though not always
appreciated, the connection between bidding markets and traditional price competition is
very close. In recent years, economists have developed a set of empirical methods for
analyzing auction markets that in many ways parallel the imperfect competition
framework discussed above (Athey and Haile, 2006; Hendricks and Porter, 2007).

To see the connection, observe that in an auction context, firms bidding to supply a
product or service set prices in a way that trades off a higher probability of winning

12

against a higher margin if they do win, just as a firm setting the price of its product in a
consumer market trades off the prospect of higher sales against having a higher margin.
In an important contribution, Guerre, Perrigne and Vuong (2000) drew on this connection
to suggest how a researcher might use bidding data to infer the underlying values or costs
of bidders. Roughly, the idea is to use data from a sequence of auctions to estimate the
probability, from an individual bidder’s perspective, that a given bid will win. This is
sufficient to calculate the optimal markup, and hence to estimate the values underlying
individual bids, just as Rosse (1970) proposed to estimate costs from observed prices. 3

Athey, Levin and Seira (2008) use this approach to study whether competition is affected
by whether bidders are asked to submit sealed bids or instead compete in an open
ascending auction. They analyze data from the U.S. Forest Service, which in the 1980s
used both sealed bid and open auctions to sell timber, in certain cases randomizing how
sales would be run. Regressions exploiting this variation reveal that sealed bid auctions
tended to attract more bidders and sometimes generated higher prices. Athey, Levin and
Seira show that a model of equilibrium bidding that incorporates differences in the size
and technologies of potential competitors can match the empirical patterns. They use the
model to test the hypothesis that certain auctions may have been prone to bidder
collusion.

In addition to providing a structured environment to analyze traditional questions about
imperfect competition, auctions also provide an empirical laboratory for studying
strategic behavior in the presence of asymmetric information. A pioneering example is
Hendricks and Porter’s (1988) study of government auctions for offshore oil drilling
rights. In these auctions, firms try to gauge the potential oil reserves by doing seismic

3

Specifically, if bidder j has cost cj of supplying a good and believes the lowest competing bid will follow
a distribution G (with density g), its optimal bid bj satisfies bj = cj + [1-G(bj)]/g(bj). This suggests an
empirical strategy in which data from a sequence of auctions is used to estimate a statistical model of bids.
The estimate then proxies for G (and g), permitting the researcher to associate each observed bid with an
underlying cost of supplying that makes the bid profit-maximizing. The connection to product pricing is
that a profit-maximizing firm sets its price pj so that pj= cj+ Q(pj)/Q'(pj), where cj is marginal cost and Q is
the firm’s residual demand taking as given the prices of the other firms. In the product market case, the
estimated demand system allows the researcher to proxy for Q, and hence infer costs from prices.

13

studies. Hendricks and Porter compare auctions for newly opened territory with sales of
territory that adjoins developed areas. In the former, no bidder has a particular advantage.
In the latter, the owner of an adjacent lease may have extra information. The auction
outcomes differ dramatically in these two cases, with adjacent lease-holders profiting in a
way that corresponds closely to predictions from the theory of asymmetric information.
This study and subsequent research by Hendricks, Pinkse and Porter (2003) provides
some of the sharpest empirical support for equilibrium models of asymmetric
information.

A recent line of research, on the advertising auctions conducted by Google and other
internet platforms illustrates the power of combining auction methods with large-scale
datasets and field experiments. For instance, Varian (2009) uses data from Google to
estimate the division of surplus in their sponsored search auctions by inferring values
under an assumption of optimal bidding. In related work, Ostrovsky and Schwarz (2009)
describe the design of a reserve price mechanism for Yahoo!’s sponsored search auctions.
They use a calibrated model of equilibrium bidding to derive optimal reserve prices. They
then implement reserve prices suggested by the model in a large-scale field experiment
and track the resulting increase in platform revenue.

These papers at the intersection of industrial organization and auction theory highlight the
value of the interplay between theory and research design. For example, in the Athey,
Levin and Seira paper, the authors use theory to translate the observed data (bids) into
structural parameters (estimates of bidder values) that allow out-of-sample predictions.
The variation in the data, however, is what allows a sharp test of the model. In the
Ostrovsky and Schwarz paper, insights from optimal auction theory guided the modeling
and calibration, and the experimental design provides confirmation of its usefulness.
Economic theory has an important role to play in empirical work, facilitating predictions
“before the fact” or “outside the data”.

Academic research on auction markets has also played a key role in regulatory policy, for
example in the case of re-structured wholesale electricity markets. These markets

14

frequently involve electricity generation facilities bidding to supply power, and analyses
along the lines described above have been influential in highlighting potential problems
associated with the exercise of market power, as well as the relationship between daily
spot markets and long-term forward contracts between electricity generators and
distributors. For example, Borenstein, Bushnell and Wolak (2002) analyze data from the
California wholesale electricity market and argue that the dramatic price spike during the
summer of 2000 was due in large part to the exercise of market power, rather than an
increase in production costs.

Determinants of Market Structure
A classic question dating back at least to Bain’s work in the 1950s is how the set of firms
in an industry and their relative capabilities affects competition, innovation, and other
market outcomes. This question leads to asking what determines, over time, the set of
firms present in an industry. In particular, what barriers to entry serve to keep markets
more or less imperfectly competitive? Questions about market structure have been the
focus of sustained research from a number of different directions.

One way to tackle the question of how market structure affects competition is by focusing
on specific episodes of entry or exit from a market. For example, Goolsbee and Syverson
(2008) study how prices are affected by Southwest Airlines entry into new routes. They
find that prices fall sharply and the decline begins in advance of the actual date of entry.
Prices begin to fall once Southwest starts operating at both endpoints of a route, making
entry into the route probable. In another example that focuses on strategic theories of
entry deterrence, Ellison and Ellison (2007) look at the behavior of pharmaceutical firms
just prior to patent expiration. They use variation in market size to identify settings where
incumbent firms might profitably engage in entry deterrence strategies, and present
evidence that these strategies are being used.

An alternative approach, pioneered by Bresnahan and Reiss (1991), also relies on
variation in market size but combines this variation with an assumption that markets are

15

in a state of long-run equilibrium. The basic idea, which also appears in Sutton (1991), is
to think of variable profits as scaling with the size of the market. 4 In the specific markets
considered by Bresnahan and Reiss, namely service providers such as dentists,
pharmacists and plumbers in small isolated towns, the idea can be stated simply. If it
takes 800 residents to support a single dentist and the entry of a second dentist does not
result in lowered margins or affect entry costs, a town of 1,600 residents should support
two dentists. However, if the presence of a second dentist intensifies competition, it will
take more than 1,600 residents. In this way, variation in market size can be used to draw
inferences about the rate at which competition causes prices to fall toward unit cost.

Bresnahan and Reiss (1991) estimate the size threshold required to support different
numbers of firms in a given industry. Their surprising finding is that compared to the
population required to support a single firm, the incremental population required to
support a second firm is much larger, sometimes even double, but the incremental
population required to support a third and a fourth firm is about the same as for the
second. Interpreted through the lens of their theory, competition appears to kick in
relatively quickly and lead to lower margins with just a few firms, but then does not ramp
up with additional entrants.

Bresnahan and Reiss’ (1991) work illustrates how even relatively sparse data, used
creatively in conjunction with economic theory and clean research design, can be applied
to tackle an important question --- in their case, how competition varies with the number
of firms in a market. This work has generated a substantial literature, although in a
direction that has focused more heavily on the econometric methods than on the original
questions. One might have expected researchers to have followed up by looking for other
sorts of identifying variation (say in fixed costs or entry restrictions) or by gathering data
on prices and costs (which is largely absent in the original work). Instead, attention has
tended to focus on developing more general econometric methods that allow for firm
If variable profits scale with the size of the market, then π(n)=S⋅q(pn)⋅(pn-c) where pn is the price charged
if there are n competitors, c is the unit cost of production, q(p) is the share of potential customers who
purchase, and S is the market size. If there is a fixed cost F to being in the market, long-run equilibrium
should exhibit the property that π(n)>F>π(n+1). The innovation of using long-run equilibrium conditions
as a basis for estimation also appears in Berry (1992).
4

16

heterogeneity or relax parametric restrictions, often applying the same Bresnahan and
Reiss identifying assumptions in situations where they are less compelling. These
extensions have lead to a rich literature on identification and estimation methods (Berry
and Tamer, 2006), but one can argue that they haven’t necessarily taken us much closer
to understanding the original question of interest.

Industry Dynamics
Many questions about market structure are most naturally viewed in a dynamic context.
For example, one may be interested in whether new industries follow a common “life
cycle” of entry and consolidation, or how firm and industry growth patterns vary with
market characteristics, or whether mergers are likely to be followed quickly by new
entry, or how industries adapt to booms and recessions. A dynamic perspective can also
temper conventional intuition on standard problems. For example, higher market
concentration can reduce static welfare due to less competition and higher prices, but the
prospect of gaining market power can provide strong incentives for innovation with
consequent welfare benefits.

The literature that addresses these types of questions can be separated roughly into two
strands. One strand has looked across industries in search of robust patterns of
survivorship, turnover and firm growth. One important take-away documented in this
literature is that even within relatively narrow industries, there is a great deal of
heterogeneity across establishments. This observation has proven very influential. It has
played an important role in the development of theoretical models of industry dynamics
(e.g. Hopenhayn, 1992), and these models in turn have helped to touch off a vast amount
of recent work in international trade on the role of firm heterogeneity (Melitz, 2003). In
addition, the recognition that entry and exit patterns are systematically related to firm
characteristics has been incorporated in the literature on industrial productivity, following
the influential work of Olley and Pakes (1996).

17

A second strand of work on industry dynamics has focused on models of dynamic
equilibrium in individual industries, often adopting the framework proposed by Ericson
and Pakes (1995). For example, Benkard (2004) applies such a model to the commercial
aircraft industry, where he documents dramatic cost effects due to learning-by-doing.
Benkard (2004) calibrates a dynamic oligopoly model using industry data and uses the
quantitative model to illustrate the (dynamic) pricing incentives. One insight is that
equilibrium can involve prices below marginal cost as a way to speed up production and
reduce future costs. In addition, the prospect of market power is what provides an
incentive to get down the learning curve. This effect gets passed on to consumers in the
form of lower prices, both early in the product life cycle (to speed up learning) and later
on (due to reduced costs). More recent work in this area has suggested empirical
strategies that allow estimation of sunk costs, learning, or adjustment frictions, using
panel data on firms or local markets (Aguirregabiria and Mira, 2007; Bajari, Benkard and
Levin, 2007; Pakes, Ostrovsky and Berry, 2007).

One difficulty in estimating and working with these types of dynamic industry models is
that the information available in the data sometimes can pale in comparison to the
ambitious questions that are being asked, forcing the researcher to fill in the gaps with
strong modeling assumptions. Our own view is that such work is best viewed as a
quantitative theory exercise. Its primary role is to shed light on certain aspects of
dynamic competition, in the context of a particular model with reasonably chosen
parameter values. Is this a problem? We don’t think so. While results from such exercises
should be taken with the appropriate caution, it seems hard to imagine a clean research
design that, say, would nail the long-run effect on innovation attributable to a change in
the patent system or a new research and development subsidy. When a single data point is
the entire time series of an industry, opportunities to utilize quasi-experimental variation
are rare, but this does not mean economists should give up on attempts to investigate
important questions about, say, the tradeoff between market power and innovation
incentives.

18

Stepping back and looking forward

Thirty years ago, empirical work in industrial organization was perhaps broadly similar to
other applied fields. At about the same time that industrial organization economists were
running cross-industry regressions and coming to terms with the problems of
endogeneity, omitted variables, and reverse causality, labor economists, for example,
were running wage regressions that faced a similar set of issues. Since then, however,
empirical work in industrial organization has evolved dramatically, and in a distinctive
direction: toward analyses of individual industries where one can obtain cleaner
measurement and identification, and toward studies that frame their empirical analysis in
terms of an economic theory of the relevant industry or a set of competing theories.

In an essay that appears in this issue of JEP, Angrist and Pischke (2010) describe a
parallel development in applied microeconomics toward analyses that uses randomized
experiments or “quasi-experiments” to identify causal effects. In the course of doing this,
they launch what strikes us as a somewhat misguided attack on empirical research in
industrial organization. In part to clear up potential confusion, and in part because we
have our own views about fruitful directions for research in industrial organization, it
seems useful to discuss how their arguments relate to the type of work we have discussed
above.

One issue relates to what constitutes acceptable identifying variation in a given study.
Here, there is really no disagreement about the ideal. Every researcher would like to first
define an object of interest and then design the perfect experiment to measure it. But in
the absence of this ideal, researchers generally face trade-offs. A common trade-off is
between how clean a measurement one obtains versus how well the measured object
proxies for the original object of interest.

Suppose a researcher is interested in consumer demand for cereal, and ideally would like
to know the n-by-n matrix of cross-price elasticities. As we described above, a common
approach in industrial organization has been to control carefully for possible shifts in

19

demand, and then rely on an assumption that the residual correlation in prices across
cities is due to changes in costs (e.g. Nevo, 2001). An alternative might be to look for
specific episodes where particular prices were shifted for explicable but plausibly
exogenous reasons, say a week when the Froot Loops pricing algorithm malfunctioned
and there was an unintended sale, and use these episodes to estimate demand elasticities
with respect to the price of Froot Loops. The researcher might then extrapolate by
assuming that a change in the price of Frosted Flakes, or perhaps any cereal, would have
analogous effects.

In the context of the example, neither approach is the ideal. The former relies on a
potentially questionable identification assumption, but frames the empirical exercise
directly in terms of the object of interest. The latter may provide a more compelling
measurement of a few entries in the matrix, but getting to the object of interest involves a
questionable extrapolation. One problem with attempts such as Angrist and Pischke’s to
evaluate entire research approaches in the abstract is that they paint too simple a picture.
The resolution to the type of trade-off described above almost certainly depends on the
research question that is being asked, the data that is available to answer it, and the
degree to which economic theory provides a compelling rationale for making
assumptions about relationships in the data. Our own view about industrial organization
is that there is room for a variety of approaches, even for tackling the same question.
Indeed, because any research can only come so close to the requisite answer, careful
work from different angles is often complementary.

A second issue about how industrial organization has evolved relates to the use of
economic theory in empirical research. It seems safe to assume that professional
economists, or at least the vast majority of them, believe that economic theory provides a
powerful lens for thinking about the world. So what is striking about Angrist and
Pischke’s picture of empirical research is that it seems to involve very little role for
economic theory in thinking about or analyzing data. Instead, and in contrast to many of
the studies described in this essay, they favor measurement strategies that are
uncontaminated by restrictions that arise from an economic model. One of their

20

justifications is transparency; they seem to conflate the use of economic theory with
complex modeling that obscures the data. This strikes us as a false equivalence, however,
in the sense that one can have a perfectly clear exposition of a model derived from
economic theory and a perfectly obscure exposition of a linear regression.

From our perspective, it seems more natural to start with a research question and then ask
to what extent economic theory helps to shed light on the problem. For instance, Angrist
and Pischke focus on the effect of class size on student learning outcomes. Standard
economic theory does not have a great deal to say about student behavior in third-grade
classrooms. So while one could start with an equilibrium model of student learning and
use it to structure empirical work, there is good reason to adopt a more statistical
approach, observing that there are thousands of third-grade classrooms engaged in
roughly parallel activities and many opportunities to find attractive variation in class size
to assess its effect on learning.

For industrial organization research, however, this frequently is not the right paradigm.
First, industrial organization is largely about the operation of firms and markets, where
economic theory has a lot to say and when appropriately used generally serves to clarify
rather than complicate one’s understanding of a market. Second, the interesting question
in many studies is not just a causal effect per se, but understanding the mechanisms at
work. As we have emphasized throughout this essay, markets differ greatly, and
importing particular numbers (about demand elasticities, production costs, or policy
effects) across markets often does not seem compelling. Instead, if one hopes to
generalize, it is often more appealing to view empirical research as building up support
for principles of strategic interaction or market functioning that are broadly applicable
across industries.

The particular example chosen by Angrist and Pischke to critique empirical work in
industrial organization --- the evaluation of mergers --- illustrates some of these points.
As we discussed above, it is common in assessing a proposed merger to frame the
problem in terms of a theoretical model of industry competition. Researchers have spent

21

considerable time developing econometric tools to quantify the potential effects in the
context of such models. Angrist and Pischke criticize this work and dismiss it as
needlessly indirect. Instead they ask why there hasn’t been more retrospective analysis of
past mergers. At one level, their point is a good one --- why not more retrospective
analysis of past mergers? At another level, it entirely misses the point. Do they seriously
think that if the Department of Justice had to review a merger of Microsoft and Yahoo! it
should rely on the price effect of past airline or office supply company mergers, or better
yet the subset that resulted from chance meetings of CEOs or lunar eclipses? It seems far
more useful to lay out a clear conceptual framework to think through the potential effects,
adding the best available evidence in a sensible way.

The bottom line, however, is that the use of economic theory and the search for
compelling sources of identifying variation are not enemies. Indeed, we hope to have
conveyed that the applied work we often find most exciting relies on careful
measurement based on data with good underlying variation, but then continues by
framing the empirical exercise in terms of a coherent economic model. The model can
then provide a way to think about the operation of the industry and potentially to draw
conclusions about policy or general principles.

To the extent that we have a concern about the current state of industrial organization
research, it is that there is not sufficient emphasis on this kind of applications, relative to,
say, expanding the set of econometric methods. Of course, better methods are valuable,
provided they eventually get used in compelling ways and do not become an end in
themselves. If we return again to the demand estimation literature, it is possible that one
reason researchers have been willing to tolerate less than ideal price variation is that in
some cases the main contribution is not the estimated price elasticities per se but the
econometric method, which can be applied more broadly. While this is not terribly
objectionable, it is important that the field at large strikes a balance between building
tools and using them convincingly. Whether the field has tipped too far is debatable, but
the fact that one might engage in a serious debate suggests some grounds for concern.

22

From a historical perspective, one way to understand the current balance relates to data
limitations. Some of the most influential methodological advances in industrial
organization were, in fact, responses to problems of limited data. The entry model of
Bresnahan and Reiss (1991) gets around missing data on prices and quantities. The
demand estimation method introduced by Berry, Levinsohn, and Pakes (1995) has been
so influential in part because it requires only market-level rather than individual-level
data. We have tried to stress throughout this essay that the situation is changing rapidly.
Virtually every major firm now collects vast amounts of data on their customers, their
employees, and other aspects of their business. It is increasingly easy to collect data on
prices and quantities, entry and exit, firm locations, and accounting information. The
advent of vastly richer data may substitute for methods, and encourage industrial
organization economists to shift some of the focus toward applying existing methods
rather than developing new ones.

There is a related concern, and to be fair, one that applies equally to the empirical
strategies favored by Angrist and Pischke, that focusing on the elegance of the solution
can lead one to gravitate toward less important questions. For example, suppose we were
to think about the research avenues one could pursue related to the internet platform
eBay. If we started with the view that it was a potential laboratory for applying elegant
empirical auction methods, it would be natural to focus on narrower and narrower
submarkets in order to isolate specific features of the auction format. While this is useful,
it might take one away from broader issues, such as why eBay as an institution has been
so successful or how they compete with Amazon and other platforms for buyers and
sellers. Indeed, economics researchers of all varieties should resist the temptation of
grabbing a hammer and letting it pull them toward a limited set of nails.

A final and important issue for the future of industrial organization relates to the shift
from cross-industry analysis to industry studies. In his post-mortem on the cross-industry
literature in industrial organization, Schmalensee (1989) pointed out that it had not taught
us much about how markets actually work. After 20 years of industry studies, we know a
lot about how specific industries work, but this knowledge is extremely disaggregated.

23

We have detailed analysis on automobiles, airlines, electricity, and cement and concrete
plants (which are not the same!). But this knowledge does not easily accumulate across
industries. As a result, industrial organization has ceded many of the interesting and
important questions about the overall organization of production in the economy to other
fields such as trade and macroeconomics. It may be time to reclaim them.

References
Aguirregabiria, Victor, and Pedro Mira (2007), “Sequential Estimation of Dynamic
Discrete Games,” Econometrica, 75(1), 1-53.
Angrist, Joshua D., and Jörn-Steffen Pischke (2010), “The Credibility Revolution in
Empirical Economics: How Better Research Design is Taking the Con Out of
Econometrics.” Journal of Economics Perspectives.
Athey, Susan, and Philip A. Haile (2006), “Empirical Models of Auctions,” in Advances
in Economics and Econometrics: Theory and Applications, Ninth World Congress,
Volume 2, Richard Blundell, Whitney K. Newey, and Torsten Persson, eds., Chapter
1, 1-45, Cambridge: Cambridge University Press.
Athey, Susan, Jonathan Levin, and Enrique Seira (2008). “Comparing Open and Sealed
Bid Auctions: Evidence from Timber Auctions.” NBER Working Paper No. 14590.
Bain, Joe S. (1951), “Relation of Profit Rate to Industry Concentration: American
Manufacturing, 1936-1940,” Quarterly Journal of Economics, 65, 293-324.
Bain, Joe S. (1956), Barriers to New Competition, Cambridge, MA: Harvard University
Press.
Bajari, Patrick, Lanier Benkard, and Jonathan Levin (2007), “Estimating Dynamic
Models of Industry Competition,” Econometrica, 75(5), 1331-1370.
Benkard, C. Lanier (2004). “A Dynamic Analysis of The Market for Wide-Bodied
Commercial Aircraft.” Review of Economic Studies, 71, 581-611.
Berry,

Steven

(1992),

“Estimation

of

Industry,” Econometrica, 60(4), 889-917.

24

a

Model

of

Entry

in

the

Airline

Berry, Steven, James Levisohn, and Ariel Pakes (1995). “Automobile Prices in Market
Equilibrium.” Econometrica, 63(4), 841-890.
Berry, Steven, and Elie Tamer (2006), “Identification in Models of Oligopoly Entry,” in
Advances in Economics and Econometrics: Theory and Applications, Ninth World
Congress, Volume 2, Richard Blundell, Whitney K. Newey, and Torsten Persson,
eds., Chapter 2, 46-85, Cambridge: Cambridge University Press.
Borenstein, Severin, James Bushnell, and Frank Wolak (2002), “Measuring Market
Inefficiencies in California’s Wholesale Electricity Industry,” American Economic
Review, 92(5), 1376-1405.
Bresnahan, Timothy F. (1987), “Competition and Collusion in the American Automobile
Industry: The 1955 Price War,” Journal of Industrial Economics, 35(4), 457-482.
Bresnahan, Timothy F. (1989), “Empirical Studies of Industries with Market Power,” in
Handbook of Industrial Organization, Volume 2, Richard Schmalensee and Robert
D. Willig, eds., Chapter 17, pp. 1011-1057, Amsterdam: North Holland.
Bresnahan, Timothy F., and Peter C. Reiss (1991), “Entry and Competition in
Concentrated Markets,” Journal of Political Economy, 99(5), 977-1009.
Cohen, Alma, and Liran Einav (2007), “Estimating Risk Preferences from Deductible
Choice,” American Economic Review, 97(3), 745-788.
Einav, Liran, Mark Jenkins, and Jonathan Levin (2008), “Contract Pricing in Consumer
Credit Markets,” Mimeo, Stanford University.
Ellison, Glenn, and Sara Ellison (2007). “Strategic Entry Deterrence and the Behavior of
Pharmaceutical Incumbents Prior to Patent Expiration.” Mimeo, MIT.
Ellison, Glenn, and Sara Ellison (2009). “Search, Obfuscation, and Price Elasticities on
the Internet.” Econometrica 77(2). 427-452.
Ericson, Richard, and Ariel Pakes (1995). “Markov-Perfect Industry Dynamics: A
Framework for Empirical Work.” Review of Economic Studies 62(1). 53-82.
Fisher, Franklin M. (1989). “Games Economists Play: A Noncooperative View.” RAND
Journal of Economics 20(1). 113-124.
Gentzkow, Matthew, and Jesse M. Shapiro (2009). “What Drives Media Slant? Evidence
from U.S. Daily Newspapers.” Econometrica, forthcoming.

25

Goolsbee, Austan, and Chad Syverson (2008). “How do Incumbents Respond to the
Threat of Entry? Evidence from the Major Airlines.” Quarterly Journal of
Economics 123(4). 1611-1633.
Guerre, Emmanuel, Isabelle M. Perrigne, and Quang H. Vuong (2000). “Optimal
Nonparametric Estimation of First-Price Auctions.” Econometrica 68. 525-574.
Hausman, Jerry A. (1997). “Valuation of New Goods Under Perfect and Imperfect
Competition,” in Bresnahan and Gordon (eds.), The Economics of New Goods,
NBER Studies in Income and Wealth (58). 209-37.
Hendel, Igal, and Aviv Nevo (2006), “Measuring the Implications of Sales and Consumer
Inventory Behavior,” Econometrica, 74(6), 1637-1673.
Hendricks, Ken, and Robert H. Porter (1988). “An Empirical Study of an Auction with
Asymmetric Information.” American Economic Review 78(5). 865-883.
Hendricks, Ken, and Robert H. Porter (2007), “An Empirical Perspective on Auctions,”
in Handbook of Industrial Organization, Volume 3, Mark Armstrong and Robert
Porter, eds., Chapter 32, pp. 2073-2144, Amsterdam: North Holland.
Hendricks, Ken, Joris Pinske, and Robert H. Porter (2003). “Empirical Implications of
Equilibrium Bidding in First-Price, Symmetric, Common Value Auctions.” Review
of Economic Studies 70(1). 115-145.
Hopenhayn, Hugo A. (1992), “Entry, Exit, and Firm Dynamics in Long Run
Equilibrium,” Econometrica, 60(5), 1127-1150.
Hortacsu, Ali, and Steven L. Puller (2008). “Understanding Strategic Bidding in MultiUnit Auctions: A Case Study of the Texas Electricity Spot Market.” RAND Journal
of Economics 39(1). 86-114.
Lewis, Randall, and David Reiley (2009). “Retail Advertising Works! Measuring the
Effects of Advertising on Sales via a Controlled Experiment on Yahoo!” Mimeo,
Yahoo! Research.
Melitz, Marc J. (2003), “The Impact of Trade on Intra-Industry Reallocations and
Aggregate Industry Productivity,” Econometrica, 71(6), 1695-1725.
Nevo, Aviv (2001). “Measuring Market Power in the Ready-to-Eat Cereal Industry.”
Econometrica 69(2). 307-342.

26

Olley, G. Steven, and Ariel Pakes (1996), “The Dynamics of Productivity in the
Telecommunications Equipment Industry,” Econometrica, 64(6), 1263-1297.
Ostrovsky, Michael, and Michael Schwarz (2009). “Reserve Prices in Internet
Advertising Auctions: A Field Experiment.” Mimeo, Stanford GSB.
Pakes, Ariel, Michael Ostrovsky, and Steven Berry (2007), “Simple Estimators for The
Parameters of Dynamic Discrete Games, with Entry/Exit Examples,” Rand Journal
of Economics, 38, 373-399.
Peltzman, Sam (1991). “The Handbook of Industrial Organization: Review Article.”
Journal of Political Economy 99(1). 201-217.
Porter, Robert H. (1983). “A Study of Cartel Stability: The Joint Executive Committee,
1880-1886.” Bell Journal of Economics 15(2). 301-314.
Rosse, James N. (1970). “Estimating Cost Function Parameters without Using Cost Data:
Illustrated Methodology.” Econometrica 38(2). 256-275.
Schmalensee, Richard (1989), “Inter Industry Studies of Structure and Performance,” in
Handbook of Industrial Organization, Volume 2, Richard Schmalensee and Robert
D. Willig, eds., Chapter 16, pp. 951-1010, Amsterdam: North Holland.
Sutton, John (1991). Sunk Costs and Market Structure. Cambridge, MA: MIT Press.
Tirole, Jean (1988). The Theory of Industrial Organization. Cambridge, MA: MIT Press.
Varian, Hal R. (2009), “Online Ad Auctions,” American Economic Review Papers and
Proceedings, 99(2), 430–34.

27

