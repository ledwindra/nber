NBER WORKING PAPER SERIES

LOW-FREQUENCY ROBUST COINTEGRATION TESTING
Ulrich Müller
Mark W. Watson
Working Paper 15292
http://www.nber.org/papers/w15292

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
August 2009

We thank participants of the Cowles Econometrics Conference, the NBER Summer Institute and the
Greater New York Metropolitan Area Econometrics Colloquium, and of seminars at Chicago, Cornell,
Northwestern, NYU, Rutgers, and UCSD for helpful discussions. Support was provided by the National
Science Foundation through grants SES-0518036 and SES-0617811. The views expressed herein are
those of the author(s) and do not necessarily reflect the views of the National Bureau of Economic
Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2009 by Ulrich Müller and Mark W. Watson. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.

Low-Frequency Robust Cointegration Testing
Ulrich Müller and Mark W. Watson
NBER Working Paper No. 15292
August 2009
JEL No. C32,E32
ABSTRACT
Standard inference in cointegrating models is fragile because it relies on an assumption of an I(1) model
for the common stochastic trends, which may not accurately describe the data's persistence. This paper
discusses efficient low-frequency inference about cointegrating vectors that is robust to this potential
misspecification. A simple test motivated by the analysis in Wright (2000) is developed and shown
to be approximately optimal in the case of a single cointegrating vector.

Ulrich Müller
Department of Economics
Princeton University
Princeton, NJ 08544-1013
umueller@princeton.edu
Mark W. Watson
Department of Economics
Princeton University
Princeton, NJ 08544-1013
and NBER
mwatson@princeton.edu

1

Introduction

The fundamental insight of cointegration is that while economic time series may be individually highly persistent, some linear combinations are much less persistent. Accordingly, a
suite of practical methods have been developed for conducting inference about cointegrating vectors, the coeﬃcients that lead to this reduction in persistence. In their standard
form, these methods assume that the persistence is the result of common I(1) stochastic
trends,1 and their statistical properties crucially depend on particular characteristics of I(1)
processes. But in many applications there is uncertainty about the correct model for the persistence which cannot be resolved by examination of the data, rendering standard inference
potentially fragile. This paper studies eﬃcient inference methods for cointegrating vectors
that is robust to this fragility.
We do this using a transformation of the data that focuses on low-frequency variability
and covariability. This transformation has two distinct advantages. First, as we have argued elsewhere (Müller and Watson (2008)), persistence (“trending behavior”) and lack of
persistence (“non-trending, I(0) behavior”) are low-frequency characteristics, and attempts
to utilize high-frequency variability to learn about low-frequency variability are fraught with
their own fragilities.2 Low-frequency transformations eliminate these fragilities by focusing
attention on the features of the data that are of direct interest for questions relating to
persistence. The second advantage is an important by-product of discarding high frequency
variability. The major technical challenge when conducting robust inference about cointegrating vectors is to control size over the range of plausible processes characterizing the
model’s stochastic common trends. Restricting attention to low frequencies greatly reduces
the dimensionality of this challenge.
The inference problem studied in this paper has a long history. Elliott (1998) provides
a dramatic demonstration of the fragility of standard cointegration methods by showing
that they fail to control size when the common stochastic trends are not I(1), but rather are
“local-to-unity” in the sense of Bobkoski (1983), Cavanagh (1985), Chan and Wei (1987) and
Phillips (1987).3 In a bivariate model, Cavanagh, Elliott, and Stock (1995) propose several
1

See, for instance, Johansen (1988), Phillips and Hansen (1990), Saikkonen (1991), Park (1992) and Stock
and Watson (1993).
2
Perhaps the most well-known example of this fragility involves estimation of HAC standard errors, see
Newey and West (1987), Andrews (1991), den Haan and Levin (1997), Kiefer, Vogelsang, and Bunzel (2000),
Kiefer and Vogelsang (2005), Müller (2007) and Sun, Phillips, and Jin (2008).
3
Also see Elliott and Stock (1994) and Jeganathan (1997).

1

procedures to adjust critical values from standard tests to control size over a range of values
of the local-to-unity parameter, and their general approach has been used by several other
researchers; Campbell and Yogo (2006) provides a recent example. Stock and Watson (1996)
and Jansson and Moreira (2006) go further and develop inference procedures with specific
optimality properties in the local-to-unity model. In the fractional cointegration literature,
the common stochastic trends are modelled as fractionally integrated, although the problem
is diﬀerent from the local-to-unity case as the fractional parameter can be consistently estimated under standard asymptotics. Yet, Müller and Watson (2008) demonstrate that, at
least based on below business cycle variation, it is a hopeless endeavor to try to consistently
discriminate between, say, local-to-unity and fractionally integrated stochastic data spanning
50 years.4 While local-to-unity and fractional processes generalize the assumption of I(1)
trends, they do so in a very specific way, leading to worries about the potential fragility of
these methods to alternative specifications of the stochastic trend.
As demonstrated by Wright (2000), it is nevertheless possible to conduct inference about
a cointegrating vector without knowledge about the precise nature of the common stochastic
trends. Wright’s idea is to use the I(0) property of the error correction term as the identifying
property of the true cointegrating vector, so that a stationarity test of the model’s putative
error correction term is used to conduct inference about the value of the cointegrating vectors. Because the common stochastic trends drop out under the null hypothesis, Wright’s
procedure is robust in the sense that it controls size under any model for the common stochastic trend. But the procedure ignores the data beyond the putative error correction term,
and is thus potentially quite ineﬃcient.
Section 2 of this paper provides a formulation of the cointegrated model in which the
common stochastic trends follow a flexible limiting Gaussian process that includes the I(1),
local-to-unity, and fractional/long-memory models as special cases. Section 3 discusses the
low-frequency transformation of the cointegrated model. Throughout the paper, inference
procedures are studied in the context of this general formulation of the cointegrated model.
The price to pay for this generality is that it introduces a potentially large number of nuisance
parameters that characterize the properties of the stochastic trends and the relationship
between the stochastic trends and the model’s I(0) components, and cannot be estimated
consistently in our framework. The main challenge of this paper is to study eﬃcient tests
4

Granger’s Frank Paish Lecture (1993) discusses a wide range of possible data generating processes beyond
the I(1) model and argues, sensibly in our opinion, that it is fruitless to attempt to identify the exact nature
of the persistence using the limited information in typical macro time series.

2

in the presence of nuisance parameters under the null hypothesis, and Sections 4—6 address
this issue.
By definition, a valid test must control size for any possible value of the nuisance parameter. In our application, the nuisance parameter is often highly dimensional, to the order of
200 × 1 even in a bivariate system. This makes it extremely hard to directly construct good,
let alone eﬃcient tests. Our strategy is thus rather to indirectly learn about the quality of
potential tests by deriving bounds on their performance. In particular, Section 4 presents
two general results for hypothesis tests in the presence of nuisance parameters under the
null. The first is an upper bound for the power of any test that controls size.5 The second result provides a lower bound on size under a more general, "auxiliary" null hypothesis
of any test that satisfies a lower bound on power and controls size under the original null
hypothesis. These bounds provide limits on the performance characteristics of tests, and
they can be computed without ever determining a test that is known to control size. Section 5 implements these bounds for tests concerning the value of cointegrating vectors in
our low-frequency framework, and discusses numerical techniques to obtain low upper power
bounds (approximate “least upper power bounds”) and high lower bounds on size under
the auxiliary null hypothesis. The power bounds provide a benchmark for the eﬃciency
of any valid test, and diﬀerences in the power bounds (interpreted as diﬀerences in least
upper bounds) associated with restrictions on the trend process (for example, restricting
the general stochastic trend process to be I(1)) quantify the restriction’s information about
the value of the cointegrating vector. We find that restrictions can be very informative in
the sense of allowing for more powerful tests, but whenever this is the case, any test that
were to successfully exploit this information would suﬀer from large size distortions under a
less restrictive trend process. Our analysis using bounds thus quantifies the intuitive notion
that extracting information from the assumption of a particular trend process (e.g., I(1) or
local-to-unity) makes inference fragile relative to this assumption.
Section 6 builds on Wright’s (2000) suggestion and derives a low-frequency test for the
value of the cointegration vectors based on an I(0) test for the putative error correction
term. Specifically, we derive a low-frequency version of a multivariate point-optimal scale and
rotation invariant test against the alternative in which the common trends are I(1). Similar
to Wright’s (2000) original suggestion, while simple, this low-frequency test for inference
about cointegrating vectors is potentially quite ineﬃcient, as it ignores the data beyond the
5

The same insight about upper bounds on power was noted independently by Andrews, Moreira, and
Stock (2008) and used for inference in IV models with potentially weak instruments.

3

putative error correction term. But the null rejection probability of this test is unaﬀected by
the properties of the common stochastic trend, so its power constitutes an easily achievable
lower bound on the power of eﬃcient tests. As it turns out, when attention is focused on a
single cointegrating vector, and regardless of the number of common trends, the power of this
test essentially coincides with the upper bound for an unrestricted version of the common
trend process under the null hypothesis, and is close to the bound for several restricted,
but still flexible common trend processes. Thus in this case, the low-frequency version of
Wright’s test–that is, ignoring the data beyond the putative error correction term–yields
an essentially eﬃcient test in the absence of strong a priori knowledge about the nature of
the persistence.
The implication for applied work is that, at least in the model with a single cointegrating
vector, approximately eﬃcient and robust inference may be carried out using the simple test
described in Section 6.2. The test is robust in two ways. First, it is robust to arbitrary
autocorrelation properties in the error correction term above the pre-specified low-frequency
band. Second, it is robust to the precise nature of persistence, as its rejection probability
under the null hypothesis does not depend on the nature of the stochastic trend. As in Wright
(2000), confidence sets for the cointegrating vector can easily be obtained be inverting the
test. We present a brief empirical illustration in Section 7.

2

Model

Let pt , t = 1, ..., T denote the n × 1 vector of variables under study. This section outlines a
time domain representation of the cointegrated model for pt in terms of canonical variables
representing a set of common trends and I(0) error correction terms. The common trends are
allowed to follow a flexible process that includes I(1), local-to-unity, and fractional models
as special cases, but aside from this generalization, the cointegrated model for pt is standard.
To begin, pt is transformed into two components, where one component is I(0) under the
null hypothesis and the other component contains elements that are not cointegrated. Let β
denote an n × r matrix whose linearly independent columns are the cointegrating vectors, let
β 0 denote the value of β under the null, and yt = β 00 pt . The elements in yt are the model’s
error correction terms under the null hypothesis. Let xt = δ 0 pt where δ is n×k with k = n−r,
and where the linearly independent columns of δ are linearly independent of the columns of
β 0 , so that the elements of xt are not cointegrated under the null. Because the cointegrated
model only determines the column space of the matrix of cointegrating vectors, the variables
4

yt and xt are determined up to transformations (yt , xt ) → (Ayy yt , Axx xt + Axy yt ), where Ayy
and Axx are non-singular. Most extant inference procedures are invariant (or asymptotically
invariant) to these transformations, and, as discussed in detail below, our analysis will also
focus on invariant tests.

2.1

Canonical Variable Representation of yt and xt

We will represent yt and xt in terms of a common stochastic trend vector vt and an I(0)
vector zt
yt = Γyz zt + Γyv vt

(1)

xt = Γxz zt + Γxv vt ,
where zt is r × 1, vt is k × 1, and Γyz and Γxv have full rank. In this representation, the
restriction that yt is I(0) corresponds to the restriction Γyv = 0. All of the test statistics
discussed in this paper are invariant to adding constants to the observations, so that constant
terms are suppressed in (1). As a technical matter, we think of {zt , vt }Tt=1 (and thus also
{xt , yt }Tt=1 ) as being generated from a triangular array; we omit the additional dependence
√
on T to ease notation. Also, we write bxc for the integer part of x ∈ R, ||A|| = tr A0 A for
any real matrix A, x ∨ y for the maximum of x, y ∈ R, ’⊗’ for the usual Kronecker product
and ’⇒’ to indicate weak convergence.
Let W (·) denote a n×1 standard Wiener process. The vector zt is a canonical I(0) vector
in the sense that its partial sums converge to a r × 1 Wiener process
T

−1/2

bsT c

X
t=1

zt ⇒ Sz W (s) = Wz (s), where Sz Sz0 = Ir .

(2)

The vector vt is a common trend in the sense that scaled versions of its level converge
to a stochastic integral with respect to W (·). For example, in the standard I(1) model,
Rs
T −1/2 vbsT c ⇒ 0 HdW (t), where H is a k ×n matrix and (H 0 , Sz0 ) has full rank. More general
trend processes, such as the local-to-unity formulation, allow the matrix H to depend on s
and t. The general representation for the common trends used in this paper is
Z s
−1/2
T
vbsT c ⇒
H(s, t)dW (t)
(3)
−∞

where H(s, t) is suﬃciently well behaved to ensure that there exists a cadlag version of the
Rs
process −∞ H(s, t)dW (t).6
6

The common scale T −1/2 for the k × 1 vector vt in (3) is assumed for convenience; with an appropriate

5

2.2

Invariance and Reparameterization

As discussed above, because cointegration only identifies the column space of β, attention is
restricted to tests that are invariant to the group of transformations
(yt , xt ) → (Ayy yt , Axx xt + Axy yt )

(4)

where Ayy and Axx are non-singular, but (Ayy , Axx , Axy ) are otherwise unrestricted real
matrices.
The restriction to invariant tests allows a simplification of notation: because the test
statistics are invariant to the transformations in (4), there is no loss of generality setting
Γyz = Ir , Γxv = Ik , and Γxz = 0. With these values, the model is
yt = zt + Γyv vt

(5)

xt = vt .

2.3

Restricted Versions of the Trend Model

We will refer to the general trend specification in (3) as the “unrestricted” stochastic trend
model throughout the remainder of the paper. The existing literature on eﬃcient tests relies
on restricted forms of the trend process (3) such as I(1) or local-to-unity processes, and we
compute the potential power gains associated with these and other a priori restrictions on
H(s, t) below. Here we describe five restricted versions of the stochastic trend.
The first model, which we will refer to as the G-model, restricts H(s, t) to satisfy
H(s, t) = G(s, t)Sv ,

(6)

where G(s, t) is k × k and Sv is k × n with Sv Sv0 = Ik and (Sz0 , Sv0 ) nonsingular. In this
model, the common trend depends on W (·) only through the k × 1 standard Wiener process
Wv (·) = Sv W (·), and this restricts the way that vt and zt interact. In this model
Z s
−1/2
vbsT c ⇒
G(s, t)dWv (t),
(7)
T
−∞

and the covariance between the Wiener process characterizing the partial sums of zt , Wz , and
Wv is equal to the r × k matrix R = Sz Sv0 . Standard I(1) and local-to-unity formulations of
cointegration satisfy this restriction and impose additional parametric restrictions on G(s, t).
definition of local alternatives, the invariance (4) ensures that one would obtain the same results for any
scaling of vt . For example, for an I(2) stochastic trend scaled by T −3/2 , set H(s, t) = 1[t ≥ 0](s − t)H, with
the k × n matrix H as in the I(1) case.

6

The second model further restricts (7) so that G(s, t) is diagonal:
G(s, t) = diag(g1 (s, t), · · · , gk (s, t)).

(8)

An interpretation of this model is that the k common trends evolve independently of one
another (recall that Wv has identity covariance matrix), where each trend is allowed to follow
a diﬀerent process characterized by the functions gi (s, t).
The third model further restricts the diagonal-G model so that the k stochastic trends
converge weakly to a stationary continuous time process. We thus impose
gi (s, t) = giS (s − t), i = 1, · · · , k.

(9)

The stationary local-to-unity model (with an initial condition drawn from the unconditional
distribution), for instance, satisfies this restriction.
Finally, we consider two parametric restrictions of G:
G(s, t) = 1[t > 0]Ik

(10)

G(s, t) = 1[t > 0]eC(s−t)

(11)

which is the I(1) model, and
which is the multivariate local-to-unity model, where C is the k × k diﬀusion matrix of the
limiting Ornstein-Uhlenbeck process (with zero initial condition).7

2.4

Testing Problem and Local Alternatives

The goal of the paper is to derive asymptotically eﬃcient tests for the value of the cointegrating vectors with controlled rejection probability under the null hypothesis for a range of
stochastic trend specifications. The diﬀerent orders of magnitude of zt and vt in (2) and (3)
suggest a local embedding of this null hypothesis against alternatives where Γyv = T −1 B for
B a constant r × k matrix, so that in model (5),
T

−1/2

bsT c

X
t=1

yt ⇒ Sz W (s) + B

7

Z sZ
0

u

H(u, t)dW (t)du.

−∞

The I(1) specification in (10) is the same as the I(1) specification given below (2) because the invariance
in (4) implies that the trend models are unaﬀected by premultiplication of H(s, t) (or G(s, t)) by an arbitrary
non-singular k × k matrix.

7

In this parametrization, the null hypothesis becomes
H0 : B = 0, H(s, t) ∈ H0

(12)

where H(s, t) is restricted to a set of functions H0 , that, in the unrestricted trend model
includes functions suﬃciently well behaved to ensure that there exists a cadlag version of
Rs
the process −∞ H(s, t)dW (t), or more restricted versions of H(s, t) as in (6), (8), (9), (10),
or (11).
Since our goal is to consider eﬃcient tests of the null hypothesis (12), we also need to
specify the alternative hypothesis. Our results below are general enough to allow for the
derivation of eﬃcient tests against any particular alternative with specified B = B1 and
stochastic trend process H(s, t) = H1 (s, t),
H1 : B = B1 , H(s, t) = H1 (s, t)

(13)

or, more generally, for tests that are eﬃcient in the sense of maximizing weighted average
power against a set of values for B1 and stochastic trend models H1 (s, t).
Our numerical results, however, focus on alternatives in which the stochastic trend vt is
I(1), so that H1 (s, t) satisfies (6) and (10). This is partly out of practical considerations:
while there is a wide range of potentially interesting trend specification, the computations
for any particular specification are involved, and these computational complications limit
the number of alternatives we can usefully consider.8 At the same time, one might consider
the classical I(1) model as an important benchmark against which it is useful to maximize
power–not necessarily because this is the only plausible model under the alternative, but
because a test that performs well against this alternative presumably has reasonable power
properties for a range of empirically relevant models. We stress that despite this focus on
the I(1) stochastic trend model for the alternative hypothesis (13), we restrict attention to
tests that control size for a range of models under the null hypothesis (12). The idea is to
control the frequency of rejections under the null hypothesis for any stochastic trend model
in H0 , so that the rejection of a set of cointegrating vectors cannot simply be explained by
the stochastic trends not being exactly I(1). In this sense, our approach is one of “robust”
cointegration testing, with the degree of robustness governed by the size of the set H0 .
8

If the non-cointegrated components are modelled as I(1) with a deterministic linear time trend, one
could choose H1 (s, t) as the sum of (10) and stΣτ to obtain tests that maximize weighted average power
for vt that is I(1) with a linear trend of slope β τ , with a weighting function β τ ∼ N (0, Στ /T 2 ). We do not
pursue this further, though.

8

2.5

Summary

To summarize, this section has introduced the time domain representation of the cointegrated
model with a focus on the problem of inference about the space of cointegrating vectors. In all
respects except one, the representation is the standard one: the data are expressed as a linear
function of a canonical vector or common trends and a vector of I(0) components. Under
the null, certain linear combinations of the data do not involve the common trends. Because
the null only restricts the column space of the matrix of cointegrating vectors, attention is
restricted to invariant tests. The goal is to construct tests with best power for an alternative
value for the matrix of cointegrating vectors under a particular model for the trend (or best
weighted average power for a collection of B1 and H1 (s, t)). The formulation diﬀers from the
standard one only in that it allows the model for trend under the null to be less restrictive
than the standard formulation. Said diﬀerently, because of potential uncertainty about the
specific form of the trend process, the formulation restricts attention to tests that control
size for a range of diﬀerent trend processes. This generalization complicates the problem of
constructing eﬃcient tests by introducing a potentially large number of nuisance parameters
(associated with the trend process) under the null hypothesis.

3

Low-Frequency Representation of the Model

Cointegration is a restriction on the low-frequency behavior of time series, and as discussed
in the introduction, we therefore focus on the low-frequency behavior of (yt , xt ). This lowfrequency variability is summarized by a small number, q, of weighted averages of the data.
In this section we discuss these weighted averages and derive their limiting behavior under
the null and alternative hypotheses.

3.1

Low-Frequency Weighted Averages

We use weights associated with the cosine transform, where the j’th weight is given
√
by Ψj (s) = 2 cos(jπs). For any sequence {at }Tt=1 , the j’th weighted average will be denoted
by
Z 1
T
X
−1
Ψj (s)absT c+1 ds = ιjT T
Ψj ( t−1/2
)at
(14)
AT (j) =
T
0

t=1

where ιjT = (2T /jπ) sin(jπ/2T ) → 1 for all fixed j. As demonstrated by Müller and Watson
(2008), the weighted averages AT (j), j = 0, · · · , q, essentially capture the variability in the
9

sequence corresponding to frequencies below qπ/T .
We use the following notation: with at a h×1 vector time series, let Ψ(s) = (Ψ1 (s), Ψ2 (s),
R1
· · · ,Ψq (s))0 denote the q × 1 vector of weighting functions, and AT = 0 Ψ(s)a0bsT c+1 ds the
q × h matrix of weighted averages of the elements of at , where Ψ0 (s) is excluded to make the
results invariant to adding constants to the data. Using this notation, the q × r matrix YT
and the q × k matrix XT summarize the variability in the data corresponding to frequencies
lower than qπ/T . With q = 12, (YT , XT ) capture variability lower than the business cycle
(periodicities greater than 8 years) for time series that span 50 years (postwar data) regardless
of the sampling frequency (months, quarters, weeks, etc.). This motivates us to consider the
behavior of these matrices as T → ∞, but with q held fixed.
The large-sample behavior of XT and YT follows from the behavior of ZT and VT . Using
the assumed limits (2) and (3), the continuous mapping theorem, and integration by parts
for the terms involves ZT , one obtains
"
#
"
#
T 1/2 ZT
Z
⇒
(15)
V
T −1/2 VT
where

"

with
ΣV Z =

Z 1 µZ
0

ΣV V

=

Z

1

−∞

vec Z
vec V

#

∼N

Ã "
0,

Irq ΣZV
ΣV Z ΣV V

#!

¶
[H(s, t) ⊗ Ψ(s)]ds [Sz ⊗ Ψ(t)]0 dt
t∨0
µZ 1
¶ µZ 1
¶0
[H(s, t) ⊗ Ψ(s)]ds
[H(s, t) ⊗ Ψ(s)]ds dt.

(16)

1

t∨0

(17)

t∨0

The relative scarcity of low-frequency information is thus formally captured by considering
the weak limits (2) and (3) as pertinent only for the subspace spanned by the weight function
Ψ(·), yielding (15) as a complete characterization of the relevant properties of the error
correction term zt and the common stochastic trend vt .
Using Γyv = T −1 B, equation (5) implies that YT = ZT + T −1 VT B 0 and XT = VT . Thus,
#
"
# "
"
#
Y
Z + V B0
T 1/2 YT
⇒
=
(18)
X
T −1/2 XT
V
where

"

vec Y
vec X

#

¡
¢
∼ N 0, Σ(Y,X)
10

(19)

with
Σ(Y,X) =

3.2

"

Ir ⊗ Iq B ⊗ Iq
0
Ik ⊗ Iq

#"

Ir ⊗ Iq ΣZV
ΣV Z ΣV V

#"

Ir ⊗ Iq
0
0
B ⊗ Iq Ik ⊗ Iq

#

.

(20)

“Best” Low-Frequency Hypothesis Tests

We consider invariant tests of H0 against H1 given in (12) and (13) based on the data
{yt , xt }Tt=1 . Because we are concerned with the model’s implications for the low-frequency
variability of the data, we restrict attention to tests that control asymptotic size for all
models that satisfy (18)-(20). Our goal is to find an invariant test that maximizes power
subject to this restriction, and for brevity we will refer to such a test as a “best” test.
Müller (2008) considers the general problem of constructing asymptotically most powerful
tests subject to asymptotic size control over a class of models such as ours. In our context,
his results imply that asymptotically best tests correspond to the most powerful invariant
tests associated with the limiting distribution (19).
Thus, the relevant testing problem has a simple form: vec(Y, X) has a normal distribution
with mean zero and covariance matrix that depends on B. Under the null B = 0, while under
the alternative B 6= 0. Tests are restricted to be invariant to the group of transformations
(Y, X) → (Y A0yy , XA0xx + Y A0xy )

(21)

where Ayy and Axx are nonsingular, and Ayy , Axx , and Axy are otherwise unrestricted. Thus,
the hypothesis testing problem becomes the problem of using an invariant procedure to test
a restriction on the covariance matrix of a multivariate normal vector.

4

Bounds on Power and Size

The general version of the hypothesis testing problem we are facing is a familiar one: Let
U denote a single observation of dimension m × 1. (In our problem, U corresponds to the
maximal invariant for (Y, X)). Under the null hypothesis U has probability density fθ (u)
with respect to some measure μ, where θ ∈ Θ is a vector of nuisance parameters. (In our
problem, the vector θ describes the stochastic trend process under the null hypothesis and
determines Σ(Y,X) via (17) and (20)). Under the alternative, U has known density h(u).
(Choices for h(u) for our problem will be discussed in Subsection 5.2.1.) Thus, the null and

11

alternative hypothesis are
H0 : The density of U is fθ (u), θ ∈ Θ
H1 : The density of U is h(u),

(22)

and possibly randomized tests are (measurable) functions ϕ : Rm 7→ [0, 1], where ϕ(u) is the
probability of rejecting the null hypothesis when observing U = u, so that size and power
R
R
are given by supθ∈Θ ϕfθ dμ and ϕhdμ, respectively.
This section presents two results on power and size in this general problem. The first
result is an upper bound on the power of any valid test of H0 versus H1 . This bound will be
useful for our specific problem because, as we show below, it can be computed numerically
and we will construct a feasible test that (essentially) achieves the bound when there is a
single cointegrating vector. This shows that the feasible test that we construct is eﬃcient
when r = 1. The second result provides a lower bound on the size under a null hypotheses
other than H0 for any test that is powerful against H1 . To see why this is useful in our context
suppose that H0 specifies that the stochastic trend follows an I(1) process, and consider a
test that exploits features of the I(1) process to increase power. Uncertainty about the trend
process means that it is useful know something about the rejection frequency of tests under
null hypotheses that allow for more general trends, such as the unrestricted trend model (3)
or other less restricted versions described above. The second result provides a lower bound
on this rejection frequency, where a large value of this lower bound highlights the fragility
of tests that exploit a particular H0 to obtain more powerful inference.

4.1

An Upper Bound on Power

A standard device for problems such as (22) is to consider a Neyman-Pearson test for a
related problem in which the null hypothesis is replaced with a mixture
Z
HΛ : The density of U is fθ dΛ(θ)
where Λ is a probability distribution for θ with support in Θ. The following lemma shows
that the power of the Neyman-Pearson test of HΛ versus H1 provides an upper power bound
for tests of H0 versus H1 .
Lemma 1 Let ϕΛ be the best level α test of HΛ against H1 . Then for any level α test ϕ of
R
R
H0 against H1 , ϕΛ hdμ ≥ ϕhdμ.
12

R
Proof. Since ϕ is a level α test of H0 , ϕfθ dμ ≤ α for all θ ∈ Θ. Therefore,
RR
RR
ϕfθ dμdΛ(θ) =
ϕfθ dΛ(θ)dμ ≤ α (where the change in the order of integration is
allowed by Fubini’s Theorem), so that ϕ is also a level α test of HΛ against H1 . The result
follows by the definition of a best test.
This result is closely related to Theorem 3.8.1 of Lehmann and Romano (2005) which
provides conditions under which a least upper bound on the power for tests H0 versus H1 is
associated with a “least favorable distribution” for θ, and that using this distribution for Λ
produces the least upper power bound. The least favorable distribution Λ∗ has the characteristic that the resulting ϕΛ∗ is a level α test for testing H0 versus H1 . Said diﬀerently, if ϕΛ∗
is the best level α test of HΛ∗ against H1 and is also a level α test for testing H0 versus H1 ,
then ϕ∗ = ϕΛ∗ , that is ϕΛ∗ is the most powerful level α test of H0 versus H1 . Unfortunately,
while the test associated with the least favorable distribution solves the testing problem (22),
there is no general and constructive method for finding the least favorable distribution Λ∗
(and it does not always exist).
With this in mind, Lemma 1 is stated so that Λ is not necessarily the least favorable
distribution. That is, the bound in Lemma 1 holds for any probability distribution Λ. The
goal of the numerical analysis carried out below is to choose Λ to approximate the least upper
bound. Importantly, even if one cannot identify the least favorable distribution, Lemma 1
shows that the power of ϕΛ provides a valid bound for the power of any test of H0 versus
H1 , for any Λ.

4.2

A Lower Bound on Size under an Auxiliary Null

Now consider the "Larger" auxiliary null hypothesis
HL : The density of U is fθ (u), θ ∈ ΘL
with an associated mixture
HΛL : The density of U is

Z

fθ dΛL (θ)

where ΛL has support in ΘL: . (In our problem HL will be a null hypothesis that allows for
a less restricted trend process than under H0 . Thus if H0 allows only for an I(1) trend, HL
might allow for a local-to-unity trend or one of the more general trends processes discussed
in Section 2.)
Consider any test ϕ of level α0 under H0 with power of at least β. The following lemma
provides a lower bound on the rejection frequency under the auxiliary null HL .
13

R
R R
R R
Lemma 2 Suppose the test ϕ∗ = 1[ ϕhdμ ≥ λ1 ϕ fθ dΛ0 (θ)dμ + λ2 ϕ fθ dΛL (θ)dμ]
RR ∗
with λ1 , λ2 ≥ 0 has rejection probability α0 =
ϕ fθ dΛ0 (θ)dμ under HΛ0 , αL =
RR ∗
R ∗
ϕ fθ dΛL (θ)dμ under HΛL and power β = ϕ hdμ. Then for any test ϕ of level α0
R
under H0 and power of at least β, supθ∈ΘL ϕfθ dμ ≥ αL .

Proof. By a variant of the generalized Neyman-Pearson Lemma (Theorem 3.6.1 in
Lehmann and Romano (2005)), the test ϕ∗ solves the program
Z Z
min ϕ fθ dΛL (θ)dμ
ϕ
Z Z
Z
s.t.
ϕ fθ dΛ0 (θ)dμ ≤ α0 and
ϕhdμ ≥ β.

R
RR
ϕfθ dμdΛ0 (θ) =
Since ϕ is a level α0 test of H0 , ϕfθ dμ ≤ α for all θ ∈ Θ, and
RR
ϕfθ dΛ0 (θ)dμ ≤ α (where the change in the order of integration is allowed by FuR
bini’s Theorem). Also, by assumption, the power of the test ϕ is at least β, ϕhdμ ≥ β,
R
so that ϕ satisfies the two constraints in the program above. Thus, supθ∈ΘL ϕfθ dμ ≥
R R
R
R
ϕ fθ dΛL (θ)dμ ≥ ϕ∗ fθ dΛL (θ)dμ = αL .
This lemma is particularly useful in conjunction with Lemma 1: Suppose application of
Lemma 1 yields that no 5% level test of a relatively restricted H0 can have power of more
than, say, 70%. This suggests that there could indeed exist a 5% level test ϕ with power,
say, 67%, and one might want to learn about the size properties of such tests under the more
general null hypothesis HL . Lemma 2 provides a way of computing a lower bound on this
size that is valid for any test with power of at least 67%. So if this size distortion is large,
then without having to determine the class of 5% level tests of H0 with power of at least
67%, one can already conclude that all such tests will be fragile. In the numerical section
below, we discuss how to determine a suitable Λ0 and ΛL to obtain a large lower bound αL
(λ1 and λ2 are determined through the two constraints on ϕ∗ ).

5

Computing Bounds

In this section we compute the power and size bounds from the last section. The analysis
proceeds in four steps. First, we derive the density of the maximal invariant of (Y, X); this
density forms the basis of the likelihood ratio. Second, since the density of the maximal
invariant depends on the covariance matrix of (Y, X), we discuss the parameterization of
Σ(Y,X) under the null and alternative hypotheses. In the third step we describe how the
14

mixing distributions Λ, Λ0 and ΛL are chosen to yield tight bounds. Finally, we present
numerical values for the bounds.

5.1

Density of a Maximal Invariant

Recall that we are considering tests that are invariant to the group of transformations
(Y, X) → (Y A0yy , XA0xx + Y A0xy ) where Ayy and Axx are nonsingular, and Ayy , Axx , and
Axy are otherwise unrestricted. Any invariant test can be written as a function of a maximal
invariant (Theorem 6.2.1 in Lehmann and Romano (2005)), so that by the Neyman-Pearson
lemma, the most powerful invariant test rejects for large values of the likelihood ratio statistic of a maximal invariant. The remaining challenge is the computation of the density of
a maximal invariant, and this is addressed in the following theorem.
Theorem 1 If vec(Y, X) ∼ N (0, Σ(Y,X) ) with positive definite Σ(Y,X) and q > r + k, the
density of a maximal invariant of (21) has the form
0
−1/2
Σ−1
(det Ω)−1/2 Eω [| det(ω Y )|q−r | det(ωX )|q−r−k ]
c(det Σ(Y,X) )−1/2 (det V0Y
(Y,X) V0Y )

where c does not depend on Σ(Y,X) , ωY and ω X are random r × r and k × k matrices,
respectively, with (vec ω0y , vec ω0x )0 ∼ N (0, Ω−1 ),
0
−1
0
−1
−1 0
−1
Ω = DY0 X Σ−1
(Y,X) DY X − DY X Σ(Y,X) V0Y (V0Y Σ(Y,X) V0Y ) V0Y Σ(Y,X) DY X ,

DY X = diag(Ir ⊗ Y, Ik ⊗ X), V0Y = (00rq×rk , Ik ⊗ Y 0 )0 , and Eω denotes integration with respect
to ω Y and ω X , conditional on (Y, X).
Theorem 1 shows that the density of a maximal invariant can be expressed in terms of
absolute moments of determinants of jointly normally distributed random matrices, whose
covariance matrix depends on (Y, X). We do not know of a useful and general closed-form
solution for this expectation; for r = k = 1, however, Nabeya’s (1951) results for the absolute
moments of a bivariate normal yields an expression in terms of elementary functions, which
we omit for brevity. When r + k > 2, the moments can be computed via Monte Carlo
integration. However, computing accurate approximations is diﬃcult when r and k are
large, and the numerical analysis reported below is therefore limited to small values of r and
k.

15

5.2

Parameterization of Σ(Y,X)

Since the density of the maximal invariant of Theorem 1 depends on Σ(Y,X) , the derivation
of eﬃcient invariant tests requires specification of Σ(Y,X) under the alternative and null
hypothesis. We discuss each of these in turn.
5.2.1

Specification of Σ(Y,X) under the Alternative Hypothesis

As discussed above, we focus on the alternative where the stochastic trends follow an I(1)
process, so that H(s, t) satisfies (6) and (10). There remains the issue of the value of B
(the coeﬃcients that show how the trends aﬀect Y ) and R (the correlation of the Wiener
processes describing the I(0) variables, zt , and the common trends, vt ). For these parameters,
we consider point-valued alternatives with B = B1 and R = R1 ; the power bounds derived
below then serve as bounds on the asymptotic power envelope over these values of B and R.
Invariance reduces the eﬀective dimension of B and R somewhat, and this will be discussed
in the context of the numerical results presented below.
5.2.2

Parameterization of Σ(Y,X) under the Null Hypothesis

From (20), under the null hypothesis with B = 0, the covariance matrix Σ(Y,X) satisfies
#
"
Irq ΣZV
.
Σ(Y,X) =
ΣV Z ΣV V
The null model’s specification of the stochastic trend determines the rq × kq matrix ΣZV
and the kq × kq matrix ΣV V by the formulae given in (17). Since these matrices contain a
finite number of elements, it is clear that even for nonparametric specifications of H(s, t),
the eﬀective parameter space for low-frequency tests based on (Y, X) is finite dimensional.
We collect these nuisance parameters in a vector θ ∈ Θ.
Section 2 discussed several trend processes, beginning with the general process given in
(3) with an unrestricted version of H(s, t), and then five restricted models: (i) the “G” model
in (6), (ii) the “Diagonal” model (8), (iii) the “Stationary” model (9), (iv) the local-to-unity
model (11), and (v) the I(1) model (10). The appendix discusses convenient parameterization
for Σ(Y,X) for these five restricted models, and the following lemma provides the basis for
parameterizing Σ(Y,X) when H(s, t) is unrestricted.

16

Lemma 3 (a) For any (r + k)q × (r + k)q positive definite matrix Σ∗ with upper left rq × rq
block equal to Irq , there exists an unrestricted trend model with H(s, t) = 0 for t < 0 such
that Σ∗ = E[vec(Z, V )(vec(Z, V ))0 ].
(b) If r ≤ k, this H(s, t) can be chosen of the form H(s, t) = G(s, t)Sv , where (Sz0 , Sv0 )
has full rank.
The lemma shows that when H(s, t) is unrestricted or r ≤ k and H(s, t) = G(s, t)Sv ,
the only restriction that the null hypothesis imposes on Σ(Y,X) is that ΣY Y = Irq .9 In other
words, since ΣZV and ΣV V have rkq2 + kq(kq + 1)/2 distinct elements, an appropriately
chosen θ of that dimension determines Σ(Y,X) under the null hypothesis in the unrestricted
model, and in the model where H(s, t) = G(s, t)Sv for r ≤ k.

5.3

Approximating the Least Upper Power and Greatest Lower
Size Bound

We develop two methods to approximate the power bound associated with the least favorable
distribution from Section 4, and use the second method also to determine a large lower size
bound. First, we develop an algorithm that simultaneously determines a low upper bound
on power, and a level α test whose power is close to that bound. This algorithm is entirely
generic in the sense that it does not exploit any specificities of the low-frequency robust
cointegration testing problem; in practice, it only requires that the densities fθ and h can be
quickly evaluated numerically. The computational complexity is such, however, that it can
only be applied when θ is low-dimensional; as such, it is useful for our problem only in the
I(1) and local-to-unity stochastic trend model for r = k = 1. Second, when the dimension of
θ is large we choose Λ (and Λ0 and ΛL for Lemma 2) so the null and alternative distributions
are close in some numerically convenient metric. Two numerical results suggest that this
second method produces a reasonably accurate estimate of the lower bound: the method
produces power bounds only marginally higher than the first method (when the first method
is feasible), and when r = 1 we find that the method produces a power bound that can be
achieved by a feasible test that we present in the next section.
We discuss the two methods in turn.
9

Without the invariance restriction (21), this observation would lead to an analytic least favorable distribution result: Factor the density of (Y, X) under the alternative into the product of the density of Y , and
the density of X given Y . By choosing ΣV Z and ΣV V under the null hypothesis appropriately, the latter
term cancels, and the Neyman-Pearson test is a function of Y only.

17

5.3.1

Low Dimensional Nuisance Parameter
R
Suppose that LRΛ = h(U)/ fθ (U )dΛ(θ) is a continuous random variable for any Λ, so that
by the Neyman-Pearson Lemma, ϕΛ is of the form ϕΛ = 1[LRΛ > cvΛ ], where the critical
RR
value cvΛ is chosen to satisfy the size constraint
ϕΛ fθ dμdΛ(θ) = α. Then by Lemma
R
1, the power of ϕΛ , β Λ = ϕΛ hdμ, is an upper bound on the power of any test that is
level α under H0 . If Λ is not the least favorable distribution, then ϕΛ is not of size α under
R
H0 , i.e. supθ∈Θ ϕΛ fθ dμ > α. Now consider a version of ϕΛ with a size-corrected critical
value cvcΛ > cvΛ , that is ϕcΛ = 1[LRΛ > cvcΛ ] with cvcΛ chosen to satisfy the size constraint
R
supθ∈Θ ϕcΛ fθ dμ = α. Because the size adjusted test ϕcΛ is of level α under H0 , the least
upper power bound must be between β cΛ and β Λ . Thus, if β cΛ is close to β Λ , then β cΛ serves
as a good approximation to the least upper bound.
The challenge is to find an appropriate Λ. This is diﬃcult because, in general, no closed
form solutions are available for the size and power of tests, so that these must be approximated by Monte Carlo integration. Brute force searches for an appropriate Λ are not computationally feasible. We exploit numerical advantages of discrete distributions for Λ, that
have point masses at only N points, and smooth out the Monte Carlo integration estimates
of size and power, so that gradient methods can be employed. The suggested algorithm
is related to, but distinct from those developed in Nelson (1966), Kempthorne (1987) and
Sriananthakumar and King (2006), and is described in detail in the appendix.
5.3.2

High Dimensional Nuisance Parameter

The dimension of θ can be very large in our problem: even when r = k = 1, the model with
unrestricted stochastic trend leads to θ of dimension q 2 + q(q + 1)/2 so that θ contains 222
elements when q = 12. Approximating the least upper power bound directly then becomes
numerically intractable. This motivates a computationally practical method for computing
a low (as oppose to least) upper power bound.
The method restricts Λ so that it is degenerate with all mass on a single point, say θ∗ ,
which is chosen so that the null distribution of the maximal invariant of Theorem 1 is close to
its distribution under the alternative. Intuitively, this should make it diﬃcult to distinguish
the null from the alternative hypothesis, and thus lead to a low power bound. Also, this
choice of θ∗ ensures that draws from the null model look empirically reasonable, as they are
nontrivial to distinguish from draws of the alternative with an I(1) stochastic trend.
Since the density of the maximal invariant is quite involved, θ∗ is usefully approximated

18

by a choice that makes the multivariate normal distribution of vec(Y, X) under the null close
to its distribution under the alternative, as measured by a convenient metric. We choose
θ∗ to minimize the Kullback-Leibler divergence (KLIC) between the null and alternative
distributions. Since the bounds from Lemmas 1 and 2 are valid for any mixture, numerical
errors in the KLIC minimization do not invalidate the resulting bound. Details are provided
in the appendix.

5.4

Numerical Bounds

Table 1 shows numerical results for power and size bounds for 5% level tests with q = 12.
Results are shown for r = k = 1 (panel A), r = 1 and k ≥ 2 (panel B), and r = 2 and k = 1
(Panel C).10 Numerical results for larger values of n = r + k are not reported because of the
large number of calculations required to evaluate the density in large models.
Power depends on the values of B and R under the alternative, and results are presented
for various values of these parameters. Because of invariance, when r = 1 (as in panels A and
B), or k = 1 (as in panel C), the distribution of the maximal invariant depends on B and R
only through ||B||, ||R||, and, if ||R|| > 0, on tr(R0 B)/(||B|| · ||R||). Thus, in panel A, where
r = k = 1, results are shown for two values of |B|, three values of |R| and for R · B < 0 and
R · B > 0, while panels B and C show results for three values of ω = tr(R0 B)/(||B|| · ||R||)
when ||R|| > 0. All of the results in Table 1 use the KLIC minimized values of θ as described
in the last subsection. Table 2 compares this KLIC-based bounds to to the numerical least
upper power bounds when the parameter space is suﬃciently small to allow calculation of
the numerical least upper bounds.
To understand the formatting of Table 1, look at panel A. The panel contains italicized
and non-italicized numerical entries. The non-italicized numbers are power bounds, and the
italicized numbers are size bounds. The first column in the table shows the trend specification allowed under H0 . The first entry, labelled “unr” corresponds to the unrestricted
trend specification in (3) and the other entries correspond to the restricted trend processes
discussed in Section 2. Because r = k = 1, there are no restrictions imposed by the assumption that H(s, t) = G(s, t)Sv or that G is diagonal, so these models are not listed in
panel A. Stationarity (G(s, t) = G(s − t)) is a restriction, and this is the second entry in
the first column. The final two entries correspond to the local-to-unity (“LTU ”) and I(1)
10

The results shown in panel B were computed using the KLIC minimized value of θ for the model with
r = 1 and k = 2. The appendix shows the resulting bounds are valid for k ≥ 2.

19

restrictions. The numerical entries shown in the rows corresponding to these trend models
are the power bounds. For example, the non-italicized entries in the first numerical column
show power bounds for |R| = 0 and |B| = 7, which are 0.36 for the unrestricted null, 0.41
when the trend is restricted to be stationary, 0.50 when the trend is restricted to follow a
local-to-unity process, and 0.50 when the trend is further restricted to follow an I(1) process.
The second column of panel A shows the auxiliary null hypotheses HL , corresponding
to the null hypothesis H0 , shown in the first column. The entries under HL represent less
restrictive models than H0 . For example, when H0 restricts the null to be stationary, an
unrestricted trend process (“unr”) is shown for HL , while when H0 restricts the trend to be
I(1), the less restrictive local-to-unity, stationary, and unrestricted nulls are listed under HL .
The numerical entries for these rows (shown in italics in the table) are the lower size bounds
for HL for 5% level tests under H0 and with power that is 3 percentage points less than
the corresponding power bound shown in the table. For example, from the first numerical
column of panel A, the power bound for the I(1) version of H0 is 0.50. For any test with
size no larger than 5% under this null and with power of at least 0.47 (= 0.50 − 0.03), the
size under a null that allows an unrestricted trend (“unr” under HL ) is at least 12%, the
size under a null that restricts the trend to be stationary is at least 8%, and the size under
a null that restricts the trend to follow a local-to-unity process is at least 4%.
Looking at the entries in Panel A, two results stand out. First, and not surprisingly,
restricting tests so that they control size for the unrestricted trend process leads to a nonnegligible reduction in power. For example, when |B| = 7, and R = 0, the power bound is
0.36, for tests that control size for unrestricted trends, the bound increases to 0.41 for tests
that control size for stationary trends, and increases to 0.50 for tests that control size for
local-to-unity or I(1) trend processes. Second, whenever there is a substantial increase in
power associated with restricting the trend process, there are large size distortions under
the null hypothesis without this restriction. For example, Elliott’s (1998) observation that
eﬃcient tests under the I(1) trend have large size distortions under a local-to-unity process
is evident in the table. From the table, when |B| = 7, |R| = 0.9, and R · B > 0, the power
bound for the null with an I(1) trend is 0.95, but any test that controls size for this null and
has power of at least 0.92 will have size that is greater than 0.50 when the trend is allowed to
follow a local-to-unity process. However, addressing Elliott’s (1998) concern by controlling
for size in the local-to-unity model, as in the analysis of Stock and Watson (1996) or Jansson
and Moreira (2006) does not eliminate the fragility of the test. For example, with the same
values of B and R, the power bound for the null that allows for a local-to-unity trend is
20

0.67, but any test that controls size for this null and has a size of at least 0.64 will have a
size greater than 0.32 when the trend is unrestricted.
Panels B (r = 1 and k = 2) and C (r = 2 and k = 1) show qualitatively similar results.
Indeed these panels show even more fragility of tests that do not allow for general trends.
For example, the lower size bound for the unrestricted trend null exceeds 0.50 in several
cases for tests that restrict trends to be I(1), local-to-unity, or stationary.
When r = k = 1, it is feasible to approximate the least upper power bound for the
I(1) and local-to-unity trend restrictions using the method described in subsection 5.3.1.
As described in the appendix, the approximate least upper bounds (LUB) in Table 2 are
no more than 2.5 percentage points above the actual least upper bound, apart from Monte
Carlo error. The diﬀerences with the KLIC minimized power bounds are small, suggesting
that the bounds in Table 1 are reasonably tight.

6

Eﬃcient Y -Only Tests

The primary obstacle for constructing eﬃcient tests of the null hypothesis that B = 0 is the
large number of nuisance parameters associated with the stochastic trend (the parameters
that determine H(s, t)). These parameters govern the values of ΣZV and ΣV V , which in turn
determine ΣY X and ΣXX . Any valid test must control size over all values of these nuisance
parameters. Wright (2000) notes that this obstacle can be avoided by ignoring the xt data
and basing inference only on yt , since under the null hypothesis, yt = zt . This section takes
up Wright’s suggestion and discusses eﬃcient low-frequency “Y -only” tests.11
We have two related goals. The first is to study the power properties of these tests
relative to the power bounds computed in the last section. As it turns out, when r = 1 (so
there is only a single cointegrating vector), this Y -only test essentially achieves the power
bound, so the test eﬃciently uses all of the information in Y and X. Given the eﬃciency
property of the Y -only test, the second goal is to develop simple formulae for implementing
the test. We discuss these in reverse order, first deriving a convenient formulae for the test
11

Wright (2000) implements this idea using a “stationarity” test of the I(0) null proposed by Saikkonen and
Luukonen (1993), using a robust covariance matrix as in Kwiatkowski, Phillips, Schmidt, and Shin (1992) for
the test proposed in Nyblom (1989). This test relies on a consistent estimator of the spectral density matrix
of zt at frequency zero. But consistent estimation requires a lot of pertinent low frequency information,
and lack thereof leads to well-known size control problems (see for example, Kwiatkowski, Phillips, Schmidt,
and Shin (1992), Caner and Kilian (2001), and Müller (2005)). These problems are avoided by using the
low-frequency components of yt only; see Müller and Watson (2008) for further discussion.

21

statistic and then studying the power of the resulting test.

6.1

Eﬃcient Tests against General Alternatives

The distribution of vec Y ∼ N (0, ΣY Y ) follows from the derivations in Section 3: Under the
null hypothesis, ΣY Y = Irq , and under the alternative, ΣY Y depends on the local alternative
B, the properties of the stochastic trend and its relationship with the error correction term
Z. For a particular choice of alternative, the testing problem thus becomes H0 : ΣY Y = I
against H1 : ΣY Y = ΣY Y 1 , and the invariance requirement (21) becomes
Y → Y A0yy for arbitrary nonsingular r × r matrices Ayy .

(23)

The density of the maximal invariant is given in the following theorem.
Theorem 2 (a) If vec Y ∼ N (0, ΣY Y ) with positive definite ΣY Y and q > r, the density of
a maximal invariant to (23) has the form
c1 (det ΣY Y )−1/2 (det ΩY )−1/2 EωY [| det(ω Y )|q−r ]
where c1 does not depend on ΣY Y , ωY is an r × r random matrix with vec ωY ∼ N (0, Ω−1
Y ),
−1
ΩY = (Ir ⊗ Y )0 ΣY Y (Ir ⊗ Y ), and EωY denotes integration with respect to the distribution of
ω Y (conditional on Y ).
(b) If in addition, ΣY Y = ṼY Y ⊗ Σ̃Y Y , then the density simplifies to
−q/2
c2 (det Σ̃Y Y )−r/2 det(Y 0 Σ̃−1
YYY )

where c2 does not depend on ΣY Y .
As in Theorem 1, part (a) of this theorem provides a formula for the density of a maximal
invariant in terms of absolute moments of the determinant of a multivariate normal matrix
with a covariance matrix that depends on the data. Part (b) provides an explicit and simple
formula when the covariance matrix is of a specific Kronecker form. This form arises under
the null hypothesis with ΣY Y = Irk , and under alternatives where each of the r putative
error correction terms in yt have the same low-frequency covariance matrix. For a simple
alternative hypothesis with ΣY Y 1 = ṼY Y 1 ⊗ Σ̃Y Y 1 , the best test then rejects for large values
of det(Y 0 Y )/ det(Y 0 Σ̃−1
Y Y 1 Y ). The form of weighted average power maximizing tests over a
set of alternative covariance matrices ΣY Y 1 are also easily deduced from Theorem 2 parts
(a) and (b).
22

6.2

Eﬃcient Tests against I(1) Alternative

As discussed above, the numerical results in this paper focus on the benchmark alternative
where the stochastic trend follows an I(1). Under this alternative, yt follows a multivariate
“local level model” (cf. Harvey (1989)), which is the alternative underlying well-known
“stationarity” tests such as Nyblom and Mäkeläinen (1983), Nyblom (1989), Kwiatkowski,
Phillips, Schmidt, and Shin (1992), Nyblom and Harvey (2000), Jansson (2004), and others.
Thus, suppose that the stochastic trend satisfies (6) and (10), so that
T

−1/2

bsT c

X
t=1

yt ⇒ Wz (s) + B

Z

s

Wv (t)dt.

(24)

0

The optimal test depends on the value of B under the alternative, and it is convenient to
assume
B = bS,
(25)
where b is a scalar and S is the r × k selection matrix equal to S = [Ir , 0k−r ] when r ≤ k
and S = [Ik , 0k−r ]0 when r > k. The invariance requirement (4) implies that (25) is without
loss of generality whenever there exist orthonormal r × r and k × k matrices Py and Px
such that Py BPx = ||B||S, which is always the case when min(r, k) = 1. In the formulation
(25), when r ≤ k (so that the number of linearly independent cointegrating vectors does not
exceed the number of common trends), each element of yt is the sum of an I(0) component
and an I(1) component, where the common relative magnitude of the two components is
determined by b. When r > k, there are fewer trends than cointegrating vectors, so that yt
can be rotated such that the trends load on only a subset of the variables in yt . This is the
“reduced rank” formulation used, for example, in the multivariate stationarity test proposed
in Eliasz, Stock, and Watson (2004).
In this parameterization, the covariance matrix of Y depends on b and R = Sz Sv0 =
E[Wz (1)Wv (1)0 ], the correlation between the Wiener processes describing zt and vt . A
straightforward calculation shows that ΣY Y can be written as
ΣY Y = (Ir ⊗ Iq ) + b2 (SS 0 ⊗ D) + b(SR0 ⊗ F ) + b(RS 0 ⊗ F 0 )

(26)

where F and D are q × q matrices, where D is a diagonal matrix with i’th diagonal element
equal to (πi)−2 and F = [fij ], with fij = 0 if i and j are both even or odd, and fij =
4/[π 2 (i2 − j 2 )] otherwise. (The simple diagonal form of D is due to the particular choice of
the weighting functions Ψ in (14); see Section 2.3 in Müller and Watson (2008)).
23

Examination of (26) suggests three simplifications of the testing problem. First, because
F = −F 0 , the final two terms cancel when SR0 is symmetric. (When r ≤ k, SR0 is symmetric
if Rij = Rji for i, j ≤ r, and when r > k, symmetry requires Rij = Rji for i, j ≤ k and
Rij = 0 for i > k and all j.) Thus, when SR0 is symmetric, ΣY Y does not depend on R,
which implies that the eﬃcient test constructed using R = 0 is uniformly most powerful for
all values of R with SR0 symmetric. Second, when r ≤ k, SS 0 = Ir , so in this case when SR0
is symmetric, ΣY Y = Ir ⊗(Iq +b2 D), and from part (b) of Theorem 2, the optimal test rejects
for large values of det(Y 0 Y )/ det(Y 0 (Iq + b2 D)−1 Y ). (This statistic with r = 1 was labeled
“LF ST ” in Müller and Watson (2008) and we continue to use that label here.) Finally, when
SR0 is symmetric, but r > k, a calculation based on Theorem 2 (a) produces an expression
for the best test. These simplifications are summarized in the following corollary.
Corollary 1 For the alternative (24) and (25), the Neyman-Pearson test constructed with
R = 0 is uniformly most powerful over all values of R with SR0 symmetric, and rejects for
large values of
LF ST (b) = det(Y 0 Y )/ det(Y 0 (Iq + b2 D)−1 Y )
when r ≤ k, and for large values of
ξ(b) = det(Y 0 Y )(q+k−r)/2 det(Y 0 (Iq + b2 D)−1 Y )−k/2 EωY [| det(ωY )|q−r ]
0
2
−1
0
when r > k, where vec ω Y ∼ N (0, Ω−1
Y ) and ΩY = diag(Ik ⊗ Y (Iq + b D) Y, Ir−k ⊗ Y Y ).

The corollary shows that when r ≤ k the best test is based on LF ST , but when r > k,
the optimal test statistic is ξ given in part (b) of the corollary.12
√
Table 3 presents 10%, 5%, and 1% critical values for the point-optimal LF ST (10/ r)
test for various values of r and q, where the alternative is chosen so that 5% test has
√
approximately 50% power for b = 10/ r.
12

This test statistic is more diﬃcult to calculate than LF ST because ξ depends on the term
EωY [| det(ω Y )|q−r ], which requires evaluating absolute moments of order q − r from an r2 -dimensional
multivariate normal distribution. In an earlier version of this paper we compared the power of ξ(b) and
LF ST (b) for (r, k) = (2, 1), (3, 1), and (3, 2) in models with SR0 symmetric for q = 12. When r = 2 and
k = 1, the power of the LF ST (b) statistic is within 3% of the power of ξ(b) when the power of ξ(b) is less
than 50%, but the diﬀerence increases to nearly 10% when the power of ξ(b) exceeds 80%. The diﬀerences
are more substantial when r = 3 and k = 1, where the power diﬀerence is approximately 7% when power is
50%; the power diﬀerences are negligible when r = 3 and k = 2. Eliasz, Stock, and Watson (2004) report
similar power diﬀerences in a related testing problem.

24

6.3

Power of Eﬃcient Y -only Tests

Table 4 shows the power of the point-optimal LF ST test and the corresponding power
envelope of the Y -only test for r = 1 in panel A, and r = 2 and k = 1 in panel B. In panel
A, the power envelope is given by the LF ST test evaluated at the value of B under the
alternative, while the point-optimal test LF ST is evaluated at B = 10. The power of the
point-optimal test is very close to the Y -only power envelope. A more interesting comparison
involves the power of the point-optimal LF ST test with the (Y, X)-power bound computed
in the last section. Because the Y -only tests control size for any trend process, the relevant
comparison is the unrestricted H(s, t) bound shown in panels A and B of Table 1. The
power of the point-optimal LF ST test diﬀers from the Table 1 power bound by no more
than 0.01 when |B| = 7 and no more that 0.03 when |B| = 14. Thus, for all practical
purposes, the point-optimal LF ST test corresponds to an eﬃcient test in models with a
single cointegrating vector (r = 1).
The results in panel B of Table 4 are somewhat diﬀerent. Here, because r > k, the
Y -only power envelope is given by the ξ test of Corollary 1 and not by the LF ST test. The
numerical results show that the relative power of the LF ST test depends on both B and
R, and the loss can be large when B, and R are large and orthogonal (ω = 0). Comparing
results in panel B of Table 4 to the corresponding results in panel C of Table 1, also show
that in this case there are potentially additional power gains associated with using data on
both Y and X. For example, when ||B|| = 20, ||R|| = 0.9, and ω = 0, the Y -only power
envelope is 0.86, while the (Y, X) power bound in 0.94.

7

Wages, Prices, Employment and Output

The long-run relationship between wages (W ), prices (P ), employment (N) and real output
(Y ) has been of long-standing interest to economists. Labor’s share of income is given by
W N/Y P , or (with lower case letters denoting logarithms) w + n − y − p, and is one of
the “Great Ratios of Economics” investigated by Klein and Kosobud (1961). The average
value of this ratio plays a key role for calibrating the aggregate function in macroeconomic
business cycle models (see King, Plosser, and Rebelo (1988) and Cooley and Prescott (1995)).
Diﬀerences between prices (p) and unit labor costs (w + n − y) or between real wages (w − p)
and labor productivity (y + n) have been used as “cost-push” or error correction terms in

25

wage inflation or price inflation Phillips curve equations.13 All of these suggest that w, p, y,
and n are cointegrated, with cointegrating relationship
w − βpp − βy y − β nn

(27)

where β p = 1, β y = 1, and β n = −1.14 In this section we investigate this hypothesis using
post-war data for the United States.
We use data for the non-farm business and non-financial corporate sectors of the U.S.
economy. Restricting attention to the non-farm business sector eliminates measurement
issues associated with the government sector and with rental income from owner-occupied
housing. Restricting attention even further to the non-financial corporate sector eliminates
problems associated with the allocation of proprietors’ income to wages and capital returns
and price index problems in the financial sector.15 Wages are measured by total labor
compensation per hour (which includes employer-paid fringe benefits), prices and output
are measured by the sector-specific price deflator and real output index, and employment is
measured by total employee hours. Data are available quarterly from 1947 for the non-farm
business sector and from 1958 for the non-financial corporate sector.16
In the standard model, (w, p, y, n) are driven by two real common trends representing
labor supply and productivity, and by a nominal common trend that aﬀects prices and wages.
The real common trends are typically modeled as I(1) processes (with drift) and the nominal
common trend is often modeled as an I(2) process. Of course, there is substantial uncertainty
associated with these I(1)/I(2) specifications, and so it is interesting to compare inference
from standard cointegration methods that rely critically on these specifications with methods
that allow for more general common trend processes.
The top panel of Table 5 shows estimates of the cointegrating coeﬃcients and standard
errors computed using a standard I(1)/I(2) estimator (here, Stock and Watson’s (1993)
DOLS estimator). The estimates are somewhat close to their null values of β p = 1, β y = 1,
and β n = −1, but the Wald statistic soundly rejects this null for both data sets. The
13

Sargan (1964) is a classic reference (although he used a smooth function of time as a proxy for productivity). More recent examples include Gordon (1985, 1998), Blanchard and Katz (1997), Brayton, Roberts,
and Williams (1999), Mehra (2000) and Staiger, Stock, and Watson (2001).
14
Hall (1986) seems to be the earliest paper to use formal I(1) cointegration methods to investigate this
relationship.
15
Gomme and Rupert (2004) discuss how these measurement issues aﬀect inference about labor’s share.
16
The data are from the DRI Economics Database. The series used are LBCPU and LCPB (wages for the
non-farm business and non-financial corporate sector), LBGDPU and LGDPB (prices), LBIPU and LIPB
(output), and LBMNU and LMNB (employee hours).

26

final row of the table shows results using the LF ST statistic of the last section.17 This
test rejects the null for the non-farm business sector (p-value = 0.004) but not for the
non-financial corporate sector (p-value = 0.28). Thus, standard I(1)/I(2) inference and
robust low-frequency inference coincide for the non-farm business sector, but not for the
non-financial corporate sector.
Figure 1 plots the putative error correction term w − p − y + n for each sector. The
data for the non-farm business sector exhibits a negative trend over sample period, which
readily explains the rejection of the null hypothesis for this sector. In contrast, the data
for the non-financial corporate sector do not exhibit an obvious trend and (to our eyes) the
plot appears to be consistent with an I(0) series. Inference based on the LF ST statistic is
consistent with this conclusion. The puzzle is then why the null is rejected so dramatically
using standard cointegration methods.
A plausible explanation is that the common stochastic trends are not exactly I(1)/I(2),
as is assumed by the standard method. Uncertainty about the nature of the common trends
is evident in confidence sets for largest autoregressive roots (local-to-unity parameters), longmemory parameters, or other nesting of the I(1) and I(2) models. This statistical evidence
is reinforced by introspection about low-frequency changes in macroeconomy such as the
productivity slowdown of the 1970s, the productivity rebound in the 1990s, changing demographics of the labor force, and shifts in monetary policy over the past five decades.. These
phenomena are consistent with a range of common trend processes beyond the standard
I(1)/I(2) model. Yet, standard inference is based on specific characteristics of the I(1)/I(2)
trend model, which sharpens inference when the common trends follow these processes, but
can lead to mistaken rejections otherwise. This can be seen in Figure 2 which plots confidence
sets for LF ST and for the I(1)/I(2) model. (The confidence sets impose the constraint that
β y = −β n so that we can show the plot in two dimensions.) The confidence ellipse constructed using the I(1)/I(2) model is markedly smaller than the ellipse constructed from
LF ST and does not contain the null value (indicated by the symbol "+" in the figure).
In contrast, the LF ST confidence set, while larger, approximately eﬃciently exploits the
available low-frequency information about the cointegrating vector in absence of specific
assumptions about the common trends, and does not reject β p = 1, β y = 1, and β n = −1.
17

The LFST statistic was computed using the Ψj transformation in (14) where q was chosen to isolate frequencies associated with periods longer than eight years. The non-farm business data set contains 62 years of
data so that q = 15, while the non-financial corporated data set contains 51 years of data, so³that q = 12. Let´
¡Pq
¢ Pq
YT2l
2
ting YT denote the q × 1 vector of obsevations, the test statistic is LF ST =
Y
,
/
2
2
l=1 T l
l=1 1+b /(πl)
with b = 10.

27

8

Conclusion

This paper studies inference about the cointegrating vector in a framework in which the
common stochastic trends are modelled in a flexible way beyond the standard I(1) framework.
The problem is studied with the low-frequency transformation approach suggested by Müller
and Watson (2008). The paper derives bounds on the power of tests that control size over
flexible stochastic trend specifications, and which maximize power against alternatives with
the usual I(1) trend. We find that a low-frequency version of Wright’s (2000) test (LF ST )
essentially achieves the upper power bound in the model with r = 1 cointegrating vectors,
making it an attractive choice for applied work.
The construction of eﬃcient tests for the value of the cointegrating vector that control
size for an unrestricted trend model when r > 1 remains an open question. However, the
power bounds computed here provide a useful check for the eﬃciency of ad hoc tests that
might be suggested for this problem, and the LF ST test remains a practically useful valid
method also if r > 1.
If the stochastic trend model is tightly parametrized, the size control issue becomes
muss less severe, as the dimension of the nuisance parameter is then small. Our algorithm
for computing the approximate least upper bound on power for such models also yields
an approximately power maximizing, feasible test that controls size. The paper thus also
suggests a way to approximately eﬃciently exploit strong a priori knowledge about the
stochastic trend.
The suggested method is generic in the sense that it computes an approximately eﬃcient
test in the presence of a low dimensional nuisance parameters under the null hypothesis.
This type of problem arises naturally in nonstandard testing problems, so we would expect
the method to be useful also in other contexts. For instance, the recent analysis of Elliott
and Müller (2009) of inference about the pre and post break parameter value builds on this
algorithm.

28

A
A.1

Appendix
Proof of Theorem 1

Write Y = (Y10 , Y20 , Y30 )0 and X = (X10 , X20 , X30 )0 , where Y1 and X1 have r rows, and Y2 and X2 have
k rows. Consider the one-to-one mapping h : Rq×n 7→ Rq×n with
⎛
⎞ ⎛
⎞
QY 1 QX1
Y1
Y1−1 X1
⎜
⎟ ⎜
⎟
h(Y, X) = Q = ⎝ QY 2 QX2 ⎠ = ⎝ Y2 (Y1 )−1
X2 − Y2 Y1−1 X1
⎠.
−1
−1
QY 3 QX3
Y3 (Y1 )−1 (X3 − Y3 Y1 X1 )(X2 − Y2 Y1 X1 )−1

A straightforward calculation shows that (vec Q0Y 2 , vec Q0Y 3 , vec Q0X3 ) is a maximal invariant to
(21). The inverse of h is given by
⎛

⎞
QY 1
QY 1 QX1
⎜
⎟
h−1 (Q) = ⎝ QY 2 QY 1
QX2 + QY 2 QY 1 QX1 ⎠ .
QY 3 QY 1 QX3 QX2 + QY 3 QY 1 QX1
Using matrix diﬀerentials (cf. Chapter 9 of Magnus and Neudecker (1988.)), a calculation shows
that the Jacobian determinant of h−1 is equal to (det QY 1 )q−r+k (det QX2 )q−k−r . The density of Q
is thus given by
−1
(2π)−qn/2 (det Σ(Y,X) )−1/2 | det QY 1 |q−r+k | det QX2 |q−k−r exp[− 12 (vec h−1 (Q))0 Σ−1
(Y,X) (vec h (Q))]

and we are left to integrate out QY 1 , QX1 and QX2 to determine the density of the maximal
invariant.
Now consider the change of variables from QY 1 , QX1 , QX2 to ω Y , ωX and ω Y X
QY 1 = Y1 ω Y
−1
−1
QX1 = ω −1
Y Y1 X1 ω X − ω Y ω Y X

QX2 = (X2 − Y2 Y1−1 X1 )ω X

with Jacobian determinant (det Y1 )r (det(X2 − Y2 Y1−1 X1 ))k det(−ωY )−k . Noting that with this
change, h−1 (Q) = (Y ωY , Xω X − Y ω Y X ), we find that the density of the maximal invariant is equal
to
Z
(2π)−qn/2 (det Σ(Y,X) )−1/2 | det Y1 |q+k | det(X2 − Y2 Y1−1 X1 )|q−r | det ω Y |q−r | det ω X |q−k−r
0
0
0
0
· exp[− 12 vec(Y ω Y , XωX − Y ω Y X )0 Σ−1
(Y,X) vec(Y ω Y , Xω X − Y ω Y X )]d(vec ω Y , vec ω X , vec ω Y X ) .

29

Since vec(Y ω Y , XωX − Y ω Y X ) = DY X vec(ω Y , ω X ) − V0Y vec(ω Y X ), we have
vec(Y ωY , Xω X − Y ω Y X )0 Σ−1
(Y,X) vec(Y ω Y , Xω X − Y ω Y X )
D
vec(ω Y , ω X )
= vec(ω Y , ω X )0 DY0 X Σ−1
(Y,X) Y X
0 0
−1
− 2 vec(ω Y , ω X )DY0 X Σ−1
(Y,X) V0Y vec(ω Y X ) + vec(ω Y X ) V0Y Σ V0Y vec(ω Y X ).

The result now follows from integrating out ω Y X by ’completing the square’.

A.2

Proof of Theorem 2

The proof to part (a) mimics the proof to Theorem 1 and is omitted. To prove part (b),
note that because of invariance, we can set ṼY Y = Ir without loss of generality, so that
−1/2 = det(Y 0 Σ̃−1 Y )−r/2 . Since
det ΣY Y = (det Σ̃Y Y )r , ΩY = (Ir ⊗ Y 0 Σ̃−1
Y Y Y ) and (det ΩY )
YY
Y
ω
),
the
density
in
part
(a)
of
the
Theorem
becomes pro(vec ωY )0 ΩY (vec ω Y ) = tr(ω 0Y Y 0 Σ̃−1
Y
YY
portional to
Z
−r/2
0 −1
−r/2
| det ω Y |q−r exp[− 12 tr(ω 0Y Y 0 Σ̃−1
det(Y Σ̃Y Y Y )
(det Σ̃Y Y )
Y Y Y ω Y )]d(vec ω Y ).
1/2 ω , so that | det ω |q−r = det(Y 0 Σ̃−1 Y )−(q−r)/2 | det ω̃ |q−r and vec ω =
Let ω̃ Y = (Y 0 Σ̃−1
Y
Y
Y
Y
YYY )
YY
−1
0
−1/2
) vec ω̃ Y , and the Jacobian determinant of the transformation from ωY to ω̃ Y
(Ir ⊗ (Y Σ̃Y Y Y )
−1
0
−r/2 . Thus, the density is proportional to
is det(Ir ⊗ (Y Σ̃Y Y Y )−1/2 ) = (Y 0 Σ̃−1
YYY)
Z
−q/2
| det ω̃Y |q−r exp[− 12 tr(ω̃ 0Y ω̃ Y )]d(vec ω̃ Y ),
Y
)
(det Σ̃Y Y )−r/2 det(Y 0 Σ̃−1
YY

and the result follows.

A.3

Proof of Lemma 3

We first establish a preliminary result.
Lemma 4 For any t > 0 and integer κ, the functions Ψl : [0, t] 7→ R with Ψl (s) =
l = 1, · · · , κ are linearly independent.

√
2 cos(πls),

P
Proof. Choose any real constants cj , j = 1, · · · , κ, so that κj=1 cj Ψj (s) = 0 for all s ∈ [0, t].
P
(i)
(i)
Then also κj=1 cj Ψj (0) = 0 for all i > 0, where Ψj (0) is the ith (right) derivative of Ψj at
√
(i)
s = 0. A direct calculation shows Ψj (0) = (−1)i/2 2(πj)i for even i. It is not hard to see that
P
(i)
the κ × κ matrix with j,ith element (−1)i/2 (πj)i is nonsingular, so that κj=1 cj Ψj (0) = 0 for
i = 2, 4, · · · , 2κ can only hold for cj = 0, j = 1, · · · , κ.

30

R1
For the proof Lemma 3 we construct H(s, t) such vec Z = 0 (Ir ⊗ Ψ(t))Sz dW (t) and vec V =
R1R1
0 t (H(s, t) ⊗ Ψ(s))ds dW (t) have the specified covariance matrix. The proof of the slightly more
diﬃcult part (b), where H(s, t) = G(s, t)Sv , is based on the following observations:
(i) Ignoring the restriction on the form of vec V , it is straightforward to construct an appropriate multivariate normal vector vec V from a linear combination of vec Z and ζ, where
ζ ∼ N (0, Ikq×kq ) independent of Z.
(ii) Suppose that R = S was allowed, where S = (Ir , 0r×(k−r) ). Then Sz = SSv , vec Z ∼
R1
0 FZR(t)Sv dW (t) for FZ (t) = S ⊗ Ψ(t), and one can also easily construct ζ as in (i) via
1
ζ = 0 Fζ (t)Sv dW (t) by an appropriate choice of Fζ . Since Ito-Integrals are linear, one
R1
could thus write vec V = 0 F (t)Sv dW (t) with F a linear combination of FZ and Fζ , using
observation (i).
(iii) For any matrix function F : [0, 1] 7→ Rkq×k that is equal to zero on the interval (1 − ε, 1] for
R1
some ε > 0, one can set G(s, t) = (Ik ⊗ Ψ(s)0 J(t)−1 )F (t), where J(t) = t Ψ(s)Ψ(s)0 ds and
R1
R1R1
obtain 0 t (G(s, t) ⊗ Ψ(s))ds Sv dW (t) = 0 F (t)Sv dW (t), since for any matrix A with k
rows and vector v, A ⊗ v = (Ik ⊗ v)A.
The following proof follows this outline, but three complications are addressed: R = S is not
allowed; the matrix function F needs to be zero on the interval (1 − ε, 1], which does not happen
Rs
automatically in the construction in (ii); one must verify that the process 0 G(s, t)Sv dW (t) admits
a cadlag version.
Set Sz to be the first r rows of In . Since Ψl (1 − s) = (−1)l Ψl (s) for all l ≥ 1, Lemma 4 implies
R1
that J(t) = t Ψ(s)Ψ(s)0 ds and Iq − J(t) are nonsingular for any t ∈ (0, 1). The rq × 1 random
R 1−ε
vector vec Zε = 0 (Sz ⊗ Ψ(s))dW (s) thus has nonsingular covariance matrix Ir ⊗ Σεq , where
Σεq = Iq − J(1 − ε). Also, since
Ã
!
Ir ⊗ Iq Σ12
∗
Σ =
Σ21
Σ22
ε
is positive definite, so is Irq − Σ12 Σ−1
22 Σ21 , so that we can choose 0 < ε < 1 such that Ir ⊗ Σq −
ε −1
Σ12 Σ−1
22 Σ21 is positive definite. With that choice of ε, also Σ22 − Σ21 (Ir ⊗ Σq ) Σ12 is positive
definite.
For part (a) of the lemma, define the [0, 1] 7→ Rkq×n function Fa (t) = Aa (In ⊗ Ψ(t)), where
Aa = (Aa1 , Aa2 ) with Aa1 = Σ21 (Ir ⊗Σεq )−1 and Aa2 = (Σ22 −Σ21 (Ir ⊗Σεq )−1 Σ12 )1/2 (Ik ⊗(Σεq )−1/2 ).
For part (b) of the lemma, choose 0 < ρ < 1 so that Σ22 − ρ−2 Σ21 (Ir ⊗ Σεq )−1 Σ12 is positive
definite. Choose Sv to be the first k rows of In multiplied by ρ, so that R = Sz Sv0 = ρS. Let Ψ̃1 (s) be
scaled residuals of a continuous time projection of 1[s ≤ 1 − ε]Ψq+1 (s) on {1[s ≤ 1 − ε]Ψl (s)}ql=1 on
the unit interval, and let Ψ̃j (s), j = 2, · · · , kq be the scaled residuals of continuous time projection

31

of 1[s ≤ 1 − ε]Ψq+j (s) on {1[s ≤ 1 − ε]Ψl (s)}ql=1 and {1[s ≤ 1 − ε]Ψ̃l (s)}j−1
l=1 . By Lemma 4, Ψ̃j (s),
j = 1, · · · , kq, are not identically zero, and we can choose their scale to make them orthonormal.
Let Ψ̃(s) = (Ψ̃1 (s), · · · , Ψ̃kq (s))0 , the k × 1 vector ιk = (1, 0, · · · , 0)0 , and Ab = (Ab1 , Ab2 ) with
Ab1 = ρ−1 Σ21 (Ir ⊗Σεq )−1 and Ab2 = (Σ22 −ρ−2 Σ21 (Ir ⊗Σεq )−1 Σ12 )1/2 . Now define the [0, 1] 7→ Rkq×n
function
Ã
!
S ⊗ Ψ(t)
Fb (t) = Ab
Sv .
ι0k ⊗ Ψ̃(t)
For both parts, that is for i ∈ {a, b}, set
Hi (s, t) = (Ik ⊗ Ψ(s)0 J(t)−1 )Fi (t) for t ∈ [0, 1 − ε]
and Hi (s, t) = 0 otherwise. With this choice
vec Vi =

Z

0

=

Z

0

=

Z

1Z 1

(Hi (s, t) ⊗ Ψ(s))ds dW (t)

t
1−ε Z 1
t

((Ik ⊗ Ψ(s)Ψ(s)0 J(t)−1 )Fi (t))ds dW (t)

1−ε

Fi (t)dW (t).

0

Thus
0

E[(vec Vi )(vec Vi ) ] =

Z

1−ε

Fi (t)Fi (t)0 dt

0

E[(vec Vi )(vec Z)0 ] =

Z

1−ε

0

R1

Fi (t)(Sz ⊗ Ψ(t))0 dt

since vec(Z − Zε ) = 1−ε (Ir ⊗ Ψ(t))Sz dW (t) is independent of vec Vi . A direct calculation
R 1−ε
R 1−ε
now shows that 0 Fa (t)Fa (t)0 dt = Aa (In ⊗ Σεq )A0a , 0 Fa (t)(Sz ⊗ Ψ(t))0 dt = Aa (Sz0 ⊗ Σεq ),
R
R 1−ε
0 dt = A diag(I ⊗ Σε , I )A0 and 1−ε F (t)(S ⊗ Ψ(t))0 dt = ρA (S 0 ⊗ Σε ), so that
F
(t)F
(t)
r
z
b
b
b
kq
b
b z
q
q
b
0
0
0
0
from the definitions of Ai , E[(vec Vi )(vec Vi ) ] = Σ22 and E[(vec Vi )(vec Z) ] = Σ21 , as required.
Rs
It thus remains to show that the processes 0 Hi (s, t)dW (t), i ∈ {a, b}, admit a cadlag version.
√
Recall that ||A|| is the Frobenius norm of the real matrix A, ||A|| = tr A0 A, which is submultiplicative. If v ∼ N (0, Σ), then E[||v||4 ] = E[(v0 v)2 ] = 2 tr(Σ2 ) + (tr Σ)2 ≤ 3(tr Σ)2 , so that with
Rt
Rt
0
s Hi (u, λ)dW (λ) ∼ N (0, s Hi (u, λ)Hi (u, λ) dλ), we find
E[||

Z

s

t

Z

4

t

Hi (u, λ)dW (λ)|| ] ≤ 3(tr
Hi (u, λ)Hi (u, λ)0 dλ)2
s
Z t
≤ 3( ||Hi (u, λ)||2 dλ)2 .
s

32

Thus, for 0 ≤ s < t ≤ 1, we have with ψ(s) = dΨ(s)/ds
Z t
Z s
E[||
Hi (t, λ)dW (λ) −
Hi (s, λ)dW (λ)||4 ]
0
0
Z t
Z s
(Hi (t, λ) − Hi (s, λ))dW (λ) +
Hi (t, λ)dW (λ)||4 ]
= E[||
0
s
Z t
Z s
||Hi (t, λ) − Hi (s, λ)||2 dλ +
||Hi (t, λ)||2 dλ]2
≤ 3[
s

0
2

−1 2

≤ 3[k ( sup

0≤λ≤1−ε

4

≤ 3k ( sup

||J(λ)

|| ||Fi (λ)|| )(||Ψ(s) − Ψ(t)||2 + (t − s) sup ||Ψ(λ)||2 )]2

−1 4

0≤λ≤1−ε

||J(λ)

2

4

0≤λ≤1
2 2

2

|| ||Fi (λ)|| )( sup ||ψ(λ)|| + sup ||Ψ(λ)|| ) (t − s)2
0≤λ≤1

0≤λ≤1

R1
where the last inequality follows from Ψ(t) − Ψ(s) = (t − s) 0 ψ(s + λ(t − s))dλ, so that by
Kolmogorov’s continuity theorem (p. 14 of Oksendal (2000)), there exist continuous (and thus
Rs
cadlag) versions of the stochastic processes 0 Hi (s, t)dW (t), i ∈ {a, b}.

A.4

Parameterization of Σ(Y,X) under H0 in the restricted trend
models

G-model with r > k: Because of invariance, it is without loss of generality to assume that the first
r−k rows of R are equal to zero, so that the first r−k columns of Z are independent of V . The joint
distribution of V and the last k rows of Z are then just as in the model with r = k, so that Lemma
3 implies that in the G-model with r > k, Σ(Y,X) is of the form Σ(Y,X) = diag(Ir−k ⊗ Iq , Σ∗k ) under
the null hypothesis, where Σ∗k is any positive definite k2 q × k 2 q matrix with upper left kq × kq block
equal to the identity matrix. The nuisance parameter θ is thus of dimension k2 q 2 + kq(kq + 1)/2
Diagonal G-model: Let ZV and ζ be q×k random matrices with vec(Z, ZV , ζ) ∼ N (0, Σ(Z,ZV ,ζ) ),
where
Ã"
#
!
Ir R
Σ(Z,ZV ,ζ) = diag
, Ik ⊗ Iq .
R0 Ik
A construction as in the proof of Lemma 3 implies that the j’th column of V can be chosen as
an arbitrary linear combination of the j’th column of ZV and the j’th column of ζ, j = 1, · · · , k
(subject to the constraint that the resulting matrix is positive definite). Thus, Σ(Y,X) may be
parametrized as Σ(Y,X) = A(Z,ZV ,ζ) Σ(Z,ZV ,ζ) A0(Z,ZV ,ζ) , where
A(Z,ZV ,ζ) =

Ã

0
0
Irq
0 diag(AV,1 , AV,2 , · · · , AV,k ) diag(Lζ,1 , Lζ,2 , · · · , Lζ,k )

!

,

AV,j are arbitrary q × q matrices and Lζ,j are arbitrary lower diagonal q × q matrices. Including
R, θ is thus of dimension rk + kq2 + kq(q + 1)/2.

33

In the stationary diagonal model where G(s, t) = diag(g1S (s − t), · · · , gkS (s − t)), we set gjS to be
step functions
¸
∙
ng
X
x
i
i−1
≤
<
(28)
ξ j,i 1
gjS (x) =
ng + 1
1+x
ng + 1
i=1

for ng = 40 and some scalar parameters ξ j,i , j = 1, · · · , k, i = 1, · · · , ng . The steps occur at the
points i/(ng + 1 − i), so that more flexibility is allowed for small values of x (half of the points
are associated with values of x less than 1, for example). The values of ΣZV and ΣV V then follow
from (17). In this specification θ contains the kng coeﬃcients ξ j,i and the rk coeﬃcients in the
correlation matrix R. While the specification (28) only captures a subset of all possible covariance
matrices Σ(Y,X) in the (nonparametric) stationary model, any test that controls size for all functions
H(s, t) of the form H(s, t) = diag(g1S (s − t), · · · , gkS (s − t))Sv a fortiori has to control size for the
specification (28). The upper bounds on power of tests that control size for all values of θ under
(28) are therefore also upper bounds for tests that control size for the generic stationary stochastic
trend model.
In the local-to-unity model, a straightforward (but tedious) calculation determines the value
of Σ(Y,X) as function of the k × k matrix C and the r × k correlation matrix R, so that θ is of
dimension k2 + rk. Finally, the I(1) model is a special case of the local-to-unity model with C = 0.

A.5

Kullback-Leibler Divergence Problem of Section 5.3.2

Let Σ1 denote the covariance matrix of vec(Y, X) under a specific I(1) alternative as described in
Subsection 5.2.1(that is, for specific values of B = B1 and R = R1 ), let Σ0 (θ) with θ ∈ Θ be the
covariance matrix of vec(Y, X) under the null for the relevant restrictions on the stochastic trend,
and define the nq × nq matrix (recall that n = r + k)
#
"
0
γ yz ⊗ Iq
A(γ) =
γ xz ⊗ Iq γ xv ⊗ Iq
where γ yz is r ×r, γ xz is k ×r, and γ xv is k ×k. This yields A(γ) vec(Y, X) ∼ N (0, A(γ)Σ0 (θ)A(γ)0 ).
Denote the Kullback-Leibler divergence between the nq × 1 distributions N (0, Σ1 ) and N (0, Σ0 )
∗
as K(Σ1 , Σ0 ) = 12 ln(det Σ1 / det Σ0 ) + 12 tr(Σ−1
0 Σ1 ) − nq. The value of θ is chosen to numerically
solve
K(Σ1 , A(γ)Σ0 (θ∗ )A(γ)0 ) =
min
K(Σ1 , A(γ)Σ0 (θ)A(γ)0 ),
(29)
min
γ∈Rr2 +k2 +kr

θ∈Θ,γ∈Rr2 +k2 +kr

that is, θ∗ numerically minimizes the Kullback-Leibler divergence (or KLIC) between the null and
alternative densities of (Y, X), allowing for transformations as described by A(γ) under the null.
While these transformations do not aﬀect the implied distribution of the maximal invariant, they
do in general lead to a diﬀerent θ∗ , which we found to yield a slightly lower upper bound. The

34

minimization problem is over a high dimensional parameter, but the objective function is quickly
computed and well behaved, so that numerical minimization is feasible.

A.6

Algorithm for Approximating the Least Upper Power Bound
and Optimal Test

A computationally more convenient variation of the size adjustment idea described in the main
text is as follows: Starting from the level α test ϕΛ = 1[LRΛ > cvΛ ] of HΛ against H1 , for some
small ε > 0, let cvεΛ be an adjusted critical value so that the resulting test ϕεΛ = 1[LRΛ > cvεΛ ]
R
(with cvεΛ > cvΛ ) has only slightly lower power than ϕΛ , i.e. ϕεΛ hdμ = β Λ − ε. Now if ϕεΛ is of
R
level α under H0 , i.e. supθ∈Θ ϕεΛ fθ dμ < α, then we have a level α test of H0 against H1 with
power that is only ε below β Λ , and the least upper bound is again sandwiched between β Λ and
β Λ − ε. The advantage of this method over the direct size adjustment discussed in the text is that
the size adjustment is costly to compute, while Λ can often be quickly dismissed by checking its
size control for a small number of values of θ under H0 .
Now consider discrete distributions for Λ: Let ΘN = {θ1 , · · · , θN } ⊂ Θ for some N > 1 and
consider the restricted null hypothesis HN :The density of U is fθ , θ ∈ ΘN . In this restricted
problem, the least favorable distribution is fully described by the point masses p∗i on θi , i =
PN ∗
P
∗
∗
∗
1, · · · , N , where N
i=1 pi = 1. The resulting test ϕN is thus of the form ϕN = 1[ i=1 pi fθi /h <
1/ cvN ]. Note that by construction, the test ϕ∗N is of level α on ΘN ⊂ Θ. The central idea of the
algorithm is to identify a (hopefully not too large) set of points ΘN so that corresponding adjusted
test ϕ∗ε
N is of level α on the whole set Θ.
P
pi fθi /h < 1/ cv] evaluated at
Introduce the notation ϕ(θ̄, p̄, cv)(u) for the test ϕ = 1[ N
PN i=1
0
0
u, with θ̄ = (θ1 , · · · , θN ) and p̄ = (p1 , · · · , pN ) , and
i=1 pi = 1 (but p̄ is not necessarily
N
the least favorable distribution on Θ ). The rejection probability of ϕ under the alternative is
R
R
Π1 (θ̄, p̄, cv) = ϕ(θ̄, p̄, cv)(u)h(u)dμ(u), and it is Π0 (θ̄, p̄, cv; θ)(u) = ϕ(θ̄, p̄, cv)fθ (u)dμ(u) under
the null hypothesis with θ ∈ Θ. We numerically approximate Π0 (θ̄, p̄, cv; θ) by
ÃP
!
m
N
1 X
i=1 pi fθi (uj ) 1
ΥL
,
Π̂0 (θ̄, p̄, cv; θ) =
(30)
m
h(uj )
cv
j=1

where for some real L > 0, ΥL : R2 7→ R is defined as ΥL (x, y) = y L /(y L + xL ). The pseudo
random variables uj , j = 1, · · · , m have density fθ and are obtained by suitably transforming some
underlying pseudo random variables ξ j , uj = gθ (ξ j ), j = 1, · · · , m. The variables ξ j are drawn
only once in the evaluation of Π̂0 at diﬀerent arguments (so the transformation gθ depends on
θ). Define Π̂1 (θ̄, p̄, cv) analogously, with uj given by gh (ξ j ). Note that as L → ∞, ΥL (x, y) →
1[x < y] + 12 1[x = y], so that for L large, Π̂0 (θ̄, p̄, cv; θ) approximates the standard Monte Carlo
integration for the rejection probability of ϕ(θ̄, p̄, cv). The advantage of choosing L < ∞ is that

35

Π̂0 (θ̄, p̄, cv; θ) and Π̂1 (θ̄, p̄, cv) become smooth and diﬀerentiable functions of their arguments, which
greatly simplifies numerical optimizations. The computations in this paper were performed with
m = 25, 000 and L = 25.
The algorithm consists of three subroutines SR1, SR2 and SR3.
SR1 The routine takes θ̄ = (θ1 , · · · , θN ) as given, and returns an estimate of the least favorable
distribution on ΘN , as described by p̄ = (p1 , · · · , pN ). By Theorem 3.8.1 of Lehmann and
Romano (2005), the least favorable distribution p̄∗ = (p̄∗1 , · · · , p̄∗N )0 has the two properties (i)
R
R
ϕ(θ̄, p̄∗ , cv)fθl dμ ≤ α for l = 1, · · · , N ; and (ii) ϕ(θ̄, p̄∗ , cv)fθl dμ < α only if pl = 0. This
motivates the joint determination of p̄ and cv as numerical solutions to
Π̂0 (θ̄, p̄, cv; θl ) ≤ α and pl (Π̂0 (θ̄, p̄, cv; θl ) − α) = 0 for l = 1, · · · , N.

(31)

Specifically, we determine appropriate p̄ and cv by minimizing the objective function
N
X
(a0 pl + exp[a1 (Π̂0 (θ̄, p̄, cv; θl ) − α)])(Π̂0 (θ̄, p̄, cv; θl ) − α)2

(32)

l=1

where a0 = 100 and a1 = 2000. As a function of p̄ and cv, (32) is continuous and with
known first derivative, so that a standard quasi-Newton optimizer can be employed. Also,
the N 2 m numbers fθi (gθl (ξ j ))/h(gθl (ξ j )) for i = 1, · · · , N , l = 1, · · · , N and j = 1, · · · , m
can be computed and stored once to speed up the the evaluation of Π̂0 (θ̄, p̄, cv; θi ) and its
partial derivatives.
SR2 The routine takes (θ̄, p̄) as inputs and returns vectors (θ̄1 , p̄1 ) of length N1 ≤ N by eliminating
pairs of values (θj , pj ) with pj approximately equal to zero.
SR3 The routine takes (θ̄, p̄) as given and either identifies (θ̄, p̄) as yielding a suﬃciently precise
approximation to the least favorable distribution, or it returns a parameter value θ̂ ∈ Θ that
needs to be included in the set of points ΘN . Specifically, the routine consists of three steps:
(a) Solve for the real number cvΛ that satisfies Π̂0 (θ̄, p̄, cvΛ ; θl ) ≤ α for all l = 1, · · · , N ,
so that the test ϕ(θ̄, p̄, cvΛ ) is the (approximate) level α likelihood ratio test of HΛ : U
P
has density N
l=1 pl fθl against H1 .

(b) Compute β Λ = Π̂1 (θ̄, p̄, cvΛ ), and numerically solve for cvεΛ ≥ cvΛ such that
Π̂1 (θ̄, p̄, cvΛ ) − Π̂1 (θ̄, p̄, cvεΛ ) = ε. By Lemma 1, β Λ is (an estimate of) a power bound on
level α tests of H0 . As described above, the size adjustment as a function of the power
implies that if ϕ(θ̄, p̄, cvεΛ ) is of level α under H0 , then we have found a test whose
power is within ε of the bound. The computations in this paper use ε = 2.5%.

36

(c) Check on a grid of values ΘG ⊂ Θ whether ϕ(θ̄, p̄, cvεΛ ) controls size, i.e. evaluate
Π̂0 (θ̄, p̄, cv; θj ) for all θ ∈ ΘG in the finite set ΘG . If Π̂0 (θ̄, p̄, cv; θj ) > α for some
j, return θ̂ = θj . Otherwise, preliminarily accept β Λ as the approximate least upper
bound, and ϕ(θ̄, p̄, cvεΛ ) as an approximately eﬃcient test. As a practical matter, it
makes sense to return θ̂j = θj even if Π̂0 (θ̄, p̄, cv; θj ) is below, but very close to α. We
use a threshold of 4.8% for α = 5%.
Overall the algorithm iterates between the subroutines as follows:
1. Initialize θ̄ with N = 25 values of θj that are spread out over the grid ΘG , and call SR1.
2. Call SR2 to obtain a new N and (θ̄, p̄) pair.
3. While SR3 returns θ̂:
Add θ̂ to θ̄ (so that N is increased by one) and call SR1.
4. Call SR2 to obtain a new N and (θ̄, p̄) pair. Repeat Step 3.
5. Perform a final check on whether ϕ(θ̄, p̄, cvεΛ ) is a level α test by evaluating its rejection
probability over a fine grid of values for θj , using a diﬀerent set of draws of pseudo-random
variables ξ j in (30). For the correlation R, we use the grid R = −0.99, −0.96, · · · , 0.99 in
the I(1) model, and in the local-to-unity model, a square grid of the same values of R, and
C = −3, −2.5, · · · , −.5, 0, e−1 , e−0.8 , · · · , e6.8 , e7 .
The advantage of the thinning operation in SR2 is that it accelerates the computation of the
test statistic, and it facilitates the numerical minimization of (32). We do not call SR2 after each
call of SR1, though, because doing so can result in cycles, so that Step 3 above could potentially
result in an infinite loop. Step 4 “cleans” the feasible test and is skipped when only an estimator
of the least upper bound is required.

A.7

Validity of r = 1, k = 2 bounds for r = 1, k > 2

Here we show that the power bounds computed using r = 1 and k = 2 also serve as power bounds
for models with r = 1 and all values of k > 2.
To see why, first consider the alternative I(1) model as described in subsection 5.2.1, Y =
Z + V B 0 and X = V . Let P be a k × k orthonormal matrix whose last k − 2 rows are orthogonal to
R and B, and whose second row is orthogonal to R. Partition X = (X12 , X3k ), where X12 contains
the first two columns of X and X3k contains the remaining k − 2 columns. By invariance, there is
0 ,
no loss in generality in setting X = X̃P = (X̃12 , X̃3k )P, so that Y = Z + X̃P B 0 = Z + X̃12 B12

37

where X̃12 and B12 are the first two columns of X̃ and B, respectively, and the last k − 1 columns
of X̃ (and thus X̃3k ) are independent of Z. The group of transformations
(Y, X̃12 , X̃3k ) → (Y Ayy , X̃12 Ãxx + Y Axy , X̃3k )

(33)

for nonsingular Ayy and Ãxx is a subset of the transformations (Y, X̃) → (Y Ayy , X̃Axx + Y Axy ),
so the best invariant test to (33) is as least as powerful as the best invariant test to (21). Let
Q̃12 be a maximal invariant to (Y, X̃12 ) → (Y Ayy , X̃12 Ãxx + Y Axy ), such that {Q̃12 , X̃3k } is a
maximal invariant to (33). Since X̃3k is independent of (Y, X̃12 ), the density of {Q̃12 , X̃3k } under
the alternative factors as fa,Q̃12 · fa,X̃3k .
For all null models discussed in subsection 5.2.2, it is possible to choose X = (X12 , X3k ) = V
in a way such that X3k is independent of X12 with marginal distribution f0,X3k = fa,X̃3k , (i.e. it
corresponds to the I(1) model) and the possibilities for X12 and its relationship with Z are the
same as in the version of the model with k = 2. Thus, with this choice, the term fa,X̃3k cancels
in the likelihood ratio test of the maximal invariant to (33), and the testing problem corresponds
precisely to the model with k = 2.18 An upper bound for the model with r = 1 and k = 2 is
therefore also an upper bound for the model with r = 1 and k > 2.

18

This is not strictly true for the stationary G-model, which excludes I(1) stochastic trends. But the lowfrequency transformation of the suitably scaled stationary local-to-unity model converges in mean squared
to the I(1) model as the local-to-unity parameter approaches zero (cf. Section 2.4 of Müller and Watson
(2008)), so that the additional discriminatory power from X3k can be made arbitrarily small, and the
conclusion continues to hold.

38

References
Andrews, D. W. K. (1991): “Heteroskedasticity and Autocorrelation Consistent Covariance Matrix Estimation,” Econometrica, 59, 817—858.
Andrews, D. W. K., M. J. Moreira, and J. H. Stock (2008): “Eﬃcient Two-Sided
Nonsimilar Invariant Tests in IV Regression with Weak Instruments,” Journal of Econometrics, 146, 241—254.
Blanchard, O. J., and L. F. Katz (1997): “What We Know and Do Not Know About
the Natural Rate of Unemployment,” Journal of Economic Perspectives, 11, 51—72.
Bobkoski, M. J. (1983): “Hypothesis Testing in Nonstationary Time Series,” unpublished
Ph.D. thesis, Department of Statistics, University of Wisconsin.
Brayton, F., F. M. Roberts, and J. C. Williams (1999): “What’s Happened to
the Phillips Curve?,” Board of Governors of the Federal Reserve System Finance and
Discussion Series No. 49.
Campbell, J. Y., and M. Yogo (2006): “Eﬃcient Tests of Stock Return Predictability,”
Journal of Financial Economics, 81, 27—60.
Caner, M., and L. Kilian (2001): “Size Distortions of Tests of the Null Hypothesis of
Stationarity: Evidence and Implications for the PPP Debate,” Journal of International
Money and Finance, 20, 639—657.
Cavanagh, C. L. (1985): “Roots Local To Unity,” Working Paper, Harvard University.
Cavanagh, C. L., G. Elliott, and J. H. Stock (1995): “Inference in Models with
Nearly Integrated Regressors,” Econometric Theory, 11, 1131—1147.
Chan, N. H., and C. Z. Wei (1987): “Asymptotic Inference for Nearly Nonstationary
AR(1) Processes,” The Annals of Statistics, 15, 1050—1063.
Cooley, T., and E. C. Prescott (1995): “Economic Growth and Business Cycles,” in
Frontiers of Business Cycle Research, ed. by T. Cooley, pp. 1—38, Princeton, NJ. Princeton
University Press.
39

den Haan, W. J., and A. T. Levin (1997): “A Practitioner’s Guide to Robust Covariance
Matrix Estimation,” in Handbook of Statistics 15, ed. by G. S. Maddala, and C. R. Rao,
pp. 309—327. Elsevier, Amsterdam.
Eliasz, P., J. H. Stock, and M. W. Watson (2004): “Optimal Tests for Reduced Rank
Time Variation in Regression Coeﬃcients and for Level Variation in the Multivariate Local
Level Model,” Working paper, Princeton University.
Elliott, G. (1998): “The Robustness of Cointegration Methods When Regressors Almost
Have Unit Roots,” Econometrica, 66, 149—158.
Elliott, G., and U. K. Müller (2009): “Pre and Post Break Parameter Inference,”
Working Paper, Princeton University.
Elliott, G., and J. H. Stock (1994): “Inference in Time Series Regression When the
Order of Integration of a Regressor is Unknown,” Econometric Theory, 10, 672—700.
Gomme, P., and P. Rupert (2004): “Measuring Labor’s Share of Income,” Federal Reserve
Bank of Cleveland Policy Dicussion Paper No. 7.
Gordon, R. J. (1985): “Understanding the Inflation in the 1980s,” Brookings Papers on
Economic Activity, 1, 263—299.
(1998): “Foundations of Goldilocks Economy: Supply Shocks and the Time-Varying
NAIRU,” Brookings Papers on Economic Activity, 2, 297—346.
Granger, C. W. J. (1993): “What Are We Learning About the Long-Run?,” The Economic Journal, 103, 307—317.
Hall, S. G. (1986): “An Application of the Granger and Engle Two-Step Estimation
Procedure to United Kingdom Aggregate Wage Data,” Oxford Bulletin of Economics and
Statistics, 48, 229—239.
Harvey, A. C. (1989): Forecasting, Structural Time Series Models and the Kalman Filter.
Cambridge University Press.
Jansson, M. (2004): “Stationarity Testing with Covariates,” Econometric Theory, 20, 56—
94.
40

Jansson, M., and M. J. Moreira (2006): “Optimal Inference in Regression Models with
Nearly Integrated Regressors,” Econometrica, 74, 681—714.
Jeganathan, P. (1997): “On Asymptotic Inference in Linear Cointegrated Time Series
Systems,” Econometric Theory, 13, 692—745.
Johansen, S. (1988): “Statistical Analysis of Cointegration Vectors,” Journal of Economic
Dynamics and Control, 12, 231—254.
Kempthorne, P. J. (1987): “Numerical Specification of Discrete Least Favorable Distributions,” SIAM Journal on Scientific and Statistical Computing, 8, 171—184.
Kiefer, N., and T. J. Vogelsang (2005):

“A New Asymptotic Theory for

Heteroskedasticity-Autocorrelation Robust Tests,” Econometric Theory, 21, 1130—1164.
Kiefer, N. M., T. J. Vogelsang, and H. Bunzel (2000): “Simple Robust Testing of
Regression Hypotheses,” Econometrica, 68, 695—714.
King, R. G., C. I. Plosser, and S. T. Rebelo (1988): “Production, Growth and Business Cycles,” Journal of Monetary Economics, 21, 195—232.
Klein, L. R., and R. F. Kosobud (1961): “Some Econometrics of Growth: Great Ratios
of Economics,” Quarterly Journal of Economics, LXXV, 173—198.
Kwiatkowski, D., P. C. B. Phillips, P. Schmidt, and Y. Shin (1992): “Testing
the Null Hypothesis of Stationarity Against the Alternative of a Unit Root,” Journal of
Econometrics, 54, 159—178.
Lehmann, E. L., and J. P. Romano (2005): Testing Statistical Hypotheses. Springer, New
York.
Magnus, J. R., and H. Neudecker (1988.): Matrix Diﬀerential Calculus. Wiley, New
York.
Mehra, R. (2000): “Wage-Price Dynamics: Are They Consistent with Cost Push,” Federal
Bank of Richmond Economic Quarterly, 86, 27—43.

41

Müller, U. K. (2005): “Size and Power of Tests of Stationarity in Highly Autocorrelated
Time Series,” Journal of Econometrics, 128, 195—213.
(2007): “A Theory of Robust Long-Run Variance Estimation,” Journal of Econometrics, 141, 1331—1352.
(2008): “Eﬃcient Tests under a Weak Convergence Assumption,” Working Paper,
Princeton University.
Müller, U. K., and M. W. Watson (2008): “Testing Models of Low-Frequency Variability,” Econometrica, 76, 979—1016.
Nabeya, S. (1951): “Absolute Moments in 2-Dimensional Normal Distribution,” Annals of
the Institute of Statistical Mathematics, 3, 2—6.
Nelson, W. (1966): “Minimax Solution of Statistical Decision Problems by Iteration,” The
Annals of Mathematical Statistics, 37, 1643—1657.
Newey, W. K., and K. West (1987): “A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix,” Econometrica, 55, 703—708.
Nyblom, J. (1989): “Testing for the Constancy of Parameters Over Time,” Journal of the
American Statistical Association, 84, 223—230.
Nyblom, J., and A. Harvey (2000): “Tests of Common Stochastic Trends,” Econometric
Theory, 16, 176—199.
Nyblom, J., and T. Mäkeläinen (1983): “Comparisons of Tests for the Presence of
Random Walk Coeﬃcients in a Simple Linear Model,” Journal of the American Statistical
Association, 78, 856—864.
Oksendal, B. (2000): Stochastic Diﬀerential Equations: An Introduction with Applications.
Springer, Berlin, 5 edn.
Park, J. Y. (1992): “Canonical Cointegrating Regressions,” Econometrica, 60, 119—143.
Phillips, P. C. B. (1987): “Towards a Unified Asymptotic Theory for Autoregression,”
Biometrika, 74, 535—547.
42

Phillips, P. C. B., and B. E. Hansen (1990): “Statistical Inference in Instrumental
Variables Regression with I(1) Processes,” Review of Economic Studies, 57, 99—125.
Saikkonen, P. (1991): “Estimation and Testing of Cointegrating Regressions,” Econometric Theory, 7, 1—27.
Saikkonen, P., and R. Luukonen (1993): “Point Optimal Tests for Testing the Order of
Diﬀerencing in ARIMA Models,” Econometric Theory, 9, 343—362.
Sargan, J. D. (1964): “Wages and Prices in the UK: A Study in Econometric Methodology,” in Econometric Analysis for National Planning, ed. by P. E. Hart, G. Mills, and
J. K. Whitaker, pp. 25—26, London. Butterworths.
Sriananthakumar, S., and M. L. King (2006): “A New Approximate Point Optimal
Test of a Composite Null Hypothesis,” Journal of Econometrics, 130, 101—122.
Staiger, D., J. H. Stock, and M. W. Watson (2001): “Prices, Wages, and the U.S.
NAIRU in the 1990s,” in The Roaring Nineties, ed. by A. B. Krueger, and R. Solow, pp.
3—60, New York. The Russell Sage Foundation.
Stock, J. H., and M. W. Watson (1993): “A Simple Estimator of Cointegrating Vectors
in Higher Order Integrated Systems,” Econometrica, 61, 783—820.
(1996): “Confidence Sets in Regressions with Highly Serially Correlated Regressors,”
Working Paper, Harvard University.
Sun, Y., P. C. B. Phillips, and S. Jin (2008): “Optimal Bandwidth Selection in
Heteroskedasticity-Autocorrelation Robust Testing,” Econometrica, 76, 175—794.
Wright, J. H. (2000): “Confidence Sets for Cointegrating Coeﬃcients Based on Stationarity Tests,” Journal of Business and Economic Statistics, 18, 211—222.

43

Table 1: Power and Size Bounds for 5% Tests (q = 12)
A. r = k = 1
H0

|B| = 7
|R| = 0.5
RB<0 RB>0

|R| = 0.9
RB<0 RB>0

0.36

0.36

0.36

0.36

0.36

0.64

0.65

0.65

0.66

0.66

unr

0.41
0.06

0.52
0.14

0.40
0.05

0.89
0.65

0.44
0.07

0.70
0.07

0.80
0.17

0.68
0.05

0.98
0.47

0.72
0.06

unr
stat

0.50
0.13
0.08

0.65
0.28
0.09

0.59
0.21
0.15

0.95
0.79
0.07

0.67
0.32
0.20

0.81
0.15
0.11

0.92
0.42
0.13

0.81
0.17
0.13

1.00
0.49
0.03

0.87
0.20
0.14

unr
stat
LTU

0.50
0.12
0.08
0.04

0.65
0.28
0.09
0.04

0.65
0.26
0.21
0.06

0.95
0.76
0.07
0.04

0.95
0.80
0.71
0.50

0.82
0.19
0.12
0.04

0.91
0.37
0.12
0.03

0.91
0.34
0.31
0.16

1.00
0.48
0.03
0.00

1.00
0.48
0.42
0.25

HB

|R| = 0

unr
stat
LTU

I(1)

|R| = 0

|B| = 14
|R| = 0.5
|R| = 0.9
RB<0 RB>0 RB<0 RB>0

B. r = 1 and k ≥ 2
H0

0.36

0.37

0.36

0.36

0.36

0.36

0.36

0.63

||B|| = 14
||R|| = 0.5
||R|| = 0.9
ω=–
ω=0 ω=1
ω=–
ω=0 ω=1
1
1
0.64
0.64 0.65
0.65
0.65 0.66

unr

0.36
0.04

0.37
0.04

0.37
0.04

0.36
0.04

0.36
0.04

0.42
0.06

0.36
0.04

0.63
0.04

0.64
0.04

0.64
0.04

0.65
0.04

0.65
0.04

0.74
0.08

0.66
0.04

unr
diag

0.40
0.05
0.05

0.48
0.10
0.10

0.42
0.07
0.06

0.39
0.05
0.05

0.83
0.55
0.55

0.59
0.19
0.15

0.44
0.07
0.07

0.68
0.06
0.06

0.77
0.12
0.12

0.71
0.08
0.07

0.67
0.05
0.05

0.97
0.45
0.45

0.84
0.20
0.14

0.71
0.06
0.06

unr
diag
stat

0.46
0.09
0.09
0.07

0.57
0.18
0.18
0.07

0.49
0.10
0.09
0.06

0.53
0.15
0.15
0.12

0.92
0.71
0.71
0.08

0.70
0.34
0.31
0.10

0.64
0.27
0.27
0.15

0.74
0.09
0.09
0.06

0.84
0.22
0.22
0.08

0.77
0.12
0.13
0.06

0.76
0.12
0.12
0.10

1.00
0.50
0.50
0.05

0.85
0.20
0.16
0.05

0.85
0.18
0.18
0.12

unr
diag
stat
LTU

0.46
0.09
0.09
0.07
0.04

0.57
0.18
0.18
0.07
0.04

0.48
0.11
0.11
0.07
0.04

0.57
0.17
0.17
0.17
0.05

0.91
0.71
0.71
0.08
0.04

0.86
0.70
0.53
0.23
0.22

0.91
0.71
0.71
0.67
0.46

0.75
0.10
0.10
0.08
0.04

0.83
0.23
0.23
0.08
0.03

0.78
0.15
0.13
0.08
0.07

0.83
0.21
0.21
0.18
0.10

0.99
0.50
0.50
0.05
0.01

0.98
0.46
0.37
0.20
0.22

0.99
0.49
0.49
0.45
0.28

HB

unr
diag
stat

LTU

I(1)

||R||=0

||B|| = 7
||R|| = 0.5
||R|| = 0.9
ω=-1 ω=0 ω=1
ω=–1 ω=0 ω=1

||R||=0

Table 1 (Continuted)
C. r = 2 and k = 1
H0

0.45

||B|| = 10
||R|| = 0.5
||R|| = 0.9
ω=-1
ω=0
ω=1
ω=–1
ω=0
ω=1
0.47
0.50
0.48
0.50
0.69
0.51

unr

0.45
0.04

0.47
0.04

0.55
0.06

0.48
0.04

0.50
0.04

0.74
0.06

unr
G

0.50
0.06
0.06

0.61
0.13
0.13

0.61
0.11
0.07

0.51
0.05
0.05

0.94
0.61
0.61

unr
G
stat

0.55
0.10
0.10
0.06

0.72
0.25
0.25
0.09

0.63
0.11
0.08
0.05

0.66
0.17
0.17
0.15

unr
G
stat
LTU

0.55
0.10
0.10
0.06
0.04

0.70
0.22
0.22
0.07
0.04

0.63
0.11
0.08
0.05
0.04

0.70
0.24
0.24
0.19
0.06

HB

unr
G

stat

LTU

I(1)

||R||=0

0.69

||B|| = 20
||R|| = 0.5
ω=–1
ω=0
ω=1
0.73
0.75
0.73

0.51
0.04

0.69
0.04

0.73
0.04

0.76
0.04

0.73
0.04

0.79
0.04

0.97
0.05

0.79
0.04

0.74
0.06
0.04

0.57
0.07
0.07

0.72
0.05
0.05

0.81
0.10
0.10

0.78
0.05
0.05

0.74
0.05
0.05

0.98
0.29
0.29

0.97
0.04
0.02

0.82
0.05
0.05

0.99
0.70
0.70
0.07

0.74
0.06
0.04
0.04

0.76
0.29
0.29
0.19

0.79
0.12
0.12
0.07

0.92
0.26
0.26
0.13

0.87
0.13
0.13
0.10

0.81
0.09
0.09
0.08

1.00
0.32
0.32
0.03

0.97
0.05
0.02
0.03

0.89
0.12
0.12
0.09

0.97
0.69
0.69
0.05
0.02

0.74
0.06
0.04
0.04
0.04

0.97
0.69
0.69
0.65
0.45

0.84
0.16
0.16
0.12
0.10

0.91
0.26
0.26
0.14
0.03

0.88
0.15
0.14
0.11
0.04

0.91
0.29
0.29
0.26
0.18

1.00
0.32
0.32
0.03
0.00

0.97
0.05
0.02
0.03
0.02

1.00
0.32
0.32
0.31
0.21

||R||=0

||R|| = 0.9
ω=0
ω=1
0.79
0.94
0.79

ω=–1

Notes: Non-italicized numerical entries are upper bounds on power for 5% tests for the null
hypothesis restricting the trend process as shown in the first column. The italicized entries are lower
bounds on size for auxiliary trend processes given in columns for 5% level tests with power greater
than the power bound less 3 percentage points. Abbreviations for the trend models in columns 1 and 2
are: (i) “unr” is the unrestricted model given in (3), (ii) “G” is the G-model in (6), (iii) “diag” is the
diagonal G-model in (8), (iv) “stat” is the stationary G model given in (9), (v) “LTU” is the local-tounity model given in (11), and (vi) “I(1)” is I(1) model given in (10). In panels B and C, ω = tr(R′B) /
(||R||·||B||). Results are based on 20,000 Monte Carlo replications.

Table 2: Comparison of KLIC Minimized and Approximate Least Upper Power Bounds
(r = k = 1, q = 12)

0.51
0.50

|B| = 7
|R| = 0.5
RB<0 RB>0
0.66
0.59
0.66
0.58

|R| = 0.9
RB<0 RB>0
0.95
0.66
0.93
0.65

0.51
0.50

0.65
0.65

0.95
0.94

H0

Bound

|R| = 0

LTU

KLIC
LUB

I(1)

KLIC
LUB

0.65
0.65

0.95
0.94

0.81
0.78

|B| = 14
|R| = 0.5
|R| = 0.9
RB<0 RB>0 RB<0 RB>0
0.93
0.80
1.00
0.86
0.88
0.78
1.00
0.82

0.82
0.81

0.92
0.90

|R| = 0

0.91
0.90

1.00
1.00

1.00
1.00

Notes: The entries labeled “KLIC” are computed using the KLIC minimization discussed in Section
5.3.2. The entries labeled “LUB” are computed using the approximate least upper algorithm discussed
in Section 5.3.1, and are by construction no more than 2.5 percentage points above the actual least
upper bound. Results are based on 20,000 Monte Carlo replications.

Table 3: 1%, 5%, and 10% Critical Values for the LFST Statistic
q
6
7
8
9
10
11
12
13
14
15
16
17
18

5.25
4.33
3.68
3.21
2.86
2.62
2.46
2.29
2.16
2.07
1.97
1.89
1.82

1
3.62
3.08
2.73
2.46
2.25
2.10
1.98
1.88
1.80
1.74
1.67
1.62
1.58

3.08
2.68
2.39
2.18
2.02
1.90
1.81
1.73
1.67
1.61
1.56
1.52
1.49

6.76
5.52
4.65
4.02
3.56
3.16
2.89
2.68
2.50
2.36
2.24
2.15
2.07

2
5.16
4.20
3.54
3.09
2.79
2.54
2.35
2.21
2.09
1.99
1.91
1.84
1.78

4.39
3.63
3.08
2.73
2.48
2.29
2.13
2.01
1.92
1.84
1.77
1.71
1.66

7.25
6.05
5.17
4.46
3.94
3.53
3.18
2.92
2.74
2.56
2.44
2.32
2.21

r
3
6.09
4.95
4.16
3.58
3.17
2.87
2.64
2.44
2.31
2.18
2.08
1.99
1.92

5.43
4.37
3.68
3.19
2.84
2.59
2.39
2.23
2.11
2.01
1.92
1.85
1.79

7.14
6.15
5.29
4.63
4.10
3.71
3.38
3.13
2.91
2.69
2.55
2.43
2.32

4
6.46
5.35
4.55
3.93
3.47
3.12
2.84
2.63
2.47
2.32
2.21
2.11
2.02

6.02
4.92
4.12
3.56
3.15
2.84
2.60
2.42
2.27
2.15
2.05
1.96
1.89

6.54
5.89
5.26
4.66
4.18
3.78
3.48
3.20
2.97
2.80
2.64
2.50
2.39

5
6.33
5.48
4.73
4.12
3.66
3.30
3.02
2.77
2.59
2.44
2.30
2.20
2.10

6.16
5.20
4.42
3.83
3.38
3.03
2.78
2.57
2.40
2.27
2.15
2.05
1.98

Notes: The table shows asymptotic critical for the LFST(b) statistic computed using b = 10 / r ,
where LFST(b) = det(Y´Y)/det(Y´(I + b2D)−1Y), with D = diag(d1,…,dk) and di = (iπ)–2. Results are
based on 50,000 Monte Carlo replications.

Table 4: Power of Y-only 5% Tests (q = 12)
A. r = 1
|B| = 7
0.36
0.36

LFST(10)
Y-Only Power Envelope

|B| = 14
0.63
0.64

B. r = 2 and k = 1
||B|| = 10

||B|| = 20
Power of LFST(10/ 2 )

0.39
||R||=0

0.58
Y-Only Power Envelope
||R|| = 0.9
||R|| = 0.5
||R||=0
ω=–1
ω=0
ω=1
ω=–1
ω=0
ω=1
0.41
0.55
0.41
0.68
0.64
0.68
0.64

||R|| = 0.5
ω=0
ω=1
0.41
0.44
0.41

ω=-1
0.41

||R|| = 0.9
ω=0
ω=1
0.64
0.86
0.64

ω=–1

Notes: The power envelope is computed using the test statistic LFST(|B|) in panel A and by ξ(B) in
panel B. In panel B , ω = tr(R′B) / (||R||·||B||). Results are based on 20,000 Monte Carlo replications.

Table 5: Cointegrating Coefficients and Tests
wt = β0 + βppt + βyyt + βnnt

P
Y
N
DOLS Wald Statistic (p-value)
LFST p-value

Non-Farm Business
a. DOLS Coefficients (SEs)
1.046 (0.020)
1.028 (0.037)
−1.227 (0.106)
b. Joint Test βp = 1, βy = 1, βn = −1
35.82 (<0.001)
0.004

Non-Financial Corporations
1.003 (0.018)
0.852 (0.023)
−0.723 (0.057)
45.41 (<0.001)
0.282

Notes: The top panel shows DOLS estimated coefficients and standard errors. The bottom panel shows
the DOLS Wald statistic (which is distributed χ 32 under the null) and associated p-value, and the LFST
p-value. The DOLS estimator uses 6 leads and lags of Δ2pt, Δyt, and Δnt, and the DOLS standard
errors are computed using Newey-West HAC estimators with 6 lags.

Figure 1: wt – pt – yt − nt
Non-Farm Business (thin black) and Non-Financial Corporations (thick blue)

Figure 2: 95% Confidence Sets for Cointegrating Coefficients, Non-Financial Corporations
LFST (thin black) and DOLS (thick blue)

