{
    "id": 18988,
    "citation_title": "Fifty Ways to Leave a Child Behind: Idiosyncrasies and Discrepancies in States' Implementation of NCLB",
    "citation_author": [
        "Elizabeth Davidson",
        "Randall Reback",
        "Jonah E. Rockoff",
        "Heather L. Schwartz"
    ],
    "citation_publication_date": "2013-04-25",
    "issue_date": "2013-04-25",
    "revision_date": "None",
    "topics": [
        "\n",
        "Public Economics",
        "\n",
        "Subnational Fiscal Issues",
        "\n",
        "Health, Education, and Welfare",
        "\n",
        "Education",
        "\n"
    ],
    "program": [
        "\n",
        "Economics of Education",
        "\n",
        "Public Economics",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nThe No Child Left Behind (NLCB) Act required states to adopt accountability systems measuring student proficiency on state administered exams.  Based on student test score performance in 2002, states developed initial proficiency rate targets and future annual benchmarks designed to lead students to 100% proficiency on state exams by 2014.  Any year a school fails to meet these targets, either across all students or by various subgroups of students, the school does not make Adequate Yearly Progress.  While the federal government's legislation provided a framework for NCLB implementation, it also gave states flexibility in their interpretation of many NCLB components, and school failure rates ranged from less than 1% to more than 80% across states.  In this paper, we explore how states' NCLB implementation decisions affected their schools' failure rates.  Wide cross-state variation in failure rates resulted from how states' decisions (e.g., confidence intervals applied to proficiency rates, numerical thresholds for a student subgroup to be held accountable) interacted with each other and with school characteristics like enrollment size, grade span, and ethnic diversity.  Subtle differences in policy implementation led to dramatic differences in measured outcomes.\n\n",
    "acknowledgement": "\nThis research project was made possible by funding from the Institute for Education Sciences and the Spencer Foundation, as well as seed grants from the Columbia University Institute for Social and Economic Research and Policy and Barnard College, and support from the Paul Milstein Center for Real Estate at Columbia Business School.  The authors are solely responsible for any opinions or errors in the paper.  We thank participants in the APPAM/INVALSI/UMD 2012 conference in Rome, Italy, \"Improving Education through Accountability and Evaluation,\" for their helpful comments.  We also thank participants in the 2013 conference of the Association for Education Finance and Policy. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}