{
    "id": 24352,
    "citation_title": "Self-Regulating Artificial General Intelligence",
    "citation_author": [
        "Joshua S. Gans"
    ],
    "citation_publication_date": "2018-02-26",
    "issue_date": "2018-02-22",
    "revision_date": "None",
    "topics": [
        "\n",
        "Microeconomics",
        "\n",
        "Game Theory",
        "\n"
    ],
    "program": [
        "\n",
        "Productivity, Innovation, and Entrepreneurship",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nThis paper examines the paperclip apocalypse concern for artificial general intelligence. This arises when a superintelligent AI with a simple goal (ie., producing paperclips) accumulates power so that all resources are devoted towards that goal and are unavailable for any other use. Conditions are provided under which a paper apocalypse can arise but the model also shows that, under certain architectures for recursive self-improvement of AIs, that a paperclip AI may refrain from allowing power capabilities to be developed. The reason is that such developments pose the same control problem for the AI as they do for humans (over AIs) and hence, threaten to deprive it of resources for its primary goal.\n\n",
    "acknowledgement": "\nThanks to Ariel Rubinstein, Gillian Hadfield, Dylan Hadfield-Menell, Barney Pell, Ken Nickerson and Danny Goroff for helpful comments. Responsibility for all views expressed remain my own. The views expressed herein are those of the author and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}