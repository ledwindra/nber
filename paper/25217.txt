NBER WORKING PAPER SERIES

ON THE INFORMATIVENESS OF DESCRIPTIVE STATISTICS FOR STRUCTURAL
ESTIMATES
Isaiah Andrews
Matthew Gentzkow
Jesse M. Shapiro
Working Paper 25217
http://www.nber.org/papers/w25217

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
November 2018
We acknowledge funding from the National Science Foundation (DGE-1654234), the Brown
University Population Studies and Training Center, the Stanford Institute for Economic Policy
Research (SIEPR), the Alfred P. Sloan Foundation, and the Silverman (1968) Family Career
Development Chair at MIT. We thank Tim Armstrong, Matias Cattaneo, Gary Chamberlain,
Liran Einav, Nathan Hendren, Yuichi Kitamura, Adam McCloskey, Costas Meghir, Ariel Pakes,
Ashesh Rambachan, Eric Renault, Jon Roth, Susanne Schennach, and participants at the Radcliffe
Institute Conference on Statistics When the Model is Wrong, the Fisher-Schultz Lecture, the HBS
Conference on Economic Models of Competition and Collusion, the University of Chicago
Becker Applied Economics Workshop, the UCL Advances in Econometrics Conference, the
Harvard-MIT IO Workshop, the BFI Conference on Robustness in Economics and Econometrics
(especially discussant Jinyong Hahn), the Cornell Econometrics-IO Workshop, and the Johns
Hopkins Applied Micro Workshop, for their comments and suggestions. We thank Nathan
Hendren for assistance in working with his code and data. The views expressed herein are those
of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.
At least one co-author has disclosed a financial relationship of potential relevance for this research.
Further information is available online at http://www.nber.org/papers/w25217.ack
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
Â© 2018 by Isaiah Andrews, Matthew Gentzkow, and Jesse M. Shapiro. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including Â© notice, is given to the source.

On the Informativeness of Descriptive Statistics for Structural Estimates
Isaiah Andrews, Matthew Gentzkow, and Jesse M. Shapiro
NBER Working Paper No. 25217
November 2018, Revised May 2020
JEL No. C18,D12,I13,I25
ABSTRACT
We propose a way to formalize the relationship between descriptive analysis and structural
estimation. A researcher reports an estimate Ä‰ of a structural quantity of interest c that is exactly
or asymptotically unbiased under some base model. The researcher also reports descriptive
statistics Î³Ì‚ that estimate features Î³ of the distribution of the data that are related to c under the
base model. A reader entertains a less restrictive model that is local to the base model, under
which the estimate Ä‰ may be biased. We study the reduction in worst-case bias from a restriction
that requires the reader's model to respect the relationship between c and Î³ specified by the base
model. Our main result shows that the proportional reduction in worst-case bias depends only on
a quantity we call the informativeness of Î³Ì‚ for Ä‰. Informativeness can be easily estimated even
for complex models. We recommend that researchers report estimated informativeness alongside
their descriptive analyses, and we illustrate with applications to three recent papers.

Isaiah Andrews
Department of Economics
Harvard University
Littauer M18
Cambridge, MA 02138
and NBER
iandrews@fas.harvard.edu
Matthew Gentzkow
Department of Economics
Stanford University
579 Jane Stanford Way
Stanford, CA 94305
and NBER
gentzkow@stanford.edu

Jesse M. Shapiro
Economics Department
Box B
Brown University
Providence, RI 02912
and NBER
jesse_shapiro_1@brown.edu

1

Introduction

Empirical researchers often present descriptive statistics alongside structural estimates that answer
policy or counterfactual questions of interest. One leading case is where the structural model is
estimated on data from a randomized experiment, and the descriptive statistics are treatmentcontrol differences (e.g., Attanasio et al. 2012a; Duflo et al. 2012; Alatas et al. 2016). Another
is where the structural model is estimated on observational data, and the descriptive statistics are
regression coefficients or correlations that capture important relationships (e.g., Gentzkow 2007a;
Einav et al. 2013; Gentzkow et al. 2014; Morten 2019). Researchers often provide a heuristic
argument that links the descriptive statistics to key structural estimates, sometimes framing this
as an informal analysis of identification.1
Such descriptive analysis has the potential to make structural estimates more interpretable.
Structural models are often criticized for lacking transparency, with large numbers of assumptions
and a high level of complexity making it difficult for readers to evaluate how the results might
change under plausible forms of misspecification (Heckman 2010; Angrist and Pischke 2010). If
a particular result were mainly driven by some intuitive descriptive features of the data, a reader
could focus on evaluating the assumptions that link those features to the result.
In this paper, we propose a way to make this logic precise. A researcher is interested in a scalar
quantity of interest c (say, the effect of a counterfactual policy). The researcher specifies a base
model that relates the value of c to the distribution F of some data (say, the joint distribution of
the data in a randomized experiment). The researcher reports an estimate cÌ‚ of c that is unbiased
(either exactly or asymptotically) under the base model. A reader of the research may not accept
all of the assumptions of the base model, and may therefore be concerned that cÌ‚ is biased.
The researcher also reports a vector Î³Ì‚ of descriptive statistics (say, sample mean outcomes
in different arms of the experiment). These statistics uncontroversially estimate some features
Î³ = Î³ (F ) of the distribution F (say, population mean outcomes in different arms). Because the
base model specifies the relationship between c and F , it also implicitly specifies the relationship
between c and Î³, which may or may not be correct.
Suppose the researcher is able to convince the reader of the relationship between c and Î³ specified
by the base model (say, by arguing that the counterfactual policy is similar to one of the arms of
the experiment). Should this lessen the readerâ€™s concern about bias in cÌ‚, even if the reader does
not accept the base model in its entirety?
We answer this question focusing on the worst-case bias when the alternative model contem1

See, for example, Fetter and Lockwood (2018, pp. 2200-2201), Spenkuch et al. (2018, pp. 1992-1993), and the
examples discussed in Andrews et al. (2017, 2020).

2

plated by the reader is local to the base model in an appropriate sense. To outline our approach,
it will be useful to define the base model as a correspondence F 0 (Â·), where F 0 (c) is the set of
distributions F consistent with a given value of c under the model. The identified set for c given
some F under the base model is found by taking the preimage of F under F 0 (Â·). We assume that
c is point identified under the base model, so that for any F consistent with the base model, the
identified set given F is a singleton.
The reader contemplates a model that is less restrictive than the base model. We describe the
readerâ€™s model by a correspondence F N (Â·), where F N (c) âŠ‡ F 0 (c) is the set of distributions F
consistent with a given value of c under the readerâ€™s model. Because F N (c) âŠ‡ F 0 (c) for all c, the
identified set for c given some F is larger under the readerâ€™s model than under the base model, and
may not be a singleton. Moreover, cÌ‚ may be biased under the readerâ€™s model. Let bN denote the
largest possible absolute bias in cÌ‚ that can arise under F N (Â·), where this bound may be infinite.
To formalize the idea that the readerâ€™s model is local to the base model, we suppose that each
FÌƒ âˆˆ F N (c) lies in a neighborhood N (F ) of an F consistent with the base model, so that
(1)

n
o
F N (c) = âˆªF âˆˆF 0 (c) FÌƒ âˆˆ N (F ) .

We take the neighborhood N (F ) to contain distributions FÌƒ within a given statistical distance of
F.
To formalize the possibility that the researcher convinces the reader of the relationship between
c and Î³ prescribed by the base model, we consider restricting attention to the elements FÌƒ âˆˆ N (F )
 
such that Î³ FÌƒ = Î³ (F ). This results in the restricted correspondence F RN (Â·)
(2)

n
 
o
F RN (c) = âˆªF âˆˆF 0 (c) FÌƒ âˆˆ N (F ) : Î³ FÌƒ = Î³ (F ) .

If F 0 (Â·) implies that only certain values of Î³ are consistent with a given value of c, then F RN (Â·)
preserves that implication whereas F N (Â·) may not. For this reason F N (c) âŠ‡ F RN (c) âŠ‡ F 0 (c) for
all c, i.e., the correspondence F RN (Â·) is less restrictive than the base model, but more restrictive
than the readerâ€™s model. Let bRN denote the largest possible absolute bias in cÌ‚ that can arise under
F RN (Â·). Because F RN (Â·) is more restrictive than F N (Â·), we know that bRN â‰¤ bN .
We focus on characterizing the ratio bRN /bN , which lies between zero and one. Section 2 shows
how to derive the correspondences F N (Â·), F RN (Â·) , and F 0 (Â·), and the worst-case biases bN and
bRN , from explicitly parameterized economic models. Section 3 provides an exact characterization
of bRN /bN in a linear model with normal errors. Section 4 provides an approximate characterization
of bRN /bN in more general nonlinear models, obtained via a local asymptotic analysis. Sections 3
3

and 4 show that under given conditions the ratio bRN /bN (or its asymptotic analogue) is equal to
âˆš
1 âˆ’ âˆ†, where âˆ† is a scalar which we call the informativeness of the descriptive statistics Î³Ì‚ for the
structural estimate cÌ‚. Informativeness is the R2 from a regression of the structural estimate on the
descriptive statistics when both are drawn from their joint (asymptotic) distribution. Intuitively,
when informativeness is high, Î³Ì‚ captures most of the information in the data that determines cÌ‚.
We propose informativeness as a way to formalize the colloquial notion of the extent to which Î³Ì‚
â€œdrivesâ€ cÌ‚.
Informativeness can be estimated at low cost even for computationally challenging models.
Section 5 shows that a consistent estimator of âˆ† can be obtained from manipulation of the estimated
influence functions of cÌ‚ and Î³Ì‚. In the large range of settings in which estimated influence functions
are available from the calculations used to obtain cÌ‚ and Î³Ì‚, the additional computation required
to estimate âˆ† is trivial. We recommend that researchers report an estimate of informativeness
whenever they present descriptive evidence as support for structural estimates.
Section 6 implements our proposal for three recent papers in economics, each of which reports or
discusses descriptive statistics alongside structural estimates. In the first application, to Attanasio
et al. (2012a), the quantity c of interest is the effect of a counterfactual redesign of the PROGRESA
cash transfer program, and the descriptive statistics Î³Ì‚ are sample treatment-control differences for
different groups of children. In the second application, to Gentzkow (2007a), the quantity c of
interest is the effect of removing the online edition of the Washington Post on readership of the print
edition, and the descriptive statistics Î³Ì‚ are linear regression coefficients. In the third application,
to Hendren (2013a), the quantity c of interest is a parameter governing the existence of insurance
markets, and the descriptive statistics Î³Ì‚ summarize the joint distribution of self-reported beliefs
about the likelihood of loss events and the realizations of these events. In each case, we report an
estimate of âˆ† for various definitions of Î³Ì‚, and we discuss the implications for the interpretation
of cÌ‚. These applications illustrate how estimates of âˆ† can be presented and discussed in applied
research.
Important limitations of our analysis include the use of asymptotic approximations to describe
the behavior of estimators, and the use of a purely statistical notion of distance to define sets of
alternative models. Ideally one would like to use exact finite-sample properties to characterize the
bias of estimators, and economic knowledge to define sets of alternative models. We are not aware
of convenient procedures that achieve this ideal in the generality that we consider. We therefore
propose the use of informativeness as a practical option to improve the precision of discussions of
the connection between descriptive statistics and structural estimates in applied research.
Our results are related to Andrews et al. (2017). In that paper, we propose a measure Î› of the

4

sensitivity of a parameter estimate cÌ‚ to a vector of statistics Î³Ì‚, focusing on the case where Î³Ì‚ are
estimation moments that fully determine the estimator cÌ‚ (and so âˆ† = 1).2 In Online Appendix A,
we generalize our main result to accommodate the setting of Andrews et al. (2017) and so provide
a unified treatment of sensitivity and informativeness.
In a related paper, Mukhin (2018) derives informativeness and sensitivity from a statisticalgeometric perspective, and notes strong connections to semiparameteric efficiency theory. Mukhin
also shows how to derive sensitivity and informativeness measures based on alternative metrics for
the distance between distributions, and discusses the use of these measures for local counterfactual
analysis.
Our work is also closely related to the large literature on local misspecification (e.g., Newey 1985;
Conley et al. 2012; Andrews et al. 2017). Much of this literature focuses on testing and confidence
set construction (e.g. Berkowitz et al. 2008; Guggenberger 2012; Armstrong and KolesaÌr, 2019) or
robust estimation (e.g., Rieder 1994; Kitamura et al. 2013; Bonhomme and Weidner 2018). Rieder
(1994) studies the choice of target parameters and proposes optimal robust testing and estimation
procedures under forms of local misspecification including the one that we consider here. Bonhomme
and Weidner (2018) derive minimax robust estimators and accompanying confidence intervals for
economic parameters of interest under a form of local misspecification closely related to the one we
study. Armstrong and KolesaÌr (2019) consider a class of ways in which the model may be locally
misspecified that nests the one we consider, derive minimax optimal confidence sets, and show that
there is limited scope to improve on their procedures by â€œestimatingâ€ the degree of misspecification,
motivating a sensitivity analysis. In contrast to this literature, we focus on characterizing the
relationship between a set of descriptive statistics and a given structural estimator, with the goal of
allowing readers of applied research to sharpen their opinions about the reliability of the researcherâ€™s
conclusions, thus improving transparency in the sense of Andrews et al. (2020).
Our use of statistical distance to characterize the degree of misspecification relates to a number
of recent papers. Our results cover the Cressie-Read (1984) family, which nests widely studied
measures including the Kullback-Leibler divergence, Hellinger divergence, and many others, up to
a monotone transformation. Kullback-Leibler divergence has been used to measure the degree of
misspecification by, for example, Hansen and Sargent (2001), Hansen and Sargent (2005), Hansen et
al. (2006), Hansen and Sargent (2016), and Bonhomme and Weidner (2018). Hellinger divergence
has been used by, for example, Kitamura et al. (2013).
Finally, our work relates to discussions about the appropriate role of descriptive statistics in
structural econometric analysis (e.g., Pakes 2014).3 It is common in applied research to describe
2
3

The present paper draws on the analysis of â€œsensitivity to descriptive statisticsâ€ in Gentzkow and Shapiro (2015).
See also Dridi et al. (2007) and Nakamura and Steinsson (2018) for discussion of the appropriate choice of

5

the data features that â€œprimarily identifyâ€ structural parameters or â€œdriveâ€ estimates of those parameters.4 As Keane (2010) and others have noted, such statements are not directly related to
the formal notion of identification in econometrics (see also Andrews et al. 2020). Their intended
meaning is therefore up for grabs. If researchers are prepared to reinterpret these as statements
about informativeness, then our approach provides a way to sharpen and quantify these statements
at low cost to researchers.

2

Setup and Key Definitions

The introduction describes our approach in terms of correspondences between the quantity of
interest c and the distribution F of the data. In this section we first show how to derive these correspondences from explicitly parameterized economic models, and then use these correspondences
to define the worst-case biases that we characterize in our analysis. Section 4 defines analogous
objects in a local asymptotic framework.
Suppose that, under the base model considered by the researcher, both the distribution of the
data F and the quantity of interest c are determined by a structural parameter Î· âˆˆ H. Formally,
under the base model, we have that F = F (Î·) and c = c (Î·) so the correspondence F 0 (Â·) is given
by
F 0 (c) = {F (Î·) : Î· âˆˆ H, c (Î·) = c} .
Because the structural parameter Î· determines the distribution F , it also determines Î³ = Î³ (Î·) =
Î³ (F (Î·)).
Suppose further that, under the readerâ€™s model, the distribution of the data F is determined
by Î· and by a misspecification parameter Î¶ âˆˆ Z (say, indexing economic forces omitted from the
researcherâ€™s model) that is normalized to zero under the base model. Formally, under the readerâ€™s
model we have that F = F (Î·, Î¶), with F (Î·, 0) = F (Î·) for all Î· âˆˆ H, and correspondingly that
Î³ = Î³ (Î·, Î¶) = Î³ (F (Î·, Î¶)), with Î³ (Î·, 0) = Î³ (Î·) for all Î· âˆˆ H. We focus on settings where forms of
misspecification indexed by Î¶ are rich, in the sense that the range of F (Î·, Î¶) under Î¶ âˆˆ Z does not
depend on Î·. For simplicity we continue to write the quantity of interest as a function of Î· alone,
c = c (Î·). Forms of misspecification that affect the mapping from Î· to c but preserve its range are
equivalent to those we study.5
moments to match when fitting macroeconomic models.
4
Andrews et al. (2017, footnotes 2 and 3) provide examples.
5
Specifically, consider a model where the distribution of the data is F = F (Î·, Î¶) as above, while the quantity of
interest is c = cÌƒ (Î·, Î¶), and the range of cÌƒ (Î·, Î¶) under Î· âˆˆ H for any Î¶ âˆˆ Z is the same as that of c (Î·) under Î· âˆˆ H. In
this case, the sets {(c (Î·) , F (Î·, Î¶)) : Î· âˆˆ H, Î¶ âˆˆ Z} and {(cÌƒ (Î·, Î¶) , F (Î·, Î¶)) : Î· âˆˆ H, Î¶ âˆˆ Z} are both Cartesian products, equal to {c (Î·) : Î· âˆˆ H} Ã— {F (Î·, Î¶) : Î· âˆˆ H, Î¶ âˆˆ Z}. Consequently, the correspondences F (Â·) that we consider

6

We formalize the idea that the readerâ€™s model is local to the base model as follows. Let r (Î·, Î¶) â‰¥
0 denote some Cressie-Read (1984) divergence between the distribution F (Î·) and the distribution
F (Î·, Î¶), so that r (Î·, 0) = 0 for all Î· âˆˆ H. For any distribution F = F (Î·) consistent with the
base model, we define the neighborhood N (F ) to consist of all distributions F (Î·, Î¶) such that the
divergence r (Î·, Î¶) is less than some scalar bound Âµ â‰¥ 0:
N (F ) = {F (Î·, Î¶) : Î· âˆˆ H, F (Î·) = F, Î¶ âˆˆ Z, r (Î·, Î¶) â‰¤ Âµ} .
We then define the readerâ€™s model F N (Â·) as in (1).6 The neighborhood N (F ) is increasing in Âµ.
Hence, larger values of Âµ imply imply a greater relaxation of assumptions as we move from the base
model F 0 (Â·) to the readerâ€™s model F N (Â·) . We suppress the dependence of N (F ) and F N (c) on Âµ
for brevity.
The base model specifies a relationship between c and Î³ in the sense that if the quantity of interest takes value c, then the feature Î³ must take a value Î³ (Î·) for some Î· âˆˆ H such that c = c (Î·). The
readerâ€™s model F N (Â·) need not respect the base modelâ€™s specification of the relationship between
c and Î³. By contrast, the model F RN (Â·), defined in (2), respects the base modelâ€™s specification of
the relationship between c and Î³ in the sense that, for any F âˆˆ F RN (c), there is some Î· âˆˆ H such
that c = c (Î·), Î³ (F ) = Î³ (Î·), and F âˆˆ N (F (Î·)) .7 Hence, a given (c, Î³) pair is compatible with
F RN (Â·) if and only if it is compatible with F 0 (Â·) .
The researcher chooses an estimator cÌ‚ that is unbiased under the base model in the sense
that EF [cÌ‚ âˆ’ c] = 0 for any F âˆˆ F 0 (c), where EF [Â·] denotes the expectation when the data are
distributed according to F . The estimator cÌ‚ may be biased under the readerâ€™s model F N (Â·), and
indeed if we take Âµ to infinity the parameter c is completely unidentified under F N (Â·). The largest
absolute bias in cÌ‚ that is possible under F N (Â·) is
bN = sup
c

sup

|EF [cÌ‚ âˆ’ c]| .

F âˆˆF N (c)

Considering F RN (Â·) rather than F N (Â·) can reduce the worst-case bias in cÌ‚. The largest absolute
bias in cÌ‚ that is possible under F RN (Â·) is
bRN = sup
c

sup

|EF [cÌ‚ âˆ’ c]| .

F âˆˆF RN (c)

The proportional reduction in worst-case bias from limiting attention to F RN (Â·) is measured by
are the same whether constructed from the model with c = c (Î·) or that with c = cÌƒ (Î·, Î¶).
6
Specifically, F N (c) = {F (Î·, Î¶) : Î· âˆˆ H, c (Î·) = c, Î¶ âˆˆ Z, r (Î·, Î¶) â‰¤ Âµ} .
7
To see that this is the case, note that F RN (c) = {F (Î·, Î¶) : Î· âˆˆ H, c (Î·) = c, Î¶ âˆˆ Z, r (Î·, Î¶) â‰¤ Âµ, Î³ (F (Î·, Î¶)) = Î³ (Î·)} .

7

the ratio bRN /bN , which is the primary focus of our analysis.

3

Informativeness in a Linear Normal Setting

To build intuition for our approach, we next specialize to a linear normal setting and provide an
exact characterization of the ratio bRN /bN . We illustrate with a stylized example, and conclude
the section with some further discussion of our approach and its limitations.

3.1

Characterization of Worst-Case Bias

Now suppose that H = Rp , Z = Rk , and that under F (Î·, Î¶) the data Y âˆˆ Rk follow
Y âˆ¼ N (XÎ· + Î¶, â„¦)

(3)

for X and â„¦ known, nonrandom matrices with full column rank.
The quantity of interest is some linear function c (Î·) = L0 Î· of the parameters, with L âˆˆ RpÃ—1 a
known, non-random vector. The researcher chooses a linear estimator cÌ‚ = C 0 Y for C a vector. The
researcher ensures that cÌ‚ is unbiased for c under F 0 (Â·) by choosing C 0 = L0 M for some matrix M
with M X = Ip .
The researcher computes the vector Î³Ì‚ = Î“0 Y of descriptive statistics, with Î“ âˆˆ RkÃ—pÎ³ a known,
non-random matrix. The vector Î³Ì‚ is trivially unbiased for Î³ (Î·, Î¶) = Î“0 (XÎ· + Î¶).
Absent any restriction on Î¶ the quantity of interest c is entirely unidentified. Intuitively, without
any restriction on Î¶, the mean of the data Y is entirely unrestricted for any fixed Î·, making it
impossible to learn c = L0 Î·. The readerâ€™s model F N (Â·) limits the size of Î¶. In particular, given (3),
the assumption that r (Â·, Â·) is in the Cressie-Read family implies that r (Î·, Î¶) is a strictly increasing
âˆš
transformation of kÎ¶kâ„¦âˆ’1 , for kV kA = V 0 AV . Thus, for this section we define N (Â·) based on the
restriction kÎ¶kâ„¦âˆ’1 â‰¤ Âµ.8
Under the base model F 0 (Â·),
ï£«
ï£­

cÌ‚
Î³Ì‚

ï£¶

ï£«ï£«

ï£¸ âˆ¼ N ï£­ï£­

L0 Î·
Î“0 XÎ·

ï£¶

ï£¶

ï£«

ï£¸ , Î£ï£¸ for Î£ = ï£­

Ïƒc2

Î£cÎ³

Î£Î³c Î£Î³Î³

ï£¶

ï£«

ï£¸=ï£­

C 0 â„¦C

C 0 â„¦Î“

Î“0 â„¦C

Î“0 â„¦Î“

We assume that Ïƒc2 > 0 and that Î£Î³Î³ has full rank.

8

That is, we let

N (F ) = F (Î·, Î¶) : Î· âˆˆ H, F (Î·) = F, Î¶ âˆˆ Z, kÎ¶kâ„¦âˆ’1 â‰¤ Âµ .

8

ï£¶
ï£¸.

Definition. The informativeness of Î³Ì‚ for cÌ‚ is
âˆ†=

Î£cÎ³ Î£âˆ’1
Î³Î³ Î£Î³c
âˆˆ [0, 1] .
Ïƒc2

Informativeness is the R2 from the population regression of cÌ‚ on Î³Ì‚ under their joint distribution.
Informativeness determines the ratio of worst-case biases bRN /bN .
Proposition 1. The set of possible biases under F N (Â·) is


EF [cÌ‚ âˆ’ c] : F âˆˆ F N (c) = [âˆ’ÂµÏƒc , ÂµÏƒc ]

for any c, while the set of possible biases under F RN (Â·) is
i
h
âˆš
âˆš

EF [cÌ‚ âˆ’ c] : F âˆˆ F RN (c) = âˆ’ÂµÏƒc 1 âˆ’ âˆ†, ÂµÏƒc 1 âˆ’ âˆ†
âˆš
for any c. Hence, bN = ÂµÏƒc , bRN = ÂµÏƒc 1 âˆ’ âˆ†, and
âˆš
bRN
= 1 âˆ’ âˆ†.
bN
All proofs are collected at the end of the paper.
Importantly, the value of âˆ†, and hence the proportional reduction in worst-case bias from
restricting from F N (Â·) to F RN (Â·), does not depend on Âµ. In addition to characterizing the worstcase biases bRN and bN , Proposition 1 characterizes the set of possible biases under F N (Â·) and
F RN (Â·), showing in particular that any absolute bias smaller than the worst case is achievable.
Imposing additional restrictions on Î¶, beyond those captured by F N (Â·) or F RN (Â·), could further
reduce the worst-case bias.

3.2

Example

To fix ideas, suppose that a researcher observes i.i.d. data from a randomized evaluation of a
conditional cash transfer program. The program gives each household a payment of size s if their
children attend school regularly. Households are uniformly randomized among subsidy levels s âˆˆ
{0, 1, 2}. We can think of those receiving s = 0 as the control group.
The data consist of the average school attendance Ys of children assigned subsidy s âˆˆ {0, 1, 2}.
The quantity of interest c is the expected attendance at a counterfactual subsidy level sâˆ— > 2.

9

Under the base model the mean of Ys is given by
(4)

Î·1 + Î·2 s

for s âˆˆ {0, 1, 2, sâˆ— }. Average attendance Ys is independent and homoskedastic across arms of the
experiment, with standard deviation Ï‰.9
Under the base model F 0 (Â·), c can be estimated by linear extrapolation of average attendance
from two or more of the observed subsidy levels s âˆˆ {0, 1, 2} to subsidy level sâˆ— . We continue to
assume that the researcher chooses a linear estimator cÌ‚ that is unbiased for c under F 0 (Â·).10
Under the readerâ€™s model F N (Â·), the estimator cÌ‚ may be biased. Intuitively, if Î¶ 6= 0 then
the mean of Ys may be nonlinear in s, so that linear extrapolation to sâˆ— may produce a biased
estimate of c. The restriction to F RN (Â·) can lessen the scope for bias. The economic content of the
restriction depends on the choice of Î“, which in turn determines the descriptive statistic Î³Ì‚ = Î“0 Y
and the informativeness âˆ†.
As a concrete example, suppose that a reader entertains that the effect of incentivizing school
attendance may be discontinuous at zero, with the mean of Ys for s âˆˆ {0, 1, 2, sâˆ— } given by
Î·Ìƒ0 + 1 {s > 0} Î·Ìƒ1 + sÎ·Ìƒ2

(5)

for Î·Ìƒ a composite of Î· and Î¶ with Î·Ìƒ1 6= 0.11 The model F N (Â·) allows that the mean of Ys may follow
(5), as long as Î·Ìƒ1 is sufficiently small.12 The bound bN thus reflects a worst case over scenarios that
include (5).
Whether the set F RN (Â·) allows that the mean of Ys may follow (5) depends on the choice of




Î“. If Î“ = e1 e2 for es the basis vector corresponding to subsidy s, so that Î³Ì‚ = Y1 Y2 ,
then under F RN (Â·) the mean of Ys is linear in s for s > 0, which is consistent with (5). If instead
9

To cast this example into the notation of Section 3.1, take
ï£«
ï£¶


1 0
0
X = ï£­ 1 1 ï£¸ , â„¦ = Ï‰ 2 I3 , L =
.
sâˆ—
1 2
âˆ’1

For example, if we take M = (X 0 X) X 0 , then cÌ‚ is the the ordinary least squares extrapolation to sâˆ— , and is
also the maximum likelihood estimator of c under F 0 (Â·).
11
Specifically, choose Î·Ìƒ so that
XÌƒ Î·Ìƒ = Î¶ + XÎ·
10

for

ï£«

1
XÌƒ = ï£­ 1
1
12

0
1
1

ï£¶
0
1 ï£¸.
2

In particular, to ensure that a given Î·Ìƒ is consistent with kÎ¶kâ„¦âˆ’1 â‰¤ Âµ, it suffices that |Î·Ìƒ1 | â‰¤ Ï‰Âµ.

10

Î“ =



e0 e1



, so that Î³Ì‚ =



Y0 Y1



, then under F RN (Â·) the mean of Ys is linear in s for

s 6= 2, which is not consistent with (5). The bound bRN thus reflects a worst case over scenarios
that may or may not include (5), depending on the choice of Î“.
The informativeness âˆ† measures the extent to which the restriction to F RN (Â·) lessens the scope
âˆš
for bias, bRN /bN = 1 âˆ’ âˆ†. Again imagine a reader who entertains that the mean of Ys may follow


(5). Learning that âˆ† is close to one for Î³Ì‚ = Y1 Y2 might be reassuring to this reader because
the restriction from F N (Â·) to F RN (Â·) greatly lessens the scope for bias in cÌ‚ while still allowing for


(5). Learning that âˆ† is close to one for Î³Ì‚ = Y0 Y1 might not be as reassuring, because in this
case the restriction from F N (Â·) to F RN (Â·) rules out (5).

3.3
3.3.1

Discussion
Relationship to Analysis of Identification

Our analysis is distinct from an analysis of identification. We focus on the behavior of a particular
estimator cÌ‚ under misspecification, taking as given that c is identified under the base model. This
is distinct from asking whether the identification of c is parametric or nonparametric, and from
asking how the identified set changes under misspecification. To see the latter point, consider a case
where Î³Ì‚ is an unbiased estimator of c under F 0 (Â·), but differs from cÌ‚.13 An analysis of identification
would conclude that c is point-identified under F RN (Â·), whereas our analysis would conclude that
the estimator cÌ‚ may be biased under F RN (Â·).
We can connect our analysis to an analysis of identification if we consider identification from
the distribution of cÌ‚ alone. In particular, Proposition 1 implies that the identified set for c based on


âˆš
âˆš
the distribution of cÌ‚ is [cÌ‚ âˆ’ ÂµÏƒc , cÌ‚ + ÂµÏƒc ] under F N (Â·) and cÌ‚ âˆ’ ÂµÏƒc 1 âˆ’ âˆ†, cÌ‚ + ÂµÏƒc 1 âˆ’ âˆ† under
F RN (Â·). Under this interpretation, the ratio bRN /bN measures how much the identified set shrinks
when we restrict from F N (Â·) to F RN (Â·).
3.3.2

Interpretation and Limitations

We pause here to discuss some other aspects and limitations of our approach.
First, our analysis focuses on bounding the absolute bias of the estimator cÌ‚. Since the variance
of cÌ‚ is unaffected by misspecification, there is a one-to-one relationship between absolute bias and
MSE. So, for fixed Âµ, âˆ† governs the extent to which restricting from F N (Â·) to F RN (Â·) reduces the
maximal MSE for cÌ‚. Unlike for absolute bias, however, the ratio of worst-case MSEs under F N (Â·)
and F RN (Â·) depends in general on Âµ.
13

For instance, Î³Ì‚ might be an estimator based on matching a statistically non-sufficient set of moments, while cÌ‚
might be the maximum likelihood estimator.

11

Second, the correspondence F RN (Â·) requires that the relationship between c and Î³ specified
by the base model be correct local to each point in the base model. This is more restrictive
than requiring that the pair (c, Î³) be globally consistent with the base model, which yields the
correspondence F GN (Â·) with
(6)

o


 
n
F GN (c) = F N (c) âˆ© âˆªF âˆ— âˆˆF 0 (c) FÌƒ : Î³ FÌƒ = Î³ (F âˆ— )

for all c. If any (c, Î³) pair is possible under F 0 (Â·), then F GN (Â·) is equivalent to F N (Â·), but F RN (Â·)
need not be. More generally F N (c) âŠ‡ F GN (c) âŠ‡ F RN (c) âŠ‡ F 0 (c), and the ratio of worst case
âˆš
bias under F GN (Â·) to worst-case bias under F N (Â·) is bounded below by 1 âˆ’ âˆ†.
Third, we see the use of statistical distance to define the neighborhoods N (F ) as a key potential
limitation of our analysis. While defining neighborhoods in this way provides a practical default for
many situations, it also means that the informativeness âˆ† depends on the sampling process that
generates the data. To illustrate, suppose we are interested in estimating the average treatment
effect c of some policy, that cÌ‚ is a treatment-control difference from an RCT, and that Î³Ì‚ is the control
group mean from the same RCT. If the control group is much larger than the treatment group,
variability in cÌ‚ will primarily be driven by the treatment group mean, and the informativeness of Î³Ì‚
for cÌ‚ will be low. If, on the other hand, the control group is much smaller than the treatment group,
variability in cÌ‚ will primarily be driven by the control group mean, and the informativeness of Î³Ì‚
for cÌ‚ will be high. Thus, the informativeness of the control group mean for the average treatment
effect estimate in this setting depends on features of the experimental design, and not solely on
economic objects such as the distribution of potential outcomes.

4

Informativeness Under Local Misspecification

This section translates our results on finite-sample bias in the linear normal model to results on
asymptotic bias in nonlinear models with local misspecification. We first introduce our asymptotic
setting and state regularity conditions. We then prove our main result under local misspecification,
develop intuition for the local misspecification neighborhoods we consider, and discuss a version of
our analysis based on probability limits.
We assume that a researcher observes an i.i.d. sample Di âˆˆ D for i = 1, ..., n. The researcher
considers a base model which implies that Di âˆ¼ F (Î·), for Î· âˆˆ H a potentially infinite-dimensional
parameter. The implied joint distribution for the sample is Ã—ni=1 F (Î·). The parameter of interest
remains c (Î·) . The researcher computes (i) a scalar estimate cÌ‚ of c and (ii) a pÎ³ Ã— 1 vector of
descriptive statistics Î³Ì‚.
12

As in Section 2, to allow the possibility of misspecification we suppose that under the readerâ€™s
model Di âˆ¼ F (Î·, Î¶) for some (Î·, Î¶) âˆˆ H Ã— Z, where F (Î·, 0) = F (Î·) for all Î· âˆˆ H. The joint
distribution for the sample under the readerâ€™s model is Ã—ni=1 F (Î·, Î¶). Defining the correspondences
F N (Â·) and F RN (Â·) as in Section 2, we are interested in the ratio of worst-case biases bRN /bN .
While Section 3 exactly characterizes bRN /bN in the linear normal model, we are not aware of
similarly tractable expressions in general nonlinear settings. In this section, we therefore instead
approximate bRN /bN by characterizing the first-order asymptotic bias of the estimator cÌ‚ under
sequences of data generating processes in which (Î·, Î¶) approaches a base value (Î·0 , 0) âˆˆ H Ã— Z at
a root-n rate.
Formally, define H and Z as sets of values such that for any h âˆˆ H and z âˆˆ Z, we have
Î·0 + th âˆˆ H and tz âˆˆ Z for t âˆˆ R sufficiently close to zero.14 For Fh,z (th , tz ) = F (Î·0 + th h, tz z),
we consider behavior under sequences of data generating processes

S (h, z) =

Ã—ni=1 Fh,z



1
1
âˆš ,âˆš
n n

âˆž
.
n=1

The statement that (Î·, Î¶) approaches (Î·0 , 0) at a root-n rate should not be taken literally to
imply that the data generating process depends on the sample size, but is instead a device to approximate the finite-sample behavior of estimators in situations where the influence of misspecification
is on the same order as sampling uncertainty.15 Section 4.4 instead considers fixed misspecification
and develops results based on probability limits.
Throughout our analysis, we state assumptions in terms of the base distribution F0 = F (Î·0 ) .
If these assumptions hold for all Î·0 âˆˆ H then our local asymptotic approximations are valid local
to any point in the base model, though many of the asymptotic quantities we consider will depend
on the value of Î·0 . Section 5 discusses consistent estimators of these quantities that do not require
a priori knowledge of Î·0 .

4.1

Regularity Conditions

We next discuss a set of regularity conditions used in our asymptotic results. Our first assumption
requires that cÌ‚ and Î³Ì‚ behave, asymptotically, like sample averages.

14

For Î·0 + th 6âˆˆ H or tz 6âˆˆ Z we may define distributions arbitrarily.
The order âˆš1n perturbation to the base-model parameter Î· is a common asymptotic tool to analyze the local
behavior of estimators (see for example Chapters 7-9 of van der Vaart, 1998). Setting the degree of misspecification
proportional to âˆš1n is likewise a common technique for modeling local misspecification (see e.g. Newey (1985),
Andrews et al. (2017), and Armstrong and KolesaÌr (2019)).
15

13

Assumption 1. Under S (0, 0) ,
âˆš

(7)

1
n (cÌ‚ âˆ’ c (Î·0 ) , Î³Ì‚ âˆ’ Î³ (Î·0 )) = âˆš
n

n
X

Ï†c (Di ) ,

i=1

n
X

!
Ï†Î³ (Di )

+ op (1) ,

i=1

for functions Ï†c (Di ) and Ï†Î³ (Di ), where EF0 [Ï†c (Di )] = 0, EF0 [Ï†Î³ (Di )] = 0. For
ï£«
Î£=ï£­

Ïƒc2

Î£cÎ³

Î£Î³c Î£Î³Î³

ï£¶

0
EF0 Ï†c (Di )
EF0 Ï†c (Di ) Ï†Î³ (Di )
ï£¸,
ï£¸=ï£­

0
EF0 [Ï†Î³ (Di ) Ï†c (Di )] EF0 Ï†Î³ (Di ) Ï†Î³ (Di )
ï£¶

ï£«

h

2

i

Î£ is finite, Ïƒc2 > 0, and Î£Î³Î³ is positive-definite.
The functions Ï†c (Di ) and Ï†Î³ (Di ) are called the influence functions for the estimators cÌ‚ and
Î³Ì‚, respectively. Asymptotic linearity of the form in (7) holds for a wide range of estimators (see
e.g. Ichimura and Newey 2015), though it can fail for James-Stein, LASSO, and other shrinkage
estimators (e.g. Hansen 2016). Asymptotic linearity immediately implies that cÌ‚ and Î³Ì‚ are jointly
asymptotically normal under S (0, 0).
We next strengthen asymptotic normality of (cÌ‚, Î³Ì‚) to hold local to Î·0 under the base model.
We impose the following.
Assumption 2. Let Î³ (Î·) denote the probability limit of Î³Ì‚ under Ã—ni=1 F (Î·), and assume that for


all h âˆˆ H, Î³ (Î·0 + th) exists for t sufficiently close to zero. For any h âˆˆ H, cn (h) = c Î·0 + âˆš1n h ,


and Î³n (h) = Î³ Î·0 + âˆš1n h , under S (h, 0) we have
âˆš

ï£«
nï£­

cn (h) âˆ’ c (Î·0 )
Î³n (h) âˆ’ Î³ (Î·0 )

ï£¶

ï£«

ï£¸â†’ï£­

c? (h)
Î³ ? (h)

ï£¶
ï£¸,

and moreover
âˆš

ï£«
nï£­

cÌ‚ âˆ’ c (Î·0 )
Î³Ì‚ âˆ’ Î³ (Î·0 )

ï£¶

ï£«ï£«

ï£¸ â†’d N ï£­ï£­

c? (h)
Î³ ? (h)

ï£¶

ï£¶

ï£¸ , Î£ï£¸ .

The first part of Assumption 2 requires that cn (h) and Î³n (h) be asymptotically well-behaved, in
the sense that with appropriate recentering and scaling they converge to limits that can be written
as functions of h. Under this assumption, we can interpret c? (h) as the local parameter of interest,
playing the same role in our local asymptotic analysis as the parameter c does in the normal model.
The second part of Assumption 2 requires that (cÌ‚, Î³Ì‚) be a regular estimator of (c (Î·) , Î³ (Î·)) at Î·0
under the base model (see e.g., Newey 1994), and is again satisfied under mild primitive conditions

14

in a wide range of settings. In particular, this assumption implies that cÌ‚ is asymptotically unbiased
for our local parameter of interest c? (h) under S (h, 0).
We next assume the distributions F (Î·, Î¶) have densities f (d; Î·, Î¶) with respect to a common
dominating measure Î½. For (th , tz ) âˆˆ R2 , if we consider the perturbed distributions Fh,z (th , tz )
with densities fh,z (d; th , tz ) then the information matrix for (th , tz ), treating (h, z) as known, is
ï£®
ï£¯
Ih,z (th , tz ) = EFh,z (th ,tz ) ï£¯
ï£°



âˆ‚
f (Di ;th ,tz )
âˆ‚th h,z
fh,z (Di ;th ,tz )

2

âˆ‚
f (Di ;th ,tz ) âˆ‚ fh,z (Di ;th ,tz )
âˆ‚th h,z
âˆ‚tz
fh,z (Di ;th ,tz )
fh,z (Di ;th ,tz )

âˆ‚
f (Di ;th ,tz ) âˆ‚ fh,z (Di ;th ,tz )
âˆ‚th h,z
âˆ‚tz
fh,z (Di ;th ,tz )
fh,z (Di ;th ,tz )



âˆ‚
f (Di ;th ,tz )
âˆ‚tz h,z
fh,z (Di ;th ,tz )

2

ï£¹
ï£º
ï£º.
ï£»


We consider the two-dimensional submodels obtained by fixing (h, z), Fh,z (th , tz ) : (th , tz ) âˆˆ R2 ,
and impose a sufficient condition for these models to be differentiable in quadratic mean at zero.
Assumption 3. For all h âˆˆ H, z âˆˆ Z, there exists an open neighborhood of zero such that for
p
(th , tz ) in this neighborhood, (i) fh,z (d; th , tz ) is continuously differentiable with respect to (th , tz )
for all d âˆˆ D and (ii) Ih,z (th , tz ) is finite and continuous in (th , tz ).
Assumption 3 imposes standard conditions used in deriving asymptotic results, and holds in a
wide variety of settings; see Chapter 7.2 of van der Vaart (1998) for further discussion.
Finally, we require that the forms of misspecification we consider be sufficiently rich. To state
this assumption, let us define sh (d) =

âˆ‚
âˆ‚th

log (fh,z (d; 0, 0)), sz (d) =

âˆ‚
âˆ‚tz

log (fh,z (d; 0, 0)) as the

score functions corresponding to h and z, respectively.
Assumption 4. The set of score functions sz (Â·) includes all those consistent with Assumption 3,
h
i
in the sense that for any s (Â·) with EF0 [s (Di )] = 0 and EF0 s (Di )2 < âˆž there exists z âˆˆ Z with
h
i
EF0 (s (Di ) âˆ’ sz (Di ))2 = 0.
Assumption 4 requires that the set of score functions sz (Di ) implied by z âˆˆ Z include all those
consistent with Assumption 3.16 Intuitively, this means that the set of nesting model distributions
holding Î· fixed at Î·0 , {F (Î·0 , Î¶) : Î¶ âˆˆ Z}, looks (locally) like the set of all distributions, and so is
the local analogue of the richness condition discussed in Section 2. If this assumption fails, the
local asymptotic bias bounds we derive below continue to hold, but need not be sharp.
Under Assumption 4, the nesting model allows forms of misspecification against which all specification tests that control size have trivial local asymptotic power.17 This highlights an important
16
That the score function sz (Di ) has mean zero and finite variance under Assumption 3 follows from Lemma 7.6
and Theorem 7.2 in van der Vaart (1998).


17
In particular, for h âˆˆ H, Assumption 4 implies that there exists z âˆˆ Z such that EF0 (sh (Di ) âˆ’ sz (Di ))2 = 0.
Arguments along the same lines as e.g. Chen and Santos (2018) then imply that S (h, 0) and S (0, z) are asymptotically
indistinguishable, and thus that no specification test which controls the rejection probability under S (h, 0) has
nontrivial power against S (0, z).

15

aspect of our local analysis. A possible justification for bounding the degree of misspecification
(see, e.g., Huber and Ronchetti 2009, p. 294, as quoted in Bonhomme and Weidner 2018) is that
specification tests eventually detect unbounded misspecification with arbitrarily high probability, so
conditional on non-rejection it is reasonable to focus on bounded, and in particular local, misspecification. By contrast, we allow some forms of misspecification that are statistically undetectable
absent knowledge of the true parameters. Hence, restrictions on the magnitude of misspecification
in our setting should be understood as a-priori restrictions on the set of models considered, rather
than a-posteriori restrictions based on which models survive specification tests.

4.2

Main Result Under Local Misspecification

We can now derive the analogue of Proposition 1 in our local asymptotic framework. As a first
âˆš
step, we note that under our assumptions, n (cÌ‚ âˆ’ c (Î·0 ) , Î³Ì‚ âˆ’ Î³ (Î·0 )) is asymptotically normal with
variance Î£. Moreover, we obtain a simple expression for its asymptotic mean.
Lemma 1. If Assumptions 1-3 hold, then under S (h, z) for any (h, z) âˆˆ H Ã— Z,
âˆš

where

ï£«
ï£­

ï£«
nï£­

ï£¶

cÌ‚ âˆ’ c (Î·0 )
Î³Ì‚ âˆ’ Î³ (Î·0 )

cÌ„ (S (h, z))
Î³Ì„ (S (h, z))

ï£¶

ï£«ï£«

ï£¸ â†’d N ï£­ï£­

ï£«

ï£¸=ï£­

cÌ„ (S (h, z))
Î³Ì„ (S (h, z))

ï£¶

ï£¶

ï£¸ , Î£ï£¸ ,

EF0 [Ï†c (Di ) (sh (Di ) + sz (Di ))]
EF0 [Ï†Î³ (Di ) (sh (Di ) + sz (Di ))]

ï£¶
ï£¸.

Moreover, c? (h) = EF0 [Ï†c (Di ) sh (Di )], and Î³ ? (h) = EF0 [Ï†Î³ (Di ) sh (Di )] .
Recall that c? (h) is the parameter of interest in our local asymptotic analysis. We can thus
interpret cÌ„ (S (h, z)) âˆ’ c? (h) = EF0 [Ï†c (Di ) sz (Di )] as the first-order asymptotic bias of cÌ‚ under
S (h, z), analogous to EF [cÌ‚ âˆ’ c] under the normal model.
As in the normal model we restrict the degree of misspecification. We first consider the case of
correct specification. Let
S 0 (c? ) = {S (h, 0) : h âˆˆ H, c? (h) = c? }
denote the set of sequences in the base model such that the local parameter of interest takes value
c? . Limiting attention to sequences S âˆˆ S 0 (c? ) imposes correct specification, and is analogous to
limiting attention to F 0 (c).
To relax the assumption of correct specification, next suppose we bound the degree of local

16

misspecification by Âµ â‰¥ 0. For S âˆˆ S 0 (Â·) =

N (S) =

S

c?

S 0 (c? ) , let us define the neighborhood


h
i1
2 2
â‰¤Âµ .
S (h, z) : h âˆˆ H, S (h, 0) = S, z âˆˆ Z, EF0 sz (Di )

For reasons elaborated in Section 4.3 below, N (S) is a sequence-space analogue of the neighborhood
N (F ) defined in Section 2. Taking a union over N (S) for S âˆˆ S 0 (c? ) yields
n
o
SÌƒ âˆˆ N (S) ,

[

S N (c? ) =

SâˆˆS 0 (c? )

which we can interpret as the sequence-space analogue of F N (c).
Finally, let us define a restricted set of sequences as
n
 
o
SÌƒ âˆˆ N (S) : Î³Ì„ SÌƒ = Î³Ì„ (S) .

[

S RN (c? ) =

SâˆˆS 0 (c? )

Limiting attention to sequences S âˆˆ S RN (c? ) is analogous to limiting attention to the set F RN (c).
Let b?N and b?RN denote the worst-case firstâ€“order asymptotic bias under S N (Â·) and S RN (Â·),
respectively:
(8)

b?N = sup

sup

b?RN = sup

sup

|cÌ„ (S) âˆ’ c? |

c? SâˆˆS N (c? )

(9)

|cÌ„ (S) âˆ’ c? | .

c? SâˆˆS RN (c? )

Our main result under local misspecification is analogous to Proposition 1 under the normal model.
Proposition 2. Under Assumptions 1-4, the set of first-order asymptotic biases for cÌ‚ under S âˆˆ
S N (Â·) is

cÌ„ (S) âˆ’ c? : S âˆˆ S N (c? ) = [âˆ’ÂµÏƒc , ÂµÏƒc ] ,
for any c? such that S N (c? ) is nonempty, while the set of first-order asymptotic biases under
S âˆˆ S RN (Â·) is


?

cÌ„ (S) âˆ’ c : S âˆˆ S

RN

?

h

âˆš

âˆš

i

(c ) = âˆ’ÂµÏƒc 1 âˆ’ âˆ†, ÂµÏƒc 1 âˆ’ âˆ† ,

for any c? such that S RN (c? ) is nonempty. Hence,
âˆš
b?RN
= 1 âˆ’ âˆ†.
?
bN

17

4.3

Scaling of Perturbations

h
i
Under regularity conditions, the bound EF0 sz (Di )2 â‰¤ Âµ in the definition of N (S) can be inter



preted as a bound on the asymptotic Cressie-Read divergence of Fh,z âˆš1n , âˆš1n from Fh,z âˆš1n , 0 .
Specifically, we consider divergences of the form

 
fh,z (Di ; th , tz )
rh,z (th , tz ) = EFh,z (th ,0) Ïˆ
fh,z (Di ; th , 0)

(10)

for Ïˆ (Â·) a twice continuously differentiable function with Ïˆ (1) = 0 and Ïˆ 00 (1) = 2. A leading class
of such divergences is the Cressie-Read (1984) family, which takes
Ïˆ (x) =



2
xâˆ’Î» âˆ’ 1 .
Î» (Î» + 1)

Many well-known measures for the difference between distributions, including Kullback-Leibler
divergence and Hellinger distance, can be expressed as monotone transformations of Cressie-Read
(1984) divergences for appropriate Î».
Online Appendix B shows that under regularity conditions
h
i
lim n Â· rh,z (th , tz ) = EF0 sz (Di )2 .

(11)

nâ†’âˆž

Hence, Cressie-Read (1984) divergences yield the same asymptotic ranking over values of z, and
h
i
therefore over sequences S (h, z), as that implied by EF0 sz (Di )2 .18 Online Appendix C shows
h
i
that bounds on EF0 sz (Di )2 also correspond to bounds on the asymptotic power of tests to
distinguish elements of N (S) from S.

4.4

Non-Local Misspecification

To clarify the role of local misspecification in our results it is helpful to consider the analogue of âˆ†
under non-local misspecification. Suppose now that the reader believes the data follow Ã—ni=1 F (Î·, Î¶),
where (Î·, Î¶) do not change with the sample size. Let us denote the probability limits of cÌ‚ and Î³Ì‚
under F by cÌƒ (F ) and Î³ (F ) , respectively. We assume for ease of exposition that these probability
limits exist.
To simplify the analysis, let us further fix a value Î·0 of the base model parameter, so the true
value of the parameter of interest is c (Î·0 ). Suppose that for a divergence r of the form considered
18

In equation (11) we scale by n to obtain a nontrivial limit, as the divergence between Fh,z


Fh,z âˆš1n , âˆš1n tends to zero as n â†’ âˆž.

18



âˆš1 , 0
n



and

in Section 4.3, r (Î·, Î¶) = EF (Î·,0) [Ïˆ (f (Di ; Î·, Î¶) /f (Di ; Î·, 0))], the reader believes that the degree of
misspecification is bounded in the sense that r (Î·0 , Î¶) â‰¤ Âµ. Given this bound, the probability limit
of |cÌ‚ âˆ’ c (Î·0 )| is no larger than
bÌƒN (Âµ) = sup {|cÌƒ (F (Î·0 , Î¶)) âˆ’ c (Î·0 )| : Î¶ âˆˆ Z, r (Î·0 , Î¶) â‰¤ Âµ} ,
where we now make the dependence on Âµ explicit. This is a non-local analogue of the bias bound
b?N , fixing Î· = Î·0 . We can likewise bound the probability limit of |cÌ‚ âˆ’ c (Î·0 )| under an analogue of
F RN (Â·),
bÌƒRN (Âµ) = sup {|cÌƒ (F (Î·0 , Î¶)) âˆ’ c (Î·0 )| : Î¶ âˆˆ Z, r (Î·0 , Î¶) â‰¤ Âµ, Î³ (F (Î·0 , Î¶)) âˆ’ Î³ (F0 ) = 0} .
This is a non-local analogue of bias bound b?RN , again fixing Î· = Î·0 .
Provided that bÌƒN (Âµ) and bÌƒRN (Âµ) are both finite and non-zero, we can define a non-local anaËœ (Âµ) of informativeness âˆ† by
logue âˆ†
q

Ëœ (Âµ) = bÌƒRN (Âµ) .
1âˆ’âˆ†
bÌƒN (Âµ)

Ëœ (Âµ) based on finite
Online Appendix D shows that, under regularity conditions, an analogue of âˆ†
collections of Î¶ values converges to âˆ† as Âµ â†’ 0. This provides a sense in which âˆ† approximates
Ëœ (Âµ) when the degree of non-local misspecification is small.
âˆ†

5

Implementation

In a wide range of applications, convenient estimates Î£Ì‚ of Î£ are available from standard asymptotic
results (e.g., Newey and McFadden 1994) or via a bootstrap (e.g., Hall 1992). Given such an
estimate one can construct a plug-in estimate
Ë† =
âˆ†

(12)

Î£Ì‚cÎ³ Î£Ì‚âˆ’1
Î³Î³ Î£Ì‚Î³c
.
ÏƒÌ‚c2

Ë† under the sequences we study
Provided Î£Ì‚ is consistent under S (0, 0), consistency of Î£Ì‚ and âˆ†
follows immediately under our maintained assumptions that Ïƒc2 > 0 and Î£Î³Î³ has full rank.
p

Assumption 5. Î£Ì‚ â†’ Î£ under S (0, 0).
p
p
Ë† â†’
Proposition 3. Under Assumptions 3 and 5, Î£Ì‚ â†’ Î£ and âˆ†
âˆ† under S (h, z) for any h âˆˆ H,

z âˆˆ Z.
19

Mukhin (2018) provides alternative sufficient conditions for consistent estimation of informativeness,
and also derives results applicable to GMM models with non-local misspecification.

5.1

Implementation with Minimum Distance Estimators

We have so far imposed only high-level assumptions (specifically Assumptions 1 and 5) on cÌ‚, Î³Ì‚,
and Î£Ì‚. While these high-level assumptions hold in a wide range of settings, minimum distance
estimation is an important special case that encompasses a large number of applications. To
facilitate application of our results, in this section we discuss estimation of Î£ in cases where c (Î·)
can be written as a function of a finite-dimensional vector of parameters that are estimated by GMM
or another minimum distance approach (Newey and McFadden 1994), and Î³Ì‚ is likewise estimated
via minimum distance.
Formally, suppose that we can decompose Î· = (Î¸, Ï‰) where Î¸ is finite-dimensional and c (Î·)
depends on Î· only through Î¸, so we can write it as c (Î¸). We assume that c (Î¸) is continuously
differentiable in Î¸.
 
The researcher forms an estimate cÌ‚ = c Î¸Ì‚ where Î¸Ì‚ solves
min gÌ‚ (Î¸)0 WÌ‚ gÌ‚ (Î¸)

(13)

Î¸

for gÌ‚ (Î¸) a kg -dimensional vector of moments and WÌ‚ a kg Ã— kg -dimensional weighting matrix. The
researcher likewise computes Î³Ì‚ by solving
min mÌ‚ (Î³)0 UÌ‚ mÌ‚ (Î³) ,

(14)

Î³

for mÌ‚ (Î³) a km -dimensional vector of moments and UÌ‚ a km Ã— km -dimensional weighting matrix.
âˆš
âˆš
Provided WÌ‚ and UÌ‚ converge in probability to limits W and U , while ngÌ‚ (Î¸ (Î·0 )) and nmÌ‚ (Î³ (Î·0 ))
are jointly asymptotically normal under S (0, 0) ,
âˆš

ï£«
nï£­

gÌ‚ (Î¸ (Î·0 ))
mÌ‚ (Î³ (Î·0 ))

ï£¶

ï£« ï£«

ï£¸ â†’d N ï£­0, ï£­

Î£gg

Î£gm

Î£mg Î£mm

ï£¶ï£¶
ï£¸ï£¸ ,

existing results (see for example Theorem 3.2 in Newey and McFadden, 1994) imply that under
S (0, 0) and standard regularity conditions,
âˆš

ï£«
nï£­

cÌ‚ âˆ’ c (Î¸ (Î·0 ))
Î³Ì‚ âˆ’ Î³ (Î·0 )

ï£¶

ï£«

ï£¸ â†’d N (0, Î£) , Î£ = ï£­

Î›cg

0

0

Î›Î³m

20

ï£¶ï£«
ï£¸ï£­

Î£gg

Î£gm

Î£mg Î£mm

ï£¶ï£«
ï£¸ï£­

Î›cg

0

0

Î›Î³m

ï£¶0
ï£¸ .

Here Î›cg = âˆ’C (G0 W G)âˆ’1 G0 W and Î›Î³m = âˆ’ (M 0 U M )âˆ’1 M 0 U are the sensitivities of cÌ‚ with
respect to gÌ‚ (Î¸ (Î·0 )) and of Î³Ì‚ with respect to mÌ‚ (Î³ (Î·0 )) as defined in Andrews et al. (2017), and
C=

âˆ‚
âˆ‚Î¸ c (Î¸ (Î·0 )).

We can consistently estimate C by CÌ‚ =

âˆ‚
âˆ‚Î¸ c

 
Î¸Ì‚ . If gÌ‚ (Î¸) and mÌ‚ (Î³) are continuously differ-

entiable then under regularity conditions (see Theorem 4.3 in Newey and McFadden, 1994) we
 
âˆ‚
âˆ‚
can likewise consistently estimate G by GÌ‚ = âˆ‚Î¸
gÌ‚ Î¸Ì‚ and M by MÌ‚ = âˆ‚Î³
mÌ‚ (Î³Ì‚).19 Hence, given
consistent estimators Î£Ì‚gg , Î£Ì‚gm , and Î£Ì‚mm we can estimate Î£ by
ï£«
Î£Ì‚ = ï£­

Î›Ì‚cg

0

0

Î›Ì‚Î³m

ï£¶ï£«
ï£¸ï£­

Î£Ì‚gg

Î£Ì‚gm

Î£Ì‚mg Î£Ì‚mm

ï£¶ï£«
ï£¸ï£­

Î›Ì‚cg

0

0

Î›Ì‚Î³m

ï£¶0
ï£¸


âˆ’1

âˆ’1
MÌ‚ 0 UÌ‚ .
GÌ‚0 WÌ‚ and Î›Ì‚Î³m = âˆ’ MÌ‚ 0 UÌ‚ MÌ‚
for Î›Ì‚cg = âˆ’CÌ‚ GÌ‚0 WÌ‚ GÌ‚


What remains is to construct estimators Î£Ì‚gg , Î£Ì‚gm , Î£Ì‚mm . When Î¸Ì‚ and Î³Ì‚ are GMM or ML
estimators, we can write
gÌ‚ (Î¸) =

n

n

i=1

i=1

1X
1X
Ï†g (Di ; Î¸) , mÌ‚ (Î³) =
Ï†m (Di ; Î³) ,
n
n

for (Ï†g (Di ; Î¸) , Ï†m (Di ; Î³)) the moment functions for GMM or the score functions for ML. We can
then estimate Î£ by

(15)

ï£«
ï£¶
n
Ï†Ì‚c (Di )2
Ï†Ì‚c (Di ) Ï†Ì‚Î³ (Di )0
1 Xï£­
ï£¸,
Î£Ì‚ =
n
Ï†Ì‚Î³ (Di ) Ï†Ì‚c (Di ) Ï†Ì‚Î³ (Di ) Ï†Ì‚Î³ (Di )0
i=1

for



âˆ’1


Ï†Ì‚c (Di ) = Î›Ì‚cg Ï†g Di ; Î¸Ì‚ = âˆ’CÌ‚ GÌ‚0 WÌ‚ GÌ‚
GÌ‚0 WÌ‚ Ï†g Di ; Î¸Ì‚
and

âˆ’1
Ï†Ì‚Î³ (Di ) = Î›Ì‚Î³m Ï†m (Di ; Î³Ì‚) = âˆ’ MÌ‚ 0 UÌ‚ MÌ‚
MÌ‚ 0 UÌ‚ Ï†m (Di ; Î³Ì‚) .


In the GMM case, Ï†g Di ; Î¸Ì‚ and Ï†m (Di ; Î³Ì‚) are available immediately from the computation
of the final objective of the solver for (13) and (14), respectively. In the case of MLE, the score is
likewise often computed as part of the numerical gradient for the likelihood. The elements of Î›Ì‚cg
and Î›Ì‚Î³m are likewise commonly precomputed. The weights WÌ‚ and UÌ‚ are directly involved in the
19

If gÌ‚ (Î¸) and mÌ‚ (Î³) are not continuously differentiable, as sometimes occurs for simulation-based estimators, we
can estimate G and M in other ways. For example, we can estimate the jth column of G by the finite difference
âˆš
(gÌ‚ (Î¸ + ej Îµn ) âˆ’ gÌ‚ (Î¸ âˆ’ ej Îµn )) /2Îµn for ej the jth standard basis vector, where Îµn â†’ 0 and Îµn n â†’ âˆž as n â†’ âˆž. See
Section 7.3 of Newey and McFadden (1994) for details on this approach and sufficient conditions for its validity.

21

calculation of the objectives in (13) and (14), respectively. When gÌ‚ (Î¸) and mÌ‚ (Î³) are differentiable,
GÌ‚ and MÌ‚ are used in standard formulae for asymptotic inference on Î¸ and Î³, and the gradient CÌ‚
is used in delta-method calculations for asymptotic inference on c.20
In this sense, in many applications estimation of Î£ will involve only manipulation of vectors
and matrices already computed as part of estimation of, and inference on, the parameters Î¸, Î³, and
c.
Recipe. (GMM/MLE With Differentiable Moments)
 
1. Estimate Î¸Ì‚ and Î³Ì‚ following (13) and (14), respectively, and compute cÌ‚ = c Î¸Ì‚ .
n 
on
2. Collect Ï†g Di ; Î¸Ì‚

i=1

and {Ï†m (Di ; Î³Ì‚)}ni=1 from the calculation of the objective functions

in (13) and (14), respectively.
3. Collect the numerical gradients GÌ‚ =

âˆ‚
âˆ‚Î¸ gÌ‚

 
Î¸Ì‚ , MÌ‚ =

âˆ‚
âˆ‚Î³ mÌ‚ (Î³Ì‚),

and CÌ‚ =

âˆ‚
âˆ‚Î¸ c

 
Î¸Ì‚ from the

calculation of asymptotic standard errors for Î¸Ì‚, Î³Ì‚, and cÌ‚.
âˆ’1

âˆ’1

MÌ‚ 0 UÌ‚ using the weights WÌ‚
GÌ‚0 WÌ‚ and Î›Ì‚Î³m = âˆ’ MÌ‚ 0 UÌ‚ MÌ‚
4. Compute Î›Ì‚cg = âˆ’CÌ‚ GÌ‚0 WÌ‚ GÌ‚
and UÌ‚ from the objective functions in (13) and (14), respectively.


5. Compute Ï†Ì‚c (Di ) = Î›Ì‚cg Ï†g Di ; Î¸Ì‚ and Ï†Ì‚Î³ (Di ) = Î›Ì‚Î³m Ï†m (Di ; Î³Ì‚) for each i.
6. Compute Î£Ì‚ as in (15).
Ë† as in (12).
7. Compute âˆ†

6

Applications

In this section we present and interpret estimates of âˆ† for three structural articles in economics,
each of which estimates the parameters Î· of a base model via maximum likelihood. In each case,
we estimate the informativeness of each vector Î³Ì‚ for the estimator cÌ‚ following the recipe in Section
5.1. Because in each case model estimation is via maximum likelihood and Î³Ì‚ can be represented as
GMM, the recipe applies directly.

6.1

The Effects of PROGRESA

Attanasio et al. (2012a) use survey data from Mexico to study the effect of PROGRESA, a randomized social experiment involving a conditional cash transfer aimed in part at increasing persistence
20

Note that in cases where the function c (Î¸) depends on features of the data beyond Î¸, for example on the
distribution of covariates, our formulation implicitly treats those features as fixed at their sample values for the
purposes of estimating âˆ†. Online Appendix E discusses how to account for such additional dependence on the data,
and presents corresponding calculations for some of our applications.

22

in school. The paper uses the estimated base model to predict the effect of a counterfactual intervention in which total school enrollment is increased via a budget-neutral reallocation of program
funds.
The estimate of interest cÌ‚ is the partial-equilibrium effect of the counterfactual rebudgeting on
the school enrollment of eligible children, accumulated across age groups (Attanasio et al. 2012a,
sum of ordinates for the line labeled â€œfixed wagesâ€ in Figure 2, minus sum of ordinates for the line
labeled â€œfixed wagesâ€ in the left-hand panel of Figure 1).
Attanasio et al. (2012a) discuss the â€œexogenous variability in [their] data that drives [their]
resultsâ€ as follows (p. 53):
The comparison between treatment and control villages and between eligible and
ineligible households within these villages can only identify the effect of the existence
of the grant. However, the amount of the grant varies by the grade of the child. The
fact that children of different ages attend the same grade offers a source of variation
of the amount that can be used to identify the effect of the size of the grant. Given
the demographic variables included in our model and given our treatment for initial
conditions, this variation can be taken as exogenous. Moreover, the way that the grant
amount changes with grade varies in a non-linear way, which also helps identify the
effect.
Thus, the effect of the grant is identified by comparing across treatment and control
villages, by comparing across eligible and ineligible households (having controlled for
being â€œnon-poorâ€), and by comparing across different ages within and between grades.
(p. 53)
Motivated by this discussion, we define three vectors Î³Ì‚ of descriptive statistics, which correspond
to sample treatment-control differences from the experimental data. The first vector (â€œimpact
on eligiblesâ€) consists of the age-grade-specific treatment-control differences for eligible children
(interacting elements of Attanasio et al. 2012a, Table 2, single-age rows of the column labeled
â€œImpact on Poor 97,â€ with the childâ€™s grade). The second vector (â€œimpact on ineligiblesâ€) consists
of the age-grade-specific treatment-control differences for ineligible children (interacting elements
of Attanasio et al. 2012a, Table 2, single-age rows of the column labeled â€œImpact on non-eligible,â€
with the childâ€™s grade). The third vector consists of both of these groups of statistics.
Table I reports the estimated informativeness of each vector of descriptive statistics. The
estimated informativeness for the combined vector is 0.28. This is largely accounted for by the
age-grade-specific treatment-control differences for eligible children.

23

Table I. Estimated informativeness of descriptive statistics for the effect of a counterfactual rebudgeting of PROGRESA (Attanasio et al. 2012a)
Descriptive statistics Î³Ì‚
All
Impact on eligibles
Impact on ineligibles

Ë†
Estimated informativeness âˆ†
0.283
0.227
0.056

Ë† of three vectors Î³Ì‚ of descriptive statistics
Notes: The table shows the estimated informativeness âˆ†
for the estimated partial-equilibrium effect cÌ‚ of the counterfactual rebudgeting on the school enrollment of eligible children, accumulated across age groups (Attanasio et al. 2012a, sum of ordinates
for the line labeled â€œfixed wagesâ€ in Figure 2, minus sum of ordinates for the line labeled â€œfixed
wagesâ€ in the left-hand panel of Figure 1). Vector Î³Ì‚ â€œimpact on eligiblesâ€ consists of the age-gradespecific treatment-control differences for eligible children (interacting elements of Attanasio et al.
2012a, Table 2, single-age rows of the column labeled â€œImpact on Poor 97,â€ with the childâ€™s grade).
Vector Î³Ì‚ â€œimpact on ineligiblesâ€ consists of the age-grade-specific treatment-control differences for
ineligible children (interacting elements of Attanasio et al. 2012a Table 2, single-age rows of the
column labeled â€œImpact on non-eligible,â€ with the childâ€™s grade). Vector Î³Ì‚ â€œallâ€ consists of both
Ë† is calculated according to the recipe in
of these groups of statistics. Estimated informativeness âˆ†
Section 5.1 using the replication code and data posted by Attanasio et al. (2012b).
Restricting from F N (Â·) to F RN (Â·) reduces the worst-case bias by an estimated factor of 1 âˆ’
âˆš

1 âˆ’ 0.28 â‰ˆ 0.15 in the sense of Proposition 2. Further reduction in the worst-case bias would

require including in Î³Ì‚ descriptive statistics that are orthogonal to the treatment-control differences
we consider, thus imposing that F RN (Â·) respects the relationship specified by the base model F 0 (Â·)
between c and the features of the distribution of the data estimated by these orthogonal statistics.
To illustrate the distinction between informativeness and identification highlighted in Section
3.3.1, now let c be the partial-equilibrium effect of the actual program on the school enrollment of
eligible children, accumulated across age groups. The parameter c is nonparametrically identified,
and can be nonparametrically estimated by comparing the school enrollment of eligible children in
treatment and control villages (as in Attanasio et al. 2012a, Table 2, column labeled â€œImpact on
Poor 97â€). The parameter c can also be estimated parametrically using the researcherâ€™s estimated
model (as in Attanasio et al. 2012a, sum of ordinates for the line labeled â€œfixed wagesâ€ in the
left-hand panel of Figure 1). The descriptive statistics Î³Ì‚ have an informativeness of 1 for a natural
nonparametric estimator, and an estimated informativeness of 0.31 for the parametric estimator,
indicating that assumptions beyond those required for nonparametric identification are necessary
to guarantee that the parametric estimator is unbiased in the sense of Proposition 2.

24

6.2

Newspaper Demand

Gentzkow (2007a) uses survey data from a cross-section of individuals to estimate demand for print
and online newspapers in Washington DC. A central goal of Gentzkowâ€™s (2007a) paper is to estimate
the extent to which online editions of papers crowd out readership of the associated print editions,
which in turn depends on a key parameter governing the extent of print-online substitutability.
The estimate of interest cÌ‚ is the change in readership of the Washington Post print edition that
would occur if the Post online edition were removed from the choice set (Gentzkow 2007a, Table
10, row labeled â€œChange in Post readershipâ€).
Gentzkow (2007a) discusses two features of the data that can help to distinguish correlated
tastes from true substitutability: (i) a set of instrumentsâ€”such as a measure of Internet access
at workâ€”that plausibly shift the utility of online papers but do not otherwise affect the utility of
print papers; and (ii) a coarse form of panel dataâ€”separate measures of consumption in the last
day and last five weekdaysâ€”that make it possible to relate changes in consumption of the print
edition to changes in consumption of the online edition over time for the same individual (p. 730).
Motivated by Gentzkowâ€™s (2007a) discussion, we define three vectors Î³Ì‚ of descriptive statistics.
The first vector (â€œIV coefficientâ€) is the coefficient from a 2SLS regression of last-five-weekday
print readership on last-five-weekday online readership, instrumenting for the latter with the set of
instruments (Gentzkow 2007a, Table 4, Column 2, first row). The second vector (â€œpanel coefficientâ€)
is the coefficient from an OLS regression of last-one-day print readership on last-one-day online
readership controlling for the full set of interactions between indicators for print readership and
indicators for online readership in the last five weekdays. Each of these regressions includes the
standard set of demographic controls from Gentzkow (2007a, Table 5). The third vector Î³Ì‚ consists
of both the IV coefficient and the panel coefficient. Thus, the first two vectors have dimension 1,
and the third has dimension 2.
Table II reports the estimated informativeness of each vector of descriptive statistics. The
estimated informativeness of the combined vector is 0.51. This is accounted for almost entirely by
the panel coefficient, which alone has an estimated informativeness of 0.50. The IV coefficient, by
contrast, has an estimated informativeness of only 0.01.
Gentzkowâ€™s (2007a) discussion of identification highlights both the exclusion restrictions underlying the IV coefficient and the panel variation underlying the panel coefficient as potential sources
of identification, and if anything places more emphasis on the former. Based on Gentzkowâ€™s (2007a)
discussion, and the large literature showing that exclusion restrictions can be used to establish nonparametric identification in closely related models (Matzkin 2007), it is tempting to conclude that
accepting the relationship specified by the base model between the counterfactual c and the pop25

Table II. Estimated informativeness of descriptive statistics for the effect of eliminating the Post
online edition (Gentzkow 2007a)
Descriptive statistics Î³Ì‚
All
IV coefficient
Panel coefficient

Ë†
Estimated informativeness âˆ†
0.514
0.009
0.503

Ë† of three vectors Î³Ì‚ of descriptive statistics
Notes: The table shows the estimated informativeness âˆ†
for the estimated effect cÌ‚ on the readership of the Post print edition if the Post online edition were
removed from the choice set (Gentzkow 2007a, table 10, row labeled â€œChange in Post readershipâ€).
Vector Î³Ì‚ â€œIV coefficientâ€ is the coefficient from a 2SLS regression of last-five-weekday print readership on last-five-weekday online readership, instrumenting for the latter with the set of excluded
variables such as Internet access at work (Gentzkow 2007a, Table 4, Column 2, first row). Vector
Î³Ì‚ â€œpanel coefficientâ€ is the coefficient from an OLS regression of last-one-day print readership on
last-one-day online readership controlling for the full set of interactions between indicators for print
readership and for online readership in the last five weekdays. Each of these regressions includes
the standard set of demographic controls from Gentzkow (2007a, Table 5). Vector Î³Ì‚ â€œallâ€ consists
Ë† is calculated
of both the IV coefficient and the panel coefficient. Estimated informativeness âˆ†
according to the recipe in Section 5.1 using the replication code and data posted by Gentzkow
(2007b).
ulation value of the IV coefficient would greatly limit the scope for bias in Gentzkowâ€™s (2007a)
estimator cÌ‚.
FN

Our findings suggest otherwise. When Î³Ì‚ consists only of the IV coefficient, restricting from
âˆš
(Â·) to F RN (Â·) reduces the worst-case bias in cÌ‚ by an estimated factor of only 1âˆ’ 1 âˆ’ 0.01 < 0.01

in the sense of Proposition 2. By contrast, when Î³Ì‚ consists only of the panel coefficient, restricting
âˆš
from F N (Â·) to F RN (Â·) reduces the worst-case bias in cÌ‚ by an estimated factor of 1 âˆ’ 1 âˆ’ 0.50 â‰ˆ
0.29. Intuitively, a reader interested in evaluating the scope for bias in cÌ‚ may wish to focus more
attention on the assumptions of the base model that relate c to the population value of the panel
coefficient (e.g., restrictions on the time structure of preference shocks), than on assumptions that
relate c to the population value of the IV coefficient (e.g., exclusion restrictions).

6.3

Long-term Care Insurance

Hendren (2013a) uses data on insurance eligibility and self-reported beliefs about the likelihood of
different types of â€œlossâ€ events (e.g., becoming disabled) to recover the distribution of underlying
beliefs and rationalize why some groups are routinely denied insurance coverage. We focus here on
Hendrenâ€™s (2013a) model of the market for long-term care (LTC) insurance.
The estimate of interest cÌ‚ is the minimum pooled price ratio among rejectees (Hendren 2013a,
Table V, row labeled â€œReject,â€ column labeled â€œLTCâ€). The minimum pooled price ratio determines
26

Table III. Estimated informativeness of descriptive statistics for the minimum pooled price ratio
(Hendren 2013a)
Descriptive statistics Î³Ì‚
All
Fractions in focal point groups
Fractions in non-focal point groups
Fraction in each group needing LTC

Ë†
Estimated informativeness âˆ†
0.700
0.005
0.018
0.676

Ë† of four vectors Î³Ì‚ of descriptive statistics
Notes: The table shows the estimated informativeness âˆ†
for the â€œminimum pooled price ratioâ€ cÌ‚ (Hendren 2013a, Table V, row labeled â€œReject,â€ column
labeled â€œLTCâ€). Vector Î³Ì‚ â€œfractions in focal point groupsâ€ consists of the fraction of respondents
who report exactly 0, the fraction who report exactly 0.5, and the fraction who report exactly 1.
Vector Î³Ì‚ â€œfractions in non-focal point groupsâ€ consists of the fractions of respondents whose reports
are in each of the intervals (0.1, 0.2], (0.2, 0.3], (0.3, 0.4], (0.4, 0.5), (0.5, 0.6], (0.6, 0.7], (0.7, 0.8],
(0.8, 0.9], and (0.9, 1). Vector Î³Ì‚ â€œfraction in each group needing LTCâ€ consists of the fractions of
respondents giving each of the preceding reports who eventually need long-term care. Vector Î³Ì‚ â€œallâ€
Ë† is calculated according to the
consists of all three of the other vectors. Estimated informativeness âˆ†
recipe in Section 5.1 using the replication code and data posted by Hendren (2013b), supplemented
with additional calculations provided by the author.
the range of preferences for which insurance markets cannot exist (Hendren 2013a, Corollary 2 to
Theorem 1). This ratio is a key output of the analysis, as it provides an economic rationale for the
insurance denials that are the paperâ€™s focus.
Hendren (2013a) explains that the parameters that determine the minimum pooled price ratio
are identified from the relationship between elicited beliefs and the eventual realization of loss events
such as long term care (pp. 1751-2).
Motivated by Hendrenâ€™s (2013a) discussion, we define four vectors Î³Ì‚ of descriptive statistics.
The first vector (â€œfractions in focal-point groupsâ€) consists of the fraction of respondents who report
exactly 0, the fraction who report exactly 0.5, and the fraction who report exactly 1. The second
vector (â€œfractions in non-focal-point groupsâ€) consists of the fractions of respondents whose reports
are in each of the intervals (0.1, 0.2], (0.2, 0.3], (0.3, 0.4], (0.4, 0.5), (0.5, 0.6], (0.6, 0.7], (0.7, 0.8],
(0.8, 0.9], and (0.9, 1). The third vector (â€œfraction in each group needing LTCâ€) consists of the
fraction of respondents giving each of the preceding reports who eventually need long-term care.
The fourth vector Î³Ì‚ consists of all three of the other vectors.
Hendrenâ€™s (2013a) discussion suggests that the third vector will be especially informative for
the minimum pooled price ratio.
Table III reports the estimated informativeness of each vector of descriptive statistics. The
estimated informativeness of the combined vector is 0.70. The estimated informativeness is 0.01
with respect to the fractions in focal point groups, 0.02 with respect to the fractions in non-focal27

point groups, and 0.68 with respect to the fraction in each group needing LTC. When Î³Ì‚ consists
only of the fraction in each group needing LTC, restricting from F N (Â·) to F RN (Â·) reduces the
âˆš
worst-case bias in cÌ‚ by an estimated factor of 1 âˆ’ 1 âˆ’ 0.68 â‰ˆ 0.43. This finding seems consistent
with the authorâ€™s discussion.

7

Conclusions

Descriptive analysis has become an important complement to structural estimation. It is common
for a researcher to report descriptive statistics Î³Ì‚ that estimate features Î³ of the distribution of the
data that are in turn related to the quantity c of interest under the researcherâ€™s model. A reader
who accepts the relationship between the features Î³ and the structural quantity c specified by the
researcherâ€™s model, and who believes that the statistics Î³Ì‚ play an important role in â€œdrivingâ€ the
structural estimate cÌ‚, may then be more confident in the researcherâ€™s conclusions.
We propose one way to formalize this logic. We define a measure âˆ† of the informativeness of
descriptive statistics Î³Ì‚ for a structural estimate cÌ‚. Informativeness captures the share of variation
in cÌ‚ that is explained by Î³Ì‚ under their joint asymptotic distribution. We show that, under some
conditions, informativeness also governs the reduction in worst-case bias from accepting the relationship between Î³ and c specified by the researcherâ€™s model. In this sense, descriptive analysis
based on statistics with high informativeness can indeed increase confidence in structural estimates.
Informativeness can be computed at negligible cost even for complex models, and we provide
a convenient recipe for computing it. We show in the context of our applications that reporting
informativeness can sharpen the interpretation of structural estimates in important economic settings. We recommend that researchers report estimated informativeness alongside their descriptive
analyses.

Proofs
Proof of Proposition 1 First consider F âˆˆ F N (c) . By the definition of F N (Â·) there exist
Î· âˆˆ Rp , Î¶ âˆˆ Rk such that F = F (Î·, Î¶) and c = c (Î·) . Note, moreover, that since c (Î·) = L0 Î· while
EF [cÌ‚] = L0 Î· + C 0 Î¶, EF [cÌ‚ âˆ’ c] = C 0 Î¶. Thus, our task reduces to showing that
n
o
C 0 Î¶ : Î¶ âˆˆ Rk , kÎ¶kâ„¦âˆ’1 â‰¤ Âµ = [âˆ’ÂµÏƒc , ÂµÏƒc ] .
1

1

Note, however, that C 0 Î¶ = C 0 â„¦ 2 â„¦âˆ’ 2 Î¶, so by the Cauchy-Schwarz inequality, |C 0 Î¶| â‰¤ Ïƒc kÎ¶kâ„¦âˆ’1 .
Hence, to prove the result we need only show that any bias cÌ„ with |cÌ„| â‰¤ ÂµÏƒc can be achieved. To

28

this end, pick such a |cÌ„| â‰¤ ÂµÏƒc . Consider Î¶ =

cÌ„
â„¦C
Ïƒc2

and note that C 0 Î¶ = cÌ„ and kÎ¶kâ„¦âˆ’1 =

cÌ„
Ïƒc

â‰¤ Âµ,

as desired.
Next consider F âˆˆ F RN (c) . By the definition of F RN (Â·) there exist Î· âˆˆ Rp , Î¶ âˆˆ Rk such that
F = F (Î·, Î¶), c = c (Î·) , and Î“0 (XÎ· + Î¶) = Î“0 XÎ·. Thus, our task reduces to showing that
n
o h
i
âˆš
âˆš
C 0 Î¶ : Î¶ âˆˆ Rk , kÎ¶kâ„¦âˆ’1 â‰¤ Âµ, Î“0 Î¶ = 0 = âˆ’ÂµÏƒc 1 âˆ’ âˆ†, ÂµÏƒc 1 âˆ’ âˆ† .
Let us first show that for any Î¶ with kÎ¶kâ„¦âˆ’1 â‰¤ Âµ and Î“0 Î¶ = 0, C 0 Î¶ satisfies these bounds. To this
0
0
0
end, define CÌƒ = C âˆ’ Î“Î›0 for Î› = Î£cÎ³ Î£âˆ’1
Î³Î³ , and note that for any Î¶ with Î“ Î¶ = 0, CÌƒ Î¶ = C Î¶. Note,
p
next, that CÌƒ 0 Î¶ â‰¤ CÌƒ 0 â„¦CÌƒ kÎ¶kâ„¦âˆ’1 by the Cauchy-Schwarz inequality, and that
2
CÌƒ 0 â„¦CÌƒ = Ïƒc2 âˆ’ Î£cÎ³ Î£âˆ’1
Î³Î³ Î£Î³c = Ïƒc (1 âˆ’ âˆ†) ,

âˆš
from which the result follows. We next want to show that for any cÌ„ with |cÌ„| â‰¤ ÂµÏƒc 1 âˆ’ âˆ† there
exists Î¶ with kÎ¶kâ„¦âˆ’1 â‰¤ Âµ and Î“0 Î¶ = 0 such that C 0 Î¶ = cÌ„. This result is trivial if âˆ† = 1, so let us
âˆš
cÌ„
suppose that âˆ† < 1, and pick some cÌ„ with |cÌ„| â‰¤ ÂµÏƒc 1 âˆ’ âˆ†. Define Î¶ = Ïƒ2 (1âˆ’âˆ†)
â„¦CÌƒ and note that
c

Î“0 Î¶ = 0 and C 0 Î¶ = CÌƒ 0 Î¶ = cÌ„, while
kÎ¶k2â„¦âˆ’1 =

cÌ„2
,
Ïƒc2 (1 âˆ’ âˆ†)

which is bounded above by Âµ2 .
Proof of Lemma 1 By Lemma 7.6 of van der Vaart (1998), Assumption 3 implies that

p

fh,z (Di ; th , tz )

is differentiable in quadratic mean in the sense that for all (h, z) âˆˆ H Ã— Z,
Z q

fh,z (Di ; th , tz ) âˆ’

q

2
q

1
fh,z (Di ; 0, 0) âˆ’ (th sh (d) + tz sz (d)) fh,z (Di ; 0, 0) dÎ½ (d) = o (th , tz )0
2

as (th , tz ) â†’ 0. Hence, Theorem 7.2 of van der Vaart (1998) implies that under S (0, 0), defining
F n = Ã—ni=1 F ,
ï£«
log ï£­

n
dFh,z



th âˆš
âˆš
, tzn
n
dF0n

ï£¶

n

X
1
ï£¸ = âˆš1
(th sh (Di ) + tz sz (Di )) âˆ’
2
n
i=1

th
tz

!0
Ih,z (0, 0)

th
tz

!
+ op (1)

h
i
h
i
and that EF0 [sh (Di )] = EF0 [sz (Di )] = 0. Since EF0 sh (Di )2 and EF0 sz (Di )2 are finite,
Assumption 1, the Central Limit Theorem, and Slutskyâ€™s Lemma imply that under S (0, 0) , for
g (Di ; h, z) = sh (Di ) + sz (Di ) ,
log

n
dFh,z



âˆš1 , âˆš1
n
n
dF0n

!0

!
âˆš1
n

P

29

Ï†c (Di )

âˆš1
n

P

Ï†Î³ (Di )

0

2



h
i ï£¶
ï£¶
âˆ’ 12 EF0 g (Di ; h, z)2
ï£· âˆ—ï£·
ï£¬ï£¬
ï£·,Î£ ï£·,
ï£¬
â†’d N ï£¬
0
ï£¸
ï£¸
ï£­ï£­
0
ï£«ï£«

for
h
i
EF0 g (Di ; h, z)2



EF0 [g (Di ; h, z) Ï†c (Di )] EF0 g (Di ; h, z) Ï†Î³ (Di )0
h
i
ï£¬


2
Î£âˆ— = ï£¬
[g
(D
;
h,
z)
Ï†
(D
)]
E
Ï†
(D
)
EF0 Ï†c (Di ) Ï†Î³ (Di )0
E
i
c
i
c
i
F0
ï£­ F0


EF0 Ï†Î³ (Di ) Ï†Î³ (Di )0
EF0 [Ï†Î³ (Di ) Ï†c (Di )]
EF0 [g (Di ; h, z) Ï†Î³ (Di )]
ï£«

ï£¶
ï£·
ï£·.
ï£¸

By LeCamâ€™sfirst lemma
 (see
 Example 6.5 of van der Vaart 1998) the convergence in distribution
1
1
n
n
of log dFh,z âˆšn , âˆšn /dF0 to a normal with mean equal to âˆ’ 12 of its variance implies that the


n
âˆš1 , âˆš1
are mutually contiguous. Le Camâ€™s third lemma (see
sequences Ã—ni=1 F0 and Ã—ni=1 Fh,z
n
n
Example 6.7 of van der Vaart 1998) then implies that under S (h, z) ,

log

n
dFh,z



âˆš1 , âˆš1
n
n
dF0n

ï£«ï£«

!0

!
âˆš1
n

1
2 EF0

h

P

âˆš1
n

Ï†c (Di )

g (Di ; h, z)2

i

P

ï£¶

Ï†Î³ (Di )

0

ï£¶

ï£¬ï£¬
ï£·
ï£·
ï£¬ E [Ï† (D ) g (D ; h, z)] ï£· , Î£âˆ— ï£· .
â†’d N ï£¬
i
ï£­ï£­ F0 c i
ï£¸
ï£¸
EF0 [Ï†Î³ (Di ) g (Di ; h, z)]
Together with contiguity, Assumption 1 implies that
âˆš


X

1 X
n cÌ‚ âˆ’ c (Î·0 ) , Î³Ì‚ 0 âˆ’ Î³ (Î·0 )0 âˆ’ âˆš
Ï†c (Di ) ,
Ï†Î³ (Di )0 = op (1) ,
n

under S (h, z) , from which the result is immediate for
cÌ„ (S (h, z))
Î³Ì„ (S (h, z))

!
=

EF0 [Ï†c (Di ) g (Di ; h, z)]
EF0 [Ï†Î³ (Di ) g (Di ; h, z)]

!
=

EF0 [Ï†c (Di ) (sh (Di ) + sz (Di ))]
EF0 [Ï†Î³ (Di ) (sh (Di ) + sz (Di ))]

!
.

Finally, note that Assumption 2 implies c? (h) = cÌ„ (S (h, 0)) and Î³ ? (h) = Î³Ì„ (S (h, 0)) . Consequently, by the results above we have c? (h) = EF0 [Ï†c (Di ) sh (Di )], and Î³ ? (h) = EF0 [Ï†Î³ (Di ) sh (Di )] .
Proof of Proposition 2 Let us first consider the case with S âˆˆ S N (c? ) , with S N (c? ) nonempty.
By the definition of S N (c? ) and Lemma 1, for any S âˆˆ S N (c? ) there exist (h, z) âˆˆ H Ã— Z with
S = S (h, z) and c? (h) = c? . For this (h, z) we can write
cÌ„ (S) âˆ’ c? = EF0 [Ï†c (Di ) (sh (Di ) + sz (Di ))] âˆ’ EF0 [Ï†c (Di ) sh (Di )] = EF0 [Ï†c (Di ) sz (Di )] .

30

Writing cÌ„z = EF0 [Ï†c (Di ) sz (Di )] for brevity, our task thus reduces to showing
n
h
i
o
cÌ„z : z âˆˆ Z, EF0 sz (Di )2 â‰¤ Âµ2 = [âˆ’ÂµÏƒc , ÂµÏƒc ] .
Note, however, that by the Cauchy-Schwarz inequality
r
|cÌ„z | â‰¤

h

EF0 Ï†c (Di )

2

ir

h

2

EF0 sz (Di )

i

â‰¤ ÂµÏƒc .

h
i
Hence, for any z âˆˆ Z with EF0 sz (Di )2 â‰¤ Âµ2 , cÌ„z necessarily satisfies the bounds. Going the other
direction, for any cÌ„ with |cÌ„| â‰¤ ÂµÏƒc , if we take sâˆ— (Di ) = ÏƒcÌ„2 Ï†c (Di ) , we have EF0 [sâˆ— (Di ) Ï†c (Di )] =
c
h
i
2
âˆ—
2
2
2
cÌ„, while EF0 s (Di ) = cÌ„ /Ïƒc â‰¤ Âµ . By Assumption 4, however, there exists z âˆˆ Z with
h
i
h
i
EF0 (sâˆ— (Di ) âˆ’ sz (Di ))2 = 0, so cÌ„z = cÌ„ and EF0 sz (Di )2 â‰¤ Âµ2 , as desired.
For the case with S âˆˆ S RN (c? ) , note that by the definition of S RN (c? ) and Lemma 1, for any
S âˆˆ S RN (c? ) there exist (h, z) âˆˆ H Ã— Z with S = S (h, z), c? (h) = c? , and
EF0 [Ï†Î³ (Di ) (sh (Di ) + sz (Di ))] âˆ’ EF0 [Ï†Î³ (Di ) sh (Di )] = EF0 [Ï†Î³ (Di ) sz (Di )] = 0.
Thus, writing Î³Ì„z = EF0 [Ï†Î³ (Di ) sz (Di )] for brevity, our task reduces to showing that
i
n
o h
h
i
âˆš
âˆš
2
2
cÌ„z : z âˆˆ Z, Î³Ì„z = 0, EF0 sz (Di ) â‰¤ Âµ = âˆ’ÂµÏƒc 1 âˆ’ âˆ†, ÂµÏƒc 1 âˆ’ âˆ† .
0
Let Î› = Î£âˆ’1
Î³Î³ Î£Î³c . For Ï†Ìƒc (Di ) = Ï†c (Di ) âˆ’ Î› Ï†Î³ (Di ) , note that if Î³Ì„z = 0 then

h
i
EF0 [Ï†c (Di ) sz (Di )] = EF0 Ï†Ìƒc (Di ) sz (Di ) .
The Cauchy-Schwarz inequality then implies that
r
h
h
i
ir
h
i
2
EF0 sz (Di )2
EF0 Ï†Ìƒc (Di ) sz (Di ) â‰¤ EF0 Ï†Ìƒc (Di )

=

q

r
Ïƒc2

âˆ’ Î›Î£Î³Î³

Î›0

r
h
i
h
i
âˆš
2
EF0 sz (Di ) = Ïƒc 1 âˆ’ âˆ† EF0 sz (Di )2 .

h
i
Hence, we see that for z such that EF0 sz (Di )2 â‰¤ Âµ2 ,
h
i
âˆš
âˆš
cÌ„z âˆˆ âˆ’ÂµÏƒc 1 âˆ’ âˆ†Âµ, ÂµÏƒc 1 âˆ’ âˆ† ,
which are the bounds stated in the proposition.
To complete the proof it remains to show that these bounds are tight, so that for any (cÌ„, Âµ)
with

h
i
âˆš
âˆš
cÌ„ âˆˆ âˆ’ÂµÏƒc 1 âˆ’ âˆ†, ÂµÏƒc 1 âˆ’ âˆ†

31

h
i
there exists z âˆˆ Z with cÌ„z = cÌ„, Î³Ì„z = 0, and EF0 sz (Di )2 â‰¤ Âµ2 .This result is trivial if âˆ† = 1, so
âˆš
let us suppose that âˆ† < 1 and pick some cÌ„ with |cÌ„| â‰¤ ÂµÏƒc 1 âˆ’ âˆ†. Now define
sâˆ— (Di ; cÌ„) = Ï†Ìƒc (Di )

cÌ„
.
Ïƒc2 (1 âˆ’ âˆ†)

Note that EF0 [Ï†Î³ (Di ) sâˆ— (Di ; cÌ„)] = 0, while
h
i
EF0 [Ï†c (Di ) sâˆ— (Di ; cÌ„)] = EF0 Ï†Ìƒc (Di )2

cÌ„
= cÌ„.
Ïƒc2 (1 âˆ’ âˆ†)

Moreover,
cÌ„2
.
Ïƒc2 (1 âˆ’ âˆ†)
h
i
âˆš
However, by the definition of cÌ„ we know that |cÌ„| â‰¤ ÂµÏƒc 1 âˆ’ âˆ†, so EF0 sâˆ— (Di ; cÌ„)2 â‰¤ Âµ2 . By
h
i
EF0 sâˆ— (Di ; cÌ„)2 =

Assumption 4, however, there exists z âˆˆ Z with
i
h
EF0 (sz (Di ) âˆ’ sâˆ— (Di ; cÌ„))2 = 0,
h
i
and thus z yields cÌ„z = cÌ„, Î³Ì„z = 0, and EF0 sz (Di )2 â‰¤ Âµ2 as desired.
Proof of Proposition
in the proof

 3 As shown 
 of Lemma 1, under Assumption 3 the log likeli1
1
1
1
n
n
hood ratio log dFh,z âˆšn , âˆšn /dFh,z âˆšn , âˆšn
converges under S (0, 0) to a normal distribution
with mean equal to âˆ’ 21 times its variance. Le Camâ€™s First Lemma thus implies that the distribution of the data under S (h, z) is mutually contiguous with that under S (0, 0). Hence, to establish
convergence in probability under S (h, z), it suffices to establish convergence in probability under
Ë† under S (0, 0) is implied by Assumption 5, the Continuous Mapping TheS (0, 0) . Consistency of âˆ†
orem (see e.g. Theorem 2.3 of van der Vaart 1998), and the maintained assumptions that Ïƒc2 > 0
and Î£Î³Î³ has full rank.

References
Alatas, Vivi, Abhijit Banerjee, Rema Hanna, Benjamin A. Olken, Ririn Purnamasari, and Matthew
Wai-Poi. 2016. Self-targeting: Evidence from a field experiment in Indonesia. Journal of
Political Economy 124(2): 371-427.
Andrews, Isaiah, Matthew Gentzkow, and Jesse M. Shapiro. 2017. Measuring the sensitivity of parameter estimates to estimation moments. Quarterly Journal of Economics 132(4): 1553â€“1592.
Andrews, Isaiah, Matthew Gentzkow, and Jesse M. Shapiro. 2020. Transparency in structural
research. NBER Working Paper No. 26631.
Angrist, Joshua D. and JoÌˆrn-Steffen Pischke. 2010. The credibility revolution in empirical economics: How better research design is taking the con out of econometrics.â€ Journal of Economic Perspectives 24(2): 3-30.
32

Armstrong, Timothy and Michal KolesaÌr. 2019. Sensitivity analysis using approximate moment
condition models. Cowles Foundation Discussion Paper No. 2158R.
SSRN: https://ssrn.com/abstract=3337748.
Attanasio, Orazio P., Costas Meghir, and Ana Santiago. 2012a. Education choices in Mexico:
Using a structural model and a randomized experiment to evaluate PROGRESA. Review of
Economic Studies 79(1): 37-66.
Attanasio, Orazio P., Costas Meghir, and Ana Santiago. 2012b. Supplementary data for Education
Choices in Mexico: Using a Structural Model and a Randomized Experiment to Evaluate PROGRESA. Accessed at <https://academic.oup.com/restud/article/79/1/37/1562110#supple mentarydata> in October 2017.
Berkowitz, Daniel, Mehmet Caner, and Ying Fang. 2008. Are â€œnearly exogenous instrumentsâ€
reliable? Economic Letters 101(1): 20â€“23.
Bonhomme, SteÌphane and Martin Weidner. 2018. Minimizing sensitivity to model misspecification.
arXiv:1807.02161v2 [econ.EM].
Chen, Xiaohong and Andres Santos. 2018. Overidentification in regular models. Econometrica
86(5): 1771-1817.
Conley, Timothy G., Christian B. Hansen, and Peter E. Rossi. 2012. Plausibly exogenous. Review
of Economics and Statistics 94(1): 260â€“272.
Cressie, Noel and Timothy RC Read. 1984. Multinomial goodness-of-fit tests. Journal of the Royal
Statistical Society Series B 46(3): 440â€“464.
Dridi, Ramdan, Alain Guay, and Eric Renault. 2007. Indirect inference and calibration of dynamic
stochastic general equilibrium models. Journal of Econometrics 136(2): 397-430.
Duflo, Esther, Rema Hanna, and Stephen P. Ryan. 2012. Incentives work: Getting teachers to
come to school. American Economic Review 102(4): 1241â€“1278.
Einav, Liran, Amy Finkelstein, Stephen P. Ryan, Paul Schrimpf, and Mark R. Cullen. 2013.
Selection on moral hazard in health insurance. American Economic Review 103(1): 178â€“219.
Fetter, Daniel K. and Lee M. Lockwood. 2018. Government old-age support and labor supply:
Evidence from the Old Age Assistance Program. American Economic Review 108(8): 21742211.
Gentzkow, Matthew. 2007a. Valuing new goods in a model with complementarity: Online newspapers. American Economic Review 97(3): 713-744.
Gentzkow, Matthew. 2007b. Supplementary data for Valuing new goods in a model with complementarity: Online newspapers. Accessed at <https://www.openicpsr.org/openicpsr/project/
116273/version/V1/view> in May 2020.
Gentzkow, Matthew and Jesse M. Shapiro. 2015. Measuring the sensitivity of parameter estimates
to sample statistics. NBER Working Paper No. 20673.
Gentzkow, Matthew, Jesse M. Shapiro, and Michael Sinkinson. 2014. Competition and ideological diversity: Historical evidence from US newspapers. American Economic Review 104(10):
3073â€“3114.

33

Guggenberger, Patrik. 2012. On the asymptotic size distortion of tests when instruments locally
violate the exogeneity assumption. Econometric Theory 28(2): 387â€“421.
Hall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer Series in Statistics. New
York: Springer-Verlag.
Hansen, Bruce E. 2016. Efficient shrinkage in parametric models. Journal of Econometrics 190(1):
115-132.
Hansen, Lars P. and Thomas J. Sargent. 2001. Acknowledging misspecification in macroeconomic
theory. Review of Economic Dynamics 4(3): 519-35.
Hansen, Lars P. and Thomas J. Sargent. 2005. Robust estimation and control under commitment.
Journal of Economic Theory 124(2): 258-301.
Hansen, Lars P. and Thomas J. Sargent. 2016. Sets of models and prices of uncertainty. NBER
Working Paper No. 22000.
Hansen, Lars P., Thomas J. Sargent, Gauhar Turmuhambetova, and Noah Williams. 2006. Robust
control and model misspecification. Journal of Economic Theory 128(1): 45-90.
Heckman, James J. 2010. Building bridges between structural and program evaluation approaches
to evaluating policy. Journal of Economic Literature 48(2): 356-98.
Hendren, Nathaniel. 2013a. Private information and insurance rejections. Econometrica 81(5):
1713â€“1762.
Hendren, Nathaniel. 2013b. Supplementary data for Private information and insurance rejections.
Accessed at <https://www.econometricsociety.org/content/supplement-private-information-a
nd-insurance-rejections-0> in March 2014.
Huber, Peter J. and Elvezio M. Ronchetti. 2009. Robust Statistics (2nd ed). Hoboken, NJ: John
Wiley & Sons.
Ichimura, Hidehiko and Whitney K. Newey. 2015. The influence function of semiparametric estimators. arXiv:1508.01378v1 [stat.ME].
Keane, Michael P. 2010. Structural vs. atheoretic approaches to econometrics. Journal of Econometrics 156(1): 3â€“20.
Kitamura, Yuichi, Taisuke Otsu, and Kirill Evdokimov. 2013. Robustness, infinitesimal neighborhoods, and moment restrictions. Econometrica 81(3): 1185-1201.
Lehmann, Erich L. and Joseph P. Romano. 2005. Testing Statistical Hypotheses. New York:
Springer.
Morten, Melanie. 2019. Temporary migration and endogenous risk sharing in village India. Journal
of Political Economy 127(1): 1-46.
Matzkin, Rosa L. 2007. Nonparametric identification. In James J. Heckman and Edward E. Leamer,
Eds., Handbook of Econometrics, Vol. 6(B), Ch. 73: 5307-5368. Amsterdam: North-Holland.
Mukhin, Yaroslav. 2018. Sensitivity of regular estimators. arXiv:1805.08883v1 [econ.EM].
Nakamura, Emi and JoÌn Steinsson. 2018. Identification in macroeconomics. Journal of Economic
Perspectives 32(3): 59-86.

34

Newey, Whitney K. 1985. Generalized method of moments specification testing. Journal of Econometrics 29(3): 229â€“256.
Newey, Whitney K. 1994. The asymptotic variance of semiparametric estimators. Econometrica
62(6): 1349-1382.
Newey, Whitney K. and Daniel McFadden. 1994. Large sample estimation and hypothesis testing.
In Robert F. Engle and Daniel L. McFadden, Eds., Handbook of Econometrics, Vol. 4, Ch. 36:
2111-2245. Amsterdam: North-Holland.
Pakes, Ariel. 2014. Behavioral and descriptive forms of choice models. International Economic
Review 55(3): 603-624.
Rieder, Helmut. 1994. Robust Asymptotic Statistics. New York: Springer.
Spenkuch, JoÌˆrg L., B. Pablo Montagnes, and Daniel B. Magleby. 2018. Backward induction in the
wild? Evidence from sequential voting in the US Senate. American Economics Review 108(7):
1971-2013.
van der Vaart, Aad W. 1998. Asymptotic Statistics. Cambridge, UK: Cambridge University Press.

35

Online Appendix for

On the Informativeness of Descriptive Statistics
for Structural Estimates
Isaiah Andrews, Harvard University and NBER
Matthew Gentzkow, Stanford University and NBER
Jesse M. Shapiro, Brown University and NBER

May 2020

A

Sensitivity and Informativeness

Proposition 2 considers the effect of limiting attention to forms of misspecification that do not
affect Î³Ì‚. In some cases, however, researchers may be interested in forms of misspecification with a
non-zero, but known, effect on Î³Ì‚. In such cases, our assumptions again imply a relationship between
the biases in cÌ‚ and Î³Ì‚.
This relationship depends on the sensitivity of cÌ‚ to Î³Ì‚. This is the natural extension of the
sensitivity measure proposed in Andrews et al. (2017) to the current setting.
Definition. The sensitivity of cÌ‚ with respect to Î³Ì‚ is
Î› = Î£cÎ³ Î£âˆ’1
Î³Î³ .
To build intuition, note that sensitivity characterizes the relationship between cÌ‚ and Î³Ì‚ in
the asymptotic distribution under the base model. If we assume, as in Section 3, that cÌ‚ and
Î³Ì‚ are normally distributed in finite samples, then Î› is simply the vector of coefficients from the
population regression of cÌ‚ on Î³Ì‚. In this case, element Î›j of Î› is the effect of changing the realization
of a particular Î³Ì‚j on the expected value of cÌ‚, holding the other elements of Î³Ì‚ constant.
Andrews et al. (2017) show that for cÌ‚ = c (Î·Ì‚), Î·Ì‚ a minimum distance estimator based on
moments gÌ‚ (Î·), and Î³Ì‚ = gÌ‚ (Î·0 ) the estimation moments evaluated at the true parameter value,
under regularity conditions sensitivity translates the effect of misspecification on Î³Ì‚ to the effect on
cÌ‚, in the sense that
cÌ„ (S (h, z)) âˆ’ cÌ„ (S (h, 0)) = Î› (Î³Ì„ (S (h, z)) âˆ’ Î³Ì„ (S (h, 0))) .
Our next proposition extends this result.

36

Proposition 4. Suppose that Assumptions 1-4 hold, and let
o
 
n
S RN (c? , Î³Ì„) = âˆªSâˆˆS 0 (c? ) SÌƒ âˆˆ N (S) : Î³Ì„ SÌƒ âˆ’ Î³Ì„ (S) = Î³Ì„ .
RN (Â·, Î³Ì„) is
Provided Âµ (Î³Ì„)2 = Âµ2 âˆ’ Î³Ì„ 0 Î£âˆ’1
Î³Î³ Î³Ì„ â‰¥ 0, the set of possible biases under S âˆˆ S



h
i
âˆš
âˆš
cÌ„ (S) âˆ’ c? : S âˆˆ S RN (c? , Î³Ì„) = Î›Î³Ì„ âˆ’ Âµ (Î³Ì„) Ïƒc 1 âˆ’ âˆ†, Î›Î³Ì„ + Âµ (Î³Ì„) Ïƒc 1 âˆ’ âˆ† ,

for any c? such that S RN (c? , Î³Ì„) is nonempty.
Proposition 4 extends the results of Andrews et al. (2017) to the case where Î³Ì‚ need not be
a vector of estimation moments, and thus we may have âˆ† < 1. It likewise extends Proposition 2.
The resulting set of first-order asymptotic biases for cÌ‚ is centered at Î›Î³Ì„ with width proportional to
âˆš
1 âˆ’ âˆ†.
q Unlike in Proposition 2, the degree of misspecification now enters the width through
q Âµ (Î³Ì„) =
âˆ’1
2
0
Âµ âˆ’ Î³Ì„ Î£Î³Î³ Î³Ì„. Intuitively, Âµ (Î³Ì„) measures the degree of excess misspecification beyond Î³Ì„ 0 Î£âˆ’1
Î³Î³ Î³Ì„,
 
which is the minimum necessary to allow Î³Ì„ SÌƒ âˆ’ Î³Ì„ (S) = Î³Ì„. If the degree of excess misspecification
is small then the first-order asymptotic bias of cÌ‚ is close to Î›Î³Ì„, while if the degree of excess
misspecification is large then a wider range of biases is possible.
Proof of Proposition 4 The proof is similar to that for Proposition 2 in the main text. By
Lemma 1 we again have
c? (h) = EF0 [Ï†c (Di ) sh (Di )] .
Note, next, that by the definition of S RN (c? , Î³Ì„) and Lemma 1, for any S âˆˆ S RN (c? , Î³Ì„) there exist
(h, z) âˆˆ H Ã— Z with S = S (h, z), c? (h) = c? , and
EF0 [Ï†Î³ (Di ) (sh (Di ) + sz (Di ))] âˆ’ EF0 [Ï†Î³ (Di ) sh (Di )] = EF0 [Ï†Î³ (Di ) sz (Di )] = Î³Ì„.
Thus, writing Î³Ì„z = EF0 [Ï†Î³ (Di ) sz (Di )] and cÌ„z = EF0 [Ï†c (Di ) sz (Di )] for brevity, our task reduces
to showing that
i
n
h
i
o h
âˆš
âˆš
cÌ„z : z âˆˆ Z, Î³Ì„z = Î³Ì„, EF0 sz (Di )2 â‰¤ Âµ2 = Î›Î³Ì„ âˆ’ Âµ (Î³Ì„) Ïƒc 1 âˆ’ âˆ†, Î›Î³Ì„ + Âµ (Î³Ì„) Ïƒc 1 âˆ’ âˆ† .
Define s (Di ; Î³Ì„) = Ï†Î³ (Di )0 Î£âˆ’1
Î³Î³ Î³, and
Îµz (Di ) = sz (Di ) âˆ’ s (Di ; Î³Ì„z ) .
Note that EF0 [Ï†Î³ (Di ) Îµz (Di )] = 0 and EF0 [s (Di ; Î³Ì„z ) Îµz (Di )] = 0 by construction. We can write


cz = EF0 [Ï†c (Di ) sz (Di )] = EF0 Ï†c (Di ) Ï†Î³ (Di )0 Î£âˆ’1
Î³Î³ Î³Ì„z + EF0 [Ï†c (Di ) Îµz (Di )]

37

= Î›Î³Ì„z + EF0 [Ï†c (Di ) Îµz (Di )] .
Next, define
Ï†Ìƒc (Di ) = Ï†c (Di ) âˆ’ Î›Ï†Î³ (Di )
and note that
h
i
EF0 [Ï†c (Di ) Îµz (Di )] = EF0 Ï†Ìƒc (Di ) Îµz (Di ) .
The Cauchy-Schwarz inequality then implies that
EF0

=

q

h

r
i
h
ir
h
i
2
Ï†Ìƒc (Di ) Îµz (Di ) â‰¤ EF0 Ï†Ìƒc (Di )
EF0 Îµz (Di )2
r

Ïƒc2 âˆ’ Î›Î£Î³Î³ Î›0

h

EF0 Îµz (Di )

2

i

âˆš

r

= Ïƒc 1 âˆ’ âˆ†

h
i
EF0 sz (Di )2 âˆ’ Î³Ì„z0 Î£âˆ’1
Î³Î³ Î³Ì„z .

h
i
Combining these results we see that for z such that Î³Ì„z = Î³Ì„ and EF0 sz (Di )2 â‰¤ Âµ2 ,


âˆš

cÌ„z âˆˆ Î›Î³Ì„ âˆ’ Ïƒc 1 âˆ’ âˆ†

q

Âµ2 âˆ’

Î³Ì„ 0 Î£âˆ’1
Î³Î³ Î³Ì„, Î›Î³Ì„

âˆš

+ Ïƒc 1 âˆ’ âˆ†

q

Âµ2 âˆ’

Î³Ì„ 0 Î£âˆ’1
Î³Î³ Î³Ì„


,

which are the bounds stated in the proposition. In particular,
h
i
0 â‰¤ EF0 Îµz (Di )2 â‰¤ Âµ2 âˆ’ Î³Ì„z0 Î£âˆ’1
Î³Î³ Î³Ì„z ,
h
i
2
2 in order that E
so if Î³Ì„z = Î³Ì„ we must have Î³Ì„ 0 Î£âˆ’1
Î³Ì„
â‰¤
Âµ
s
(D
)
â‰¤ Âµ2 . Hence, if Âµ2 âˆ’ Î³Ì„ 0 Î£âˆ’1
z
i
F0
Î³Î³
Î³Î³ Î³Ì„ < 0
h
i
there exists no z with Î³Ì„z = Î³Ì„ and EF0 sz (Di )2 â‰¤ Âµ2 .
To complete the proof it remains to show that these bounds are tight, so that for any (cÌ„, Î³Ì„, Âµ)
with

(16)

âˆš

cÌ„ âˆˆ Î›Î³Ì„ âˆ’ Ïƒc 1 âˆ’ âˆ†

q

Âµ2

âˆ’

Î³Ì„ 0 Î£âˆ’1
Î³Î³ Î³Ì„, Î›Î³Ì„

+ Ïƒc

âˆš


q
âˆ’1
2
0
1 âˆ’ âˆ† Âµ âˆ’ Î³Ì„ Î£Î³Î³ Î³Ì„

h
i
there exists z âˆˆ Z with cÌ„z = cÌ„, Î³Ì„z = Î³Ì„, and EF0 sz (Di )2 â‰¤ Âµ2 . If âˆ† < 1, define
sâˆ— (Di ; cÌ„, Î³Ì„) = s (Di ; Î³Ì„) + Ï†Ìƒc (Di )

cÌ„ âˆ’ Î›Î³Ì„
.
Ïƒc2 (1 âˆ’ âˆ†)

Note that
EF0 [Ï†Î³ (Di ) sâˆ— (Di ; cÌ„, Î³Ì„)] = Î³Ì„
while
h
i
EF0 [Ï†c (Di ) sâˆ— (Di ; cÌ„, Î³Ì„)] = Î›Î³Ì„ + EF0 Ï†Ìƒc (Di )2

38

cÌ„ âˆ’ Î›Î³Ì„
= cÌ„.
âˆ’ âˆ†)

Ïƒc2 (1

Moreover,
h
i
h
i
h
i (cÌ„ âˆ’ Î›Î³Ì„)2
EF0 sâˆ— (Di ; cÌ„, Î³Ì„)2 = EF0 s (Di ; Î³Ì„)2 + EF0 Ï†Ìƒc (Di )2
Ïƒc4 (1 âˆ’ âˆ†)2
= Î³Ì„ 0 Î£âˆ’1
Î³Î³ Î³Ì„ +

(cÌ„ âˆ’ Î›Î³Ì„)2
.
Ïƒc2 (1 âˆ’ âˆ†)

However, by (16) we know that
âˆš

|cÌ„ âˆ’ Î›Î³Ì„| â‰¤ Ïƒc 1 âˆ’ âˆ†
and thus that

q

Âµ2 âˆ’ Î³Ì„ 0 Î£âˆ’1
Î³Î³ Î³Ì„


(cÌ„ âˆ’ Î›Î³Ì„)2
2
0 âˆ’1
â‰¤
Âµ
âˆ’
Î³Ì„
Î£
Î³Ì„
Î³Î³
Ïƒc2 (1 âˆ’ âˆ†)

h
i
so EF0 sâˆ— (Di ; cÌ„, Î³Ì„)2 â‰¤ Âµ2 . By Assumption 4, however, there exists z âˆˆ Z with
h
i
EF0 (sz (Di ) âˆ’ sâˆ— (Di ; cÌ„, Î³Ì„))2 = 0,
h
i
and thus z yields cÌ„z = cÌ„, Î³Ì„z = Î³Ì„, and EF0 sz (Di )2 â‰¤ Âµ2 as desired. In cases with âˆ† = 1, on the
other hand, we can use sâˆ— (Di ; cÌ„, Î³Ì„) = s (Di ; Î³Ì„) .

B

Asymptotic Divergence

This section studies the asymptotic behavior of the divergence

(17)

rh,z

1
1
âˆš ,âˆš
n n



 ï£¶ï£¹

fh,z Di ; âˆš1n , âˆš1n

 ï£¸ï£»
= EFh,z (th ,0) ï£°Ïˆ ï£­
1
âˆš
fh,z Di ; n , 0
ï£® ï£«

as n â†’ âˆž, where as in the main text we assume that Ïˆ (1) = 0 and Ïˆ 00 (1) = 2. To derive our
results we impose the following assumption.
Assumption 6. For t = (th , tz ) âˆˆ R2 and fh,z (Di ; t) = fh,z (Di ; th , tz ) , fh,z (Di ; t) is twice continuously differentiable in t at 0, and there exists an open neighborhood B of zero such that
"
EF0 sup
tâˆˆB

fh,z (Di ; th , 0) 0
âˆ‚2
âˆ‚
fh,z (Di ; t) +
fh,z (Di ; t) +
Ïˆ
2
âˆ‚tz
âˆ‚tz
fh,z (Di ; 0)
ï£®
EF0 ï£° sup
(t,tÌƒ)âˆˆB2

and

fh,z (Di ; th , 0) 0
Ïˆ
fh,z (Di ; 0)

ï£®
EF0 ï£° sup
(t,tÌƒ)âˆˆB2

fh,z (Di ; th , 0) 00
Ïˆ
fh,z (Di ; 0)

!
fh,z Di ; tÌƒ
fh,z (Di ; t)
!
fh,z Di ; tÌƒ
fh,z (Di ; t)
39



fh,z (Di ; t)
fh,z (Di ; t)

âˆ‚2
f
âˆ‚t2z h,z

Di ; tÌƒ

fh,z (Di ; t)



âˆ‚
âˆ‚tz fh,z

fh,z (Di ; t)

 ï£¹
ï£»,

 !2 ï£¹
Di ; tÌƒ
ï£»
fh,z (Di ; t)

âˆ‚
âˆ‚tz fh,z

(Di ; t)

!#
,

are finite.
Under this assumption, we obtain the asymptotic approximation to divergence discussed in
the main text.
Proposition 5. Under Assumptions 3 and 6,

lim n Â· rh,z

nâ†’âˆž

1
1
âˆš ,âˆš
n n

Proof of Proposition 5 Recall that rh,z





âˆš1 , âˆš1
n
n

h
i
= EF0 sz (Di )2 .



can be written as in (17). Assumption 6 and

Leibnizâ€™s rule implies for n sufficiently large we can exchange integration and differentiation twice,
so by Taylorâ€™s Theorem with a mean-value residual,21

n Â· rh,z

1
2



âˆš1 , 0
n

, tÌƒn =



âˆš1 , tÌƒz,n
n



n Â· rh,z
âˆš

ï£®
1 fh,z (Di ;tn )
2 fh,z (Di ;0)

Ïˆ0



ï£¹
ï£º
ï£º
ï£º
ï£»

i
h
and tÌƒz,n âˆˆ 0, âˆš1n . Thus, since Ïˆ (1) = 0 by assumption,


ï£¯
EF0 ï£¯
ï£°

=

fh,z (Di ;tn )
fh,z (Di ;0)

ï£¯
ï£¯
n Â· EF0 ï£¯
ï£°




 


 âˆ‚
fh,z (Di ;tn )
0 fh,z (Di ;tn ) âˆ‚tz fh,z (Di ;tn ) âˆš1 +
+
Ïˆ
Ïˆ fh,z
(Di ;tn )
fh,z (Di ;tn )
fh,z (Di ;tn )
n

 âˆ‚2

 âˆ‚
2 ! !
f
D
;
tÌƒ
2 fh,z (Di ;tÌƒn )
f
D
;
tÌƒ
f
D
;
tÌƒ
)
(
n
i
)
(
)
(
h,z
n
n
i
i
h,z
h,z
âˆ‚t
âˆ‚tz
1
00
z
Ïˆ 0 fh,z (Di ;tn )
n
fh,z (Di ;tn ) + Ïˆ
fh,z (Di ;tn )
fh,z (Di ;tn )

ï£®

for tn =

1
1
âˆš ,âˆš
n n

fh,z (Di ;tÌƒn )
fh,z (Di ;tn )



1
1
âˆš ,âˆš
n n

nÏˆ 0 (1)

âˆ‚2
fh,z
âˆ‚t2
z


=

âˆ‚
âˆ‚tz

ï£¹

fh,z (Di ;tn )
fh,z (Di ;0) +

(Di ;tÌƒn )

fh,z (Di ;tn )

+ Ïˆ 00



fh,z (Di ;tÌƒn )
fh,z (Di ;tn )



âˆ‚
âˆ‚tz

fh,z (Di ;tÌƒn )
fh,z (Di ;tn )

2 ! ï£º
ï£º.
ï£»

Assumption 6 and Leibnizâ€™s rule imply that for n sufficiently large,
"
EF0

âˆ‚
âˆ‚tz fh,z

(Di ; tn )

fh,z (Di ; 0)

#

Z
=





Z
âˆ‚
1
1
âˆ‚
fh,z d; âˆš , 0 dÎ½ (d) =
fh,z d; âˆš , 0 dÎ½ (d) = 0.
âˆ‚tz
âˆ‚tz
n
n

Hence, we see that

n Â· rh,z

21

1
1
âˆš ,âˆš
n n


=

Specifically, note that for q(th , tz ) = rh,z (th , tz ) we can write
q(th , tz ) = q(th , 0) +

âˆ‚
1 âˆ‚2
q(th , 0)tz +
q(th , tÌƒz )t2z
âˆ‚tz
2 âˆ‚t2z

with tÌƒz âˆˆ [0, tz ].

40

ï£«
f
(D
;
t
)
1
i n ï£­ 0
h,z
EF0 ï£°
Ïˆ
2 fh,z (Di ; 0)
ï£®

!
fh,z Di ; tÌƒn
fh,z (Di ; tn )

âˆ‚2
f
âˆ‚t2z h,z

Di ; tÌƒn



!
fh,z Di ; tÌƒn
fh,z (Di ; tn )

+ Ïˆ 00

fh,z (Di ; tn )

âˆ‚
âˆ‚tz fh,z

Di ; tÌƒn

fh,z (Di ; tn )

 !2 ï£¶ ï£¹
ï£¸ï£» .

Since Ïˆ 00 (1) = 2, the Dominated Convergence Theorem and Assumption 6 imply that
ï£«

ï£®
EF0 ï£°

1 fh,z (Di ; tn ) ï£­ 0
Ïˆ
2 fh,z (Di ; 0)

!
fh,z Di ; tÌƒn
fh,z (Di ; tn )
ï£®

âˆ‚2
f
âˆ‚t2z h,z

Di ; tÌƒn



fh,z (Di ; tn )

âˆ‚2
f
âˆ‚t2z h,z

(Di ; 0)

1
â†’ EF0 ï£°Ïˆ 0 (1)
2
fh,z (Di ; 0)
ï£®

+ Ïˆ 00

âˆ‚
âˆ‚tz fh,z

+ Ïˆ 00 (1)

âˆ‚2
f
âˆ‚t2z h,z

!
fh,z Di ; tÌƒn
fh,z (Di ; tn )

(Di ; 0)

1
= EF0 ï£° Ïˆ 0 (1)
2
fh,z (Di ; 0)

(Di ; 0)

fh,z (Di ; 0)

âˆ‚
âˆ‚tz fh,z

Di ; tÌƒn

fh,z (Di ; tn )

 !2 ï£¶ ï£¹
ï£¸ï£»

!2 ï£¹
ï£»

ï£¹
+ sz (Di )2 ï£» .

However, Assumption 6 and Leibnizâ€™s rule imply that
ï£®
EF0

âˆ‚2
2 fh,z
ï£° âˆ‚tz

(Di ; 0)

fh,z (Di ; 0)

ï£¹
Z
ï£»=

âˆ‚2
âˆ‚2
f
(d;
0)
dÎ½
(d)
=
h,z
âˆ‚t2z
âˆ‚t2z

so


lim n Â· rh,z

nâ†’âˆž

1
1
âˆš ,âˆš
n n



Z
fh,z (d; 0) dÎ½ (d) = 0,

h
i
= EF0 sz (Di )2 ,

as we wanted to show.

C

Asymptotic Distinguishability

In Section 4.3 of the paper, and Section B above, we discuss that the neighborhoods studied in our
localasymptotic
analysis
to bounds on the asymptotic Cressie-Read divergence between

 correspond

1
1
1
Fh,z âˆšn , 0 and Fh,z âˆšn , âˆšn . In the section, we show that they also correspond to bounds on
the asymptotic power of tests to distinguish S (h, z) and S (h, 0).
Proposition 6. Under Assumption 3, the most powerful level Î± test of the null hypothesis
H0 : (D1 , ..., Dn ) âˆ¼

Ã—ni=1 Fh,z




1
âˆš ,0
n

against
H1 : (D1 , ..., Dn ) âˆ¼

has power converging to 1âˆ’FN (0,1) vÎ± âˆ’

Ã—ni=1 Fh,z



1
1
âˆš ,âˆš
n n



r

h
i
2
EF0 sz (Di )
for vÎ± the 1âˆ’Î± quantile of the standard

normal distribution.
The proof of Proposition 6 shows that the most powerful test corresponds asymptotically to a
41

r
z-test, where the z-statistic has mean

h
i
EF0 sz (Di )2 under H1 .

Proof of Proposition 6 By the Neyman-Pearson Lemma (see Theorem 3.2.1 in
and
 Lehmann

1
n
Romano 2005), the most powerful level-Î± test of H0 : (D1 , ..., Dn ) âˆ¼ Ã—i=1 Fh,z âˆšn , 0 against


1 âˆš1
n
âˆš
, n rejects when the log likelihood ratio
H1 : (D1 , ..., Dn ) âˆ¼ Ã—i=1 Fh,z
n





1
1
1
n
n
log dFh,z âˆš , âˆš
/dFh,z âˆš , 0
n n
n
exceeds a critical value vÎ±,n chosen to ensure rejection probability Î± under H0 (and may randomize
when the log likelihood ratio exactly equals the critical value). Here we again abbreviate Ã—ni=1 F =
F n.
By Assumption 3 and the quadratic expansion of the likelihood in the proof of Lemma 1,
however, we see that under S (0, 0) , for g (Di ; h, z) = sh (Di ) + sz (Di ) ,

log

for



âˆš1 ,0
n
dF0n

n
dFh,z

ï£«

!

log

n
dFh,z



âˆš1 , âˆš1
n
n
dF0n

!

!0

i ï£¶ ï£¶
h
âˆ’ 21 EF0 g (Di ; h, 0)2
h
i ï£¸ , Î£Ìƒï£¸
â†’d N ï£­ï£­
âˆ’ 12 EF0 g (Di ; h, z)2
ï£«ï£«

h
i
EF0 g (Di ; h, 0)2

ï£¶
EF0 [g (Di ; h, 0) g (Di ; h, z)]
h
i
ï£¸.
Î£Ìƒ = ï£­
EF0 [g (Di ; h, 0) g (Di ; h, z)]
EF0 g (Di ; h, z)2

Le Camâ€™s third lemma thus implies that under S (h, 0) ,

log



âˆš1 ,0
n
dF0n

n
dFh,z

ï£«ï£«
N ï£­ï£­

!

log

1
2 EF0i
2

h
âˆ’ 12 EF0 g (Di ; h, z)

h

n
dFh,z



âˆš1 , âˆš1
n
n
dF0n

g (Di ; h, 0)2

!

!0
â†’d

i

ï£¶

+ EF0 [g (Di ; h, 0) g (Di ; h, z)]

ï£¶

ï£¸ , Î£Ìƒï£¸ ,

while under S (h, z) ,

log



âˆš1 ,0
n
dF0n

n
dFh,z

!

log

ï£«ï£«

n
dFh,z



âˆš1 , âˆš1
n
n
dF0n

!

!0
â†’d

h
i
ï£¶ ï£¶
âˆ’ 12 EF0 g (Di ; h, 0)2 + EF0 [g (Di ; h, 0) g (Di ; h, z)]
h
i
ï£¸ , Î£Ìƒï£¸ .
N ï£­ï£­
2
1
E
g
(D
;
h,
z)
i
2 F0

42

Since
ï£«
log ï£­

n
dFh,z



n
dFh,z

âˆš1 , âˆš1
n
n



âˆš1 , 0
n

ï£¶

ï£«

 ï£¸ = log ï£­

n
dFh,z



âˆš1 , âˆš1
n
n
n
dF0

ï£¶

ï£«

ï£¸ âˆ’ log ï£­



âˆš1 , 0
n
n
dF0

n
dFh,z

ï£¶
ï£¸,

and sz (d) = sh (d) = 0 when h = z = 0, g (Di ; h, 0) âˆ’ g (Di ; h, z) = âˆ’g (Di ; 0, z) we see that


ï£¶

ï£± 
h
i
h
i
ï£²N âˆ’ 1 EF g (Di ; 0, z)2 , EF g (Di ; 0, z)2
0
0
i
h
i
 2 h

 ï£¸ â†’d
log ï£­
1
ï£³
n
N 12 EF0 g (Di ; 0, z)2 , EF0 g (Di ; 0, z)2
dFh,z âˆšn , 0
ï£«

n
dFh,z

âˆš1 , âˆš1
n
n

Under S (h, 0)
Under S (h, z) .

h
i
h
i
Hence, since EF0 g (Di ; 0, z)2 = EF0 sz (Di )2 and vÎ±,n corresponds to the 1 âˆ’ Î± quantile of the
log likelihood ratio under the null, we have that
log



!

âˆš1 , âˆš1
 n n
n
âˆš1 ,0
dFh,z
n

n
dFh,z

âˆ’ vÎ±,n

r
h
i
EF0 sz (Di )2

ï£±
ï£´
ï£²N (âˆ’vÎ± , 1)
under S (h, 0)
r

h
i
â†’d
ï£´
EF0 sz (Di )2 âˆ’ vÎ± , 1
under S (h, z)
ï£³N

for vÎ± the 1 âˆ’ Î± quantile of a standard normal distribution, from which the result follows.

D

Non-Local Misspecification

This section develops our informativeness measure based on probability limits, rather than firstorder asymptotic bias.
Under Assumptions 1, 3, and 4, provided the estimators cÌ‚ and Î³Ì‚ are regular in the sense
discussed in Newey (1994), Theorem 2.1 of Newey (1994) implies that the probability limits cÌƒ (Â·)
and Î³ (Â·) are asymptotically linear functionals, in the sense that
(18)

limtz â†’0 kcÌƒ (F0,z (0, tz )) âˆ’ c (Î·0 ) âˆ’ tz EF0 [sz (Di ) Ï†c (Di )]k /tz = 0 for all z âˆˆ Z
limtz â†’0 kÎ³ (F0,z (0, tz )) âˆ’ Î³ (F0 ) âˆ’ tz EF0 [sz (Di ) Ï†Î³ (Di )]k /tz = 0 for all z âˆˆ Z.

Assumption 2 would be implied by an assumption that (cÌ‚, Î³Ì‚) are regular in the base model, so
the assumption of regularity of (cÌ‚, Î³Ì‚) in the nesting model can be understood as a strengthening
of Assumption 2. See Newey (1994) and Rieder (1994) for discussion. Since (18) only restricts
Ëœ (rÌ„) as defined in the main text let us instead
behavior as tz â†’ 0 for fixed z, rather than studying âˆ†
consider an analogue defined
i collections of paths. Specifically, continuing to define
h  using finite
fh,z (Di ;th ,tz )
rh,z (th , tz ) = EFh,z (th ,0) Ïˆ fh,z (Di ;th ,0) , for each z âˆˆ Z let
tÌ„ (z, Âµ) = inf {tz âˆˆ R+ : r0,z (0, tz ) â‰¥ Âµ}
denote the largest value of t such that r0,z (0, tz ) < Âµ for all tz < tÌ„ (z, Âµ) . Let Z+ âŠ‚ Z denote the

43

h
i
set of z âˆˆ Z with EF0 sz (Di )2 > 0.
Let Q âŠ‚ Z+ denote a finite subset of Z+ , and let Q denote the set of all such finite subsets.
Finally, let
bÌƒN (Âµ, Q) = sup {|cÌƒ (F0,z (0, tz )) âˆ’ c (Î·0 )| : z âˆˆ Q, tz < tÌ„ (z, Âµ)}
denote the analogue of bÌƒN (Âµ) based on the finite set of paths Q, and for Îµ > 0 let
âˆš
bÌƒRN,Îµ (Âµ, Q) = sup {|cÌƒ (F0,z (0, tz )) âˆ’ c (Î·0 )| : z âˆˆ Q, tz < tÌ„ (z, Âµ) , kÎ³ (F0,z (0, tz )) âˆ’ Î³ (F0 )k â‰¤ Îµ Âµ}
denote the analogue of bÌƒRN (Âµ, Q) based on Q which allows the probability limit of Î³Ì‚ to change by
âˆš
at most Îµ Âµ. Because bÌƒRN,0 (Âµ, Q) may equal 0 even for large Âµ due to the approximation error in
(18), we consider limits as Îµ â†“ 0 (i.e., as Îµ â†’ 0 from above). Based on these objects, we define the
Ëœ (Âµ) as
analogue of âˆ†
Ëœ (Âµ, Q) = sup inf lim bÌƒRN,Îµ (Âµ, Q1 ) ,
âˆ†
Q1 âˆˆQ Q2 âˆˆQ Îµâ†“0 bÌƒN (Âµ, Q2 )
provided the limit exists.
Proposition 7. Suppose Assumptions 1, 3, and 4 hold, that the estimators cÌ‚ and Î³Ì‚ are regular,
and that Assumption 6 holds for h = 0 and all z âˆˆ Z+ . For Ïˆ (Â·) twice continuously differentiable
and Ïˆ (1) = 0, Ïˆ 00 (1) = 2,
sup inf lim lim

bÌƒRN,Îµ (Âµ, Q1 )

Q1 âˆˆQ Q2 âˆˆQ Îµâ†“0 Âµâ†“0

bÌƒN (Âµ, Q2 )

=

âˆš

1 âˆ’ âˆ†.

It is important that we take the limit as Âµ â†“ 0 inside the limit as Îµ â†“ 0 and the sup and inf,
since this order of limits allows us to take advantage of the approximation result (18).
Proof of Proposition 7 Note, first, that our Assumptions 1, 3, and 4 imply the conditions of
Theorem 2.1 of Newey (1994) other than regularity of (cÌ‚, Î³Ì‚). Specifically, conditions (i) and (ii) of
Theorem 2.1 in Newey (1994) follow from our Assumptions 3 and 4. Condition (iii) is implied by
our Assumption 1. Regularity of (cÌ‚, Î³Ì‚) is assumed, so Theorem 2.1 of Newey (1994) implies (18).
Note, next, that for any z âˆˆ Z+ , the proof of Proposition 5 implies that
h
i
lim r0,z (0, tz ) /t2z = EF0 sz (Di )2 .

tz â†“0

1

h
iâˆ’
âˆš
2
. For all z âˆˆ Z+ , (18) implies that
Hence, as Âµ â†“ 0, tÌ„ (z, Âµ) / Âµ â†’ E sz (Di )2
limÂµâ†“0 suptz â‰¤tÌ„(z,Âµ) kcÌƒ (F0,z (0, tz )) âˆ’ c (Î·0 ) âˆ’ tz EF0 [sz (Di ) Ï†c (Di )]k /tz = 0
limÂµâ†“0 suptz â‰¤tÌ„(z,Âµ) kÎ³ (F0,z (0, tz )) âˆ’ Î³ (F0 ) âˆ’ tz EF0 [sz (Di ) Ï†Î³ (Di )]k /tz = 0,

44

and thus that



â†’

1
âˆš (cÌƒ (F0,z (0, tz )) âˆ’ c (Î·0 ) , Î³ (F0,z (0, tz )) âˆ’ Î³ (F0 )) : tz â‰¤ tÌ„ (z, Âµ)
Âµ



h
iâˆ’ 1 
2
2
tÌƒz (EF0 [sz (Di ) Ï†c (Di )] , EF0 [sz (Di ) Ï†Î³ (Di )]) : tÌƒz â‰¤ EF0 sz (Di )

in the Hausdorff sense as Âµ â†“ 0. Correspondingly, for any Q âˆˆ Q,



â†’


1
âˆš (cÌƒ (F0,z (0, tz )) âˆ’ c (Î·0 ) , Î³ (F0,z (0, tz )) âˆ’ Î³ (F0 )) : z âˆˆ Q, tz â‰¤ tÌ„ (z, Âµ)
Âµ

h
iâˆ’ 1 
2
.
tÌƒz (EF0 [sz (Di ) Ï†c (Di )] , EF0 [sz (Di ) Ï†Î³ (Di )]) : z âˆˆ Q, tÌƒz â‰¤ EF0 sz (Di )2

Hence, for any nonempty Q âˆˆ Q
ï£±
ï£´
ï£² |E

1
âˆš bÌƒN (Âµ, Q) â†’ max
ï£´
Âµ
ï£³

ï£¼
ï£´
ï£½
F0 [sz (Di ) Ï†c (Di )]|
: z âˆˆ Q as Âµ â†“0.
h
i1
ï£´
2
ï£¾
EF0 sz (Di )2

Matters are somewhat more delicate for bÌƒRN,Îµ (Âµ, Q) . Note, in particular, that for Îµ > 0, as
Âµ â†“ 0 we have
1
âˆš bÌƒRN,Îµ (Âµ, Q) â†’
Âµ


h

2

sup tÌƒz EF0 [sz (Di ) Ï†c (Di )] : z âˆˆ Q, tÌƒz â‰¤ EF0 sz (Di )

iâˆ’ 1
2


, tÌƒz kEF0 [sz (Di ) Ï†Î³ (Di )]k â‰¤ Îµ

h
iâˆ’ 1
2
= sup tÌƒz EF0 [sz (Di ) Ï†c (Di )] : z âˆˆ Q, tÌƒz â‰¤ min EF0 sz (Di )2
,




Îµ
kEF0 [sz (Di ) Ï†Î³ (Di )]k


,

where we define Îµ/0 = âˆž for Îµ > 0. Consequently,
1
âˆš bÌƒRN,Îµ (Âµ, Q) â†’
Âµ
h
iâˆ’ 1
2
sup tÌƒz |EF0 [sz (Di ) Ï†c (Di )]| : z âˆˆ Q, tÌƒz â‰¤ min EF0 sz (Di )2
,





Îµ
.
kEF0 [sz (Di ) Ï†Î³ (Di )]k
h
i
Note, however, that by the Cauchy-Schwarz inequality and EF0 sz (Di )2 < âˆž, EF0 [sz (Di ) Ï†c (Di )]
is finite for all z âˆˆ Z, so for any z with EF0 [sz (Di ) Ï†Î³ (Di )] 6= 0,
Îµ
EF [sz (Di ) Ï†c (Di )] â†’ 0
kEF0 [sz (Di ) Ï†Î³ (Di )]k 0

45

as Îµ â†“ 0. Hence, as Îµ â†“ 0,


h
iâˆ’ 1
2
,
sup tÌƒz |EF0 [sz (Di ) Ï†c (Di )]| : z âˆˆ Q, tÌƒz â‰¤ min EF0 sz (Di )2

â†’ max

ï£±
ï£´
ï£² |E
ï£´
ï£³

Îµ
kEF0 [sz (Di ) Ï†Î³ (Di )]k



ï£¼
ï£´
ï£½

[sz (Di ) Ï†c (Di )]|
: z âˆˆ Q0
h
i1
ï£´
2 2
ï£¾
EF0 sz (Di )

F0

for Q0 = {z âˆˆ Q : EF0 [sz (Di ) Ï†Î³ (Di )] = 0}, where we define this max to be zero if Q0 is empty.
This immediately implies that

h
i1
2 2
max |EF0 [sz (Di ) Ï†c (Di )]| /EF0 sz (Di )
: z âˆˆ Q1,0
bÌƒRN,Îµ (Âµ, Q1 )


lim lim
=
h
i1
Îµâ†“0 Âµâ†“0 bÌƒN (Âµ, Q2 )
2 2
max |EF0 [sz (Di ) Ï†c (Di )]| /EF0 sz (Di )
: z âˆˆ Q2


for Q1,0 = {z âˆˆ Q1 : EF0 [sz (Di ) Ï†Î³ (Di )] = 0} , provided the denominator on the right hand side is
non-zero.22
To complete the proof, note that for Q0 the set of possible Q0 ,


h
i1
2
supQ0 âˆˆQ0 max |EF0 [sz (Di ) Ï†c (Di )]| /EF0 sz (Di )2 : z âˆˆ Q0
bÌƒRN,Îµ (Âµ, Q1 )

 .
sup inf lim lim
=
h
i1
Q1 âˆˆQ Q2 âˆˆQ Îµâ†“0 Âµâ†“0 bÌƒN (Âµ, Q2 )
2 2
supQâˆˆQ max |EF0 [sz (Di ) Ï†c (Di )]| /EF0 sz (Di )
:zâˆˆQ
The proof of Proposition 2 shows, however, that
h
i1
2
max |EF0 [sz (Di ) Ï†c (Di )]| /EF0 sz (Di )2 = Ïƒc

zâˆˆZ+

and
h

max
zâˆˆZ+ :EF0 [sz (Di )Ï†Î³ (Di )]=0

2

|EF0 [sz (Di ) Ï†c (Di )]| /EF0 sz (Di )

i1

2

âˆš
= Ïƒc 1 âˆ’ âˆ†.

Hence,
sup inf lim lim

Q1 âˆˆQ Q2 âˆˆQ Îµâ†“0 Âµâ†“0

bÌƒRN,Îµ (Âµ, Q1 )
bÌƒN (Âµ, Q2 )

=

âˆš

1 âˆ’ âˆ†,

as we wanted to show.

E

Accounting for Richer Dependence of cÌ‚ on the Data

In Section 5, for cases where the function c (Î¸) depends on the distribution of the data other than
through Î¸, we effectively fix the distribution of the data at the empirical distribution for the purposes
22

If the denominator on the right hand side is zero, we define the limit as +âˆž.

46

of estimating âˆ† and Î›. Here we discuss how to allow for uncertainty about the distribution of data
in a special case, and present corresponding calculations for our applications.
Suppose in particular that
(19)

cÌ‚ =


1X 
c Î¸Ì‚; Di
n
i

for some function c (Â·). In contrast to the setup in Section 5, here we allow that cÌ‚ depends on the
data directly, and not only through the dependence of cÌ‚ on Î¸Ì‚.
In this case, one can show that the recipe in Section 5 applies, with the modification that
(20)





Ï†Ì‚c (Di ) = c Î¸Ì‚; Di + Î›Ì‚cg Ï†g Di ; Î¸Ì‚



where Ï†g Di ; Î¸Ì‚ and Î›Ì‚cg are as defined in Section 5, and CÌ‚ in the definition of Î›Ì‚cg is now given
P
by the gradient of n1 i c (Î¸; Di ) with respect to Î¸ at Î¸Ì‚.
The proof of this result, which we omit, proceeds by noting that we can augment the GMM parameter vector as (c, Î¸), and correspondingly augment the moment equation as (c (Î¸; Di ) âˆ’ c, Ï†g (Di ; Î¸)),
following which we can derive the estimated influence function for cÌ‚ as we would for any element
of Î¸Ì‚.
In the cases of Attanasio et al. (2012a) and Gentzkow (2007a), we can represent the calculation
Ë† using the modified estimated influence function
of cÌ‚ in the form given in (19) and thus calculate âˆ†
in (20). In the case of Attanasio et al. (2012a), the estimates in Table I change from 0.283, 0.227,
and 0.056, respectively, to 0.277, 0.221, and 0.055. In the case of Gentzkow (2007a), the estimates
in Table II change from 0.514, 0.009, and 0.503, respectively, to 0.517, 0.008, and 0.507.

47

