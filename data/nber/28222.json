{
    "id": 28222,
    "citation_title": "Measuring Racial Discrimination in Algorithms",
    "citation_author": [
        "David Arnold",
        "Will S. Dobbie",
        "Peter Hull"
    ],
    "citation_publication_date": "2020-12-21",
    "issue_date": "2020-12-17",
    "revision_date": "2021-01-13",
    "topics": [
        "\n",
        "Econometrics",
        "\n",
        "Estimation Methods",
        "\n",
        "Labor Economics",
        "\n",
        "Demography and Aging",
        "\n",
        "Other",
        "\n",
        "Law and Economics",
        "\n"
    ],
    "program": [
        "\n",
        "Labor Studies",
        "\n"
    ],
    "projects": null,
    "working_groups": [
        "\n",
        "Economics of Crime",
        "\n",
        "Race and Stratification in the Economy",
        "\n"
    ],
    "abstract": "\n\nThere is growing concern that the rise of algorithmic decision-making can lead to discrimination against legally protected groups, but measuring such algorithmic discrimination is often hampered by a fundamental selection challenge. We develop new quasi-experimental tools to overcome this challenge and measure algorithmic discrimination in the setting of pretrial bail decisions. We first show that the selection challenge reduces to the challenge of measuring four moments: the mean latent qualification of white and Black individuals and the race-specific covariance between qualification and the algorithm\u2019s treatment recommendation. We then show how these four moments can be estimated by extrapolating quasi-experimental variation across as-good-as-randomly assigned decision-makers. Estimates from New York City show that a sophisticated machine learning algorithm discriminates against Black defendants, even though defendant race and ethnicity are not included in the training data. The algorithm recommends releasing white defendants before trial at an 8 percentage point (11 percent) higher rate than Black defendants with identical potential for pretrial misconduct, with this unwarranted disparity explaining 77 percent of the observed racial disparity in algorithmic recommendations. We find a similar level of algorithmic discrimination with regression-based recommendations, using a model inspired by a widely used pretrial risk assessment tool.\n\n",
    "acknowledgement": "\nWe thank Ashesh Rambachan and Jonathan Roth for insightful comments. Jeremy Albright, Jerray Chang, and Alexia Olaizola provided excellent research assistance. The data we analyze are provided by the New York State Division of Criminal Justice Services (DCJS), and the Office of Court Administration (OCA). The opinions, findings, and conclusions expressed in this paper are those of the authors and not those of DCJS or OCA. Neither New York State, DCJS or OCA assumes liability for its contents or use thereof. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}