NBER WORKING PAPER SERIES

MONOPSONY IN ONLINE LABOR MARKETS
Arindrajit Dube
Jeff Jacobs
Suresh Naidu
Siddharth Suri
Working Paper 24416
http://www.nber.org/papers/w24416

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
March 2018

We thank Gary Hsieh and Panos Ipeirotis for sharing data as well as Bentley Macleod, Aaron
Sojourner, and Glen Weyl for helpful comments. The views expressed herein are those of the
authors and do not necessarily reflect the views of the National Bureau of Economic Research.
At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w24416.ack
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
¬© 2018 by Arindrajit Dube, Jeff Jacobs, Suresh Naidu, and Siddharth Suri. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including ¬© notice, is given to the source.

Monopsony in Online Labor Markets
Arindrajit Dube, Jeff Jacobs, Suresh Naidu, and Siddharth Suri
NBER Working Paper No. 24416
March 2018
JEL No. J01,J42
ABSTRACT
On-demand labor platforms make up a large part of the ‚Äúgig economy.‚Äù We quantify the extent of
monopsony power in one of the largest on-demand labor platforms, Amazon Mechanical Turk
(MTurk), by measuring the elasticity of labor supply facing the requester (employer) using both
observational and experimental variation in wages. We isolate plausibly exogenous variation in
rewards using a double-machine-learning estimator applied to a large dataset of scraped MTurk
tasks. We also re-analyze data from 5 MTurk experiments that randomized payments to obtain
corresponding experimental estimates. Both approaches yield uniformly low labor supply
elasticities, around 0.1, with little heterogeneity.
Arindrajit Dube
Department of Economics
Crotty Hall
412 North Pleasant Street
University of Massachusetts
Amherst, MA 01002
and IZA
adube@econs.umass.edu
Jeff Jacobs
Columbia University
420 W 118th street
New York, NY 10027
jpj2122@columbia.edu

Suresh Naidu
Columbia University
420 West 118th Street
New York, NY 10027
and NBER
sn2430@columbia.edu
Siddharth Suri
Microsoft Research, NYC
641 Avenue of the Americas, 7th Floor
New York, NY 10011
suri@microsoft.com

1

Introduction

Online platforms are becoming an increasingly important feature of the labor market. For example, Katz and
Krueger (2016) measured a roughly 50% increase in flexible work arrangements in the U.S. economy between
2005 and 2015 and estimated that this increase accounts for ‚Äú94% of the net employment growth in the U.S.
economy‚Äù during this time. Katz and Krueger define flexible work arrangements as involving ‚Äútemporary
help agency workers, on-call workers, contract workers, and independent contractors or freelancers‚Äù, which
includes work done via digital labor markets such as Uber, TaskRabbit or Amazon Mechanical Turk (MTurk).
Other recent surveys corroborate the importance of online labor platforms (Smith (2016)). Taken as a
whole, these studies show that short-term jobs allocated by online labor platforms have risen as a share of
employment in recent years and will continue to do so in the near future. This raises a basic question: how
competitive are online labor markets?
While a number of prior works have suggested that employers in online labor markets have a surprising
degree of of market power, they have stopped short of quantifying it.1 In this paper, we rigorously estimate
the degree of requester market power in a widely-used online labor market ‚Äì Amazon Mechanical Turk. This
is the most popular online micro-task platform, allowing requesters (employers) to post jobs which workers
can complete for pay.
We provide initial evidence regarding how sensitive the duration of task vacancies are to task rewards,
using data from a near-universe of tasks scraped from MTurk. This evidence thus provides us with an
estimate of wage-setting (monopsony) power facing task requesters (Manning (2003); Card et al. (2016)).
We isolate plausibly exogenous variation in rewards using a double-machine-learning (Chernozhukov et al.
(2017)) method, which controls for a highly predictive function of observables generated from the textual
and numeric metadata of each task.
We then present results from a number of independent experiments on the sensitivity of workers‚Äô acceptance of tasks to the level of pay offered. We analyze data from 5 previous experiments that randomized
wages of MTurk subjects, with the full list of experiments we surveyed given in Appendix B. While the previous experimenters had randomly varied the wage, none except (Dube et al. (2017)) recognized that they
had estimated a task-specific labor supply curve, nor noticed that this reflected monopsony power on the
MTurk marketplace. We empirically estimate both a ‚Äúrecruitment‚Äù elasticity where workers see a reward and
associated task as part of their normal browsing for jobs, and a ‚Äúretention‚Äù elasticity where workers, having
1 Kingsley et al. (2015) describe a variety of reasons why market power persists in the MTurk marketplace. They document
(i) an information asymmetry where employers have more information on workers than vice versa, (ii) a serious amount of
market concentration where 10% of the requesters post over 98% of the tasks, and (iii) ex ante wage rates set by the requester.

2

already accepted a task, are given an opportunity to perform additional work for a randomized bonus payment. The recruitment elasticity is based on a novel ‚Äúhoneypot‚Äù experimental design, where randomly-varied
wage offers were made observable only to random subsets of MTurk workers.
Together, these very different pieces of evidence provide a remarkably consistent estimate of the labor
supply elasticity facing MTurk requesters, indicating the robustness of our results. The three experiments
with a ‚Äúhoneypot‚Äù design suggest a recruitment elasticity between 0.05 and 0.11. Similarly, retention probabilities do not increase very much as a function of reward posted, with implied retention elasticities in the
0.1 to 0.5 range for the two experiments using that design. The precision-weighted average experimental
requester‚Äôs labor supply elasiticity is 0.14, and in particular the pooled recruitment elasticity is 0.06, remarkably close to the corresponding 0.08 estimate produced by our preferred double-ML specification. The
estimates are uniformly small across subsamples, with little heterogeneity by reward amount. This close
agreement suggests that the constant elasticity specification commonly used in the literature may not be
a bad approximation in this context. As a further contribution, our paper provides an independent ‚Äì and
favorable ‚Äì assessment of the double-ML estimator against an experimental benchmark.
The rest of this paper is structured as follows. Section 2 outlines a model of monopsony in a task market.
Section 3 uses double-machine-learning to isolate plausibly-exogenous variation in wages when estimating
the requester‚Äôs labor supply function from observational data, then uses these double-ML estimates to assess
external validity of the experimental estimates. Section 4 provides experimental evidence on the extent of
labor market power on MTurk. Section 5 concludes.

2

Monopsony in A Task Market

Monopsony is characterized by two features: wage-setting power and inability to wage-discriminate. MTurk,
with its task-posting structure, did not offer many margins for wage-discrimination until very recently (after
our sample period). In our sample period, requesters could only restrict the set of eligible workers based
on prior acceptance rates (the rates at which previous requesters had deemed their work satisfactory) or
location (e.g., India or the United States)
Monopsony power may arise due to a small number of employers on the platform, from search frictions
in locating higher paying tasks, or from idiosyncratic preferences over task characteristics. Given the sizable
number of requesters per worker, and the fact that tasks are centrally posted on MTurk, we posit that
a model based on discrete choice over differentiated tasks ‚Äì as in Card et al. (2016) ‚Äì provides the most

3

compelling explanation for the presence of monopsony power in online platforms. In their model, individuals
have idiosyncratic, privately observed tastes over each job, creating an upward sloping labor supply curve
facing each firm.
In Appendix A we present a nested logit model of the MTurk market, with the first nest being the
‚Äúrecruitment‚Äù margin, that brings workers into a HIT (Human Intelligence Task) batch, and the second
nest being the ‚Äúretention‚Äù margin that incentivizes workers to complete tasks within the batch. The model
illustrates how requesters will jointly set both initial wages and bonus wages, and shows why these two wages
need independent experiments in order to estimate the two elasticities. The model also yields the formula
for the sum of both wages as a markdown on productivity that depends on both the recruitment elasticity
as well as the retention elasticity.
While we proceed with a random utility interpretation of the nested logit, Fosgerau et al. (2016) show
a generic equivalence between rational inattention and random utility based discrete choice models (in
particular the nested logit can be expressed as a rational inattention model with a generalized Shannon
entropy cost of information processing). While MTurk makes many work options available and easy to find,
employers may have outsized market power either due to idiosyncratic tastes of workers for particular tasks
(random utility) or due to costly information processing that makes it difficult to discern which task is best
(rational inattention).

3

Observational Evidence on Recruitment Elasticity from MTurk

3.1

Data and empirical strategy

For our observational analysis, we use two primary sources of scraped MTurk data. The first dataset was
obtained from Ipeirotis (2010), and covers the January 2014 to February 2016 period. The data consists of
over 400,000 scraped HIT batches from the Mechanical Turk Tracker web API2 . This scraper downloaded
the newest 200 HIT batches posted to MTurk every six minutes, then the status page for each discovered
HIT batch was checked every minute until the page reported that all HITs in the batch had been accepted.
Beginning in May 2016 we launched our own scraper, which took snapshots of all HIT batches on MTurk
every 30 minutes, later increased to every 10 minutes beginning in March 2017. This scraping strategy may
miss batches that are posted and filled too quickly for the scraper to detect (i.e. duration less than 30 or
10 minutes). This scraping strategy yielded over 300,000 HIT batches, but stopped working on August 22,
2 http://crowd-power.appspot.com/#/general

4

2017, and we have been unable to collect more data since then. We show results separately for these two
datasets, and find broadly similar results. Further details on the data are in Appendix C, including densities
of the log durations showing similar support.
We use the time it takes for a posted task to disappear as a measure of the probability of acceptance, and
regress the duration of the task posting on the observed reward to obtain an estimate of Œ∑1 , the recruitment
elasticity. We take advantage of the vast amount of available online crowdsourcing data to estinate Œ∑, using
high-dimensional regression adjustment applied to numeric and textual characteristics of the tasks to control
for possible sources of endogenous task characteristics.
The resulting linear specification is estimated on observations of HIT batch durations and rewards, and
is given by:

ln(durationh ) = ‚àíŒ∑1 ln(rewardh ) + ŒΩh + h

(1)

Where ŒΩ is a nuisance parameter that is correlated with both rewards and durations, and  is an error
term that is conditionally independent of durations, so E[|ŒΩ] = 0. An unbiased estimate of Œ∑1 requires that
we correctly control for ŒΩ, the determinants of duration that are correlated with rewards, in particular labor
demand. The virtue of the experimental estimates in the fourth section is that randomization ensures that ŒΩ
is independent of log(reward). With observational data, we must rely on a sufficiently rich set of observables
to control for ŒΩ, and it is impossible to be completely confident that all possible sources of omitted variable
bias have been eliminated. However, the large and high-dimensional nature of the observational MTurk data
lets us push the limits of observational analysis. We use two different approaches for this analysis ‚Äì fixed
effects regression and double-machine-learning.

3.2

Fixed-Effects Regression

In our first strategy, we control for requester and time fixed effects, along with fixed effects for deciles of the
time allotted by the requester, with the idea that much variation in labor demand is idiosyncratic to a given
requester asking a given task in a distinct time period. Formally, we assume that ŒΩ = œÅr + œÑt + Œ¥d . This
says that the unobserved relative task attractiveness is captured by the identity of the employer, the time
the task is first posted, and the decile of the time allotted for the task d.
We can then estimate a standard fixed-effects regression:

ln(duration) = ‚àíŒ∑1 ln(reward) + œÅr + œÑt + Œ¥d + 
5

(2)

3.3

Double Machine Learning

As our second approach, we implement a ‚Äúdouble-machine-learning‚Äù(double-ML) estimator recently developed by Chernozhukov et al. (2017), which in our case uses an ensemble machine learning approach to model
the unobserved ŒΩ.
In particular, we suppose that ŒΩ in equation 1 is equal to g0 (Z), an unknown function of a high-dimensional
vector of observable variables Z. We further suppose that variation in rewards is generated by another
function of Z so that log(rewards) = m0 (Z) + ¬µ. Combining these two equations we get:

ln(duration) = ‚àíŒ∑1 ln(reward) + g0 (Z) + , E[|Z, ln(reward)] = 0
ln(reward) = m0 (Z) + ¬µ, E[¬µ|Z] = 0,

and we obtain an unbiased and

(3)
(4)

‚àö
n-consistent estimator of Œ∑1 via the double-ML procedure Chernozhukov

et al. (2017). The benefit of this procedure stems from the fact that it allows us to utilize any number
of state-of-the-art machine learning methods, such as neural nets or random forests, to obtain estimates of
\
\
the conditional expectation functions gb0 (Z) = E[log(duration)|Z]
and m
c0 (Z) = E[log(rewards)|Z]
which
are then ‚Äúde-biased‚Äù to obtain our desired estimator Œ∑Àá1 . Specifically, from our machine learning-estimated
gb0 (Z) and m
c0 (Z) we can compute the residuals from (3) and (4) as ¬µ
b = log(reward) ‚àí m
c0 (Z) and b
 =
log(duration) ‚àí gb0 (Z), respectively, and use these residuals to compute the final estimator as

n

0

Œ∑Àá1 =

1X 2
¬µbi
n i=1

!‚àí1

n

1X
¬µbi bi ,
n i=1

(5)

The bias from overfitting will not asymptotically go to 0 if the same data is used to estimate g0 (Z) and
m0 (Z) and Œ∑1 . However, if a different sample is used to estimate g0 (Z) and m0 (Z) and Œ∑Àá1 is averaged over
multiple folds, then the estimator is consistent and unbiased.
The intuition behind this estimator is similar to the classic partial regression formula. In equation 1
the partial regression formula implies that Œ∑1 could be recovered from a regression of E[ln(duration)|ŒΩ]
on E[ln(reward)|ŒΩ]. The double-ML estimator uses machine learning to form proxies for ŒΩ that fit both
conditional expectations very well, implying that the resulting residuals have ‚Äúpartialled out‚Äù a very flexible
function of all covariates that capture as much of the variation as possible.

6

Double-machine-learning allows us to leverage a large number of covariates for identifying causal effects,
using whichever prediction algorithm has highest goodness-of-fit (e.g. R2 ) in held-out data. We construct
a large set of covariates, in this case n-grams and other textual features derived from the HIT batch titles,
descriptions, and keywords, as well as numeric features such as the number of HIT tasks posted in the batch,
time allotted, and time posted. We experiment with a number of methods, discussed below, with parameters
tuned via 5-fold cross-validation.

3.3.1

HIT Features Used For Prediction

We use both non-textual and textual features as inputs to the double-ML procedure. First, we use nontextual features from the HIT including information about the batch size, time allotted for each HIT in
the group by the requester, time remaining before expiration of the HIT group, required qualifications (e.g.,
worker acceptance rate required to be above x%), the volume of HIT groups posted by the requester across
the marketplace, and so on (the full set of features is described in Appendix D.3).
Additionally, we generate four distinct types of textual features from each HIT group‚Äôs description, title,
and list of keywords: n-grams, topic distributions, Doc2Vec embeddings, and hand-engineered features. The
details can be found in Appendix D.

3.3.2

Double Machine Learning Procedure

To satisfy the sample-splitting requirement of the double-ML estimator (Chernozhukov et al., 2017), the full
set of HIT groups is split into two equally-sized subsets, A and B. Each subset is further split into training
and validation sets, with 80% of the observations in A going into Atrain and 20% into Aval , and similarly
for Btrain and Bval . The machine learning then proceeds in two ‚Äústages‚Äù.
In the first stage, the n-gram features are computed for Atrain and Btrain , and two series of learning
algorithms are run, the first with Atrain as training data and Aval as test data, the second with Btrain as
training data and Bval as test data. For each series and each dataset (Ipeirotis (2010) and our own scraped
data) the algorithm which achieves the highest total validation score (here the sum of validation scores for
reward prediction and duration prediction) is selected as the ‚Äúfinal‚Äù algorithm to be used for the remainder
of the procedure3 .
3 In every case, scikit-learn‚Äôs RandomForestRegressor achieved the highest score out of {AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor, SVR (SupportVectorRegressor)}. The random
forest regression constructs a series of decision trees, each of which is built based on a random subset of all available features,
and takes the mean prediction over all of these trees to be the estimate (thus random forest regressions are a type of ensemble
method). For more on random forest regression, see Breiman (2001), Section 11.

7

Log Duration ML Residuals

.2

.1

0

-.1

-.2
-2

-1

0

1

2

Log Reward ML Residuals

Figure 1: Binned scatterplot (20 ventiles) for double-ML residuals of log duration and log rewards. Residuals
are calculated as difference between observed value and predicted value from a random forest trained on a
held-out sample, as described in Section 3.3.2.

To begin the second stage of the procedure, we select the 100 textual features which best predicted the
reward values in the first stage, along with the 100 which best predicted the duration values, and set these as
the first 200 columns of our second-stage feature matrix. The additional text features described in Section
3.3.1 are then appended to the matrix, along with numeric features described in Appendix D. The ‚Äúfinal‚Äù
algorithm discovered in stage one is then run twice, the first time with the HIT groups in A used as training
data and groups in B used as test data, and the second with the training and test sets reversed. These two
values are then averaged (so as to satisfy the sample-splitting requirement of the double-ML estimator) as
specified in equation 5 to produce the final estimate Œ∑Àá1 0 (along with its standard error) for each dataset.

3.4

Results

In Table 1 we present basic OLS results, fixed effects regressions, and the double-ML regressions (with
and without fixed effects). Column 1 shows the simple bivariate regression of log duration on log reward.
Unsurprisingly this regression is inconclusive, likely because of extensive omitted variables that are correlated
with task attractiveness and the intensity of requester demand, both of which would be correlated with both
the reward posted as well as the time until the HIT is filled.
Column 2 implements the fixed-effects specification, controlling for deciles of time allotted for the task as
8

Table 1: Duration Elasticities from Observational MTurk Data

Log Reward

(1)
0.186
(0.0947)

(2)
-0.0708
(0.0672)

Log Reward-ML res.
N
Clusters
Type
Data

644873
41167
OLS
Pooled

629756
26050
FE
Pooled

(3)

(4)

(5)

(6)

(7)

-0.0841
(0.00820)
644873
41167
ML
Pooled

-0.0589
(0.0108)
629756
26050
ML-FE
Pooled

-0.0762
(0.0169)
93775
6962
ML
2017

-0.169
(0.0250)
292746
18340
ML
2016-2017

-0.0373
(0.00429)
258352
24923
ML
2014-2016

Notes: This Table presents OLS, FE, and Double-ML estimates using the data obtained from scraping MTurk between Jan.
2014 and Feb. 2016 (from Ipeirotis (2010)), May 2016-March 2017 (scraped every 30 minutes), and March-August 2017
(scraped every 10 minutes). FE indicates fixed-effects for the hour of the first appearance of the HIT, requester, and time
allotted fixed effects. Standard errors are clustered at the requester level.

well as fixed effects for requester and the time posted described above. The coefficient on log reward is ‚àí0.07.
Columns 3 through 7 show the results from the double-ML estimator. Column 3 shows the bivariate OLS
regression of residualized durations on residualized rewards, and here the coefficient on residualized rewards
is a strongly significant ‚àí0.08. Figure 1 shows the corresponding binned scatterplot, which shows the binned
residuals falling quite close to the linear fit implied by a constant elasticity. Column 4 in Table 1 adds the
fixed effects from Column 2 to the ML specification, and obtains a quite similar estimate of ‚àí0.06, suggesting
that the double-ML procedure is effectively purging the effects of observable variables omitted from Column
1 (as a large change in the coefficient would suggest that there were other unobserved variables confounding
the regression). Columns 5-7 show the double-ML specifications for the different scraped samples. While
there is some heterogeneity, the implied elasticities are uniformly small.

4

Experimental Evidence on Labor Supply Elasticity Facing Requesters on MTurk

The observational evidence is quite suggestive of a low requester‚Äôs recruitment elasticity, Œ∑1 , but even in the
double-ML estimates concerns about omitted variable bias may linger. It is possible that not all task-relevant
characteristics have been adequately controlled for, despite the high predictive power of our conditional
expectation functions above. If we have experimental (random) variation in rewards, we can estimate the
following regression:

P r(Accept) = Œ± + Œ≤reward + 

9

(6)

and an estimate of Œ∑ would be recovered by Œ∑ = Œ≤ √ó

E[reward]
P r(Accept)

with the expectation taken over the

population of workers in the sample. We can compare this estimate of Œ∑ to the double-ML estimate from the
observational data above to bolster our confidence in our estimate: if both estimates yield similar results it
suggests that the double-ML estimator is indeed adequately controlling for unobserved variation, and that
the experimental estimates are externally valid. In our framework above, however, the Œ∑ are differentiated
depending on whether the acceptance decision is the initial ‚Äúrecruitment‚Äù (Œ∑1 ), or the subsequent ‚Äúretention‚Äù
decision (Œ∑2 ). Experimentally estimated Œ∑1 ‚Äôs are the most directly comparable to the Double-ML estimates,
but experimental estimates of Œ∑2 are available from high-powered experiments, as detailed in the next subsection. We then describe how ‚Äúhoneypot‚Äù experiments can be engineered to estimate Œ∑1 experimentally and
show results from existing experiments.

4.1

Experimental Retention Elasticities

Horton et al. (2011) and Dube et al. (2017) both run variants of the following experiment. A simple uniformly
priced (say, 10 cent) HIT is posted. Subjects give demographic information and perform a simple task (e.g.,
tagging an image). The subjects are then asked if they would like to perform a given number of additional
identical tasks for a randomized bonus wage. The change in the probability of acceptance as a function
of the wage gives the responsiveness of requester‚Äôs labor supply to random wage posting, with low values
suggesting a great deal of market power. This is a ‚Äúretention‚Äù elasticity, Œ∑2 , as workers have already been
drawn into a HIT i when asked whether they wish to continnue.
Experiment 1 was conducted by Horton et al. (2011), and was among the earliest attempts to estimate
economic parameters from MTurk. The authors aimed to elicit the labor-supply elasticity of online workers
to the market, but this design does not elicit the market labor supply, but rather the requester‚Äôs labor supply
(i.e., the supply to the experimenter/requester for the particular task). The task in this experiment was
transcribing Tagalog translations of paragraphs from Adam Smith‚Äôs The Theory of Moral Sentiments.
Experiment 2 was conducted by Dube et al. (2017) in 2016, deliberately emulating the design of the
Horton et al. (2011) study with the aim of testing for left-digit bias in the requester‚Äôs labor supply of
online workers. Hence the rewards are substantially lower, between 5 and 15 cents, but the sample sizes
are correspondingly larger. The task here was tagging sheets of the 1850 US census slave schedules for the
presence of marks in the fugitive slave columns.
We show results for both the full sample and sophisticates (defined as working more than 10 hours on
MTurk and primarily for money). The resulting requester‚Äôs labor supply elasticities are shown in Columns

10

Table 2: Offer Acceptance and Offered Rewards from Retention Experiments
(1)

(2)

(3)

(4)

Panel A: Horton et al. 2011 Probability of Accepting Offer
Reward
N
Œ∑2
SE
Avg. Reward
Sophisticated
Controls

0.127
(0.0219)
328
0.234
0.0334
11.60
No
No

0.140
(0.0241)
307
0.241
0.0364
11.63
No
Yes

0.0861
(0.0292)
125
0.192
0.0594
11.37
Yes
No

0.0973
(0.0333)
107
0.202
0.0664
11.50
Yes
Yes

Panel B: Dube et al. 2017 Probability of Accepting Offer
Reward
Controls
N
Œ∑2
SE
Avg. Reward
Sophisticated

0.0267
(0.0171)
No
5184
0.052
0.0333
9
No

0.0486
(0.0202)
Yes
5017
0.077
0.0322
9
No

0.0764
(0.0348)
No
1702
0.118
0.0534
9
Yes

0.0782
(0.0329)
Yes
1618
0.114
0.0479
9
Yes

Notes: Coefficients from logit regressions of accept indicator on log reward from ‚Äúretention‚Äù experiments, and calculated
elasticities, assessed at the specification sample mean. Robust standard errors in parenthesis.

1-4 of Table 2. The implied Œ∑2 from the Horton et al. estimates are quite low, between 0.19 and 0.25, while
implied Œ∑2 from the Dube et al. estimates are even lower, always below 0.12. Besides differences in the
tasks, one likely reason for the very slight difference is the different support of the reward variation (Dube
et al. randomize between 5 and 15 cents, while Horton et al. randomize between 10 and 25 cents), and the
composition of workers and requesters likely changed considerably between 2011 and 2016. But broadly the
estimates are quite comparable, despite these differences.

4.2

Experimental Recruitment Elasticities

Engineering an experiment to test the recruitment elasticity is much more challenging than estimating the
retention elasticity. We take advantage of three pieces of prior work ‚Äì Ho et al. (2015), Hsieh and Kocielnik
(2016), and Yin et al. (2018) ‚Äì that presented tasks with varying pay rates to random subsets of the MTurk
population such that workers assigned one pay rate could not see the tasks available to other workers who
had a different pay rate. We stress that none of the papers actually estimated a labor supply elasticity using
this random variation in pay.

11

All of these experiments use a two-phase ‚Äúhoneypot‚Äù design. In the first phase a generic HIT is posted
at a fixed pay rate. In this simple task, workers are asked a couple of survey questions including whether
they would like to be notified of future work opportunities. The IDs of the workers who said yes are then
randomized into treatment conditions. During the second phase of the experiment HITs corresponding to
the different treatment conditions are launched with identical tasks but varying rewards. This design uses a
relatively obscure piece of the MTurk API that lets a requester make a HIT group visible to only a subset
of workers. Thus each HIT group can only be seen by and accepted by those treated, and it appears as a
regular HIT group in the MTurk interface for them. This design, which first appeared in Section 5 of Ho
et al. (2015) and was later refined in Yin et al. (2018), replicates the search environment workers are facing
before having said yes to the task.
In the first experiment (Ho et al. (2015)), 800 people were recruited via a 0.05 cent ‚Äúhoneypot‚Äù HIT,
and then randomly split into four treatment groups of 200 workers each. Three groups were given $0.50 to
complete the HIT (one control, one with a surprise $1.00 bonus, and one with a performance based bonus),
and a fourth group was given $1.50. We drop the group that was given an additional performance-based
incentive to focus on the recruitment elasticity.
In the second experiment (Yin et al. (2018)), 1,800 workers recruited using the same ‚Äúhoney pot‚Äù protocol
were randomly split into three treatment groups, with rewards for the additional task of $0.03, $0.04, and
$0.05, respectively. For the task itself, users were asked to categorize an Amazon.com review as positive or
negative. Of the 600 in each group, 357 in the $0.03 group accepted, 351 in the $0.04 group accepted, and
371 in the $0.05 group accepted.
In the third experiment (Hsieh and Kocielnik (2016)), 927 workers were recruited via a similar design,
with the task being to brainstorm the ‚Äúnumber of uses of a brick‚Äù (a measure of creative thinking) and given
one of 7 random rewards: 0 cents, 5 cents, 25 cents, 1% chance of $5, 1% chance of $25, and 25 and 50 cent
donation to charity. We drop the lottery and charity treatments and examine only the variation in rewards
(0, 5 or 25 cents) , which leaves us with 338 observations. Of these, 131 were in the 0 cent reward group
(68 accepted), 89 were in the 5 cent group (52 accepted), and 118 were in the 25 cent group (82 accepted).
We made a synthetic dataset based on these numbers in communication with the authors, as the replication
data was unavailable.
Neither of the first two experiments asked demographic characteristics, and replication data for the third
is unavailable, so there is limited capacity to control for observables. However, the randomized assignment
of the reward mitigates any role for covariates besides improving precision. Table 3 shows the simple OLS

12

Table 3: Recruitment Elasticities From Three Experiments
(1)

(2)

(3)

(4)

Reward

0.00186
(0.00188)

0.0451
(0.0587)

0.0287
(0.0104)

0.00744
(0.00385)

N
Œ∑1
SE
Avg. Reward
Experiment

600
0.0497
0.0503
83.33
Spot Diff.

1800
0.0724
0.0944
4
Tag Ads

338
0.115
0.0417
10.04
Brainstorming

2738
0.0610
0.0290
22.13
Pooled

Notes: Coefficients from logit regressions of an accept indicator on reward from ‚Äúrecruitment‚Äù experiments, and calculated
elasticities, assessed at the experimental sample mean. The pooled specification includes experiment fixed effects, and is
weighted by the inverse of the standard deviation of rewards within each experiment. Robust standard errors in parentheses.

regression results using the same logit specification as equation 6, separately by experiment, and then pooled,
controlling for experiment fixed effects and weighting by the inverse of the standard deviation of rewards
within each experiment.
While not strongly significant, even when all experiments are pooled, the point estimates are remarkably
similar despite the very different wage levels at which the experiments were run, and close to the very small
estimates obtained from the double-ML procedure above. The implied recruitment elasticity from the pooled
three experiments is 0.06 and is distinguishable from 0 at 5% significance. While the first 2 experiments have
insignificant elasticities, in the third experiment we obtain a statistically significant, but still small elasticity,
despite a smaller sample size, possibly due to the more attractive nature of the ex-post task relative to the
other two.

4.3

Comparison of Estimates

Figure 2 shows the estimates from each of the experiments, together with the double-ML estimates obtained
from the two samples, each split by quintiles of the reward distribution in each of the two scraped samples.
The consistency of the estimates is remarkable, and generally implies a low labor supply elasticity facing
requesters on MTurk, with some estimates unable to rule out 0 with 95% confidence.
While the double-ML estimates are again quite consistent with the experimental retention elasticities,
the more natural comparison is with the experimental recruitment elasticities. In this domain the larger
sample from the 2014-2016 period is numerically quite close to the experimental elasticities (which were run
in this period), while the 2016-2017 estimate is slightly larger.
The graph also plots the precision-weighted mean experimental elasticity (weighted by the inverse of the

13

Elasticity of Labor Supply Facing Requester

.4
.3
.2

Precision-weighted mean
experimental elasticity = .14

.14
.1
0
-.1
-.2
2

10

50

100

150

200

Mean Reward in Cents (Log Scale)
Recruitment (Experimental)
Retention (Experimental)
Recruitment (Double-ML, pooled MTurk samples) By Reward Quintile

Figure 2: Baseline estimates from both ‚Äúrecruitment‚Äù and ‚Äúretention‚Äù experimental designs (Column 1 of
Table 2 and Columns 1-3 of Table 3), as well as Double-ML recruitment elasticities from observational data,
plotted by mean reward of sample.

variance of the estimated elasticities) of 0.14. The double-ML estimates are all very close to this line, despite
being estimated using very different sources of variation in the rewards.
We can use our estimates to infer the distribution of MTurk surplus between workers and requesters,
following the formula in Appendix A. The markdowns are quite large, with workers paid less than 20% of
their productivity. These are close to the markdowns implied by firm‚Äôs labor supply elasticities estimated
for nurses by Staiger et al. (2010), among the lowest in the literature.

5

Discussion and Conclusion

In his review of Manning‚Äôs 2003 book Monopsony in Motion, Peter Kuhn made the following conjecture.
‚Äú[U]pward-sloping labor supply curves‚Äîwhether induced by search or other factors‚Äîseem unlikely to me to
be a serious constraint for most firms. This seems even more likely to be the case in the near future, as ...
information technology has the potential to reduce search frictions.‚Äù (Kuhn, 2004, pp. 376). The emergence
of online platforms represents an idealized environment where search costs are presumably very low. Despite
this, we find a highly robust and surprisingly high degree of market power even in this large and diverse spot
labor market.

14

While monopsony power over small stakes tasks on MTurk may seem unimportant, our results have
important implications for platform design and the distribution of gains from digital labor markets, which
are likely to become more important over time. If one believes that requesters are fully exploiting their
monopsony power (as the calibration of requester misoptimization in Dube et al. (2017) suggests), then the
markdown of productivity in the wage is considerable, with workers paid less than 20% of their productivity.
This suggests that much of the surplus created by this online labor market platform is captured by employers.
The source of the monopsony power on MTurk likely lies in the information and market environment
presented to workers and requesters, together with the absence of bargaining or many margins of wage
discrimination. In particular, the tastes different workers have for a given task may be quite dispersed and
not easily discerned by requesters, which induces requesters posting a wage to trade-off the probability of
acceptance against a lower wage. Further, this may be exacerbated by the information environment facing
workers, which makes searching for alternative jobs difficult. Jobs are highly heterogeneous in time required,
entertainment value (‚Äúfun‚Äù) to the worker, and the reliability of the requester in approving payments (Benson
et al., 2017). There is no single dimensional index of job quality that can be used to order HIT groups while
searching: workers can‚Äôt sort HIT groups by the real wage. Potentially this could be easily remedied: indeed
our double-ML predictions of HIT duration could be used to discount rewards by a summary measure of
task attractiveness.
Why does this inefficient market structure persist? While answering this is beyond the scope of this
paper, one possibility is that MTurk itself has a considerable amount of market power as a platform for
crowdsourcing. Amazon‚Äôs market power as a platform may allow it to underinvest in market design, allowing
inefficient market structures to persist. However, the recent addition of ‚Äúpremium‚Äù categories of workers
for requester targeting may signal Amazon‚Äôs attempt to eliminate inefficiencies created by requester market
power, while preserving the unequal distribution of the surplus between requesters and workers. If requesters
are the more elastic side of the crowdsourcing market, it may be optimal for Amazon to provide a platform
that enables the bulk of the surplus to be captured by employers rather than workers.
MTurk workers and their advocates have long noted the asymmetry in market structure amongst themselves. Others (e.g. Arrieta-Ibarra et al. (2017)) have pointed out that monopsony in data markets may lead
to inefficiently small and low-quality sample sizes in machine learning or survey applications. Both efficiency
and equality concerns have led to the rise of competing, ‚Äúworker-friendly‚Äù platforms such as Stanford‚Äôs Dynamo, mechanisms for sharing information about good and bad requesters and HITs such as Turkopticon
and online discussion fora. Scientific funders such as Russell Sage have instituted minimum wages for crowd-

15

sourced work. Our results suggest that these sentiments and policies may have an economic justification.

16

References
Abernethy, Jacob, Yiling Chen, Chien-Ju Ho, and Bo Waggoner, ‚ÄúLow-Cost Learning via Active
Data Procurement,‚Äù in ‚ÄúProceedings of the Sixteenth ACM Conference on Economics and Computation‚Äù
EC ‚Äô15 ACM New York, NY, USA 2015, pp. 619‚Äì636.
Arrieta-Ibarra, Imanol, Leonard Goff, Diego Jim√©nez Hern√°ndez, Jaron Lanier, and E Weyl,
‚ÄúShould We Treat Data as Labor? Moving Beyond‚ÄôFree‚Äô,‚Äù 2017.
Benson, Alan, Aaron Sojourner, and Akhmed Umyarov, ‚ÄúThe value of employer reputation in the
absence of contract enforcement: A randomized experiment,‚Äù 2017.
Berinsky, Adam J, Gregory A Huber, and Gabriel S Lenz, ‚ÄúEvaluating Online Labor Markets for
Experimental Research: Amazon.com‚Äôs Mechanical Turk,‚Äù Political Analysis, 2012, 20 (3), 351‚Äì368.
Blei, David M, Andrew Y Ng, and Michael I Jordan, ‚ÄúLatent Dirichlet Allocation,‚Äù Journal of
Machine Learning Research, mar 2003, 3, 993‚Äì1022.
Breiman, Leo, ‚ÄúRandom Forests,‚Äù Mach. Learn., October 2001, 45 (1), 5‚Äì32.
Buhrmester, Michael, Tracy Kwang, and Samuel D. Gosling, ‚ÄúAmazon‚Äôs Mechanical Turk: A New
Source of Inexpensive, Yet High-Quality, Data?,‚Äù Perspectives on Psychological Science, 2011, 6 (1), 3‚Äì5.
Callison-Burch, Chris, ‚ÄúCrowd-workers: Aggregating information across turkers to help them find higher
paying work,‚Äù in ‚ÄúSecond AAAI Conference on Human Computation and Crowdsourcing‚Äù 2014.
Card, David, Ana Rute Cardoso, J√∂rg Heining, and Patrick Kline, ‚ÄúFirms and labor market
inequality: Evidence and some theory,‚Äù Technical Report, National Bureau of Economic Research 2016.
Chandler, Dana and John Horton, ‚ÄúLabor Allocation in Paid Crowdsourcing: Experimental Evidence
on Positioning, Nudges and Prices,‚Äù 2011.
Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,
Whitney Newey, and James Robins, ‚ÄúDouble/debiased machine learning for treatment and structural
parameters,‚Äù The Econometrics Journal, 2017.
Crump, Matthew J. C., John V. McDonnell, and Todd M. Gureckis, ‚ÄúEvaluating Amazon‚Äôs
Mechanical Turk as a Tool for Experimental Behavioral Research,‚Äù PLOS ONE, 03 2013, 8 (3), 1‚Äì18.
Dasgupta, Anirban and Arpita Ghosh, ‚ÄúCrowdsourced Judgement Elicitation with Endogenous Proficiency,‚Äù in ‚ÄúProceedings of the 22Nd International Conference on World Wide Web‚Äù WWW ‚Äô13 ACM
New York, NY, USA 2013, pp. 319‚Äì330.
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, ‚ÄúImagenet: A large-scale
hierarchical image database,‚Äù in ‚ÄúComputer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE
Conference on‚Äù IEEE 2009, pp. 248‚Äì255.
Difallah, Djellel Eddine, Michele Catasta, Gianluca Demartini, and Philippe Cudr√©-Mauroux,
‚ÄúScaling-Up the Crowd: Micro-Task Pricing Schemes for Worker Retention and Latency Improvement,‚Äù
in ‚ÄúHCOMP‚Äù 2014.
, , , Panagiotis G. Ipeirotis, and Philippe Cudr√©-Mauroux, ‚ÄúThe Dynamics of Micro-Task
Crowdsourcing: The Case of Amazon MTurk,‚Äù in ‚ÄúProceedings of the 24th International Conference on
World Wide Web‚Äù WWW ‚Äô15 International World Wide Web Conferences Steering Committee Republic
and Canton of Geneva, Switzerland 2015, pp. 238‚Äì247.
Doerrenberg, Philipp, Denvil Duncan, and Max L√∂ffler, ‚ÄúAsymmetric labor-supply responses to
wage-rate changes: Evidence from a field experiment,‚Äù 2016, (16-006).
17

Dube, Arindrajit, Alan Manning, and Suresh Naidu, ‚ÄúMonopsony, Misoptimization, and Round
Number Bunching in the Wage Distribution,‚Äù 2017.
Fosgerau, Mogens, Emerson Melo, and Matthew Shum, ‚ÄúDiscrete choice and rational inattention:
A general equivalence result,‚Äù 2016.
Gadiraju, Ujwal, Ricardo Kawase, and Stefan Dietze, ‚ÄúA Taxonomy of Microtasks on the Web,‚Äù in
‚ÄúProceedings of the 25th ACM Conference on Hypertext and Social Media‚Äù HT ‚Äô14 ACM New York, NY,
USA 2014, pp. 218‚Äì223.
Heer, Jeffrey and Michael Bostock, ‚ÄúCrowdsourcing Graphical Perception: Using Mechanical Turk to
Assess Visualization Design,‚Äù in ‚ÄúProceedings of the SIGCHI Conference on Human Factors in Computing
Systems‚Äù CHI ‚Äô10 ACM New York, NY, USA 2010, pp. 203‚Äì212.
Ho, Chien-Ju, Aleksandrs Slivkins, Siddharth Suri, and Jennifer Wortman Vaughan, ‚ÄúIncentivizing High Quality Crowdwork,‚Äù in ‚ÄúProceedings of the 24th International Conference on World Wide
Web‚Äù WWW ‚Äô15 International World Wide Web Conferences Steering Committee Republic and Canton
of Geneva, Switzerland 2015, pp. 419‚Äì429.
Hofmann, Thomas, Bernhard Sch√∂lkopf, and Alexander J. Smola, ‚ÄúKernel methods in machine
learning,‚Äù Ann. Statist., 06 2008, 36 (3), 1171‚Äì1220.
Honnibal, Matthew and Mark Johnson, ‚ÄúAn Improved Non-monotonic Transition System for Dependency Parsing,‚Äù in ‚ÄúConference on Empirical Methods in Natural Language Processing‚Äù 2015.
Horton, John J, David G Rand, and Richard J Zeckhauser, ‚ÄúThe online laboratory: Conducting
experiments in a real labor market,‚Äù Experimental Economics, 2011, 14 (3), 399‚Äì425.
Horton, John Joseph and Lydia B. Chilton, ‚ÄúThe Labor Economics of Paid Crowdsourcing,‚Äù in
‚ÄúProceedings of the 11th ACM Conference on Electronic Commerce‚Äù EC ‚Äô10 ACM New York, NY, USA
2010, pp. 209‚Äì218.
Hsieh, Gary and Rafa≈Ç Kocielnik, ‚ÄúYou get who you pay for: The impact of incentives on participation
bias,‚Äù in ‚ÄúProceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social
Computing‚Äù ACM 2016, pp. 823‚Äì835.
Huang, Eric, Haoqi Zhang, David C. Parkes, Krzysztof Z. Gajos, and Yiling Chen, ‚ÄúToward
Automatic Task Design: A Progress Report,‚Äù in ‚ÄúProceedings of the ACM SIGKDD Workshop on Human
Computation‚Äù HCOMP ‚Äô10 ACM New York, NY, USA 2010, pp. 77‚Äì85.
Ipeirotis, Panagiotis G., ‚ÄúAnalyzing the Amazon Mechanical Turk Marketplace,‚Äù XRDS, December 2010,
17 (2), 16‚Äì21.
Katz, Lawrence F. and Alan B. Krueger, ‚ÄúThe Rise and Nature of Alternative Work Arrangements in
the United States, 1995-2015,‚Äù Working Paper 22667, National Bureau of Economic Research September
2016.
Kingsley, Sara Constance, Mary L. Gray, and Siddharth Suri, ‚ÄúAccounting for Market Frictions
and Power Asymmetries in Online Labor Markets,‚Äù Policy & Internet, 2015, 7 (4), 383‚Äì400.
Kuhn, Peter, ‚ÄúIs monopsony the right way to model labor markets? a review of Alan Manning‚Äôs monopsony
in motion,‚Äù International Journal of the Economics of Business, 2004, 11 (3), 369‚Äì378.
Le, Quoc and Tomas Mikolov, ‚ÄúDistributed Representations of Sentences and Documents,‚Äù in Eric P.
Xing and Tony Jebara, eds., Proceedings of the 31st International Conference on Machine Learning, Vol. 32
of Proceedings of Machine Learning Research PMLR Bejing, China 22‚Äì24 Jun 2014, pp. 1188‚Äì1196.

18

Lindley, Dennis V, ‚ÄúThe choice of sample size,‚Äù Journal of the Royal Statistical Society: Series D (The
Statistician), 1997, 46 (2), 129‚Äì138.
Manning, Alan, Monopsony in motion: Imperfect competition in labor markets, Princeton University Press,
2003.
Manyika, James, Susan Lund, Kelsey Robinson, John Valentino, and Richard Dobbs, ‚ÄúA labor
market that works: Connecting talent with opportunity in the digital age,‚Äù McKinsey Global Institute,
2015.
Marge, Matthew, Satanjeev Banerjee, and Alexander I Rudnicky, ‚ÄúUsing the Amazon Mechanical
Turk for transcription of spoken language,‚Äù in ‚ÄúAcoustics Speech and Signal Processing (ICASSP), 2010
IEEE International Conference on‚Äù IEEE 2010, pp. 5270‚Äì5273.
Mason, Winter and Duncan J. Watts, ‚ÄúFinancial Incentives and the ‚ÄúPerformance of Crowds‚Äù,‚Äù in
‚ÄúProceedings of the ACM SIGKDD Workshop on Human Computation‚Äù HCOMP ‚Äô09 ACM New York,
NY, USA 2009, pp. 77‚Äì85.
and Siddharth Suri, ‚ÄúConducting behavioral research on Amazon‚Äôs Mechanical Turk,‚Äù Behavior research methods, 2012, 44 (1), 1‚Äì23.
Radanovic, Goran and Boi Faltings, ‚ÄúLearning to scale payments in crowdsourcing with properboost,‚Äù
in ‚ÄúFourth AAAI Conference on Human Computation and Crowdsourcing‚Äù 2016.
Robinson, Peter M., ‚ÄúRoot-N-Consistent Semiparametric Regression,‚Äù Econometrica, 1988, 56 (4), 931‚Äì
954.
Rogstadius, Jakob, Vassilis Kostakos, Aniket Kittur, Boris Smus, Jim Laredo, and Maja
Vukovic, ‚ÄúAn Assessment of Intrinsic and Extrinsic Motivation on Task Performance in Crowdsourcing Markets,‚Äù 2011.
Smith, Aaron, ‚ÄúGig work, online selling and home sharing,‚Äù Pew Research Center, 2016.
Sorokin, Alexander and David Forsyth, ‚ÄúUtility data annotation with amazon mechanical turk,‚Äù in
‚ÄúComputer Vision and Pattern Recognition Workshops, 2008. CVPRW‚Äô08. IEEE Computer Society Conference on‚Äù IEEE 2008, pp. 1‚Äì8.
Staiger, Douglas O, Joanne Spetz, and Ciaran S Phibbs, ‚ÄúIs there monopsony in the labor market?
Evidence from a natural experiment,‚Äù Journal of Labor Economics, 2010, 28 (2), 211‚Äì236.
Yin, Ming, Mary L. Gray, and Siddharth Suri, ‚ÄúRunning Out of Time: The Impact and Value of
Flexibility in On-Demand Work,‚Äù Under Review at The ACM CHI Conference on Human Factors in
Computing Systems 2018, 2018.

19

Appendix A

A Nested Logit Model of Mechanical Turk

Many crowdsourcing tasks and experiments that we will consider have a 2-part structure, which we model
via a nested logit. First a wage w1 is posted to draw subjects into an experimental protocol, then a second
wage w2 is randomized. Suppose agents face N HIT batches and each HIT batch i is made up of Ti individual
tasks, indexed by j, each of which pay can pay a different wage. Agents get an initial wage w1i and a task
specific wage w2ij . Utility from doing task j in batch i is (suppressing covariates):

Uij = Œ∑1 ln(w1i ) + Œ∑2 ln(w2ij ) + ij

(7)

 œÅi
 P
PTi

N
exp œÅiji
where ij has cumulative distribution F (ij ) = exp ‚àí i=1 ( j=1
. 1 ‚àí œÅi measures the
correlation in the errors within a batch i, but errors are independent across batches. Different batches can
have differing numbers of tasks Ti , each of which pays a different (‚Äúbonus‚Äù) wage wij , with differing elasticity
of substitution between the tasks within a batch œÅi .
Within a batch i, the probability of choosing task j is Pj|i =

e

Œ∑2 ln(w2 )
ji
œÅi

PTi
tion‚Äù elasticity is

k=1

Œ∑2
œÅi .

Œ∑2 ln(w2 )
ki
œÅi
e

Œ∑2
œÅ

i
w2ij

=P
Ti

k=1

Œ∑2
œÅ

so the ‚Äúreten-

wiki

Given the task-specific wages and number of tasks in the batch i, the probability of choosing HIT i is
PTi Œ∑œÅ2i œÅ
Œ∑
w1i1 (
w2ij ) i
j
Pi = P
.
Œ∑2
P
N

h=1

Œ∑

1(
w1h

Th

k=1

œÅ

i )œÅi
w2hk

For expositional ease suppose w2ij = w2i for all j within a batch i. We can represent our different
identification strategies in this setup. Taking logs we get an equation for the ‚Äúrecruitment‚Äù probability:

log(Pi ) = Œ∑1 log(w1i ) +

Œ∑2
1
log(w2i ) + log Ti + ¬µi
œÅi
œÅi

(8)

Where ¬µi is an error term that is possibly correlated with the other terms on the right hand side of the
equation. Note that the recruitment probability depends on the (possibly expected) second-stage wages and
number of tasks. Thus a simple regression of acceptance probability on posted batch wages would likely be
confounded by properties of the batch tasks and the bonus wages paid.
Taking a percent change approximation to the log we get:

Œ∑1 ‚âà

dPi
1
dlog(wi ) PÃÑi

20

(9)

and
dPj|i
1
Œ∑2
‚âà
¬Ø
œÅi
dlog(w2i ) Pj|i

(10)

Identification of Œ∑1 is obtained from 8 by a) using machine-learned functions to control for batch properties
(i.e. w2i , œÅi , and Ti as well as any other characteristics of the task that influence worker utility) in the doubleML approach and b) using the ‚Äúhoneypot‚Äù design described below to randomize w1i holding the batch
properties constant. Identification of

Œ∑2
œÅi

is obtained from the ‚Äúretention‚Äù experiments described below, with

possibly heterogeneous elasticities coming from differences in œÅi across batches. The nested logit structure
captures the difference between initial ‚Äúrecruitment‚Äù elasticities that measure the responsiveness of workers
into a HIT batch and secondary ‚Äúretention‚Äù elasticities that measure the completion of tasks within the
batch.
Consider a decision maker or researcher who needs a given volume of tasks (e.g., images labeled) that
they value at pi each. The requester must choose wages so as to maximize (pi ‚àíw1i ‚àíw2i )P (i|w1i )P (j|i, w2i ).
The optimal choice of the wages w1i and w2i are given by the first-order conditions to this problem, which
can be manipulated to give:
œÅi Œ∑ 1
w2i
Œ∑2
œÅi
=
œÅi Œ∑1 + Œ∑2

w1i =
pi ‚àí w1i ‚àí w2i
w1i + w2i

(11)
(12)

The first expression shows that it is difficult to isolate variation in w1i that is not also correlated with w2i ,
as both are determined by task productivity pi . Thus in tasks with a 2-part structure, it is important
to control for w2i when estimating Œ∑1 with variation in w1i . The second expression gives the percentage
gap between the value of the task and the total wage being paid, or markdown, as a variant of the Lerner
condition. As either Œ∑1 or

Œ∑2
œÅi

become large, the gap between wages and productivity goes to 0, indicating

perfect competition. In the crowdsourcing context, the recruitment and retention elasticities, together with
the assumption that requesters are optimizing, is a measure that simultaneously captures the distribution of
crowdsourcing value between requesters and labelers, as well as the inefficiencies that result.

Appendix B

Other Experiments Surveyed

We surveyed a large number of MTurk experiments, shown in 4. However, we did not include those that
did not randomize the wage within the same batch. In a large number of MTurk studies, researchers will

21

Study
Berinsky et al. (2012)

Included
No

Buhrmester et al. (2011)

No

Callison-Burch (2014)
Chandler and Horton (2011)
Crump et al. (2013)

No
No
No

Doerrenberg et al. (2016)
Dube et al. (2017)
Heer and Bostock (2010)

No
Yes
No

Ho et al. (2015)
Horton and Chilton (2010)

Yes
No

Horton et al. (2011)
Huang et al. (2010)
Hsieh and Kocielnik (2016)
Marge et al. (2010)

Yes
No
Yes
No

Mason and Watts (2009)
Rogstadius et al. (2011)

No
No

Sorokin and Forsyth (2008)

No

Yin et al. (2018)

Yes

Reason
HIT groups posted sequentially (not
randomized)
HIT groups posted sequentially (not
randomized)
Unable to obtain data
Unable to obtain data
HIT groups posted sequentially (not
randomized)
Piecemeal wage, non-honeypot setup
Replicated
HIT groups posted sequentially (not
randomized)
Randomized ‚Äúhoneypot‚Äù design
Labor supply elasticity (0.34) imputed,
not estimated directly
Replicated
Unable to obtain data
Randomized ‚Äúhoneypot‚Äù design
HIT groups posted sequentially (not
randomized)
Piecemeal wage, non-honeypot setup
Common HIT pool creates nonindependence of accept/reject decisions
HIT groups posted sequentially (not
randomized)
Randomized ‚Äúhoneypot‚Äù design

Table 4: All Experiments Surveyed
issue batches of HITs sequentially, with each batch being given a different wage4 The majority of these are
not randomized and thus we cannot use them to recover even quasi-experimental requester‚Äôs labor supply
elasticities. See Table 4 for full explanations of the inclusion/exclusion criteria for each study.

4 We examined the estimates in the following papers: Berinsky et al. (2012), Buhrmester et al. (2011), Crump et al. (2013),
Doerrenberg et al. (2016), Heer and Bostock (2010), Horton and Chilton (2010), Marge et al. (2010), Mason and Watts (2009),
Rogstadius et al. (2011), Sorokin and Forsyth (2008)

22

Figure 3: Sample interface page from Amazon Mechanical Turk

Appendix C

Observational Data Appendix

Figure 3 shows a sample of the MTurk interface for workers. We use two different scraping strategies. Section
Appendix C.1 describes data from Ipeirotis (2010) obtained via the Mechanical Turk Tracker API5 , and goes
from January 2014 through February 2016, when the account was ended by Amazon. Beginning in May
2016, we ran our own scraper, which took snapshots of all HITs available to a worker with a US address
every 30 minutes, though the frequency was increased to every 10 minutes beginning in May 2017. Data
from this latter scrape is described in Section Appendix C.2.

Appendix C.1

Data for January 2014 ‚Äì February 2016

Ipeirotis (2010) introduces the Mehcanical Turk Tracker, a web interface allowing researchers to view hourly
market data (e.g., number of HITs available) and demographic information (e.g., proportion of workers
who identify as male/female or who are from India/the United States) for the Amazon Mechanical Turk
marketplace. An Application Programming Interface (API) is provided alongside the web interface, allowing
for programmatic queries to be issued to the database. Using this API, we downloaded both ‚Äúcross-sectional‚Äù
data (e.g., requester name, title, description, keywords) and ‚Äútime series‚Äù data (number of HITs available
in the group for each run of the scraper) for 410,284 HIT groups. Of these, 125,337 were either posted
to the marketplace after February 1st, 2017 or had observations after this date, the date Amazon changed
5 https://crowd-power.appspot.com/#/general

23

(1)
2016-2017
mean
sd
Duration
Reward
Log Reward Prediction
Log Duration Prediction
Log Reward Residual
Log Duration Residual
Time Allotted

8

2.080 ¬∑ 10
70.397
3.427
6.222
0.002
‚àí0.012
3.573 ¬∑ 104

Observations

(2)
2014-2016
mean
sd
8

8

5.840 ¬∑ 10
92.420
1.394
1.263
0.501
1.420
1.750 ¬∑ 105

2.020 ¬∑ 10
38.014
2.634
5.221
0.004
‚àí0.015
4.668 ¬∑ 103

292746

(3)
2017
mean
8

5.650 ¬∑ 10
63.741
1.184
2.617
0.707
0.904
1.227 ¬∑ 104

sd
8

1.370 ¬∑ 10
61.774
3.282
5.321
0.003
‚àí0.017
2.607 ¬∑ 104

258352

5.030 ¬∑ 108
87.358
1.334
1.889
0.486
0.837
1.262 ¬∑ 105

93775

Table 5: The top panel presents from scraping MTurk between Jan. 2014 and Feb. 2016. The middle panel
presents analogous estimates using data obtained from May. 2016 through August 2017.

its interface and the scraper ceased working, and thus were dropped from our analysis. Of the remaining
284,947, we dropped any that had
‚Ä¢ Zero-valued reward,
‚Ä¢ Only one observation (since we‚Äôre unable to compute durations for these groups), or
‚Ä¢ Rewards or durations greater than the 99.5th percentile of their respective distributions (approx. 90,000
minutes for durations and $10.00 for rewards),
leaving us with 263,213 ‚Äúfinal‚Äù observations.

Appendix C.2

Data for May 2016 ‚Äì August 2017

At the beginning of the project in May 2016, we set up a scraper which would log in to MTurk as a user with
a US address and download all available information about each HIT group listed in the web interface as
shown in Figure 3. The scraper ran every 30 minutes (on the hour and on the half-hour) starting at midnight
EST on May 31st 2016, though this was increased to every 10 minutes beginning at midnight EST on May
31st 2017. The scraper was finally banned by Amazon on August 21st 2017 at 7:30pm EST. The every30-minute scrapes from May 2016 to May 2017 produced 363,181 observations, while the every-10-minute
scrapes from May 2017 to August 2017 produced 110,732.

24

Appendix D
Appendix D.1

Full Double-ML Procedure
Data Loading/Merging

For each of our three datasets, the initial data processing proceeded as follows. First, a scraped panel dataset
is loaded which contains, for each HIT group, the number of HITs available and the timestamp of each scrape
in which the group was observed. This panel data then gets collapsed into a cross-sectional dataset consisting
of several features derived from the distribution of the timestamps and HITs available ‚Äì for example, into
min(timestamp), max(timestamp), min(hits_available), and max(hits_available) for each HIT group. Then,
a separate cross-sectional metadata file (containing, for example, the titles, descriptions, and requester names
for each HIT group) is merged into the collapsed panel dataset via the unique Amazon-supplied group ID6 .

Appendix D.2

Data Cleaning

All observations with a reward greater than $5 or duration greater than 90,000 minutes (approximately two
months) are dropped7 . Then all observations with 0 reward or 0 duration values are dropped, to allow
transformation of the dependent variables into log space.

Appendix D.3

Feature Selection and Test/Training Split

We transform the text scraped with each HIT batch into a large number of text features as follows:
‚Ä¢ N-grams: An n-gram is an ordered sequence of n words. For example, if the full description for a
HIT is ‚Äúquick transcription task,‚Äù this will produce three 1-grams ‚Äúquick,‚Äù ‚Äúdescription‚Äù and ‚Äútask‚Äù;
two 2-grams ‚Äúquick description‚Äù and ‚Äúdescription task‚Äù; and a single 3-gram ‚Äúquick description task.‚Äù
We use sliding windows of 1 to 3 words over all words within the title, HIT description and keyword
list to form 1, 2 and 3-grams. The frequency of these n-grams in each HIT is then a feature used by
the ML algorithm. We use the standard English stopword list in Scikit-learn to eliminate stopwords.
‚Ä¢ Topic Distributions: Besides ordered sequence of words, sometimes sets of particular words (‚Äútopic‚Äù)
convey important information. A topic model is essentially an algorithm which searches for sets of words
that tend to occur together in a corpus. For example, one of our topics identifies the words ‚Äúimage,‚Äù
‚Äútext,‚Äù and ‚Äútranscribe‚Äù as its top words. HITs requesting transcription of text from an image will tend
6 This final cross-sectional file contains 411,196 observations for the Jan 2014 - Feb 2016 data, 363,181 for the May 2016 May 2017 data, and 110,732 for the May 2017 - Aug 2017 data, as described in the previous section.
7 These values correspond approximately to the 99.5th percentiles of the original distribution.

25

to have high feature values for this topic and lower values for other topics. The resulting features for
each HIT is then the distribution over topics found in that HIT‚Äôs title, description, and keyword list.8
We use the NLTK English stopword corpus to drop stopwords. The top 5 words for each topic model
run with K ‚àà {5, 10, 15, 20} are available online at textlab.econ.columbia.edu/topicwords.pdf.
‚Ä¢ Doc2Vec Embeddings: Unlike LDA which tries to generate features by splitting documents into
discrete human-interpretable topics, the goal of Doc2Vec is to generate a vector space in which vectors
for words which are semantically similar are close together, and then infer a document-level vector
within this same vector space via amalgamation of the learned vectors for its constituent words. For
example, since ‚Äúsurvey‚Äù and ‚Äúquestionnaire‚Äù are semantically similar in the sense that they are used in
similar contexts (‚Äúa short [survey/quetionnaire]‚Äù, ‚Äúfill out this [survey/questionnaire]‚Äù), their vectors
will be close together in the constructed vector space, and this will ‚Äúpull‚Äù the document-level vectors
for descriptions containing either word closer together.

9

‚Ä¢ Hand-Engineered Features: Finally, we use a set of custom regular-expression-based features,
which are generally binary variables describing the presence or absence of certain salient keywords
(e.g., ‚Äúsurvey‚Äù, ‚Äútranscribe‚Äù), but also real-valued variables capturing (for example) time estimates
given in the titles/descriptions (e.g., ‚Äú5-minute survey‚Äù). The bulk of these features are derived from
the explicit features described in Difallah et al. (2015), and the HIT taxonomy scheme developed in
Gadiraju et al. (2014), described more fully in Appendix Appendix D. The hand-engineered features
are as follows:
‚Äì Based on common patterns we observed in HIT titles, descriptions, and keywords, dummy
variables were created indicating the presence or absence of the following regular expressions:
easy, transcr* (capturing, e.g., ‚Äútranscription‚Äù or ‚Äútranscribe‚Äù), writ* (capturing, e.g., ‚Äúwritten‚Äù,
‚Äúwrite‚Äù, or ‚Äúwriting‚Äù), audio, image|picture, video, bonus, copy, search, ident* (capturing, e.g.,
‚Äúidentify‚Äù), text, date, fun, simpl*, summar*, only, improve, five|5, ?, and !.
‚Äì Based on the HIT taxonomy scheme developed in Gadiraju et al. (2014), a numerical category
was assigned to each HIT group via the following regular expressions:
‚àó Information Finding (IF): find
8 We

run a Latent Dirichlet Allocation (LDA) topic model model (Blei et al. (2003)) on all descriptions. LDA requires the
choice of a parameter K which determines how many topics the algorithm should try to discover: we estimate models with
K ‚àà {5, 10, 15, 20} .
9 We run Doc2Vec model Le and Mikolov (2014) on all titles, descriptions, and keywords in the data, producing a 50dimensional semantic information vector for each.

26

‚àó Verification and Validation (VV): check, match
‚àó Interpretation and Analysis (IA): choose, categor*
‚àó Content Creation (CC): suggest, translat*
‚àó Surveys (S): survey
‚àó Content Access (CA): click, link, read
‚Äì The following numeric features were extracted, some of which were derived from features used in
Difallah et al. (2015):
‚àó time_allotted: The time a worker is given to complete a given HIT
‚àó time_left: The time remaining before the HIT group expires (expired HIT groups are removed from the marketplace)
‚àó first_hits: The number of HITs initially posted to the marketplace
‚àó last_hits: The number of HITs remaining to be completed in the group at the time it was
last observed
‚àó min_hits: The minumum number of HITs available observed for the group across all scrapes
‚àó max_hits: The maximum number of HITs available observed for the group across all scrapes
‚àó avg_hitrate: The average rate (per hour) at which HITs within the group were filled by
workers
‚àó avg_hits_completed: The average change in available HITs between subsequent observations
of the group
‚àó med_hits_completed: The median change in available HITs between subsequent observations
of the group
‚àó min_hits_completed: The minimum change in available HITs between subsequent observations of the group
‚àó max_hits_completed: The maximum change in available HITs between subsequent observations of the group
‚àó num_zeros: The number of observations for which the number of available HITs in the group
was listed as 0
‚àó req_mean_reward: The average reward over all HITs posted by the requester
‚àó req_mean_dur: The average duration of all HIT groups posted by the requester

27

‚àó title_len: The length of the HIT group‚Äôs title
‚àó desc_len: The length of the HIT group‚Äôs description
‚àó keywords_len: The sum of the lengths of the HIT group‚Äôs keywords
‚àó num_keywords: The number of keywords given for the HIT group
‚àó title_words: The number of words in the HIT group‚Äôs title
‚àó desc_words: The number of words in the HIT group‚Äôs description
‚àó minutes_title: The number of minutes if a phrase including ‚ÄúX minutes‚Äù appears in the
title
‚àó minutes_desc: The number of minutes if a phrase including ‚ÄúX minutes‚Äù appears in the
description
‚àó minutes_kw: The number of minutes if a phrase including ‚ÄúX minutes‚Äù appears in the keyword list
‚àó qual_len: The length of the string given in the HIT group‚Äôs description which lists the
qualifications
‚àó num_quals: The number of qualifications required for the HIT group
‚àó custom_not_granted: The number of custom qualifications required for the HIT group for
which our ‚Äúblank‚Äù account (an account which had never accepted or completed a HIT) was
not qualified
‚àó custom_granted: The number of custom qualifications required for the HIT group for which
our ‚Äúblank‚Äù account (an account which had never accepted or completed a HIT) was qualified
‚àó any_loc: A dummy variable representing whether or not the HIT group had a location
restriction (e.g., US only)
‚àó us_only: A dummy variable which is 1 if the HIT group is restricted to US workers, and 0
otherwise
‚àó appr_rate_gt: The lower bound on approval rate required for workers to be eligible for the
HIT, coded as -1 if no lower bound was enforced
‚àó rej_rate_lt: The upper bound on rejection rate required for workers to be eligible for the
HIT, coded as 101 if no upper bound was enforced
‚àó appr_num_gt: The lower bound on number of approvals required for workers to be eligible
for the HIT, coded as -1 if no lower bound was enforced
28

‚àó rej_num_lt: The upper bound on number of rejections required for workers to be eligible for
the HIT, coded as 999 if no upper bound was enforced
‚àó adult_content: A dummy variable which is 1 if the HIT group indicated that it contained
adult content, and 0 otherwise
To save on computation time, we utilized a ‚Äútwo stage‚Äù double ML procedure as outlined in Section
3.3.2. Given the initial split of the data into A and B sets, and the subsequent split of these sets into Atrain ,
Aval , Btrain , and Bval , a given run of our procedure (A ‚Üí B or B ‚Üí A; here we describe the A ‚Üí B run
without loss of generality) proceeds as follows. First, in the ‚Äúfeature selection‚Äù phase, the full set of n-gram
features were generated as described in Section 3.3.1, and a ‚Äúpreliminary‚Äù run of the learning algorithm was
performed using only these n-gram features as the feature matrix for Atrain , with the goal of predicting the
reward and duration values in Aval . Upon completion of this stage, we ‚Äúthrew away‚Äù all but the top 100
most predictive n-gram features for reward and top 100 most predictive n-gram features for duration, and
for the remainder of the run only those n-gram features were included. For illustration, the top 100 most
predictive reward and duration features for the A ‚Üí B run on the Jan 2014 - Feb 2016 data were as follows:
Once this feature selection phase was complete, a second ‚Äúfull data‚Äù phase was performed, as outlined
in Section 3.3.2. In this phase, the top 200 n-gram features from the feature selection phase are included
in the feature matrix for A along with the LDA, Doc2Vec, and hand-engineered features, with the goal of
predicting the reward and duration values in B.

Appendix D.4

Regression via Random Forests

Before computing predictions on the test set, the validation set was used to tune not only hyperparameters
but also which learning method was chosen. Random forest regression, implemented by RandomForestRegressor in scikit-learn, greatly outperformed the other classifiers we employed: {AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor, SVR (SupportVectorRegressor)}, and thus a trained random forest regression was our choice for computing predictions for
the test data. The random forest method constructs a series of individual decision tree estimators, where
each regressor is trained on a subset of the full feature set, and then reports the mean prediction over all
regressors. Based on two additional cross-validation procedures, for the first-stage feature selection an random forest regression with 40 decision tree estimators was used (with the number of estimators optimized
over {10, 20, . . . , 100}) while for the second-stage ML the model was run with 600 estimators (optimized

29

2
Rreward
2
Rduration

Jan 2014 - Feb 2016
A‚ÜíB
B‚ÜíA

May 2016 - May 2017
A‚ÜíB
B‚ÜíA

May 2017 - Aug 2017
A‚ÜíB
B‚ÜíA

0.7527
0.8937

0.8878
0.4485

0.8872
0.5377

0.7571
0.8944

0.8861
0.4479

0.8832
0.5304

Table 6: R2 scores for each run of the Double-ML regressions
over {100, 200, . . . , 1000}, with the increased order of magnitude made feasible due to the fact that in the
second stage all but 200 of the approximately 800,000 total n-gram features are dropped).

Appendix D.5

Computing the Double-ML estimate

Once the ML algorithm has finished its runs and the predicted log duration and log reward values have been
generated for each fold of the data, the estimated Œ∑ value is computed straightforwardly via Equation 5 with
n = 2.

30

