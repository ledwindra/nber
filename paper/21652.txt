NBER WORKING PAPER SERIES

THE EFFECT OF PUBLIC FUNDING ON RESEARCH OUTPUT:
THE NEW ZEALAND MARSDEN FUND
Jason Gush
Adam B. Jaffe
Victoria Larsen
Athene Laws
Working Paper 21652
http://www.nber.org/papers/w21652

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
October 2015

This project grew out of a suggestion made by Dean Peterson, formerly of the Royal Society of New
Zealand. We are grateful to Ryan Burnell for compiling the publications information and to Dave Maré
and Shaun Hendy for advice. Useful comments were provided by Ariel Dora Stern, Bronwyn Hall
and seminar participants at University of Melbourne, University of Waikato, the New Zealand Economics
Association, NBER Summer Institute and Otago University. Financial support from the Motu Research
and Education Foundation, Queensland University of Technology and the Ministry of Business Innovation
and Employment is gratefully acknowledged. All errors and opinions belong solely to the authors.
The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.
At least one co-author has disclosed a financial relationship of potential relevance for this research.
Further information is available online at http://www.nber.org/papers/w21652.ack
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2015 by Jason Gush, Adam B. Jaffe, Victoria Larsen, and Athene Laws. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.

The Effect of Public Funding on Research Output: the New Zealand Marsden Fund
Jason Gush, Adam B. Jaffe, Victoria Larsen, and Athene Laws
NBER Working Paper No. 21652
October 2015
JEL No. O31,O34,O38
ABSTRACT
We estimate the impact of participating in the NZ Marsden Fund on research output trajectories, by
comparing the subsequent performance of funded researchers to those who submitted proposals but
were not funded. We control for selection bias using the evaluations of the proposals generated by
the grant selection process. We carry out the analysis in two data frames. First we consider the researcher
teams behind 1263 second-round proposals submitted 2003-2008, and look at the post-proposal publication
and citation performance of the team as a whole, as a function of pre-proposal performance, the ranking
of the proposal by the panel, and the funding. This estimation does not deal with individual researchers’
multiple proposals and funding over time. To disentangle these effects, we consider the 1500 New
Zealand researchers who appeared on any of these proposals, and estimate a model predicting annual
individual performance as a function of previous performance, recent proposal activity, ranking of
any recent proposals, and funding received through recent proposals. Overall, we find that funding
is associated with a 6-15% increase in publications and a 22-26% increase in citation-weighted papers
for research teams. For individuals, funding is associated with a 3-5% increase in annual publications,
and a 5-8% increase in citation-weighted papers for 5 years after grant; however, the lag structure
and persistence of this effect post-grant is difficult to pin down. Surprisingly, we find no systematic
evidence that the evaluation of proposals by the Marsden system is predictive of subsequent success.
We conclude that the Marsden Fund is modestly successful in increasing scientific performance, but
that the selection process does not appear to be effective in discriminating among second-round proposals
in terms of their likely success.

Jason Gush
Royal Society of New Zealand
11 Turnbull St, Thorndon
Wellington 6011
New Zealand
jason.gush@royalsociety.org.nz

Victoria Larsen
University of Otago
362 Leith Street
North Dunedin, Dunedin 9016
New Zealand
larvi165@student.otago.ac.nz

Adam B. Jaffe
Motu Economic and Public Policy Research
PO Box 24390
Wellington 6142
New Zealand
and Queensland University of Technology
and also NBER
adam.jaffe@motu.org.nz

Athene Laws
Motu Economic and
Public Policy Research
PO Box 24390
Wellington 6142
New Zealand
athene.laws@gmail.com

Contents
1.

Introduction ......................................................................................................................................... 4

2.

Background .......................................................................................................................................... 5

3.

4.

2.1.

Previous work ................................................................................................................. 5

2.2.

Institutional Setting ........................................................................................................ 7

Analysis of performance of proposal teams .................................................................................... 8
3.1.

Regression model ........................................................................................................... 8

3.2.

Regression results ......................................................................................................... 10

3.3.

Regression discontinuity estimates ............................................................................ 19

Modelling performance of individual researchers ........................................................................ 19
4.1.

Regression Model ......................................................................................................... 19

4.2.

Regression results ......................................................................................................... 22

4.3.

Variations and Robustness ......................................................................................... 29

5.

Summary and Conclusion ................................................................................................................ 32

6.

References .......................................................................................................................................... 35

Appendix .................................................................................................................................................... 37

Tables
Table 1: Summary statistics on proposal-team dataset ........................................................................ 11
Table 2: OLS Log-Log baseline regressions for proposal-team data ................................................ 12
Table 3: Negative Binomial baseline regressions for proposal-team data ........................................ 15
Table 4: Negative Binomial Regression variants on proposal-team data ......................................... 17
Table 5: Summary statistics on investigator-panel dataset .................................................................. 22
Table 6: Baseline results investigator-panel data .................................................................................. 23
Table 7: Comparison of selection metric – Zero inflated negative binomial regressions on
investigator-panel data ................................................................................................................ 30
Table 8: Exploration of lag pattern – ZINB regressions on investigator-panel data ..................... 31

Figures
Figure 1: Relationship between residuals and scaled rank .................................................................. 13
Figure 2: Frequency distribution of preliminary (1st stage) proposals, full (2nd stage) proposals and
contracts by investigator, 2000-2012 ........................................................................................ 20
Figure 3: Hypothetical researcher under three funding scenarios ..................................................... 28

3.

1. Introduction
There is a long history of programme evaluation in worker training and health delivery
settings, but systematic evaluation of the effectiveness of government research support
programmes is rare (Jaffe, 2002). While many research organizations tout the successes
associated with their research grants, few make any serious effort to calculate how this rate of
success compares to what would have occurred absent the programme’s support, i.e. to calculate
the “treatment effect” associated with receiving funding. Doing so requires comparing the
success of grantees with that of a control group, and since the grants are explicitly made to those
potential recipients judged most likely to succeed, any such comparison must control for
selection bias.
The Marsden Fund is the premiere funding mechanism for basic research in New
Zealand. It is funded by the government, but selection and administration is delegated to the
Royal Society of New Zealand (RSNZ). With the intent to evaluate the Fund, the RSNZ has
maintained records of the researchers associated with both successful and unsuccessful
proposals, as well as the evaluation metrics (external referee scores, panel scores and panel ranks)
of all of those proposals by the expert panels that form the basis of the funding decisions. This
allows us to estimate the impact of receiving funding while controlling, via the evaluation scores,
for the selectivity bias, separating the overall difference in success between the funded and
unfunded teams into a selection effect and a treatment effect.
We look at the effects of the grant process on research output in two ways. First, we
consider the researcher teams and look at the post-proposal publication and citation performance
of the team as a whole, as a function of pre-proposal performance, the ranking of the proposal
by the panel, and the funding received. We do this both with OLS and count model estimation
of a parametric functional form for the relationship between research outputs and prior research
outputs, Marsden funding, and the panel ranking, and with a non-parametric regression
discontinuity design, simply comparing the success of the proposals just above the funding
cutoff with the success of those just below the cut-off. Aggregating impacts in this way across
team members and across years mitigates the inherent noisiness of publication output, and is
robust to variations in the time lags between grant activity and scholarly output. But this
approach cannot sort out the effect of multiple interactions with the Marsden programme on
researchers’ scholarly output over time. The average researcher on these teams made 6
proposals and received 1.2 grants 2000-2012. When we look at the post-proposal performance
of a team—whether funded or not in a given round—it will typically include researchers who
were also on other current or subsequent proposal teams, some of which were funded and some
of which were not. To disentangle these effects, we consider all researchers who appeared on
4.

any of these proposals, and assemble their publications/citations, proposals, the evaluation of
those proposals, and funding decisions on an annual basis. We estimate models on this panel
predicting annual researcher performance as a function of previous performance, recent
proposal activity, Marsden evaluations of any recent proposals, and funding received through
recent proposals.
As a side-effect of estimating the parametric model of the selection and treatment effects,
we learn something about how well the Marsden evaluation process predicts the future success
of the proposals. The results suggest that receiving funding through the Marsden Fund does
increase the success of the funded proposals, but that the various evaluations used to select
funding recipients are themselves not systematically correlated with proposals’ or researchers’
subsequent performance. In other words, the study was constructed to separate the treatment
effect from the selection effect, but in these data the selection effect does not seem to be
present.

2. Background
2.1.

Previous work
Arora et al. (2000) present a structural model with endogenous decisions by applicants

and the granting agency. It evaluates the impact of funding from an Italian biotechnology
programme using a large and comprehensive dataset of proposals to the biotechnology and bioinstrumentation programme from 1989 to 1993, using impact-factor-weighted publications as the
measure of output. It found an estimated structural elasticity of research output with respect to
the granted budget of about 0.6 but varying depending on research characteristics. This elasticity
is estimated taking into account that previously successful researchers are more likely to be
funded and, knowing this, request larger budget allocations.
Arora & Gambardella (2005) study the impact of National Science Foundation (NSF)
funding on economics researchers in the USA, comparing the research output of successful and
unsuccessful applicants in a difference-in-difference framework. Using data from 1473
applications to the NSF from 1985 to 1990, they find that funding is associated with increased
research output for younger economists, but not for senior applicants.
Jaffe (2002) (which was actually written after Arora and Gambardella (2005)) discusses
the practicalities of different approaches to measuring the impact of public funding in the
presence of selection bias, using as a benchmark the possibilty of a randomized control trial
(“RCT”). It suggests that under typical grant programme circumstances, the regression-

5.

discontinuity approach introduced by Thistlethwaite & Campbell (1960) provides almost as good
an estimate of the funding effect as the realistic RCT formulation.
A closely related methodology was used by Jacob & Lefgren (2011) to evaluate the
impact of funding by the U.S. National Institute of Health (NIH) on research output. They
compared successful and unsuccessful research proposals to the NIH from 1980 to 2000, using
the priority scores from independent scientific reviews to control for selection bias. The NIH
allocates funding in such a way that there is a highly non-linear relationship between priority
scores and likelihood of being funded. This non-linearity is exploited in an instrumental variables
framework to estimate the effect of funding purged of selectivity bias. Funding is associated with
a 7% increase in publications over the 5-10 years post-funding. The IV and OLS estimates are
qualitatively similar, suggesting that within this group of applying researchers the selectivity bias
is not large. The authors argue that the small estimated effect is consistent with a model where
the loss of an NIH grant is simply replaced by other sources of funding. They explore how being
funded by the NIH affects future funding, and how non-NIH funded researchers are funded
elsewhere. They find that most non-NIH funded researchers are funded elsewhere, and those
who get NIH funding are less likely to receive NSF funding. This highlights the difference
between the direct effect of a particular funding programme and its effect when viewed in
interaction with other programmes. We return to this issue below.
Li and Agha (2015) looked at scientific outputs (papers, citations, patent citations)
associated with more than 130,000 research project (R01) grants funded by the U.S. National
Institutes of Health from1980 to 2008. It finds that better peer-review scores are consistently
associated with better research outcomes and that this relationship persists even after controlling
for an investigator’s publication history, grant history, institutional affiliations, career stage, and
degree types. A one–standard deviation worse peer-review score among awarded grants is
associated with 15% fewer citations, 7% fewer publications, 19% fewer high-impact publications,
and 14% fewer follow-on patents. For high-impact publications, they try counts of publications
in the top 5%, 1% and .1% of all publications in the same year; they find predictive power for
the referee score at all of these levels. This is the largest and most thorough study we identified
of the predictive value of referee scores. It shows effects that are of modest but important size.
Of course, in looking only at successful proposals, this work says nothing about the treatment
effect.
Moving to the New Zealand and in particular the Marsden Fund context, a bibliometric
analysis of the Fund was undertaken in 2001 comparing the number and impact of publications
of Marsden Funded research teams to non-Marsden Funded publications (Knox (2004)). Across
all fields, over the period 1997 to 2001 the Marsden Funded share of New Zealand authored
6.

publications rose from 2.2% to 7.7%. Marsden Funded publications were on average cited 1.7
times more than the non-Marsden Funded counterparts. As noted above, however, a study of
this kind does not distinguish the extent to which the Marsden Fund is good at identifying the
best research from the extent to which the receipt of funding increases research output.

2.2.

Institutional Setting
The Marsden Fund was established in 1994 by the New Zealand Government to support

New Zealand science on a competitive basis1. The fund is named after Sir Ernest Marsden, a
prominent New Zealand born researcher who inspired physicist Ernest Rutherford to continue
research into the structure of the atom. In 2013, $67.9 million was allocated in research grants
through the Marsden Fund.2 In a typical year, the Marsden Fund receives approximately 1000
proposals, and funds around 100 of those. The government delegates the administration of the
programme, including the selection of grantees from among applicants, to the Royal Society of
New Zealand (RSNZ), which was modelled after the Royal Society in Britain and is loosely
analogous to the National Academy of Sciences (NAS) in the U.S. In this, the RSNZ acts on
behalf of the Government-appointed Marsden Fund Council.
Proposal review is carried out by assessment panels of between 5 and 10 members
appointed by the Royal Society. The overall budget is allocated to each panel by the RSNZ.
Given the small size of the programme, each of these panels is rather broad in coverage; for
example one is “Physical Sciences and Engineering” and one is “Economics and Human
Behaviour.” A list of the panels and their sizes over 2003-2008 is presented in the Appendix.
A proposal research team is made up of as few as one researcher, or as many as 8. A
team can be made up of Principle Investigators, Associate Investigators, Post Docs, Research
Assistants/Technicians, and Post Graduate Students. The budget specifies the fraction of full
time (FTE) that each investigator proposes to devote to the project.3 There are two types of
grant given by the Marsden Fund. The standard grant is for any research team, and can run for
up to three years. The maximum budget varied slightly by year and panel, but was on the order
of NZD 300,000 per year.4 Applicants within 7 years of their PhD award have the option of
apply for a “Fast-Start” (“FS”) grant, which is limited to NZD 100,000 per year. FS proposals
are ranked against other FS applicants rather than being compared with the general pool. Each
panel decides how much of its allocated budget to use for FS and how much for the standard
grants.
http://www.royalsociety.org.nz/programmes/funds/marsden/about/background/
http://assets.royalsociety.org.nz/media/2014/07/Profiling-Excellence-2013-web.pdf
3 Proposals can and do include investigators with zero FTE, i.e. they are associated with the project but will not be
paid from the budget. In particular, non-New Zealand researchers can be included in proposals but cannot be paid
from the proposal budget.
4 The NZD is worth approximately US$0.65.
1
2

7.

The application process has two stages. A one-page initial proposal is reviewed by the
panel without benefit of external referees. Each proposal is reviewed by a subset of the panel and
given a preliminary score, based on the merit of the proposal and the potential of the project
team. On this basis, the panels reject 71-84 percent of the proposals, with each panel deciding
internally how many proposals to advance to the second stage. In the second stage, longer
proposals are submitted and sent to external (including international) anonymous referees for
review. Proponents are given an opportunity to respond to referee comments, and then the
panels score these proposals based on the referee reports, the proponents’ responses, their own
judgment, and discussion within the panel. Each panel chooses a cutoff for regular proposals
and a cutoff for Fast-start proposals, such that funding the proposals with rank above that cutoff
fits within the allocated budget.

3.

Analysis of performance of proposal teams

3.1.

Regression model
This analysis is based on 1263 Marsden proposals from the second round reviews for the

years 2003-2008. Overall, 41% of the proposals were funded. About 25% of the proposals were
Fast-Start proposals, and of these slightly more than half were funded. We measure the research
success of each proposal team by identifying all of the publications of all team members, and all
citations received by those publications, from 1995 through 2012. This information was
collected from Scopus based on the researchers listed in the proposals appearing as publication
authors. Thus we are not making any attempt to identify publications specifically related to the
research as described in the proposal. We are simply investigating the relationship between
researchers’ overall research output and their participation in the Marsden process.
For each proposal i submitted in year t, we consider the prior success of that research
team to be proxied by their publications (or citations) received from 1995 until year t-1, and the
subsequent success to be proxied by their publications (or citations) received from year t+1
through 2012. We capture the panel’s subjective evaluation of each proposal (and control for
selection bias) using its “scaled rank,” defined as 1 minus the ratio of the integer ranking
assigned by the panel (where the best proposal is assigned rank 1) to the total number of
proposals ranked by the panel (i.e. lowest rank for that panel). Thus scaled rank is 0 for the
lowest rank proposals and approaches unity (1-1/n) for the top ranked proposals. This scaling
allows the rank to be comparable across panels in which different numbers of proposals were
evaluated.
Our core analysis is based on the following variables:

8.

Dependent variables
Publications (annual)

The number of publications authored by members of the research team
after the proposal, scaled by the number of years until 2012. i.e.

Citations (annual)

The number of citations to publications authored by members of the
research team after the proposal, scaled by the number of years until
2012. i.e.

Log[(publications + 1)]

The natural log of post proposal publications as follows:

Log[(citations + 1)]

The natural log of post proposal citations as follows:

Independent variables
Log(Past performance+1)

The natural log of publications or citations to publications authored by
members of the research team before the proposal as follows:

Log(Past performance)

The natural log of publications or citations to publications authored by
members of the research team before the proposal, scaled by the
number of years since 1996. If past publications/citations equals zero,
this is set to zero. i.e.
if
0

Dummy: Past
performance=0

if

Dummy equals 1 if past performance (publications or citations) is zero.
A dummy correction for Log(Past performance) to distinguish between
those with no publications/citations and those with an average of one
per year (Log1=0).

Funded

A dummy variable equal to 1 if the proposal was funded.

Fast-Start

A dummy variable equal to 1 if the proposal was a Fast-Start proposal.

FS*Funded

An interaction term between ‘Funded’ and ‘Fast-Start’

Scaled Rank

The panel rank the proposal received during the second round scaled
by panel size. Ranges to 0 (lowest ranked) and approaches 1 (highest
ranked)

Average referee score

The average external referee score received by the proposal during the
second round. Ranges from 1 (top 5% proposals) to 5 (below average).

9.

Log(FTE)

The natural log of the full time equivalent (FTE) of proposal
investigators recorded in the proposal.

Budget (NZ$million)

The size of the funded budget (in millions of NZ dollars).

FS*budget

An interaction term between ‘Fast-Start’ and ‘Budget’.

Subsequent contract

A dummy variable equal to 1 if any member of the proposal team
received funding from any year t+1 to 2012.

Overdispersion: lnalpha

The natural log of alpha - the over-dispersion parameter associated
with negative binomial regressions. Alpha equals 0 in a Poisson
regression.

Panel Dummies

Nine panel dummies: equals 1 if proposal i is submitted to the
corresponding panel. ‘BMS’ is omitted from regressions.

Time dummies

Six year dummies: equals 1 if t equals the corresponding year. 2003 is
omitted from regressions.

By including previous performance in the regression, we model success as likely to be
persistent over time, with participants being on different success trajectories, but with the
possibilities that the Marsden process shifts that trajectory. By including a dummy for Fast-Start
proposals, we allow for the possibility that these (younger) investigators are on different
trajectories than the average “Standard” proposal team. By including the interaction term
between the Fast-Start dummy and the Funding dummy, we allow for the possibility that these
younger investigators benefit differentially from the receipt of funding.
Panel dummies allow for the overall rate of publication and citation to differ across
disciplines, and time dummies absorb the fact that the post-application intervals are of different
durations for proposals from different years. We thereby assume that the proportional impact of
Marsden funding is the same across fields and across observation periods of differing length.
Finally, we include the dummy variable for subsequent funding because we are not attempting to
identify the extent to which subsequent publications are directly tied to the research funded in
the proposal whose effects we are trying to measure. For any given competitive round, there will
be researchers, both among those funded and those rejected, who subsequently received funding
from the Marsden programme in a subsequent competitive round. Since we measure publication
success for all teams out to 2012, we would expect that this subsequent proposal activity could
also be reflected in the overall subsequent performance of the teams.

3.2.

Regression results
Descriptive statistics for the regression variables are presented in Table 1. Baseline OLS

regression results using the log of performance-plus-one (scaled by the number of years pre or
10.

post submission) are presented in Table 2. The interpretation of all of the variable coefficients is
the percentage increase in the performance measure, regardless of whether the base for that year
and discipline is high or low. We add one to the publication and citation counts before taking
logs because a very small number of observations have zero publications or zero citations either
before or after the proposal round. We have also estimated the OLS log-log equations dropping
the observations with zeros and the results are almost identical to those reported.
Table 1: Summary statistics on proposal-team dataset
Variable
Past publications
Future publications
Past citations
Future citations
Future publications per year
Past publications per year
Future cites per year
Past cites per year
Log(past pubs per year)
Log(past cites per year)
Scaled rank
Funded
Fast Start
FS*Funded
Subsequent contract
Investigator FTE
budget
budget (Funded subsample)
budget (Funded FS subsample)

Mean
Std.Dev. Min
Max
70.96041
71.26
0
>500
70.72051 66.57817
0
>400
2829.276 3594.865
0 >25000
1054.112 1480.084
0 >14000
11.31195 10.08695
0
63
7.305418 7.027658
0 57.18182
161.5095 203.414
0 1849.125
292.7203 365.6876
0
2839
1.442799 1.231489 -2.48491 4.046236
4.619873 1.979748 -2.48491 7.951207
0.458334 0.28851
0 0.966667
0.408551 0.491761
0
1
0.249406 0.432841
0
1
0.122724 0.32825
0
1
0.335709 0.472425
0
1
1.41563 1.003453
0
9.27
0.180517 0.268883
0 1.243264
0.441847 0.247939 0.041479 1.243264
0.138839 0.055838 0.041479 0.302222

Percentage zeros
No past publications
No past citations
No future publications
No future citations

% of teams
3.40%
3.64%
1.58%
3.17%

Columns 1 and 4 present the simplest test for a funding effect, without attempting to
correct for selection bias. They indicate that funding is associated with an increase in
publications of about 6% and citations about 12% relative to what would have been predicted
based on previous performance. The coefficient on previous performance is approximately .75,
and highly significant statistically, indicating that there is significant persistence in success but
with some regression to the mean (coefficient less than unity).

11.

Columns 2 and 5 include the addition of the proposal’s scaled rank, the Fast-Start
dummy, the interaction term between Fast-Start and Funding, and the dummy for subsequent
funding. The treatment effect associated with funding is increased to 15% for publications and
26% for citations. Fast-Start teams are associated with about 16% greater research output
(controlling for pre-proposal performance), consistent with these younger investigators being, on
average, on a steeper upward output trajectory than other researchers. The interaction term
between Fast-Start and Funding is, however, essentially zero, indicating that there is no
observable tendency for these younger researchers to benefit differentially from the receipt of
funding.
Table 2: OLS Log-Log baseline regressions for proposal-team data
VARIABLES
Log(Past Performance + 1)
Funded

(1)
Pubs
0.766***
(0.0144)
0.0638**
(0.0291)

Fast-Start
FS*Funded
Subsequent contract
Scaled rank
Constant
Observations
R-squared

0.554***
(0.0627)
1,263
0.822

(2)
Pubs

(3)
ΔPubs

0.787***
(0.0156)
0.145***
0.126**
(0.0516)
(0.0553)
0.154*** 0.329***
(0.0465)
(0.0480)
0.0327
0.0572
(0.0645)
(0.0691)
0.160*** 0.0912***
(0.0328)
(0.0348)
-0.214*** -0.217**
(0.0798)
(0.0856)
0.444***
0.0926
(0.0658)
(0.0650)
1,263
1,263
0.829
0.186

(4)
Cites

(5)
Cites

(6)
ΔCites

0.733***
(0.0149)
0.124***
(0.0456)

0.734***
(0.0155)
0.264***
(0.0807)
0.159**
(0.0716)
-0.0242
(0.101)
0.338***
(0.0513)
-0.293**
(0.125)
0.583***
(0.125)
1,263
0.847

0.255***
(0.0897)
0.412***
(0.0779)
-0.00149
(0.112)
0.222***
(0.0565)
-0.360***
(0.139)
-0.806***
(0.106)
1,263
0.135

0.755***
(0.121)
1,263
0.840

Time and panel dummies included in all regressions
Standard errors in parentheses
*** p<0.01, ** p<0.05, * p<0.1

The positive coefficient on the subsequent funding dummy is quite significant
statistically. On average proposal teams (both those funded in the current round and those
denied in the current round) that had a team member who received funding in some subsequent
round received about 17% more publications and 35% more citations than those that did not. It
was this finding that led us to the investigator-year model discussed below.
The surprising result in columns 2 and 5 is that the coefficient on scaled rank is negative.
This means that, controlling for the other regressors—including the effect of the funding itself—
proposal teams that were highly ranked by the RSNZ panels actually performed worse than
12.

those that were ranked lower. Specifically, because the rank is scaled so that it is roughly one for
the best-ranked proposal and zero for the worst, the coefficient of -.2 to -.3 means that the worst
ranked proposal team got 20-30% more output than the best team, after controlling for all other
attributes, including previous performance.5
Figure 1: Relationship between residuals and scaled rank

The combination of the funding and rank effects is shown graphically in Figure 1. To
construct this Figure, the log-citations variable was regressed on year dummies, field dummies,
and previous log-citations. The Figure then plots the residuals from this regression by scaled
rank, and distinguishes those proposals that were funded from those that were not. These
plotted points are therefore the post-proposal performance of each team, after taking out the
effects of previous performance, year and discipline. The pattern is not particularly
distinguishable by eye because the effect is small and the scatter is large, but the funded points

5 If previous performance is excluded from the regression, the coefficient on scaled rank is essentially zero,
suggesting that the negative estimate conditional on previous performance may reflect a tendency for panels to
overweight past performance in predicting future performance. In order to ensure that the estimated lack of any
positive selection effect is not an artifact of estimating the treatment and selection effects together, we also tried
estimating the regressions in columns 2 and 5 of Table 2 separately for the unfunded and funded groups. The
coefficient on scaled rank is negative for both groups in both regressions, but statistically significant only for the
unfunded teams.

13.

are, on average, slightly higher than the unfunded points, and, at the same time, within both the
funded and unfunded groups there is a slightly downward trend with rank.6
Columns 3 and 6 of Table 2 take the difference in the log-performance measure before
and after the proposal round as the dependent variable. This corresponds to the regression in
the previous column, except that the coefficient on previous performance is constrained to be
unity. Since this constraint is strongly rejected by the data, we prefer the formulation where the
relationship between ex post and ex ante performance is allowed to be determined by the data.
But we show the constrained version as well because it corresponds to the “difference in
difference” estimator of the funding effect, which is frequently used in this context (e.g. Arora
and Gambardella, 2005). The estimated effects are all qualitatively similar to those in the
previous column, with the exception of the Fast-Start effect, which is about twice as big in the
difference formulation. This makes sense: the Fast-Start applicants have lower previous
performance; when we constrain previous performance to have a unitary impact on subsequent
performance their ex post performance is under-predicted; the increase in the Fast-Start dummy
compensates for this and so fits the data better.
Table 3 converts these regressions into a more formal count data framework. The
dependent performance variable is left in raw form – count of publications or citations to these
publications, scaled by the number of years until 2012. The scaling factor does result in some
non-integer dependent variable observations which is not strictly a count. However, the models
perform well, and produce similar results to unscaled counts.7 For the independent variable, we
take the natural log of past performance and set this equal to zero if past performance equals
zero. To distinguish between observations with past performance equals one (log1=0) and those
where the log has been manually set to zero, we include a dummy correction equal to one if past
performance is zero. It was found that conditional over-dispersion in our data warranted the use
of a negative binomial model rather than a Poisson model, as the latter constrains the variance to
equal the mean. While not precisely correct, the resulting coefficients are approximately equal to
percentage effects for small changes.

There are visible in the figure a handful of zero-rank proposals that were nonetheless funded. This is
because the Fast-Start proposals are ranked separately from the regular proposals, and in a couple of instances
panels received very few Fast-Start proposals and chose to fund them all.
7 Horton, Kim and Saitz (2007) have argued that non-integer scaled values for dependent variables do not
create a practical problem for count model estimation.
6

14.

Table 3: Negative Binomial baseline regressions for proposal-team data
VARIABLES
Count regression:
Log(past performance)
Dummy: Past performance=0
Funded

(1)
Pubs

0.727***
(0.0155)
-1.947***
(0.279)
0.0256
(0.0256)

-2.555***
(0.0928)
1,263
-3367

-2.594***
(0.0941)
1,263
-3351

Scaled rank
Subsequent contract

Observations
Log likelihood

(4)
Cites

0.787***
(0.0599)

FS*Funded

Overdispersion: lnalpha
Constant

(3)
Cites

0.747*** 0.636*** 0.663***
(0.0167)
(0.0151)
(0.0162)
-1.990*** -1.472*** -1.421***
(0.279)
(0.235)
(0.232)
0.0644
0.159*** 0.222***
(0.0454)
(0.0417)
(0.0730)
0.132***
0.170**
(0.0460)
(0.0668)
0.0401
0.113
(0.0645)
(0.0936)
-0.114
-0.237**
(0.0722)
(0.115)
0.0863***
0.313***
(0.0291)
(0.0452)
0.695*** 1.720*** 1.371***
(0.0638)
(0.116)
(0.124)

Fast-Start

Constant

(2)
Pubs

-0.779*** -0.840***
(0.0413)
(0.0416)
1,263
1,263
-6669
-6632

Time and panel dummies included in all regressions
Standard errors in parentheses
*** p<0.01, ** p<0.05, * p<0.1

Many of these regression findings are comparable to those from Table 2. Performance
demonstrates persistent but mean reverting success with a coefficient on past performance of
around 0.75 for publications and 0.65 for citations. The dummy correction is strongly negative
indicating that those teams with past performance equals zero are expected to produce less in the
future than those team with past performance equal to one. Fast-Start has a positive coefficient
of 0.13 for publications and 0.17 for citations, while the interaction term is statistically zero –
younger investigators are still found to be on a steeper upward trajectory than their more senior
counterparts but do not benefit differentially from funding. A team member receiving a
subsequent contract is still associated with a positive and statistically significant increase in future
performance. The coefficient on scaled rank remains negative and significant for citations, but
loses significance for publications. The adjustment of most note is that the effect of receiving
funding is no longer significant for publications, however for citations a coefficient of 0.16 to
0.22 remains significant at the 1% level.
15.

Table 4 tests the robustness of these results to some alternative model specifications. To
conserve space we show these variations only for the citations performance measure, but results
for publications are analogous. Using scaled rank to control for selection bias relies on an
assumption about the functional form of the relationship between rank and performance; there
is of course no reason why that relationship should be linear. Acting on the prior that panels
might be able to identify very strong applicants and/or very weak applicants, but struggle to rank
those in between, we replace scaled rank with two dummies in column one: high scaled rank
equals one if the proposal received a scaled rank of 0.9 or higher, and low scaled rank if the rank
was 0.2 or below. Neither had significant coefficients and so this does not appear to be a fruitful
direction.
The argument for including scaled rank in the regression was to control for a selection
effect, but that argument assumes that rank is positively associated with performance. Given
that we find a negative relationship between rank and performance, it is less clear that the best
way to test for a funding effect is after controlling for the relationship between rank and
performance. Further, if the “true” relationship between rank and performance is non-linear,
then the finding of a negative rank effect and a positive funding effect might simply reflect a
negative rank effect that operates only over low rank but not higher rank. To explore this issue,
Column two of Table 4 simply drops rank from the regression entirely. The effect is, again, to
reduce the estimated treatment effect to about 0.11 although it remains highly significant
statistically. This suggests that the estimated funding effect is not solely an artefact of the
negative relationship between performance and rank. As discussed further below, whether the
“right” estimate for the funding effect is that with or without the rank variable in the regression
is largely one of interpretation.
In column three we replace scaled rank with the average referee score which ranges from
1 (top 5% of proposals) to 5 (below average). There is some evidence in the international
literature that panel discussions of collated referee scores worsen rather than improving the
selection process. (Fogelholm, et al, 2012). We were therefore interested to see whether the
unprocessed referee scores were more predictive of success than panel rank. The coefficient,
however, is not significant.

16.

Table 4: Negative Binomial Regression variants on proposal-team data
VARIABLES
Count regression:
Log(Past performance)
Dummy: Past
Performance=0
Funded
Fast-Start
FS*Funded
Subsequent contract
Dummy: High scaled rank
Dummy: Low scaled rank
Average referee score
Scaled rank
log(FTE)

(1)
Cites

(2)
Cites

(3)
Cites

(4)
Cites

(5)
FTE*Cites

(6)
Cites

0.663***
(0.0162)

0.661***
(0.0162)

0.662***
(0.0162)

0.661***
(0.0163)

0.667***
(0.0162)

0.660***
(0.0162)

-1.417*** -1.412*** -1.405*** -1.400***
(0.232)
(0.232)
(0.232)
(0.232)
0.157*** 0.105** 0.146*** 0.220***
(0.0551)
(0.0466)
(0.0539)
(0.0729)
0.178*** 0.196*** 0.184*** 0.207***
(0.0667)
(0.0657)
(0.0661)
(0.0724)
0.121
0.118
0.117
0.110
(0.0951)
(0.0937)
(0.0936)
(0.0936)
0.309*** 0.305*** 0.307*** 0.309***
(0.0451)
(0.0451)
(0.0451)
(0.0453)
-0.0746
(0.0882)
0.0797
(0.0508)
0.0528
(0.0348)
-0.230**
(0.115)
0.0478
(0.0375)

-1.661***
(0.269)
0.210***
(0.0723)
0.218***
(0.0726)
0.112
(0.0942)
0.308***
(0.0450)

-1.420***
(0.232)

-0.209*
(0.114)
0.377***
(0.0417)

-0.0958
(0.107)

budget (NZ$million)

Overdispersion: lnalpha
Constant
Observations
Log likelihood

0.303***
(0.0453)

1.369***
(0.123)

1.330***
(0.122)

0.233**
(0.111)
0.789
(0.571)
1.414***
(0.124)

-0.840*** -0.837*** -0.839*** -0.842***
(0.0416)
(0.0416)
(0.0416)
(0.0416)
1,263
1,263
1,263
1,262
-6633
-6634
-6633
-6628

-0.863***
(0.0418)
1,262
-6801

-0.833***
(0.0415)
1,263
-6637

FS*budget
Constant

0.243***
(0.0639)

1.290***
(0.128)

1.351***
(0.123)

1.192***
(0.161)

Time and panel dummies included in all regressions
Standard errors in parentheses
Regression 5 weights citations by the FTE of investigators stated in the proposal
*** p<0.01, ** p<0.05, * p<0.1

17.

As noted above, we are measuring success in terms of the overall publication output of
the proposal team members. But a researcher’s participation in a Marsden proposal can range
between zero FTE and full-time. One might think that the effect of participation in the Marsden
process would be greatest for those researchers most involved in the proposal, which might be
proxied by their FTE as stated in the proposal. Columns 4 and 5 of Table 4 investigate this
possibility. In Column 4, we simply add the log of total budgeted FTE to the regression. Its
effect is zero, although the estimated effect of funding increases in magnitude. In Column 5, we
change the dependent variable, weighting all of the citations received by papers authored by team
members by total budgeted investigators FTE in the proposal. The coefficient on funding
remains higher at 0.21, log(FTE) unsurprisingly turns very significantly positive and yet the other
coefficients barely change. These results would seem to suggest that at this team-aggregate level
we cannot distinguish differential impact of funding based on the different funding levels of the
team members. Note that foreign investigators included in proposals always have FTE of zero.
Thus this variation excludes their publications and demonstrates that the findings are not
significantly affected by how foreign investigators are treated.
Finally, Column 6 of Table 4 retains the basic structure of Table 3 Column 4 but replaces
the dummy variable for receiving funding with the actual budgeted dollar amount (set to zero for
those proposals that were not funded). The estimated coefficient of 0.233 corresponds,
approximately, to funding of $1 million NZD being associated with a 23% increase in citations.
The nonlinear nature of the model precludes direct extrapolation to other budget amounts, but
this is qualitatively similar to the other models given that the average grant over this period was
about NZD 580K. Note that the Fast-Start-budget interaction term coefficient of .789 suggests
that on average Fast-Start proposals get a much bigger boost per dollar than regular proposals, but
this coefficient is very imprecisely estimated. Since overall winning Fast-Start proposals are
given about one-third as much money as winning regular proposals, the previous result that they
get about the same boost as regular proposals on a dummy-variable yes/no basis is also
consistent with their getting a bigger boost per budget dollar, but the imprecision with which all
of the Fast-Start effects is estimated makes it hard to make strong statements.
To this point, we have allowed each disciplinary pool to have its own average
publication/citation level, but have constrained the other regressors to have the same effect
across disciplines. Appendix Table 3 (publications) and Appendix Table 4 (citations) presents the
results of estimating negative binomial regressions separately for each disciplinary panel or pool.
Unfortunately, the results are extremely noisy. Although the effect of prior performance is .6 to
.8, for every panel, the other coefficients of interest are very imprecisely estimated and the point
estimates vary a lot. The effect of funding on publications is positive and statistically significant
18.

only for Biomedical Sciences. For citations, Biomedical Sciences, Mathematics and Information
Sciences and Social Sciences panels have statistically positive funding effects. The coefficient on
scaled rank is negative and significant for several panels.

3.3.

Regression discontinuity estimates
The advantage of the regression analysis described in the previous section is that it allows

us to use all of the data on funded and unfunded proposals in the attempt to measure the effect
of funding on research output. The disadvantage is that it is dependent on functional form
assumptions to control for differences between the funded and unfunded proposals other than
their funding status. Further, it assumes that the “treatment effect” associated with funding is
the same for all proposals, whereas in reality very high-ranked and very low-ranked proposals
might enjoy different benefits from receiving funding. An alternative is to use regressiondiscontinuity methods, which essentially estimate similar regressions to the models presented
above, but utilize only those observations that fall in some pre-selected “bandwidth” around the
cutoff point along the rank ordering (Benavente et al (2012)). This means that the results are less
sensitive to functional form, and provide an accurate estimate of the “local” funding effect, i.e.
the effect of funding on proposals that are near the funding margin.
We explored using this approach to estimate the Marsden funding effect, using
bandwidth of 6 proposals on either side of the cutoff and the log-log specification from Table 2.
The results produce an estimate of about .12 for the funding effect, with a standard error of
about .21. It appears that signal/noise ratio of this effect is too small to identify it with any
precision using only the observations in the vicinity of the cutoff.

4.

Modelling performance of individual researchers

4.1.

Regression Model
We begin with all of the named investigators who appear on any of the proposals

considered in the previous section. This is about 2300 individuals which we then restrict to New
Zealand based researchers, bringing us to around 1500 individuals. We deem New Zealand
researchers to be our sample of interest as they are financially eligible for Marsden grants and are
more likely to repeatedly interact with the Fund.8 We then identified all of the first-round and
second-round proposals (since 2000) and research contracts (since 1996) on which these
researchers appeared. We have also captured the first round panel score, first round panel rank,
second round referee scores, second round panel score and second round panel rank for all of
Marsden Fund guidelines stipulate that overseas researchers may be members of proposal teams but are not
eligible to receive funds.
8

19.

these proposals since 2003. We have assembled these data into a researcher-year panel, 19962012, incorporating their publication and citation record. Not surprisingly, many of these
researchers participated in multiple proposals over that period and some of them received
multiple grants. The overall frequency distribution of first and second round proposals and
grants/contracts is shown in Figure 2. As can be seen, about 90% of these researchers
submitted two or more preliminary proposals, 60% submitted two or more full proposals and
30% received two or more contracts from 2000-2012. This pattern motivates our decision to
switch to a panel that allows for analysis of multiple proposal submissions and contract receipts.
Figure 2: Frequency distribution of preliminary (1st stage) proposals, full (2nd stage)
proposals and contracts by investigator, 2000-2012

Sample: New Zealand based researchers who have submitted at least one full proposal.

In our regression analysis of these data, we restrict observations to only those individuals
who have submitted second round proposals in the preceding five years. We do this so as to
make this section analogous to the above proposal-team quasi-experimental approach and to
reduce the noise associated with incorporating individuals who are no longer, or not yet, research
active. We then regress a dynamic model of current performance (citations or publications) on
past performance, past contracts and controls. We begin our timeframe in 2004 so as to allow
for sufficient lags. The variables of interest are listed below:
Dependent variables
Publications

The number of publications authored in time t

Normalised citations

The number of citations to publications authored in time t, normalised
by panel and year. The mean of citations-per-publication was
normalised to one for each panel in each year – this non-parametrically

20.

adjusts for discipline heterogeneity and the phenomenon that older
publications have longer time-frames to accrue citations.
Independent variables
Log(Average Performance
over past 5 years)
Dummy: Performance in
past 5 years=0

The log of mean publications or mean normalised citations across t-1
to t-5. If the mean equals zero, the value is set to zero.
Dummy equals 1 if mean performance (publications or citations-peryear) across t-1 to t-5 is zero. A dummy correction for Log(Average
Performance over past 5 years) to distinguish between those with no
publications/normalised citations and those with an average of one per
year (Log1=0).

Number of contracts in past

The sum of contracts (funded proposals) received in t-1 to t-5.

five years
Best lagged scaled rank

The maximum scaled rank for a full (second stage) proposal in t-1 to t5. As per the research team regressions, this ranges from zero (lowest
ranked in panel) and approaches unity (highest ranked in panel).

Best lagged percent grade

The maximum percentage grade for a full (second stage) proposal in t-1
to t-5. Ranges from 0 to 100.

Best lagged referee score

The best (lowest) average referee score for a full (second stage)
proposal in t-1 to t-5. Ranges from 1 to 5.

Years post degree

This is the number of years since the researcher received their highest
degree (generally PhD, excludes MD) and can be conceptualised as
professional age.

Dummy: Full FS in past five
years

Dummy equals 1 if investigator submitted a full (second stage) Fast-

FS*Contracts in past 5 years

Count of the number of contracts received in t-1 to t-5, if at least one

Start proposal in t-1 to t-5.
FS proposal was submitted. Note: if an individual received a Fast-Start
and a Standard contract during the same year both will be counted.

Panel Dummies

Nineteen panel dummies: equals 1 if proposal i is submitted to the
corresponding panel. Twelve are for specific panels (e.g. MIS, ESA).
Researchers involved in more than one panel during time t are
classified in one of seven multidisciplinary panels. Details of the
classification can be seen in the appendix. The panel dummy ‘ALL’ is
omitted from regressions.

Time dummies

Nine year dummies: equals 1 if t equals the corresponding year. 2004 is
omitted from regressions.

Table 5 presents descriptive statistics of these variables restricted to the sample of New
Zealand based researchers who have submitted a full (second stage) proposal in the preceding
five years t=2004-2012. As can be observed, about 25% of researcher-years have zero
21.

publications and around 30% have zero citations. We therefore switch entirely to a count-data
framework so as to avoid the log-log model that deals poorly with zeros.
Table 5: Summary statistics on investigator-panel dataset
Restricted to New Zealand based researchers who have submitted a full proposal in the preceding five
years, 2004-2012.

Variable
Count publications
Count cites to publications
Normalised citations
Mean normalised cites past 5 yrs
Mean pubs past 5 yrs
Sum contracts past 5 yrs
Max lag full grade past 5 yrs
Max lag full rank past 5 yrs
Min referee grade past 5 yrs
Log(Mean pubs past 5 yrs)
Log(Mean norm cites past 5 yrs)
Sum full proposals past 5 yrs
Sum prelim proposals past 5 yrs

Percentage zeros
No publications time t
No citations time t
No publications in past 5
years
No citations past in 5 years

4.2.

Mean
Std.Dev. Min
3.135469 3.830887
36.72034 90.65661
3.609825 7.537339

Max

N

0
0
0

46
3134
182.5571

9862
9862
9862

3.153417

4.920935

0

80.91252

9862

2.72176
0.700061
64.32144
0.482743
2.257098
0.591157
0.488354

2.992051
0.777875
22.96838
0.311114
0.968271
0.979442
1.251274

0
0
0
0
1
-1.60944
-4.97135

28.2
6
100
0.966667
5
3.339322
4.393369

9862
9862
9862
9862
9862
9862
9862

1.734435
3.792131

1.092492
2.711005

1
0

8
26

9862
9862

% of
researchers
24.69%
30.71%
8.48%
9.89%

Regression results
We found that conditional over-dispersion in our data was inconsistent with the Poisson

distributional assumption of equal mean and variance. A negative binomial distribution, which
relaxes this assumption, was found to be a better fit for both citations and publications.

22.

Table 6: Baseline results investigator-panel data
NB = Negative binomial
NB FE = Negative binomial with fixed effects
ZINB = Zero inflated negative binomial

VARIABLES

(1)
Pubs
NB

(2)
Pubs
NB FE

(3)
Pubs
ZINB

(4)
Pubs
ZINB

0.795***
(0.0123)
-2.036***
(0.110)
0.0373***
(0.0130)
-0.0310
(0.0408)
-0.0009***
(0.000304)
0.153
(0.202)

-0.135***
(0.0232)
0.0927
(0.123)
0.0453***
(0.0158)
-0.140***
(0.0464)
-0.0003
(0.00181)
2.530***
(0.0913)

0.791***
(0.0139)
-1.054***
(0.232)
0.0365***
(0.0130)
-0.0296
(0.0409)
-0.0009***
(0.000305)
0.161
(0.202)

0.796***
(0.0139)
-1.050***
(0.233)
0.0341**
(0.0134)
-0.0148
(0.0536)
0.0959**
(0.0435)
-0.0226
(0.0409)
-0.0008***
(0.000256)
0.161
(0.204)

-1.683***
(0.0509)

2.530***
(0.0913)

-1.721***
(0.0696)

-1.730***
(0.0674)

9,843
-18425

9,193
-13072
1,439

-0.708***
(0.140)
5.337***
(1.583)
-4.028***
(1.142)
9,843
-18407

-0.702***
(0.136)
5.248***
(1.360)
-3.940***
(0.981)
9,843
-18400

Count regression:
Log(Average Performance over past 5
years)
Dummy: Performance in past 5 years=0
Number of contracts in past 5 years
FS*Contracts in past 5 years
Dummy: Full FS in past five years
Max lagged scaled rank
Years post degree
Constant
Overdispersion: lnalpha
Constant
Zero inflation regression
Log(Average Performance over past 5
years)
Dummy: Performance in past 5 years=0
Constant
Observations
Log likelihood
N Researchers

ZINB and NB regression errors are clustered around researchers
Time and panel dummies included in all count regressions
Time dummies included in all zero inflation regressions
Sample restricted to NZ based researchers with a full proposal in the previous 5 years
Robust standard errors in parentheses

23.

Table 6 Continued

VARIABLES
Count regression:
Log(Average Performance over past 5
years)
Dummy: Performance in past 5 years=0
Number of contracts in past 5 years
FS*Contracts in past 5 years
Dummy: Full FS in past five years
Max lagged scaled rank
Years post degree
Constant
Overdispersion: lnalpha
Constant
Zero inflation regression
Log(Average Performance over past 5
years)
Dummy: Performance in past 5 years=0
Constant
Observations
Log likelihood
N Researchers

(5)
Cites
NB

(6)
Cites
NB FE

(7)
Cites
ZINB

(8)
Cites
ZINB

0.692***

-0.0297

0.675***

0.676***

(0.0178)

(0.0190)

(0.0175)

(0.0175)

-2.097***

0.214

-0.518**

-0.502*

(0.180)

(0.143)

(0.259)

(0.258)

0.0459*

0.0724***

0.0479*

0.0551**

(0.0251)

(0.0220)

(0.0249)

(0.0243)

-

-

-

-0.197*

-

-

-

(0.103)

-

-

-

0.244**

0.0918

-0.145**

0.0846

(0.0970)

(0.0733)

(0.0648)

(0.0723)

(0.0725)

-0.0010***

-

-0.0009***

-0.000879***

(0.000214)

-

(0.000219)

(0.000198)

0.166

0.291***

0.154

0.152

(0.339)

(0.0541)

(0.354)

(0.353)

0.0103

0.291***

-0.0966**

-0.100**

(0.0402)

(0.0541)

(0.0420)

(0.0410)

-

-

-0.831***

-0.832***

-

-

(0.101)

(0.101)

-

-

9.297***

9.306***

-

-

(1.182)

(1.181)

-

-

-6.381***

-6.378***

-

-

(0.966)

(0.967)

9,843
-20025

8,944
-14115

9,843

9,843

-19844

-19833

0.105

1,394

ZINB and NB regression errors are clustered around researchers
Time and panel dummies included in all count regressions
Time dummies included in all zero inflation regressions
Sample restricted to NZ based researchers with a full proposal in the previous 5 years
Robust standard errors in parentheses
24.

Table 6 presents our baseline negative binomial regressions. Columns one through three
regress current publications on the log of past publications (set to zero with a dummy correction
equal to one if publications equals zero), the maximum scaled rank received on a full proposal in
the last five years, the professional age of the researcher and time and panel dummies under
different distributional assumptions.
Column one performs a standard negative binomial regression and clusters the errors
around researchers to account for a lack of independently distributed observations. We see
similar estimates of the coefficient on previous performance to those in the proposal-team
analysis: 0.8 and highly statistically significant, suggesting persistent success with some mean
reversion. The estimate on the zero past performance dummy, as would be expected, is negative
indicating that those who have not published in the recent past are likely to publish less than
those who have published an average of once per year over the past five years. The coefficient of
0.0375 on contracts in the past five years, our estimated ‘treatment’ effect, can be approximately
interpreted as each contract resulting in a publication rate in each of the subsequent 5 years
3.75% higher than it otherwise would have been. Professional age (years post degree) has a
statistically significant but quantitatively negligible effect on publication counts and there is a
small yet significant estimated degree of over-dispersion. Yet again, the maximum full scaled
rank received in the past five years has no predictive ability holding all else constant.
Column two retains the same covariates and also incorporates researcher fixed effects,
necessarily dropping those individuals with entirely zero outcomes in the process.
Unsurprisingly, the coefficients on past performance change dramatically, as the individual fixed
effect picks up most of the effect of lagged performance. The ‘treatment’ effect of receiving a
contract increases slightly, although not significantly, to 0.045 suggesting that, if anything,
omitting fixed effects provides a lower bound estimate of receiving funding. Our hypothesised
control for selection bias, the maximum full scaled rank received in the past five years, turns
negative and highly significant.
Column three presents a zero inflated negative binomial regression with clustered errors.
This model simultaneously maximises the likelihood of a negative binomial count model and a
model of excess zeros to account for the high percentage of dependent variable observations
equalling zero. The estimated coefficients in the negative binomial count regression are very
similar to those from column one, with the exception of the past performance dummy whose
estimated effect halves. The zero inflation regression uses a logit model to predict the binary
outcome of an individual researcher having strictly zero publications or being included in the
count regression. The coefficients rather intuitively suggest that not publishing in the previous
five years greatly increases the probability of not publishing in a given year and the stronger a
25.

researcher’s average performance over the past five years, the less likely they will be unpublished
in a given year. This model fits the data better than the non-zero-inflated version, but does not
alter fundamentally the interpretation of the results.
Column four repeats column three’s zero inflated negative binomial model but with the
addition of two variables: a dummy for submitting a second stage ‘Fast-Start’ in the previous five
years and a count of the contracts received while the researcher was a second stage ‘Fast-Start’
applicant in the previous five years. This fourth regression is analogous to the ‘Fast-Start’
interaction models presented in section III and the results are qualitatively similar. A second
stage ‘Fast-Start’ applicant, irrespective of funding, is on a steeper publication growth trajectory
than ‘Standard’ applicants but the incremental effect of receiving ‘Fast-Start’ funding over
‘Standard’ funding is statistically zero.
Columns five through eight repeat these specifications using citations to papers
published in a given year, normalised by year and panel, as the performance metric. The simple
negative binomial model in column five, as for publications, estimated strong persistent and
mean reverting performance – as evidenced by a positive coefficient less than unity on past
performance - and a strongly negative dummy correction for zero past citations. The impact of
receiving a funded contract is estimated as an approximate 4.6% increase in annual citations for
each of the subsequent five years, relative to what otherwise would have occurred. As this point
estimate is higher than that for publications (~3.7%), it again suggests that the Marsden Fund
modestly increases not only the quantity of publications but the average number of citations
those publications receive. The maximum scaled full proposal rank received in the past five years
remains statistically zero and professional age has a very minor estimated effect on citations per
year.
The normalised citations negative binomial fixed effect model, presented in column 6,
varies from the baseline negative binomial in a similar manner to that of the publications version.
The removal of the fixed effects component markedly changes the coefficient estimates on past
performance, and the estimated treatment effect of funding increases to ~7.2% per year. The
maximum scaled full rank remains statistically zero.
Columns seven and eight present zero inflated negative binomial models for citations per
year. The estimates of the ‘treatment’ effect are similar to the negative binomial model in column
5 – funding generates a 5% to 5.5% increase in citations per year for the subsequent five years.
As per the zero inflated logit model, an investigator with zero citations in the preceding five
years is more likely to receive zero citations in the year in question and those with a recent
history of more citations are less likely to be uncited. The addition of a ‘Fast-Start’ dummy and
interaction with funding shows that ‘Fast-Start’ researchers are expected to be on a faster growth
26.

trajectory but the incremental effect of receiving a ‘Fast-Start’ grant, if anything, removes the
funding effect (significant at the 10% level).
To assist with interpreting the effect of funding in the non-linear zero-inflated negative
binomial model, Figure 3 uses regressions 4 and 8 to simulate a hypothetical researcher’s output
trajectory under three funding scenarios. In scenario A, the baseline case, the researcher never
receives funding and produces just under 2 papers per year that receive a normalised citation
count of around 1.5. In scenario B, the researcher receives a Marsden contract in 2003 only. For
the next five years, s/he benefits from a direct boost in output in the order of ~4% for
publications and ~6% for normalised citations. From 2005 onwards, s/he additionally
experiences a positive indirect effect from the increase in the lagged performance covariate. As
performance is persistent, but mean reverting, we see the total treatment effect persists after the
direct has ended, but this boost gradually dies out over time. Effectively, receiving a grant pushes
a researcher onto a higher output trajectory from which s/he only gradually reverts to baseline
performance.
Our specification allows us to estimate the effects of multiple Marsden grants. Indeed,
the mean researcher in our sample receives more than one grant, so scenario C simulates the
cumulative effect of the researcher receiving a grant in 2003 and another in 2008. The impact of
the second grant begins once the direct effect of the first finishes. The combined impacts of
both grants results in a percentage increase over baseline for scenario C that peaks in 2013 at
10% for publications and 14% for normalised citations. Because we do not find heterogeneous
treatment effects, and the model is close to proportional, these percentage differences should be
qualitative similar across different researchers.

27.

Figure 3:

Hypothetical researcher under three funding scenarios
Simulations of the Table 6 ZINB Regressions (columns 4 and 8)

Scenario A: No funding (baseline)
Scenario B: Funding in 2003
Scenario C: Funding in 2003 and 2008.
1: Simulated publication count trajectory, 1996-2016

2: Simulated normalised citation count trajectory, 1996-2016.

28.

4.3.

Variations and Robustness
Table 7 presents six zero inflated negative binomial models, three each for normalised

citations and publications, that investigate various assessment metrics to control for selection
bias. Columns one and four are the same as columns four and eight from Table 6 – they use the
maximum scaled full rank received in the previous five years. Columns two and five switch to
the maximum scaled full grade received in the past five years and columns three and six to the
best average referee score of a full proposal in the past five years (note that 1 is the best grade
and 5 the weakest). The coefficients change very little between the three publications regressions
however the estimated treatment effect in terms of normalized citations increases when these
alternate evaluation metrics are substituted for scaled rank. This illustrates that despite the
absence of a clear selection effect, the magnitude of the estimated treatment effect is somewhat
sensitive to conditioning in different ways on the programme’s evaluation.
Table 8 includes four zero inflated negative binomial regressions with clustered errors
that examine the lag structure of the dynamic panel and uncover some puzzling phenomena.
Columns one and three, for publications and normalised citations respectively, separate the
contracts received and the scaled ranks associated with full proposals by lags. When no full
proposal was submitted in a given lag the scaled rank was set to zero and a dummy correction
(not shown) was included to account for this. The lagged treatment effects are as one would
expect. There is no impact of receiving a contract in the immediate year after funding however
positive impacts on research output commence from the second year - ~8% for publications and
~20% for normalised citations. The lags are highly correlated potentially rendering the model
unable to separately identify the impacts of contracts received in lag three (and lag four for
citations). For both publications and citations, the fifth lag turns significantly positive - likely
picking up a long tail of the funding effect. The coefficient on lagged scaled rank, noisy even
when aggregated over the past five years, is unable to be significantly estimated when separated
out beyond lag two.
Columns two and four, in response to a suspected long tail, move away from a five lag
funding window and instead include as covariates the number of contracts received since 1996
and the maximum scaled full rank ever received since 2003. The estimated effect of funding
disappears and even turns statistically negative in the case of publications. Overall, there just
does not seem to be a strong enough signal/noise ratio to really get a handle on the time lags in
the process.

29.

Table 7: Comparison of selection metric – Zero inflated negative binomial regressions on
investigator-panel data
VARIABLES
Count regression:
Log(Average Performance over
past 5 years)
Dummy: Performance past 5
yrs=0
Number of contracts in past 5
years
FS*Contracts in past 5 years
Dummy: Full FS in past 5 years
Max lag scaled rank

(1)
Pubs

(2)
Pubs

(3)
Pubs

(4)
Cites

(5)
Cites

(6)
Cites

0.796***
(0.0139)

0.795***
(0.0139)

0.795***
(0.0139)

0.676***
(0.0175)

0.676***
(0.0175)

0.676***
(0.0176)

-1.050***
(0.233)

-1.048***
(0.233)

-1.049***
(0.233)

-0.502*
(0.258)

-0.503*
(0.258)

-0.501*
(0.259)

0.0341**
(0.0134)
-0.0148
(0.0536)
0.0959**
(0.0435)
-0.0226
(0.0409)

0.0339***
(0.0120)
-0.0145
(0.0536)
0.0969**
(0.0435)

0.0315***
(0.0114)
-0.0157
(0.0537)
0.0969**
(0.0435)

0.0551**
(0.0243)
-0.197*
(0.103)
0.244**
(0.0970)
0.105
(0.0725)

0.0708***
(0.0216)
-0.193*
(0.102)
0.238**
(0.0966)

0.0794***
(0.0211)
-0.190*
(0.102)
0.237**
(0.0962)

Max lag percent grade

-0.000389
(0.000465)

Best lag referee score
Years post degree
Constant
Overdispersion: lnalpha
Constant
Zero inflation regression
Log(Average Performance over
past 5 years)
Dummy: Performance in past 5
years=0
Constant
Observations
Log likelihood

0.000676
(0.000830)

-0.0008***
(0.000256)
0.161
(0.204)

-0.0008***
(0.000256)
0.166
(0.204)

0.00586
(0.0105)
-0.0008***
(0.000256)
0.138
(0.209)

-0.0009***
(0.000198)
0.152
(0.353)

-0.0009***
(0.000196)
0.134
(0.351)

-0.000424
(0.0210)
-0.0009***
(0.000196)
0.135
(0.366)

-1.730***
(0.0674)

-1.730***
(0.0674)

-1.730***
(0.0672)

-0.100**
(0.0410)

-0.0998**
(0.0411)

-0.0998**
(0.0412)

-0.702***
(0.136)

-0.700***
(0.136)

-0.700***
(0.136)

-0.832***
(0.101)

-0.832***
(0.101)

-0.833***
(0.101)

5.248***
(1.360)
-3.940***
(0.981)
9,843
-18400

5.246***
(1.352)
-3.943***
(0.975)
9,843
-18400

5.244***
(1.344)
-3.935***
(0.970)
9,843
-18400

9.306***
(1.181)
-6.378***
(0.967)
9,843
-19833

9.307***
(1.181)
-6.382***
(0.967)
9,843
-19834

9.305***
(1.182)
-6.388***
(0.968)
9,843
-19835

Regression errors are clustered around researchers
Time and panel dummies included in all count regressions
Time fixed dummies included in all zero inflation regressions
Sample restricted to NZ based researchers with a full proposal in the previous 5 years
Robust standard errors in parentheses

30.

Table 8: Exploration of lag pattern – ZINB regressions on investigator-panel data
VARIABLES
Count regression:
Log(Average Performance over past 5
years)
Dummy: Performance in past 5 years=0
Contracts lag 1
Contracts lag 2
Contracts lag 3
Contracts lag 4
Contracts lag 5
Numbers of contracts since 1996
max scaled rank lag 1
max scaled rank lag 2
max scaled rank lag 3
max scaled rank lag 4
max scaled rank lag 5
max scaled rank since 2003 (records
began)
Years post degree
Constant
Overdispersion: lnalpha
Constant
Zero inflation regression
Log(Average Performance over past 5
years)
Dummy: Performance in past 5 years=0
Constant
Observations
Log likelihood

(1)
Pubs

(2)
Pubs

(3)
Cites

(4)
Cites

0.791***
(0.0143)
-1.047***
(0.233)
-0.00712
(0.0385)
0.0814**
(0.0332)
0.0288
(0.0294)
0.0595**
(0.0299)
0.0598**
(0.0272)

0.796***
(0.0138)
-1.063***
(0.232)

0.668***
(0.0208)
-0.548**
(0.267)
0.0168
(0.0827)
0.193***
(0.0600)
0.0183
(0.0558)
0.00500
(0.0536)
0.0976*
(0.0515)

0.674***
(0.0210)
-0.544**
(0.265)

-0.0305
(0.0752)
-0.166**
(0.0690)
0.00713
(0.0633)
-0.0756
(0.0617)
-0.0518
(0.0663)

-0.0166**
(0.00804)

-0.0345
(0.151)
-0.212*
(0.115)
0.117
(0.111)
0.0695
(0.127)
-0.0326
(0.120)

0.00971
(0.0139)

-0.0008***
(0.000292)
0.323
(0.215)

0.0444***
(0.0171)
-0.0008***
(0.000287)
0.205
(0.208)

-0.0009***
(0.000213)
0.270
(0.366)

0.0307
(0.0295)
-0.0010***
(0.000221)
0.187
(0.354)

-1.719***
(0.0736)

-1.724***
(0.0666)

-0.106**
(0.0435)

-0.0995**
(0.0428)

-0.704***
(0.144)
5.648**
(2.353)
-4.272**
(1.786)
9,843
-18393

-0.709***
(0.133)
5.230***
(1.332)
-3.881***
(0.977)
9,843
-18408

-0.958***
(0.156)
6.363***
(1.911)
-4.213***
(1.059)
9,843
-19814

-0.951***
(0.159)
6.468***
(2.050)
-4.258***
(1.149)
9,843
-19831

Regression errors clustered around researchers, robust standard errors in parentheses.
Sample restricted to NZ based researchers with a full proposal in the previous 5 years
Time & panel dummies in count regressions, time dummies in zero inflated regressions
31.

Appendix Tables 5 and 6 run zero inflated negative binomial models with clustered
errors on panel subsamples of researchers. These are the individual-researcher equivalents to the
panel regressions in part III. The eight largest panels were chosen and to be included in a given
regression, a researcher must have submitted a second stage proposal to the relevant panel in the
preceding five years. Individuals who have submitted full proposals to two or more of these
eight panels are included in more than one regression. As can be observed, the noise associated
with small subsamples makes precise estimation difficult. For the publications regressions,
researchers involved with the Biomedical Sciences (BMS), Cellular, Molecular and Physiological
biology (CMP), Ecology, Evolution and Behaviour (EEB) and Social Sciences (SOC) panels are
estimated to experience positive funding effects ranging from ~6% to ~9% per year for 5 years.
Researchers involved in other panels experience statistically zero funding effects. For normalised
citations, the regressions are noisier and only BMS has a statistically positive funding effect at
~12%.

5.

Summary and Conclusion
All of the these estimation results are consistent with a modest but statistically robust

boost in research output associated with receiving Marsden funding, although the statistical
precision and exact size of the estimate depends on the empirical formulation. The effect on
citations is consistently larger than the effect on publications, suggesting that funding leads both
to more papers and papers that are more highly cited.9 It is important to emphasize that what is
captured here is a general impact on the publication/citation success of the researchers. It seems
likely that Marsden funding shifts researchers’ focus to some extent towards the subject of the
grant, so that the funding impact on research outputs directly related to the proposal would be
greater than those estimated here, but our empirical framework does not allow us to measure
that.
We also cannot determine the extent to which the increase comes from direct use of the
Marsden money versus indirect impact of Marsden success on researcher opportunities and
resources. Whether this is a weakness or strength of the findings depends on the question being
asked. The Marsden fund is the premier basic science funding mechanism in New Zealand, and
participants certainly believe that Marsden success is a general certification of quality that often
translates into broader success. From the perspective of the Marsden Fund itself, it may make
sense to attribute all of this success to Marsden. From the broader perspective of the social
return to public science investment, the apparent benefit of Marsden funding may include

9 If citations/paper is used explicitly as the success measure, the funding dummy has the expected
coefficient but it is not statistically significant.

32.

double-counting of benefits associated with other public research funds, if part of the Marsden
effect is greater success in receiving those other funds.
While our initial intention was to include panel rank in the analysis to control for
selectivity bias, we find no evidence of selection based on likely research success in the second
round Marsden process. We have tested many different versions of how that selection might
operate, including trying both panel scores and raw referee scores, testing for an effect with or
without conditioning on prior performance, testing for a variety of non-linearities in the selection
effect, and testing for predictive power of panel rank comparing proposals with the same
funding status. There really seems to be nothing there. It is possible that some other evaluation
mechanism would do a better job, but there are several reasons to believe that distinguishing
among these proposals ex ante is very hard:




the inherent uncertainty of research success;
the two-step Marsden process means the panels are attempting to distinguish among the
merits of proposals in the upper tail of the population;
the broad disciplinary coverage of the Marsden panels means that the panels are frequently
comparing apples to oranges.10

Given the significant researcher and RSNZ time and resources that are devoted to second-round
selection, this suggests a potentially large misallocation of resources.
Publications and citations are, of course, only proxies for research output. One could
argue that the lack of a positive correlation between ex ante evaluations and performance by these
metrics reflects a specific effort by the panels to identify research proposals with a particular kind
of potential that is not captured by these metrics. We cannot rule out this possibility, but we find
it hard to describe a plausible conception of the programme’s goals that, if successful, would not
produce research that would be expected to be highly cited.
The investigator-year analysis provides qualitatively similar results to those from the
proposal-team quasi-experimental analysis. We do not yet understand why the estimated funding
effect is smaller for the researcher-year model (3-5% for publications and 5-8% for citations)
than for the project-team model (6-15% for pubs and 22-26% for citations). These are, of
course, not apples-to-apples comparisons, as the former is a change for each of the subsequent 5
years, while the latter is an average percentage increase over the entire remaining observation
period. The simulation results in Figure 3 show that there is some additional benefit to funding
that flows through the cumulative effect of performance on future performance. But this effect
does not appear to be large enough to explain the difference. It is also true that the percentage
10 We speculate that this last factor likely explains the contrast between our results and those of Li and
Agha (2015), who found that NIH peer review scores have significant explanatory power regarding subsequent
performance, within the set of funded researchers.

33.

benefit for the team as a whole is not arithmetically equivalent to the average percentage benefit
enjoyed by each member of the team. But it remains puzzling that difference is systematic and
apparently large.
The two sets of results bracket those of Jacob and Lefgren (2011), who found an
increase in publications of about 7%. They suggested that this relatively small effect was likely do
to the variety of funding options available to NIH researchers in the U.S. It is hard to say
conclusively, but it appears that there are fewer alternatives to Marsden funding for fundamental
science research in New Zealand. Further, the Jacob and Lefgren results are for Individual
Researcher (R01) grants, while research teams are a prominent aspect of the Marsden proposals.
It is likely that the true importance of the Marsden grant varies significantly across the members
of the team, in ways that are not captured very well by the recorded FTE figures. If so, this
means that the treatment effect is measured with considerable error in the individual-year model,
which implicitly assumes that every New Zealand researcher on a Marsden grant is affected by
the grant in the same way. For the project-team analysis, this problem is greatly mitigated. It is
much more reasonable to think that across research teams, the benefit of grant receipt is roughly
the same for different teams, particularly since there is relatively little variation in the budget
sizes. This line of thought suggests that the individual-year estimate of the treatment effect is
biased downward, and the project team estimates may be more reflective of the true treatment
effect.
More generally, the analysis demonstrates the benefit of retaining and utilizing
information on both successful and unsuccessful grant proposals. This basic strategy for
identifying the treatment effect in the presence of potential selection bias is powerful in concept
but very rarely applied in practice.

34.

6. References
Arora, Ashish, & Gambardella, Alfonso. 2005. The impact of NSF support for basic research in
economics. Annales d'Economie et de Statistique, 91-117.
Arora, Ashish, David, Paul A, & Gambardella, Alfonso. 2000. Reputation and competence in
publicly funded science: estimating the effects on research group productivity. Pages 141176 of: The Economics and Econometrics of Innovation. Springer.
Benavente, Jose Miguel, Crespi, Gustavo, Figal Garone, Lucas, & Maffioli, Alessandro. 2012.
The impact of national research funds: A regression discontinuity approach to the
Chilean FONDECYT. Research Policy, 41(8), 1461-1475.
Chabris, Christopher and Daniel Simons, 2010. The Invisible Gorilla: And Other Ways Our
Intuitions Deceive Us, Random House
Chudnovsky, Daniel, Lopez, Andres, Rossi, Martin A, & Ubfal, Diego. 2008. Money for Science?
The Impact of Research Grants on Academic Output*. Fiscal Studies, 29(1), 75-87.
Fogelholm, Mikael, Saara Leppinen, Anssi Auvinen, Jani Raitanen, Anu Nuutinen, and Kalervo
Väänänen. “Panel Discussion Does Not Improve Reliability of Peer Review for Medical
Research Grant Proposals.” Journal of Clinical Epidemiology 65, no. 1 (January 2012): 47–52.
doi:10.1016/j.jclinepi.2011.05.001.
Gerritsen, Sander, & Plug, Erik. 2013. Up or out? How individual research grants affect academic careers in
the Netherlands. CPB Netherlands Bureau for Economic Policy Analysis.
Graves, Nicholas, Barnett, Adrian G, & Clarke, Philip. 2011. Funding grant proposals for
scientific research: retrospective analysis of scores by members of grant review panel.
BMJ, 343.
Horton, Nicholas, Kim, Eugenia & Saitz, Richard. 2007. A cautionary note regarding count
models of alcohol consumption in randomised controlled trials. BMC Medical Research
Methodology, 7(9), 1-9.
Imbens, Guido, & Kalyanaraman, Karthik. 2011. Optimal Bandwidth Choice for the Regression
Discontinuity Estimator. The Review of Economic Studies, rdr043.
Imbens, Guido W, & Lemieux, Thomas. 2008. Regression discontinuity designs: A guide to
practice., Journal of Econometrics, 142(2), 615-635.
Jacob, Brian A, & Lefgren, Lars. 2004. Remedial education and student achievement: A
regression-discontinuity analysis. Review of Economics and Statistics, 86(1), 226-244.
Jacob, Brian A, & Lefgren, Lars. 2011. The impact of research grant funding on scientific
productivity. Journal of Public Economics, 95(9), 1168-1177.
Jacob, Robin Tepper, Zhu, Pei, Somers, Marie-Andree, & Bloom, Howard S. 2012. A Practical
Guide to Regression Discontinuity. MDRC.
Jaffe, Adam B. 2002. Building programme evaluation into the design of public research-support
programmes. Oxford Review of Economic Policy, 18(1), 22-34.
Kahneman, Daniel, 2011. Thinking Fast and Slow, Farrar, Straus and Giroux
35.

Knox, Andrea. 2004. The Impact of Marsden-funded Research: a bibliometric assessment of Marsdenfunded publications, 1997-2001. The Royal Society of New Zealand.
Lee, Hyunshik, & Munk, Tom. 2008. Using Regression Discontinuity Design for Program Evaluation.
Li, Danielle, and Leila Agha. 2015. “Big Names or Big Ideas: Do Peer-Review Panels Select the
Best Science Proposals?” Science 348, no. 6233 (April 24, 2015): 434–38.
doi:10.1126/science.aaa0185
Li, Danielle and Leila Agha. “Big Names or Big Ideas: Do Peer-Review Panels Select the Best
Science Proposals?”—Supplementary Materials (mimeo, 2015)
McCrary, Justin. 2008. Manipulation of the running variable in the regression discontinuity
design: A density test. Journal of Econometrics, 142(2), 698-714.
Rubin, Donald B. 1977. Assignment to Treatment Group on the Basis of a Covariate. Journal of
Educational and Behavioral Statistics, 2(1), 1-26.
Stephan, Paula E. 1996. The economics of science. Journal of Economic literature, 1199-1235.
Thistlethwaite, Donald L, & Campbell, Donald T. 1960. Regression-discontinuity analysis: An
alternative to the ex post facto experiment. Journal of Educational Psychology, 51(6), 309
Wang, D., Song, C. & Barabási, A-L. Science 342, 127–132 (2013).

36.

Appendix
Information on panels

Single panels

Broad areas

Categories
MIS
ESA
PSE
EIS
PCB
B&B
CMP
EEB
BMS
SOC
HUM
EHB
B&B+CMP+EEB+B
LIF
MS
HSC SOC+HUM+EHB
MIS+ESA+PSE+EIS
PSM
+PCB
LPM LIF+PSM

Across two
broad areas

LSH

LIF+HSC

SPM HSC+PSM
All areas

ALL

LIF+HSC+PSM

Research areas
Mathematical and Information Sciences
Earth Sciences and Astronomy
Physical Sciences and Engineering
Engineering and Interdisciplinary Sciences
Physics, Chemistry and Biochemistry
Biochemical and biomedical sciences
Cellular, Molecular and Physiological biology
Ecology, Evolution and Behaviour
Biomedical sciences
Social Sciences
Humanities
Economics and Human Behaviour
Life sciences
Humanities and social sciences
Physical sciences and mathematics
Life sciences, physical sciences and
mathematics
Life sciences, humanities and social sciences
Social sciences, humanities, physical sciences
and mathematics
Broadly interdisciplinary

For the proposal-team dataset, dichotomous variables for the nine panels in operation
from 2003-2008 are included (MIS, ESA, PSE, CMP, EEB, BMS, SOC, HUM, EHB).
For the investigator-panel dataset, nineteen dichotomous variables for panels are
included. Twelve are for specific panels in operation during the timeframe (MIS, ESA, PSE, EIS,
PCB, B&B, CMP, EEB, BMS, SOC, HUM, EHB). The remaining nine are grouped
classifications relevant when a researcher was involved with more than one panel in a given year
- either by submitting two or more proposals to separate panels and/or one proposal to two or
more panels. The groupings are displayed in the table above.

37.

Appendix Table 1
Year and Panel statistics from proposal-team dataset. 2003-2008.
2003

Number of
Observations
168

2004

195

2005

207

2006

241

2007

233

2008

219

BMS

145

CMP

167

EEB

188

EHB

61

ESA

122

HUM

96

MIS

118

PSE

164

SOC

202

Dummy coefficient from Column
4, Table 3
n.a.
(.)
0.250***
(0.0752)
0.0891
(0.0737)
0.0385
(0.0748)
0.0174
(0.0746)
0.0418
(0.0777)
n.a.
(.)
-0.257***
(0.0768)
0.0248
(0.0746)
-0.271***
(0.105)
-0.0697
(0.0825)
-1.035***
(0.134)
-0.315***
(0.0853)
-0.0897
(0.0771)
-0.516***
(0.0791)

38.

Appendix Table 2
Year and Panel statistics from researcher-panel dataset. 2004-2012.
2004

Number of Observations
660

2005

876

2006

1030

2007

1217

2008

1382

2009

1411

2010

1246

2011

1101

2012

939

ALL

12

B&B

9

BMS

1022

CMP

893

EEB

1201

EHB

507

EIS

97

ESA

1057

HSC

306

HUM

521

LIF

397

LPM

487

LSH

125

MIS

610

PCB

192

PSE

824

PSM

98

SOC

1428

SPM

76

Dummy coefficient from Column 8, Table 6
n.a.
(.)
0.0587
(0.0993)
0.0666
(0.103)
0.0420
(0.106)
0.0923
(0.110)
0.00930
(0.102)
0.109
(0.108)
0.169
(0.112)
0.445***
(0.118)
n.a.
(.)
0.192
(0.460)
0.199
(0.352)
0.176
(0.351)
0.278
(0.350)
0.0782
(0.356)
0.419
(0.368)
0.339
(0.356)
0.170
(0.380)
0.425
(0.402)
0.352
(0.356)
0.323
(0.355)
0.719*
(0.384)
0.440
(0.357)
0.136
(0.363)
0.352
(0.352)
0.138
(0.379)
0.326
(0.356)
0.354
(0.378)

39.

Appendix Table 3: Negative Binomial Regressions by panel (discipline) on proposal-team data – Publications

VARIABLES
Count regression:
Log(Past
Performance)
Dummy: Past
Performance=0
Funded
Scaled rank
Subsequent contract
Constant
Overdispersion:
lnalpha
Constant
Observations
Log likelihood

(1)
BMS
Pubs

(2)
CMP
Pubs

(3)
EEB
Pubs

(4)
EHB
Pubs

(5)
ESA
Pubs

(6)
HUM
Pubs

(7)
MIS
Pubs

(8)
PSE
Pubs

(9)
SOC
Pubs

0.727***
(0.0496)

0.828***
(0.0497)

0.741***
(0.0344)

0.674***
(0.0642)

0.740***
(0.0414)

0.749***
(0.0953)

0.611***
(0.0695)

0.663***
(0.0392)

0.804***
(0.0331)

-2.454
(1.650)
0.479***
(0.129)
-0.781***
(0.211)
0.0663
(0.0913)
0.800***
(0.159)

0.143
(0.122)
-0.432**
(0.201)
0.0430
(0.0829)
0.678***
(0.142)

0.0794
(0.0828)
-0.0596
(0.136)
0.0536
(0.0557)
1.043***
(0.111)

0.260
(0.186)
-0.480
(0.297)
0.405***
(0.138)
1.087***
(0.164)

-0.114
(0.0747)
0.0792
(0.134)
-0.00946
(0.0566)
1.004***
(0.122)

-2.01***
(0.365)
0.0963
(0.412)
-0.0403
(0.655)
-0.0432
(0.257)
0.563
(0.380)

-2.562
(1.953)
0.222
(0.205)
-0.330
(0.333)
0.296**
(0.138)
1.237***
(0.230)

-0.454
(0.775)
0.00842
(0.112)
0.0132
(0.191)
0.0883
(0.0768)
1.073***
(0.131)

-2.10***
(0.591)
-0.0118
(0.108)
0.155
(0.178)
0.0398
(0.0711)
0.711***
(0.125)

-2.506***
(0.264)
145
-398.5

-2.533***
(0.242)
167
-449.5

-3.393***
(0.307)
188
-547.0

-2.857***
(0.459)
61
-160.5

-25.23
(0)
122
-325.3

-4.659
(5.583)
96
-103.2

-1.305***
(0.182)
118
-361.6

-2.461***
(0.214)
164
-520.0

-4.486***
(1.310)
202
-406.4

Time and panel dummies included in all regressions
EHB began in 2006. Dummies for 2007/2008 only included
No past performance dummy omitted from CMP EEB EHB and ESA as past publications values all positive
Standard errors in parentheses
*** p<0.01, ** p<0.05, * p<0.1

40.

Appendix Table 4: Negative Binomial Regressions by panel (discipline) on proposal-team data – Citations

VARIABLES
Count regression:
Log(Past
Performance)
Dummy: Past
Performance=0
Funded
Scaled rank
Subsequent contract
Constant
Overdispersion:
lnalpha
Constant
Observations
Log likelihood

(1)
BMS
Cites

(2)
CMP
Cites

(3)
EEB
Cites

(4)
EHB
Cites

(5)
ESA
Cites

(6)
HUM
Cites

(7)
MIS
Cites

(8)
PSE
Cites

(9)
SOC
Cites

0.420***
(0.0663)

0.689***
(0.0453)

0.628***
(0.0336)

0.539***
(0.0424)

0.669***
(0.0514)

0.665***
(0.0725)

0.679***
(0.0468)

0.763***
(0.0459)

0.766***
(0.0360)

-2.377**
(0.946)
0.668***
(0.229)
-0.951**
(0.382)
0.430***
(0.160)
2.626***
(0.473)

0.199
(0.158)
-0.493*
(0.257)
0.231**
(0.108)
1.263***
(0.277)

0.0956
(0.133)
-0.0783
(0.222)
0.165*
(0.0935)
1.811***
(0.231)

-0.0984
(0.236)
0.175
(0.381)
0.280
(0.185)
1.666***
(0.245)

0.127
(0.142)
0.309
(0.243)
0.106
(0.104)
1.415***
(0.309)

-1.890***
(0.416)
-0.531
(0.491)
1.162
(0.760)
-0.0692
(0.319)
-0.0230
(0.538)

-3.046
(2.751)
0.455**
(0.226)
-0.577
(0.380)
0.447***
(0.161)
1.140***
(0.304)

1.752**
(0.734)
0.241
(0.183)
-0.581*
(0.303)
0.141
(0.119)
0.928***
(0.288)

-1.706***
(0.622)
0.522***
(0.181)
-0.411
(0.294)
0.440***
(0.124)
0.543***
(0.203)

-0.481***
(0.110)
145
-885.3

-1.173***
(0.108)
167
-951.5

-1.281***
(0.103)
188
-1135

-1.397***
(0.189)
61
-307.1

-1.540***
(0.133)
122
-705.0

-0.609**
(0.275)
96
-165.2

-0.718***
(0.133)
118
-580.9

-0.953***
(0.108)
164
-959.4

-0.755***
(0.114)
202
-830.8

Time and panel dummies included in all regressions
EHB began in 2006. Dummies for 2007/2008 only included
No past performance dummy omitted from CMP EEB EHB and ESA as past citations values all positive
Standard errors in parentheses
*** p<0.01, ** p<0.05, * p<0.1

41.

Appendix Table 5: Zero inflated negative binomial regressions by panel (discipline) on investigator-panel data: publications
(4)

(5)

EEB

ESA

HUM

MIS

PSE

SOC

Pubs

Pubs

Pubs

Pubs

Pubs

Pubs

0.749***

0.708***

0.762***

0.704***

0.865***

0.787***

0.828***

(0.0256)

(0.0288)

(0.0315)

(0.0330)

(0.0680)

(0.0432)

(0.0313)

(0.0276)

-0.478

-2.191***

-2.065*

-1.059**

-1.744***

-1.402***

-0.698

-0.748**

(0.471)

(0.365)

(1.123)

(0.420)

(0.563)

(0.303)

(0.489)

(0.326)

0.0642**

0.0755***

0.0582**

0.0266

0.201

-0.0308

-0.0116

0.0882**

(0.0298)

(0.0256)

(0.0234)

(0.0277)

(0.149)

(0.0403)

(0.0359)

(0.0360)

-0.139

-0.160*

-0.000331

-0.0669

0.137

-0.00887

0.0502

0.0235

(0.0975)

(0.0878)

(0.0791)

(0.104)

(0.320)

(0.138)

(0.0993)

(0.107)

0.199**

0.241***

0.470***

0.504***

0.137

0.555***

0.468***

0.186*

(0.0813)

(0.0674)

(0.0674)

(0.0941)

(0.303)

(0.106)

(0.0715)

(0.103)

-1.896***

-1.939***

-1.818***

-1.673***

-0.911***

-1.569***

-1.807***

-1.864***

(0.120)

(0.159)

(0.115)

(0.128)

(0.231)

(0.135)

(0.115)

(0.191)

-1.856***

-0.158

-0.580**

0.207

0.0545

-43.43***

-64.04***

-0.181

(0.703)

(0.245)

(0.262)

(0.804)

(0.397)

(1.345)

(3.500)

(0.469)

Dummy: Performance in past 5 years=0

34.27***

-12.04***

0.852

38.70***

1.794

98.55***

136.0***

5.956***

(1.064)

(2.078)

(11.21)

(1.879)

(2.479)

(5.737)

(7.081)

(1.534)

Constant

-20.01***

-16.99***

-18.46***

-20.16***

-1.268

-54.94***

-75.44***

-4.179***

(1.180)

(0.571)

(1.385)

(1.715)

(1.204)

(1.867)

(3.290)

(1.612)

Observations

1,534

1,632

1,861

1,404

699

853

1,426

2,170

Log likelihood

-3173

-3191

-3952

-2823

-653.3

-1847

-3179

-3073

VARIABLES

(1)

(2)

(3)

BMS

CMP

Pubs

Pubs

0.822***

(6)

(7)

(8)

Count regression:
Log(Average Performance over past 5 years)
Dummy: Performance in past 5 years=0
Number of contracts in past 5 years
Max lagged scaled rank
Constant
Overdispersion: lnalpha
Constant
Zero inflation regression
Log(Average Performance over past 5 years)

Regression errors clustered around researchers, robust standard errors in parentheses.
Sample restricted to NZ based researchers with a full proposal submitted to the relevant panel in the previous 5 years
Time & panel dummies in count regressions, time dummies in zero inflated regressions

42.

Appendix Table 6: Zero inflated negative binomial regressions by panel (discipline) on investigator-panel data: citations

VARIABLES

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

BMS

CMP

EEB

ESA

HUM

MIS

PSE

SOC

Cites

Cites

Cites

Cites

Cites

Cites

Cites

Cites

0.717***

0.680***

0.673***

0.624***

0.644***

0.691***

0.685***

0.685***

(0.0453)

(0.0493)

(0.0407)

(0.0549)

(0.0922)

(0.0643)

(0.0354)

(0.0371)

-0.822

-0.522

-2.141***

0.848

-0.861**

-0.606

-1.597**

-0.603

(0.515)

(0.754)

(0.411)

(0.600)

(0.417)

(0.502)

(0.682)

(0.464)

0.125**

0.0670

0.0545

0.0364

-0.120

0.0432

0.0185

0.0935

(0.0566)

(0.0467)

(0.0463)

(0.0597)

(0.249)

(0.0783)

(0.0577)

(0.0690)

-0.0579

-0.150

0.236

0.243

0.206

-0.131

0.0518

0.130

(0.146)

(0.149)

(0.150)

(0.221)

(0.558)

(0.228)

(0.144)

(0.188)

0.228*

0.182*

0.431***

0.755**

0.543

0.739***

0.535***

0.0794

(0.137)

(0.100)

(0.108)

(0.323)

(0.609)

(0.173)

(0.0984)

(0.145)

-0.309***

-0.139

-0.229***

-0.0759

1.459***

0.0185

-0.382***

0.179**

(0.0919)

(0.0872)

(0.0705)

(0.115)

(0.141)

(0.164)

(0.0719)

(0.0737)

-1.156**

-2.948***

-0.520***

-1.119***

-5.181**

-0.977***

-0.960***

-0.814***

(0.490)

(0.888)

(0.196)

(0.322)

(2.420)

(0.300)

(0.254)

(0.199)

69.12

24.74***

36.28***

42.42

35.48***

6.267***

32.53***

20.61***

(2.137)

(1.758)

(7.836)

(2.044)

(0.652)

(0.816)

-18.71***

-23.53***

-24.07***

-18.97***

-12.21**

-5.490***

-17.69***

-18.06***

Count regression:
Log(Average Performance over past 5 years)
Dummy: Performance in past 5 years=0
Number of contracts in past 5 years
Max lagged scaled rank
Constant
Overdispersion: lnalpha
Constant
Zero inflation regression
Log(Average Performance over past 5 years)
Dummy: Performance in past 5 years=0
Constant

(0.562)

(2.035)

(1.884)

(0.501)

(5.432)

(1.044)

(0.358)

(0.886)

Observations

1,534

1,632

1,861

1,404

699

853

1,426

2,170

Log likelihood

-3550

-3469

-4215

-3079

-658.7

-1872

-3291

-3371

Regression errors clustered around researchers, robust standard errors in parentheses.
Sample restricted to NZ based researchers with a full proposal submitted to the relevant panel in the previous 5 years
Time & panel dummies in count regressions, time dummies in zero inflated regressions

43.

