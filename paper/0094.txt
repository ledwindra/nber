NBER WORK[NG PAPER SERIES

THE DECISION-STATE METHOD:
CONVERGENCE PROOF, SPECIAL APPLICATIONS,
AND COMPUTATIONAL D<PERIENCE

V.K. Dharmdhikari*
Working Paper No. 9L

COMPUTER RESEARCH CENTER FOR ECONOMICS AND MANAGEMENT SCIENCE

National Bureau of Economic Research, Inc.
575 Technology Square

Cambridge, Massachusetts 02139

July 1975

Preliminary: not for quotation
NBER working papers are distributed infonma.iiy and in limited
numbers for camrxents only. They should not be quoted without
written permission.

This report has not undergone the review accorded official NBER

publications; in particular, it has not yet been submitted for
approval by the Board of Directors.

*NBER Computer Research Center. Research supported in part by
National Science Foundation Grant DCR 70U 356_A0L. to the National
Bureau of Economic Research, Inc.

Abstract

This paper presents a new method for obtaining exact optimal solutions
for a class of discrete-variable non-linear resource-allocation problems.
The new method is called the decision-state method because, unlike the
conventional

space, the

dynamic programming method which works only in the state

new method works in the state space and

the decision space.

It generates and retains only a fraction of the points in

the state space

at which the state functions are discontinuous; and thus overcomes to
some extent the curse of dimensionality. It carries the cumilative
decision-strongs associated with these points, and thus avoids the backtracking entailed by the conventional dynamic programming method for
recovering the optimal decisions.
A concise and complete statement of the method is given in Algorithm
2 and it is proved that the algorithm finds all exact optimal solutions.

addition the method is adapted for solving sar problems with special
structures such as block-angular or split-block--angular constraints arid
the resultant substantial advantages are dencnstrated. The performance
of Algorithm 2 on many resource-allocations problems is reported, along
with investigations on many tactical decisions which have substantial
In

impact on the perfonnance. The performance of the computer implementa-

tion of Algorithm 2 is compared with that of the P algorithm and it
showed that for the class of problems at which the two are aimed, the
decision-state Algorithm 2 performed better than MIF algorithm both in
terms of storage requirement and solution tine. In fact, it achieved
an order of magnitude saving in storage requirement.

Contents

1.

Representation of the Problem .

2.

Literature Survey

2

3.

The Decision-State Method

14

14.

Proof of Convergence for Algorithm 2

16

5.

Computational and Storage Efficiency

214

6.

Comparison of Algorithm 2 with the MMDP Algorithm . . 30

7.

Non-integrality

30

8.

Block-angular Constraint Set

31

9.

Mixed-Integer Problems

314

10.

Bounding Elimination

36

11.

Conclusions

38

References

'40

Tables

Table 1. Input Data for Non-linear Problems 1 to 9 .
Table 2. General Non-linear Problems

. .

. 26
27

A large number of planning situations involve optimization of a sum of nonlinear return functions of discrete—variables, such that the sums of nonlinear, resource—consumption functions do not exceed the given resource—

availabilities. Consider, for example, the following situations:

1.

Design of multiple—reservoir, water—supply systems.

2.

Deployment of self—contained manpower units for servicing a
missile system subjected to constraints on personnel of different kinds, on equipment of different kinds, and constraints
of logistics.

3.

Selection of investment projects where, instead of mere acceptance!
rejection, it is required to decide among various specified levels
of projects in a given set.

1. pesentation of the Problem.

Let 1+ denote the set of non—negative integers, and let R+ denote the

non—negative real line. Let functions f.(.), g() be defined as
f.

: 1 ÷ R, g.. : 1 -÷

for

all and j.

Let the M-cornponent vector

B(bl,b2___bM) represent the given resource availabilities, and M and N,

the given positive integers. Then the problem can be represented as,

—2—
I.

Maximize

Subject to

1Jf(x)
J
g..(X.) <
X.

b.,

1 <

i<N

1<j <N

In this paper, we deal with the abov problem where f(.)

g• (.)

assumed to be non—decreasing functions such that some real num(.) are
ber a 0 3 g.. (a) > b for all j. Very often f (.) and
polynomials or linear functions, and M is much sn.ller than N. Problem I
are

g

is thus a special type of an integer proaniming problem.
2.

Literature Survey

Two main categories of techniques that may be considered for solving

the above problem are: 1) dynamic programming and 2) implicit enumera—

tion. In this paper, we present a method that was derived from dynamic programming, but which also shares some characteristics of the

implicit enumeration or directed tree search category. The standard
dynamic programming procedure suffers from what Bellman [2] called,
"The Curse of Diinensionality", arising from multiple constraints. For
continuous variable problems, Larson's [31 State Increment Dynamic
Programming method was able to handle at most 4 to 5 constraints by

carrying out computations in what are called "blocks". In this method,
as in other recent approaches such as those of Wong & Luenberger [i.

I,

Wong [5 ], and Yormark & Baker [6 1, the exhorbitant memory requirement is reduced at the cost of increased computation.

.

—3—

For the multiple-constraint, knapsack problems with zero-one vari-

ables, Weingartner and Ness [7], and Nemhauser and Ullman [

8]

developed

specialized dynamic programming algorithms that perform much better than

the conventional dynamic programming algorithm. In these algorithms, the
dimensionality problem of conventional dynamic programming is mitigated by
noting that the optimal return functions are monotone step functions and

thus it is sufficient to record the optimal returns and decisions only at

the points at which the step functions change value. Weingartner and Ness have
also reported on various heuristics that can be used with their algorithm to

achieve computational savings. They also consider elimination of solutions
through bounding in addition toel.imination through dominance. In a way,
the theoretical work of Haymond [9] in the one-dimensional case, and some
portions of the extensive paper on knapsack functions by Gilmore and Gomory
[10] can be said to contain the germs of the ideas that are developed in

[7], [8] cited above.
Although the algorithms in [7], [8] performed much better than
conventional dynamic programming, they are quite inferior to other specialized
algorithms such as Geoffrion's RIP-30 [ill designed for solving zero-one

problems. Thus the main value of the ideas contained in these works is in
how they can be applied to or extended for solving problems which are intractable to other known techniques.

Motivated by the above papers, Morin and Marsten [12] recently developed their Imbedded State Space Approach which can be considered as an

extension of the ideas contained in those papers. It is aimed at solving

-4-

general, non-linear, multi—dimensional knapsack problems Later, an
improved and somewhat more complete version of their algorithm was given

in [13]. A different paper by Morin and Esogbue [14] gives a theoretical
exposition of the method and its extension to a broader class of sequential

decision problems with additive and multiplicative returns. Some compu—
.tational results are given in [13], [14]. The Imbedded State Space Approach
developed by Morin et al., it is seen, is conceptually similar to the

decision-state approach developed in this dissertation. The implementations
of the two basically similar approaches in the form of specific algorithms

are, however, quite dissimilar in many important aspects;
3.

The Decision-State Method

We now proceed to qive an alqorithmic statement.of the decision—state method.
By an algorithm we mean, in accordance with Knuth1s [15] definition, a procedure consisting of a sequence of instructions which are unambiguous and

I

executable such that theprocedure terminates in finite time.
In this section we define the symbols and operations used in the

statement of the algorithm and its proof. The algorithm is stated in an
easy to read FORTRAN-like language whose instructions are numbered state-

ments in upper case letters. The instructions are preceded by explanatory
comments distinguished by asterisks.

.

—5—

3.1

Definitions

Let

the k-tuple 0 =(x1, x2, ..., Xk) for 1 <

k < N,

where all xeI÷,

denote

a particular set of values of the first k variables. For each decision

vector

D, the M-component vector-valued function ks(D) is defined as kS(D) =

(s1,

1<i<M

(xj) for

= S where each component is given by s1 =

S2,..., SM)

and the scalar-valued function kv(D) is defined as kv(D) =

v.
E! f(x) =

For a given 0, the triplet (D,S,v) is said to form a list

entry or 'entry, where D is the decision vector, S is the state vector, and
v

is

the objective value.
If there exist two

such that v' <

v°

v") and

(D", S"I

list

>S'

and S'

(D',

entries E' =

then (..D, S',

this is expressed

v')

as E" > E'

S', v') and E° =

(0",

S",

v")

is said to be dominated by

or E' <

E".

It is established

in Lemma 4 that a dominated entry (D', S', v') cannot be a part of an optimal

solution
val ue.

because it

can

be replaced by

(0", S1, v")

to increase the objective

Hence, while searching for optimal solutions, only the undominated

entries need be examined.

For

an integer k > 0,

given D =

(x1,

2'

notation (0, x÷.) denotes the (k + 1)-tuple (x1, x2,
generally, for n > 0, the notation (D, Xk÷1,.i..Xk+n)

Xk) and Xk+lEI÷ the

Xkl Xk+l). More
denotes

tuple (x1, x2, •..xk+n). In the special case of k = o when

notation (D, xk+l) denotes the one-tuple (x1). Given an
a

the (k +
D is vacuous the

integer Xk÷lCI+ and

list L with k—component decision vectors, the notation (L, Xk+l)

denotes

a list which is identical with the list L except that in each entry (D, 5, v)
•D is replaced by (D,xk÷l) and S and

v are

adjusted accordingly. Given a list L

and an entry (D,S, v), the notation L.plus.(D, S, v) denotes the list containing all entries from L and the entry (D, 5, v). Similarly, for a list
L and an entry CD, S, v) which may or may not be in it, the notation L.minus.
(D, S, v) denotes the list of all

entries from L except the entry (0, S, v).

Given ak-stage feasible entry (0, S1 v), for k < N,
order descendents as follows:

we

define

its various

-6-

O-th order descendents
S, v)]

{(D, 5, v)}

n-th order descendents for n

1, 2, ...,

c
DescE(D, 5, v)]

S', v')

1

N-k,

(Dxk+1,...,xk+fl),

Xk+jCI÷ 1 < j <

k+ns(DI) BV -

k+nv(DI)

J
In

addition, the set of n-th order descendent entries of the entries in

the k-stage list L is defined as

tDesc [L] =

3.2

E1c L

Desc [E1].

.

Decision-State Algorithm 2

In this section we develop and present Algorithm 2. It is divided
in subsections, a subsection beginning with some motivation underlying the
development. The steps of the procedure are preceded by asterisked cOrn—
ment statements.

* We

-

begin with k =

0,

and the list L containing one entry (D,S,v)

* where D is vacuous, and S and v are zero. L occupies one top
*

location

*

fied

of the storage area, the rest of the locations are identi-

as being empty.

.

—7—

Step 0: k
3.2.1

0, L =

(-,O,0),

KOUNTL =

1,

KOUNTC = 1

Systematic Entry Generation

In Algorithm 1 we construct many entries (D,S,v) with S >B
which, upon testing for feasibility of state are then discarded. This
is so because each value of Xk+l is combined with each entry in the k-th
stage list of undominated entries, in order to insure that all feasible

entries are constructed. By noting the non-decreasing property of the
gjj(•) functions, a new procedure is developed which tends to generate

fewer infeasible entries. To achieve this, we first assume that the entries In L are in the non—increasing order of objective value. Then we
combine each entry E11cL in a non-empty location [Ni] starting with

Ni = KOUNTC, with successively increasing values of Xk+lcI+. As soon as
an infeasible entry is generated, the cycle is started all over for the
next entry EnhiCL with Ni =

Ni-i;

and so on. We describe below how the

(k+1)- stage entry list C is generated from the k— stage list L. It is
assumed that enough storage locations are available between the top and

bottom of the storage area to carry out the procedure. Later in Chapter 4
we will deal with the question of precisely how many storage locations

are needed in our specific computer implementation. For Ni an integer,
[Ni] denotes the Ni-th location of the storage area and if [Ni] is not

empty then EN1 denotes the entry (DN1,S1U,v) in location [Ni].

*

*
*

Steps 1 to 6 constitute the Systematic Entry Generation phase.
We

begin with list L occupying the upper KOUNTC locations of the

storage

area. It contains KOIJNTL non-empty locations, containing

—8--

* the

k- stage entries interspersed with KOUNTC-KOUNTL empty

*

locations.

*

The entries are

*

that

*

end of the phase we will have generated list C of the (k-fl)—

*

stage entries. C will be occupying the lower contiguous loca-

*

tions

*

We

*

the list C empty, the counter KOUNTC reset to zero and defining

*

The rest of the locations below [KOUNTC] are empty.

in

the top entry has the largest objective value. At the

of the storage area.

start with location KOUNTC at the bottom of the list L, with

to equal KOUNTL.

Step 1: Ni = KOUNTC, C =
*

monotonic order of the objective value so

KOUNTC =

,

0,

2

= KOUNIL

When the location [Ni] is empty we go to the next location.

Step 2: IF [Ni] IS EMPTY THEN GO TO 6
*

When

Ni contains an entry, we refer

*

EN1 =

(DNiSNivNi)

*

stage-variable

to

its three parts as

We start the cycle by setting the new

to zero.

Step 3: Xk+i = 0
*

Construct

Step 4: D =

a (k+1)- stage triplet by combining Xk+l with EN1.

(DN1,xk÷i),

s = k+1 s(D), v =

k+i

v(D)

* The new triplet (D,S,v) is added at the lowermost empty location
*

in

*

above the previous entry, if

*

of the entries in

the storage area if and only if S is feasible. It is placed

C is

there was one. The count KOUNTC

now incremented and so

is

the value of

*xk+i
Step 5: IF S <B THEN PLACE (D,S,v) IN LOWERMOST EMPTY LOCATION,
KOUNTC = KOUNTC +

i,

Xk+i =

Xk+i

+ 1, GO TO 4

—9—

*

When S Is infeasible we go to the next location from list L.

Step6: Nl=Ni-l,
IF Ni > 0 THEN GO TO 2

*

When Ni equals zero, the Systematic Entry Generation phase is

*

complete. The entries in the lower contiguous locations con—

*

stitute

*

scendents of an entry in L are said to constitute a sublist.

*

The £

*

sublists

*

list C. All the feasible entries in C which are de—

entries

from L will give rise to L

Let the

be numbered in the order in which these were generated.

SL1 will be the sublist occupying the lowermost locations and

*

will

*

from L with the smallest objective value. The top sublist will

consist of the feasible descendents of the lowermost entry

* be SLL. We define C(k+1) =
*

3.2.2

sublists.

the

C

for the current value of k and

current list C.

Merging
In this phase we merge the sublists comprising C into one merged

list A. The task of this phase is similar to that in a situation where
a given set of numbers is to be arranged in descending order. There are
a number of ways ( See Knuth [ 23 ]) this can be done, some being more

suited for some conditions than others. We develop here a new merging

procedure that exploits the particular situation at hand. Let the number
of entries in the sublists SL1,SL2,...ISL& be denoted, respectively, by
I'(i), P(2),...,P(2). Let P = MAX {P(i), 1 <i <z}. In Lema 1 we prove

that only P additional locations are sufficient for merging these sub-

—10—

lists
are

by the procedure given below. We assume that P empty locations

available

above the uppermost sublist SL. Again, how this affects

the precise size required of the storage area in our computer implementation is dealt with in Chapter 4.
*

Steps 7 to 13 constitute the Merging phase. From the non-de—

*

creasing property of k•' we know that the entries within each

*

sublist

*

smallest

*

dered list we need to merely merge these sublists. This we do

*

by first copying SL in the upper locations of the storage area

*

and calling it list A. Then we merge A with SL,1 if it exists,

*

and again call the result, list A. Then we merge A with SLL_2,

*

and so on. We begin by copying sublist SL.

are in monotonic order of the objective value, with the

entry at the bottom. Thus, in order to obtain one or-

Step 7: COPY SUBLIST SL IN LOCATIONS 11] TO [PCi)] PRESERVING THE
ORDER OF THE ENTRIES, KOUNTA =

SL

*

When there is only one sublist we iniiediately go to the next

*

stage.
= 1 THEN GO TO 31

Step 8: IF
*

S

Initialize

the counters for the first merge.

Step 9: Mi = —1, Ni = P(2.), N2 = P(M1), N3 =

0,

N4 = 0

*

The list occupying the upper contiguous locations will be called

*

A.

*

the end of the first merge

*

list

*

In

At this point it consists of only the entries from SL. At

it will be the resultant ordered

of the merge of SL and SL , and so on.

this step we identify the current lowest entries from the

.

-11-

* current

list A called the initial list A and the portion re—

*

maining currently of sublist SLM1 and also identify their corn-

*

ponents with specific labels.

Step 10: EA = (DA,SA,vA) IS THE LOWEST REMAINING ENTRY IN THE INITIAL
LIST A, EM1 = (0M1 sMl vM1) IS THE LOWEST REMAINING ENTRY IN

SLM1

* Now we compare A with M1 and remove the entry with the smaller
*

value and transfer it to a new location in the merged list A.

*

Increment the counters and repeat until the sublist is exhausted.

Step 11: IF A< M1 THEN REMOVE EA FROM THE INITIAL LIST A,
TRANSFER EA TO [N1+N2-N3] IN THE MERGED LIST A, N3 = N3 + 1,

IF bo>M1 THEN REMOVE EM1 FROM SLM1,
TRANSFER EM1 TO [Ni+N2—N3] IN THE MERGED LIST A, N3 = N3 + 1,

N4=N4+i
*

As long as the counter N4 of the number of entries removed from

*

is smaller than P(M1), we repeat with the new lowest entries,

SLM1

*

EA in the initial list A and EM1 in the remaining sublist SLM1.

Step 12: IF N4< P(Mi) THEN GO TO 10
*

When N4 = P(M1) the merged list A contains the result of the

*

merge of initial list A and SLM1. If any more sublists remain

*

to

*

the

be merged, we reinitialize the counters and start aqain with
next sublist and current A as the initial list A.

Step 13: Mi =

Mi-i,

Ni =

Ni +P(Mi÷i), N2 = P(Mi),

N3 =

0,

N4 =

0,

IF Ml> 0 THEN GO TO 10
*

When Ml = 0 all the sublists will have been merged into list A

-12—

* which

will now contain all the KOUNTC entries arranged in

*

monotonic order with the smallest objective value at the bot-

*

torn in [KOUNTC]. We set counter KOUNTA to current value of

*

KOUNTC.

In order to avoid the substantial computations in

* the Identification and Elimination phases, when no greater
* than 10 new entries are generated in C as compared to those
*

in

L, we bypass these phases.

Step 14: KOUNTA = KOUNTC, IF (KOUNTC—KOUNTL)< 10 THEN GO TO 31

Identification

3.2.3

In this phase we identify certain entries as distinguished
entries, which are potentially likely to be found dominated, i.e., the

non-distinguished entries cannot be dominated. These propositions are

proved later in Section 3.3. As the distinguished entries are identifled, certain M-component vectors called T vectors are stored as mar-

ker vectors in association with some entries. These are used in the

Elimination phase. We assume that enough storage locations are available to store the marker vectors.

*

Steps 15 to 21 constitute the Identification phase. For

*

Ni

*

I

*

I vector is defined as the smallest of the i-th components of

*

the state vectors sMl of the entries in A where M1< Ni. If

*

the

*

then ENl is

=

1,2,...,

KOUNTC we compute recursively an M-component

vector for each entry ENEA. The i-th componentT of the

N1-th I vector does not exceed

in any component,

identified as a dfstinguished entry. For a

•

-13-

* prespecified positive
*
*
*

integer t, every t-th vector is stored

as a marker vector and the entry is marked to show this.
We

begin by setting the initial T vector equal to the right

hand side vector B.

0,

Step 15: T = B, Ni =

Ml = 0

*

Increment the counters. If all entries have been examined then

*

go to the Elimination procedure, otherwise proceed to the next

*

step.

Step 16: Ni =

Nil-i,

Ml = Ml +

1,

IF Ni> KOUNTC THEN GO TO 22
*

Test

*

component of the current I vector. When it is, then EN1 cannot

*

be dominated by entries above it.

if any component of

Step 17: IF 3t,

1<i<M

is smaller than the corresponding

SUCH THAT S!<T THEN GO TO 19

* When S>T then it is possible for EN1 to be dominated.
Step 18: IDENTIFY EN1 AS A DISTINGUISHED ENTRY, GO TO 16
*

Update the T vector.

Step 19: T =

(Tl,T2,...,TM)

WHERE I = MIN

(s!', T)

* Store every t-th vector as a marker vector.
Step 20: IF Ml =t THEN STORE T AS A MARKER VECTOR IN ASSOCIATION WITH
ENTRY EN1, Mi = 0
Step 21: GO TO 16

3.2.4

Elimination

In this phase we detect which of the distinguished entries are

_1'-I_

actually dominated, and then eliminate these by identifying- the entry

location as being empty. The marker vectors stored in association with
some of the

of

number
Section

entries in the Identification

entry comparisons whenever possible, making use of Lemma 3 in

4.

* Steps
*

phase are used to reduce the

22 to 30 constitute the Elimination phase. When there

are no distinguished entries we bypass the Elimination phase.

Step 22: IF THERE ARE NO DISTINGUISHED ENTRIES IN A THEN GO TO 31

*

If there exist any distinguished entries, we start with the

*

lowest

one, with the smallest objective value.

Step 23: Ni = THE LOCATION OF THE LOWEST DISTINGUISHED ENTRY, Ml =
We go the next Ml if location [Mi] is empty or if Ml =

Ni-i

0.

Step 24: IF (Ml = 0 OR [Ml] IS EMPTY) THEN GO TO 29
*

When there is a marker vector in association with EM1, we

*

identify

it with a label.

Step 25: IF THERE IS NO MARKER VECTOR IN ASSOCIATION WITH EM1 THEN

G0T027,
OTHERWISE LET TM BE THE MARKER VECTOR
*

If any component of the state of EN1

*

spective component of

*

further

*

distinguished

TM, then EN1

entries above EM1;

is smaller than the re-

cannot be dominated by

therefore, we proceed with

any

the next

entry.

Step 26: IF3i, 1<i<M SUCH THAT S< TM THEN GO TO 30
*

In

order to ensure that all alternative optimal solutions are

-

* obtained,
*

for

—15—

entries with equal objective value are not compared

dominance.

Mi

Step 27: IF v11 =

THEN GO TO 29

*

When 5N1 >sM] then EN1 is dominated by EM1 and we eliminate

*

EN1 by identifying [Ni] as being empty and decrease the counter

*

KOUNTA by 1.

Step 28: IF 31, i<i<M, SUCH THAT S!'< S11 THEN GO TO 29,
OTHERWISE ELIMINATE EN1 AND IDENTIFY [Ni] AS BEING EMPTY,
KOUNTA = KOUNTA-i
*

We prepare to examine the next upper location.

Step 29: Ml =

Mi-i,

IF Mi >0 THEN GO TO 24
*

At

*

(Ni].

*

entry

*

cycle.

this point we are finished with the distinguished entry in

The elimination phase is complete ifthere.is no distinguished
above (Ni], Otherwise we reinitialize Ni, Mi for the next

Step 30: IF EN1 WAS THE UPPERMOST DISTINGUISHED ENTRY ThEN GO TO 3i
OTHERWISE Ni = THE LOCATION OF THE NEXT DISTINGUISHED ENTRY

ABOVE CURRENT [Ni], Ml =

Ni-i,

GO TO 24

*

At

*

KOUNTA non-empty locations, each •in an undominated (k+i)- stage

*

entry.

*

the

*

identify

*

to

this point A occupies the upper KOUNTC locations and contains

Now we rename this list as L, reinitialize KOUNIL with

current value of KOUNTA, increment the stage number and
all locations below [KOUNTC] as being empty, and go back

Systematic Entry Generation for the next stage if k

is less

-

* than

N, the number of stage-variables in the given problem.

*

Define

*

list A.
A,

Step 31: L =

—16—

L (k+1) = A for the current value of k and the current

KOUNTL = KOUNTA, k =

k+i,

IF k< N THEN GO TO 1

*

When k equals N, the final list L of undominated entries con-.

*

tains

all the optimal entries. Since the entries are

*

tonic

order of the objective value, with the top entry having

*

the

*

up from the top.

*

We

in

mono—

largest value, the optimal entries can be easily picked

start with the top entry E1= (D1,S,v)

Step 32: Ni = 1, OPTLIST = {E'11}, OPTVAL =

N1,

N2 = 1

*

When the next entry below has equal objective value, we add

*

it to OPTLIST. Otherwise we terminate the algorithm.

Step 33: N2 = N2 + 1

IF N2> KOUNTL THEN GO TO 35

Step 34: IF t2 =

N1

THEN OPTLIST = OPTLIST.PLUS.EN2, GO TO 33

*

When I2 Ni then OPTLIST will contain all the entries yield-

*

ing

*

entries

(D,S,V)EOPTLIST are the optimal solutions to the given

*

problem

1.

the optimal value OPTVAL. The decision vectors D of the

Step 35: END

4.

Proof of Convergence for Algorithm 2
In this section we will establish that the 35—step procedure

presented in Section 3.1 achieves what it sets out to do. First we will

.

—17—

in Theorem 1 that the procedure can properly be called an algor-.

prove

ithm

in the sense that each step of the procedure is unam&*guous and

executable, and that the procedure terminates finitely. Then we will
prove

that

four

lermias that will help in proving Theorem 2 which states

Algorithm 2 finds all optimal solutions to the given Problem 1.
The procedure of section 3 is an algorithm.

Theorem 1:

Proof:

The

ditions

of values, additions or deletions of list entries, storing

procedure involves operations such as comparisons and ad-

entries in storage locations, identifying or marking storage locations,

and so on, which are clearly executable. Thus we need only to establish
finiteness. The Systematic Entry Generation phase starts with stagenumber k = 0 and the list L containing only one entry. For each entry

in the list L, we start with Xk+l =
formed

by Step 4 and Step 5 we

0 and at each pass through the loop

increment

Xk+l

by 1.

Since the functions

are such that there exists a non-negative real number a for which

(a)> b

for.all I and j, the termination from this loop occurs in a

finite number of repetitions. Since we start with a finite number of
entries in the list L, termination from the Systematic Entry Generation
loop formed by Step 2 and Step 6 occurs finitely, and the list C contains a finite number of entries.

The number of subliststo be merged is finite, hence termination
from the merging procedure occurs finitely. In the Identification procedure each entry is compared once and only once with the recursively

-18-

computed I vector, hence termination from this procedure

occurs

finitely

and the number of entries distinguished remains finite. In the Elimination procedure, each distinguished entry is compared with only a finite
number

of entries above it,

hence termination

from the Elimination pro-

cedure occurs finitely with the new list L containing only a finite
number of entries.

In Step 31 the stage-number k is incremented by 1, and the next

cycle through the procedure begins all over. Since we started with
k

0, and since k is incremented at each cycle, the termination through

the largest loop, formed by Step 1 and Step 31, occurs finitely, after

the N-th pass. Exit from the loop formed by Step 33 and Step 34 occurs
finitely because the loop starts with N2 =

1,

increments N2 by 1 at each

pass, and since there are a finite number, KOUNTL entries in the list L.

Thus termination from the entire procedure occurs finitely. VV

Lena 1 establishes that for the Merging procedure of Section
3.2.2 only P additional storage locations are sufficient to achieve a

complete merge without losing any of the entries. This is done by showing that every entry is transferred to a location that has been made
empty before the transfer. Leniiia 2 helps us in determining some entries

that cannot be dominated by any entries in the list. By the use of
Lemma 2, in the Identification procedure we identify some entries from
the list A as those that can possibly be dominated by some other entries.
Lemma 3 helps us in reducing the number of entry comparisons in the
process of determining if a distinguished entry is actually dominated.

.

—19—

Whenever we find a marker vector, at least one component of-which
exceeds the

corresponding component

of the

state

vector of

the

distin-

guished entry, we can stop the entry comparisons because Lemma 3 proves
that no further comparisons can show that the distinguished entry is

dominated. Lema 4 establishes that any descendent of a dominated
entry cannot be optimal.
Leimia 1: Suppose there are £

sublists

SL1,SL2,...,SLL. Each sublist

is in monotonic order of the objective value with the smallest entry

at the bottom. Suppose P(1), P(2),..., P() are the counts of the
number of entries in the sublists, respectively; and that P is the

largest of these numbers. Then the P additional storage locations
attached above the topmost list SL are sufficient to completely merge
the sublists.

Proof: The procedure starts by copying P(L) entries from SL into the
upper locations of the P additional locations attached and P(L)< P.

This leaves P - P(&) empty locations. The first merge of SL1 with
SLL entries will transfer entries to the uppermost P(&) +

P(L-1) loca-

P(,)

of locations

tions, which number is no larger than the numberP +

available for the resultant list A since P (2.—i) P. Thus:.P(2j+P(z-1)<P+P(2.).

Moreover, no entry from SL2._1 is transferred to a location that was occu-

pied by an entry from SL2., until the latter was removed from it. The same
holds for the next merge because P(2.) + P(&-1) +

P(&-2)<P + P(2.)

+

P(2.-i).

This continues to hold for all merges because P(L)+P(2.-1)+...+P(L—n)<P

+P(&)+...+P(&-n+i) for 1<n<L-1. vv

—20—

Lena

2 In the Identification procedure, if there is an entry EN1 =

(D,S,y)eA such that a component of sN] is smaller than the corresponding component of the (Ni—1)-th I vector, then the entry EN1 cannot
be dominated by any entry in the list A.

Proof: From the procedure, we see that the recursion for the i-th com-

ponent, 1<i<M, of the (N1—l)-th T vector Is given by T. =
Thus I1 = MIN

MIN(S'T).

Now for M1< Ni, M1> Ni from the

Merging procedure, and there exists an i such that s!"<

for all

Mi< Ni. Therefore EN1 cannot be dominated by EM1 for Mi< Ni. But for

M1>N1, M1<N1, therefore EN1 cannot be dominated by EM1 for M1>N1.v

Lemma 3:

In the Identification procedure, suppose TM is a marker vector

stored in association with the M1—th entry EM1 and that EN1 is a distin-

guished entry where Mi< Ni. Suppose also that a component of S is
smaller than the corresponding component of TM. Then EN1 cannot be dominated by any entries above EM1.

Proof: By its construction, we know that TM1 = MIN

We

are given that there exists an i ,i< i< M such that s!< TM. Therefore,
s!1< TM1< s!12 for all integers M2< Ml. From the Merging procedure we

know that v11< M2 for M2< Mi. Hence EN1 cannot be dominated by any entry
above EM1. vv

Lemma 4: Let E1 c C(N) and E2 c C(k) for k<N. Suppose E1
If there exists an entry E3 C(k) such that E3> E2, then
optimal.

N—k

Desc [E2].

cannot be

.

—21—

Proof: First, suppose that k = N.

Then

E1 c 0Desc [E2]. Since E3> E2, v3> V2

nial. Now suppose k< N. Since E1 £
j =

1,2,...k.

=

X3

for j =

1,2,...,k

N_k5

this entry, for i =

=

1,2,...,M

(E2], we have

g(X)

(D4,S4,v4),

D4 =

for

and X,j4 =

C(N), and

= v1 and hence E1 cannot be opti-

Since E2< E3, S<S =E1

Now construct a new entry
=

c C(N), E3

for I =

X =

X

for

1,2,...,M.

(X14,X24,...,XN4) where

j =

k+1k+2,....,N.

For

we have

(X4)

s14

z: g1(X4) + E:1 g1(X4)

=

s3
I

—
—

+3=N

j=k+1 g1

g(X1) = S1<B

g1(X1) +

Therefore, S4

and entry E4

is feasible. Now

4

V

—
—

,.j=N

,.,

4

=

f(X4) + E:1 f(X4)
=

f(X) + z:÷1 f(X')
Since E2< E3, z f(X3) = v3>
Therefore, v4>

fx'

+

v2 =

E

f(X2) =

E

f(X')

f(X,1) = v1.

Thus (D4,S4,v4) is a feasible N-stage entry with greater objective value

than that of E1. Therefore, E1 cannot be optimal. vv

—22—

Theorem 2: Algorithm 2 finds all optimal solutions to the given
Problem 1.

Proof: In Theorem 1 we proved that the procedure in Section 3.3 termed
Algorithm 2 can properly be called an algorithm. 'In Lemma 1 we proved

that the P additional storage locations attached above the top sublist
SL&, where P is the largest of the sublist counts P(1),P(2),...,P(2),
are sufficient for the Merging procedure in that all the entries in the
i.

sublists

are preserved and arranged in one contiguous list A in mono-

tonic order at the end of the merge. Lemma 2 and Lemma 3 prove that the
Elimination procedure removes all the dominated entries from the list A

and only the dominated entries from the list A. Lemma 4 proves that no
descendent of a dominated entry can be optimal, hence no such descendents need be generated nor kept in the search for the optimal solutions.

Using all the above, we provebelow that the final list L(N) produced
by the procedure of Section 3.2.4 contains all the optimal list entries
having

the optimal objective value. Let F and F0

denote, respectively,

the sets of feasible and optimal entries for the N-stage Problem 1.
Symbolically,

(X1,X2,... ,XN), X j, i<j<N,
= NS(D), v = "v(D), S<B

D =
F =

1 (D,S,v)
s

(D,S,v)

(

F =

(D,S,v)

F, v>v

forany(D,S,V)cF

Clearly F0CF. From the definition of the descendents we know that for
k
=

k1

1,2,...,N—1,

k2 =

1,2,...,N—k1,

Desc [C(k1)} =

k—i
2

Desc [1Desc]

—23—

k

k

k

2Desc (C(k,)] =
k

2Desc [L(k1)] U 2Desc (C(k1).rninus.L(k1)]

(C(k1)]],
k
and • =
Desc [L(k1)]fl Desc C(k1).minus.L(k1) .

first

At the end of the

pass through the Systematic Entry Generation phase, C(1) contains

all feasible entries for the 1—stage problem, i.e., C(1) = UD,S,v) /
D = (X1)

1÷, S =

B, v = 1v(D)}. Therefore F = N_lDesc [C(1)].

(Recall that L(1)= C(1)). At the end of the second pass through the
Systematic Entry Generation phase, C(2) = 1Desc [L.(1)]. Therefore, F =
N_lDesc

(L(1)] = N_2sc[lDesc [L(1)]]= N_2Desc (C(2)]. Now C(2) =

L(2) U (C(2).minus,L(2)). Thus, F = N_2Desc

(L(2)] u N_2oesc [C(2).minus..

L(2)]. From Lemma 4 we have, since F0CF, F0C12Desc [L(2)].
Proceeding in a similar manner, at the next pass we will obtain

F0C3Desc [1Desc [L(2)]] = N_3Desc [C(3)] =

N_35

[L(3)] U N_3Desc

(C(3).minus.L(3)]. This process can be continued till the (N-1)th pass

to obtain F0CN_(1)Desc L(N-1)]. From this we get F0C1Desc [L(N—1)] =

C(N).

Thus all optimal entries are members of the final list C(N) and

hence also of the final list L(N) of undominated entries. Since the
optimal entries have the maximum objective value, they will be at the
top of the list and thus OPTLIST will contain all of them. vv

5.

Conutationa1 and

Storage

Efficiency

.

As with any solution technique, the ultimate test of an algorithm is
in its efficiency. It is not difficult to see how our algorithm is a
substantial improvement over a standard dynamic programming algorithm

(SDPA). SDPA requires as many locations for storing the state function
table for each stage, as the total number of state values. For an H—
i=M
dimensional resource allocation problem, this number is 'rr1_i(b),

and

since it grows exponentially, it becomes too large even for a modern day
computer when N > 3 for any realistic problem. Our algorithm avoids
this excessive storage requirement by considering only those points in

the state space at which the function changes value. Thus, whereas
the standard dynamic programming algorithm examines all the lattice
points of the state space, our method examines only a fraction of

these points imbedded in this complete state space. Noting this, Morin

and

Marsten [13] have named the method, "The

Imbedded State Space

Approach".

More

siiificant and, in practice, more important evaluation of

our algorithm can be obtained by comparing it to the MMDP algorithm.

Since both algorithms are aimed at solving the non-linear resourceallocation problems, and the perfonnance of I1DP algorithm is

reported on nine such problems, the identical nine problems were
solved by using a computer implementation of our algorithm. These
problems were constructed from Peterson' s problems [16] and the data

for these
the

is given in Table 1. At the outset, it was evident that

algorithms are

extremely sensitive to the sequence in which

.

—25—

the stage variables of the problem are considered. For example, one
10-constraint, 28-variable problem took 39.9 seconds with one sequence

and 8.5 seconds with another. The empirical performance reported
below was obtained on the problems with the variables arranged in
non-decreasing order of the peak resource consumption ratio defined

below; the variable with the smallest consumption ratio was the first
stage variable. The CPU time taken by the sequencing program is

included in the solution time. The peak resource consumption ratio

for the j-th variable of problem I is given by

max {g1(l)/b1}.
The

data for the nine non-linear problems solved by I'fi'U)P algorithm

is given in Table 1.

.

—26—

TABLE 1: INPUT DATA FOR NON-LINEAR PROBLEMS 1 TO 9

ci
1

2

3

4

5

6

7

8

9

10

60

110

20

40

60

70

10

40

50

50

882

40

40

5

25

25

25

10

20

20

20

650

30

60

0

10

15

20

0

0

15

20

320

22

22

6

6

6

6

8

0

8

8

500

20

20

60

60

60

60

5

55

65

600

24

44

6

9

11

11

0

4
0

6

8

220

90

120

14

24

29

29

6

1

30

30

580

13

13

4

6

7

7

1

0

9

9

90

18

28

10

20

20

20

10

2

28

28

520

28

28

12

18

18

18

0

1

10

10

160

80

100

6

16

20

22

0

2

30

30

403

50

70

0

30

50

50

10

2

25

30

450

40

40

12

20

24

28

0

0

40

50

327

80

100

20

40

50

55

10

2

30

35

400

80

90

30

40

40

40

10

2

20

20

400

32

•32

6

16

21

21

3

0

18

20

140

70

100

.20

30

40

40

4

1

29

29

300

45

75

8

16

19

21

0

0

12

16

205

12

27

5

10

15

20

10

2

25

25

300

20

40

3

11

17

0

1

18

18

100

21

15

25.

3

5

.7

17
9

6

1

12

15

120

22

13

.13

4

8

8

8

0

0

4

4

1200

23

12

12

6

10

13

13

0

0

2

2

600

24

64

75

.18

32

42

48

0

0

0

8

2400

25

30

90

10

20

20

20

0

0

25

25

950

26

41

41

4

12

20

20

0

0

4

2000

50

80

40

50

55

55

10

.3

4

27

50

55

1100

28

20

55

5

13

25

2i

0

0

18

22

480

B

90

120

60

60

60

70

10

45

55

65

RHS

•

.

.

.

.

c'.J

Prob.
.

No.

MMDP
Estimated
(1)

Equivalent

MMDP

Algorithm 2

(4)

Difference

Solution time in seconds of Cyber-72 Cpu

Table 2: General Non-linear Problems

(1)

(3—

46%

(4)

2.6

(3)

3.8

(2

Difference

Storage in 60-bit words
Algorithm 2
(2)

1,025%

320

45

3,600

3.6

1.

5.2

%

%

1,185

82 %

385

6.2

5,140

11.3

52 %

2

970 %

2.5

960

3.8

10,280

%

3

1,831

320

57 %

6,180

3.3

4

5.2

400

%

5,620

J,305

5

73 %

1,061

6.3

770

10.9

8,940

1,096 %

%

6

320

57 %

3,830

2.6
•

4.1

7

1,047 %

53 %

360

3.4

4,130

5.2

8

871 %

79 %

640

5.3

6,220

9,5

9

.

.

—28—

.
Briefly, these problems are of the following form,

c f (x,)

Maximize

j=l

b,

g (xi) <

x

=

f (xi) =v',j;

Problems 3, 6, 9 have
have
in

=

g

6 have f (xi) =

4 -

x;

g (xi) =V; 2, 5,

x. Problems 1

and 7 —

8 have

l,2,...lO

0, 1, 2,3, 4, 5

where the functions f and g are chosen as x2, x or 1.
have

I =

Problems 1 -

3

9 have f (xi) = x.
=

g (xi)

x;

and 1, 4, 7

to 9 correspond to MMDP problems 15 to 23, respectively,

[iè].
The storage requirements and solution times (inclusive of the time

taken by the sequencing program) for these problems are given in Table 7.1.
The storage requirement for the MMDP algorithm depends on the maximum length
UL of the list of undominated entries at any stage, and the total number LL of

feasible list entries generated throughout. Since their paper [12] does not
report these values, we estimate these by solving the problems by Algorithm 2
using the variables in the sequence in whi.ch they appear in the formulations

provided by Morin and Marsten [12]. The solution times thus obtained are
higher than those reported by Morin and Marsten which indicates that a sequence

favorable to one algorithm is not necessarily favorable to the other. Our
interest in this exercise was, however, to estimate the list lengths LL and UL

that were obtained by the MMDP algorithm. When both algorithms use the same
sequence of variables, the list lengths UL and LL yielded by Algorithm 2 provide
good

estimates for

those yielded by the MMDP algorithm. Using, these list

length statistics, we estimate the storage requirement for the MMDP algorithm

as follows. If M is the number of constraints, then each undominated list

—29—

entry takes (2M +

4) computer words, in addition to the 2LL words needed to

store the TRACE entries to enable retracing of the optimal solutions. Thus
the MMDP storage requirement is given by (UL)(2M +

4) + 2LL.

In Algorithm 2

however, the storage requirement depends on L, the maximum size of the list
of feasible entries at any stage, and on fM/Si where IA1 represents the smallest

integer greater than or equal to A. In addition, we need memory space for
storing the state of every tenth entry in the identification phase as mentioned

in chapter 4. Thus the storage requirement for Algorithm 2 is given by
(L)(fM/si + 1) +

(IL/ioi)(rM/s1.

The computer programs for both algorithms were written in CDC's Extended
FORTRAN and were compiled at optimization level 2, so that some code optimization

was obtained. The solution times of the MMDP algorithm reported in P2] are
based on a DCD-6400 computer., whereas those for Algorithm 2 are based on a

Cyber—72. The Cyber-72 is similar to the CDC-6400 except that the CPU is slightly
slower, being comparable to the CPU of a CDC-6200. According to a CDC manual
for same (Boolean) instructions the CDC—6200 takes 1.6 times the time
taken. by the CDC-6400, while for some others (Shift, Memory Access) the factor

is 1.5, and finally for some arithmetic operations (Floating Multiply), the

factor is 1.05. As a reasonable average factor we consider that 1 second of
a CDC-6400 CPU is equivalent to 1.3 seconds of a CDC-6200 CPU. The running
times reported in column (3) of Table 7.1 are then the equivalent Cyber-72 times
for the times reported by Morin and Marsten.

—30—

6.

Comparison of Algprithm 2 with the MMDP Algorithm
Both Algorithm 2 and the MMDP algorithm are aimed at solving general

non—linear, resource—allocation problems involving integer-valued variables.

Nine such problems were solved by Morin and Marsten [12]. The solution times
reported in [12] are the smallest of the times obtained by using two variable
sequencing heuristics and the time taken by the sequencing program is not in-

cluded in reporting the solution times. For Algorithm 2 we solved these same
problems using only the variable-sequencing heuristic given in Section 7.1,

and the time taken by the sequencing program is included in the solution times

reported. For these nine problems, Algorithm 2 performed better both in terms
of high-speed storage requirement and in terms of solution time. Algorithm 2
saved an order of magnitude in storage and so appears significantly better in

this respect. In terms of solution time, the MMDP algorithm took 45 to 82%
more time than that taken by Algorithm2, but this time difference could be
attributed to programming techniques, and is thus not as conclusive as the
storage improvement.

7.

Non—integrality.

The standard dynamic programming algorithm, as well as most other integer programming techniques demand that the resource availabilities b.,
1

and the values taken by the constraint functions g..(.) be integers.
If, for example, b. equals 203.443, or if one of the

values

taken by

g..(.) is 19.4321, then the corresponding constraints will have to
be multiplied with sufficiently large powers of 10 to make these values

integers, accurate to desired number of significant digits. This would
substantially increase the size of the right hand side values and thus

—31—

the storage and computational requirements. In contrast to this, it
is interesting to note that our algorithm is entirely insensitive to

the non—integrality of these values. Instead of examining all lattice
points of the state space as in SDPA, our algorithm examines only
those points itnbedded in the state space at which the state functions

change value. Since the algorithm generates these points as it proceeds, instead of requiring these to be known a priori, the algorithm

handle the non-integral values without any difficulty.

can

8.

Block—angular Constraint Set.

Suppose we have an M by N problem of a diagonal matrix structure

depicted in Figure 1. This problem has a set of M0 coupling constraints,
and several blocks of constraints
+ ...;

N N1 +

N2 + ...;

each of

size M1, il,2.... Let M

arid assume that M0 +

.M* for

M.

M0 +

1,2

Now consider applying our algorithm to this M by N problem. Each list

entry, as usual, will have an M-component state vector. For the first

block of N1 stage variables x, 1
1+

+

<

i

<

M,

will all be identically equal to zero. Thus, the

siguificant information in
only

j . N, the state components S,

the state vectors can

be stored by storing

the first (M0 + N1) non—zero state components.

Now consider the second block of stage variables x, 1 +

remain unchanged for all subsequent stages j >
1 +

<i

For these stages, M1 state components Si, 1 +

N2.

N.

M1.

to N1 +

+

1

N1.

<

M0

<

j

<

Thus, for stages

+
M1

<

i

<i

<

+

M0

+
M1

in addition to the first M0 state components of the coupling constraints,
since the components Si, 1 +
zero.

M0

+

+ M2 <

i

+

N.

+ M1, will

N2, we do not need the state components Si, 1 +

We need only theM2 state components Si, 1 +

N1

< M are identically equal to

+

M2,

—32—

Continuing in this manner, we see that during the stages in the k-th

block, we need store and update only the Mk state components Si, 1 + EM1
<

i

<

in addition to the first M0 state components corresponding to

M,

the coupling constraints. At the beginning of the next block,j =

1

+ N1 +

+ Nk we can clear out the storage locations containing the Mk components
<

Si, 1 +

i

<

M

and use the same locations to store the newly

active Mk+l components of S1, 1 + E M1 <
for i =

1,2,...,

i<

M1. Hence, if M +M1<M*

then M* storage locations per list entry to store the

active state components will be sufficient. Thus we will have effectively
reduced the original M by N problem to an M* by N problem.

Resource-allocation problems of this special structure can occur,
for example, in situations involving multiple time-period planning, or in
situations involving variables that represent activities that use mutually

exclusive classes of resources in addition to a few common resources that
couple them together. In most such situations

much larger than M*. Thus we can achieve

=

M

is likely to be

substantial storage savings

by recognizing and exploiting the block-angular constraint structure.

Note also that in order to achieve this storage reduction, we did not have

to incur any additional computation. In fact, by noting that many state
components are inactive and hence need not be computed, we have actually
reduced the amount of computation by a factor comparable to that for the

reduction in storage. The above scheme of dealing with block-angular
constraint structure was empirically tested on a few problems, and the
substantial storage savings and some computational savings obtained are
reported in

1

.

—33—

This can be easily generalized further to include the cases where the
variables that use the same resources or have non-zero functions in

the same constraints do not belong to adjacent blocks. Such a situation can occur in resource allocation and production scheduling problems
where some resources are used by variables in more than one but not in

all blocks. Usually such constraints will be lumped with the coupling
constraints. We call such a constraint structure a split-block-angular
structure and it is depicted in Figure 2. Our algorithm handles such a
structure with only a slight additional bookkeeping work. For further
details regarding the split-block-angular structure, see [1].

N

M51

1
Figure 2

— 314_

9. Mixed-Integer

Problems

The decision-state method can very easily handle problems where
some variables are discrete and some are continuous, as long as the van-

ables are sum-separable. Consider the following mixed problem;
Maximize

f(x) +

Subject to

g(x) +

f (y)

g(y',)

<

b.,

1 <

i<M

x e I,

1< j<N

Y ER,

1 <j<N'.

We will divide the mixed problem into two component problems..
Continuous—Component Problem:

Subject

Max E f't')
to

g'(y') b., 1

y' R...,l

I

M

<j <N'

DiscreteComponent Problem: Max E! f(x)

Subject toEg(x)<

x3cI,

I

< M

1 <j

<N

<

The continuous-component problem can be solved parametrically on b =

(bl,b2...,bM),the

right-hand-side, i.e., the optimal solution can be ob-

tained for the continuous—component problem for all feasible right-hand-

side values ranging between zero and the given right-hand-side. This can
be done by any conventional, non-linear programming techniques such as
separable programming [18], [19]; a most advantageous technique which is

inherently parametric on the right-hand-side is GLM o]. In the simplest
situation, which is perhaps the most prevalent in practice, the continuous—

—35—

component problem is a linear programming problem. In this case, most
production LP codes would be capable of providing the optimal solutions w4th
some computational effort, for all feasible right-hand-sides.

The discrete-component problem, of course, can be solved by the
decision-state method, which, being a dynamic programming method, automatically gives the optimal solution for all feasible right-hand side

values. Let us denote the list of optimal solutions to the discrete-component problem as LD. Each entry in the list LD contains an optimal solution,
the corresponding state vector and, the optimal value.

Let LD be

arranged in non-decreasing order of the optimal values. Then given any
right-hand-side vector bs. <

b,

the optimal solution to the discrete problem

with right hand side b' is given by the entry with the largest objective

value whose state vector does not exceed b' in any component. Similarly,
let LC denote the list of optimal solutions to the continuous component-

problem, parametrically on b, arranged in a manner similar to ID. Then
we can obtain the optimal solution to the mixed problem by determining a
combination of one entry from LD, (D',S',v') c LD, and one entry from LC,

c LC, such that v+v11 >v1+v2 for all entries (&,S1,v1)
and (D2,S21v2) c IC, and such that combination is feasible.

LD

—36—

10.

Bounding Elimination

Considering the given N-stage problem as a decision tree, each
feasible decision vector D =

(x1,

x2, ...,

Xk) can be regarded as a node at

the k-th echelon. The main thrust of the decision-state method is to generate only the feasible nodes on the undominated branches of the decision tree.

When a list entry (D, s, v) corresponding to a node is found dominated, we
know that the branches emanating from this node cannot be optimal for any

member of the family of problems with identical f(.) g1(.) functions and
with different right—hand sides. Typically, however, our lists will contain
many entries that are undominated but are non-optimal for the particular

problem being solved. It would be useful, therefore, if we find a way to
detect and eliminate some of these 'dead-weight' entries in addition to
eliminating those that are clearly dominated.

The directed-tree-search methods eliminate such nodes through the

technique of bounding. That is, if the upper bound on the objective value
for all nodes on the branches emanating from a given node is smaller than
the value of the current best feasible solution, then the given node is said

to be fathomed and can be eliminated. A similar approach can be used with
our method. There are a number of ways in which an upper bound can be
obtained. One simple way is to first identify one constraint, say the 9-th,
as the tightest or the most binding constraint. Then the upperbound for a
k-th stage node (D, S, v) is given by v +

greatest integer for which g() <

(b

f () where

is the

s) for each j from k+l to N. A

better upper bound can be obtained through a little more computational effort
as v + Z

for each .3 from k+l to N is computed as the

+l f() where

3 — (b.1

greatest integer for which g. .(x.) <
1,)

-

s.)

for all i from 1 to M. The

.

—37—

upper bounds obtained in these ways are likely to be loose.

Thus, they will

tend to be useful in eliminating a significant number of list entries when
only the unproductive stage variables remain to be considered and most of

the productive stage variables have already been considered. A workable
strategy, then, may be to call upon bounding elimination only towards the

tail end of the algorithm, the exact stage number dependent on the particular
problem.

Of course, better and tighter bounds can be obtained by other so-

phisticated technqiues from the branch and bound category. A crucial question,
however, is whether the computational effort expended in doing this would

pay off in terms of reducing the entry lists and their computation. This
area needs to be investigated empirically by solving problems of realistic

sizes and structures. In the absence of such hard evidence, it seems that
the option of bounding elimination should first be tried with the crude,
easily obtainable upper bounds.

The above bounding elimination procedure assumes knowledge of a

lower bound, that is, the value of the current best feasible solution. Of
course, the entry in the current list with the largest objective value identi-

fied during the merging phase obviously gives such a lower bound. It may be
possible, however, to improve this lower bound by using up the unused re-

source amounts. If the entry with the largest value is (D,S,v), then an
improved lower bound can be obtained as v + Z
greatest integer such that

(b1 —

f(i) where xk+1 is the

s1)

the greatest integer such that

—

for all i and

s.

-

for

all

1, etc. The larger the lower bound, and the smaller the upper bound associated with the nodes, the more list entries can be eliminated by this bounding

elimination procedure. A good discussion of different bounding strategies, and
their empirical evaluation can be found in [al].

—38—

11.

Conclusions

In this paper we presented a new method for obtaining exact

optimal solutions to certain types of discrete-variable, non-linear, resource

allocation problems. The new method is called the decision-state method because, unlike the conventional dynamic programming method which works only in
the state space, the new method works in the decision space as well as the

state space. It generates and retains only a fraction of the points in the
state space, thus overcoming much of the curse of dimensionality. It carries
the cumulative decision strings associated with these points, and thus avoids

the backtracking entailed bythe conventional dynamic programming method for
recovery of the optimal decisions at all the stages.

A concise yet complete and self-contained statement of the method was
given in Chapter 3 in the form of an algorithm (Algorithm 2), and it was proven
there that the algorithm indeed finds all exact optimal solutions to the given
general, non-linear, resource allocation problem with discrete—value variables.
In Chapter 5 we considered problems with special conditions such as blockangular or split—block-angular constraints, non-integrality and core limitations.

We showed how the method could be specialized or adapted to accommodate effectively the above conditions. In Chapter 6 we considered such tactical options
as sequencing the decision-variables, sequencing and inclusion of constraints,
and bounding elimination.

Although an algorithmic statement of a problem-solving method may be
precise and complete enough mathematically or theoretically, the algorithm can

.

—39—

be. implemented on a computer in different ways, some being more efficient than

others. We gave, therefore, an advantageous computer implementation of the
decision-state Algorithm 2 which combines the flexibility and adaptability of
the basic algorithm with the characteristics of a particular digital computer

and some good programing practices.
As with most large-scale, general-purpose, mathematical programming

methods, the decision-state method offers certain options or opportunities for
making the application of the method to a particular problem more advantageous.
We discussed the availability and use of such options with regard to the method

as well as in the formulation of the problem. We also gave some empirical
evaluation of some of the different options that are intuitively appealing.

The computer implementation of Algorithm 2 developed in this dissertation
was empirically tested and evaluated by using a number of resource allocation
problems from the open literature and a number of problems that were artificially
constructed to have certain desired structural characteristics simulating ex-

pected real conditions. The performance of Algorithm 2 was also compared with
that of the MMDP algorithm of Morin and Marsten on identical problems. For all
9 problems run with both algorithms, Algorithm 2 performed consistently better
than the MMDP algorithm, both in terms of high-speed memory requirement and in

terms of solution time. In fact Algorithm 2 achieved an order of magnitude
saving in memory requirement, and the MMDP algorithm took 45 to 82% more time

than that taken by Algorithm 2. Although the storage saving is substantial,
the time saving is not, since the latter might be attributed to the programming
techniques used.

_L.O_

REFERENCES

1. Dharmadhikari, V.K., "Solving Discrete-Variable Multiple-Constraint
Non-linear Programs: The Decision-State Method; doctoral dissertation,
Department of Computer Science and Operations Research, Southern
Methodist University, Dallas, Texas, (1975).
2.

Bellman, R., Dynamic Programming, Princeton University Press, Princeton,
N.J., (1957).

3.

Larson, R.E., State Increment Dynamic Programming, American Elsevier
Publishing Company, New York, (1968).

4.

Wong, P.3., and Luenberger, D.G., "Reducing Memory Requirements of
Dynamic Programming" Oper. Res., 16, 1115 - 1125, (1968)

5.

Wong, P.3., "A New Decomposition Procedure for Dynamic Programming,"
Oper. Res., 18, 119—131, (1970).

6.

Yormark, J.S., and Baker, N.R., "On a Two—Dimensional Resource Allocation Problem," presented at 39th ORSA Meeting, Dallas, Texas, (1971).

7.

Weingartner, H.M., and Ness, D.N., "Methods for the Solution of the MultiDimensional 0-1 Knapsack Problem," Oper. Res., 15, 83—103, (1967).

8.

Nemhauser, G.L., and Ullman, Z., "Discrete Dynamic Programming and
Capital Allocation," Management Sci., 15, 494-505, (1969).

9.

Haymond, R.E., "Discontinuities in the Optimal Return in Dynamic Programming," Jour. Math. Anal. Appl., 30, 159-169, (1970).

10. Gilmore, P.C., and Gomory, R.E., "The Theory and Computation of Knapsack
Functions," Oper. Res., 14, 1045-1074, (1966).

11. Geoffrion, A.M., and Marsten, RE., "Integer Programming Algorithms:
A Framework and State—of—the-Art Survey," Management Sci., 18, (1972).
12. Morin, T.L., and Marsten, R.E., "An Algorithm for Non-linear Knapsack
Problems," presented at ORSA/TIMS Meeting, Boston, Mass., (1974).
13. Morin, T.L., and Marsten, R.E., "An Efficient Dynamic Programming Algorithm for the General Non-linear Multidimensional Knapsack Problem,"
Discussion Paper (Rough Draft) I.E. .and M.S. Dept., Northwestern University,
Evanston, Ill., (1972).

14. Morin, T.L., and Esogbue, A.0., "Reduction of Dimensionality in Dynamic
Programming of Higher Dimensions with the Imbedded State Space Approach,"
Discussion Paper, IE and MS Dept., Northwestern University, Evanston, Ill.,
(1973).

15. Knuth, D.E., The Art of Computer Programmin9, Vol. 1, 2, 3, Addison—
Wesley Publishing Company, Reading, Mass., (1968).

—'41—

16.

Peterson, C.C., "Computational Experience with Variants of the Balas
Algorithm Applied to the Selection of R and D Projects," Management
Sci., 13, 736-750, (1967).

17. Grishman, Ralph, Assembly Language Programing for the Control Data
6000 Series and the Cyber 70 Series, Algorithmic Press, New York, (1974).
18. Hadley, G., Nonlinear and Dynamic Programming, Addison-Wesley Publishing
Company, Reading, Mass., (1964).

19. Nemhauser, G.L., Introduction to Dynamic Programming, John Wiley, New
York, (1966).
20. Greenberg, H.J., and Robbins, T.C., "The Theory and Computation of
Everett's Lagrange Multipliers by Generalized Linear Programing,"
Technical Report CP-70008, Computer Science/Operations Research Center,
Southern Methodist University, (1971).
21. Morin, T.L., and Marsten,R.E., "Branch and Bound Strategies for
Dynamic Programming," Working Paper WP 750-754, Sloan School of Management,
Massachusetts Institute of Technology, Cambridge, Mass., (1974).

