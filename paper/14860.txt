NBER WORKING PAPER SERIES

COMPLEMENTARITY AND AGGREGATE IMPLICATIONS OF ASSORTATIVE MATCHING:
A NONPARAMETRIC ANALYSIS
Bryan S. Graham
Guido W. Imbens
Geert Ridder
Working Paper 14860
http://www.nber.org/papers/w14860

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
April 2009

Financial support for this research was generously provided through NSF grant SES 0136789, SES
0452590, and SES 0820361. We thank participants in seminars at Harvard-MIT, Princeton, UC Berkeley,
NBER Labor Studies, Cemmap, and Brown University for comments. We thank Cristine Pinto for
excellent research assistance. The views expressed herein are those of the author(s) and do not necessarily
reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
¬© 2009 by Bryan S. Graham, Guido W. Imbens, and Geert Ridder. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including ¬© notice, is given to the source.

Complementarity and Aggregate Implications of Assortative Matching: A Nonparametric
Analysis
Bryan S. Graham, Guido W. Imbens, and Geert Ridder
NBER Working Paper No. 14860
April 2009
JEL No. C14,C21,C52
ABSTRACT
This paper presents methods for evaluating the effects of reallocating an indivisible input across production
units, taking into account resource constraints by keeping the marginal distribution of the input fixed.
When the production technology is nonseparable, such reallocations, although leaving the marginal
distribution of the reallocated input unchanged by construction, may nonetheless alter average output.
Examples include reallocations of teachers across classrooms composed of students of varying mean
ability. We focus on the effects of reallocating one input, while holding the assignment of another,
potentially complementary, input fixed. We introduce a class of such reallocations -- correlated matching
rules -- that includes the status quo allocation, a random allocation, and both the perfect positive and
negative assortative matching allocations as special cases. We also characterize the effects of local
(relative to the status quo) reallocations. For estimation we use a two-step approach. In the first step
we nonparametrically estimate the production function. In the second step we average the estimated
production function over the distribution of inputs induced by the new assignment rule. These methods
build upon the partial mean literature, but require extensions involving boundary issues. We derive
the large sample properties of our proposed estimators and assess their small sample properties via
a limited set of Monte Carlo experiments.

Bryan S. Graham
Department of Economics
University of California, Berkeley
508-1 Evans Hall #3880
Berkeley, CA 94720-3880
and NBER
bgraham@econ.berkeley.edu
Guido W. Imbens
Department of Economics
Littauer Center
Harvard University
1805 Cambridge Street
Cambridge, MA 02138
and NBER
imbens@fas.harvard.edu

Geert Ridder
Department of Economics
University of Southern California
Kaprielian Hall
Los Angeles, CA 90089
ridder@usc.edu

1

Introduction

Consider a production function depending on a number of inputs. We are interested in the
effect of a particular input on output, and specifically in the average effects of policies that
change the allocation of this input across production units. For each production unit output
may be monotone in this input, but at different rates. If the input is indivisible, and its
aggregate stock fixed, it is impossible to simultaneously raise the input level for all production
units. In such cases it may be of interest to consider the output effects of reallocations of
the input across production units. Here we investigate econometric methods for assessing
the effect of such reallocations on average output. We will call the average causal effects of
such policies Aggregate Redistributional Effects (AREs). A key feature of the reallocations
we consider is that, although they potentially alter input levels for each firm, they keep the
marginal distribution of the input across the population of firms fixed.
The first contribution of our paper is to introduce a framework for considering such reallocations, and to define novel estimands that capture their key features. These estimands
include the effects of focal reallocations, and a semiparametric class of reallocations, as well as
the effect of a local reallocation. One focal reallocation redistributes the input across production units such that it has perfect rank correlation with a second input. We refer to this as
the positive assortative matching allocation. We also consider a negative assortative matching
allocation where the primary input is redistributed to have perfect negative rank correlation
with the second input. A third allocation involves randomly assigning the input across firms.
This allocation, by construction, ensures independence of the two inputs. A fourth allocation
simply maintains the status quo assignment of the input. More generally, we consider a two
parameter family of feasible reallocations that include these four focal allocations as special
cases. Reallocations in this family may depend on the distribution of a second input or firm
characteristic. This characteristic may be correlated with the firm-specific return to the input
to be reallocated. Our family of reallocations, called correlated matching rules, includes each
of the four focal allocations as special cases. In particular the family traces a path from the
positive to negative assortative matching allocations. Each reallocation along this path keeps
the marginal distribution of the two inputs fixed, but it induces a different level of correlation
between the two inputs. Each of the reallocations we consider are members of a general class of
reallocation rules that keep the marginal distributions of both inputs fixed. We also provide a
local measure of complementarity that requires much weaker conditions on the support of the
input distribution. This estimand measures whether a small step away from the status quo,
towards perfect assortative matching allocation raises average output.
The second contribution of our paper is to derive statistical methods for estimation and
inference for the proposed estimands. We derive an estimator for average output under all
correlated matching allocations, and for the local complementarity measure. Our estimator
requires that the first input is exogenous conditional on the second input and additional firm
characteristics. Except for the case of perfect negative and positive rank correlation the estimator has the usual parametric convergence rate. For the two extremes the rate of convergence
is slower, comparable to that of estimating a regression function with a scalar covariate at a
point. In all cases we drive the asymptotic distribution of the estimator. In the first step of
the estimation procedure we use a nonparametric estimator for the production function. We
modify existing kernel estimators to deal with boundary issues that arise in our setting.
Our focus on reallocation rules that keep the marginal distribution of the inputs fixed is

[1]

appropriate in applications where the input is indivisible, such as in the allocation of teachers
to classes, or managers to production units. In other settings it may be more appropriate to
consider allocation rules that leave the total amount of the input constant by fixing its average
level. Such rules would require some modification of the methods considered in this paper.
Our methods may be useful in a variety of settings. One class of examples concerns complementarity of inputs in production functions (e.g. Athey and Stern, 1998). If the first and
second inputs are everywhere complements, then the difference in average output between the
positive and negative assortative matching allocations provides a nonparametric measure of
the degree of complementarity. This measure is invariant to monotone transformations of the
inputs. If the production function is not supermodular, the interpretation of this difference is
not straightforward, although it still might be viewed as some sort of ‚Äòglobal‚Äô measure of input
complementarity.
A second example concerns educational production functions. Card and Krueger (1992)
study the relation between adult wages and teacher quality. Teacher quality may improve
outcomes for all students, but average outcomes may be higher or lower depending on whether,
given a fixed supply of teachers, the best teachers are assigned to the least prepared students
or vice versa. Parents concerned solely with outcomes for their own children may be most
interested in the effect of raising teacher quality on expected outcomes. A school board, however,
may be more interested in maximizing expected outcomes given a fixed set of classes and and
a fixed set of teachers, by optimally matching teachers to classes.
A third class of examples arises in settings with social interaction (c.f., Manski 1993; Brock
and Durlauf 2001). Sacerdote (2001) studies peer effects in college by looking at the relation between individual outcomes and roommate characteristics. From the perspective of the
individual student it may again be of interest whether having a roommate with different characteristics would, in expectation, lead to a different outcome. This is what Manski (1993) calls
an exogenous or contextual effect. The college, however, may be interested in a different effect,
namely the effect on average outcomes of changing the procedures for assigning roommates.
While it may be very difficult for a college to quickly change the distribution of characteristics
in incoming classes, it may be under its control to change the way roommates are assigned.
In Graham, Imbens and Ridder (2006b) we study the peer effect setting further, developing
methods appropriate for social groups of arbitrary size when agents are binary-typed. Our
focus in that work is on the outcome and inequality effects of segregation.
If production functions are additive in inputs, the questions posed above have trivial answers: average outcomes are invariant to input reallocations. Although reallocations may raise
outcomes for some units in that case, they will necessarily lower them by an offsetting amount
for others. Reallocations are zero-sum games in this additive setting. With additive and linear
functions, even more general assignment rules that allow the marginal input distribution to
change, while keeping its average level fixed, do not affect average outcomes. In order for these
questions to have non-trivial answers, one therefore needs to explicitly recognize, and allow
for, non-additivity and non-linearity of a production function in its inputs. For this reason our
approach is fully nonparametric.
The current paper builds on the larger treatment effect and program evaluation literature.1
More directly, it is complementary to the small literature on the effect of treatment assignment
rules (Manski, 2004; Dehejia, 2004; Hirano and Porter, 2005). Our focus is different from
1
For recent surveys see Angrist and Krueger (2001), Heckman, Lalonde and Smith (2000), and Imbens and
Wooldridge (2009).

[2]

that in the Manski, Dehejia, and Hirano-Porter studies. First, we allow for continuous rather
than discrete or binary treatments. Second, our assignment policies take into account resource
constraints (by leaving unchanged he marginal distribution of the treatment), whereas in the
previous papers treatment assignment for one unit is not restricted by the assignments for other
units. Our policies are redistributions. In the current paper we focus on estimation and inference
for specific assignment rules. It is also interesting to consider optimal rules as in the Manski,
Dehejia and Hirano-Porter studies. The class of feasible reallocations/redistributions includes
all joint distributions of the two inputs with fixed marginal distributions. When the inputs are
continuously-valued, as we assume in the current paper, this class of potential rules is very large.
Characterizing the optimal allocation within this class is therefore a non-trivial problem. When
both inputs are discretely-valued the problem with finding the optimal allocation is tractable
as the joint distribution of the inputs is characterized by a finite number of parameters. In
Graham, Imbens and Ridder (2006a) we consider optimal allocation rules when both inputs are
discrete, allowing for general complementarity or substitutability of the inputs.
Our paper is also related to recent work on identification and estimation of models of
social interactions (e.g., Manski, 1993; Brock and Durlauf, 2001; Graham, 2008; Moffitt, 2001).
We do not focus on directly characterizing the within-group structure of social interactions, an
important theme of this literature. Rather our goal is simply to estimate the average relationship
between group composition and outcomes. The average we estimate may reflect endogenous
behavioral responses by agents to changes in group composition, or even equal an average over
multiple equilibria. Viewed in this light our approach is reduced form in nature. However it is
sufficient for, say, an university administrator to characterize the outcome effects of alternative
roommate assignment procedures, as long as the average response to group composition remains
unchanged across such procedures.
The econometric approach taken here builds on the partial mean literature (e.g., Newey,
1994; Linton and Nielsen, 1995). In this literature one first estimates a regression function
nonparametrically. In the second stage the regression function is averaged, possibly after some
weighting with a known or estimable weight function, over some of the regressors. Similarly
here we first estimate a the production function nonparametrically as the conditional mean
of the outcome given the observed inputs. In the second stage the averaging is over the distribution of the regressors induced by the new assignment rule. This typically involves the
original marginal distribution fo some of the regressors, but a different conditional distribution
for others. Complications arise because this conditional covariate distribution may be degenerate, which will affect the rate of convergence for the estimator. In addition the conditional
covariate distribution itself may require nonparametric estimation through its dependence on
the assignment rule. For the policies we consider the assignment rule will involve distribution
functions and their inverses similar to the way these enter in the changes-in-changes model of
Athey and Imbens (2006).
The next section lays out our basic model and approach to identification. Section 3 then
defines and motivates the estimands we seek to estimate. Section 4 presents of our estimators,
and derives their large-sample properties, for the case where inputs are continuously-valued.
Section 5 presents the results from a small Monte Carlo exercise.

[3]

2

Model

In this section we present the basic set up and identifying assumptions. For clarity of exposition
we use the production function terminology; although our methods are appropriate for a wide
range of applications, as emphasized in the introduction. For production unit or firm i, for
i = 1, . . ., N , the production function relates a triple of observed inputs, (Wi , Xi, Vi), and an
unobserved input Œµi , to an output Yi :
Yi = k(Wi , Xi, Vi, Œµi).

(2.1)

The inputs Wi and Xi, and the output Yi are scalars. The third observed input Vi and the
unobserved input Œµi can both be vectors. We are interested in reallocating the input W across
production units. We focus upon reallocations which hold the marginal distribution of W fixed.
As such they are appropriate for settings where W is a plausibly indivisible input, such as a
manager or teacher, with a certain level of experience and expertise. The presumption is also
that the aggregate stock of W is difficult to augment. In addition to W there are two other
(observed) firm characteristics that may affect output: X and V , where X is a scalar and V
is a vector of dimension LV . The first characteristic X could be a measure of, say, the quality
of the long-run capital stock, with V being other characteristics of the firm such as location
and age. These characteristics may themselves be inputs that can be varied, but this is not
necessary for the arguments that follow. In particular the exogeneity assumption that we make
for the first input need not hold for these characteristics.
We observe for each production unit, indexed by i = 1, . . ., N , the level of the input, Wi ,
the characteristics Xi and Vi , and the realized output level, Yi . In the educational example the
unit of observation would be a classroom. The variable input W would be teacher quality, and
X would be a measure of quality of the class, e.g., average test scores in prior years. The second
characteristic V could include other measures of the class, e.g., its age or gender composition,
as elements. In the roommate example the unit would be the individual, with W the quality
of the roommate (measured by, for example, a high school test score), and the characteristic
X would be own quality. The second set of characteristics V could be other characteristics of
the dorm or of either of the two roommates such as smoking habits (which may be used by
university administrators in the assignment of roommates).
Our key identifying assumption is that conditional on firm characteristics (X, V ) the assignment of W , the level of the input to be reallocated, is exogenous:
Assumption 2.1 (Exogeneity)
Œµ ‚ä• W

X, V.

Let
g(w, x, v) = E[Y |W = w, X = x, V = v],

(2.2)

œÉ 2 (w, x, v) = V[Y |W = w, X = x, V = v],

(2.3)

and

denote the expectation and the variance of the output conditional on input level w and characteristics x and v. We often refer to the derivative of g(w, x, v) with respect to w, which will
[4]

be denoted by
gW (w, x, v) =

‚àÇ
g(w, x, v),
‚àÇw

(2.4)

Under exogeneity we have ‚Äì among firms with identical values of X and V ‚Äì an equality between
the counterfactual average output that we would observe if all firms in this subpopulation
were assigned W = w, and the average output we observe for the subset of firms within this
subpopulation that are in fact assigned W = w. Alternatively, the exogeneity assumption
implies that the difference in g(w, x, v) evaluated at two values of w, w0 and w1 , has a causal
interpretation as the average effect of assigning W = w1 rather than W = w0 :
g(w1, x, v) ‚àí g(w0, x, v) = E [ k(w1 , X, V, Œµ) ‚àí k(w0 , X, V, Œµ)|X = x, V = v] .
Assumption 2.1 is often controversial. It holds under conditional random assignment of W to
units; as would occur in a randomized experiment. However randomized allocation mechanisms
are also used by administrators in some institutional settings. For example some universities
match freshman roommates randomly conditional on responses to housing questionnaires (e.g.,
Sacerdote 2001). This assignment mechanism is consistent with Assumption 2.1. In other
settings, particularly where assignment is bureaucratic, as may be true in some educational
settings, a plausible set of conditioning variables may be available. In this paper we focus
upon identification and estimation under Assumption 2.1. In principle, however, the methods
could be extended to accommodate other approaches to identification based upon, for example,
nonparametric instrumental variables methods (e.g., Matzkin, 2008, Imbens and Newey, 2009).
Much of the treatment effect literature (e.g., Angrist and Krueger, 2000; Heckman, Lalonde
and Smith, 2000; Manski, 1990; Imbens and Wooldridge, 2009) has focused on the average
effect of an increase in the value of the treatment. In particular, in the binary treatment case
(w ‚àà {0, 1}) interest has centered on the average treatment effect
E[g(1, X, V ) ‚àí g(0, X, V )].
With continuous inputs one may be interested in the full average output function g(w, x, v)
(Imbens, 2000; Flores, 2005) or in its derivative with respect to the input,
gW (w, x, v),
at a point, or a weighted average,
E [œâ(W, X, V ) ¬∑ gW (W, X, V )] ,
See Powell, Stock and Stoker (1989) or Hardle and Stoker, (1989) for estimands of this type.
Here we are interested in a fundamentally different class of estimands, one which has received
little attention in the econometrics literature. We focus on policies that redistribute the input
W , according to a rule based on the X characteristic of the unit. For example upon assignment
mechanisms that match teachers of varying experience to classes of students based on average
ability in the classes. One might assign those teachers with the most experience (highest
values of W ) to those classrooms with the highest ability students (highest values of X) and
so on. In that case average outcomes would reflect perfect rank correlation between W and
X. Alternatively, we could be interested in the average outcome if we were to assign W to
[5]

be negatively perfectly rank correlated with X. A third possibility is to assign W so that it
is independent of X. We are interested in the effect of such policies on the average value of
the output. We refer to such effects in general as Aggregate Redistributional Effects (AREs).
The three reallocations mentioned are a special case of a general set of reallocation rules that
fix the marginal distributions of W and X, but allow for correlation in their joint distribution.
For perfect assortative matching the correlation is 1, for negative perfect assortative matching
-1, and for random allocation 0. By using a bivariate normal cupola we can trace out the path
between these extremes.
We wish to emphasize that there are at least two limitations to our approach. First, we focus
on comparing specific assignment rules, rather than searching for the optimal assignment rule.
The latter problem is a particularly demanding problem in the current setting with continuouslyvalued inputs as the optimal assignment for each unit depends both on the characteristics of
that unit as well as on the marginal distribution of characteristics in the population. When
the inputs are discretely-valued both the problems of inference for a specific rule as well as
the problem of finding the optimal rule become considerably more tractable. In that case any
rule, corresponding to a joint distribution of the inputs, is characterized by a finite number of
parameters. Maximizing estimated average output over all rules evaluated will then generally
lead to the optimal rule. Graham, Imbens and Ridder (2006a) and, motivated by an early
version of the current paper, Bhattacharya (2008), provide a discussion for the case with discrete
covariates.
A second limitation is that the class of assignment rules we consider leaves all aspects of the
marginal distribution of the inputs unchanged. This latter restriction is perfectly appropriate in
cases where the inputs are indivisible, as, for example, in the social interactions and educational
examples. In other cases one need not be restricted to such assignment rules. A richer class
of estimands would allow for assignment rules that maintain some aspects of the marginal
distribution of inputs but not others. An interesting class consists of assignment rules that
maintain the average (and thus total) level of the input, but allow for its arbitrary distribution
across units. This can be interpreted as assignment rules that ‚Äúbalance the budget‚Äù. In such
cases one might assign the maximum level of the input to some subpopulation and the minimum
level of the input to the remainder of the population. Finally, one may wish to consider arbitrary
decision rules where each unit can be assigned any level of the input within a set. In that
case interesting questions include both the optimal assignment rule as a function of unit-level
characteristics as well as average outcomes of specific assignment rules. In the binary treatment
case such problems have been studied by Dehejia (2005), Manski (2004), and Hirano and Porter
(2005).

3

Aggregate Redistributional Effects

Let fW |X,V (w|x, v) denote the conditional distribution of W given (X, V ) in the data, and let
fÀúW |X,V (w|x, v) denote a potentially different conditional distribution. We will allow fÀúW |X,V (w|x, v)
to correspond to any distribution such that the implied marginal distribution for Wi remains
unchanged, or
Z
Z
Àú
fW |X,V (w|x, v)fX,V (x, v)dwdxdv = fW |X,V (w|x, v)fX,V (x, v)dwdxdv.

[6]

This includes degenerate conditional distributions. In general we are interested in the average
outcome that would result from the current distribution of (X, V, Œµ), if the distribution of W
given (X, V ) were changed from its current distribution, fW |X,V (w|x, v) to fÀúW |X,V (w|x, v). We
denote the expected output given such a reallocation by
Z
are
Œ≤fÀú = g(w, x, v)fÃÉW |X,V (w|x, v)fX,V (x, v)dwdxdv.
(3.5)
In the next two sections we discuss some specific choices for fÀú(¬∑).

3.1

Positive and Negative Assortive Matching Allocations

The first estimand we consider is expected average outcome given perfect assortative matching
of W on X conditional on V :
h 
i
‚àí1
Œ≤ pam = E g FW
(F
(X|V
)|V
),
X,
V
,
(3.6)
X|V
|V

‚àí1
where FX|V (X|V ) denotes the conditional CDF of X given V , and FW
|V (q|V ) is the q-th quan-

‚àí1
tile (for q ‚àà [0, 1]) associated with the conditional distribution of W given V (i.e., FW
(q|V ) is
|V

‚àí1
a conditional quantile function). Therefore FW
|V (FX|V (X|V )|V ) computes a unit‚Äôs location on
the conditional CDF of X given V and reassigns it the corresponding quantile of the conditional
distribution of W given V . Thus, among units with the same realization of V , those with the
highest value of X are reassigned the highest value of W , and so on.
In order for Œ≤ pam to be well defined, we need some conditions on the joint distribution of
(Y, W, X, V ). We do not state these conditions here explicitly. When we discuss estimation,
in Section 4, we provide conditions for consistent estimation., including compact support and
smooth distributions for (W, X), and moment conditions for the conditional distribution of Y
given (W, X). These conditions imply that Œ≤ pam is well defined.
The focus on reallocations within subpopulations defined by V , as opposed to populationwide reallocations, is motivated by the fact that the average outcome effects of such reallocations
solely reflect complementarity or substitutability between W and X. To see why this is the
case consider the alternative estimand


‚àí1
Œ≤ pam‚àípop = E g FW
(FX (X)) , X, V .
(3.7)

This gives average output associated with population-wide perfect assortative matching of W
on X. If, for example, X and V are correlated, then this reallocation, in addition to altering
the joint distribution of W and X, will alter the joint distribution of W and V . Say V is also a
scalar and is positively correlated with X. Population-wide positive assortative matching will
induce perfect rank correlated between W and X, but it will also affect the degree of correlation
between W and V . This complicates the interpretation of the estimand when g (w, x, v) is nonseparable in w and v, as well as in w and x.
An example helps to clarify the issues involved. Let W denote an observable measure of
teacher quality, X mean (beginning-of-year) achievement in a classroom, and V the fraction
of the classroom that is female. If begining-of-year achievement varies with gender, (say, with
classes with a higher fraction of girls having higher average achievement) then X and V will be
correlated. A reallocation that assigns high quality teachers to high achievement classrooms,
will also tend to assign such teachers to classrooms will an above average fraction of females.
[7]

Average achievement increases observed after implementing such a reallocation may reflect
complementarity between teacher quality and begining-of-year student achievement or it may
be that the effects of changes in teacher quality vary with gender and that, conditional on
gender, their is no complementarity between teacher quality and achievement. By focusing
on reallocations of teachers across classrooms with similar gender mixes, but varying baseline
achievement, (3.6) provides a more direct avenue to learning about complementarity between
W and X.2
Both (3.6) and (3.7) may be policy relevant, depending on the circumstances, and both
are identified under Assumption 2.1 and additional support conditions (which we make explicit
below). Under the additional assumption that
g(w, x, v) = g1 (w, x) + g2 (v),
the estimands, although associated with different reallocations, also have the same basic interpretation. In the current paper we focus upon (3.6), although it is conceptually straightforward
to extend our results to (3.7).
Our second estimand is the expected average outcome given negative assortative matching:
i
h 

‚àí1
1
‚àí
F
(X|V
)|V
,
X,
V
.
(3.8)
Œ≤ nam = E g FW
X|V
|V

If, within subpopulations homogenous in V , the two inputs W and X are everywhere complements, then the difference Œ≤ pam ‚àí Œ≤ nam provides a measure of the strength of input complementarity. When g (¬∑) is not supermodular, the interpretation of this difference is not straightforward. In Section 3.1 below we present a measure of ‚Äòlocal‚Äô (relative to the status quo allocation)
complementarity between X and W .

3.2

Correlated Matching Allocations

The perfect positive and negative assortative allocations are focal allocations, being emphasized
in the economic theory literature (e.g., Becker and Murphy, 2000; Legros and Newman, 2004).
There are many more possible allocations. Two others that are of particular importance are the
status quo allocation, and the random matching allocation. Average output under the status
quo allocation is given by
Œ≤ sq = E[Y ] = E[g(W, X, V )].
Average output under the random matching allocation is given by

Z Z Z
Œ≤ rm =
g(w, x, v)fW |V (w|v)fX|V (x|v) fV (v)dwdxdv.
v

x

w

This last estimand gives average output when W and X are independently assigned within
subpopulations indexed by V .
These allocations are just four among the class of feasible allocations. This class is comprised of all joint distributions of inputs consistent with fixed marginal distributions (within
subpopulations homogenous in V ). As noted in the introduction, if the inputs are continuously
distributed this class of joint distributions is very large. For this reason we only consider a
2

We make the connection to complementarity more explicit in Section 3.3.

[8]

subset of these joint distributions. To be specific, we concentrate on a family of the feasible
allocations, indexed by two parameters, œÑ and œÅ, that includes as special cases the negative and
positive assortative matching allocations, the independent allocation, and the status quo allocation. Let Œ≤ cm (œÑ, œÅ) denote average output under the allocation indexed by œÑ and œÅ. By changing
the two parameters we trace out a ‚Äúpath‚Äù in two directions: further from or closer to the status
quo allocation, and further from, or closer to, the perfect sorting allocations. Borrowing a
term from the literature on cupolas, we call this class of feasible allocations ‚Äúcomprehensive,‚Äù
because it contains all four focal allocations as a special case. For ease of exposition we focus
in the remainder of the paper on the case with no covariates beyond W and X, and so drop
the argument V in the production function.
For the purposes of estimation, the correlated matching allocations are redefined using a
truncated bivariate normal cupola. The truncation ensures that the denominator in the weights
of the correlated matching ARE are bounded from 0, so that we do not require trimming. The
bivariate standard normal probability density function (pdf) is
œÜ(x1 , x2; œÅ) =

1
1
‚àí
(x2 ‚àí2œÅx1 x2 +x22 )
p
e 2(1‚àíœÅ2 ) 1
,
2œÄ 1 ‚àí œÅ2

‚àí‚àû < x1 , x2 < ‚àû

with a corresponding joint cumulative distribution function (cdf) denoted by Œ¶(x1 , x2 ; œÅ). Observe that
Pr(‚àíc < x1 ‚â§ c, ‚àíc < x2 ‚â§ c) = Œ¶(c, c; œÅ) ‚àí Œ¶(c, ‚àíc; œÅ) ‚àí [Œ¶(‚àíc, c; œÅ) ‚àí Œ¶(‚àíc, ‚àíc; œÅ)],
so that the truncated standard bivariate normal pdf is given by
œÜc (x1 , x2 ; œÅ) =

œÜ(x1 , x2 ; œÅ)
,
Œ¶(c, c; œÅ) ‚àí Œ¶(c, ‚àíc; œÅ) ‚àí [Œ¶(‚àíc, c; œÅ) ‚àí Œ¶(‚àíc, ‚àíc; œÅ)]

‚àíc < x1 , x2 ‚â§ c.

Denote the truncated bivariate cdf by Œ¶c .
The truncated normal bivariate CDF gives a comprehensive cupola, because the corresponding joint cdf

‚àí1
HW,X (w, x) = Œ¶c Œ¶‚àí1
c (FW (w)), Œ¶c (FX (x)); œÅ

has marginal cdf‚Äôs equal to HW,X|V (w, ‚àû|v) = FW (w) and HW,X (‚àû, x) = FX (x), it reaches
the upper and lower FreÃÅchet bounds on the joint cdf for œÅ = 1 and œÅ = ‚àí1, respectively, and it
has independent W, X as a special case for œÅ = 0.
To define Œ≤ cm (œÅ, œÑ ), we note that joint pdf associated with HW,X (w, x) equals
‚àí1
hW,X (w, x) = œÜc Œ¶‚àí1
c (FW (w)), Œ¶c (FX (x)); œÅ



fW (w)fX (x)

.
‚àí1
œÜc Œ¶c (FW (w)) œÜc Œ¶‚àí1
c (FX (x))

Then we define Œ≤ cm (œÅ, 0) in terms of the truncated normal, as
Œ≤

cm


‚àí1
œÜc Œ¶‚àí1
c (FW (w)), Œ¶c (FX (x)); œÅ

fW (w)fX (x)dwdx.
(œÅ, 0) =
g(w, x)
‚àí1
œÜc Œ¶‚àí1
w,x
c (FW (w)) œÜc Œ¶c (FX (x))
Z

(3.9)

Average output under the correlated matching allocation is given by
Œ≤ cm (œÅ, œÑ ) = œÑ ¬∑ E[Y ] + (1 ‚àí œÑ ) ¬∑ Œ≤ cm (œÅ, 0)

(3.10)
[9]

= œÑ ¬∑ E[Yi ] + (1 ‚àí œÑ )

Z
‚àí1
œÜc Œ¶‚àí1
c (FW (w)), Œ¶c (FX (x)); œÅ

fW (w)fX (x)dwdx,
√ó
g(w, x)
‚àí1
œÜc Œ¶‚àí1
x,w
c (FW (w)) œÜc Œ¶c (FX (x))

for œÑ ‚àà [0, 1] and œÅ ‚àà (‚àí1, 1).
The case with œÑ = 1 corresponds to the status quo:
Œ≤ sq = Œ≤ cm (œÅ, 1).

The case with œÑ = œÅ = 0 corresponds to random matching allocation of inputs:
Z Z
rm
cm
Œ≤ = Œ≤ (0, 0) =
g(w, x)dFW (w)dFX (x).
x

w

The cases with (œÑ = 0, œÅ ‚Üí 1) and (œÑ = 0, œÅ ‚Üí ‚àí1) correspond respectively to the perfect
positive and negative assortative matching allocations:
Œ≤ pam = lim Œ≤ cm (œÅ, 0),
œÅ‚Üí1

and Œ≤ nam = lim Œ≤ cm (œÅ, 0).
œÅ‚Üí‚àí1

More generally, with œÑ = 0 we allocate the inputs using a normal copula in a way that allows for
arbitrary correlation between W and X indexed by the parameter œÅ. It would be conceptually
straightforward to use other copulas.

3.3

Local Measures of Complementarity

A potential disadvantage of the correlated matching reallocation family of estimands Œ≤ cm (œÅ, œÑ ),
including the focal allocations Œ≤ pam and Œ≤ nam is that the support requirements that allow for
precise estimation may be difficult to satisfy in practice. This is particularly relevant for allocations ‚Äòdistant‚Äô from the status quo. For example, if the status quo is characterized by a high
degree of correlation between the inputs, evaluating the effect of allocations with a small, or
even negative, correlation between inputs, such as random matching, or negative assortative
matching, can be difficult because such allocations rely on knowledge of the production funhction at pairs of input values (W, X) that are infrequently seen in the data. For this reason a
measure of local (close to the status quo) complementarity between W and X would be valuable. To this end we next characterize the expected effect on output associated with a ‚Äòsmall‚Äô
increase toward either positive or negative assortative matching. Such estimands may also be
informative regarding the effects of ‚Äúmodest‚Äù policies that stay close to the status quo. The
resulting estimand forms the basis of a simple test for local efficiency of the status quo allocation. We derive this local measure by considering matching on a family of transformations
of Xi and Wi , indexed by a scalar parameter Œª, where for some values of Œª the matching is
on Wi (corresponding to the status quo), and for other values of Œª the matching is on Xi or
‚àíXi , corresponding to positive and negative assortative matching respectively. We then focus
on the derivative of the expected outcomes from matching on this family of transformations,
evaluated at the value of Œª that corresponds to the status quo.
For technical reasons, and to be consistent with the subsequent formal statistical analysis
in Section 4 of the previously discussed estimands Œ≤ pam and Œ≤ nam , we assume that the support
of Xi is the interval [xl , xu ], with midpoint xm = (xu + xl )/2, and similarly that the support of
Wi is the interval [wl , wu], with midpoint wm = (wu + wl )/2. Without loss of generality we will
[10]

assume that xl = 0, xm = 1/2, xu = 1, wl = 0, wm = 1/2, and wu = 1. To focus on the key
conceptual issues we continue to ignore the presence of additional covariates Vi. First define a
smooth function d(w) that goes to zero at the boundary of the support of Wi :
d(w) = 1w>wm ¬∑ (wu ‚àí w) + 1w‚â§wm ¬∑ (w ‚àí wl ).
We implement our local reallocation as follows: for Œª ‚àà [‚àí1, 1], define the random variable UŒª
as a transformation of (X, W ):
p
UŒª = Œª ¬∑ X ¬∑ d(W )1‚àí|Œª| + ( 1 ‚àí Œª2 ) ¬∑ W.

This gives us a parametric transformation of (W, X) that moves smoothly between W = U0
and X = U1 . Now we consider reallocations based on positive assortative matching on UŒª , for
a range of values of Œª, as a smooth way of moving from the status quo (matching on W ) to
positive assortative matching (matching on X). For general Œª the average output associated
with positive assortative matching on UŒª is given by the local reallocation
‚àí1
Œ≤ lr (Œª) = E[g(FW
(FUŒª (UŒª )), X)].

(3.11)

For Œª = 0 and Œª = 1 we have UŒª = W and UŒª = X respectively, and hence Œ≤ lr(0) = Œ≤ sq and
Œ≤ lr (1) = Œ≤ pam . Perfect negative assortative matching is also nested in this framework since
Pr (‚àíX ‚â§ ‚àíx) = Pr (X ‚â• x) = 1 ‚àí FX (x) ,
and hence for Œª = ‚àí1 we have Œ≤ lr(‚àí1) = Œ≤ nam . Values of Œª close to zero induce reallocations
of W that are ‚Äòlocal‚Äô to the status quo, with Œª > 0 and Œª < 0 generating shifts toward positive
and negative assortative matching respectively.
We focus on the effect of a small reallocation as our local measure of complementarity:
Œ≤ lc =

‚àÇŒ≤ lr
(0).
‚àÇŒª

(3.12)

This local complementarity measure has two interesting alternative representations which are
given in the following theorem. Before stating this result we introduce one assumption. This
assumption is stronger than needed for this theorem, but its full force will be used later. The
required values of the parameters in this assumption, p and q, will be specified in the theorems.
Assumption 3.1 (Distribution of Data)
(i) (Y1 , W1 , X1), (Y2 , W2 , X2), . . . , , (YN , WN , XN ) are independent and identically distributed,
(ii) The support of W is W = [wl , wu], a compact subset of R,
(iii) the support of X is X = [xl , xu ], a compact subset of R,
(iv) the joint probability density function of W and X is bounded and bounded away from zero,
and q times continuously differentiable on W √ó X,
(v) g(w, x) is q times continuously differentiable with respect to w and x on W √ó X,
(vi) E[|Yi |p|Xi = x] is bounded.
The first representation is as the expected value of the conditional (on W ) covariance of X and
‚àÇg
the returns to W , gW (w, x) = ‚àÇw
(w, x), weighted by d(W ). The second representation is as a
‚àÇ 2g
(w, x). Formally:
weighted average of the cross-derivative ‚àÇw‚àÇx
[11]

Theorem 3.1 Suppose Assumption 3.1 holds with q ‚â• 2. Then, Œ≤ lc has two equivalent representations:
Œ≤ lc = E [d(W ) ¬∑ Cov ( gW (W, X) , X| W )] ,

(3.13)



‚àÇ2g
Œ≤ = E Œ¥(W, X) ¬∑
(W, X) ,
‚àÇw‚àÇx

(3.14)

and,
lc

where the weight function Œ¥(w, x) is non-negative and has the form
Œ¥(w, x) = d(w)¬∑


FX|W (x|w) ¬∑ (1 ‚àí FX|W (x|w)) 
¬∑ E [X|X > x, W = w]‚àíE [X|X ‚â§ x, W = w] .
fX|W (x|w)

The proofs for the Theorems given in the body of the text are presented in Appendix C.
Representation (3.13), as we demonstrate below, suggests a straightforward nonparametric
approach to estimating Œ≤ lc . Representation (3.14) is valuable for interpretation. Equation
(3.14) demonstrates that a test of H0 : Œ≤ lc = 0 is a test of the the null hypothesis of no
complementarity or substitutability between W and X. If Œ≤ lc > 0, then in the ‚Äòvicinity of the
status quo‚Äô W and X are complements; if Œ≤ lc < 0, they are substitutes. The precise meaning
of the ‚Äúvicinity of the status quo‚Äù is implicit in the form of the weight function Œ¥(w, x).
Deviations of Œ≤ lc from zero imply that the status quo allocation does not maximize average
outcomes. For Œ≤ lc > 0 a shift toward positive assortative matching will raise average outcomes,
while for Œ≤ lc < 0 a shift toward negative assortative matching will do so. Theorem 3.1 therefore
provides the basis of a test of the null hypothesis that the status quo allocation is locally efficient.

4

Estimation and inference with continuously-valued inputs

In this section we discuss estimation and inference. For ease of exposition we focus on the case
without additional exogenous covariates. Allowing for these would complicate the notation,
without adding much insight. The estimators are all weighted averages of (derivatives of) nonparametric estimators for the regression function. These are what Newey (1994) calls full and
partial means and derivatives. First, in Section 4.1 we describe the nonparametric estimators
for the regression functions. In order to deal with boundary issues we use develop a new nonparametric kernel estimator. Note that in Newey (1994) fixed trimming methods are used to
deal with these boundary issues. These are less attractive here because they change the nature
of the estimands. Next, in Section 3.1 we present estimators for the first pair of estimands,
Œ≤ pam and Œ≤ nam . In Section 4.3 we discuss estimation and inference for Œ≤ cm (including Œ≤ rm ),
and in Section 4.4 we discuss Œ≤ lc. Estimation of and inference for the status quo allocation Œ≤ sq
is straightforward, as this estimand is a simple expectation, estimated by a sample average.

4.1

Estimating the Production and Distribution Functions

For the two distributions functions we use the empirical distribution functions:
N
1 X
FÃÇW (w) =
1Wi ‚â§w ,
N
i=1

and

N
1 X
FÃÇX (x) =
1Xi ‚â§x .
N
i=1

[12]

For the inverse distribution functions we use the definition:
‚àí1
FÃÇW
(q) = inf 1FÃÇW (w)‚â•q ,
w‚ààW

and

‚àí1
FÃÇX
(q) = inf 1FÃÇX (x)‚â•q .
x‚ààX

The estimands we consider in this paper depend on the regression function g(w, x) (in the
case of Œ≤ pam , Œ≤ nam, and Œ≤ cm ), or its derivative in the case of Œ≤ lc . The latter also depends on
the regression function m(w), defined as
m(w) = E[X|W = w].

(4.15)

In order to estimate these objects, we need estimators for the regression functions m(w) and
g(w, x), and the derivative gW (w, x). Write the regression function as
g(w, x) = E [Y |W = w, X = x] =

h2 (w, x)
,
h1 (w, x)

where
h1 (w, x) = fW X (w, x),

and h2 (w, x) = g(w, x) ¬∑ fW X (w, x).

To simplify the following discussion, we rewrite h1 (w, x) and h2 (w, x) as
h
i
hm (w, x) = E YÃÉm |W = w, X = x ¬∑ fW X (w, x),

(4.16)

for m = 1, 2, where YÃÉ = (YÃÉ1 YÃÉ2 )0 , with YÃÉ1 = 1, YÃÉ2 = Yi .
We focus on estimators for hm (w, x), and use those to estimate g(w, x) and its derivatives.
The standard Nadaraya-Watson (NW) estimator for hm (w, x) is, for some bivariate kernel
K(¬∑, ¬∑),


N
Wi ‚àí w X i ‚àí x
1 X
hÃÇnw,m (w, x) =
YÃÉim ¬∑ K
,
.
N ¬∑ b2
b
b

(4.17)

i=1

We denote the resulting nonparametric estimator by gÃÇ(w, x). We estimate the derivative of
g(w, x) with respect to w by taking the derivative of the NW estimator of g(w, x).
Because the support of (Wi , Xi) is assumed to be bounded, we have to deal with boundary
bias of the kernel estimators. Because we also need bias reduction by using higher order
kernels we adopt the Nearest Interior Point (NIP) estimator of Imbens and Ridder (2009). This
estimator divides, for given bandwidth b, the support of (W, X) into an internal region and a
boundary region. On the internal region the uniform convergence of the standard NW kernel
estimators holds, but the estimators must be modified on the boundary region of the support.
The NIP estimator coincides with the usual NW kernel estimator on the internal set, but it is
equal to a polynomial on the boundary set. The coefficients of this polynomial are those of a
Taylor series expansion in a point of the internal set.
To obtain a compact expression for the NIP estimator we adopt the following notation. The
vector z = (w x)0 has L = 2 components. Some of the results below are stated for general L,
although we only use the case with L = 2. Let Z = W √ó X denote the (compact) support of Z.
P
QL
Let Œª denote an L vector of nonnegative integers, with |Œª| = L
l=1 Œªl , and Œª! =
l=1 Œªl !. For L
[13]

vectors of nonnegative integers Œª and ¬µ let ¬µ ‚â§ Œª be equivalent to ¬µl ‚â§ Œªl for all l = 1, . . . , L,
and define


Œª
¬µ



L

L

Y
Y
Œª!
Œªl !
=
=
=
¬µ!(Œª ‚àí ¬µ)!
¬µl !(Œªl ‚àí ¬µl )!
l=1

For L vectors Œª and z, let z Œª =
g we use g (Œª)(z):
g (Œª)(z) =

l=1

QL

Œªl
l=1 zl .



Œªl
¬µl



.

As shorthand for partial derivatives of some function

‚àÇg |Œª|
(z).
‚àÇz Œª

The definition of the internal region depends on the support of the kernel. Let K : RL 7‚Üí R
denote the kernel function. We will assume that K(u) = 0 for u ‚àà
/ U with U compact, and
K(u) bounded. For the bandwidth b define the internal set of the support Z as the subset of Z
such that all zÃÉ with a distance of up to b times the support of the kernel from z are also in Z




I
L z ‚àí zÃÉ
Zb = z ‚àà Z zÃÉ ‚àà R
‚ààU ‚äÇZ .
(4.18)
b
This is a compact subset of the interior of Z that contains all points that are sufficiently far
away from the boundary that the standard kernel density estimator at those points is not
affected
of the density at the boundary. If U = [‚àí1, 1]L and
NL by any potential discontinuity
NL
I
Z = l=1 [zll , zul ], we have Zb = l=1 [zll + b, zul ‚àí b].3 The complement of the interior region
is the boundary region:


z ‚àí zÃÉ
I
ZB
=
Z/Z
=
z
‚àà
Z
‚àÉzÃÉ
‚àà
/
Z
s.t.
‚àà
U
.
(4.19)
b
b
b
Next, we need to develop some notation for Taylor series approximations. Define for a given,
q times differentiable function g : Z 7‚Üí R, a point r ‚àà RL and an integer s ‚â§ q, the (s ‚àí 1)-th
order polynomial function t : Z 7‚Üí R based on the Taylor series, expansion of order s ‚àí 1, of
g(z) around the point r ‚àà Z:
s‚àí1 X
X
1
t(z; g, r, s) =
¬∑ g (Œª)(r) ¬∑ (z ‚àí r)Œª .
Œª!

(4.20)

j=0 |Œª|=j

Because the function g(z) is q ‚â• s times continuously differentiable on Z, the remainder term
in the Taylor series expansion is
g(z) ‚àí t(z, g, r, s) =

X 1
g (Œª)(r(s)) ¬∑ (z ‚àí r)Œª .
Œª!

|Œª|=s

with r(z) intermediate between z and r. Because Z is compact, and the the s-th order continuous, the sth order derivative must be bounded, and therefore this remainder term is bounded
3

The set [‚àí1, 1]L is the set of L vectors with components that are between -1 and 1. The set
the set of L vectors with l-th component between zll and zul .

[14]

NL

l=1 [zll , zul ]

is

by C|z ‚àí r|s . For the NIP estimator we use this Taylor series expansion around a point that depends on z and the bandwidth. Specifically, we take the expansion around rb (z), the projection
on the internal region
rb (z) = argminr‚ààZI kz ‚àí rk

(4.21)

b

With this preliminary discussion, the NIP estimator of order s of hm (z) can be defined as:
hÃÇm,nip,s (z) =

p‚àí1 X
X
1
¬∑ hÃÇ(¬µ) (rb(z))(z ‚àí rb (z))¬µ
Œª! m,nw

(4.22)

j=0 |Œª|=j

(Œª)

with hÃÇm,nw the Œª-th derivative of the kernel estimator hÃÇm,nw . For values of z in the internal
region ZIb , the NIP estimator is identical to the NW kernel estimator, hÃÇm,nip,s (z) = hÃÇm,nw (z).
It is only in the boundary region that a s ‚àí 1-th order Taylor series expansion is used to address
the poor properties of the NS estimator in that region.
Now the NIP estimator for g(w, x) is
gÃÇnip,s (w, x) =

hÃÇ2,nip,s (w, x)
hÃÇ1,nip,s (w, x)

,

(4.23)

and the NIP estimator for the first derivative of g(w, x) with respect to w is
\
‚àÇg
nip,s
(w, x) =
‚àÇw

‚àÇ
‚àÇw hÃÇ2,nip,s (w, x)

hÃÇ1,nip,s (w, x)

‚àí

‚àÇ
hÃÇ1,nip,s (w, x)
hÃÇ2,nip,s (w, x) ¬∑ ‚àÇw
.

2
hÃÇ1,nip,s (w, x)

(4.24)

Unlike the NW kernel estimator, the NIP estimator is uniformly consistent. Its properties are
discussed in more detail in Imbens and Ridder (2009). A formal statement of the relevant
properties for our discussion is given in Lemmas A.9, A.10, and A.11, and Theorems A.1, A.2,
and A.3 in Appendix A.
In the remainder of the paper we drop the subscripts from the estimator of the regression
function. Unless specifically mentioned, gÃÇ(w, x) will be used to denote gÃÇnip,s (w, x), for s equal
to the order of the kernel, with its value stated in the Lemmas and Theorems.
Next we introduce two more assumptions. Assumption 4.1 describes the properties of the
kernel function, and Assumption 4.2 gives the rate on the bandwidth. Before stating the next
assumption we need to introduce a class of restrictions on kernel functions. The restrictions
govern the rate at which the kernel, which is assumed to have compact support), goes to zero
on the boundary of its support. This property allows us to deal with some of the boundary
issues. Such properties have previously been used in, for example, Powell, Stock and Stoker
(1989).
Definition 4.1 (Derivative Order of a Kernel) A kernel function K : U 7‚Üí R is of
derivative order d, if, for all u in the boundary of the set U, and all |Œª| ‚â§ d ‚àí 1,
‚àÇŒª
K(v) = 0.
v‚Üíu ‚àÇuŒª
lim

[15]

Assumption 4.1 (Kernel)
Q
(i) K : RL 7‚Üí R, with K(u) = L
l=1 K(ul ),
(ii) K(u) = 0 for u ‚àà
/ U, with U = [‚àí1, 1]L,
(iii) K(¬∑) is r times continuously differentiable, with the r-th derivative bounded on the interior
of U,
R
R
(iv) K(¬∑) is a kernel of order s, so that U K(u)du = 1 and U uŒª K(u)du = 0 for all Œª such that
0 < |Œª| < s, for some s ‚â• 1,
(v) K is a kernel of derivative order d.
We refer a kernel satisfying Assumption 4.2 as a derivative kernel of order (s, d).
Assumption 4.2 (Bandwidth) The bandwidth bN = N ‚àíŒ¥ for some Œ¥ > 0.

4.2

Estimation and Inference for Œ≤bpam and Œ≤bnam

In this section we introduce the estimators for Œ≤ pam and Œ≤ nam and present results on the large
sample properties of the estimators. We estimate Œ≤ pam and Œ≤ nam by substituting nonparametric
estimators for the unknown functions g(w, x), FW (w), and FX (x):
N

1 X  ‚àí1
Œ≤bpam =
gÃÇ FÃÇW (FÃÇX (Xi )), Xi ,
N

(4.25)

i=1

and

N

1 X  ‚àí1
nam
b
Œ≤
=
gÃÇ FÃÇW (1 ‚àí FÃÇX (Xi )), Xi .
N

(4.26)

i=1

It is straightforward to demonstrate consistency for these estimators. The nonparametric estimators gÃÇ, FÃÇW , and FÃÇX are uniformly consistent under our assumptions, and consistency of
Œ≤ÃÇ pam follows directly from that. It is more difficult to derive the large sample distributions
for these estimators. There are four components to their asymptotic approximations. Here we
discuss the decomposition for Œ≤ÃÇ pam . A similar argument holds for Œ≤ÃÇ nam . In both cases the first
component corresponds to the estimation error in g(w, x). This component converges at a rate
slower than the regular parametric (root-N ) rate. This is because we estimate in the first stage
a nonparametric regression function with more arguments than we average over in the second
stage. As a result Œ≤ÃÇ pam (and Œ≤ÃÇ nam ) is a partial (as opposed to a full) mean in the terminology
of Newey (1994). The other three terms converge faster, at the regular root‚àíN rate. There is
one term each corresponding to the estimation error in FW (w) and FX (x) respectively, and one
‚àí1
corresponding to the difference between the average of g(FW
(FX (Xi )), Xi) and its expectation.
In describing the large sample properties we include all four of these terms, which leaves a remainder that is op (N ‚àí1/2). In principle one could ignore the three terms of order Op (N ‚àí1/2),
since they will get dominated by the term describing the uncertainty stemming from estimation of g(w, x), but including the additional terms is likely to lead to more accurate confidence
intervals. We provide evidence for this in the simulations in Section 5.
In order to describe the formal properties of the estimator Œ≤ÃÇ pam it is useful to introduce
notation for an intermediate quantity, and some additional functions. Define the average with
the true regression function g(w, x) (but still the estimated distribution functions FÃÇW and FÃÇX ),
Œ≤ÃÉ

pam

N


1 X  ‚àí1 
=
g FÃÇW FÃÇX (Xi ) , Xi ,
N

(4.27)

i=1

[16]

so that we can write Œ≤ÃÇ pam ‚àí Œ≤ pam = (Œ≤ÃÇ pam ‚àí Œ≤ÃÉ pam) + (Œ≤ÃÉ pam ‚àí Œ≤ pam). Then the first term
‚àí1/2
Œ≤ÃÉ pam ‚àí Œ≤ pam = Op (N ‚àí1/2), and the second term Œ≤ÃÇ pam ‚àí Œ≤ÃÉ pam = Op(N ‚àí1/2 bN ). Recall the
notation for the derivative of g(w, x) with respect to w,
gW (w, x) =

‚àÇg
(w, x),
‚àÇw

and define
q pam (w, x) =

‚àí1

gW (FW
(FX (x)), x)
¬∑ 1FW (w)‚â§FX (x) ‚àí FX (x) ,
‚àí1
fW (FW (FX (x)))

pam
œàW
(w) = E [q pam (w, X)] ,

r pam (x, z) =

‚àí1
gW (FW
(FX (z)), z)
¬∑ (1x‚â§z ‚àí FX (z)) ,
‚àí1
fW (FW (FX (z)))

and
pam
œàX
(x) = E [r pam(x, X)] .

Theorem 4.1 (Large Sample Properties of Œ≤ÃÇ pam )
Suppose Assumptions 2.1, 3.1, 4.1, and 4.2 hold, with q ‚â• 2s + 1, r ‚â• s + 3, p ‚â• 4, d ‚â• s ‚àí 1,
and 1/(2s) < Œ¥ < 1/8. Then

 !
   pam

1/2
‚àö
bN Œ≤ÃÇ pam ‚àí Œ≤ÃÉ pam
0
‚Ñ¶11
0
d
N¬∑
‚àí‚Üí N
,
,
pam
0
0
‚Ñ¶22
Œ≤ÃÉ pam ‚àí Œ≤ pam
where
‚Ñ¶pam
11

"

=E œÉ

2

‚àí1
FW

(FX (X)) , Xi ¬∑

¬∑fW |X
and
‚Ñ¶pam
22 = E

h



Z

u1

Z

K

u2

u1 +

fW

!
!2
fX (X)
 ¬∑ u2 , u2 du2 du1
‚àí1
FW
(FX (X))

(4.28)

#

‚àí1
FW
(FX (X)) |X ,

pam
pam
‚àí1
œàW
(W ) + œàX
(X) + g(FW
(FX (X)), X) ‚àí Œ≤ pam

2 i

.

pam
captures the uncertainty resulting from
In the expression for the large sample variance, œàX
pam
estimation of FX (x), and œàW
captures the uncertainty resulting from estimation of FW (w).
Note that the component of the variance that captures the uncertainty from estimation
of g(w, x), ‚Ñ¶pam
11 , depends on the kernel in a way that involves the distribution of the data.
Often when one estimates nonparametric functionals at parametric rates, the dependence on
the kernel vanishes asymptotically if one undersmoothes. Here the kernel shows up in the
leading term. This is also the case in the discussion of partial means in Newey (1994).
Suppose we wish to construct a 95% confidence interval for Œ≤ pam . In that case we approxipam
pam
‚àí1
mate the variance of Œ≤ÃÇ pam ‚àí Œ≤ pam by VÃÇ = ‚Ñ¶ÃÇ11 ¬∑ N ‚àí1 ¬∑ b‚àí1
, using suitable plug-in
N + ‚Ñ¶ÃÇ22 ¬∑ N

[17]

p
pam
pam
pam ‚àí 1.96 ¬∑
VÃÇ, Œ≤ÃÇ pam +
estimators
‚Ñ¶ÃÇ
and
‚Ñ¶ÃÇ
,
and
construct
the
confidence
interval
as
(
Œ≤ÃÇ
11
22
p
1.96 ¬∑ VÃÇ). Although the first term in VÃÇ will dominate the second term in large samples, in
finite samples the second term may still be important. We shall see this in the simulations in
Section 5.
Similar results hold for Œ≤ nam , with some appropriately redefined concepts. Define
Œ≤ÃÉ nam =

N


1 X  ‚àí1 
g FÃÇW 1 ‚àí FÃÇX (Xi ) , Xi ,
N

(4.29)

i=1

q nam (w, x) =

‚àí1

gW (FW
(1 ‚àí FX (x)), x)
¬∑ 1FW (w)‚â§FX (x) ‚àí FX (x) ,
‚àí1
fW (FW (1 ‚àí FX (x)))

nam
nam
œàW
(w) = E [qW
X (w, X)] ,

r nam (x, z) =
and

‚àí1
gW (FW
(1 ‚àí FX (z)), z)
¬∑ (1x‚â§z ‚àí FX (z)) ,
‚àí1
fW (FW (1 ‚àí FX (z)))

nam
nam
œàX
(x) = E [rXZ
(x, X)] .

Theorem 4.2 (Large Sample Properties of Œ≤ÃÇ nam )
Suppose Assumptions 2.1, 3.1, 4.1, and 4.2 hold, with q ‚â• 2s + 1, r ‚â• s + 3, p ‚â• 4, d ‚â• s ‚àí 1,
and 1/(2s) < Œ¥ < 1/8. Then

 !
   nam

1/2
‚àö
bN Œ≤ÃÇ nam ‚àí Œ≤ÃÉ nam
0
‚Ñ¶11
0
d
N¬∑
‚àí‚Üí N
,
,
0
0
‚Ñ¶nam
22
Œ≤ÃÉ nam ‚àí Œ≤ nam
where
‚Ñ¶nam
11

"

=E œÉ

2

‚àí1
FW


(1 ‚àí FX (X)) , X ¬∑

‚àí1
¬∑fW |X FW

and

Z

u1

Z

K

u1 +

u2

(1 ‚àí FX (X)) |X

#


fW

fX (X)
 ¬∑ u2 , u2
‚àí1
FW (1 ‚àí FX (X))

!!2

du1

,

i
h
nam
nam
nam 2
‚Ñ¶nam
) .
22 = E (œàW (W ) + œàX (X) + g(W, X) ‚àí Œ≤

4.3

Estimation and Inference for Œ≤ cm(œÅ, œÑ )

The starting point for estimation of Œ≤ cm is the representation of Œ≤ cm (œÅ, 0) in equation (3.9):

Z
‚àí1
œÜc Œ¶‚àí1
c (FW (w)), Œ¶c (FX (x)); œÅ
cm

fW (w)fX (x)dwdx.
Œ≤ (œÅ, 0) =
g(w, x)
‚àí1
œÜc Œ¶‚àí1
w,x
c (FW (w)) œÜc Œ¶c (FX (x))

[18]

Note that this expression is an integral over the product of the marginal pdf‚Äôs of W and X,
not the joint. We estimate this by replacing the integrals with sums over the two empirical
distribution functions to get analog estimator


‚àí1 (FÃÇ (W )), Œ¶‚àí1(FÃÇ (X )); œÅ
N X
N
œÜ
Œ¶
X
c
W
i
X
j
c
c
1
 
.
Œ≤bcm (œÅ, 0) = 2
gÃÇ(Wi , Xj ) 
‚àí1
N
œÜ Œ¶c (FÃÇ (W )) œÜ Œ¶‚àí1
c (FÃÇ (X ))
i=1 j=1

c

W

i

c

X

j

This estimator would be a standard second order V statistic if we had the true regression function and the true distribution functions. The dependence on the esimated regression function
complicates its analysis.
Observe that if œÅ = 0 (random matching) the ratio of densities on the right hand side is
equal to 1, so that
Œ≤ÃÇ rm =

N N
1 XX
gÃÇ(Wi, Xj ).
N2
i=1 j=1

For œÑ > 0, the Œ≤ cm (œÅ, œÑ ) estimand is a convex combination of average output under the
status quo and a correlated matching allocation. The corresponding sample analog is
Œ≤bcm (œÅ, œÑ ) = œÑ ¬∑ Œ≤bsq + (1 ‚àí œÑ ) ¬∑ Œ≤bcm (œÅ, 0),
P
where Œ≤bsq = Y = N
i=1 Yi /N , the average outcome. This estimator is linear in the nonparametric regression estimator gÃÇ and nonlinear in the empirical CDFs of X and W .
A useful and insightful representation of Œ≤ cm (œÅ, 0) is as an average of partial means (c.f.,
Newey 1994). This representation provides intuition both about the structure of the estimand
as well as its large sample properties. Fixing W at W = w, but averaging over the distribution
of X we get the partial mean:
Œ∑ (w) = EX [g(w, X) ¬∑ d(w, X)],

(4.30)

where
d(w, x) =

‚àí1
œÜc (Œ¶‚àí1
c (FW (w)), Œ¶c (FX (x)); œÅ)
.
‚àí1
œÜc (Œ¶c (FW (w)))œÜc(Œ¶‚àí1
c (FX (x)))

(4.31)

Observe that (4.30) is a weighted averaged of the production function over the distribution of
X holding the value of the input to be reallocated W fixed at W = w. The weight function
d(w, X) depends upon the truncated normal cupola. In particular, the weights give greater
emphasis to realizations of g(w, X) that are associated with values of X that will be assigned
a value of W close to w as part of the correlated matching reallocation. Thus (4.30) equals
the average post-reallocation output for those firms being assigned W = w. To give a concrete
example (4.30) is the post-reallocation expected achievement of those classrooms that will be
assigned a teacher of quality W = w.
Equation (4.30) also highlights the value of using the truncated normal copula. Doing so
ensures that the denominators of the copula ‚Äòweights‚Äô in (4.30) are bounded from zero. The
copula weights thus play the role similar to fixed trimming weights used by Newey (1994).
If we average these partial means over the marginal distribution of W we get Œ≤ cm (œÅ, 0), since
Œ≤ cm (œÅ, 0) = EW [Œ∑ (W )] ,
[19]

yielding average output under the correlated matching reallocation.
¬øFrom the above discussion it is clear that our correlated matching estimator can be viewed
as a semiparametric two-step method-of-moments estimator with a moment function of
m(Y, W, Œ≤ cm(œÅ, œÑ ), Œ∑ (W )) = œÑ Y + (1 ‚àí œÑ ) Œ∑ (W ) ‚àí Œ≤ cm (œÅ, œÑ ).
Our estimator, Œ≤bcm (œÅ, œÑ ), is the feasible GMM estimator based upon the above moment function
after replacing the partial mean (Œ∑(w) defined in (4.30)) with a consistent estimate. While the
above representation is less useful for deriving the asymptotic properties of Œ≤bcm (œÅ, œÑ ) it does
provide some insight as to why we are able to achievement parametric rates of convergence.
To state the large sample properties of the correlated matching estimator we need some
additional notation. Define:
eW (w, x) =

‚àí1
œÅœÜc (Œ¶‚àí1
c (FW (w)), Œ¶c (FX (x)); œÅ)
√ó
‚àí1 b
2
(1 ‚àí œÅ2 )œÜc (Œ¶‚àí1
c (FW (w))) œÜc (Œ¶c (FX (x)))
 ‚àí1

Œ¶c (FX (x)) ‚àí œÅŒ¶‚àí1
c (FW (w)) ,

‚àí1
œÅœÜc (Œ¶‚àí1
c (FW (w)), Œ¶c (FX (x)); œÅ)
√ó
‚àí1 b
2
(1 ‚àí œÅ2 )œÜc (Œ¶‚àí1
c (FW (w)))œÜc(Œ¶c (FX (x)))
 ‚àí1

Œ¶c (FW (w)) ‚àí œÅŒ¶‚àí1
c (FX (Xk )) ,

‚àí1
‚àí1
œÜ
Œ¶
(F
(w)),
Œ¶
(F
(x));
œÅ
c
W
X
c
c

,
œâ cm (w, x) =
‚àí1
œÜc Œ¶‚àí1
c (FW (w)) œÜc Œ¶c (FX (x))

eX (w, x) =

œà0cm (y, w, x) =

(4.32)
(4.33)

(E [g(W, x) ¬∑ œâ(W, x)] ‚àí Œ≤ cm (œÅ, 0)) + (E [g(w, X) ¬∑ œâ(w, X)] ‚àí Œ≤ cm (œÅ, 0)) ,

fW (w) ¬∑ fX (x)
(y ‚àí g(w, x))œâ(w, x),
fW X (w, x)
Z Z
cm
œàW (y, w, x) =
g(s, t)eW (s, t) (1w‚â§s ‚àí FW (s)) fW (s) fX (t) dsdt,

œàgcm (y, w, x) =

and

cm
œàX
(y, w, x) =

Z Z

g(s, t)eX (s, t) (1x‚â§t ‚àí FX (t)) fW (s) fX (t) dsdt.

(4.34)
(4.35)

(4.36)

Theorem 4.3 Suppose Assumptions 2.1, 3.1, 4.1, and 4.2 hold with q ‚â• 2s ‚àí 1, r ‚â• s + 1,
p ‚â• 3, d ‚â• s ‚àí 1, and (1/2s) < Œ¥ < 1/4, then
and

p
Œ≤bcm (œÅ, œÑ ) ‚Üí Œ≤ cm (œÅ, œÑ )

‚àö
where

and

d

N(Œ≤bcm (œÅ, œÑ ) ‚àí Œ≤ cm (œÅ, œÑ )) ‚àí‚Üí N (0, ‚Ñ¶cm),

h
i
‚Ñ¶cm = E (œÑ (Y ‚àí Œ≤ sq ) + (1 ‚àí œÑ ) œà cm(Y, W, X))2 ,
cm
cm
œà cm (y, w, x) = œà0cm(y, w, x) + œàgcm(y, w, x) + œàW
(y, w, x) + œàX
(y, w, x).

[20]

(4.37)

Note that this estimator is rootN consistent, unlike Œ≤ÃÇ pam and Œ≤ÃÇ nam .
If there was no estimation error in gÃÇ(w, x), FÃÇW (w), and FÃÇX (x), the estimator would be
root‚àíN consistent with normalized asymptotic variance equal to [œà0cm (Yi , Wi, X)i)2]. The recm
cm
maining terms in the influence function, œàW
(y, w, x), œàX
(y, w, x), and œàgcm(y, w, x), capture
the uncertainty coming from estimation of FW (w), FX (x), and g(w, x) respectively.

Estimation and Inference for Œ≤ lc

4.4

Estimation of Œ≤ lc proceeds in two steps. First we estimate g (w, x) = E[Y |W = w, X = x] (and
its derivative with respect to w) and m (w) = E[X|W = w] using kernel methods as in Section
4.1. In the second step we estimate Œ≤ lc by method-of-moments using the sample analog of the
moment condition


‚àÇ
lc
E
g (W, X) ¬∑ d(W ) ¬∑ (X ‚àí m (W )) ‚àí Œ≤ = 0.
‚àÇw
Thus,
Œ≤ÃÇ lc =
Define

1 XN ‚àÇ
gb (Wi , Xi) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m
b (Wi )).
i=1 ‚àÇw
N

œàglc (y, w, x) = ‚àí

(4.38)

‚àÇfW,X (w, x)
1
d(w) (y ‚àí g (w, x)) (x ‚àí m (w))
fW,X (w, x)
‚àÇW

‚àÇm
(w)d(w) (y ‚àí g (w, x))
‚àÇw
‚àÇd
‚àí
(w)(y ‚àí g(w, x))(x ‚àí m (w)).
‚àÇw

‚àí

and
lc
œàm
(y, w, x)




‚àÇ
=E
g (w, X) W = w ¬∑ d(w) ¬∑ (x ‚àí m(w)).
‚àÇw

As in the previous results, the œà lc are the influence functions, with œàglc(y, w, x) capturing the unlc (y, w, x) capturing the uncertainty from estimation
certainty from estimation of g(w, x), and œàm
of m(w).
The asymptotic properties of Œ≤ÃÇ lc are summarized by Theorem 4.4.
Theorem 4.4 Suppose Assumptions 2.1, 3.1, 4.1, and 4.2 hold with q ‚â• 2s + 1, r ‚â• s + 1,
p ‚â• 4, d ‚â• s ‚àí 1, and 1/(2s) < Œ¥ < 1/12. Then
p

Œ≤ÃÇ lc ‚Üí Œ≤ lc,
and
‚àö
where
lc

d

N(Œ≤ÃÇ lc ‚àí Œ≤ lc ) ‚Üí N (0, ‚Ñ¶lc),

‚Ñ¶ =E

"

‚àÇ
g (W, X) ¬∑ d(W ) ¬∑ (X ‚àí m (W )) ‚àí Œ≤ lc
‚àÇw
[21]



+ œàglc (Y, W, X) +

2 #

lc
œàm
(Y, W, X)

.

5

A Monte Carlo Study

To assess whether the asymptotic properties derived in Section 4 provide useful approximations
to finite sample distributions, we carry out a small simulation study. In the interest of brevity we
focus on Œ≤ pam and Œ≤ lc . We consider the following data generating process. The pair (Wi‚àó , Xi‚àó)
are drawn from a bivariate normal distribution with both means equal to zero, both variances
equal to one, and correlation coefficient equal to œÅ. The two covariates Wi and Xi are then
constructed as Wi = 2 ¬∑ Œ¶(Wi‚àó ) ‚àí 1 and Xi = 2 ¬∑ Œ¶(Xi‚àó ) ‚àí 1, so that both Wi and Xi have a
uniform distribution on [‚àí1, 1], with potentially some correlation between them. The outcome
is generated as
Yi = Wi + Xi + Wi ¬∑ Xi + Œµi ,

Œµi |Wi , Xi ‚àº N (0, 0.25).

Under this data generating process Œ≤ pam = 0.3333, irrespective of the value of the correlation
between the covariates, œÅ. The expected outcome under the current allocation is E[Y ] = 0 if
œÅ = 0, and E[Y ] = 0.1212 if œÅ = 0.5. We fix the weight function d(w) in the definition of the
local complementarity measure at d(w) = 1 ‚àí |w|. The value of the local reallocation parameter
is Œ≤ lc = 0.1667 if œÅ = 0 and Œ≤ lc = 0.1355 if œÅ = 0.5.
We estimate Œ≤ pam using equation (4.26), and Œ≤ lc using equation (4.38). We use a rectangular
kernel on [‚àí1, 1], and local linear regression for estimating g(w, x). The bandwidth for the
regression estimation is choosen using cross-validation, after which we divide the bandwidth
by two to ensure some undersmoothing. For density estimation we use the Silverman rule of
thumb, modified for a uniform kernel. For univariate density estimation this leads to
bN = 1.84 ¬∑ œÉ ¬∑ N ‚àí1/5 .
For estimating the bivariate density we use a bivariate uniform kernel, with the bandwidths in
each direction equal to
b0N = 1.84 ¬∑ œÉ ¬∑ N ‚àí1/6 ,
where the œÉ is estimated on the data, and so may differ in the two directions for the bivariate
kernel.
We consider four designs, based on two sample sizes, N = 200 and N = 1000, and two
dependence structures, œÅ = 0 and œÅ = 0.5. For both designs we calculate the two estimators
Œ≤ÃÇpam and Œ≤ÃÇlc, and their variances. In Table 1 we report some summary statistics from the
simulations. We report the average and median bias, the standard deviation, the average of
the standard errors, the root mean squared error, the median absolute error, and the coverage
rates for the nominal 90 and 95% confidence intervals. The estimators appear to work fairly
well. Note that the average standard error for Œ≤ÃÇ lc is large relative to its standard deviation (the
ratio is more than six). The reason is that occasionally the estimated standard error is very
large. This happens with low probability, so the median standard error is not affected, and the
coverage rate is also fine.
The estimators have a complicated structure, with the asymptotic distribution relying on a
number of approximations. We further investigate these approximations in Table 2. Define
Œ≤ÃÇgpam

N

1 X
‚àí1
gÃÇ FW
(FX (Xi )) , Xi ,
=
N

(5.39)

i=1

[22]

pam
Œ≤ÃÇW
=

N

1 X  ‚àí1
g FÃÇW (FX (Xi)) , Xi ,
N

(5.40)

i=1

pam
Œ≤ÃÇX

N


1 X  ‚àí1 
=
g FW FÃÇX (Xi ) , Xi ,
N

(5.41)

i=1

and
g

pam

N

1 X
‚àí1
g FW
(FX (Xi)) , Xi .
=
N

(5.42)

i=1

Then, as stated formally in Appendix A, Lemma A.15,
Œ≤ÃÇ pam ‚àí Œ≤ pam =

 
 



pam
pam
Œ≤ÃÇgpam ‚àí gpam + Œ≤ÃÇW
‚àí gpam + Œ≤ÃÇX
‚àí gpam +(gpam ‚àí Œ≤ pam )+op N ‚àí1/2 . (5.43)

In Panel A of Table 2, we show the mean and standard deviation of Œ≤ÃÇ pam ‚àí Œ≤ pam , Œ≤ÃÇgpam ‚àí gpam ,
pam
pam
Œ≤ÃÇW
‚àí gpam , Œ≤ÃÇX
‚àí gpam , and the remainder term,


rem = Œ≤ÃÇ pam ‚àí Œ≤ pam
‚àí


 
 


pam
pam
Œ≤ÃÇgpam ‚àí gpam + Œ≤ÃÇW
‚àí gpam + Œ≤ÃÇX
‚àí g pam + (gpam ‚àí Œ≤ pam ) .

The results in Panel A of Table 2 suggest that the remainder term is indeed small compared
to the terms that are taken into account in the asymptotic distribution. Moreover, the relative
magnitude of the Op (N ‚àí1/2) terms are supportive of the fact that we take into account these
‚àí1/2
terms, not just the leading term which is N ‚àí1/2 bN .
In the appendix we also show that


d
1/2
(5.44)
N 1/2 bN ¬∑ Œ≤ÃÇgpam ‚àí g pam ‚àí‚Üí N (0, ‚Ñ¶pam
11 ) ,
pam

where ‚Ñ¶11

and

is defined in (4.28),


 pam

d
pam
N 1/2 ¬∑ Œ≤ÃÇW
‚àí gpam ‚àí‚Üí N 0, E œàW
(W )2 ,

(5.45)



 pam

d
pam
N 1/2 ¬∑ Œ≤ÃÇX ‚àí gpam ‚àí‚Üí N 0, E œàX (X)2 ,

(5.46)


h
2 i
d
‚àí1
N 1/2 ¬∑ (gpam ‚àí Œ≤ pam ) ‚àí‚Üí N 0, E g(FW
(FX (Xi), Xi)) ‚àí Œ≤ pam
.

(5.47)

To assess the normal approximations we calculate the t-statistics based on these distributions
(the point estimates divided by estimates of the standard deviations), and report in Panel B of
Table 2 summary statistics for these random variables, which should have approximate normal
distributions. The summary statistics we report are averages, standard deviations, and tail
frequencies. We find that the actual means, standard deviations, and tail frequencies are close
to the nominal ones from the normal distribution.
[23]

6

Conclusions

In this paper we introduce a new class of estimands involving reallocation of inputs, and develop
statistical methods for analyzing them. We consider a class of problems where a fixed set of
inputs is reallocated to a fixed set of units. Whereas a large part of the literature in econometrics
has focused on estimating the causal effects of changing inputs for all units, or for a subset of
units, here we focus on reallocation rules that take into account resource constraints, by keeping
the distribution of the inputs fixed. The effects we focus on depend critically on the degree
of complementarity between inputs. We therefore follow a flexible nonparametric approach
where the nature of the complementarity is not restricted to a parametric form. We propose
estimators for the effects of various reallocation rules, and derive the asymptotic properties of
these estimators.

[24]

References
Angrist, J. D. and A. B. Krueger. (1999). ‚ÄúEmpirical Strategies in Labor Economics,‚Äù Handbook
of Labor Economics 3A: 1277 - 1366 (O. Ashenfelter and D. Card, Eds). New York: Elsevier
Science.
Athey, S., and G. Imbens (2006), ‚ÄúIdentification and Inference in Nonlinear Difference-In-Differences
Models,‚ÄùEconometrica, 74(2): 431-497.
Athey, S., and S. Stern. (1998). ‚ÄúAn Empirical Framework for Testing Theories About Complementarity in Organizational Design‚Äù, NBER Working Paper No. 6600.
Becker, Gary S. and Kevin M. Murphy. (2000). Social Economics: Market Behavior in a Social
Environment. Cambridge, MA: Harvard University Press.
Bhattacharya, D., (2008), ‚ÄúInferring Optimal Peer Assignment from Experimental Data,‚Äù
forthcoming Journal of the American Statistical Association.
Brock, W. and S. Durlauf. (2001). ‚ÄúInteractions-based Models,‚Äù Handbook of Econometrics 5 :
3297 - 3380 (J. Heckman & E. Leamer, Eds.). Amsterdam: North-Holland.
Card, D., and A. Krueger. (1992). ‚ÄúDoes School Quality Matter? Returns to Education and
the Characteristics of Public Schools in the United States, ‚ÄùJournal of Political Economy 100(1):
1-40.
Dehejia, R. (2005). ‚ÄúProgram evaluation as a decision problem,‚Äù Journal of Econometrics 125 (1-2):
141 - 173.
Flores, C. (2005). ‚ÄúEstimation of Dose-Response Functions and Optimal Doses with a Continuous
Treatment,‚Äù Mimeo.
Graham, B. (2008). ‚ÄúIdentifying Social Interactions Through Conditional Variance Restrictions,‚Äù
Econometrica 76(3): 643-660.
Graham, B., G. Imbens, and G. Ridder,(2006a). ‚ÄúComplementarity and the Optimal Allocation
of Inputs,‚Äù Mimeo.
Graham, B., G. Imbens, and G. Ridder,(2006b). ‚ÄúMeasuring the Average Outcome and Inequality
Effects of Segregation in the Presence of Social Spillovers,‚Äù Mimeo.
Glaeser, E., and J. Scheinkman, (2003), ‚ÄúNonmarket Interactions,‚ÄùAdvances in Economics and
Econometrics: Theory and Applications, Eighth World Congress 1: 339 - 369, (M. Dewatripont el
al, Eds.). Cambridge: Cambridge University Press.
HaÃàrdle, W. and T.M. Stoker. (1989). ‚ÄúInvestigating smooth multiple regression by the method
of average derivatives,‚Äù Journal of the American Statistical Association 84 (408): 986 - 995.
Heckman, J., R. Lalonde, and J. Smith. (2000). ‚ÄúThe Economics and Econometrics of Active
Labor Markets Programs,‚Äù Handbook of Labor Economics 3A: 1865-2097 (O. Ashenfelter and D.
Card, Eds). New York: Elsevier Science.
Heckman, J., J. Smith, and N. Clements. (1997). ‚ÄúMaking The Most Out Of Programme
Evaluations and Social Experiments: Accounting For Heterogeneity in Programme Impacts,‚Äù
Review of Economic Studies 64 (4): 487 - 535.
Hirano, K. and J. Porter. (2005). ‚ÄúAsymptotics for statistical treatment rules,‚Äù Mimeo.
Imbens, G. (2000). ‚ÄúThe Role of the Propensity Score in Estimating Dose-Response Functions,‚Äù
Biometrika 87 (3): 706 - 710.
Imbens, G. (2004). ‚ÄúNonparametric Estimation of Average Treatment Effects under Exogeneity: A
Survey,‚Äù Review of Economics and Statistics 86 (1): 4 - 30.
Imbens, G., and G. Ridder (2009) ‚ÄúEstimation and Inference for Generalized Full and Partial Means
and Derivatives,‚Äùunpublished manuscript, dept of economics, Harvard University,
http://www.economics.harvard.edu/faculty/imbens/papers imbens.
Imbens, G., and J. Wooldridge (2009). ‚ÄúNew Developments in the Econometrics of Program
Evaluation,‚Äù Journal of Economic Literature 47(1): 5 - 86.

[25]

Lehman, E. (1998), Elements of Large-Sample Theory, Springer-Verlag, New York.
Linton, O., and J. Nielsen. (1995), ‚ÄúA Kernel Method of Estimating Structured Nonparametric
Regression Based on Marginal Integration,‚Äù Biometrika 82 (1): 93 - 100.
Manski, C. (1990). ‚ÄúNonparametric Bounds on Treatment Effects,‚Äù American Economic Review
80(2): 319 - 323.
Manski, C. (1993). ‚ÄúIdentification of Endogenous Social Effects: The Reflection Problem,‚Äù Review of
Economic Studies 60(3): 531 - 542.
Manski, C. (2003). Partial Identification of Probability Distributions. New York: Springer-Verlag.
Manski, C. (2004). ‚ÄúStatistical Treatment Rules for Heterogenous Populations,‚Äù Econometrica 72(4):
1221 - 1246.
Moffitt, R. (2001), ‚ÄúPolicy interventions, low-level equilibria and social interactions,‚ÄùSocial Dynamics: 45 - 82 (S. Durlauf and P. Young, Eds.). Cambridge, MA: MIT Press.
Newey, W. (1994). ‚ÄúKernel Estimation of Partial Means and a General Variance Estimator,‚Äù Econometric Theory 10(2): 233 - 253.
Powell, J., J. Stock and T. Stoker. (1989). ‚ÄúSemiparametric Estimation of Index Coefficients,‚Äù
Econometrica 57(6): 1403 - 1430.
Rosenbaum, P., and D. Rubin. (1983). ‚ÄúThe Central Role of the Propensity Score in Observational
Studies for Causal Effects,‚Äù Biometrika 70(1): 41 - 55.
Legros, P. and A. Newman. (2004). ‚ÄúBeauty is a beast, frog is a prince: assortative matching with
nontransferabilities,‚Äù Mimeo.
Sacerdote, B. (2001). ‚ÄúPeer effects with random assignment: results for Dartmouth roommates,‚Äù
Quarterly Journal of Economics 116(2): 681 - 704.

[26]

Table 1: Simulation Results for Œ≤ÃÇ pam and Œ≤ÃÇ lc , 10,000 simulations

mean bias
median bias
s.d
ave s.e.
median s.e.
r.m.s.e.
m.a.e.
cov rate 90% c.i.
cov rate 95% c.i.

N = 200
œÅ = 0.0
œÅ = 0.5
pam
lc
pam
Œ≤ÃÇ
Œ≤ÃÇ
Œ≤ÃÇ
Œ≤ÃÇ lc
-0.009 -0.018 -0.002 -0.016
-0.010 -0.020 -0.003 -0.017
0.093 0.039 0.088 0.043
0.085 0.256 0.088 0.578
0.085 0.051 0.087 0.064
0.093 0.043 0.088 0.046
0.061 0.027 0.060 0.028
0.871 0.938 0.897 0.959
0.929 0.968 0.947 0.980

[27]

N = 1000
œÅ = 0.0
œÅ = 0.5
pam
lc
pam
Œ≤ÃÇ
Œ≤ÃÇ
Œ≤ÃÇ
Œ≤ÃÇ lc
-0.003 -0.011 -0.000 -0.010
-0.003 -0.011 -0.001 -0.010
0.040 0.013 0.039 0.013
0.041 0.418 0.044 0.306
0.040 0.020 0.044 0.039
0.040 0.017 0.039 0.016
0.028 0.013 0.027 0.012
0.905 0.931 0.935 0.991
0.953 0.965 0.971 0.997

Table 2: Simulation Results: Assessing the Adequacy of the Asymptotic Approximations for Œ≤ÃÇ pam (N = 1000, œÅ = 0.0)

Panel A
mean
s.d.

Panel B
mean
s.d.
pr(|T | ‚â• 1.645)
pr(|T | ‚â• 1.96)
pr(T ‚â• 1.645)
pr(T ‚â§ ‚àí1.645)
pr(T ‚â• 1.96)
pr(T ‚â§ ‚àí1.96)

Œ≤ÃÇ pam
‚àíŒ≤ pam

Œ≤ÃÇgpam
‚àígpam

pam
Œ≤ÃÇW
‚àíg pam

pam
Œ≤ÃÇX
‚àíg pam

g pam
‚àíŒ≤ pam

remainder

-0.003
0.040

-0.002
0.031

-0.000
0.019

0.001
0.019

0.000
0.038

-0.001
0.004

tÃÇpam

tÃÇg

tÃÇW

tÃÇX

tÃÇg

nominal

-0.074
0.989
0.095
0.047
0.039
0.056
0.018
0.029

-0.078
1.018
0.105
0.055
0.043
0.062
0.021
0.034

-0.015
1.005
0.102
0.053
0.052
0.050
0.026
0.027

0.049
1.018
0.111
0.058
0.062
0.049
0.033
0.025

0.002
1.008
0.107
0.052
0.053
0.054
0.027
0.025

0.000
1.000
0.100
0.050
0.050
0.050
0.025
0.025

[28]

Appendix A: Additional Lemmas and Theorems
In this appendix we state a number of additional results that will be used in the proofs of the four Theorems
3.1-4.4. Specifically, Theorem 3.1 uses Lemmas A.1 and A.2. Theorems 4.1 and 4.2 use Lemmas A.3-A.8, A.14,
A.15, Theorem A.1, Lemmas A.16-A.18, A.9-A.11. Theorem 4.3 uses Lemmas A.14, Theorem A.1, Lemmas
A.9-A.11, Theorem A.3, and Lemmas A.24-A.28. Theorem 4.4 uses Lemma A.13, Theorem A.1, and Lemmas
A.19-A.11.
Definition 6.1 (Sobolev Norm) The norm that we use for functions g : Z ‚äÇ RL ‚Üí R that are at least j times
continuously differentiable is the Sobolev norm
Àõ |Œª|
Àõ
Àõ ‚àÇg
Àõ
Àõ.
|g|j = sup ÀõÀõ
(z)
Àõ
Œª
z‚ààZ,|Œª|‚â§j ‚àÇz

Lemma A.1 Let f : X 7‚Üí R, with X = [xl, xu ] a compact subset of R, be a twice continuously differentiable
function, and let g : R 7‚Üí R satisfy a Lipschitz condition, |g(x + y) ‚àí g(x)| ‚â§ c ¬∑ |y|. Then
Àõ
Àõ 2
Àõ
‚Äû
¬´Àõ
Àõ
Àõ
Àõ
Àõ
Àõf (g(Œª)) ‚àí f (g(0)) + ‚àÇ f (g(0)) ¬∑ (g(Œª) ‚àí g(0)) Àõ ‚â§ 1 ¬∑ sup Àõ ‚àÇ f (x)Àõ ¬∑ c2 ¬∑ Œª2 .
Àõ
Àõ
Àõ
‚àÇx
2 x‚ààX Àõ ‚àÇx2

Lemma A.2 Let X be a real-valued random variable with support X = [xl, xu ], with density fX (x) > 0 for all
x ‚àà X, and let h : X 7‚Üí R be a continous function. Suppose that E[|h(X) ¬∑ X] is finite. Then
‚Äì
¬ª
‚àÇ
Cov(h(X), X) = E
h(X) ¬∑ Œ≥(X) ,
‚àÇx
where
Œ≥(x) =

FX (x) ¬∑ (1 ‚àí FX (x)
¬∑ (E [X|X > x] ‚àí E [X|X ‚â§ x]) ,
fX (x)

and FX (x) is the cumulative distribution function of X.
For completeness we state a couple of results from Athey and Imbens (2006, AI from hereon).
Lemma A.3 (Lemma A.2 in AI) Suppose Y is a real-valued, continuously distributed random variable with
compact support Y = [yl, yu ], with the probability density function fY (y) continuous, bounded, and bounded away
from zero, on Y. Then, for any Œ¥ < 1/2:
p

sup N Œ¥ ¬∑ |FÃÇY (y) ‚àí FY (y)| ‚Üí 0.
y‚ààY

Lemma A.4 (Lemma A.3 in AI) Suppose Y is a real-valued, continuously distributed random variable with
compact support Y = [yl, yu ], with the probability density function fY (y) continuous, bounded, and bounded away
from zero, on Y. Then, for any Œ¥ < 1/2:
p

sup N Œ¥ ¬∑ |FÃÇY‚àí1 (q) ‚àí FY‚àí1 (q)| ‚Üí 0.

q‚àà[0,1]

Lemma A.5 (Lemma A.5 in AI) Suppose Y is a real-valued random variable with compact support Y = [yl , yu ],
and suppose that the cumulative distribution function FY (y) is twice continuously differentiable on Y, with its
Y
first derivative fY (y) = ‚àÇF
(y) bounded away from zero on Y. Then, for 0 < Œ∑ < 3/4 and Œ¥ > max(2Œ∑ ‚àí 1, Œ∑/2),
‚àÇy
Àõ
Àõ
Àõ
Àõ p
sup
N Œ∑ ¬∑ ÀõFÃÇY (y + x) ‚àí FÃÇY (y) ‚àí (FY (y + x) ‚àí FY (y))Àõ ‚àí‚Üí 0.
y‚ààY,x‚â§N ‚àíŒ¥ ,x+y‚ààY

[29]

Lemma A.6 (Lemma A.6 in AI) Suppose Y is a real-valued random variable with compact support Y = [yl , yu ],
and suppose that the cumulative distribution function FY (y) is twice continuously differentiable on Y, with its
Y
first derivative fY (y) = ‚àÇF
‚àÇy (y) bounded away from zero on Y. Then, for all 0 < Œ∑ < 5/7,
Àõ
‚Äú
‚ÄùÀõÀõ
Àõ
1
p
‚àí1
Àõ‚Üí
sup N Œ∑ ¬∑ ÀõÀõFÃÇY‚àí1 (q) ‚àí FY‚àí1 (q) +
FÃÇ
(F
(q))
‚àí
q
0.
Y
Y
Àõ
fY (FY‚àí1 (q))
q‚àà[0,1]
Lemma A.7 Suppose X and Y are real-valued, continuously distributed, random variables with compact support
Y = [yl , yu ] and X = [xl, xu ], with the probability density functions fY (y) and fX (x) continuous, bounded, and
bounded away from zero, on Y and X. Then, for any Œ¥ < 1/2:
Àõ
Àõ
‚Äú
‚Äù
Àõ
Àõ p
sup N Œ¥ ¬∑ ÀõFÃÇY‚àí1 FÃÇX (x) ‚àí FY‚àí1 (FX (x))Àõ ‚Üí 0.
x‚ààX

Lemma A.8 Suppose Y is a real-valued random variable with compact support Y = [yl , yu ], and the cumulative
Y
distribution function FY (y) is twice continuously differentiable on Y, with its first derivative fY (y) = ‚àÇF
(y)
‚àÇy
bounded away from zero on Y. Then, for 0 < Œ∑ < 3/4 and Œ¥ > max(2Œ∑ ‚àí 1, Œ∑/2),
Àõ
Àõ
Àõ
Àõ p
sup
N Œ∑ ¬∑ ÀõFÃÇY (y + x) ‚àí FÃÇY (y) ‚àí fY (y) ¬∑ xÀõ ‚àí‚Üí 0.
y‚ààY,x‚â§N ‚àíŒ¥ ,x+y‚ààY

The next three lemmas are given without proof. Proofs can be found in IR. The first gives a bound on the bias
of the NIP estimator.
Lemma A.9 (Bias)
If for m = 1, 2 Assumptions 3.1-4.1 hold, and q ‚â• 2s ‚àí 1 and r ‚â• s ‚àí 1, then
Àõ h
Àõ
i
Àõ
Àõ
sup ÀõE hÃÇm,nip,s (z) ‚àí hm (z)Àõ = O (bs ) .
z‚ààZ

Note that by matching the order of the kernel and the degree of the polynomial in the NIP estimator we obtain
the same reduction in the bias on the full support as on the internal region, i.e. the NIP estimator has a bias
that is of the same order as that of the NW estimator on the internal region. The variance is bounded in the
following lemma. We only use the following two results for the case with L = 2, but for convenience we give the
general results.
Lemma A.10 (Variance)
If Assumptions 3.1-4.1 hold and q ‚â• s ‚àí 1, r ‚â• s ‚àí 1 + L, then
‚Äû
¬´1/2 !
Àõ
h
iÀõ
log N
Àõ
Àõ
.
sup ÀõhÃÇm,nip,s (z) ‚àí E hÃÇm,nip,s (z) Àõ = Op
N bL
z‚ààZ
N

This is the same bound as for the NW estimator on the internal set.
The two lemmas imply a uniform rate for the NIP estimator
Lemma A.11 (Uniform Convergence)
If Assumptions 3.1-4.1 hold and q ‚â• 2s ‚àí 1, r ‚â• s ‚àí 1 + L, then
!
‚Äû
¬´1/2
Àõ
Àõ
log N
Àõ
Àõ
s
sup ÀõhÃÇm,nip,s (z) ‚àí hm (z)Àõ = Op
+ bN .
N ¬∑ bL
z‚ààZ
N

Lemma A.12 If hÃÇ(z) is a nonparametric estimator of h(z) then
‚Äû
¬´
inf |hÃÇ(z)| = inf |h(z)| + Op sup |hÃÇ(z) ‚àí h(z)|
z‚ààZ

z‚ààZ

z‚ààZ

[30]

Therefore if supz‚ààZ |hÃÇ(z) ‚àí h(z)| = op(1) and inf z‚ààZ |h(z)| > 0, then inf z‚ààZ |hÃÇ(z)| converges in probability to a
positive number. This lemma is useful if hÃÇ(z) appears in the denominator. In this paper z = (w, x) or z = w.
Lemma A.13 Suppose Assumptions 3.1-4.2 hold. Moreover, suppose that in these assumptions q ‚â• 2s ‚àí 1,
r ‚â• s. Then,
!
‚Äû
¬´1/2
ln(N )
s
sup |mÃÇ(w) ‚àí m(w)| = Op
+ bN .
N ¬∑ bN
w‚ààW
Lemma A.14 Suppose Assumptions 3.1-4.2 hold. Moreover, suppose that q ‚â• 2s + 1 and r ‚â• s + 3, Then, (i)
!
‚Äû
¬´1/2
ln(N )
s
sup |gÃÇ(w, x) ‚àí g(w, x)| = Op
+ bN ,
N ¬∑ b2N
w‚ààW,x‚ààX
(ii)
Àõ
Àõ
Àõ ‚àÇgÃÇ
Àõ
‚àÇg
Àõ
sup Àõ
(w, x) ‚àí
(w, x)ÀõÀõ = Op
‚àÇw
w‚ààW,x‚ààX ‚àÇw

‚Äû

ln(N )
N ¬∑ b4N

¬´1/2

+

!

,

bsN

!

bsN

and iii),

Àõ 2
Àõ
Àõ ‚àÇ gÃÇ
Àõ
‚àÇ2 g
Àõ = Op
sup ÀõÀõ 2 (w, x) ‚àí
(w,
x)
Àõ
2
‚àÇw
‚àÇw
w‚ààW,x‚ààX

‚Äû

ln(N )
N ¬∑ b6N

¬´1/2

+

.

The next lemma shows that we can separate out the uncertainty in Œ≤ÃÇ pam into five components: the uncertainty
‚àí1
from estimating g(¬∑), the uncertainty from estimating FÃÇW
(¬∑), the uncertainty from estimating FÃÇX
‚Äú (¬∑), and
‚Äù the

‚àí1
uncertainty from averaging g(FW
(FX (Xi)), Xi ) over the sample, and a remainder term that is op N ‚àí1/2 . As
defined in section 5

Œ≤ÃÇgpam =

pam
Œ≤ÃÇW
=

pam
Œ≤ÃÇX
=

and
g pam =

N
¬¥
1 X ` ‚àí1
gÃÇ FW (FX (Xi )) , Xi ,
N i=1
N
‚Äù
1 X ‚Äú ‚àí1
g FÃÇW (FX (Xi )) , Xi ,
N i=1

N
‚Äù
‚Äù
1 X ‚Äú ‚àí1 ‚Äú
g FW FÃÇX (Xi ) , Xi ,
N i=1
N
¬¥
1 X ` ‚àí1
g FW (FX (Xi)) , Xi .
N i=1

Lemma A.15 Suppose Assumptions 3.1, 4.1, and 4.2 hold with q ‚â• 2s + 1, r ‚â• s + 3, and 0 ‚â§ Œ¥ < 1/6. Then
Œ≤ÃÇ pam ‚àí Œ≤ pam =
‚Äú
‚Äù ‚Äú
‚Äù ‚Äú
‚Äù
‚Äú
‚Äù
pam
pam
Œ≤ÃÇgpam ‚àí gpam + Œ≤ÃÇW
‚àí gpam + Œ≤ÃÇX
‚àí gpam + (g pam ‚àí Œ≤ pam ) + op N ‚àí1/2 .

(A.1)

The next two results are special cases of theorems in Imbens and Ridder (2009). The first one refers to the full
mean case, and focuses on the case where we take full means of regression functions and their first derivatives.
The second result focuses on partial means of regression functions. The results in Imbens and Ridder (2009)
allow for more general dependence on higher order derivatives, even in the partial mean case. Here we also
restrict the analysis to the case where the regressors are the pair (Wi , Xi ). We also state the conditions that IR
invoke.
Let Zi = (Wi , Xi ), with Xi ‚àà X ‚äÇ RLX , Wi ‚àà W ‚äÇ RLW , Zi ‚àà W √ó X ‚äÇ RLZ , with LZ = LX + LW . As before
h(z) = (h1 (z), h2 (z))0, with h1 (z) = fZ (z), and h2 (z) = E[Y |Z = z] ¬∑ fZ (z). Let n : RK 7‚Üí R, t : X 7‚Üí W, and

[31]

œâ : X 7‚Üí R, and define YÃÉ = (YÃÉi1 YÃÉi2 )0 , with YÃÉi1 = 1 and YÃÉi2 = Yi . We are interested in full means (possibly
depending on derivatives) of the regression function,
h
‚Äú
‚Äùi
Œ∏fm = E œâ(Z)n h[Œª] (Z) ,
(A.2)
or partial means,

Œ∏pm = E [œâ(X)n (h (X, t(X)))] .

(A.3)

Note that in the full mean case œâ : Z 7‚Üí R, and in the partial mean case œâ : X 7‚Üí R: the weight function
depends only on the covariates that are being averaged over. In the full mean example h[Œª] denotes the vector
with elements including all derivatives h(¬µ) for ¬µ ‚â§ Œª. The estimators we focus on are
Œ∏ÃÇfm =

N
‚Äú
‚Äù
1 X
[Œª]
œâ (Zi ) n hÃÇnip,s (Zi ) ,
N i=1

and Œ∏ÃÇpm =

N
‚Äú
‚Äù
1 X
œâ(Xi )n hÃÇnip,s (t(Xi ), Xi ) .
N i=1

It will also be useful to define the averages over the true regression functions and their derivatives,
Œ∏

fm

=

N
‚Äú
‚Äù
1 X
œâ(Zi )n h[Œª] (Zi ) ,
N i=1

and Œ∏

pm

=

N
1 X
œâ(Xi )n (h(t(Xi ), Xi )) .
N i=1

Assumption A.1 (Distribution)
(i) (Y1 , Z1 ), (Y2 , Z2 ), . . . , are independent
N and identically distributed,
(ii) the support of Z is Z ‚äÇ RL , Z = L
m=1 [zml , zmu ], zll < zul for all l = 1, . . . , L.
(iii) supz‚ààZ E[|Y |p |Z = z] < ‚àû.
(iv) g(z) = E[Y |Z = z] is q times continuously differentiable on the interior of Z with the q-th derivative bounded,
(v) fZ (z) is bounded and bounded away from zero on Z, is q times continuously differentiable on the interior of
Z with the q-th derivative bounded.
Assumption A.2 (Kernel) Q
(i) K : RL ‚Üí R, with K(u) = L
l=1 K(ul ),
(ii) K(u) = 0 for u ‚àà
/ U, with U = [‚àí1, 1]L , and U1 = [‚àí1, 1]LW , and U2 = [‚àí1, 1]LX ,
(iii) K is r times continuously differentiable,
with the r-thRderivative bounded on the interior of U,
R
(iv) K is a kernel of order s, so that U K(u)du = 1 and U uŒª K(u)du = 0 for all Œª such that 0 < |Œª| < s, for
some s ‚â• 1,
(v) K is a kernel of derivative order d.
Assumption A.3 The bandwidth bN = N ‚àíŒ¥ for some Œ¥ > 0.
Assumption A.4 (Smoothness of n and œâ)
(i) The function n is t times continuously differentiable with its t-th derivative bounded, and
¬µ
(ii) the function œâ is t times differentiable on X with bounded t-th derivative, and ‚àÇ‚àÇz ¬µœâ (z) is zero on the boundary
of Z.
Assumption A.5 (Smoothness of t)
The function t : X 7‚Üí W is twice continuously differentiable on X with its first derivative positive, bounded, and
bounded away from zero.
Theorem A.1 (Generalized Full Mean and Average Derivative, [Theorem 4.2, Imbens and Ridder,
2009])
If Assumptions A.1, A.2, A.3, and A.4 hold with q ‚â• |Œª| + 2s ‚àí 1, r ‚â• |Œª| + s ‚àí 1 + L, t ‚â• |Œª| + s, p ‚â• 3,
d ‚â• max{Œª1 , . . . , ŒªL } + s ‚àí 1, all ¬µ ‚â§ Œª, 0 ‚â§ |¬µ| ‚â§ |Œª| ‚àí 1, and
(
)
2 ‚àí p4
1
1
< Œ¥ < min
,
2s
2L + 4 max{1, |Œª|} 2L + 4|Œª|
then Œ∏ÃÇfm is asymptotically linear with
‚àö

N
h
‚Äú
‚Äùi‚Äù
1 X‚Äú
N (Œ∏ÃÇfm ‚àí Œ∏fm ) = ‚àö
œâ(Zi )n(h[Œª] (Zi )) ‚àí E œâ(Zi )n h[Œª] (Zi )
N i=1

[32]

with

0
1
N
2 ‚Äú
‚Äù
X
1 X @X
(Œ∫)
A + op(1).
+‚àö
(‚àí1)|Œ∫|
Œ±(Œ∫)
Œ∫m (Xi )YÃÉim ‚àí E[Œ±Œ∫m(X)YÃÉm ]
N i=1 Œ∫‚â§Œª
m=1
‚àÇn

(Œ∫)

Œ±Œ∫1 (z) = fX (z)œâ(z)

(Œ∫)

‚àÇh1 (z)

(Œ∫)

(h[Œª] (z)),

and Œ±Œ∫2 (z) = fX (z)œâ(z)

‚àÇn
(Œ∫)

‚àÇh2 (z)

(h[Œª] (z)),

and YÃÉ = (YÃÉi1 YÃÉi2 )0 , with YÃÉi1 = 1 and YÃÉi2 = Yi .
The second theorem from IR gives the asymptotic properties of the GPM estimators
Theorem A.2 (Generalized Partial Mean, [Theorem 4.3, Imbens and Ridder, 2009])
If Assumptions A.1, A.2, A.3, A.4, and A.5 hold with q ‚â• 2s ‚àí 1, r ‚â• s ‚àí 1 + L, t ‚â• s, p ‚â• 4, d ‚â• s ‚àí 1, and
(
)
2 ‚àí p4 1
1
< Œ¥ < min
,
,
2s
2L + 4 2L
then Œ∏ÃÇpm is asymptotically linear with
‚Äú pm
‚Äù
‚àö
‚àö
N (Œ∏ÃÇpm ‚àí Œ∏pm) = N ¬∑ Œ∏ ‚àí Œ∏pm
+

with

¬´
Wi ‚àí t(Xi)
‚àÇ
+
t(X
)
¬∑
u
,
u
i
2
2 du2
bN
‚àÇx
bN LW N i=1 m=1
U2
¬ª
‚Äû
¬´
‚Äì¬´
Z
W ‚àí t(X)
‚àÇ
‚àíE Œ±m (X)0YÃÉm
K
+
t(X) ¬∑ u2 , u2 du2
+ op (1) ,
bN
‚àÇx
U2
1

‚àö

¬∑

N X
2 ‚Äû
X

Œ±1 (x) = fZ (t(x), x)œâ(x)

Œ±m(Xi )0 YÃÉim

Z

K

‚àÇn
(h(t(x), x)),
‚àÇh1

‚Äû

and

Œ±2 (x) = fZ (t(x), x)œâ(x)

‚àÇn
(h(t(x), x)).
‚àÇh2

Moreover,
‚àö

‚àö

‚Äú pm
‚Äù
N ¬∑ Œ∏ ‚àí Œ∏pm
L

N bNW

/2

(Œ∏ÃÇpm ‚àí Œ∏

pm

)

!

d

‚àí‚Üí N

‚Äû‚Äû

0
0

¬´ ‚Äû
V1
,
0

0
V2

‚ÄûZ

K

¬´¬´

,

with

and

ÀÜ
Àú
V1 = E (œâ(X)n(h(t(X), X)) ‚àí Œ∏pm )2 ,
V2 =

Z
2
2
X
X

m=1 m0 =1

X

¬µmm0 (x, t(x))Œ±m(x)Œ±m0 (x)

Z

U2

U1

‚Äû
¬´
¬´2
‚àÇt
u1 ,
(x)u1 + u2 du1 du2 fX (x, t(x))dx1 ,
‚àÇx

with ¬µmm0 (x) = E[YÃÉim YÃÉim0 |X = x] for m, m0 = 1, 2.
Lemma A.16 Suppose Assumptions 3.1, 4.1, and 4.2 hold, with q ‚â• 2s ‚àí 1, r ‚â• s + 1, p ‚â• 4, d ‚â• s ‚àí 1, and
1/(2s) < Œ¥ < 1/8. Then
‚Äù
‚àö ‚Äú pam
N Œ≤ÃÇg ‚àí gpam

!
Z
N
‚àí1
X
Wi ‚àí FW
(FX (Xi ))
1
fX (Xi )
` ‚àí1
¬¥ ¬∑ u2 , u2 du2
= ‚àö
(Yi ‚àí g(Wi , Xi )) ¬∑
K
+
bN
fW F W
(FX (Xi ))
N bN i=1
u2
(
!
Z
N
‚àí1
X
`
¬¥
Wi ‚àí FW
(FX (Xi ))
1
fX (Xi)
‚àí1
` ‚àí1
¬¥ ¬∑ u2 , u2 du2
+‚àö
g(Wi , Xi ) ‚àí g(FW
(FX (Xi )), Xi ) ¬∑
K
+
bN
fW FW (FX (Xi ))
N bN i=1
u2
"
!
#)
Z
‚àí1
`
¬¥
Wi ‚àí FW
(FX (X))
fX (X)
‚àí1
` ‚àí1
¬¥ ¬∑ u2 , u2 du2
‚àí E g(W, X) ‚àí g(FW (FX (X)), X) ¬∑
K
+
bN
fW F W
(FX (X))
u2

[33]

+op (1) .
and
‚àö

N

1/2

N bN

‚Äú

‚Äù
d
Œ≤ÃÇgpam ‚àí gpam ‚àí‚Üí

"
Z
` ‚àí1
¬¥
0, E œÉ2 FW
(FX (X)) , X ¬∑

u1

Z

u2

K

u1 +

fW

`

fX (X)
¬¥ ¬∑ u2 , u2
‚àí1
FW
(FX (X))

!

du2

!2

#!
` ‚àí1
¬¥
du1 ¬∑ fW |X FW
(FX (X)) |X
.

Lemma A.17 Suppose Assumptions 3.1, 4.1, and 4.2 hold with q ‚â• 2. Then
pam
Œ≤ÃÇW
‚àí gpam =

N
‚Äú
‚Äù
1 X pam
œàW (Wi ) + op N ‚àí1/2 .
N i=1

Lemma A.18 Suppose Assumptions 3.1, 4.1, and 4.2 hold, with q ‚â• 2. Then
pam
Œ≤ÃÇX
‚àí gpam =

N
‚Äú
‚Äù
1 X pam
œàX (Xi ) + op N ‚àí1/2 .
N i=1

Define
Œ≤ÃÇglc =

lc
Œ≤ÃÇm
=

and
g lc =

N
1 X ‚àÇgÃÇ
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi )),
N i=1 ‚àÇw
N
1 X ‚àÇg
(Wi , Xi) ¬∑ d(Wi ) ¬∑ (Xi ‚àí mÃÇ(Wi )),
N i=1 ‚àÇw
N
1 X ‚àÇg
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi )).
N i=1 ‚àÇw

Lemma A.19 Suppose Assumptions 3.1, 4.1, and 4.2 hold. Moreoever, suppose that the estimators for g(w, x)
and m(w), gÃÇ(w, x) and mÃÇ(w) respectively, satisfy
Àõ
Àõ
Àõ ‚àÇgÃÇ
Àõ
‚àÇg
Àõ
sup Àõ
(w, x) ‚àí
(w, x)ÀõÀõ = op(N ‚àíŒ∑ )
and sup |mÃÇ(w) ‚àí m(w)| = op (N ‚àíŒ∑ ),
‚àÇw
‚àÇw
w‚ààW,x‚ààX
w‚ààW
for some Œ∑ > 1/4. Then
‚Äú
‚Äù ‚Äú
‚Äù ‚Äú
‚Äù
‚Äú
‚Äù
lc
Œ≤ÃÇ lc ‚àí Œ≤ lc = Œ≤ÃÇglc ‚àí g lc + Œ≤ÃÇm
‚àí g lc + g lc ‚àí Œ≤ lc + op N ‚àí1/2 .

(A.4)

Lemma A.20 Suppose Assumptions 3.1, 4.1, and 4.2 hold, with q ‚â• 2s, r ‚â• s, p ‚â• 3, d ‚â• s, and 1/(2s) < Œ¥ <
1/12. Then:
Œ≤ÃÇglc ‚àí g lc =

N
‚Äú
‚Äù
1 X lc
œàg (Yi , Wi , Xi ) + op N ‚àí1/2 ,
N i=1

where
œàglc (Y, W, X) = ‚àí

‚àÇfW,X (W, X)
1
(Y ‚àí g (W, X)) d(W ) (X ‚àí m (W ))
fW,X (W, X)
‚àÇW
‚àÇm (W )
d(W ) (Y ‚àí g (W, X))
‚àÇW
‚àÇd
+
(W ) (X ‚àí m(W )) (Y ‚àí g(W, X)) .
‚àÇw

‚àí

[34]

(A.5)

Lemma A.21 Suppose Assumptions 3.1-4.2 hold, with q ‚â• 2s ‚àí 1, r ‚â• s, p ‚â• 3, d ‚â• s ‚àí 1, and
1
1
2
<Œ¥< ‚àí
,
2s
3
3p

then

Àõ
Àõ
Àõ
‚Äú
‚Äù2 Àõ
‚Äú
‚Äù
Àõ 1
Àõ
sup Àõ
fbW (w) ‚àí fW (w) Àõ = op N ‚àí1/2 .
Àõ
w‚ààW Àõ fbW (w)

Lemma A.22 Let h (w) = (h1 (w) , h2 (w))0 = (E [ X| W = w] fW (w) , fW (w))0 , and suppose Assumptions 3.14.2 hold, with q ‚â• 2s ‚àí 1, r ‚â• s, p ‚â• 3, d ‚â• s ‚àí 1, and
1
2
1
<Œ¥< ‚àí
,
2s
3
3p

then

Àõ
Àõ
Àõ
‚Äú
‚Äù‚Äú
‚ÄùÀõ
‚Äú
‚Äù
Àõ 1
Àõ
b
sup Àõ
h1 (w) ‚àí h1 (w) b
h2 (w) ‚àí h2 (w) Àõ = op N ‚àí1/2 .
Àõ
w‚ààW Àõ b
h2 (w)

Lemma A.23 Suppose Assumptions 3.1-4.2 hold, with q ‚â• 2s ‚àí 1, r ‚â• s, p ‚â• 3, and
1
1
<Œ¥< .
2s
8

Then
lc
Œ≤ÃÇm
‚àí g lc =

N
‚Äú
‚Äù
1 X
E [ gW (Wi , Xi )| Wi ] ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi )) + op N ‚àí1/2 ,
N i=1

(A.6)

0
0 0
Before the next theorem we need some additional definitions. We split Zi into (Zi1
, Zi2
) , with the dimension
of Zi1 equal to LZ1 , and the dimension of Zi2 equal to LZ2 , so that L = LZ1 + LZ2 . We are interested in the
distribution of
!
N
N
N N
‚àö
1 XX
1 XX
V = N¬∑
n(hÃÇnip,s (Z1j , Z2k )) ‚àí 2
n(h(Z1j , Z2k )) .
(A.7)
N 2 j=1
N j=1
k=1

k=1

We show that this is, to first order, equivalent to a single normalized sum.
Theorem A.3 Suppose that Assumptions A.1-A.4, hold with q ‚â• 2s ‚àí 1, r ‚â• s ‚àí 1 + L, 1/(2s) < Œ¥ < 1/(2L),
and t ‚â• 2. Then
¬ª
‚Äìff
N Ôöæ
1 X ‚àÇn
‚àÇn
0
0
V = ‚àö
(h(Zi )) YÃÉi fZ1 (Z1i )fZ2 (Z2i ) ‚àí EZ
(h(Z)) YÃÉ fZ1 (Z1i )fZ2 (Z2i )
+ op (1).
(A.8)
‚àÇh
N i=1 ‚àÇh
(To be clear here we index the expectation by the random variable the expectation is taken over, in this case Z.)
Before stating some additional lemmas that will be used for proving Theorem 4.3 we need some additional
definitions. Define
`
¬¥
N
N
‚àí1
œÜc Œ¶‚àí1
1 XX
c (FW (Wi )), Œ¶c (FX (Xj )); œÅ
¬¥ ` ‚àí1
¬¥
g cm = 2
g(Wi , Xj ) ` ‚àí1
N i=1 j=1
œÜc Œ¶c (FW (Wi )) œÜc Œ¶c (FX (Xj ))
`
¬¥
N
N
‚àí1
œÜc Œ¶‚àí1
1 XX
c (FW (Wi )), Œ¶c (FX (Xj )); œÅ
`
¬¥
`
¬¥
gÃÇ(W
,
X
)
i
j
‚àí1
N 2 i=1 j=1
œÜc Œ¶‚àí1
c (FW (Wi )) œÜc Œ¶c (FX (Xj ))
‚Äú
‚Äù
‚àí1
‚àí1
N X
N
œÜ
Œ¶
(
FÃÇ
(W
)),
Œ¶
(F
(X
));
œÅ
c
W
i
X
j
X
c
c
1
‚Äù `
= 2
g(Wi , Xj ) ‚Äú
¬¥
‚àí1
N i=1 j=1
œÜc Œ¶‚àí1
c (FÃÇW (Wi )) œÜc Œ¶c (FX (Xj ))

Œ≤ÃÇgcm =

cm
Œ≤ÃÇW

cm
Œ≤ÃÇX

‚Äú
‚Äù
‚àí1
N
N
œÜc Œ¶‚àí1
c (FW (Wi )), Œ¶c (FÃÇX (Xj )); œÅ
1 XX
‚Äù
= 2
g(Wi , Xj ) `
¬¥ ‚Äú ‚àí1
N i=1 j=1
œÜc Œ¶‚àí1
c (FW (Wi )) œÜc Œ¶c (FÃÇX (Xj ))

[35]

Lemma A.24 Suppose Assumptions 3.1-4.2 hold with q ‚â• 2s + 2, r ‚â• s + 3, and 1/(2s) < Œ¥ < 1/4, then
Œ≤bcm(œÅ, 0) ‚àí Œ≤ cm(œÅ, 0)
‚Äú
‚Äù ‚Äú
‚Äù ‚Äú
‚Äù
‚Äú
‚Äù
cm
cm
= Œ≤ÃÇgcm ‚àí gcm + Œ≤ÃÇW
‚àí g cm + Œ≤ÃÇX
‚àí g cm + (g cm ‚àí Œ≤ cm(œÅ, 0)) + op N ‚àí1/2 .

Lemma A.25 Suppose Assumptions 3.1-4.2 hold, then
Œ≤ÃÇgcm ‚àí g cm =
where

N
‚Äú
‚Äù
1 X cm
œàg (Yi , Wi , Xi) + op N ‚àí1/2 ,
N i=1

œàgcm (y, w, x) =

fW (w) ¬∑ fX (x)
(y ‚àí g(w, x))œâ(w, x).
fW X (w, x)

Lemma A.26 Suppose Assumptions 3.1-4.2 hold, then
cm
Œ≤ÃÇW
‚àí g cm =

where
cm
œàW
(y, w, x)

N
‚Äú
‚Äù
1 X cm
œàW (Yi , Wi , Xi) + op N ‚àí1/2 ,
N i=1

=

Z Z

g(s, t)eW (s, t) (1 (w ‚â§ s) ‚àí FW (s))fW (s) fX (t) dsdt.

Lemma A.27 Suppose Assumptions 3.1-4.2 hold, then
cm
Œ≤ÃÇX
‚àí g cm =

where

N
‚Äú
‚Äù
1 X cm
œàX (Yi , Wi , Xi) + op N ‚àí1/2 .,
N i=1

cm
œàX
(y, w, x) =

Z Z

g(s, t)eX (s, t) (1 (x ‚â§ t) ‚àí FX (t))fW (s) fX (t) dsdt.

Lemma A.28 Suppose Assumptions 3.1-4.2 hold, then
g cm ‚àí Œ≤ cm(œÅ, 0) =
where

N
‚Äú
‚Äù
1 X cm
œà0 (Yi , Wi , Xi ) + op N ‚àí1/2 ,
N i=1

œà0cm (w, x) = (E [g(W, x) ¬∑ œâ(W, x)] ‚àí Œ≤ cm (œÅ, 0)) + (E [g(w, X) ¬∑ œâ(w, X)] ‚àí Œ≤ cm(œÅ, 0)) .

(A.9)

The following theorem is a simplifed version of the V-statistics results in Lehman (1998).
Theorem A.4 (V-statistics) Suppose Z1 , . . . , ZN are independent and identically distributed random vectors
with dimension K, with support Z ‚äÇ RK . Let œà : ZK √ó ZK 7‚Üí R be a real-valued function. Define
Œ∏ = E [œà(Z1 , Z2 )] ,

œà1 (z) = E [œà(z, Z)] ,

œà2 (z) = E [œà(Z, z)] ,

2

œÉ = Cov(œà(Z1 , Z2 ), œà(Z1 , Z3 )) + Cov(œà(Z2 , Z1 ), œà(Z1 , Z3 ))
+Cov(œà(Z1 , Z2 ), œà(Z3 , Z1 )) + Cov(œà(Z2 , Z1 ), œà(Z3 , Z1 )).
and
V =

N
N
1 XX
œà(Zi , Zj ).
N 2 i=1 j=1

Then, if 0 < œÉ2 < ‚àû,
V =
and

‚àö

N
‚Äú
‚Äù
1 X
{(œà1 (Zi ) ‚àí Œ∏) + (œà2 (Zi ) ‚àí Œ∏)} + op N ‚àí1/2 ,
N i=1

`
¬¥
d
N ¬∑ (V ‚àí Œ∏) ‚àí‚Üí N 0, œÉ2 .

[36]

Appendix B: Proofs of Additional Lemmas and Theorems
In the following proofs c is a generic constant.
Proof of Lemma A.1: Because f (¬∑) is twice continuously differentiable on X, a compact subset of R, it follows
that for all a, b ‚àà X, by a Taylor series expansion,
f (b) = f (a) +

‚àÇf
1 ‚àÇ2f
(a) ¬∑ (b ‚àí a) + ¬∑
(c) ¬∑ (b ‚àí a)2 ,
‚àÇx
2 ‚àÇx2

for some c ‚àà X. Hence
Àõ
Àõ 2
Àõ
‚Äû
¬´Àõ
Àõ
Àõ
Àõ
Àõ
Àõf (g(Œª)) ‚àí f (g(0)) + ‚àÇf (g(0)) ¬∑ (g(Œª) ‚àí g(0)) Àõ ‚â§ 1 ¬∑ sup Àõ ‚àÇ f (x)Àõ ¬∑ (g(Œª) ‚àí g(0))2 .
Àõ
Àõ
Àõ
Àõ
2
‚àÇx
2 x‚ààX ‚àÇx
By the Lipschitz condition on g(Œª), this is bounded by
Àõ 2
Àõ
Àõ‚àÇ f
Àõ
1
Àõ
¬∑ sup
(x)ÀõÀõ ¬∑ c2 ¬∑ Œª2 .
2 x‚ààX Àõ ‚àÇx2


Rx
Proof of Lemma A.2: Let ¬µ = E[X], and write h(x) = h(xl ) + x

‚àÇ

l ‚àÇx

Cov(h(X), X) = E [h(X) ¬∑ (X ‚àí ¬µ)] = E
=E
=E
=

Z

¬ªZ
¬ªZ

xu

xl
xu

=

Z

xl
xu

=

Z

xl

Z

X
xl

‚àÇ
h(z)dz ¬∑ (X ‚àí ¬µ)
‚àÇx

xu
xl

1X>z ¬∑

‚Äì

¬ª‚Äû
Z
h(xl ) +

‚àÇ
h(z)dz ¬∑ (X ‚àí ¬µ)
‚àÇx

X

xl

‚àÇ
h(z)dz
‚àÇx

h(z)dz. Then:
¬´

¬∑ (X ‚àí ¬µ)

‚Äì

‚Äì

‚àÇ
h(z) ¬∑ E [1X>z ¬∑ (X ‚àí ¬µ)] dz
‚àÇx
‚àÇ
h(z) ¬∑ E [X ‚àí ¬µ|X > z] ¬∑ Pr(X > z)dz
‚àÇx
‚àÇ
h(z) ¬∑ FX (z) ¬∑ (1 ‚àí FX (z)) ¬∑ (E[X|X > z] ‚àí E[X|X ‚â§ z]) dz
‚àÇx

xu

FX (z) ¬∑ (1 ‚àí FX (z))
‚àÇ
h(z) ¬∑
¬∑ (E[X|X > z] ‚àí E[X|X ‚â§ z]) fX (z)dz
‚àÇx
fX (z)
¬ª
‚Äì
‚àÇ
=E
h(X) ¬∑ Œ≥(X) .
‚àÇx

=

xl


Proof of Lemma A.7: By the triangle inequality
Àõ
Àõ
‚Äú
‚Äù
Àõ
Àõ
sup N Œ¥ ¬∑ ÀõFÃÇY‚àí1 FÃÇX (x) ‚àí FY‚àí1 (FX (x))Àõ
x‚ààX

Àõ
‚Äú
‚Äù
‚Äú
‚ÄùÀõ
Àõ
Àõ
‚â§ sup N Œ¥ ¬∑ ÀõFÃÇY‚àí1 FÃÇX (x) ‚àí FY‚àí1 FÃÇX (x) Àõ
x‚ààX

Àõ
‚Äú
‚Äù
‚Äú
‚ÄùÀõ
Àõ
Àõ
+ sup N Œ¥ ¬∑ ÀõFY‚àí1 FÃÇX (x) ‚àí FY‚àí1 FÃÇX (x) Àõ
x‚ààX

Àõ
Àõ
Àõ
Àõ
‚â§ sup N Œ¥ ¬∑ ÀõFÃÇY‚àí1 (q) ‚àí FY‚àí1 (q)Àõ
q‚àà[0,1]

+

sup

x‚ààX,y‚ààY

NŒ¥ ¬∑

Àõ
Àõ
1 Àõ
Àõ
ÀõFÃÇX (x) ‚àí FX (x)Àõ .
fY (y)

The first term is op (1) by Lemma A.4, and the second by the fact that fY (y) is bounded away from zero, in
combination with Lemma A.3. 

[37]

Proof of Lemma A.8: By the triangle inequality
Àõ
Àõ
Àõ
Àõ
sup
N Œ∑ ¬∑ ÀõFÃÇY (y + x) ‚àí FÃÇY (y) ‚àí fY (y) ¬∑ xÀõ
y‚ààY,x‚â§N ‚àíŒ¥ ,x+y‚ààY

‚â§

sup

y‚ààY,x‚â§N ‚àíŒ¥ ,x+y‚ààY

+

sup

Àõ
Àõ
Àõ
Àõ
N Œ∑ ¬∑ ÀõFÃÇY (y + x) ‚àí (FY (y + x) ‚àí FY (y))Àõ

y‚ààY,x‚â§N ‚àíŒ¥ ,x+y‚ààY

N Œ∑ ¬∑ |FY (y + x) ‚àí FY (y) ‚àí fY (y) ¬∑ x| .

The first term on the right-hand side converges to zero in probability by Lemma A.5. To show that the second
term converges to zero note that
sup
y‚ààY,x‚â§N ‚àíŒ¥ ,x+y‚ààY

‚â§
‚â§
‚â§

N Œ∑ ¬∑ |FY (y + x) ‚àí FY (y) ‚àí fY (y) ¬∑ x|
sup

y‚ààY,x‚â§N ‚àíŒ¥ ,x+y‚ààY,Œª‚àà[0,1]

N Œ∑ ¬∑ |fY (y + Œªx) ¬∑ x ‚àí fY (y) ¬∑ x|

Àõ
Àõ
Àõ ‚àÇfY
Àõ
N Œ∑ ¬∑ ÀõÀõ
(z)ÀõÀõ ¬∑ Œªx2
‚àÇy
‚àíŒ¥
y‚ààY,z‚ààY,x‚â§N
,x+y‚ààY,Œª‚àà[0,1]
sup

N Œ∑ x2

sup

y‚ààY,x‚â§N ‚àíŒ¥

‚àÇfY
(y) ‚Üí 0,
‚àÇy

Y
because ‚àÇf
(y) is bounded, x < N ‚àíŒ¥ , and Œ¥ > Œ∑/2. 
‚àÇy
Proof of Lemma A.12: By the inequality |a| ‚â• |b| ‚àí |a ‚àí b|

inf |hÃÇ(z)| ‚â• inf |h(z)| ‚àí sup |hÃÇ(z) ‚àí h(z)|

z‚ààZ

z‚ààZ

z‚ààZ

from which the result follows. 
Proof of Lemma A.13: This follows directly from Theorem 7.1 in IR 
Proof of Lemma A.14: This follows directly from Theorem 7.1 in IR. 
Proof of Lemma A.15: First note that by the assumptions in the Lemma the conditions for Lemma A.14
are satisfied. Moreoever, by the assumption that 0 < Œ¥ < 1/6, it follows that Op (bN ) = op(N ‚àíŒ∑ ) for Œ∑ < Œ¥ ¬∑ s,
‚àí1+2Œ¥
‚àí1+4Œ¥
) = op(N ‚àíŒ∑ ) for
and Op (ln(N )N ‚àí1 b‚àí2
) = op (1), Op (ln(N )N ‚àí1 b‚àí4
N ) = Op (ln(N )N
N ) = Op (ln(N )N
‚àí1 ‚àí6
‚àí1+6Œ¥
Œ∑ < 1 ‚àí 4Œ¥, and Op (ln(N )N bN ) = Op (ln(N )N
) = op (1). Hence the results from Lemma A.14 imply
!
‚Äû
¬´1/2
ln(N )
s
sup |gÃÇ(w, x) ‚àí g(w, x)| = Op
+ bN = op(1),
(B.1)
N ¬∑ b2N
w‚ààW,x‚ààX
Àõ
Àõ
Àõ
Àõ ‚àÇgÃÇ
‚àÇg
Àõ
sup Àõ
(w, x) ‚àí
(w, x)ÀõÀõ = Op
‚àÇw
w‚ààW,x‚ààX ‚àÇw

for Œ∑ < min(1 ‚àí 4Œ¥, Œ¥ ¬∑ s), and
Àõ 2
Àõ
Àõ ‚àÇ gÃÇ
Àõ
‚àÇ2 g
Àõ = Op
sup ÀõÀõ 2 (w, x) ‚àí
(w,
x)
Àõ
2
‚àÇw
‚àÇw
w‚ààW,x‚ààX

‚Äû

ln(N )
N ¬∑ b4N
‚Äû

¬´1/2

ln(N )
N ¬∑ b6N

+

¬´1/2

bsN

+

!

bsN

`
¬¥
= op N ‚àíŒ∑ ,
!

= op (1).

(B.2)

(B.3)

Now,

Œ≤ÃÇ pam ‚àí Œ≤ pam =
=

N
‚Äù
‚Äù
ÀÜ ` ‚àí1
¬¥Àú
1 X ‚Äú ‚àí1 ‚Äú
gÃÇ FÃÇW FÃÇX (Xi ) , Xi ‚àí E g FW
(FX (X)) , X
N i=1

N
N
‚Äù
‚Äù
‚Äù
‚Äù
1 X ‚Äú ‚àí1 ‚Äú
1 X ‚Äú ‚àí1 ‚Äú
gÃÇ FÃÇW FÃÇX (Xi) , Xi ‚àí
g FÃÇW FÃÇX (Xi ) , Xi
N i=1
N i=1

‚àí
+

!
N
N
¬¥
¬¥
1 X ` ‚àí1
1 X ` ‚àí1
gÃÇ FW (FX (Xi)) , Xi ‚àí
g FW (FX (Xi)) , Xi
N i=1
N i=1

N
N
¬¥
¬¥
1 X ` ‚àí1
1 X ` ‚àí1
gÃÇ FW (FX (Xi )) , Xi ‚àí
g FW (FX (Xi )) , Xi
N i=1
N i=1

[38]

(B.4)

(B.5)

(B.6)

N
N
‚Äù
‚Äù
‚Äù
‚Äù
1 X ‚Äú ‚àí1 ‚Äú
1 X ‚Äú ‚àí1 ‚Äú
g FÃÇW FÃÇX (Xi ) , Xi ‚àí
g FW FÃÇX (Xi ) , Xi
N i=1
N i=1
!
N
N
‚Äù
¬¥
1 X ‚Äú ‚àí1
1 X ` ‚àí1
‚àí
g FÃÇW (FX (Xi )) , Xi ‚àí
g FW (FX (Xi )) , Xi
N i=1
N i=1

(B.7)

+

+

+

+

(B.8)

N
N
‚Äù
¬¥
1 X ‚Äú ‚àí1
1 X ` ‚àí1
g FÃÇW (FX (Xi )) , Xi ‚àí
g FW (FX (Xi)) , Xi
N i=1
N i=1

(B.9)

N
N
‚Äù
‚Äù
¬¥
1 X ‚Äú ‚àí1 ‚Äú
1 X ` ‚àí1
g FW FÃÇX (Xi ) , Xi ‚àí
g FW (FX (Xi)) , Xi
N i=1
N i=1

(B.10)

N
¬¥
ÀÜ ` ‚àí1
¬¥Àú
1 X ` ‚àí1
g FW (FX (Xi )) , Xi ‚àí E g FW
(FX (X)) , X .
N i=1

(B.11)

Since (B.6) is equal to Œ≤ÃÇpam,g ‚àí g pam , (B.9) equals Œ≤ÃÇpam,W ‚àí g pam , (B.10) equals Œ≤ÃÇpam,X ‚àí g pam , and (B.11)
equals gpam ‚àí Œ≤ pam, we only need to show that the sum of (B.4), (B.5), and that of (B.7), (B.8) are op (N ‚àí1/2 ).
First consider the sum of (B.4) and (B.5) that is equal to
N
N
‚Äù
‚Äù
¬¥
1 X ` ‚àí1
1 X ‚Äú ‚àí1 ‚Äú
gÃÇ FÃÇW FÃÇX (Xi ) , Xi ‚àí
gÃÇ FW (FX (Xi )) , Xi
N i=1
N i=1

‚àí

N
N
‚Äù
‚Äù
¬¥
1 X ‚Äú ‚àí1 ‚Äú
1 X ` ‚àí1
g FÃÇW FÃÇX (Xi ) , Xi ‚àí
g FW (FX (Xi )) , Xi
N i=1
N i=1

!

.

‚àí1
By a second order Taylor series expansion of gÃÇ and g in FW
(FX (Xi )) this is, for some WÃÉi and WÃÑi , equal to
N
‚Äù
‚Äù
¬¥ ‚Äú ‚àí1 ‚Äú
1 X ‚àÇgÃÇ ` ‚àí1
‚àí1
FW (FX (Xi )) , Xi FÃÇW
FÃÇX (Xi ) ‚àí FW
(FX (Xi ))
N i=1 ‚àÇw

+

‚àí

N
‚Äù‚Äú
‚Äú
‚Äù
‚Äù2
1 X ‚àÇ 2 gÃÇ ‚Äú
‚àí1
‚àí1
WÃÉ
,
X
FÃÇ
FÃÇ
(X
)
‚àí
F
(F
(X
))
i
i
X
i
X
i
W
W
2N i=1 ‚àÇw 2

(B.12)

N
‚Äù
‚Äù
¬¥ ‚Äú ‚àí1 ‚Äú
1 X ‚àÇg ` ‚àí1
‚àí1
FW (FX (Xi )) , Xi FÃÇW
FÃÇX (Xi ) ‚àí FW
(FX (Xi ))
N i=1 ‚àÇw

N
‚Äù
‚Äù2
¬¥ ‚Äú ‚àí1 ‚Äú
1 X ‚àÇ2 g `
‚àí1
WÃÑ
,
X
¬∑
FÃÇ
FÃÇ
(X
)
‚àí
F
(F
(X
))
.
i
i
X
i
X
i
W
W
2N i=1 ‚àÇw 2
¬´
N ‚Äû
¬¥
¬¥
1 X ‚àÇgÃÇ ` ‚àí1
‚àÇg ` ‚àí1
FW (FX (Xi )) , Xi ‚àí
FW (FX (Xi )) , Xi
=
N i=1 ‚àÇw
‚àÇw
‚Äú
‚Äú
‚Äù
‚Äù
‚Äú
‚Äù
‚àí1
‚àí1
√ó FÃÇW
FÃÇX (Xi ) ‚àí FW
(FX (Xi )) + op N ‚àí1/2 .
Àõ
Àõ
Àõ ‚àÇgÃÇ ` ‚àí1
¬¥ ‚àÇg ` ‚àí1
¬¥Àõ
‚â§ sup ÀõÀõ
FW (FX (x)) , x ‚àí
FW (FX (x)) , x ÀõÀõ
‚àÇw
x‚ààX ‚àÇw
Àõ‚Äú
‚Äú
‚Äù
‚ÄùÀõ
‚Äú
‚Äù
Àõ ‚àí1
Àõ
‚àí1
√ó sup Àõ FÃÇW FÃÇX (x) ‚àí FW
(FX (x)) Àõ + op N ‚àí1/2 .

‚àí

(B.13)

(B.14)
(B.15)

x‚ààX

‚àí1
We used the fact that (B.13) is op (N ‚àí1/2 ) because ‚àÇ 2 g(w, x)/‚àÇw 2 is bounded and because supx‚ààX (FÃÇW
(FÃÇX (x)) ‚àí
‚àí1
2
‚àí1/2
‚àí1/2
FW (FX (x))) is op (N
) by Lemma A.7. Also (B.12) is op(N
) by the same argument because the
bandwidth choice implies supw‚ààW,x‚ààX |‚àÇ 2 gÃÇ(w, x)/‚àÇw 2 ‚àí ‚àÇ 2 g(w, x)/‚àÇw 2 | = op (1) by (B.3), so that
Àõ 2
Àõ
Àõ 2
Àõ
Àõ 2
Àõ
Àõ 2
Àõ
2
Àõ ‚àÇ gÃÇ(w, x) Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ ‚â§ sup Àõ ‚àÇ g(w, x) Àõ + sup Àõ ‚àÇ gÃÇ(w, x) ‚àí ‚àÇ g(w, x) Àõ = sup Àõ ‚àÇ g(w, x) Àõ + op(1)
sup ÀõÀõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
2
2
2
2
2
‚àÇw
‚àÇw
‚àÇw
‚àÇw
‚àÇw
w‚ààW,x‚ààX
w‚ààW,x‚ààX
w‚ààW,x‚ààX
w‚ààW,x‚ààX

Finally by Lemma A.7
Àõ
Àõ
‚Äú
‚Äù
‚Äú
‚Äù
Àõ ‚àí1
Àõ
‚àí1
sup ÀõFÃÇW
FÃÇX (x) ‚àí FW
(FX (x))Àõ = op N ‚àí1/2+Œ∑
x‚ààX

[39]

for all Œ∑ > 0. By the assumption of the lemma
Àõ
Àõ
Àõ ‚àÇgÃÇ ` ‚àí1
¬¥
¬¥Àõ
‚àÇg ` ‚àí1
sup ÀõÀõ
FW (FX (x)) , x ‚àí
FW (FX (x)) , x ÀõÀõ = op(N ‚àíŒ∑ )
‚àÇw
x‚ààX ‚àÇw
‚Äú
‚Äù
for some Œ∑ > 0. We conclude that the sum of (B.4) and (B.5) is op N ‚àí1/2 .

Next, consider the sum of (B.7) and (B.8) that is bounded by
Àõh ‚Äú
‚Äú
‚Äù ‚Äù
‚Äú
‚Äú
‚Äù ‚Äùi h ‚Äú
‚Äù
` ‚àí1
¬¥iÀõÀõ
Àõ
‚àí1
‚àí1
‚àí1
sup Àõ g FÃÇW
FÃÇX (x) , x ‚àí g FW
FÃÇX (x) , x ‚àí g FÃÇW
(FX (x)) , x ‚àí g FW
(FX (x)) , x Àõ
x‚ààX

By a second order Taylor series expansion with intermediate values WÃÉ (x) and WÃÑ (x) and the triangle inequality
this is bounded by
Àõ
‚Äù ‚Äùh
‚Äú
‚Äù
‚Äú
‚Äùi
Àõ ‚àÇg ‚Äú ‚àí1 ‚Äú
‚àí1
‚àí1
sup ÀõÀõ
FW FÃÇX (x) , x FÃÇW
FÃÇX (x) ‚àí FW
FÃÇX (x)
x‚ààX ‚àÇw
iÀõÀõ
¬¥ h ‚àí1
‚àÇg ` ‚àí1
‚àí1
F (FX (x)) , x FÃÇW (FX (x)) ‚àí FW (FX (x)) ÀõÀõ +
‚àí
‚àÇw W
Àõ
Àõ 2
‚Äùh
‚Äú
‚Äù
‚Äú
‚Äùi2 ÀõÀõ 1
i2 ÀõÀõ
Àõ‚àÇ g `
¬¥ h ‚àí1
1 Àõ ‚àÇ2 g ‚Äú
‚àí1
‚àí1
‚àí1
Àõ
Àõ
Àõ
sup ÀõÀõ
WÃÉ
(x),
x
FÃÇ
FÃÇ
(x)
‚àí
F
FÃÇ
(x)
+
sup
WÃÑ
(x),
x
FÃÇ
(F
(x))
‚àí
F
(F
(x))
X
X
X
X
W
W
W
W
Àõ 2 x‚ààX Àõ ‚àÇw 2
Àõ
2
x‚ààX 2 ‚àÇw
where because the second derivative of g(w, x) is bounded on W √ó X, by Lemma A.4 the expression on the last
line is op(N ‚àí1/2 ). The first term is bounded by
Àõ¬ª
‚Äì
‚Äù ‚Äù
‚Äù
‚Äú
‚ÄùiÀõÀõ
Àõ ‚àÇg ‚Äú ‚àí1 ‚Äú
¬¥ h ‚àí1 ‚Äú
‚àÇg ` ‚àí1
‚àí1
sup ÀõÀõ
FW FÃÇX (x) , x ‚àí
FW (FX (x)) , x
FÃÇW FÃÇX (x) ‚àí FW
FÃÇX (x) ÀõÀõ
‚àÇw
‚àÇw
x‚ààX
Àõ
h
‚Äù
‚Äú
‚Äù
iÀõÀõ
Àõ ‚àÇg ` ‚àí1
¬¥ ‚àí1 ‚Äú
‚àí1
‚àí1
‚àí1
+ sup ÀõÀõ
FW (FX (x)) , x FÃÇW
FÃÇX (x) ‚àí FW
FÃÇX (x) ‚àí FÃÇW
(FX (x)) + FW
(FX (x)) ÀõÀõ
x‚ààX ‚àÇw
‚Äú
‚Äú
‚Äù ‚Äù
‚àí1
‚àÇg
FW
FÃÇX (x) , x in FX (x) we have, because the second derivative
By a first order Taylor series expansion of ‚àÇw

of g(w, x) is bounded and the density of W is bounded from 0 on its support, that by Lemmas A.4 and A.3, the
expression on the first line is op (N ‚àí1/2 ). The bound on the expression in the second line is proportional to
Àõ
Àõ
‚Äú
‚Äù
‚Äú
‚Äù
Àõ ‚àí1
Àõ
‚àí1
‚àí1
‚àí1
sup ÀõFÃÇW
FÃÇX (x) ‚àí FW
FÃÇX (x) ‚àí FÃÇW
(FX (x)) + FW
(FX (x))Àõ
x‚ààX

This expression is bounded by
Àõ
Àõ
Àõ
h
iÀõÀõ
h
‚Äú
‚Äú
‚Äù‚Äù
i
Àõ
`
¬¥
1
1
‚àí1
‚àí1
‚Äú
‚Äú
‚Äù‚Äù FÃÇW FW
` ‚àí1
¬¥ FÃÇW FW
sup ÀõÀõ
FÃÇX (x) ‚àí FÃÇX (x) ‚àí
(FX (x)) ‚àí FX (x) ÀõÀõ
‚àí1
fW F W
(FX (x))
x‚ààX Àõ f
Àõ
FÃÇX (x)
W FW
Àõ
Àõ
Àõ
‚Äù
‚Äú
‚Äù
h
‚Äú
‚Äú
‚Äù‚Äù
iÀõÀõ
Àõ ‚àí1 ‚Äú
1
‚àí1
‚àí1
‚Äú
‚Äú
‚Äù‚Äù FÃÇW FW FÃÇX (x) ‚àí FÃÇX (x) ÀõÀõ
+ sup ÀõÀõFÃÇW FÃÇX (x) ‚àí FW FÃÇX (x) ‚àí
‚àí1
x‚ààX Àõ
Àõ
fW F W
FÃÇX (x)
Àõ
Àõ
Àõ
h
iÀõ
` ‚àí1
¬¥
1
Àõ ‚àí1
Àõ
‚àí1
` ‚àí1
¬¥ FÃÇW FW
+ sup ÀõFÃÇW
(FX (x)) ‚àí FW
(FX (x)) ‚àí
(FX (x)) ‚àí FX (x) Àõ
Àõ
fW FW (FX (x))
x‚ààX Àõ

By Lemma A.6 the expressions in the last two lines are op(N ‚àí1/2 ). The expression in the first line is bounded
by
Àõ2
Àõ
3
Àõ
h
‚Äú
‚Äú
‚Äù‚Äù
iÀõÀõ
Àõ
1
1
‚àí1
‚Äú
‚Äú
‚Äù‚Äù ‚àí
` ‚àí1
¬¥ 5 FÃÇW FW FÃÇX (x) ‚àí FÃÇX (x) ÀõÀõ
sup ÀõÀõ4
fW F W
(FX (x))
x‚ààX Àõ fW F ‚àí1 FÃÇX (x)
Àõ
W
Àõ
Àõ
Àõ
h
‚Äú
‚Äú
‚Äù‚Äù
iÀõ
` ‚àí1
¬¥
1
Àõ
Àõ
‚àí1
`
¬¥
+ sup Àõ
FÃÇW FW FÃÇX (x) ‚àí FÃÇX (x) ‚àí FÃÇW FW (FX (x)) + FX (x) Àõ
‚àí1
Àõ
x‚ààX Àõ fW FW (FX (x))

[40]

The expression in the first line is bounded by
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
‚Äú
‚Äú
‚Äù‚Äù
Àõ
Àõ
1
1
Àõ
Àõ
‚àí1
‚Äú
‚Äú
‚Äù‚Äù ‚àí
` ‚àí1
¬¥ ÀõÀõ √ó sup ÀõFÃÇW FW
sup ÀõÀõ
FÃÇX (x) ‚àí FÃÇX (x)Àõ
fW FW (FX (x)) Àõ x‚ààX
x‚ààX Àõ fW F ‚àí1 FÃÇX (x)
W

By a first order Taylor series expansion of

1
‚àí1
fW (FW
(FÃÇX (x)))

in FX (x), the fact that fW (w) is bounded from 0 and

its derivative bounded on W, and Lemma A.3 the first factor is op (N ‚àíŒ¥ ) for all Œ¥ < 1/2 and by Lemma A.3 the
same is true for the second factor, so that the product is op (N ‚àí1/2 ). Because fW (w) is bounded from 0 on W,
the expression on the second line has a bound that is proportional to
Àõ
Àõ
‚Äú
‚Äú
‚Äù‚Äù
` ‚àí1
¬¥
Àõ
Àõ
‚àí1
sup ÀõFÃÇW FW
FÃÇX (x) ‚àí FÃÇX (x) ‚àí FÃÇW FW
(FX (x)) + FX (x)Àõ
x‚ààX

We rewrite this as
Àõ
‚Äú
‚Äú
‚Äù‚Äù
‚Äú
‚Äú
‚Äù
` ‚àí1
¬¥ ‚Äú
` ‚àí1
¬¥‚Äù‚ÄùÀõÀõ
Àõ
‚àí1
‚àí1
sup ÀõFÃÇW FW
FÃÇX (x) ‚àí FÃÇW FW
(FX (x)) ‚àí FW FW
FÃÇX (x) ‚àí FW FW
(FX (x))
Àõ‚â§
x‚ààX

Àõ
‚Äú
‚Äú
‚Äù‚Äù
‚Äú
‚Äú
‚Äù
` ‚àí1
¬¥ ‚Äú
` ‚àí1
¬¥‚Äù‚ÄùÀõÀõ
Àõ
‚àí1
‚àí1
sup ÀõFÃÇW FW
FÃÇX (x) ‚àí FÃÇW FW
(FX (x)) ‚àí FW FW
FÃÇX (x) ‚àí FW FW
(FX (x))
Àõ√ó
x‚ààX

+ 4 ¬∑ 1sup |F ‚àí1 (FÃÇ (x))‚àíF ‚àí1 (F (x))|>N ‚àíŒ¥
|
|
x‚ààX
X
X
W
W
By Lemma A.7 and the mean value theorem, the final term is op(1) if 1/3 < Œ¥ < 1/2. By
‚Äú
‚Äù
h
‚Äú
‚Äù
i
‚àí1
‚àí1
‚àí1
‚àí1
FW
FÃÇX (x) = FW
(FX (x)) + FW
FÃÇX (x) ‚àí FW
(FX (x))
1sup

‚àí1
‚àí1
‚àíŒ¥
x‚ààX FW (FÃÇX (x))‚àíFW (FX (x)) ‚â§N

‚Äú
‚Äù
‚àí1
‚àí1
‚àí1
(FX (x)) and wÃÉ = FW
FÃÇX (x) ‚àí FW
(FX (x)) we have that the first term on the right
and defining w = FW
hand side is bounded by
Àõ
Àõ
Àõ
Àõ
‚àí2/3
sup
)
ÀõFÃÇW (w + wÃÉ) ‚àí FÃÇW (w) ‚àí (FW (w + wÃÉ) ‚àí FW (w))Àõ = op (N
w‚ààW,|wÃÉ|‚â§N ‚àíŒ¥ ,wÃÉ+w‚ààW

by Lemma A.5 with 1/3 < Œ¥ < 1/2, Œ∑ = 2/3, so that we finally conclude that the sum of (B.7) and (B.8) is
op (N ‚àí1/2 ). 
Proof of Lemma A.16: The proof involves checking the conditions for Theorem A.2 from IR (given in Appendix
A in the current paper), and simplifying the conclusions from that Theorem to the case at hand.
Define
h1 (w, x) = fW X (w, x),
n(h) =

and

h2 (w, x) = fW X (w, x) ¬∑ g(w, x),

h2
,
h1

so that
œâ(x) = 1,
` ‚àí1
¬¥
g( FW
(FX (x)) , x
‚àÇn
h2
`
¬¥,
(h) = ‚àí
=
‚àí
‚àí1
‚àÇh1
(h1 )2
fW X ( F W
(FX (x)) , x
‚àÇn
1
1
` ‚àí1
¬¥,
(h) =
=
‚àÇh2
h1
fW X ( FW (FX (x)) , x

‚àÇ
fX (x)
t(x) =
,
‚àí1
‚àÇx
fW (FW
(FX (x)))
` ‚àí1
¬¥
Œ±1 (x) = ‚àíg( FW (FX (x)) , x ,
Œ±2 (x) = 1.
‚àí1
t(x) = FW
(FX (x)) ,

With YÃÉi = (YÃÉi1 YÃÉi2 )0 = (1 Yi )0 , we have
` ‚àí1
¬¥
Œ±(x)0 yÃÉ = y ‚àí g( FW
(FX (x)) , x .

Applying the results in Theorem A.2, we have
‚Äû
¬´
‚Äû
¬´
Z
Z
‚àí1
Wi ‚àí FW
(FX (Xi ))
Wi ‚àí t(Xi )
‚àÇt
fX (Xi)
K
+
(Xi ) ¬∑ u2 , u2 du2 =
K u,
+
¬∑ u du,
‚àí1
bN
‚àÇx
bN
fW (FW (FX (Xi )))
U2
u

[41]

Substituting this into the result from Theorem A.2 we get
‚Äù
‚àö ‚Äú pam
N Œ∏ÃÇg ‚àí g pam

!
Z
N
‚àí1
X
`
` ‚àí1
¬¥¬¥
Wi ‚àí FW
(FX (Xi ))
1
fX (Xi )
` ‚àí1
¬¥ ¬∑ u, u du
Yi ‚àí g FW (FX (Xi )) , Xi ¬∑
K
= ‚àö
+
bN
fW F W
(FX (Xi))
N bN i=1
u
! #!
"
Z
‚àí1
`
` ‚àí1
¬¥¬¥
W ‚àí FW
(FX (X))
fX (X)
` ‚àí1
¬¥ ¬∑ u, u du
+
‚àíE Y ‚àí g FW (FX (X)) , X ¬∑
K
bN
fW F W
(FX (X))
u
+op (1) .

Adding and subtracting g(Wi , Xi ) in both terms, this is equal to
!
(
Z
N
‚àí1
X
Wi ‚àí FW
(FX (Xi))
1
fX (Xi )
` ‚àí1
¬¥ ¬∑ u, u du
‚àö
(Yi ‚àí g(Wi , Xi )) ¬∑
K
+
bN
fW F W
(FX (Xi))
N bN i=1
u
! #)
"
Z
‚àí1
W ‚àí FW
(FX (X))
fX (X)
` ‚àí1
¬¥ ¬∑ u, u du
+
‚àíE (Y ‚àí g(W, X)) ¬∑
K
bN
fW F W
(FX (X))
u
(
!
Z
N
‚àí1
X
`
` ‚àí1
¬¥¬¥
Wi ‚àí FW
(FX (Xi ))
1
fX (Xi )
` ‚àí1
¬¥ ¬∑ u, u du
+‚àö
g(Wi , Xi ) ‚àí g FW (FX (Xi)) , Xi ¬∑
K
+
bN
fW F W
(FX (Xi ))
N bN i=1
u
"
! #)
Z
‚àí1
`
` ‚àí1
¬¥¬¥
W ‚àí FW
(FX (X))
fX (X)
` ‚àí1
¬¥ ¬∑ u, u du
‚àíE g(W, X) ‚àí g FW (FX (X)) , X ¬∑
K
+
bN
fW F W
(FX (X))
u
+op (1) .
Z
(Yi ‚àí g(Wi , Xi )) ¬∑
K

!
‚àí1
Wi ‚àí FW
(FX (Xi ))
1
fX (Xi )
` ‚àí1
¬¥ ¬∑ u2 , u2 du2
= ‚àö
+
bN
fW F W
(FX (Xi ))
N bN i=1
u2
(
!
Z
N
‚àí1
X
`
¬¥
Wi ‚àí FW
(FX (Xi ))
1
fX (Xi)
‚àí1
` ‚àí1
¬¥ ¬∑ u2 , u2 du2
+‚àö
g(Wi , Xi ) ‚àí g(FW (FX (Xi )), Xi ) ¬∑
K
+
bN
fW F W
(FX (Xi ))
N bN i=1
u2
"
!
#)
Z
‚àí1
`
¬¥
Wi ‚àí FW
(FX (X))
fX (X)
‚àí1
` ‚àí1
¬¥ ¬∑ u2 , u2 du2
‚àí E g(W, X) ‚àí g(FW (FX (X)), X) ¬∑
K
+
bN
fW F W
(FX (X))
u2
N
X

+op (1) .

Having checked the conditions for Theorem A.2, the second part of the result in the Lemma follows directly from
the second part of the Theorem. 
Proof of Lemma A.17: We prove the result in three parts. First, we show
N
N
‚Äù
¬¥
1 X ‚Äú ‚àí1
1 X ` ‚àí1
g FÃÇW (FX (Xi )) , Xi ‚àí
g FW (FX (Xi )) , Xi
N i=1
N i=1

=

N
‚Äù
‚Äú
‚Äù
` ‚àí1
¬¥ ‚Äú ‚àí1
1 X
‚àí1
gW FW
(FX (Xi )) , Xi ¬∑ FÃÇW
(FX (Xi )) ‚àí FW
(FX (Xi )) + op N ‚àí1/2
N i=1

(B.16)

Second, we will prove that

N
‚Äù
` ‚àí1
¬¥ ‚Äú ‚àí1
1 X
‚àí1
gW FW
(FX (Xi )) , Xi ¬∑ FÃÇW
(FX (Xi )) ‚àí FW
(FX (Xi ))
N i=1

=

` ‚àí1
¬¥
N
‚Äù
‚Äú
‚Äù
` ‚àí1
¬¥
1 X gW FW (FX (Xi )) , Xi ‚Äú
` ‚àí1
¬¥ ¬∑ FÃÇW FW
(FX (Xi )) ‚àí FX (Xi ) + op N ‚àí1/2 .
N i=1 fW FW (FX (Xi ))

Third, we will show that
` ‚àí1
¬¥
N
‚Äù
` ‚àí1
¬¥
1 X gW FW (FX (Xi )) , Xi ‚Äú
` ‚àí1
¬¥ ¬∑ FÃÇW FW
(FX (Xi )) ‚àí FX (Xi )
N i=1 fW FW (FX (Xi ))

[42]

(B.17)

=

N
‚Äú
‚Äù
1 X pam
œàW (Wi ) + op N ‚àí1/2 .
N i=1

(B.18)

Together these three claims, (B.16)-(B.18), imply the result in the Lemma.
First we prove (B.16).
Àõ
N
N
Àõ X
‚Äú
‚Äù
¬¥
1 X ` ‚àí1
Àõ1
‚àí1
g FÃÇW
(FX (Xi )) , Xi ‚àí
g FW (FX (Xi )) , Xi
Àõ
ÀõN
N
i=1
i=1

Àõ
N
‚ÄùÀõ
` ‚àí1
¬¥ ‚Äú ‚àí1
1 X
Àõ
‚àí1
‚àí
gW FW (FX (Xi )) , Xi ¬∑ FÃÇW (FX (Xi )) ‚àí FW (FX (Xi )) Àõ
Àõ
N i=1
Àõ ‚Äú
‚Äù
` ‚àí1
¬¥
Àõ
‚àí1
‚â§ sup Àõg FÃÇW
(FX (x)) , x ‚àí g FW
(FX (x)) , x
x‚ààX

‚ÄùÀõ
` ‚àí1
¬¥ ‚Äú ‚àí1
Àõ
‚àí1
‚àígW FW
(FX (x)) , x ¬∑ FÃÇW
(FX (x)) ‚àí FW
(FX (x)) Àõ
Àõ 2
Àõ
Àõ
Àõ2
Àõ ‚àÇ
Àõ
1
Àõ
‚àí1
‚àí1
Àõ ¬∑ sup ÀõÀõFÃÇW
‚â§ ¬∑ sup ÀõÀõ
g(w,
x)
(q)) ‚àí FW
(q)Àõ .
Àõ
2
2 w‚ààW,x‚ààX ‚àÇw
q‚àà[0,1]
Àõ
Àõ
Àõ ‚àí1
Àõ
‚àí1
By Lemma A.3 it follows that for all Œ¥ < 1/2, supq‚àà[0,1] N Œ¥ ¬∑ ÀõFÃÇW
(q) ‚àí FW
(q)Àõ = op(1). In combination with

the fact that

‚àÇ2g
(w, x)
‚àÇw 2

is bounded this implies that
Àõ 2
Àõ
Àõ
Àõ2
‚Äú
‚Äù
Àõ‚àÇ g
Àõ
Àõ ‚àí1
Àõ
‚àí1
sup ÀõÀõ 2 (w, x)ÀõÀõ ¬∑ sup ÀõFÃÇW
(q) ‚àí FW
(q)Àõ = op N ‚àí1/2 .
w‚ààW,x‚ààX ‚àÇw
q‚àà[0,1]

This finishes the proof of (B.16).
Next, we prove (B.17).
Àõ
N
Àõ X
‚Äù
` ‚àí1
¬¥ ‚Äú ‚àí1
Àõ1
‚àí1
gW FW
(FX (Xi )) , Xi ¬∑ FÃÇW
(FX (Xi )) ‚àí FW
(FX (Xi ))
Àõ
ÀõN
i=1

Àõ
` ‚àí1
¬¥
N
‚ÄùÀõ
` ‚àí1
¬¥
1 X gW FW (FX (Xi )) , Xi ‚Äú
Àõ
` ‚àí1
¬¥ ¬∑ FÃÇW FW (FX (Xi )) ‚àí FX (Xi ) Àõ
+
Àõ
N i=1 fW FW
(FX (Xi ))
Àõ
Àõ
Àõ
‚Äú
‚Äù
‚Äú
‚ÄùÀõ
` ‚àí1
¬¥
gW (w, x)
Àõ
Àõ
‚àí1
‚àí1
` ‚àí1 ¬¥ ¬∑ FÃÇW FW (q) ‚àí q Àõ
‚â§
sup
ÀõgW (w, x) ¬∑ FÃÇW (q) ‚àí FW (q) +
Àõ
fW F W
(q)
w‚ààW,x‚ààX,q‚àà[0,1] Àõ
Àõ
Àõ
Àõ‚Äú
‚Äú
‚ÄùÀõ
‚Äù
` ‚àí1 ¬¥
1
Àõ
Àõ ‚àí1
‚àí1
` ‚àí1 ¬¥ ¬∑ FÃÇW FW (q) ‚àí q Àõ ,
‚â§ sup |gW (w, x)| ¬∑ sup Àõ FÃÇW (q) ‚àí FW (q) +
Àõ
fW F W
(q)
w‚ààW,x‚ààX
q‚àà[0,1] Àõ

so that Lemma A.6 implies that (B.17) holds.
Finally, let us prove (B.18).
` ‚àí1
¬¥
N
‚Äù
` ‚àí1
¬¥
1 X gW FW (FX (Xi )) , Xi ‚Äú
` ‚àí1
¬¥ ¬∑ FÃÇW FW
(FX (Xi )) ‚àí FX (Xi )
N i=1 fW FW (FX (Xi ))

!
` ‚àí1
¬¥
N
N
1 X gW FW (FX (Xi )) , Xi
1 X
` ‚àí1
¬¥ ¬∑
=
1
‚àí FX (Xi )
‚àí1
N i=1 fW FW
N j=1 Wj ‚â§FW (FX (Xi ))
(FX (Xi ))
` ‚àí1
¬¥
N N
‚Äù
1 X X gW FW (FX (Xi )) , Xi ‚Äú
` ‚àí1
¬¥ ¬∑ 1F (Wj )‚â§F (Xi ) ‚àí FX (Xi ) .
= 2
W
X
N i=1 j=1 fW FW (FX (Xi))

This is a two-sample V-statistic. The projection is the sample average of the sum of the expectation over Wj if
pam
we fix Xi = x (this expectation is zero), and the expectation over Xi if we fix Wj = w, which gives œàW
(w).
Thus,
` ‚àí1
¬¥
N
N
‚Äù
‚Äú
‚Äù
` ‚àí1
¬¥
1 X gW FW (FX (Xi )) , Xi ‚Äú
1 X pam
` ‚àí1
¬¥ ¬∑ FÃÇW FW
œàW (Wi ) + op N ‚àí1/2 ,
(FX (Xi )) ‚àí FX (Xi ) =
N i=1 fW FW (FX (Xi ))
N i=1

[43]

which is the claim in (B.18). 
Proof of Lemma A.18: We prove this result in two steps. First we prove
Àõ
N
N
Àõ X
‚Äú
‚Äú
‚Äù
‚Äù
¬¥
1 X ` ‚àí1
Àõ1
g Fw‚àí1 FÃÇX (Xi ) , Xi ‚àí
g Fw (FX (Xi )) , Xi
Àõ
ÀõN
N i=1
i=1
Àõ
` ‚àí1
¬¥
‚ÄùÀõ
‚Äú
‚Äù
gW FW (FX (Xi)) , Xi ‚Äú
Àõ
` ‚àí1
¬¥ ¬∑ FÃÇX (Xi ) ‚àí FX (Xi ) Àõ = op N ‚àí1/2 .
‚àí
Àõ
fw FW (FX (Xi ))

(B.19)

Second, we prove

` ‚àí1
¬¥
N
N
‚Äù
‚Äú
‚Äù
1 X gW FW (FX (Xi )) , Xi ‚Äú
1 X pam
` ‚àí1
¬¥ ¬∑ FÃÇX (Xi ) ‚àí FX (Xi ) =
œàX (Xi) + op N ‚àí1/2 .
N i=1 fW FW (FX (Xi ))
N i=1

(B.20)

Together these two results imply the claim in Lemma A.18.
First we prove (B.19). By a second order Taylor series expansion, using the fact that g(w, x) is at least twice
continuously differentiable,
Àõ
N
N
Àõ X
‚Äú
‚Äú
‚Äù
‚Äù
¬¥
1 X ` ‚àí1
Àõ1
g Fw‚àí1 FÃÇX (Xi ) , Xi ‚àí
g Fw (FX (Xi )) , Xi
Àõ
ÀõN
N i=1
i=1
Àõ
` ‚àí1
¬¥
‚ÄùÀõ
gW FW (FX (Xi )) , Xi ‚Äú
Àõ
` ‚àí1
¬¥ ¬∑ FÃÇX (Xi ) ‚àí FX (Xi ) Àõ
‚àí
Àõ
fW F W
(FX (Xi ))
Àõ
Àõ
` ‚àí1
¬¥
Àõ ‚Äú
‚Äú
‚Äù ‚Äù
‚ÄùÀõ
` ‚àí1
¬¥ gW FW
(FX (x)) , x ‚Äú
Àõ
Àõ
‚àí1
`
¬¥
‚â§ sup Àõg FW FÃÇX (x) , x ‚àí g FW (FX (x)) , x ‚àí
¬∑ FÃÇX (x) ‚àí FX (x) Àõ
‚àí1
Àõ
fW F W
(FX (x))
x‚ààX Àõ
Àõ 2
Àõ
‚àÇf
Àõ ‚àÇ g (w, x)
Àõ
Àõ2
‚Äú
‚Äù
gW (w, x) ¬∑ ‚àÇw
(w) ÀõÀõ
1
Àõ 2
Àõ
Àõ
‚àí1/2
‚â§
sup Àõ ‚àÇw
‚àí
,
Àõ sup ÀõFÃÇX (x) ‚àí FX (x)Àõ = op N
2
Àõ x‚ààX
2 w‚ààW,x‚ààX Àõ fW (w)
(fW (w))
by Lemma A.3. This finishes the proof of (B.19).
Second we prove (B.20).
` ‚àí1
¬¥
N
‚Äù
1 X gW FW (FX (Xi )) , Xi ‚Äú
` ‚àí1
¬¥ ¬∑ FÃÇX (Xi ) ‚àí FX (Xi )
N i=1 fW FW (FX (Xi ))
=

` ‚àí1
¬¥
N N
¬¥
1 X X gW FW (FX (Xi )) , Xi `
`
¬¥
¬∑ 1Xj ‚â§Xi ‚àí FX (Xi )
‚àí1
2
N i=1 j=1 fW FW (FX (Xi))

This is a one-sample V-statistic. To obtain the projection we first fix Xi = x and take the expectation over Xj .
pam
This gives 0 for all x. Second, we fix Xj = x and take the expectation over Xi . This gives œàX
(x) defined
above. This finishes the proof of (B.20), and thus completes the proof of Lemma A.18. 
Proof of Lemma A.19: Adding and subtracting terms we have
Œ≤ÃÇ lc ‚àí Œ≤ lc
=

N
N
1 X ‚àÇgÃÇ
1 X ‚àÇg
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí mÃÇ(Wi )) ‚àí
(Wi , Xi) ¬∑ d(Wi ) ¬∑ (Xi ‚àí mÃÇ(Wi ))
N i=1 ‚àÇw
N i=1 ‚àÇw

N
N
1 X ‚àÇgÃÇ
1 X ‚àÇg
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi )) ‚àí
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi ))
N i=1 ‚àÇw
N i=1 ‚àÇw

‚àí
+

+

+

N
N
1 X ‚àÇgÃÇ
1 X ‚àÇg
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi )) ‚àí
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi ))
N i=1 ‚àÇw
N i=1 ‚àÇw
N
N
1 X ‚àÇg
1 X ‚àÇg
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí mÃÇ(Wi )) ‚àí
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi ))
N i=1 ‚àÇw
N i=1 ‚àÇw
N
1 X ‚àÇg
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi )) ‚àí Œ≤ lc .
N i=1 ‚àÇw

[44]

(B.21)
!

(B.22)

(B.23)

(B.24)

(B.25)

lc
Because (B.23) is equal to Œ≤glc ‚àí g lc , (B.24) is equal to Œ≤m
‚àí glc , and (B.25) is equal to glc ‚àí Œ≤ lc , it follows that it
is sufficient for the proof of Lemma A.19 to show that the sum of (B.21) and (B.22) is op(N ‚àí1/2 ). We can write
the sum of (B.21) and (B.22) as
N
N
1 X ‚àÇgÃÇ
1 X ‚àÇg
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí mÃÇ(Wi )) ‚àí
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí mÃÇ(Wi ))
N i=1 ‚àÇw
N i=1 ‚àÇw

‚àí

N
N
1 X ‚àÇg
1 X ‚àÇgÃÇ
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi )) ‚àí
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi ))
N i=1 ‚àÇw
N i=1 ‚àÇw

‚Äû
¬´
N
1 X
‚àÇgÃÇ
‚àÇg
d(Wi ) ¬∑
(Wi , Xi ) ‚àí
(Wi , Xi ) ¬∑ (m(Wi ) ‚àí mÃÇ(Wi ))
N i=1
‚àÇw
‚àÇw
Àõ
Àõ
Àõ ‚àÇgÃÇ
Àõ
`
¬¥
`
¬¥
‚àÇg
‚â§ sup |d(w)| ¬∑ sup ÀõÀõ
(w, x) ‚àí
(x, x)ÀõÀõ ¬∑ sup |mÃÇ(w) ‚àí m(w)| = C ¬∑ op N ‚àíŒ∑ ¬∑ op N ‚àíŒ∑ ,
‚àÇw
w‚ààW
w‚ààW,x‚ààX ‚àÇw
w‚ààW

!

=

for some Œ∑ > 1/4, and so this expression is op(N ‚àí1/2 ). 
Proof of Lemma A.20: The proof consists of checking the conditions for Theorem A.1, and specializing the
result in Theorem A.1 to the case in the Lemma.
We apply Theorem A.1 with z = (z1 z2 )0 = (w x)0 , Zi = (Wi Xi )0 , œâ(z) = d(z1 ) ¬∑ (z‚Äû2 ‚àí m(z
¬´ 1 )) = d(w) ¬∑ (x ‚àí m(w))
1
(so that œâ(z) goes smoothly to zero on the boundary of Z), L = 2, and Œª =
. Then {Œ∫ : Œ∫ ‚â§ Œª} =
0
Ôöæ‚Äû
¬´ ‚Äû
¬´ff
0
1
{Œ∫0 , Œ∫1 } =
,
, and
0
0
0 (Œ∫ )
1
h1 0 (w, x)
B (Œ∫0 )
C
B h2 (w, x) C
h[Œª] (w, x) = B (Œ∫
C,
)
@ h1 1 (w, x) A
(Œ∫1 )
h2 (w, x)

with

(Œ∫0 )

h1

(w, x) = fW X (w, x)

(Œ∫0 )

(w, x) = fW X (w, x) ¬∑ g(w, x)
‚àÇ
=
fW X (w, x)
‚àÇw
‚àÇ
‚àÇ
(Œ∫ )
fW X (w, x) + fW X (w, , x) ¬∑
g(w, x).
h2 1 (w, x) = g(w, x) ¬∑
‚àÇw
‚àÇw
The functional of interest is
h2

(Œ∫ )
h1 1 (w, x)

(Œ∫ )
(Œ∫ )
(Œ∫ )
‚Äú
‚Äù
‚àÇ
h 1
h 0 ¬∑ h1 1
n h[Œª] =
g(¬∑) = 2(Œ∫ ) ‚àí 2‚Äú
‚Äù
2
‚àÇw
(Œ∫ )
h1 0
h1 0

The derivatives of this functional are
‚àÇ
(Œ∫ )
‚àÇh1 0

(Œ∫ )
(Œ∫ )
(Œ∫ )
‚Äú
‚Äù
h 1
h 0 ¬∑ h1 1
n h[Œª] = ‚àí ‚Äú 2 ‚Äù2 + 2 2‚Äú
‚Äù3
(Œ∫ )
(Œ∫ )
h1 0
h1 0

=‚àí
=‚àí

fW X (w, x) ¬∑
‚àÇ
g(w, x)
‚àÇw

fW X (w, x)

‚àÇ
g(w, x)
‚àÇw

+

+ g(w, x) ¬∑

(fW X (w, x))

‚àÇ
f
(w, x)
‚àÇw W X

2

‚àÇ
g(w, x) ¬∑ ‚àÇw
fW X (w, x)
(fW X (w, x))2

(Œ∫ )
‚àÇ
‚Äú
‚Äù
h1 1
[Œª]
‚àÇw fW X (w, x)
n
h
=
‚àí
=
‚àí
‚Äú
‚Äù
2
(Œ∫ )
(Œ∫ )
(fW X (w, x))2
‚àÇh2 0
h1 0

‚àÇ

[45]

+2

g(w, x) ¬∑ fW X (w, x) ¬∑

‚àÇ
f
(w, x)
‚àÇw W X
3

(fW X (w, x))

(Œ∫ )
‚Äú
‚Äù
g(w, x)
g(w, x) ¬∑ fW X (w, x)
h2 0
[Œª]
=‚àí
n
h
=
‚àí
‚Äú
‚Äù2 = ‚àí
2
(Œ∫1 )
f
(Œ∫
)
W X (w, x)
(fW X (w, x))
‚àÇh1
h1 0

‚àÇ

‚àÇ
(Œ∫ )
‚àÇh2 1

‚Äú
‚Äù
n h[Œª] =

1
(Œ∫0 )

h1

=

1
.
fW X (w, x)

Œ±Œ∫0 ,1 (w, x) = d(w) ¬∑ (x ‚àí m(w)) ¬∑ fW (w, x) ¬∑
= d(w) ¬∑ (x ‚àí m(w)) ¬∑

‚àí

‚àÇ
g(w, x)
‚àÇw

fW X (w, x)

+

‚àÇ
f
(w, x)
‚àÇw W X
(fW X (w, x))2

g(w, x) ¬∑

‚àÇ
g(w, x) ¬∑ ‚àÇw
fW X (w, x)
‚àÇ
‚àí
g(w, x) +
‚àÇw
fW X (w, x)

Œ±Œ∫0 ,2 (w, x) = d(w) ¬∑ (x ‚àí m(w)) ¬∑ fW X (w, x) ¬∑

‚àÇ
fW X (w, x)
‚àí ‚àÇw
(fW X (w, x))2

!

!

!

= ‚àíd(w) ¬∑ (x ‚àí m(w)) ¬∑

‚àÇ
‚àÇw fW X (w, x)

fW X (w, x)

¬´
‚Äû
g(w, x)
Œ±Œ∫1 ,1 (w, x) = d(w) ¬∑ (x ‚àí m(w)) ¬∑ fW X (w, x) ¬∑ ‚àí
= ‚àíd(w) ¬∑ (x ‚àí m(w)) ¬∑ g(w, x)
fW X (w, x)
Œ±Œ∫1 ,2 (w, x) = d(w) ¬∑ (x ‚àí m(w)) ¬∑ fW X (w, x) ¬∑

(‚àí1)

|Œ∫0 |

(Œ∫ )
Œ±Œ∫00,1 (w, x)

1
= d(w) ¬∑ (x ‚àí m(w))
fW X (w, x)

= Œ±Œ∫0 ,1 (w, x) = d(w) ¬∑ (x ‚àí m(w)) ¬∑

(Œ∫ )

(‚àí1)|Œ∫0 | Œ±Œ∫00,2 (w, x) = Œ±Œ∫0 ,2 (w, x) = ‚àíd(w) ¬∑ (x ‚àí m(w)) ¬∑

(Œ∫ )

(‚àí1)|Œ∫1 | Œ±Œ∫11,1 (w, x) =

‚àÇ
g(w, x) ¬∑ ‚àÇw
fW X (w, x)
‚àÇ
‚àí
g(w, x) +
‚àÇw
fW X (w, x)

!

‚àÇ
f
(w, x)
‚àÇw W X

fW X (w, x)

‚Äû
¬´
‚àÇ
d(w) ¬∑ (x ‚àí m(w)) ¬∑ g(w, x)
‚àÇw

= d(w) ¬∑ (x ‚àí m(w)) ¬∑
(Œ∫ )

(‚àí1)|Œ∫1 | Œ±Œ∫11,2 (w, x) = ‚àí

‚àÇ
‚àÇ
‚àÇ
g(w, x) + g(w, x) ¬∑ (x ‚àí m(w)) ¬∑
d(w) ‚àí g(w, x) ¬∑ d(w) ¬∑
m(w)
‚àÇw
‚àÇw
‚àÇw

‚àÇ
‚àÇ
‚àÇ
(d(w) ¬∑ (x ‚àí m(w))) = ‚àí(x ‚àí m(w)) ¬∑
d(w) + d(w) ¬∑
m(w)
‚àÇw
‚àÇw
‚àÇw

Then
X

Œ∫‚â§Œª

(‚àí1)|Œ∫|

2
X

(Œ∫)
Œ±Œ∫m
(w, x)yÃÉim

m=1
(Œ∫ )

(Œ∫ )

(Œ∫ )

(Œ∫ )

= (‚àí1)|Œ∫0 | Œ±Œ∫00,1 (w, x) + Yi ¬∑ (‚àí1)|Œ∫0 | Œ±Œ∫00,2 (w, x) + (‚àí1)|Œ∫1 | Œ±Œ∫11,1 (w, x) + Yi ¬∑ (‚àí1)|Œ∫1 | Œ±Œ∫11,2 (w, x)
!
‚àÇ
g(w, x) ¬∑ ‚àÇw
fW X (w, x)
‚àÇ
g(w, x) +
= d(w) ¬∑ (x ‚àí m(w)) ¬∑ ‚àí
‚àÇw
fW X (w, x)
‚àíYi ¬∑ d(w) ¬∑ (x ‚àí m(w)) ¬∑

‚àÇ
f
(w, x)
‚àÇw W X

fW X (w, x)
‚àÇ
‚àÇ
‚àÇ
+d(w) ¬∑ (x ‚àí m(w)) ¬∑
g(w, x) + g(w, x) ¬∑ (x ‚àí m(w)) ¬∑
d(w) ‚àí g(w, x) ¬∑ d(w) ¬∑
m(w)
‚àÇw
‚àÇw
‚àÇw

[46]

‚Äû
¬´
‚àÇ
‚àÇ
+Yi ¬∑ ‚àí(x ‚àí m(w)) ¬∑
d(w) + d(w) ¬∑
m(w)
‚àÇw
‚àÇw
= ‚àí (Y ‚àí g(W, X))¬∑

‚àÇ
f
(W, X)
‚àÇw W X

fW X (W, X)

‚àÇ
‚àÇ
¬∑ d(w) ¬∑ (X ‚àí m(W )) + (X ‚àí m(W )) ¬∑
d(W ) ‚àí d(W ) ¬∑
m(W )
‚àÇw
‚àÇw

!

Since
"

E ‚àí (y ‚àí g(w, x)) ¬∑

‚àÇ
‚àÇw fW X (w, x)

fW X (w, x)

¬∑ d(w) ¬∑ (x ‚àí m(w)) + (x ‚àí m(w)) ¬∑

‚àÇ
‚àÇ
d(w) ‚àí d(w) ¬∑
m(w)
‚àÇw
‚àÇw

!#

= 0,

it follows that
0
1
N
2
h
i
X
1 X @X
A = 0,
‚àö
(‚àí1)|Œ∫|
E Œ±(Œ∫)
Œ∫m (Wi , Xi )YÃÉim
N i=1 Œ∫‚â§Œª
m=1

and therefore
‚àö
where

0
1
N
2
N
X
1 X @X
1 X lc
|Œ∫|
(Œ∫)
N (Œ≤ÃÇ ‚àí Œ≤ ) = ‚àö
(‚àí1)
Œ±Œ∫m (Wi , Xi )YÃÉim A = ‚àö
œà (Yi , Wi , Xi ).
N i=1 Œ∫‚â§Œª
N i=1
m=1
lc

lc

œàlc (y, w, x) = ‚àí (y ‚àí g(w, x))¬∑

‚àÇ
‚àÇw fW X (w, x)

fW X (w, x)

¬∑ d(w) ¬∑ (x ‚àí m(w)) + (x ‚àí m(w)) ¬∑

!
‚àÇ
‚àÇ
d(w) ‚àí d(w) ¬∑
m(w) .
‚àÇw
‚àÇw

.
Proof of Lemma A.21: We start with the inequality
‚Äû
Àõ
Àõ¬´2
Àõb
Àõ
Àõ
Àõ
sup
f
(w)
‚àí
f
(w)
Àõ
Àõ
W
W
Àõ
‚Äú
‚Äù2 Àõ
w‚ààW
Àõ 1
Àõ
Àõ
Àõ
sup Àõ
.
fbW (w) ‚àí fW (w) Àõ ‚â§
Àõ
Àõ
Àõ
w‚ààW Àõ fbW (w)
inf ÀõfbW (w)Àõ
w‚ààW

Under the stated restriction on Œ¥ the bandwidth sequence satisfies
N 1/4 1/2
p
bN ‚Üí ‚àû,
ln(N )

N 1/4 bsN ‚Üí 0,

which, by Lemma A.11, implies
‚Äû
Àõ
Àõ¬´2
‚Äú
‚Äù
Àõ
Àõ
sup ÀõfbW (w) ‚àí fW (w)Àõ
= op N ‚àí1/2 .
w‚ààW

Àõ
Àõ
Àõ
Àõ
Now observe that the the denominator is bounded away from zero since, by the TI, we have ÀõfbW (w)Àõ+|fW (w)| ‚â•
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõb
Àõ
Àõ
Àõ
Àõ
ÀõfW (w) ‚àí fW (w)Àõ and therefore inf ÀõfbW (w)Àõ ‚â• sup ÀõfbW (w) ‚àí fW (w)Àõ ‚àí inf |fW (w)| ‚â• inf |fW (w)| ‚àí
w‚ààW
w‚ààW
w‚ààW
w‚ààW
Àõ
Àõ
Àõ
Àõ
sup ÀõfbW (w) ‚àí fW (w)Àõ . By Assumption 3.1 inf |fW (w)| is bounded away from zero, with the result then

w‚ààW

w‚ààW

following. 
Proof of Lemma A.22:We start with the inequality

Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
Àõ
sup Àõb
h1 (w) ‚àí h1 (w)Àõ √ó sup Àõb
h2 (w) ‚àí h2 (w)Àõ
Àõ
Àõ
‚Äú
‚Äù
‚Äú
‚Äù
w‚ààW
w‚ààW
Àõ 1
Àõ
b
Àõ
Àõ
sup Àõ
h1 (w) ‚àí h1 (w) b
h2 (w) ‚àí h2 (w) Àõ ‚â§
.
Àõ
Àõ
Àõ
w‚ààW Àõ b
h2 (w)
inf Àõb
h (W )Àõ
w‚ààW

The remainder of the proof is along the lines of that to Lemma A.21. 

[47]

2

i

.

Proof of Lemma A.23: Let h (w) = (h1 (w) , h2 (w))0 = (m(w) ¬∑ fW (w) , fW (w))0 , then
!
N
b
1 X
h1,nip (Wi )
Œ≤ÃÇlc,m =
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑ Xi ‚àí
b
N
h2,nip (Wi )
i=1

N
1 X
=
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m(Wi ))
N i=1

‚àí

N
1 X
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑
N i=1

Expanding the ratio4 in (B.27) yields
N
1 X
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑
N i=1

=‚àí
‚àí
‚àí

b
h1,nip (Wi )
h1 (Wi )
‚àí
b
h2 (Wi )
h2,nip (Wi )

b
h1 (Wi )
h1,nip (Wi )
‚àí
b
h2 (Wi )
h2,nip (Wi )

!

.

(B.26)

(B.27)

!

N
‚Äú
‚Äù
1
1 X
b
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑
h1,nip (Wi ) ‚àí m(Wi )b
h2,nip (Wi )
N i=1
fW (Wi )

N
‚Äú
‚Äù2
h1 (Wi )
1 X
b
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑
h
(W
)
‚àí
h
(W
)
2,nip
i
2
i
N i=1
h2 (Wi )2 b
h2,nip (Wi )

(B.28)

(B.29)

N
‚Äú
‚Äù‚Äú
‚Äù
1 X
h1 (Wi )
b
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑
h1,nip (Wi ) ‚àí h1 (Wi ) b
h2,nip (Wi ) ‚àí h2 (Wi ) .
N i=1
h2 (Wi ) b
h2,nip (Wi )
(B.30)

First consider (B.29). By Lemma A.12,
Àõ
Àõ
N
Àõ X
‚Äú
‚Äù2 Àõ
h1 (Wi )
Àõ1
Àõ
b
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑
h2,nip (Wi ) ‚àí h2 (Wi ) Àõ
Àõ
ÀõN
Àõ
h2 (Wi )2 b
h2,nip (Wi )
i=1
Àõ
Àõ
Àõ
Àõ
Àõ
‚Äú
‚Äù2 Àõ
Àõ
Àõ 1
Àõ 1
Àõ
b
Àõ
Àõ
gW (w, x) ¬∑ d(w) ¬∑ m(w)Àõ sup Àõ
fW (w) ‚àí fW (w) Àõ
‚â§ sup Àõ
Àõ
w‚ààW Àõ fbW (w)
w‚ààW,x‚ààX fW (w)
‚Äú
‚Äù
= op N ‚àí1/2

1
if the NIP estimator is uniformly op (N ‚àí1/4‚Äú) which‚Äù holds if 4s
< Œ¥ < 81 . An analogous application of Lemma
‚àí1/2
A.12 can be used to show that (B.30) is op N
under the same condition.

Now consider (B.28) that we express as the sum of a variance and a bias term
‚àí

N
‚Äú
h
i
‚Äú
h
i‚Äù‚Äù
1 X
1
b
gW (Wi , Xi )¬∑d(Wi )¬∑
h1,nip (Wi ) ‚àí E b
h1,nip (Wi ) ‚àí m(Wi ) b
h2,nip (Wi ) ‚àí E b
h2,nip (Wi )
+
N i=1
fW (Wi )
N
‚Äú
h
i
‚Äú
h
i‚Äù‚Äù
1 X
1
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑
h1 (Wi ) ‚àí E b
h1,nip (Wi ) ‚àí m(Wi ) h2 (Wi ) ‚àí E b
h2,nip (Wi )
N i=1
fW (Wi )

The bias term is Op (N ‚àí1/2 ) if Œ¥ >
‚àí

4

‚Äú

1
2s .

After substitution of the NIP estimator the variance term is

N
s‚àí1 X
X
1 X
1
1
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑
N i=1
fW (Wi ) j=0
¬µ!
|¬µ|=j

(¬µ)

hÃÇ1,N W

h
i
‚Äú
h
i‚Äù
‚Äù
(¬µ)
(¬µ)
(¬µ)
‚àí E hÃÇ1,N W (rb (Wi )) ‚àí m(Wi ) hÃÇ2,N W (rb (Wi )) ‚àí E hÃÇ2,N W (rb (Wi )) (rb (Wi ))(Wi ‚àí rb (Wi ))¬µ

The ratio expansion is of the form
a
b
a
1‚Äú
a ‚Äù
a b
a
‚àí =
a‚àí b
b
b +
(b ‚àí b)2 ‚àí (b
a ‚àí a)(b
b ‚àí b).
b
b
b
b
b
b2b
b
bb
b

[48]

We consider separately
‚àí

N
s‚àí1 X
h
i‚Äù
X
1 X
1
1 ‚Äú (¬µ)
(¬µ)
gW (Wi , Xi)¬∑d(Wi )¬∑
hÃÇ1,N W (rb (Wi )) ‚àí E hÃÇ1,N W (rb (Wi )) (Wi ‚àírb (Wi ))¬µ (B.31)
N i=1
fW (Wi ) j=0
¬µ!
|¬µ|=j

and
N
s‚àí1
h
i‚Äù
1 X
m(Wi ) X X 1 ‚Äú (¬µ)
(¬µ)
gW (Wi , Xi )¬∑d(Wi )¬∑
hÃÇ2,N W (rb (Wi )) ‚àí E hÃÇ2,N W (rb (Wi )) (Wi ‚àírb (Wi ))¬µ (B.32)
N i=1
fW (Wi ) j=0
¬µ!
|¬µ|=j

We show that (B.31) is asymptotically equivalent to an average. The same method shows that (B.32) is also
asymptotically equivalent to an average, but we omit the details. The expression (B.31) is a linear combination
of terms
D¬µ = ‚àí
‚àí
with

N
‚Äú
h
i‚Äù
1 X
1
(¬µ)
(¬µ)
gW (Wi , Xi ) ¬∑ d(Wi ) ¬∑
hÃÇ1,N W (rb (Wi )) ‚àí E hÃÇ1,N W (rb (Wi )) (Wi ‚àí rb (Wi ))¬µ =
N i=1
fW (Wi )

N
N
1 XX
aN,¬µ (Wi , Xi , Xj , Wj )
N i=1 j=1

aN,¬µ (Wi , Xi , Xj , Wj ) =

E

"

1

XK (¬µ)
1+|¬µ|
bN

‚Äû

gW (Wi , Xi ) d(Wi )
fW (Wi )

W ‚àí rbN (Wi )
bN

¬´#!

1

Xj K (¬µ)
1+|¬µ|
bN

‚Äû

Wj ‚àí rbN (Wi )
bN

¬´

‚àí

(Wi ‚àí rbN (Wi ))¬µ

Therefore D¬µ is a V-statistic with a kernel that depends on N so that the usual projection theorem does not apply
directly. Instead we derive the projection directly. First we bound the second moments of aN,¬µ (Wi , Xi , Xj , Wj ).
For j 6= i we have
"
‚Äû
¬´2 #
ÀÜ
supw‚ààW |w ‚àí rb (w)|2|¬µ|
Wj ‚àí rbN (Wi )
2
(¬µ)
2Àú
E Xj K
E aN,¬µ (Wi , Xi, Xj , Wj ) ‚â§ C
‚â§
2|¬µ|+2
bN
b
N

"

C
E K (¬µ)
b2N

‚Äû

Wj ‚àí rbN (Wi )
bN

¬´2 #

because the conditional variance of X given W is bounded. Because given Wi = wÃÉ
# Z
"
‚Äû
¬´2
‚Äû
¬´2
w ‚àí rbN (wÃÉ)
Wj ‚àí rbN (Wi )
(¬µ)
|Wi = wÃÉ =
K (¬µ)
fW (w)dw
E K
bN
bN
W
we have by a change of variables to t = (w ‚àí rbN (wÃÉ))/bN with Jacobian bN and the boundedness of K (¬µ)(t) and
fW (w) that this integral is bounded by CbN and we conclude
ÀÜ
Àú
E aN,¬µ (Wi , Xi, Xj , Wj )2 = O(b‚àí1
N )

For j = i we have
ÀÜ

E aN,¬µ (Wi , Xi , Xi, Wi )

"

gW (Wi , Xi )2 d(Wi )2 2 (¬µ)
= 2+2|¬µ| E
Xi K
fW (Wi )2
bN
1

‚Äû

Wi ‚àí rbN (Wi )
bN

¬´2

(Wi ‚àí rbN (Wi ))

2¬µ

#

+

#
¬ª
‚Äû
¬´‚Äì2
W ‚àí rbN (Wi )
gW (Wi , Xi)2 d(Wi )2
(¬µ)
2¬µ
E
E XK
(Wi ‚àí rbN (Wi ))
‚àí
2+2|¬µ|
bN
fW (Wi )2
bN
¬ª
‚Äû
¬´ ¬ª
‚Äû
¬´‚Äì
‚Äì
Wi ‚àí rbN (Wi )
W ‚àí rbN (Wi )
2
gW (Wi , Xi )2 d(Wi )2
(¬µ)
(¬µ)
2¬µ
E
X
K
E
XK
(W
‚àí
r
(W
))
i
i
b
i
N
2
2+2|¬µ|
bN
bN
fW (Wi )
bN
1

"

2Àú

[49]

The first term on the right hand side is bounded by
"
‚Äû
¬´2 #
‚Äû
¬´2
Z
Wi ‚àí rbN (Wi )
w ‚àí rbN (w)
C
C
(¬µ)
(¬µ)
E K
= 2
K
fW (w)dw+
b2N
bN
bN WI
bN
bN

C
b2N

Z

K (¬µ)

W\WI
bN

‚Äû

w ‚àí rbN (w)
bN

¬´2

fW (w)dw

with WIbN the internal set of the support. Because the argument of K (¬µ) is 0 on the interior set, the first integral
is obviously O(b‚àí2
N ). The second integral is
‚Äû
¬´2
‚Äû
¬´2
Z wl +bN
Z wu
w ‚àí wl
C
w ‚àí wu
C
(¬µ)
(¬µ)
K
‚àí
1
f
(w)dw
+
K
+
1
fW (w)dw
W
b2N wl
bN
b2N wu ‚àíbN
bN
Because the kernel has support [‚àí1, 1] and its derivatives op to order ¬µ are bounded so that
‚Äû
¬´2
‚Äû
¬´2
w ‚àí wl
w ‚àí wu
K (¬µ)
‚àí 1 ‚â§ C1wl ‚â§w‚â§wl +2bN
K (¬µ)
+ 1 ‚â§ C1wu ‚àí2bN ‚â§w‚â§wu
bN
bN
so that the second integral by the boundedness of fW is O(b‚àí1
N ). The second term on the right hand side is
bounded by
‚Äû
¬´
¬´2
‚Äû
¬´2
Z ‚ÄûZ
Z Z
w ‚àí rbN (wÃÉ)
w ‚àí rbN (wÃÉ)
C
C
(¬µ)
(¬µ)
K
f
(w)dw
f
(
wÃÉ)d
wÃÉ
‚â§
K
fW (w)fW (wÃÉ)dwdwÃÉ
W
W
b2N W
bN
b2N W W
bN
W
This integral is O(b‚àí1
N ) by a change of variables with Jacobian bN in the inner integral. The third term on the
right hand side is bounded by
ÀõZ
Àõ
‚Äû
¬´Z
‚Äû
¬´
Àõ
wÃÉ ‚àí rbN (wÃÉ)
w ‚àí rbN (wÃÉ)
C ÀõÀõ
(¬µ)
(¬µ)
Àõ = O(b‚àí1
K
K
f
(w)dwf
(
wÃÉ)d
wÃÉ
W
W
N )
Àõ
b2N Àõ W
bN
bN
W
by a change of variables in the inner integral. We conclude
ÀÜ
Àú
E aN,¬µ (Wi , Xi, Xi, Wi )2 = O(b‚àí2
N )
The next step is to express D¬µ as an average. Define
cN,¬µ (Xj , Wj ) =
Z Z
X

W

gW (w, x) d(x)
fW (w)

and

E¬µ = ‚àí

1
1+|¬µ|
bN

¬∑

‚Äû
‚Äû
¬´
¬ª
‚Äû
¬´‚Äì¬´
Wj ‚àí rbN (w)
W ‚àí rbN (w)
Xj K (¬µ)
‚àí E XK (¬µ)
(w ‚àírbN w))¬µ fW X (w, x)dwdx
bN
bN

N
1 X
cN,¬µ (Xj , Wj )
N j=1

Then
D¬µ ‚àí E¬µ =

N (N ‚àí 1)
(D¬µ,1 ‚àí E¬µ ) +
N2

‚Äû

¬´
N (N ‚àí 1)
‚àí
1
E¬µ + D¬µ,2
N2

with
D¬µ,1 = ‚àí

N
X
1
aN,¬µ (Wi , Xi , Xj , Wj )
N (N ‚àí 1)

D¬µ,2 = ‚àí

i6=j=1

N
1 X
aN,¬µ (Wi , Xi , Xi , Wi )
N 2 i=1

Now
D¬µ,1 ‚àí E¬µ = ‚àí

N
X
1
(aN,¬µ (Wi , Xi , Xj , Wj ) ‚àí cN,¬µ (Xj , Wj ))
N (N ‚àí 1) i6=j=1

[50]

with
E[(aN,¬µ (Wi , Xi , Xj , Wj ) ‚àí cN,¬µ (Xj , Wj ))(aN,¬µ (Wi0 , Xi0 , Xj 0 , Wj 0 ) ‚àí cN,¬µ (Xj 0 , Wj 0 ))] = 0
if (i) i 6= i0 , j 6= j 0 , (ii) i = i0 , j 6= j 0 (iii) i 6= i0 , j = j 0 , because
E[aN,¬µ (Wi , Xi , Xj , Wj )|Wi , Xi ]

=

0

E[aN,¬µ (Wi , Xi , Xj , Wj )]

=

0

E[aN,¬µ (Wi , Xi , Xj , Wj )|Xj , Wj ]

=

cN,¬µ (Wj , Xj )

E[cN,¬µ (Wj , Xj )]

=

0

Therefore
E[(D¬µ ‚àí E¬µ )2 ] =

XX
1
E[(aN,¬µ (Wi , Xi, Xj , Wj ) ‚àí cN,¬µ (Xj , Wj ))(aN,¬µ (Wi0 , Xi0 , Xj 0 , Wj 0 ) ‚àí cN,¬µ (Xj 0 , Wj 0 ))] =
N 2 (N ‚àí 1)2
0
0
i6=j i 6=j

X
1
E[(aN,¬µ (Wi , Xi , Xj , Wj ) ‚àí cN,¬µ (Xj , Wj ))2 ] =
2
2
N (N ‚àí 1) i6=j

Because

E[(aN,¬µ (Wi , Xi , Xj , Wj ) ‚àí cN,¬µ (Xj , Wj ))2 ] = E[(aN,¬µ (Wi , Xi , Xj , Wj )2 ] ‚àí E[cN,¬µ (Xj , Wj ))2 ] ‚â§
E[(aN,¬µ (Wi , Xi , Xj , Wj )2 ] = O(b‚àí1
N )
we have
E[(D¬µ ‚àí E¬µ )2 ] = O(N ‚àí2 b‚àí1
N )
so that
N (N ‚àí 1)
‚àí1/2
(D¬µ,1 ‚àí E¬µ ) = Op (N ‚àí1 bN )
N2
Also
E[cN,¬µ (Xj , Wj )2 ] ‚â§
"‚ÄûZ Z
‚Äû
¬´
¬´2 #
1
gW (w, x) d(x)
Wj ‚àí rbN (w)
(¬µ)
¬µ
E
X
K
(w
‚àí
r
w))
f
(w,
x)dwdx
‚â§
j
bN
WX
2+2|¬µ|
fW (w)
bN
bN
X W
"Z Z
#
‚Äû
¬´2
gW (w, x)2 d(x)2 2 (¬µ) Wj ‚àí rbN (w)
1
2¬µ
E
X
K
(w
‚àí
r
w))
f
(w,
x)dwdx
‚â§
bN
WX
j
2+2|¬µ|
bN
fW (w)2
bN
X W
‚Äû
¬´2
Z Z Z
C
wÃÉ ‚àí rbN (w)
(¬µ)
K
fW X (w, x)dwdxfW (wÃÉ)dwÃÉ = O(b‚àí1
N )
b2N W X W
bN

by a change of variables in the outer integral, so that
‚Äû
¬´
N (N ‚àí 1)
‚àí1/2
‚àí
1
E¬µ = Op (N ‚àí1 bN )
N2
Finally
E[|D¬µ,2|] ‚â§
so that

1
1p
E[|aN,¬µ (Wi , Xi , Xi , Wi )|] ‚â§
E[aN,¬µ (Wi , Xi , Xi , Wi )2 ] = O(N ‚àí1 b‚àí1
N )
N
N

D¬µ,2 = Op (N ‚àí1 b‚àí1
N )
Therefore if Œ¥ < 1/2 then
D¬µ = E¬µ + op(N ‚àí1/2 )

[51]

Under the same condition (B.32) is a linear combination of terms
F¬µ =

with

N
h
i‚Äù
1 X gW (Wi , Xi ) d(Wi )m(Wi ) ‚Äú (¬µ)
(¬µ)
hÃÇ2,N W (rb (Wi )) ‚àí E hÃÇ2,N W (rb (Wi )) (Wi ‚àí rb (Wi ))¬µ =
N i=1
fW (Wi )

N N
1 XX
eN,¬µ (Wi , Xi, Wj )
N i=1 j=1

eN,¬µ (Wi , Xi , Wj ) =

E

"

1
1+|¬µ|

bN

K

(¬µ)

‚Äû

gW (Wi , Xi ) d(Wi )m(Wi )
fW (Wi )

W ‚àí rbN (Wi )
bN

¬´#!

1

K (¬µ)
1+|¬µ|
bN

‚Äû

Wj ‚àí rbN (Wi )
bN

¬´

‚àí

(Wi ‚àí rbN (Wi ))¬µ

such that
F¬µ = G¬µ + op (N ‚àí1/2 )
with
G¬µ =

N
1 X
fN,¬µ (Wj )
N i=1

and
fN,¬µ (Wj ) =

1
1+|¬µ|
bN

¬∑

‚Äû
‚Äû
¬´
¬ª
‚Äû
¬´‚Äì¬´
Wj ‚àí rbN (w)
W ‚àí rbN (w)
gW (w, x) d(x)m(w)
(¬µ)
(¬µ)
K
‚àíE K
(w‚àírbN w))¬µ fW X (w, x)dwdx
f
(w)
b
b
W
N
N
X W
The final step is to show that
Z Z

E0 = ‚àí

N
N
1 X
1 X
cN,¬µ (Xj , Wj ) = ‚àí
(Œ∂j ‚àí E[Œ∂j ]) + op (N ‚àí1/2 )
N j=1
N j=1

with
Œ∂j = Xj E[gW (Wj , X)d(X)|Wj ]
where the expectation is over the conditional distribution of X given W , and
G0 =

N
N
1 X
1 X
cN,¬µ (Xj , Wj ) =
(Œæj ‚àí E[Œæj ]) + op (N ‚àí1/2 )
N j=1
N j=1

with
Œæj = m(Wj )E[gW (Wj , X)d(X)|Wj ]
and
E¬µ = op (N ‚àí1/2 )

G¬µ = op(N ‚àí1/2 )

for |¬µ| ‚â• 1. We only consider E0 and E¬µ . The proof for G0 and G¬µ is analogous. Define
‚Äû
¬´
Z Z
Wj ‚àí rbN (w)
1
gW (w, x) d(x)
(¬µ)
œàN,¬µ,j = 1+|¬µ|
Xj K
(w ‚àí rbN w))¬µ fW X (w, x)dwdx
f
(w)
b
W
N
bN
X W
so that cN,¬µ (Xj , Wj ) = œàN,¬µ,j ‚àí E[œàN,¬µ,j ]. Now
œàN,0,j = œàN,0,j,0 + œàN,0,j,1

[52]

with
1
bN

Z Z

1
=
bN

Z Z

œàN,0,i,0 =

X

wu ‚àíbN

wl +bN

gW (w, x) d(x)
Xj K
fW (w)

‚Äû

Wj ‚àí w
bN

¬´

fW X (w, x)dwdx

and
œàN,0,j,1
1
bN
so that

Z Z
X

E0 = ‚àí

wu

X

gW

wu ‚àíbn

‚Äû
¬´
gW (w, x) d(x)
Wj ‚àí wl
Xj K
fW X (w, x)dwdx+
fW (w)
bN
wl
‚Äû
¬´
(w, x) d(x)
Wj ‚àí wu
Xj K (¬µ)
fW X (w, x)dwdx
fW (w)
bN
wl +bN

N
N
1 X
1 X
(œàN,0,j,0 ‚àí E[œàN,0,j,0 ]) ‚àí
(œàN,0,j,1 ‚àí E[œàN,0,j,1 ])
N j=1
N j=1

Obviously
2

N
N
1 X
1 X
E4 ‚àí
(œàN,0,j,0 ‚àí E[œàN,0,j,0 ]) +
(Œ∂j ‚àí E[Œ∂j ])
N i=1
N j=1

!2 3
5 ‚â§ 1 E[(œàN,0,j,0 ‚àí Œ∂j )2 ]
N

By a change of variables to t = (Wj ‚àí w)/bN with Jacobian bN
Z Z 1
gW (Wj ‚àí bN t, x) d(x)
Xj K (t) fW X (Wj ‚àí bN t, x)dtdx
œàN,0,j,0 =
1 Wj ‚àíwu
Wj ‚àíwl
1+
‚â§t‚â§‚àí1+
fW (Wj ‚àí bN t)
bN
bN
X ‚àí1
so that
|œàN,0,j,0 ‚àí Œ∂i | ‚â§

Z Z
X

1

‚àí1

1

1+

Wj ‚àíwu
W ‚àíwl
‚â§t‚â§‚àí1+ jb
bN
N

Àõ
Àõ
Àõ gW (Wj ‚àí bN t, x) d(x)
Àõ
gW (Wj , x) d(x)
Àõ
Àõ |Xj ||K (t) |dtdx+
f
(W
‚àí
b
t,
x)
‚àí
f
(W
,
x)
W
X
j
N
W
X
j
Àõ
Àõ
fW (Wj ‚àí bN t)
fW (Wj )
Àõ
Àõ
Z 1 Àõ
Z Àõ
Àõ
Àõ
Àõ gW (Wj , x) d(x)
Àõ
Àõ dx|Xj |
Àõ1 Wj ‚àíwu
Àõ
Àõ
f
(W
,
x)
Wj ‚àíwl ‚àí 1 |K (t) |dt+
W
X
j
Àõ
Àõ 1+ b
Àõ
Àõ
‚â§t‚â§‚àí1+ b
fW (Wj )
X

‚àí1

N

N

By the mean value theorem the first term on the right hand side is bN |Xj |p(Wj ) with p(Wj ) a (generic) bounded
function of Wj . The second term on the right hand side is |Xj |p(Wj )(1 ‚àí Pr(wl + 2bN ‚â§ Wj ‚â§ wu ‚àí 2bN )).
Therefore
|œàN,0,j,0 ‚àí Œ∂i | ‚â§ |Xj |p(Wj )(bN + (1 ‚àí Pr(wl + 2bN ‚â§ Wj ‚â§ wu ‚àí 2bN )))
so that
E[(œàN,0,j,0 ‚àí Œ∂j )2 ] = O(bN )
and
Àõ
Àõ
N
N
Àõ
Àõ
1 X
Àõ 1 X
Àõ
(œàN,0,j,0 ‚àí E[œàN,0,j,0 ]) +
(Œ∂j ‚àí E[Œ∂j ])Àõ = op (N ‚àí1/2 )
Àõ‚àí
Àõ N
Àõ
N j=1
i=1

if Œ¥ < 21 . For œàN,0,j,1 we consider the first term on the right hand side
Àõ ‚Äû
Àõ
¬´
Z wl +bN Z
Àõ
Àõ
gW (w, x) d(x)
ÀõK Wj ‚àí wl Xj 1
fW X (w, x)dxdw ÀõÀõ ‚â§ C|Xj |1wl ‚â§Wj ‚â§wl +bN
Àõ
bN
bN wl
f
(w)
W
X

For the other term on the right hand side we get a similar bound and we conclude
2
E[œàN,0,j,1
] = O(bN )

[53]

so that if Œ¥ < 12
Àõ
Àõ
N
Àõ X
Àõ
Àõ1
Àõ
(œàN,0,j,1 ‚àí E[œàN,0,j,1 ])Àõ = op (N ‚àí1/2 )
Àõ
ÀõN
Àõ
j=1

Finally if ¬µ ‚â• 1
œàN,¬µ,j =

1
1+|¬µ|

bN
Z Z wu

1
1+|¬µ|

bN

X

Z Z

wu ‚àíbN

X

wl +bN
wl

gW (w, x) d(x)
Xj K (¬µ)
fW (w)

gW (w, x) d(x)
Xj K (¬µ)
fW (w)

‚Äû

‚Äû

Wj ‚àí wu
bN

Wj ‚àí wl
bN

¬´

¬´

(w ‚àí wl )¬µ fW X (w, x)dwdx+

(w ‚àí wu )¬µ fW X (w, x)dwdx

The first term on the right hand side is bounded by
Àõ
Àõ
‚Äû
¬´Àõ
Z Z wl +bN Àõ
Àõ (¬µ) Wj ‚àí wl Àõ
Àõ gW (w, x) d(x)
Àõ
ÀõK
Àõ |Xj | 1
Àõ
Àõ dwdx ‚â§ C|Xj |1w ‚â§W ‚â§w +b
f
(w,
x)
W
X
j
l
l
N
Àõ
Àõ
Àõ
Àõ
bN
bN X wl
fW (w)
so that

2
E[œàN,¬µ,j
] = O(bN )

and therefore
E¬µ = ‚àí

N
X
(œàN,¬µ,j ‚àí E[œàN,¬µ,j ]) = op (N ‚àí1/2 )
j=1

if Œ¥ < 12 . 
Proof of Theorem A.3: Because the class of ‚Äòdoubly averaged‚Äô estimators has not been considered previously,
we provide a somewhat detailed proof. The proof consists of four steps. In the first we approximate the estimator
by a linear function of the kernel estimator hÃÇnip,s (linearization). Formally, we show that
V =

‚Äû
N X
N
Àõ2 ¬´
‚Äú
‚Äù
X
‚àö ÀõÀõ
‚àÇn
Àõ
(h(Z
,
Z
))
hÃÇ
(Z
,
Z
)
‚àí
h(Z
,
Z
)
+
O
N
hÃÇ
‚àí
h
Àõ
Àõ .
1j
2k
nip,s
1j
2k
1j
2k
p
nip,s
N N j=1 k=1 ‚àÇh0
1
‚àö

(B.33)

By the assumptions, and Lemma A.11, the remainder term is op (1).
In the second step we express the difference between the linearized estimator and the estimand as the sum of a
bias term (that is asymptotically negligible) and a variance term (bias-variance decomposition). The bias term
will be shown to satisfy
Àõ
Àõ
N
N
Àõ
Àõ
‚Äú‚àö
‚Äù
Àõ 1 X X ‚àÇn
Àõ
(h(Z1j , Z2k ))(E[hÃÇnip,s (Z1j , Z2k )] ‚àí h(Z1j , Z2k ))Àõ = O
N bpN .
(B.34)
Àõ ‚àö
0
ÀõN N
Àõ
‚àÇh
j=1
k=1

By the assumption on the bandwidth rate, the remainder term is o(1). Note that by E[hÃÇ(Zi1 , Zi2 ] we mean the
expectation of hÃÇ(z1 , z2 ), evaluated at z1 = Z1i and z2 = Z2j : the expectation is taken over the estimator of the
function h(¬∑).
The second step leaves us with
‚Äû
Àõ2 ¬´
‚àö ÀõÀõ
‚àö
Àõ
V = W + Op
N ÀõhÃÇnip,s ‚àí hÀõ + O( N bpN ),
where

W =

N
N X
‚Äú
h
i‚Äù
X
‚àÇn
(h(Z1j , Z2k )) hÃÇ(Z1j , Z2k ) ‚àí E hÃÇnip,s (Z1j , Z2k ) .
0
N N j=1 k=1 ‚àÇh

1
‚àö

Define
ŒΩ(z1 , z2 ) =

‚àÇn
(h(z1 , z2 )),
‚àÇh0

[54]

(B.35)

and
aN,¬µ (YÃÉi , Zi , Z1j , Z2k )
¬´
‚Äû
Zi ‚àí rbN (Z1j , Z2k )
(¬µ)
YÃÉ
K
i
L+|¬µ|
bN
bN
"
‚Äû
¬´#! ‚Äû‚Äû
¬´
¬´¬µ
Z ‚àí rbN (Z1j , Z2k )
1
Z1j
(¬µ)
‚àíEYÃÉ Z L+|¬µ| YÃÉ K
¬∑
‚àí rbN (Z1j , Z2k )
Z2k
bN
b
1

= ŒΩ(Z1j , Z2k )0 ¬∑

N

so that
W =

X

1
W¬µ ,
¬µ!

¬µ:|¬µ|‚â§s‚àí1

(B.36)

where
W¬µ =

1
‚àö

N2 N

N X
N X
N
X

aN,¬µ (YÃÉi , Zi , Z1j , Z2k )

i=1 j=1 k=1

Define
cN,¬µ (YÃÉi , Zi ) =

1
L+|¬µ|

bN

Z

Z2

¬´
¬ª
‚Äû
¬´‚Äì¬´
‚Äû
‚Äû
Zi ‚àí rbN (z1 , z2 )
Z ‚àí rbN (z1 , z2 )
ŒΩ(z1 , z2 )0 YÃÉi K (¬µ)
‚àí EYÃÉ Z YÃÉ K (¬µ)
bN
bN
Z1

Z

√ó ((z10 z20 )0 ‚àí rbN (z1 , z2 ))¬µ fZ1 (z1 )fZ2 (z2 )dz1 dz2
and
N
1 X
U¬µ = ‚àö
cN,¬µ (YÃÉi , Zi ),
N i=1

or, equivalently
U¬µ =
‚àö Z Z
N
Z2

(¬µ)

Z1

(¬µ)

ŒΩ(z1 , z2 )0 (hÃÇN W (rbN (z1 , z2 )) ‚àí E[hÃÇN W (rbN (z1 , z2 ))])((z10 z20 )0 ‚àí rbN (z1 , z2 ))¬µ fZ1 (z1 )fZ2 (z2 )dz1 dz2 .

In the third step we show that
‚Äú
‚Äù
W¬µ = U¬µ + Op N ‚àí1/2 b‚àíL
N

(B.37)

In the fourth step we show that

¬ª
‚Äìff
N Ôöæ
‚àÇn
1 X ‚àÇn
0
0
U0 = ‚àö
(h(Zi )) YÃÉi fZ1 (Z1i )fZ2 (Z2i ) ‚àí EZ
(h(Z)) YÃÉ fZ1 (Z1i )fZ2 (Z2i )
+ op (1).
‚àÇh
N i=1 ‚àÇh

(B.38)

which gives us the representation in the Theorem.
In the fifth and final step we show that we can ignore U¬µ for ¬µ such that |¬µ| ‚â• 1, because for such ¬µ,
U¬µ = Op (bN ) .

(B.39)

Proving these statements implies the result in the Theorem.
Now we turn to proving each of the statements (B.33), (B.34), (B.37), (B.38), and (B.39).

Step 1: Linearization In the first step of the proof we prove equality (B.33). First define
d(z1 , z2 ) ‚â° n(hÃÇnip,s (z1 , z2 )) ‚àí n(h(z1 , z2 )) ‚àí

‚àÇn
(h(z1 , z2 ))(hÃÇnip,s (z1 , z2 ) ‚àí h(z1 , z2 )).
‚àÇh0

By a second order Taylor series expansion of n(hÃÇnip,s (Z1i , Z2j )) around h(Z1i , Z2j ) we have,
Àõ
Àõ
Àõ
1Àõ
‚àÇ2 n
Àõ
|d(z1 , z2 )| = ÀõÀõ(hÃÇnip,s (z1 , z2 ) ‚àí h(z1 , z2 ))0
(h(z
,
z
))(
hÃÇ
(z
,
z
)
‚àí
h(z
,
z
))
1
2
nip,s
1
2
1
2
Àõ
2
‚àÇh‚àÇh0

[55]

Àõ 2
Àõ Àõ
Àõ2
Àõ ‚àÇ n
Àõ Àõ
Àõ
Àõ
‚â§ sup Àõ
(h(z))ÀõÀõ ¬∑ ÀõhÃÇnip,s (z1 , z2 ) ‚àí h(z1 , z2 )Àõ
0
‚àÇh‚àÇh
z
Àõ
Àõ2
Àõ
Àõ
‚â§ C ¬∑ ÀõhÃÇnip,s ‚àí hÀõ ,

with h(z1 , z2 ) intermediate between hÃÇnip,s (z1 , z2 ), and h(z1 , z2 ) so that
Àõ
Àõ
‚ÄûÀõ
N
N
Àõ
Àõ
Àõ2
Àõ2 ¬´
Àõ
Àõ
Àõ
Àõ 1 XX
Àõ
Àõ
Àõ
d(Z1j , Z2k )Àõ ‚â§ C ÀõhÃÇnip,s ‚àí hÀõ = Op ÀõhÃÇnip,s ‚àí hÀõ .
Àõ 2
Àõ
ÀõN
j=1
k=1

Hence

1
‚àö

N N

N X
N h ‚Äú
‚Äù
i
X
n hÃÇnip,s (Z1j , Z2k ) ‚àí n (h(Z1j , Z2k ))
j=1 k=1

=

N
N
‚Äú
‚Äù
1 X X ‚àÇn
‚àö
(h(Z1j , Z2k )) hÃÇnip,s (Z1j , Z2k ) ‚àí h(Z1j , Z2k )
0
N N j=1 k=1 ‚àÇh

+

1
‚àö

N N

N X
N
X

d(Z1j , Z2k )

j=1 k=1

‚Äû
N
N
Àõ2 ¬´
‚Äú
‚Äù
‚àö ÀõÀõ
1 X X ‚àÇn
Àõ
‚àö
(h(Z
,
Z
))
hÃÇ
(Z
,
Z
)
‚àí
h(Z
,
Z
)
+
O
N
hÃÇ
‚àí
h
Àõ
Àõ
1j
2k
nip,s
1j
2k
1j
2k
p
nip,s
N N j=1 k=1 ‚àÇh0
Àõ2
‚àö ÀõÀõ
Àõ
so that the linearization remainder has the same stochastic order as N ÀõhÃÇnip,s ‚àí h0 Àõ .
=

Step 2: Bias-variance decomposition In the second step of the proof we proof equation (B.34).
Define
E=

N
N
‚Äú
‚Äù
1 X X ‚àÇn
‚àö
(h(Z1j , Z2k )) hÃÇnip,s (Z1j , Z2k ) ‚àí h(Z1j , Z2k ) .
0
N N j=1 k=1 ‚àÇh

so that
V = E + Op

‚Äû

‚àö

Àõ
Àõ2 ¬´
Àõ
Àõ
N ÀõhÃÇnip,s ‚àí hÀõ .

We decompose E into a bias and variance part, E = EUbias + W , where
Ebias =

N
N
‚Äú h
i
‚Äù
1 X X ‚àÇn
‚àö
(h(Z1j , Z2k )) E hÃÇnip,s (Z1j , Z2k ) ‚àí h(Z1j , Z2k ) ,
0
N N j=1 k=1 ‚àÇh

and W is defined in (B.35). The bias part is bounded by
Àõ
Àõ
N
N
Àõ
‚Äú h
i
‚ÄùÀõ
Àõ 1 X X ‚àÇn
Àõ
(h(Z1j , Z2k )) E hÃÇnip,s (Z1j , Z2k ) ‚àí h(Z1j , Z2k ) Àõ
Àõ ‚àö
0
ÀõN N
Àõ
‚àÇh
j=1 k=1
Àõ
Àõ
Àõ
‚àö
Àõ ‚àÇn
Àõ ‚àö ÀõÀõ
Àõ
‚â§ sup ÀõÀõ (h(z))ÀõÀõ N ÀõE[hÃÇnip,s ] ‚àí hÀõ = O( N bpN ),
‚àÇh
z‚ààZ

due to smoothness of the function and Lemma A.9.

Step 3: Projection In the third step of the proof we prove equation (B.37), W¬µ = U¬µ + Op (N ‚àí1/2 b‚àíL
N ).
This is the most complicated step. First, note that
W¬µ =

1
‚àö

N2 N

N X
N X
N
X

aN,¬µ (YÃÉi , Zi , Z1j , Z2k ),

i=1 j=1 k=1

[56]

is a third order V-statistic with kernel (that depends on N ) aN,¬µ . We show that this V-statistic is asymptotically
equivalent to a projection that is a single sum. Because the kernel depends on N we cannot use a standard
result.
The projection of W¬µ is

with

N
1 X
U¬µ = ‚àö
cN,¬µ (YÃÉi , Zi ),
N i=1

cN,¬µ (YÃÉi , Zi )
=

1
L+|¬µ|

bN

Z

Z2

Z

¬´
¬ª
‚Äû
¬´‚Äì¬´
‚Äû
‚Äû
Z ‚àí rbN (z1 , z2 )
Zi ‚àí rbN (z1 , z2 )
‚àí EYÃÉ Z YÃÉ K (¬µ)
ŒΩ(z1 , z2 )0 YÃÉi K (¬µ)
bN
bN
Z1

√ó((z10 z20 )0 ‚àí rbN (z1 , z2 ))¬µ fZ1 (z1 )fZ2 (z2 )dz1 dz2 .
The projection remainder is
‚Äû
¬´
N (N ‚àí 1)(N ‚àí 2)
N (N ‚àí 1)(N ‚àí 2)
W¬µ ‚àí U¬µ =
(W
‚àí
U
)
+
‚àí
1
U¬µ + W¬µ,2 + W¬µ,3 + W¬µ,4 + W¬µ,5 (B.40)
¬µ,1
¬µ
N3
N3

with

W¬µ,1 ‚â°
W¬µ,2 ‚â°
W¬µ,3 ‚â°
W¬µ,4 ‚â°
W¬µ,5 ‚â°

‚àö

X
N
aN,¬µ (YÃÉi , Zi , Z1j , Z2k )
N (N ‚àí 1)(N ‚àí 2)
i6=j6=k
‚àö
N X
aN,¬µ (YÃÉi , Zi , Z1i , Z2k )
N3
i=j6=k
‚àö
N X
aN,¬µ (YÃÉi , Zi , Z1j , Z2i )
N3
i=k6=j
‚àö
N X
aN,¬µ (YÃÉi , Zi , Z1j , Z2j )
N3
i6=j=k
‚àö
N X
aN,¬µ (YÃÉi , Zi , Z1i , Z2k )
N 3 i=j=k

We prove that the projection remainder W¬µ ‚àí U¬µ = Op (N ‚àí1/2 b‚àíL
N ) by proving the following six equalities:
‚Äú
‚Äù
‚àíL/2
W¬µ,1 ‚àí U¬µ = Op N ‚àí1 bN
,
(B.41)
¬´
‚Äû
‚Äú
‚Äù
N (N ‚àí 1)(N ‚àí 2)
‚àíL/2
‚àí 1 U¬µ = Op N ‚àí1 bN
,
(B.42)
N3
‚Äú
‚Äù
‚àíL+L2 /2
W¬µ,2 = Op N ‚àí1/2 bN
,
(B.43)
‚Äú
‚Äù
‚àíL+L1 /2
W¬µ,3 = Op N ‚àí1/2 bN
,
(B.44)
‚Äú
‚Äù
‚àíL/2
W¬µ,4 = Op N ‚àí3 bN
,
(B.45)
‚Äú
‚Äù
‚àí1/2 ‚àíL
W¬µ,5 = Op N
bN .
(B.46)

In order to prove these results, we establish bounds on the second moment of aN,¬µ (YÃÉi , Zi , Z1j , Z2k ). This will
be relatively straightforward if i 6= j and i 6= k. The derivation of the bound is more involved if i = j and/or
i = k. We could simplify the proof by omitting these observations and redefining the estimator by restricting
the averaging to observations with i 6= j and i 6= k. This would amount to redefining the kernel estimator in
(A.7) by omitting observations i = j and i = k in hÃÇnip,s . We will keep these observations and derive bounds on
all second moments. We derive the following bounds, considering four separate cases (note that the bounds do
not depend on ¬µ)
E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )2 ] = O(b‚àíL
N )
2

E[aN,¬µ (YÃÉi , Zi , Z1i , Z2i ) ] = O(b‚àí2L
)
N
‚àí2L+L2
E[aN,¬µ (YÃÉi , Zi , Z1i , Z2k )2 ] = O(bN
)
‚àí2L+L1
E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )2 ] = O(bN
)

j 6= i, and k 6= i,

i = j = k,
k 6= i = j,
j 6= i = k.

[57]

(B.47)
(B.48)
(B.49)
(B.50)

Step 3A: Equation (B.47) First if j 6= i and k 6= i
E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )2 ]
#
"
¬´2
‚Äû
Zi ‚àí rbN (Z1j , Z2k )
1
0
0 0
‚â§ 2L+2|¬µ| E K (¬µ)
ŒΩ(Z1j , Z2k )0 YÃÉi YÃÉi0 ŒΩ(Z1j , Z2k )((Z1j
Z2k
) ‚àí rbN (Z1j , Z2k ))2¬µ
bN
bN
#
"
¬´2
‚Äû
supz‚ààZ |z ‚àí rbN (z)|2¬µ
Zi ‚àí rbN (Z1j , Z2k )
0
0
(¬µ)
‚â§
ŒΩ(Z1j , Z2k ) YÃÉi YÃÉi ŒΩ(Z1j , Z2k )
E K
2L+2|¬µ|
bN
bN
"
#
‚Äû
¬´2
Zi ‚àí rbN (Z1j , Z2k )
1
0
0
(¬µ)
‚â§ 2L E K
ŒΩ(Z1j , Z2k ) YÃÉi YÃÉi ŒΩ(Z1j , Z2k )
bN
bN
Now by the Cauchy-Schwartz inequality
"
#
‚Äû
¬´2
Zi ‚àí rbN (Z1j , Z2k )
E K (¬µ)
ŒΩ(Z1j , Z2k )0 YÃÉi YÃÉi0 ŒΩ(Z1j , Z2k )
bN
"
#
‚Äû
¬´2
Zi ‚àí rbN (Z1j , Z2k )
(¬µ)
0
2
(ŒΩ(Z1j , Z2k ) YÃÉi )
=E K
bN
#
"
¬´2
‚Äû
Zi ‚àí rbN (Z1j , Z2k )
(¬µ)
2
2
‚â§E K
|ŒΩ(Z1j , Z2k | |YÃÉi |
bN
"
#
‚Äû
¬´2
h
i
Zi ‚àí rbN (Z1j , Z2k )
(¬µ)
2
2
=E K
|ŒΩ(Z1j , Z2k | E |YÃÉi | |Zi
bN
h
i
By Assumption 3.1 E |YÃÉ |2 |Z = z an ŒΩ are bounded on Z so that this is bounded by (condition on Z1j and Z2k )
"

CE K

(¬µ)

‚Äû

Zi ‚àí rbN (Z1 , Z2 )
bN

¬´2 #

=C

Z

Z

K (¬µ)

‚Äû

z ‚àí rbN (Z1 , Z2 )
bN

¬´2

fZ (z)dz

and by a change of variables to t = (z ‚àí rbN (Z1 , Z2 ))/bN with Jacobian bL
N we obtain
Z
Z
CbL
K (¬µ) (t)2 fZ (bN t + rbN (Z1 , Z2 ))dt ‚â§ C1 bL
K (¬µ) (t)2 dt ‚â§ C2 bL
N
N
N
{t|t=(z‚àírb

N

(Z1 ,Z2 ))/bN ,z‚ààZ}

U

by Assumptions 3.1 and 4.1. We conclude
E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )2 ] = O(b‚àíL
N )

(B.51)

The same proof and the same bound holds if j 6= k 6= i or j = k 6= i .

Step 3B: Equation (B.48) Next, we consider E[aN,¬µ (YÃÉi , Zi , Z1i , Z2i )2 ] where we note that E[aN,¬µ (YÃÉi , Zi , Z1i , Z2i )] 6=
0. Because

aN,¬µ (YÃÉi , Zi , Z1i , Z2i )
¬ª
‚Äû
¬´
‚Äû
‚Äû
¬´
¬´‚Äì
Zi ‚àí rbN (Zi )
Z ‚àí rbN (Zi )
1
= L+|¬µ| ŒΩ(Zi )0 YÃÉi K (¬µ)
(Zi ‚àí rbN (Zi ))¬µ ‚àí EZ ŒΩ(Zi )0(¬µ)
(Zi ‚àí rbN (Zi ))¬µ
bN
bN
bN
we have
E[aN,¬µ (YÃÉi , Zi , Z1i , Z2i )2 ]
"
#
‚Äû
¬´2
Zi ‚àí rbN (Zi )
1
(¬µ)
0
2
2¬µ
= 2L+2|¬µ| E K
(ŒΩ(Zi ) YÃÉi ) (Zi ‚àí rbN (Zi ))
bN
bN
¬ªÔöæ
‚Äû
¬´
Zi ‚àí rbN (Zi )
2
‚àí 2L+2|¬µ| EZi
ŒΩ(Zi )0 g(Zi )K (¬µ)
bN
bN
¬ª
‚Äû
¬´‚Äì
ff‚Äì
Z ‚àí rbN (Zi )
‚àí EZ ŒΩ(Z)g(Z)K (¬µ)
(Zi ‚àí rbN (Zi ))2¬µ
bN

[58]

(B.52)
(B.53)
(B.54)

"‚Äû
#
‚Äû
‚Äû
¬´¬´¬´2
Z ‚àí rbN (Zi )
0
(¬µ)
2¬µ
ŒΩ(Zi ) EZ g(Z)K
+ 2L+2|¬µ| EZi
(Zi ‚àí rbN (Zi ))
bN
bN
1

(B.55)

By Assumption 3.1 and smoothness (B.53) is bounded by
‚Äû
¬´2
‚Äû
¬´2
Z
Z
C
z ‚àí rbN (z)
C
z ‚àí rbN (z)
(¬µ)
(¬µ)
K
f
(z)dz
=
K
fZ (z)dz
Z
bN
bN
b2L
b2L
Z
ZI
N
N
bN
‚Äû
¬´2
Z
z ‚àí rbN (z)
C
(¬µ)
K
+ 2L
fZ (z)dz
bN
bN Z\ZI
bN

If rb (z) is the projection on the internal set, then z ‚àí rb (z) = 0 if z is in the internal set. Therefore
‚Äû
¬´2
Z
z ‚àí rbN (z)
C
CK (¬µ)(0)2
(¬µ)
K
fZ (z)dz ‚â§
2L
bN
bN ZI
b2L
N
bN

I
Next we consider the second integral. If s ‚àà ZB
bN ‚â° Z \ ZbN , then at least one component of z is in the boundary
B
B
region. We can subdivide ZbN into disjoint subsets ZbN ,p , p = 1, . . . , 2L ‚àí 1 and in each such subset Lp ‚â• 1
B
components of z are within bN from the boundary. We further partition ZB
bN ,p into disjoint sets ZbN ,p,r , r =
Lp
1, . . . 2 with 0 ‚â§ Kr ‚â§ Lp components with zll ‚â§ Zl ‚â§ zll + bN and the remaining Lp ‚àí Kr components with
zul ‚àí bN ‚â§ Zl ‚â§ zul . Without loss of generality we assume that the first Kr components of z are near the lower
bound, the next Lp ‚àí Kr are near the upper bound and the rest is in the internal region, so that

C
b2L
N
=

Z

Z

K (¬µ)
ZB
bN ,p,r

zl1 +bN
zl1

¬∑¬∑¬∑

Z

‚Äû

z ‚àí rbN (z)
bN

zl,Kr +bN

zl,Kr

Z

¬´2

fZ (z)dz

zu,Kr +1
zu,Kr +1 ‚àíbN

¬∑¬∑¬∑

Z

zu,Lp
zu,Lp ‚àíbN

Z

zu,Lp+1 ‚àíbN

zl,Lp +1 +bN

Lp

√ó

Y

(¬µl )

l=Kr +1

Kl

‚Äû

¬∑¬∑¬∑

Z

Zl ‚àí zul
bN

Kr
zuL ‚àíbN Y

zlL +bN

¬´2
+1

l=1

(¬µl )

Kl

L
Y

l=Lp +1

‚Äû

(¬µl )

Kl

¬´2
Zl ‚àí zll
‚àí1
bN
(0)2 fZ (z)dz

(¬µ )

Because the support of the kernel is [‚àí1, 1] and by Assumption 4.1 Kl l is bounded on this support we have
‚Äû
¬´
‚Äû
¬´
Zl ‚àí zll
zl ‚àí zul
(¬µl )
(¬µl )
Kl
‚àí 1 ‚â§ C ¬∑ 1 (zll ‚â§ zl ‚â§ zll + 2bN )
Kl
+ 1 ‚â§ C ¬∑ 1 (zul ‚àí 2bN ‚â§ zl ‚â§ zul )
bN
bN
and substitution gives the upper bound
C1
b2L
N

Z

zl1 +bN

¬∑¬∑¬∑

zl1

Z

zlKr +bN

zlKr

Z

zu,Kr +1

zu,Kr +1 ‚àíbN

√ó

¬∑¬∑¬∑

Lp
Y

l=Kr +1

Z

zu,Lp

Kr
Y

zu,Lp ‚àíbN l=1

1 (zll ‚â§ zl ‚â§ zll + 2bN )

1 (zul ‚àí 2bN ‚â§ zl ‚â§ zul ) fZ (z1 , . . . zLp )dz1 . . . dzLp ‚â§

C2
2L‚àíLp
bN

Because Lp ‚â• 1 the integral over the boundary region is O(b‚àí2L+1
). Combining the results we have that
N
E[aN,¬µ (YÃÉi , Zi , Z1i , Z2i )2 ] = O(b‚àí2L
)
N

(B.56)

which is larger than the bound in (B.51) and could be a reason to omit the terms i = j = k (and redefine the
kernel estimator).

Step 3C: Equation (B.49) Third, we consider E[aN,¬µ (YÃÉi , Zi , Z1i , Z2k )2 ]. Again we have E[aN,¬µ (YÃÉi , Zi , Z1i , Z2k )] 6=
0. We have

E[aN,¬µ (YÃÉi , Zi , Z1i , Z2k )2 ]

=

1
2L+2|¬µ|

bN

"

E K

(¬µ)

‚Äû

Zi ‚àí rbN (Z1i , Z2k )
bN

¬´2

ŒΩ(Z1i , Z2k )

[59]

0

0
YÃÉi YÃÉi0 ŒΩ(Z1i , Z2k )((Z1i

0
Z2k
)0

‚àí rbN (Z1i , Z2k ))

2¬µ

#

(B.57)
¬´
¬ª
‚Äû
Zi ‚àí rbN (Z1i , Z2k )
2
‚àí 2L+2|¬µ| EZi Z2k ŒΩ(Z1i , Z2k )0 g(Z1i , Z2k )K (¬µ)
bN
bN
‚Äû
‚Äû
¬´¬´
‚Äì
Z ‚àí rbN (Z1i , Z2k )
0
0 0
√óEZ g(Z)0(¬µ)
((Z1i
Z2k
) ‚àí rbN (Z1i , Z2k ))2¬µ
bN
1

E
2L+2|¬µ| Zi Z2k
bN

+

"‚Äû

ŒΩ(Z1i , Z2k )0 EZ

(B.58)

#
¬´¬´¬´2
‚Äû
‚Äû
Z ‚àí rbN (Z1i , Z2k )
0
0 0
√ó ((Z1i
Z2k
) ‚àí rbN (Z1i , Z2k ))2¬µ
g(Z)K (¬µ)
bN
(B.59)

By Assumptions 3.1 and smoothness (B.57) is bounded by
¬´2
‚Äû
Z Z
C
z ‚àí rbN (z1 , zÃÉ2 )
fZ (z)fZ2 (zÃÉ2 )dzdzÃÉ2
K (¬µ)
2L
bN
bN Z2 Z
‚Äû
¬´2
Z Z
Z
z ‚àí rbN (z1 , zÃÉ2 )
C
(¬µ)
= 2L
K
fZ (z1 , z2 )fZ2 (zÃÉ2 )dz2 dz1 dzÃÉ2
bN
bN Z2 ZI
Z2
bN ,1
‚Äû
¬´2
Z Z
Z
C
z ‚àí rbN (z1 , zÃÉ2 )
+ 2L
fZ (z1 , z2 )fZ2 (zÃÉ2 )dz2 dz1 dzÃÉ2
K (¬µ)
bN
bN Z2 Z1 \ZI
Z2
bN ,1

Because z1 ‚àí rb (z1 , zÃÉ2 ) = 0 if z1 ‚àà ZIbN ,1 , the first term on the right hand side is equal to
‚Äû
¬´2
Z Z
Z
(¬µ )
z2 ‚àí rbN (z1 , zÃÉ2 )
CK1 1 (0)
‚àí2L+L2
(¬µ2 )
fZ (z1 , z2 )fZ2 (zÃÉ2 )dz2 dz1 dzÃÉ2 = O(bN
),
K
bN
b2L
Z2 ZI
Z2
N
bN ,1

2
(where K(u) is the univariate kernel), by a change of variables to t2 = (z2 ‚àí rbN (z1 , zÃÉ2 )/bN with Jacobian bL
N .
B
I
B
L1
For the second integral we partition Z1,bN ‚â° Z1 \ ZbN ,1 into sets Z1,bN ,p , p = 1, . . . , 2 ‚àí 1 in which 1 ‚â§ L1p ‚â§ L1
B
L1p
components of z1 are in the boundary region. Each ZB
1,bN ,p is partitioned further into sets Z1,bN ,p,r , r = 1, . . . , 2
in which 0 ‚â§ K1r ‚â§ L1r components of z1 are near the lower, L1r ‚àí K1r are near the upper boundary, and the
remaining L1 ‚àí L1p components are in the internal set. Hence, if we without loss of generality assume that the
first Kr components of z1 are near the lower boundary, the next L1p ‚àí K1r are near the upper boundary, and
the remaining components are in the internal set,

C
b2L
N
=

¬∑¬∑¬∑

Z

Z2

Z

Z

Z1 \ZI
b

C
b2L
Z2
N
Z zu1,L

Z

N ,1

zu1,L1p ‚àíbN

l=K1r +1

¬∑¬∑¬∑

zl,11

L1p

√ó

Z2

(¬µ )
Kl l

Z

‚Äû

¬´2
z ‚àí rbN (z1 , zÃÉ2 )
fZ (z1 , z2 )fZ2 (zÃÉ2 )dz2 dz1 dzÃÉ2
bN
Z zl1,K +bN Z zu1,K +1

K (¬µ)

zl,l1 +bN

1p

Y

Z

1r

zu1,L1p+1 ‚àíbN
zl1,L1p +1 +bN

‚Äû

1r

zl1,K1r

z1l ‚àí zu1l
bN

¬∑¬∑¬∑

¬´2
+1

zu1,K1r +1 ‚àíbN

Z

zu1,L1 ‚àíbN

zl1,L1 +bN
L1
Y

l=L1p +1

Z

(¬µ )
Kl l

K
1r
Y

Z2 l=1

(0)

2

(¬µl )

Kl

(¬µ )
K2 2

‚Äû

‚Äû

¬´2
z1l ‚àí zl1l
‚àí1
bN

z2 ‚àí rbN (z1 , zÃÉ2 )
bN

¬´2

fZ (z1 , z2 )fZ2 (zÃÉ2 )dz2 dz1 dzÃÉ2

2
After a change of variables to t2 = (z2 ‚àí rbN (z1 , zÃÉ2 )/bN with Jacobian bL
N we have by analogous argument as
‚àí2L+L2 +L1p
above that this term is O(bN
). Because L1p ‚â• 1 we have by combining the results

‚àí2L+L2
E[aN,¬µ (YÃÉi , Zi , Z1i , Z2k )2 ] = O(bN
)

(B.60)

Step 3D: Equation (B.50) An analogous argument gives
‚àí2L+L1
E[aN,¬µ (YÃÉi , Zi , Z1j , Z2i )2 ] = O(bN
)

(B.61)

This finishes the derivation of the bounds on the second moments of the kernel of the V-statistic.
Now we turn to the proofs of equalities (B.41)-(B.46).

[60]

Step 3E: Equation (B.41) For the first term
W¬µ,1 ‚àí U¬µ =

‚àö

X
N
(aN,¬µ (YÃÉi , Zi , Z1j , Z2k ) ‚àí cN,¬µ (YÃÉi , Zi ))
N (N ‚àí 1)(N ‚àí 2)

(B.62)

i6=j6=k

so that
E[(W¬µ,1 ‚àí U¬µ )2 ]
X
N
= 2
N (N ‚àí 1)2 (N ‚àí 2)2 i6=j6=k

X

i0 6=j 0 6=k0

E[(aN,¬µ (YÃÉi , Zi , Z1j , Z2k ) ‚àí cN,¬µ (YÃÉi , Zi ))

√ó (aN,¬µ (YÃÉi0 , Zi0 , Z1j 0 , Z2k0 ) ‚àí cN,¬µ (YÃÉi0 , Zi0 ))]
This expression can be simplified using
E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )] = 0

(B.63)

E[cN,¬µ (YÃÉi , Zi )] = 0

(B.64)

E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )|YÃÉi , Zi ] = cN,¬µ (YÃÉi , Zi )

(B.65)

E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )aN,¬µ (Vi0 , Zi0 , Z1j 0 , Z2k )|Z2k ] = 0

(B.66)

E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )aN,¬µ (Vi0 , Zi0 , Z1j , Z2k0 )|Z1j ] = 0

(B.67)

E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )aN,¬µ (Vi0 , Zi0 , Z1j , Z2k0 )|Z1j , Z2k ] = 0

(B.68)

Therefore
E[(aN,¬µ (YÃÉi , Zi , Z1j , Z2k ) ‚àí cN,¬µ (YÃÉi , Zi ))(aN,¬µ (Vi0 , Zi0 , Z1j 0 , Z2k0 ) ‚àí cN,¬µ (Vi0 , Zi0 ))] = 0
if i 6= i0 , j 6= j 0 , k 6= k0 by (B.63) and (B.64), if i = i0 , j 6= j 0 , k 6= k0 by (B.65), if i 6= i0 , j 6= j 0 , k = k0 by (B.66),
and if i 6= i0 , j = j 0 , k 6= k0 by (B.67), and if i =
6 i0 , j = j 0 , k = k0 by (B.68). Using this we obtain
E[(W¬µ,1 ‚àí U¬µ )2 ]
X
N
= 2
E[(aN,¬µ (YÃÉi , Zi , Z1j , Z2k ) ‚àí cN,¬µ (YÃÉi , Zi ))2 ]
2
2
N (N ‚àí 1) (N ‚àí 2) i6=j6=k
+
+

X

N
N 2 (N ‚àí 1)2 (N ‚àí 2)2

i6=j6=k6=k0

N
‚àí 1)2 (N ‚àí 2)2

i6=k6=j6=j 0

N 2 (N

X

(B.69)

E[(aN,¬µ (YÃÉi , Zi , Z1j , Z2k ) ‚àí cN,¬µ (YÃÉi , Zi ))(aN,¬µ (YÃÉi , Zi , Z1j , Z2k0 ) ‚àí cN,¬µ (YÃÉi , Zi ))]
E[(aN,¬µ (YÃÉi , Zi , Z1j , Z2k ) ‚àí cN,¬µ (YÃÉi , Zi ))(aN,¬µ (YÃÉi , Zi , Z1j 0 , Z2k ) ‚àí cN,¬µ (YÃÉi , Zi ))]

Because E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )|YÃÉi , Zi ] = cN,¬µ (YÃÉi , Zi ) we have E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )cN,¬µ (YÃÉi , Zi )] = E[cN,¬µ (YÃÉi , Zi )2 ]
so that by the bounds on the second moment of aN,¬µ (YÃÉi , Zi , Z1j , Z2k ), given in (B.47)-(B.50),
E[(aN,¬µ (YÃÉi , Zi , Z1j , Z2k ) ‚àí cN,¬µ (YÃÉi , Zi ))2 ] = E[(aN,¬µ (YÃÉi , Zi , Z1j , Z2k )2 ] ‚àí E[cN,¬µ (YÃÉi , Zi )2 ]
‚â§ E[(aN,¬µ (YÃÉi , Zi , Z1j , Z2k )2 ] = O(b‚àíL
N ).

Further (note that E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )aN,¬µ (YÃÉi , Zi , Z1j , Z2k0 )] = E[(EZ2 [aN,¬µ (YÃÉi , Zi , Z1j , Z2 )])2 ] ‚â• 0)
E[(aN,¬µ (YÃÉi , Zi , Z1j , Z2k ) ‚àí cN,¬µ (YÃÉi , Zi ))(aN,¬µ (YÃÉi , Zi , Z1j , Z2k0 ) ‚àí cN,¬µ (YÃÉi , Zi ))]
= E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )aN,¬µ (YÃÉi , Zi , Z1j , Z2k0 )] ‚àí E[cN,¬µ (YÃÉi , Zi )2 ]

‚â§ E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )aN,¬µ (YÃÉi , Zi , Z1j , Z2k0 )]

[61]

and
E[aN,¬µ (YÃÉi , Zi , Z1j , Z2k )aN,¬µ (YÃÉi , Zi , Z1j , Z2k0 )]
¬ª
‚Äû
¬´
‚Äû
¬´
1
Zi ‚àí rbN (Z1j , Z2k )
Zi ‚àí rbN (Z1j , Z2k0 )
= 2|¬µ|+2L E ŒΩ(Z1j , Z2k )0 YÃÉi YÃÉi0 ŒΩ(Z1j , Z2k0 )K (¬µ)
K (¬µ)
bN
bN
bN
Àú
0
0 0
¬µ
0
0
0
¬µ
√ó((Z1j Z2k ) ‚àí rbN (Z1j , Z2k )) ((Z1j Z2k0 ) ‚àí rbN (Z1j , Z2k0 ))
¬ª
¬ª
‚Äû
¬´‚Äì
¬ª
‚Äû
¬´‚Äì
S ‚àí rbN (Z1j , Z2k )
Z ‚àí rbN (Z1j , Z2k0 )
1
‚àí 2|¬µ|+2L E ŒΩ(Z1j , Z2k )0 EYÃÉ Z YÃÉ K (¬µ)
EYÃÉ Z YÃÉ V 0(¬µ)
ŒΩ(Z1j , Z2k0 )
bN
bN
bN
0
0 0
0
0
0
¬µÀú
√ó((Z1j
Z2k
) ‚àí rbN (Z1j , Z2k ))¬µ ((Z1j
Z2k
0 ) ‚àí rbN (Z1j , Z2k0 ))
¬´
‚Äû
¬´
¬ª
‚Äû
Zi ‚àí rbN (Z1j , Z2k0 )
1
Zi ‚àí rbN (Z1j , Z2k )
K (¬µ)
‚â§ 2|¬µ|+2L E ŒΩ(Z1j , Z2k )0 YÃÉi YÃÉi0 ŒΩ(Z1j , Z2k0 )K (¬µ)
bN
bN
bN
Àú
0
0 0
¬µ
0
0
0
¬µ
√ó((Z1j Z2k ) ‚àí rbN (Z1j , Z2k )) ((Z1j Z2k0 ) ‚àí rbN (Z1j , Z2k0 ))

because both expectations are nonnegative. By Assumptions 3.1 and smoothness this is bounded by
¬ª
‚Äû
¬´
‚Äû
¬´‚Äì
Zi ‚àí rbN (Z1j , Z2k )
Zi ‚àí rbN (Z1j , Z2k0 )
C1
(¬µ)
(¬µ)
E
K
K
‚â§ C2 b‚àíL
N
b2L
bN
bN
N

by a change of variables to t = (Zi ‚àí rbN (Z1j , Z2k ))/bN with Jacobian bL
N and Assumption 4.1. By interchanging
the roles of j and k we obtain a bound of the same order for the third term on the right hand side of (B.69).
Combining these results we find
‚àí1 ‚àíL
E[(W¬µ,1 ‚àí U¬µ )2 ] = O(N ‚àí2 b‚àíL
bN ) = O(N ‚àí1 b‚àíL
N ) + O(N
N )

(B.70)
‚àíL/2

so that by the Markov inequality the first term in the projection remainder (B.40) is Op (N ‚àí1/2 bN

).

Step 3F: Equation (B.42) For the second term of the projection remainder (B.40) we have by the
Cauchy-Schwartz inequality
E[cN,¬µ (YÃÉi , Zi )2 ]
"‚ÄûZ Z
‚Äû
¬´
¬´2 #
Zi ‚àí rbN (z1 , z2 )
1
0
(¬µ)
0
0 0
¬µ
‚â§ 2L+2|¬µ| E
ŒΩ(z1 , z2 ) YÃÉi K
((z1 z2 ) ‚àí rbN (z1 , z2 )) fZ1 (z1 )fZ2 (z2 )dz1 dz2
bN
bN
Z2 Z1
"
Àõ
‚ÄûZ Z
‚Äû
¬´Àõ
¬´2 #
Àõ (¬µ) Zi ‚àí rbN (z1 , z2 ) Àõ Àõ 0 0 0
Àõ
1
2
¬µÀõ
Àõ
Àõ
Àõ
‚â§ 2L+2|¬µ| E |YÃÉi |
|ŒΩ(z1 , z2 )| ÀõK
Àõ ((z1 z2 ) ‚àí rbN (z1 , z2 ) fZ1 (z1 )fZ2 (z2 )dz1 dz2
bN
bN
Z2 Z1
"‚ÄûZ Z Àõ
#
‚Äû
¬´Àõ
¬´2
Àõ (¬µ) Zi ‚àí rbN (z1 , z2 ) Àõ
C1
ÀõK
Àõ fZ1 (z1 )fZ2 (z2 )dz1 dz2 E(|YÃÉi |2 |Zi )
‚â§ 2L EZi
Àõ
Àõ
bN
bN
Z2 Z1
‚Äû
¬´Àõ
¬´2
Z ‚ÄûZ Z Àõ
Àõ (¬µ) zÃÉ ‚àí rbN (z1 , z2 ) Àõ
C2
ÀõK
Àõ fZ1 (z1 )fZ2 (z2 )dz1 dz2 fZ (zÃÉ)dzÃÉ
‚â§ 2L
Àõ
Àõ
bN Z
bN
Z2 Z1

by Assumptions 3.1 and smoothness. By a change of variables t = (zÃÉ ‚àí rbN (z1 , z2 ))/bN with Jacobian bL
N we
conclude that
E[cN,¬µ (YÃÉi , Zi )2 ] = O(b‚àíL
N )

(B.71)

Therefore
E[U¬µ2 ] = E[cN,¬µ (YÃÉi , Zi )2 ] = O(b‚àíL
N )
‚àíL/2

so that the second term of the projection remainder is Op (N ‚àí1bN

[62]

).

Step 3F: Equations (B.43)-(B.46) The other terms of the projection remainder can be bounded
using (B.47)-(B.50). For the third term (note E[aN,¬µ (YÃÉi , Zi , Z1i , Z2k )] 6= 0) by (B.47)-(B.50)
‚àö
N(N ‚àí 1)
E[|W¬µ,2 |] ‚â§
E[|aN,¬µ (YÃÉi , Zi , Z1i , Z2k )|]
N2
‚àö
q
N(N ‚àí 1)
‚â§
E[|aN,¬µ (YÃÉi , Zi , Z1i , Z2k )|2 ]
2
¬´
‚ÄûN
1 ‚àíL+ L2
2
= O N ‚àí 2 bN

‚Äû
¬´
L
1 ‚àíL+ 2
2
so that that term is Op N ‚àí 2 bN
. In the same way by (B.47)-(B.50) the fourth term of the remainder is
¬´
‚Äû
L1
1 ‚àíL+
2
Op N ‚àí 2 bN
. For the fifth term (note aN,¬µ (YÃÉi , Zi , Z1j , Z2j ) = 0)
‚Äú
‚Äù
N 2 (N ‚àí 1)
E[aN,¬µ (YÃÉi , Zi , Z1j , Z2j )2 ] = O N ‚àí3 b‚àíL
N
6
N
‚Äú
‚Äù
‚àíL/2
so that that term is Op N ‚àí3/2 bN
. Finally, the sixth term (note E[aN,¬µ (YÃÉi , Zi , Z1i , Z2i )] 6= 0) is by a similar
‚Äú
‚Äù
argument as for the third term and by (B.47)-(B.50), Op N ‚àí1/2 b‚àíL
. This is the largest term in the projection
N
remainder.
This finishes the proof of
‚Äù
‚Äú
1
(B.72)
W¬µ = U¬µ + Op N ‚àí 2 b‚àíL
N
2
E[W¬µ,4
]=

Note again that the remainder is smaller if we redefine the kernel estimators. In that case the sixth term of the
projection remainder is 0.

Step 4: Asymptotic distribution The fourth step in the proof is the derivation of the asymptotically
normal distribution of the projection U¬µ . In particular, we show that U0 is asymptotically normal and we obtain
the variance of that distribution. We show that U¬µ /bN also converges to a normal distribution for |¬µ| ‚â• 1 so
that U¬µ = Op (bN ) if |¬µ| ‚â• 1. Because W in (B.36) is a linear combination of the W¬µ that are asymptotically
equivalent to the U¬µ if a rate condition is met, W is asymptotically equivalent to U0 under that rate condition.
Define
¬´
‚Äû
Z Z
Zi ‚àí rbN (z1 , z2 )
1
œàN,¬µ,i ‚â° L+|¬µ|
((z10 z20 )0 ‚àí rbN (z1 , z2 ))¬µ fZ1 (z1 )fZ2 (z2 )dz1 dz2
ŒΩ(z1 , z2 )0 YÃÉi K (¬µ)
bN
bN
Z2 Z1
so that
cN,¬µ (YÃÉi , Zi ) = œàN,¬µ,i ‚àí E[œàN,¬µ,i ].
We have
œàN,0,i ‚â°

1
bL
N

Z

Z2

Z

ŒΩ(z1 , z2 )0 YÃÉi K

Z1

‚Äû

Zi ‚àí rbN (z1 , z2 )
bN

¬´

fZ1 (z1 )fZ2 (z2 )dz1 dz2

The integration region Z1 √ó Z2 can be partitioned into a set where all components of z1 and z2 are in the internal
region, ZI1,bN √ó ZI2,bN , and its complement, Z1 √ó Z2 \ ZI1,bN √ó ZI2,bN . We define
‚Äû
¬´
‚Äû
¬´
Z
Z
1
Z1i ‚àí z1
Z2i ‚àí z2
œàN,0,i,0 ‚â° L
ŒΩ(z1 , z2 )0 YÃÉi K1
K2
fZ1 (z1 )fZ2 (z2 )dz1 dz2
bN
bN
bN ZI
ZI
2,bN

1,bN

and
N
1 X
U0,0 ‚â° ‚àö
(œàN,0,i,0 ‚àí E[œàN,0,i,0 ])
N I=1

[63]

We apply the Liapounov central limit theorem for triangular arrays that requires
` ÀÜ
Àú¬¥2
N 2 E |œàN,0,i,0 ‚àí E[œàN,0,i,0 ]|3
‚Üí0
N 3 Var(œàN,0,i,0 )
and a sufficient condition is that E [|œàN,0,i,0 |m ] < ‚àû for m = 1, 2, 3. By a change of variables to t1 = (Z1i ‚àíz1 )/bN
L2
1
and t2 = (Z2i ‚àí z2 )/bN with Jacobians bL
N and bN , respectively
|œàN,0,i,0 |m
ÀõZ Z L ‚Äû
¬´ L2 ‚Äû
¬´
Àõ
1
Y
Z1li ‚àí zu1l
Z1li ‚àí zl1l Y
Z2li ‚àí zu2l
Z2li ‚àí zl2l
Àõ
=Àõ
1 1+
‚â§ t1l ‚â§ ‚àí1 +
1 1+
‚â§ t2l ‚â§ ‚àí1 +
Àõ U1 U2
bN
bN
bN
bN
l=1
l=1
Àõm
Àõ
0
√óŒΩ (Z1i ‚àí bN t1 , Z2i ‚àí bN t2 ) YÃÉi K1 (t1 )K2 (t2 )fZ1 (Z1i ‚àí bN t1 )fZ2 (Z2i ‚àí bN t2 )dt1 dt2 Àõ
¬´ L2 ‚Äû
¬´
‚Äû
Z Z Y
L1
Z1li ‚àí zl1l Y
Z2li ‚àí zu2l
Z2li ‚àí zl2l
Z1li ‚àí zu1l
‚â§ t1l ‚â§ ‚àí1 +
1 1+
‚â§ t2l ‚â§ ‚àí1 +
1 1+
‚â§
bN
bN
bN
bN
U1 U2 l=1
l=1
‚Äùm
√ó|ŒΩ (Z1i ‚àí bN t1 , Z2i ‚àí bN t2 ) | ¬∑ |YÃÉi | ¬∑ |K1(t1 )| ¬∑ |K2(t2 )|fZ1 (Z1i ‚àí bN t1 )fZ2 (Z2i ‚àí bN t2 )dt1 dt2

n
o
n
o
Z ‚àíz
Z ‚àíz
by the Cauchy-Schwartz inequality . Because max ‚àí1, 1 + jlib ujl ‚â§ tjl ‚â§ min 1, ‚àí1 + jlib ljl , j = 1, 2
N
N
if and only if zljl + bN ‚â§ Zjli ‚àí bN tjl ‚â§ zujl ‚àí bN , we obtain by Assumptions 3.1, 4.1 and smoothness
|œàN,0,i,0 |m ‚â§ C|YÃÉi |m

(B.73)

and E[|YÃÉ |3 ] is finite by Assumption 3.1. Therefore the condition of the Liapounov theorem holds.
The above expressions also show that for almost all Z1i , Z2i
œàN,0,i,0 ‚Üí ŒΩ (Z1i , Z2i )0 YÃÉi fZ1 (Z1i )fZ2 (Z2i )
m
] converges to the corresponding expectation by dominated convergence. The conclusion
and by (B.73) E[œàN,0,i,0
is that U0,0 has the same asymptotic distribution as
N
o
1 Xn
‚àö
ŒΩ (Z1i , Z2i )0 YÃÉi fZ1 (Z1i )fZ2 (Z2i ) ‚àí E[ŒΩ (Z1 , Z2 )0 YÃÉ fZ1 (Z1 )fZ2 (Z2 )]
N i=1

(B.74)

We still have to derive the stochastic order of
N
1 X
(œàN,0,i,1 ‚àí E[œàN,0,i,1 ])
U0,1 ‚â° ‚àö
N I=1

with the integration region in œàN,0,i,1 , i.e. Z1 √ó Z2 \ ZI1,bN √ó ZI2,bN , such that at least one component of z1 or z2 is
B
L1
in the boundary region. We partition Z1 √óZ2 \ZI1,bN √óZI2,bN into subsets ZB
, p2 =
1,bN ,p1 √óZ1,bN ,p2 , p1 = 1, . . . , 2
1, . . . , 2L2 , min{p1 , p2 } ‚â• 1 and in each such set 0 ‚â§ L1p1 ‚â§ L1 , 0 ‚â§ L2p2 ‚â§ L2 , min{L1p1 L1p1 } ‚â• 1 components
I
B
I
of z1 and z2 are near the boundary. We take without loss of generality ZB
1,bN ,1 = Z1,bN and Z2,bN ,1 = Z2,bN
so that we exclude the set with p1 = p2 = 1 because in that set all components are in the internal region. For
L1pj
B
j = 1, 2 each ZB
in which 0 ‚â§ Kjrj ‚â§ Ljrj
j,bN ,pj is partitioned further into sets Zj,bN ,pj ,rj , rj = 1, . . . , 2
components of zj are near the lower, Ljrj ‚àí K1rj are near the upper boundary, and the remaining Lj ‚àí Ljpj
components are in the internal set. Without loss of generality we assume that the first Kjrj components of zj
are near the lower boundary, the next Ljpj ‚àí Kjrj are near the upper boundary, and the remaining components

[64]

are in the internal set, j = 1, 2. Therefore
Àõ
Àõm
¬´
‚Äû
Z
Àõ
Àõ
Zi ‚àí rbN (z1 , z2 )
Àõ 1
Àõ
m
0
|œàN,0,i,1 | = Àõ L
fZ1 (z1 )fZ2 (z2 )dz1 dz2 Àõ
(B.75)
ŒΩ(z1 , z2 ) YÃÉi K
Àõ bN Z2 √óZ1 \ZI √óZI
Àõ
bN
1,bN
2,bN
!m
Àõ ‚Äû
¬´Àõ
Z
Àõ
Zi ‚àí rbN (z1 , z2 ) ÀõÀõ
1
Àõ
‚â§
|ŒΩ(z1 , z2 )||YÃÉi | ÀõK
Àõ fZ1 (z1 )fZ2 (z2 )dz1 dz2
bN
bL
Z2 √óZ1 \ZI
√óZI
N
1,bN

‚â§

Z

XXXX
p1

p2

r1

zl,l1 +bN

¬∑¬∑¬∑

zl,l1

1
bL
N

r2

Z

2,bN

Z

¬∑¬∑¬∑

zl21

zl,1K1r +bN
1

zl,1K1r

Z

zl21 +bN

Z

zl2K2r

zu1,K1r +1
1

zu1,K1r +1 ‚àíbN
1

1

‚Äû
¬´Àõ
Y ÀõÀõ
Àõ
ÀõK1l Z1li ‚àí zl1l ‚àí 1 Àõ
Àõ
Àõ
bN

K1r1

L1p1

l=1

l=K1r1 +1

Y

Z

zl2K2r +bN
2
2

¬∑¬∑¬∑

Z

zu2,K2r +1
2

zu2,K2r +1 ‚àíbN
2

zu1,L1p

1

zu1,L1p ‚àíbN
1

Z

Z

zu2,L2p

2

zu2,L2p ‚àíbN
2

zu1,L1p +1 ‚àíbN
1
zu1,L1p +1 +bN
1

Àõ
‚Äû
¬´Àõ
Àõ
Àõ
ÀõK1l Z1li ‚àí zl1l + 1 Àõ
Àõ
Àõ
bN

L1
Y

l=L1p1 +1

Àõ
Àõ
‚Äû
¬´Àõ LY
¬´Àõ
‚Äû
2p2
Y Àõ
Àõ
Àõ
Àõ
ÀõK2l Z2li ‚àí zl2l ‚àí 1 Àõ
ÀõK2l Z2li ‚àí zu2l + 1 Àõ
Àõ
Àõ
Àõ
Àõ
bN
bN
l=1
l=K2r2 +1
1m
Àõ
¬´Àõ
‚Äû
L2
Y
Àõ
Àõ
Z
‚àí
z
2li
2l
Àõ fZ1 (z1 )fZ2 (z2 )dz1 dz2 A
ÀõK2l
Àõ
Àõ
bN

K2r1

¬∑¬∑¬∑

¬∑¬∑¬∑

Z

Z

zu2,L2p +1 ‚àíbN
2

zl2,L2p +1 +bN
2

zu1,L1 ‚àíbN

zl1,L1 +bN

¬∑¬∑¬∑

Z

zu2,L2 ‚àíbN

zl2,L2 +bN

|ŒΩ(z1 , z2 )||YÃÉi |

Àõ
‚Äû
¬´Àõ
Àõ
Àõ
ÀõK1l Z1li ‚àí z1l Àõ
Àõ
Àõ
bN

l=L2p2 +1

By a change of variables to t1l = (Z1li ‚àí z1l )/bN , l = L1p1 + 1, L1 and t2l = (Z2li ‚àí z2l )/bN , l = L2p2 + 1, L2 with
L‚àíL1p1 ‚àíL2p2
Jacobian bN
we have
|œàN,0,i,1 |m ‚â§

XXXX

¬∑¬∑¬∑
¬∑¬∑¬∑

p1

Z

p2

1

‚àí1

Z

1

Z

r1

r2

zl,l1 +bN

zl,l1

2
Y

1
m(L1p1 +L2p2 )

bN
Z
¬∑¬∑¬∑

zl,1K1r +bN
1

zl,1K1r

Lj
Y

1

‚àí1 j=1 l=L
jpj +1

‚Äû

Z

zl,21 +bN

zl,21

Z

¬∑¬∑¬∑

Z

zl,2K2r +bN
2
zl,2K2r

zu1,K1r +1
1

zu1,K1r +1 ‚àíbN
1

1

¬∑¬∑¬∑

Z

2

Z

zu2,K2r +1
2
zu2,K2r +1 ‚àíbN
2

zu1,L1p

1

zu1,L1p ‚àíbN
1

Z

¬∑¬∑¬∑

Z

zu2,L2p

2

zu2,L2p ‚àíbN
2

Z

1

‚àí1

1
‚àí1

¬´
Zjli ‚àí zujl
Zjli ‚àí zljl
+ 1 ‚â§ tjl ‚â§
‚àí1
bN
bN

|ŒΩ(z11 , . . . , z1L1p1 , Z1,L1p1 +1,i ‚àí bN t1L1p1 +1 , . . . , Z1,L1 ,i
‚àí bN t1L1 , z21 , . . . , z2L2p2 , Z2,L2p2 +1,i
¬´Àõ LY
‚Äû
1p1
Y ÀõÀõ
Àõ
ÀõK1l Z1li ‚àí zl1l ‚àí 1 Àõ
Àõ
Àõ
bN

K1r1

l=1

l=K1r1 +1

Àõ
‚Äû
¬´Àõ
Y Àõ
Àõ
ÀõK2l Z2li ‚àí zl2l ‚àí 1 Àõ
Àõ
Àõ
bN

K2r1

l=1

L1

Y

l=L1p1 +1

L2

|K1l (t1l )|

Y

l=L2p2 +1

L2p2

Y

l=K2r2 +1

‚àí bN t2L2p2 +1 , . . . , Z2,L2 ,i ‚àí bN t2L2 )||YÃÉi|
Àõ
‚Äû
¬´Àõ
Àõ
Àõ
ÀõK1l Z1li ‚àí zl1l + 1 Àõ
Àõ
Àõ
bN
Àõ
‚Äû
¬´Àõ
Àõ
Àõ
ÀõK2l Z2li ‚àí zu2l + 1 Àõ
Àõ
Àõ
bN

|K2l (t2l )| fZ1 (z11 , . . . , z1L1p1 , Z1,L1p1 +1,i ‚àí bN t1L1p1 +1 , . . . , Z1,L1 ,i ‚àí bN t1L1 )

fZ2 (z21 , . . . , z2L2p2 , Z2,L2p2 +1,i ‚àí bN t2L2p2 +1 , . . . , Z2,L2 ,i ‚àí bN t2L2 )dz11 ¬∑ ¬∑ ¬∑ dz1L1p1 dt1,L1p1 +1 ¬∑ ¬∑ ¬∑ dt1L1
¬¥m
dz21 ¬∑ ¬∑ ¬∑ dz2L2p2 dt2,L2p2 +1 ¬∑ ¬∑ ¬∑ dt2L2

In this integral the function ŒΩ takes only values in the support Z and this function and the kernel functions are

[65]

bounded by smoothness and Assumption 4.1 so that
|œàN,0,i,1 |m
‚â§ C|YÃÉi |m
0

XXXX
p1

p2

r1

r2

1
m(L
+L
)
bN 1p1 2p2

Àõ
‚Äû
¬´Àõ
Y Àõ
Àõ
ÀõK1l Z1li ‚àí zl,11 ‚àí 1 Àõ
√ó@
Àõ
Àõ
bN
K1r1

l=1

zl,21 +bN

zl,21

¬∑¬∑¬∑

Z

Y

l=1

Àõ
‚Äû
¬´Àõ
Y Àõ
Àõ
ÀõK2l Z2li ‚àí zl2l ‚àí 1 Àõ
Àõ
Àõ
bN

K2r1

Z

L1p1

1

‚àí1

Z

Z

¬∑¬∑¬∑

zl,2K2r +bN
2

zl,2K2r

zl,l1 +bN

zl,l1

¬∑¬∑¬∑

Y

l=K2r2 +1

Z

Àõ
‚Äû
¬´Àõ
Àõ
Àõ
ÀõK2l Z2li ‚àí zu2l + 1 Àõ
Àõ
Àõ
bN

zu2,K2r +1
2

zu2,K2r +1 ‚àíbN
2

2

Z

l=K1r1 +1

L2p2

Àõ
‚Äû
¬´Àõ
Àõ
Àõ
ÀõK1l Z1li ‚àí zl1l + 1 Àõ
Àõ
Àõ
bN

zl,1K1r +bN
1

zl,1K1r

Z

¬∑¬∑¬∑

Z

zu2,L2p

2

zu2,L2p ‚àíbN
2

zu1,K1r +1
1

zu1,K1r +1 ‚àíbN
1

1

¬∑¬∑¬∑

Z

Z

1

‚àí1

zu1,L1p

1

zu1,L1p ‚àíbN
1

Z

1

‚àí1

¬∑¬∑¬∑

Z

1

‚àí1

fZ1 (z11 , . . . , z1L1p1 , Z1,L1p1 +1,i ‚àí bN t1L1p1 +1 , . . . , Z1,L1 ,i ‚àí bN t1L1 )

√ó fZ2 (z21 , . . . , z2L2p2 , Z2,L2p2 +1,i ‚àí bN t2L2p2 +1 , . . . , Z2,L2 ,i ‚àí bN t2L2 )
¬¥m
dz11 ¬∑ ¬∑ ¬∑ dz1L1p1 dt1,L1p1 +1 ¬∑ ¬∑ ¬∑ dt1L1 dz21 ¬∑ ¬∑ ¬∑ dz2L2p2 dt2,L2p2 +1 ¬∑ ¬∑ ¬∑ dt2L2
L

Because the density is bounded, the integral is bounded by CbN1p1
support [‚àí1, 1]L and is bounded on that support we have that
‚Äû
¬´Àõ
Y ÀõÀõ
Àõ
ÀõK1l Z1li ‚àí zl1l ‚àí 1 Àõ
Àõ
Àõ
bN

K1r1

L1p1

l=1

l=K1r1 +1

Àõ
‚Äû
¬´Àõ
Y Àõ
Àõ
ÀõK2l Z2li ‚àí zl2l ‚àí 1 Àõ
√ó
Àõ
Àõ
bN
K2r1

l=1

Y

L2p2

Y

l=K2r2

K1r1

‚â§C

Y
l=1

+L2p2

. Moreover because the kernel has

Àõ
¬´Àõ
‚Äû
Àõ
Àõ
ÀõK1l Z1li ‚àí zl1l + 1 Àõ
Àõ
Àõ
bN

Àõ
‚Äû
¬´Àõ
Àõ
Àõ
ÀõK2l Z2li ‚àí zu2l + 1 Àõ
Àõ
Àõ
bN
+1
L1p1

1 (zl1l ‚â§ Z1li ‚â§ zl1l + 2bN )

Y

l=K1r1 +1

1 (zl1l ‚àí 2bN ‚â§ Z1li ‚â§ zl1l )

K2r1

√ó

Y
l=1

L2p2

1 (zl2l ‚â§ Z2li ‚â§ zl2l + 2bN )

Y

l=K2r2 +1

1 (zu2l ‚àí 2bN ‚â§ Z2li ‚â§ zu2l )

Therefore
L1p1

K

|œàN,0,i,1 |m ‚â§ C|YÃÉi |m

1r1
XXXX Y

p1

p2

r1

r2

l=1

1 (zl1l ‚â§ Z1li ‚â§ zl1l + 2bN )

K2r1

√ó

Y

l=1

Y

l=K1r1 +1

1 (zl1l ‚àí 2bN ‚â§ Z1li ‚â§ zu1l )

L2p2

1 (zl2l ‚â§ Z2li ‚â§ zl2l + 2bN )

Y

l=K2r2 +1

1 (zu2l ‚àí 2bN ‚â§ Z2li ‚â§ zu2l )

and because E[|YÃÉ |3 |Z = z] is bounded on Z and the density of Z is bounded, we have because L1p1 + L2p2 ‚â• 1
for m = 1, 2, 3
E [|œàN,0,i,1 |m ] = O(bN )
By the Liapounov central limit theorem U01 /bN converges in distribution and hence
U01 = Op (bN )

(B.76)

[66]

Step 5: Ignoring Higher Order Terms The final step is to show that U¬µ is asymptotically negligible
if |¬µ| ‚â• 1. Note that if |¬µ| ‚â• 1, then the integrand in œàN,¬µ,i is 0 if z1 and z2 are both in the internal region.
Hence we can take the integration region such that at least one component of either z1 or z2 is in the boundary
region
|œàN,¬µ,i |m
Àõ
Àõm
¬´
‚Äû
Z
Àõ
Àõ
Zi ‚àí rbN (z1 , z2 )
Àõ 1
Àõ
= Àõ L+|¬µ|
((z10 z20 )0 ‚àí rbN (z1 , z2 ))¬µ fZ1 (z1 )fZ2 (z2 )dz1 dz2 Àõ
ŒΩ(z1 , z2 )0 YÃÉi K (¬µ)
Àõb
Àõ
b
I
I
N
Z2 √óZ1 \Z1,b √óZ2,b
N
N
N
Àõ
‚Äû
¬´Àõ
Z
Àõ
Zi ‚àí rbN (z1 , z2 ) ÀõÀõ
1
‚â§
|ŒΩ(z1 , z2 )||YÃÉi | ÀõÀõK (¬µ)
Àõ
L+|¬µ|
bN
I
I
bN
Z2 √óZ1 \Z1,b √óZ2,b
N
N
‚Äùm
|(z10 z20 )0 ‚àí rbN (z1 , z2 )||¬µ| fZ1 (z1 )fZ2 (z2 )dz1 dz2
!m
Àõ
‚Äû
¬´Àõ
Z
Àõ (¬µ) Zi ‚àí rbN (z1 , z2 ) Àõ
1
Àõ
Àõ
|ŒΩ(z1 , z2 )||YÃÉi | ÀõK
‚â§
Àõ fZ1 (z1 )fZ2 (z2 )dz1 dz2
bN
bL
Z2 √óZ1 \ZI
√óZI
N
1,bN

2,bN

We obtained a bound on the right hand side in (B.75). Therefore by the Liapounov central limit theorem
converges in distribution so that if |¬µ| ‚â• 1
U¬µ = Op (bN )

U¬µ
bN

(B.77)

By (B.33) (linearization), (B.34) (bias), (B.72) (projection), (B.76) (boundary remainder), and (B.77) (NIP
remainder) we have that
‚àö

N (Œ∏ÃÇ ‚àí Œ∏) =

1
‚àö

N N

N
N X
X
(n(h0 (Z1j , Z2k )) ‚àí Œ∏)

(B.78)

j=1 k=1

¬ª
‚Äìff
N Ôöæ
‚àÇn
1 X ‚àÇn
(h0 (Zi ))0 YÃÉi fZ1 (Z1i )fZ2 (Z2i ) ‚àí EYÃÉ Z
(h0 (S))0 YÃÉ fZ1 (Z1 )fZ2 (Z2 )
+‚àö
‚àÇh
N i=1 ‚àÇh
‚Äû
¬´
Àõ
Àõ
‚àö Àõ
‚àö
Àõ2
+ Op
N ÀõhÃÇnip,s ‚àí h0 Àõ + O( N bpN ) + Op (N ‚àí1 b‚àíL
N ) + Op (bN )

The first term on the right hand side is a V statistic that is asymptotically equivalent to
N
1 X
‚àö
{(E[n(h0 (Z1i , Z2 )) ‚àí Œ∏) + E[n(h0 (Z1 , Z2i )) ‚àí Œ∏)} .
N i=1


Proof of Lemma A.24: Using Lemma A.14, the assumptions imply that
!
‚Äû
¬´1/2
`
¬¥
ln(N )
s
sup |gÃÇ(w, x) ‚àí g(w, x)| = Op
+ bN = op N ‚àíŒ∑ ,
2
N
¬∑
b
w‚ààW,x‚ààX
N

(B.79)

For 1/4 < Œ¥ < 1/4s we can find an Œ∑ > 1/4 such that this holds. Using the definitions preceding the statement
of the Lemma, we have, by adding and subtracting terms,
Œ≤bcm(œÅ, 0) ‚àí Œ≤ cm(œÅ, 0) = (Œ≤bcm (œÅ, 0) ‚àí Œ≤ÃÇgcm )
‚àí

‚àí

cm
(Œ≤ÃÇW

cm
(Œ≤ÃÇX

‚Äú

‚àíg
‚àíg

cm

)

cm

)
‚Äù

(B.80)
(B.81)

‚Äú

‚Äù

‚Äú

‚Äù

cm
cm
+ Œ≤ÃÇgcm ‚àí g cm + Œ≤ÃÇW
‚àí g cm + Œ≤ÃÇX
‚àí g cm + (gcm ‚àí Œ≤ cm(œÅ, 0)) .

‚Äú
‚Äù
The result then follows if we can show that the sum of (B.80), (B.81) and (B.82) is op N ‚àí1/2 . Define
‚Äú
‚Äù
‚àí1
œÜc Œ¶‚àí1
c (FÃÇW (w)), Œ¶c (FÃÇX (x)); œÅ
‚Äú
‚Äù ‚Äú
‚Äù,
œâÃÇ cm (w, x) =
‚àí1
œÜc Œ¶‚àí1
c (FÃÇW (w)) œÜc Œ¶c (FÃÇX (x))

[67]

(B.82)

and

‚Äú
‚Äù
‚àí1
œÜc Œ¶‚àí1
c (FÃÇW (w)), Œ¶c (FX (x)); œÅ
cm
‚Äú
‚Äù `
œâÃÇW
(w, x) =
¬¥,
‚àí1
œÜc Œ¶‚àí1
c (FÃÇW (w)) œÜc Œ¶c (FX (x))
cm
œâÃÇX
(w, x)

‚Äú
‚Äù
‚àí1
œÜc Œ¶‚àí1
c (FW (w)), Œ¶c (FÃÇX (x)); œÅ
‚Äù,
=
`
¬¥ ‚Äú ‚àí1
œÜc Œ¶‚àí1
c (FW (w)) œÜc Œ¶c (FÃÇX (x))

Then, using the definition of œâ cm (w, x) given in (4.32), we can write the sum of these three components as
cm
cm
(Œ≤bcm(œÅ, 0) ‚àí Œ≤ÃÇgcm) ‚àí (Œ≤ÃÇW
‚àí g cm) ‚àí (Œ≤ÃÇX
‚àí g cm )

=

N N
1 XX
gÃÇ(Wi , Xj ) [b
œâ cm (Wi , Xj ) ‚àí œâ cm (Wi , Xj )]
2
N i=1 j=1

‚àí
‚àí
=

‚àí

N
N
1 XX
g(Wi , Xj ) [b
œâX (Wi , Xj ) ‚àí œâ cm (Wi , Xj )]
N 2 i=1 j=1

N N
1 XX
gÃÇ(Wi , Xj ) [b
œâ cm (Wi , Xj ) ‚àí œâ cm (Wi , Xj )]
2
N i=1 j=1

N
N
N
N
1 XX
1 XX
cm
cm
g(W
,
X
)
[b
œâ
(W
,
X
)
‚àí
œâ
(W
,
X
)]+
g(Wi , Xj ) [b
œâ cm (Wi , Xj ) ‚àí œâ
bW (Wi , Xj )]
i
j
i
j
i
j
N 2 i=1 j=1
N 2 i=1 j=1

‚àí

=

N
N
1 XX
g(Wi , Xj ) [b
œâW (Wi , Xj ) ‚àí œâ cm (Wi , Xj )]
N 2 i=1 j=1

N
N
1 XX
g(Wi , Xj ) [b
œâX (Wi , Xj ) ‚àí œâ cm (Wi , Xj )]
N 2 i=1 j=1

N N
1 XX
[gÃÇ(Wi , Xj ) ‚àí g(Wi , Xj )] [b
œâ cm (Wi , Xj ) ‚àí œâ cm (Wi , Xj )]
2
N i=1 j=1

(B.83)

N
N
1 XX
g(Wi , Xj ) [b
œâ cm (Wi , Xj ) ‚àí œâ
bW (Wi , Xj ) ‚àí œâ
bX (Wi , Xj ) + œâ cm(Wi , Xj )]
N 2 i=1 j=1
‚Äú
‚Äù
It remains to be shown that both (B.83) and (B.84) are op N ‚àí1/2 .
Now define
`
¬¥
‚àí1
œÜc Œ¶‚àí1
c (z1 ), Œ¶c (z2 ); œÅ
`
¬¥
`
¬¥
k(z1 , z2 ) =
so that œâ
b cm(w, x) = k(FÃÇW (w), FÃÇX (x)).
‚àí1
œÜc Œ¶‚àí1
c (z1 ) ¬∑ œÜc Œ¶c (z2 )

+

(B.84)

(B.85)

By a second order Taylor expansion we have
œâ
b cm (w, x) ‚àí œâ cm (w, x) =
+

+

‚àÇk
‚àÇk
(FW (w), FX (x))(FÃÇW (w) ‚àí FW (w)) +
(FW (w), FX (x))(FÃÇX (x) ‚àí FX (x))
‚àÇz1
‚àÇz2

1 ‚àÇ2 k
1 ‚àÇ2 k
2
(F W (w), F X (x))(FÃÇX (x) ‚àí FX (x))2
2 (F W (w), F X (x))(FÃÇW (w) ‚àí FW (w)) +
2 ‚àÇz1
2 ‚àÇz22

1 ‚àÇ2 k
(F W (w), F X (x))(FÃÇW (w) ‚àí FW (w))(FÃÇX (x) ‚àí FX (x))
2 ‚àÇz1 ‚àÇz2

with F W (w) and F X (x) intermediate values. By Lemma A.3 it follows that for any 0 < Œ¥ < 1/2, supx |FÃÇX (x) ‚àí
FX (x)| = op (N ‚àíŒ¥ ), and supw |FÃÇW (w) ‚àí FW (w)| = op(N ‚àíŒ¥ ). In combination with the fact that |‚àÇ 2 k/‚àÇz12 |,
|‚àÇ 2 k/‚àÇz22 |, and |‚àÇ 2 k/‚àÇz1 ‚àÇz2 | are bounded, this implies that
œâ
b cm (w, x) ‚àí œâ cm (w, x)
=

‚àÇk
‚àÇk
(FW (w), FX (x))(FÃÇW (w) ‚àí FW (w)) +
(FW (w), FX (x))(FÃÇX (x) ‚àí FX (x)) + op
‚àÇz1
‚àÇz2

[68]

‚Äú

(B.86)
‚Äù
N ‚àí1/2 .

The same argument implies that

and

œâ
bW (w, x) ‚àí œâ cm (w, x) =

‚Äú
‚Äù
‚àÇk
(FW (w), FX (x))(FÃÇW (w) ‚àí FW (w)) + op N ‚àí1/2 ,
‚àÇz1

‚Äú
‚Äù
‚àÇk
(FW (w), FX (x))(FÃÇX (x) ‚àí FX (x)) + op N ‚àí1/2 .
‚àÇz2
‚Äú
‚Äù
Substituting in these results, it follows that (B.84) is op N ‚àí1/2 .
œâ
bX (w, x) ‚àí œâ cm (w, x) =

Equation (B.86) also implies, by Lemma A.3, that
‚Äú
‚Äù
œâ
b cm (w, x) ‚àí œâ cm (w, x) = op N ‚àí1/4 .

‚Äú
‚Äù
In combination with (B.79), this implies that (B.83) is also op N ‚àí1/2 . 
Proof of Lemma A.25: The proof of this Lemma make use of an application of Theorem A.3. Using the
notation of that Theorem we have Z1 = W, Z2 = X, YÃÉ = (Y, 1)0 ,
¬´
‚Äû
g(w, x) ¬∑ fW X (w, x)
h (w, x) =
fW X (w, x)
and
n(h(w, x)) =

h1 (w, x) cm
œâ (w, x) = g(w, x) ¬∑ œâ cm (w, x).
h2 (w, x)

In terms of this notation we can write this in the form of Theorem A.3:
Œ≤ÃÇgcm ‚àí g cm =

N
N
N
N
‚Äù
1 XX ‚Äú
1 XX
n
hÃÇ(W
,
X
)
‚àí
n (h(Wi , Xj )) .
i
j
N 2 i=1 j=1
N 2 i=1 j=1

We also have
‚àÇn
(h(w, x)) =
‚àÇh

1
h2 (w,x)
h1 (w,x)
‚àí h (w,x)2
2

!

œâ

cm

(w, x) =

1
fW X (w,x)
g(w,x)
‚àíf
W X (w,x)

!

œâ cm (w, x) ,

and hence
fW (w)fX (x)
‚àÇn
(h(w, x))0 yÃÉfW (w)fX (x)) =
¬∑ (y ‚àí g(w, x)) ¬∑ œâ cm (w, x) ,
‚àÇh
fW X (w, x)
which is mean zero. Therefore, by the result of Theorem A.3, we have
Œ≤ÃÇgcm ‚àíŒ≤ cm(œÅ, 0) =
=

¬ª
‚Äì
N
‚Äú
‚Äù
1 X ‚àÇn
‚àÇn
(h(Wi , Xi ))0 YÃÉi fW (Wi )fX (Xi ))‚àíE
(h(W, X))0 yÃÉY fW (W )fX (X)) +op N ‚àí1/2
N i=1 ‚àÇh
‚àÇh

¬ª
‚Äì
N
‚Äú
‚Äù
1 X fW (Wi )fX (Xi )
fW (W )fX (X)
¬∑(Yi ‚àí g(Wi , Xi ))¬∑œâ cm (Wi , Xi )‚àíE
¬∑ (Y ‚àí g(W, X)) ¬∑ œâ cm (W, X) +op N ‚àí1/2
N i=1 fW X (Wi , Xi)
fW X (W, X)
=

=

N
‚Äú
‚Äù
1 X fW (Wi )fX (Xi )
¬∑ (Yi ‚àí g(Wi , Xi )) ¬∑ œâ cm (Wi , Xi ) + op N ‚àí1/2
N i=1 fW X (Wi , Xi )
N
‚Äú
‚Äù
1 X cm
œàg (Yi , Wi , Xi ) + op (N ‚àí1/2 ) + op N ‚àí1/2 .
N i=1


Proof of Lemma A.26:
Lemma A.24 we have
cm
Œ≤ÃÇW
‚àí g cm =

Using the definition of k(z1 , z2 ) in (B.85) and the Taylor expansion in the proof of

N
N
1 XX
‚àÇk
g(Wi , Xj )
(FW (Wi ), FX (Xj ))(FÃÇW (Wi ) ‚àí FW (Wi ))
N 2 i=1 j=1
‚àÇz1

[69]

+

N
N
‚àÇ2k
1 1 XX
g(Wi , Xj ) 2 (F W (Wi ), F X (Xj ))(FÃÇW (Wi ) ‚àí FW (Wi ))2 .
2
2 N i=1 j=1
‚àÇz1

By Lemma A.3 supw |FÃÇW (w) ‚àí FW (w)| = op(N ‚àíŒ¥ ) for all Œ¥ < 1/2, and using the fact that the second derivatives
of k(z1 , z2 ) are bounded, this implies
cm
Œ≤ÃÇW
‚àí g cm =

N
N
‚Äú
‚Äù
1 XX
‚àÇk
g(Wi , Xj )
(FW (Wi ), FX (Xi ))(FÃÇW (Wi ) ‚àí FW (Wi )) + op N ‚àí1/2 .
2
N i=1 j=1
‚àÇz1

Inspection of the definition of eW (w, x) shows that eW (w, x) =
cm
Œ≤ÃÇW
‚àí g cm =

=

‚àÇk
(FW (w), FX (x))
‚àÇs1

and therefore

N
N
‚Äú
‚Äù
1 XX
g(Wi , Xj )eW (Wi , Xj ) (FÃÇW (Wi ) ‚àí FW (Wi )) + op N ‚àí1/2
2
N i=1 j=1

N N
N
‚Äú
‚Äù
1 XXX
g(Wi , Xj )eW (Wi , Xj ) (1 (Wk ‚â§ Wi ) ‚àí FW (Wi )) + op N ‚àí1/2 .
3
N i=1 j=1
k=1

This is, up to the op (N
cm
Œ≤ÃÇW
‚àí g cm

where
V =

‚àí1/2

) term, a third order V -statistic,
‚Äú
‚Äù
= V + op N ‚àí1/2

N
N
N
1 XXX
œà(Wi , Xi , Wj , Xj , Wk , Xk ),
N 3 i=1 j=1
k=1

with
œà(w1 , x1 , w2 , x2 , w3 , x3 ) = g(w1 , x2 )eW (w1 , x2 ) (1 (w3 ‚â§ w1 ) ‚àí FW (w1).
Define
œà1 (w, x) = E [œà(w, x, W2 , X2 , W3 , X3 )] ,

œà2 (w, x) = E [œà(W1 , X1 , w, x, W3 , X3 )] ,

œà3 (w, x) = E [œà(W1 , X1 , W2 , X2 , w, x)] ,

and Œ∏ = E[œà(W1 , X1 , W2 , X2 , W3 , X3 )].

Using V-statistic theory, this V-statistic can be approximated as
V =

N
‚Äú
‚Äù
1 X
{(œà1 (Wi , Xi ) ‚àí Œ∏) + (œà2 (Wi , Xi ) ‚àí Œ∏) + (œà3 (Wi , Xi ) ‚àí Œ∏)} + op N ‚àí1/2 .
N i=1

Note that E[œà(w1 , x1 , w2 , x2 , W, X)] = 0. Hence Œ∏ = 0, œà1 (w, x) = 0, and œà2 (w, x) = 0. Thus,
V =

N
‚Äú
‚Äù
1 X
œà3 (Wi , Xi ) + op N ‚àí1/2 .
N i=1

=

N Z Z
‚Äú
‚Äù
1 X
g(s, t)eW (s, t) (1 (Wi ‚â§ s) ‚àí FW (s))fW (s) fX (t) dsdt + op N ‚àí1/2 ,
N
k=1

=

N
‚Äú
‚Äù
1 X cm
œàW (Yi , Wi , Xi ) + op N ‚àí1/2 ,
N i=1

as required. 
Proof of Lemma A.27: The proof is entirely analogous to that of Lemma A.26 and therefore omitted. 
Proof of Lemma A.28: Define
œà(w, x) = g(w, x) ¬∑ œâ cm(w, x),
œà1 (w) = E [œà(w, X)] = E [g(w, X) ¬∑ œâ cm (w, X)] ,

[70]

and
œà2 (x) = E [œà(W, x)] = E [g(W, x) ¬∑ œâ cm(W, x)] .
Then, by the V-Statistic Projection Theorem, given as Theorem A.4 in Appendix A, it follows that
g cm ‚àí Œ≤ cm(œÅ, 0) =
=

N
‚Äú
‚Äù
1 X
{(œà1 (Wi ) ‚àí Œ≤ cm(œÅ, 0)) + (œà2 (Xi ) ‚àí Œ≤ cm(œÅ, 0))} + op N ‚àí1/2
N i=1

N
‚Äú
‚Äù
1 X cm
œà0 (Yi , Wi , Xi ) + op N ‚àí1/2 .
N i=1


P
PN
2
Proof of Theorem A.4: Define œÜ(z1 , z2 ) = (œà(z1 , z2 ) + œà(z2 , z1 ))/2. Then V = N
i=1
j=1 œÜ(Zi , Zj )/N is a
V-statistic with a symmetric kernel. In the notation of Lehman (1998),
œÉ12 = Cov (œÜ(Zi , Zj ), œÜ(Zi , Zk )) ,
for i, j, k distinct, which simplifies to œÉ12 = œÉ2 /4. Therefore, by Theorems 6.1.2 (with a = 2) and 6.2.1 in Lehman
(1998), the result follows. .

[71]

Appendix C: Proofs of Theorems in Text
Proof of Theorem 3.1 Define
VŒª,i = Œª ¬∑ Xi ¬∑ d(Wi ) + Wi ,
h(Œª, a) = pr(VŒª ‚â§ a) = FVŒª (a),

and

First we focus on

k(w, x, Œª) = h(Œª, Œª ¬∑ x ¬∑ d(w, x) + w).

ÀÜ
Àú
ÀÜ
Àú
‚àí1
‚àí1
Œ≤ lr,v (Œª) = E g(FW
(FVŒª (VŒª,i )), X) = E g(FW
(k(Wi , Xi, Œª)), Xi ) .

We then prove four results. First, we show that for small Œª, Œ≤ lr,v (Œª) and Œ≤ lr (Œª) are close, or
Œ≤ lr,v (Œª) = Œ≤ lr(Œª) + o(Œª).

(C.1)

Second, we show that
Œ≤ lr,v (Œª) = E[g(W, X)]
¬ª
‚Äì
‚àÇg
1
+E
(Wi , Xi)
(k(Wi , Xi , Œª) ‚àí k(Wi , Xi , 0)) + o(Œª).
‚àÇw
fW (Wi )

(C.2)

Next we show that Œ≤ lr,v (Œª) has the two representations in Theorem 3.1. In particular, the third part of the proof
lr,v
shows that Œ≤ lc,v = ‚àÇŒ≤‚àÇŒª (0) satisfies
‚Äì
¬ª
‚àÇg
(Wi , Xi) ¬∑ (Xi ¬∑ d(Wi , Xi ) ‚àí E [ Xi ¬∑ d(Wi , Xi )| Wi ]) .
(C.3)
Œ≤ lc,v = E
‚àÇw
Fourth, we show that Œ≤ lc,v satisfies
¬ª
‚Äì
‚àÇ2 g
Œ≤ lc,v = E Œ¥(Wi , Xi ) ¬∑
(Wi , Xi ) .
‚àÇw‚àÇx

(C.4)

We start with the proof of (C.1). Define
p
u(w, x, Œª) = Œª ¬∑ x ¬∑ d(w, x)1‚àí|Œª| + 1 ‚àí Œª2 ¬∑ w,

and

u(w, x, Œª) = Œª ¬∑ x ¬∑ d(w, x) + w.

Then

sup
w‚ààW,x‚ààX

Define also

` ¬¥
|u(w, x, Œª) ‚àí v(w, x, Œª)| = O Œª2 .

hU (Œª, a) = pr(UŒª ‚â§ a),

and

kU (w, x, Œª) = hU (Œª, u(w, x, Œª)).

Then
` ¬¥
sup |hU (Œª, a) ‚àí h(Œª, a)| = O Œª2 ,
a

and

sup
w‚ààW,x‚ààX

` ¬¥
|kU (w, x, Œª) ‚àí k(w, x, Œª)| = O Œª2 .

Combined with the smoothness assumptions, this implies that
ÀÜ
Àú
ÀÜ
Àú
‚àí1
‚àí1
Œ≤ lr,v (Œª) ‚àí Œ≤ lr(Œª) = E g(FW
(k(Wi , Xi , Œª)), Xi ) ‚àí E g(FW
(kU (Wi , Xi , Œª)), Xi ) = O(Œª2 ).

This finishes the proof of (C.1).
Next, we prove (C.2). Let c1 and c2 satisfy

sup |k(w, x, Œª + Œ≥) ‚àí k(w, x, Œª)| ‚â§ c1 ¬∑ Œ≥,

x,w,Œ≥,Œª

[72]

and
Àõ 2
Àõ
Àõ ‚àÇ
Àõ
Àõ
sup Àõ 2 g(w, x)ÀõÀõ ‚â§ c2 ,
w‚ààW,x‚ààX ‚àÇw

‚àí1
respectively. Then, applying Lemma A.1 with f (a) = g(FW
(a), x) and h(Œª) = k(w, x, Œª), we obtain
Àõ
!Àõ
‚àí1
Àõ
‚àÇ
Àõ
Àõ
Àõ
‚àí1
‚àí1
‚àÇw g(FW (k(w, x, 0), x)
(k(w, x, Œª) ‚àí k(w, x, 0)) Àõ
Àõg(FW (k(w, x, Œª), x) ‚àí g(FW (k(w, x, 0), x) +
‚àí1
Àõ
Àõ
fW (FW (k(w, x, 0)))

‚â§ c2 c21 Œª2 = o(Œª).

Since the bound does not depend on x and w, we can average over W and X and it follows that
Àõ
#Àõ
"
Àõ
Àõ ÀÜ
‚àÇ
Àú
g(W, X)
Àõ
Àõ
‚àí1
‚àÇw
(k(W, X, Œª) ‚àí W ) Àõ ,
ÀõE g(FW (k(W, X, Œª), X) ‚àí E [g(W, X)] ‚àí E
Àõ
Àõ
fW (W )
where we also use the fact that k(w, x, 0) = FW (w). This finishes the proof of (C.2).
Now we prove (C.3). By definition,
h(Œª, a) = Pr (VŒª,i < a) = Pr (VŒª,i < a, Wi < wm ) + Pr (VŒª,i < a, Wi ‚â• wm )
= Pr (Œª ¬∑ Xi ¬∑ d(Wi , Xi ) + Wi ‚â§ a, Wi < wm )

+Pr (Œª ¬∑ Xi ¬∑ d(Wi , Xi ) + Wi ‚â§ a, Wi ‚â• wm ) .

= Pr (Œª ¬∑ Xi ¬∑ (Wi ‚àí wl ) + Wi ‚â§ a, Wi < wm )

+Pr (Œª ¬∑ Xi ¬∑ (wu ‚àí Wi ) + Wi ‚â§ a, Wi ‚â• wm )
‚Äû
¬´¬´
a + Œª ¬∑ Xi ¬∑ w l
= Pr Wi ‚â§ min wm ,
1 + Œª ¬∑ Xi
‚Äû
¬´
a ‚àí Œª ¬∑ Xi ¬∑ w u
+Pr wm ‚â§ Wi ‚â§
.
1 ‚àí Œª ¬∑ Xi
For Œª sufficiently close to zero, we can write this as
‚Äû
¬´
a + Œª ¬∑ Xi ¬∑ w l
h(Œª, a) = 1a>wm ¬∑ Pr(Wi ‚â§ wm ) + 1a‚â§wm ¬∑ Pr Wi ‚â§
1 + Œª ¬∑ Xi
‚Äû
¬´
a ‚àí Œª ¬∑ Xi ¬∑ w u
+1a>wm ¬∑ Pr wm < Wi ‚â§
1 ‚àí Œª ¬∑ Xi
‚Äû
¬´
a + Œª ¬∑ Xi ¬∑ w l
= 1a‚â§wm ¬∑ Pr Wi ‚â§
1 + Œª ¬∑ Xi
‚Äû
¬´
a ‚àí Œª ¬∑ Xi ¬∑ w u
+1a>wm ¬∑ Pr Wi ‚â§
1 ‚àí Œª ¬∑ Xi
Àõ ¬´‚Äì
¬ª ‚Äû
a + Œª ¬∑ Xi ¬∑ wl ÀõÀõ
= 1a‚â§wm ¬∑ E Pr Wi ‚â§
Xi
1 + Œª ¬∑ Xi Àõ
Àõ ¬´‚Äì
¬ª ‚Äû
a ‚àí Œª ¬∑ Xi ¬∑ wu ÀõÀõ
+1a>wm ¬∑ E Pr Wi ‚â§
Xi
1 ‚àí Œª ¬∑ Xi Àõ
Àõ ¬´
‚Äû
Z
a + Œª ¬∑ z ¬∑ wl ÀõÀõ
= 1a‚â§wm ¬∑ FW |X
z fX (z)dz
1+Œª¬∑z Àõ
Àõ ¬´
‚Äû
Z
a ‚àí Œª ¬∑ z ¬∑ wu ÀõÀõ
+1a>wm ¬∑ FW |X
z fX (z)dz
1‚àíŒª¬∑z Àõ
‚Äû

Substituting a = Œª ¬∑ x ¬∑ d(w, x) + w, we get
Àõ ¬´
‚Äû
Z
Œª ¬∑ x ¬∑ d(w, x) + w + Œª ¬∑ z ¬∑ wl ÀõÀõ
k(w, x, Œª) = 1Œª¬∑x¬∑d(w,x)+w‚â§wm ¬∑ FW |X
Àõ z fX (z)dz
1+Œª¬∑z
Àõ ¬´
‚Äû
Z
Œª ¬∑ x ¬∑ d(w, x) + w ‚àí Œª ¬∑ z ¬∑ wu ÀõÀõ
+1Œª¬∑x¬∑d(w,x)+w>wm ¬∑ FW |X
Àõ z fX (z)dz
1‚àíŒª¬∑z

[73]

Àõ ¬´
Œª ¬∑ x ¬∑ (w ‚àí wm) + w + Œª ¬∑ z ¬∑ wl ÀõÀõ
Àõ z fX (z)dz
1+Œª¬∑z
Àõ ¬´
‚Äû
Z
Œª ¬∑ x ¬∑ (wu ‚àí w) + w + Œª ¬∑ z ¬∑ wl ÀõÀõ
+1Œª¬∑x¬∑(wu ‚àíw)+w‚â§wm 1w>wm ¬∑ FW |X
Àõ z fX (z)dz
1+Œª¬∑z
Àõ ¬´
‚Äû
Z
Œª ¬∑ x ¬∑ (w ‚àí wm ) + w ‚àí Œª ¬∑ z ¬∑ wu ÀõÀõ
+1Œª¬∑x¬∑(w‚àíwl )+w>wm ¬∑ 1w‚â§wm FW |X
Àõ z fX (z)dz
1‚àíŒª¬∑z
Àõ ¬´
‚Äû
Z
Œª ¬∑ x ¬∑ (wu ‚àí w) + w ‚àí Œª ¬∑ z ¬∑ wu ÀõÀõ
+1Œª¬∑x¬∑(wu ‚àíw)+w>wm ¬∑ 1w>wm FW |X
Àõ z fX (z)dz
1‚àíŒª¬∑z
Àõ ¬´
‚Äû
Z
Œª ¬∑ x ¬∑ (w ‚àí wm ) + w + Œª ¬∑ z ¬∑ wl ÀõÀõ
= 1w‚â§wm (1+Œªxwl /wm )/(1+Œªx) ¬∑ FW |X
Àõ z fX (z)dz
1+Œª¬∑z
Àõ ¬´
‚Äû
Z
Œª ¬∑ x ¬∑ (wu ‚àí w) + w + Œª ¬∑ z ¬∑ wl ÀõÀõ
+0 ¬∑ FW |X
Àõ z fX (z)dz
1+Œª¬∑z
Àõ ¬´
‚Äû
Z
Œª ¬∑ x ¬∑ (w ‚àí wm ) + w ‚àí Œª ¬∑ z ¬∑ wu ÀõÀõ
+1wm (1+Œªxwl /wm )/(1+Œªx)‚â§w‚â§wm ¬∑ FW |X
Àõ z fX (z)dz
1‚àíŒª¬∑z
Àõ
‚Äû
¬´
Z
Œª ¬∑ x ¬∑ (wu ‚àí w) + w ‚àí Œª ¬∑ z ¬∑ wu ÀõÀõ
+1w‚â•wm ¬∑ FW |X
Àõ z fX (z)dz.
1‚àíŒª¬∑z
= 1Œª¬∑x¬∑(w‚àíwl )+w‚â§wm 1w‚â§wm ¬∑

Z

FW |X

‚Äû

The last equality uses the following four facts: (i), Œª ¬∑ x ¬∑ (w ‚àí wl ) + w ‚â§ wm implies w ‚â§ wm(1 + Œªxwl /wm )/(1 +
Œªx) ‚â§ wm , (ii) Œª¬∑x¬∑(wu‚àíw)+w ‚â§ wm implies w ‚â§ wm (1‚àíŒªxwu /wm )/(1‚àíŒªx) < wm , (iii), Œª¬∑x¬∑(w‚àíwl)+w > wm
implies w ‚â• wm (1+Œªxwl/wm )/(1+Œªx), and (iv) Œª¬∑x¬∑(wu‚àíw)+w > wm implies w ‚â• wm (1‚àíŒªxwu/wm )/(1‚àíŒªx).
Now we will look at
‚Äì
¬ª
1
‚àÇg
(Wi , Xi )
k(Wi , Xi , Œª)
E
‚àÇw
fW (Wi )
Z xu Z wu
‚àÇg
1
=
(w, x)
k(w, x, Œª)fW,X (w, x)dwdx.
‚àÇw
f
(w)
W
xl
wl
Substituting the three terms of k(w, x, Œª) in here we get
¬ª
‚Äì
‚àÇg
1
E
(Wi , Xi )
k(Wi , Xi , Œª)
‚àÇw
fW (Wi )
Z

=

xu

xl

+

Z

xu
xl

Z

wm

1+Œªxwl /wm
1+Œªx

‚àÇg
‚àÇw

wm
wm

FW |X

Z

FW |X

(w, x)
fW (w)

wl

Z

Z

‚àÇg
‚àÇw

(w, x)
fW (w)

(1+Œªxwl /wm
1+Œªx

‚Äû

Àõ ¬´
Œª ¬∑ x ¬∑ (w ‚àí wl) + w + Œª ¬∑ z ¬∑ wl ÀõÀõ
Àõ z fX (z)dzfW,X (w, x)dwdx
1+Œª¬∑z

‚Äû

Àõ ¬´
Œª ¬∑ x ¬∑ (w ‚àí wl ) + w ‚àí Œª ¬∑ z ¬∑ wu ÀõÀõ
Àõ z fX (z)dzfW,X (w, x)dwdx
1‚àíŒª¬∑z

(C.5)

(C.6)

+

Z

xu

xl

Z

wu ‚àÇg
‚àÇw

wm

(w, x)
fW (w)

Z

FW |X

‚Äû

Àõ ¬´
Œª ¬∑ x ¬∑ (wu ‚àí w) + w ‚àí Œª ¬∑ z ¬∑ wu ÀõÀõ
Àõ z fX (z)dzfW,X (w, x)dwdx
1‚àíŒª¬∑z

(C.7)

Next, we take the derivative with respect to Œª for each of these three terms, and evaluate that derivative at
Œª = 0. For the first term, (C.5) this derivative consists of two terms, one corresponding to the derivative with
respect to the Œª in the bounds of the integral, and one corresponding to the derivative with respect to Œª in the
integrand. For the second term we only have the term corresponding to the derivative with respect to the Œª in
the bounds of the integral since the other term vanishes when we evaluate it at Œª = 0. The third term, (C.6)
only has Œª in the integrand. So,
¬ª
‚ÄìÀõ
Àõ
‚àÇ
‚àÇg
1
E
(Wi , Xi )
k(Wi , Xi , Œª) ÀõÀõ
‚àÇŒª
‚àÇw
fW (Wi )
Œª=0
Àõ
¬ª
‚Äì
Àõ
‚àÇ
= (wl ‚àí wm ) ¬∑ E
g(wm , Xi )ÀõÀõ Wi = wm
‚àÇw

[74]

Z xu
‚àÇ
1
g(w, x)
fW |X (w|z) (x ¬∑ (w ‚àí wl ) + z ¬∑ wl ‚àí z ¬∑ w) fX (z)dzfW,X (w, x)dwdx
‚àÇw
fW (w) xl
xl
wl
Àõ
‚Äì
¬ª
Àõ
‚àÇ
g(wm , Xi )ÀõÀõ Wi = wm
‚àí(wl ‚àí wm) ¬∑ E
‚àÇw
Z xu Z wm
Z xu
‚àÇ
1
+
g(w, x)
fW |X (w|z) (x ¬∑ (wu ‚àí w) + z ¬∑ w ‚àí z ¬∑ wu ) fX (z)dzfW,X (w, x)dwdx
‚àÇw
fW (w) xl
xl
wl
Z xu Z wm
Z xu
‚àÇ
=
g(w, x)
fX|W (z|w) (x ¬∑ d(w, x) ‚àí z ¬∑ d(w, z)) dzfW,X (w, x)dwdx
‚àÇw
xl
wl
xl
Z xu Z wm
Z xu
‚àÇ
+
g(w, x)
fX|W (z|w) (x ¬∑ d(w, x)) ‚àí z ¬∑ d(w, z)) dzfW,X (w, x)dwdx
‚àÇw
xl
wl
xl
Z xu Z wu
Z xu
‚àÇ
=
g(w, x)
fX|W (z|w) (x ¬∑ d(w, x) ‚àí z ¬∑ d(w, z)) dzfW,X (w, x)dwdx
‚àÇw
xl
wl
xl
Z xu Z wu
‚àÇ
=
g(w, x) ((Xi ¬∑ d(w, Xi) ‚àí E [ Xi ¬∑ d(w, Xi ))| Wi = w]) fW,X (w, x)dwdx
‚àÇw
xl
wl
¬ª
‚Äì
‚àÇg
(Wi , Xi ) ¬∑ (Xi ¬∑ d(Wi , Xi) ‚àí E [ Xi ¬∑ d(Wi , Xi )| Wi ]) = Œ≤ lc,v .
=E
‚àÇw
+

Z

xu

Z

wm

This finishes the proof of (C.3).
Finally, we show (C.4), by showing the equality of
¬ª
‚Äì
‚àÇg
Œ≤ lc,v = E
(Wi , Xi) ¬∑ (Xi ¬∑ d(Wi ) ‚àí E [ Xi ¬∑ d(Wi )| Wi ]) ,
‚àÇw

(C.8)

and
‚Äì
¬ª
‚àÇ2 g
(Wi , Xi) .
E Œ¥ (Wi , Xi ) ¬∑
‚àÇw‚àÇx
Define

(C.9)

Àõ
‚Äì
Àõ
‚àÇg
(w, Xi) ¬∑ (Xi ¬∑ d(w) ‚àí E [ Xi ¬∑ d(w)| Wi = w])ÀõÀõ Wi = w
‚àÇw
Àõ
¬ª
‚Äì
Àõ
‚àÇg
=E
(w, Xi ) ¬∑ d(w) ¬∑ (Xi ‚àí E [ Xi | Wi = w])ÀõÀõ Wi = w ,
‚àÇw

b(w) = E

¬ª

so that Œ≤ lc,v = E[b(W )]. Apply Lemma A.2, with h(x) =
¬ª 2
‚Äì
‚àÇ
b(w) = E
g(w, X) ¬∑ Œ¥(w, X)
‚àÇw‚àÇx

‚àÇg
‚àÇw

(w, x) ¬∑ d(w), to get

with
Œ¥(w, x) = d(w) ¬∑

FX|W (x|w) ¬∑ (1 ‚àí FX|W (x|w))
¬∑ (E [X|X > x, W = w] ‚àí E [X|X ‚â§ x, W = w]) .
fX|W (x|w)

Thus
Œ≤ lc,v = E[b(W )] = E

¬ª

‚Äì
‚àÇ2
g(W, X) ¬∑ Œ¥(W, X) .
‚àÇw‚àÇx


Proof of Theorem 4.1: We apply Lemmas A.15-A.18. The assumptions in the theorem imply that the
conditions for those lemmas are satisfied. 
Proof of Theorem 4.2: The proof is essentially the same as that for Theorem 4.1 and is omitted. 
Proof of Theorem 4.3: We apply Lemma‚Äôs A.24-A.28 to get an asymptotic linear representation for Œ≤ÃÇ cm(œÅ, œÑ ).
The assumptions in the Theorem imply that the conditions for the applications of these lemmas are satisfied.
Therefore, by Lemma A.24, we have
‚Äú
‚Äù ‚Äú
‚Äù ‚Äú
‚Äù
‚Äú
‚Äù
cm
cm
Œ≤ÃÇ cm(œÅ, 0) = Œ≤ cm (œÅ, 0) + Œ≤ÃÇgcm ‚àí g cm + Œ≤ÃÇW
‚àí gcm + Œ≤ÃÇX
‚àí gcm + (gcm ‚àí Œ≤ cm(œÅ, 0)) + op N ‚àí1/2 .

[75]

By Lemmas A.25-A.28, this is equal to
Œ≤ cm(œÅ, 0)+

N
‚Äú
‚Äù
¬Ø
1 X Àò cm
cm
cm
œàg (Yi , Wi , Xi ) + œàW
(Yi , Wi , Xi ) + œàX
(Yi , Wi , Xi ) + œà0cm (Yi , Wi , Xi ) +op N ‚àí1/2
N i=1

= Œ≤ cm(œÅ, 0) +

N
‚Äú
‚Äù
1 X
œà(Yi , Wi , Xi ) + op N ‚àí1/2 ,
N i=1

cm
cm
with œàgcm (y, w, x) given in (4.34), œàW
(y, w, x) given in (4.35), œàX
(y, w, x) given in (4.36), œà0cm (y, w, x) given in
(4.33), and œà(y, w, x) given in (4.37). Then we have an asymptotic linear representation for Œ≤ÃÇ cm(œÅ, œÑ ):

Œ≤ÃÇ cm(œÅ, œÑ ) = œÑ ¬∑ Y + (1 ‚àí œÑ ) ¬∑ Œ≤ÃÇ cm(œÅ, 0)

‚Äú
‚Äù
= Œ≤ cm (œÅ, œÑ ) + œÑ ¬∑ (Y ‚àí Œ≤ cm(œÅ, 1)) + (1 ‚àí œÑ ) ¬∑ Œ≤ÃÇ cm(œÅ, 0) ‚àí Œ≤ cm(œÅ, 0)
= Œ≤ cm (œÅ, œÑ ) + œÑ ¬∑ (Y ‚àí Œ≤ cm(œÅ, 1)) + (1 ‚àí œÑ ) ¬∑

N
1 X
œà(Yi , Wi , Xi ).
N i=1

P
Since by a law of large numbers Y ‚Üí Œ≤ cm (œÅ, 1), and i œà(Yi , Wi , Xi )/N ‚Üí E[œà(Yi , Wi , Xi )] = 0, it follows that
Œ≤ÃÇ cm (œÅ, œÑ ) ‚Üí Œ≤ cm(œÅ, œÑ ). By a central limit theorem the second part of the Theorem follows. 
Proof of Theorem 4.4: The proof uses Lemmas A.13, A.14, A.19, A.20, and A.23.
By the conditions on q, r, s, and Œ¥, Lemma A.13 implies that for some Œ∑ > 1/4
`
¬¥
sup |mÃÇ(w) ‚àí m(w)| = op N ‚àíŒ∑ .
w‚ààW

Moreover, by the same conditions, Lemma A.14 implies that for some Œ∑ > 1/4,
Àõ
Àõ
Àõ ‚àÇgÃÇ
Àõ
`
¬¥
‚àÇg
sup ÀõÀõ
(w, x) ‚àí
(w, x)ÀõÀõ = op N ‚àíŒ∑ .
‚àÇw
w‚ààW,x‚ààX ‚àÇw
Then, the conditions for Lemma A.19 are satisfied, so we can write
‚Äù
‚àö ‚Äú lc
N Œ≤ÃÇ ‚àí Œ≤ lc
N
‚àö
1 X ‚àÇg
= ‚àö
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (X ‚àí m(Wi )) ‚àí N ¬∑ Œ≤ lc
N i=1 ‚àÇw

N
N
1 X ‚àÇg
1 X ‚àÇgÃÇ
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (X ‚àí m(Wi )) ‚àí ‚àö
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (X ‚àí m(Wi ))
+‚àö
N i=1 ‚àÇw
N i=1 ‚àÇw

N
N
1 X ‚àÇg
1 X ‚àÇg
+‚àö
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (X ‚àí mÃÇ(Wi )) ‚àí ‚àö
(Wi , Xi ) ¬∑ d(Wi ) ¬∑ (X ‚àí m(Wi )) + op (1).
N i=1 ‚àÇw
N i=1 ‚àÇw

By Lemma A.20,

N
N
1 X ‚àÇ
1 X ‚àÇ
‚àö
gb (Wi , Xi) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m (Wi )) ‚àí ‚àö
g (Wi , Xi ) ¬∑ d(Wi ) ¬∑ (Xi ‚àí m (Wi ))
N i=1 ‚àÇw
N i=1 ‚àÇw

where

N
1 X lc
= ‚àö
œàg (Yi , Wi , Xi) + op (1),
N i=1

œàglc (y, w, x) = ‚àí

1
‚àÇfW,X (w, x)
(y ‚àí g (w, x)) d(w) (x ‚àí m (w))
fW,X (w, x)
‚àÇW
‚àÇm (w)
d(w) (y ‚àí g (w, x))
‚àÇW
‚àÇ
+
d(w) (x ‚àí m(w)) (y ‚àí g(w, x)) .
‚àÇw

‚àí

[76]

By Lemma A.23,
N
N
1 X ‚àÇ
1 X ‚àÇ
‚àö
g (Wi , Xi) ¬∑ d(Wi ) ¬∑ m
b (Wi ) ‚àí ‚àö
g (Wi , Xi ) ¬∑ d(Wi ) ¬∑ m (Wi )
N i=1 ‚àÇw
N i=1 ‚àÇw
N
1 X lc
œàm (Yi , Wi , Xi ) + op (1),
= ‚àö
N i=1

where

lc
œàm
(y, w, x) = E

¬ª

Àõ
‚Äì
‚àÇg(w, Xi ) ÀõÀõ
W
=
w
¬∑ d(w) ¬∑ (x ‚àí m(w)).
i
Àõ
‚àÇW

Combining these results implies that
‚àö

N
‚Äú
‚Äù
1 X lc
œà (Yi , Wi , Xi ) + op(1),
N Œ≤ÃÇ lc ‚àí Œ≤ lc = ‚àö
N i=1

with
œàlc (y, w, x) =

‚Äû

‚àÇg(w, x)
¬∑ d(w) ¬∑ (x ‚àí m(w)) ‚àí Œ≤ lc
‚àÇw

¬´

lc
+ œàglc (y, w, x) + œàm
(y, w, x).

Using a law of large numbers then implies the first result in the theorem, and using a central limit theorem
implies the second result in the Theorem. 

[77]

Notation: (page number indicates where it was first introduced)
(Yi , Wi , Xi, Vi) observed variables for unit i, i = 1, . . . , N . Yi , Wi , Xi are scalars, Vi is vector.
(page 3)
k(w, x, v, Œµ) is production function (page 3)
g(w, x, v) is average production function (conditional expectation of Y given (W, X, V )). (page
4, equation 2.2)
œÉ 2 (w, x, v) is conditional variance of Y given (W, X, V )). (page 4, equation 2.3)
gW (w, x, v) is derivative of average production function. (page 4, equation 2.4)
hW |X,V (w|x, v) is potential conditional distribution of W given X and V (page 6)
fW |X,V (w|x, v) is conditional distribution of W given X and V (page 6)
Œ≤hare is output given new allocation indexed by h. (page 6)
FW |V (w|v) denotes conditional distribution function of W given V . (page 6)
Œ≤ pam is positive assortive matching output (page 6, equation (3.6))
Œ≤ pam‚àípop is alternative positive assortive matching output (page 7, equation (3.7))
Œ≤ nam is negative assortive matching output (page 7, equation (3.8))
Œ≤ sq is status quo output (page 8)
Œ≤ rm is random matching output (page 8)
œÜ(x1 , x2 , œÅ) bivariate normal density with correlation œÅ, (page 8)
Œ¶(x1 , x2 , œÅ) bivariate normal distribution with correlation œÅ, (page 8)
œÜc (x1 , x2 , œÅ) truncated bivariate normal density with correlation œÅ, (page 8)
Œ¶c (x1 , x2 , œÅ) truncated bivariate normal distribution with correlation œÅ, (page 9)
HW,X (w, mx) joint distribution function from truncated bivariate normal cupola (page 9).
hW,X (w, mx) joint density function from truncated bivariate normal cupola (page 9).
Œ≤ cm (œÅ, œÑ ) correlated matching estimand (page 8, and page 9, equation (3.10))
d(w) weight function in local complementarity measure (page 10)
UŒª combination of W and X for local allocation (page 10)
Œ≤ lr (Œª) path of local reallocations page 10, equation (3.11))
Œ≤ lc local reallocation measure (page 11, equation (3.12)
W support of W (page 11, assumption 3.1)
[78]

X support of X (page 11, assumption 3.1)
Œ¥(w, x) weight function in local complementarity measure in representation as weighted average
of cross derivative (page 11)
q is the number of derivatives of g and fW X (page 11).
FÃÇW (w) estimate of cumulative distribution function for W (page 12)
m(w) (page 12, equation 4.15)
h1 (w, x) = fW,X (w, x) notation for density and product of density and regression function
(page 12)
h2 (w, x) = g(w, x) ¬∑ FW,X (w, x) notation for density and product of density and regression
function (page 12, equation (4.16))
YÃÉ = (YÃÉi1 , YÃÉi2 ) with YÃÉi1 = 1, YÃÉi2 = Yi (page 12)
hÃÇnw,m (w, x) nadaraya-watson kernel estimator for hm (w, x) (page 12, equation (4.17))
z = (w, x)0 and Z = (W, X)0 compact notation for pair of covariates (page 13)
L dimension of Z (is equal to 2 (page 13)
Œª vector of nonnegative integers of dimension L = 2 (page 13)
Q
Œªl
zŒª = L
l=1 zl (page 13)
g (Œª)(z) =

‚àÇg |Œª|
(z)
‚àÇzŒª

(page 13)

ZIb internal region (page 13, equation (4.18)
ZB
b boundary region (page 13, equation (4.19)
t(z; g, r, p) taylor series expansion evaluated at z, equation (4.20)
rb (z) projection on internal region (page 14, equation 4.21)
hÃÇm,nip,s (z) NIP estimator for hm (z) (page 14, equation 4.22)
gÃÇnip,s (w, x) NIP estimator for g(w, x) (page 14, equation 4.23)
\
‚àÇ gnip,s
‚àÇw

(w, x) NIP estimator for derivative of g(w, x) 4.24

gÃÇ(w, x) = gÃÇnip,s (w, x) NIP estimator for g(w, x) short hand for NIP estimator (page 14)
derivative order of kernel is defined in definition 4.1 on page 14
K(¬∑) bivariate kernel (page 15)
K(¬∑) univariate kernel (page 15)
U support of bivariate kernel (page 15).
[79]

r is number of derivatives of kernel K(u) (page 15)
s is order of kernel K(u), and order of NIP kernel estimator (page 14, 15)
d is derivative order of kernel K(u) (page 15)
bN = N ‚àíŒ¥ is bandwidth (page 15)
Œ≤ÃÇ pam estimator for Œ≤ pam (page 15, equation 4.25)
Œ≤ÃÇ nam estimator for Œ≤ nam (page 15, equation 4.26)
Œ≤ÃÉ pam estimator for Œ≤ pam given known g(¬∑) (page 15, equation 4.27)
q pam (w, x) (page 16)
pam
(w) (page 16)
œàW

r pam (x, z) (page 16)
pam
œàX
(x) (page 16)

‚Ñ¶pam
11 (page 16)
‚Ñ¶pam
22 (page 16)
Œ≤ÃÉ nam (page 16, equation 4.29)
q nam (w, x) (page 17)
nam(w) (page 17)
œàW
nam (x, z) (page 17)
rXZ
nam(x) (page 17)
œàX

‚Ñ¶nam
11 (page 17)
‚Ñ¶nam
22 (page 17)
Œ≤ÃÇ cm (œÅ, œÑ ) (page 17)
Œ≤ÃÇ sq (page 17)
Œ∑(w) (page 18, equation 4.30)
d(w, x) (page 18, equation 4.31)
m(Y, W, Œ≤ cm(œÅ, œÑ ), Œ∑ (W )) moment function (page 18)
eW (w, x) (page 18)
eX (w, x) (page 18)
œâ cm (w, x) (page 18)

[80]

œà0cm (y, w, x) (page 19)
œàgcm (y, w, x) (page 19)
cm
œàW
(y, w, x) (page 19)
cm
œàX
(y, w, x) (page 19)

‚Ñ¶cm (page 19)
œà cm (y, w, x) (page 19)
‚Ñ¶lc (page 20)
œàglc(y, w, x) (page 20)
lc
œàm
(y, w, x) (page 20)
pam

Œ≤ÃÇg

(page 21, equation 5.39)

pam
Œ≤ÃÇW
(page 21, equation 5.40)
pam
(page 21, equation 5.41)
Œ≤ÃÇX

g pam (page 21, equation 5.42)
Z = (W, X)0 (page 29)
œâ(Z) and œâ(X) (page 29)
n(h[Œª] ) and n(h) (page 29)
t(x) (page 29)
Œ∏fm (page 29, A.2)
Œ∏¬± (page 29, A.3)
Œ∏ÃÇfm (page 29)
Œ∏ÃÇpm (page 29)
Œ∏
Œ∏

fm
pm

(page 29)
(page 29)

(Œ∫)

Œ±Œ∫1 (z) (page 30)
Œ±m (z) (page 30)
V1 , V2 (page 30)
Œ≤ÃÇglc (page 31)
lc (page 31)
Œ≤ÃÇm

[81]

g lc (page 31)
g cm (page 33)
Œ≤ÃÇgcm (page 33)
cm
Œ≤ÃÇW
(page 33)
cm (page 33)
Œ≤ÃÇW
cm
Œ≤ÃÇX
(page 33)

[82]

