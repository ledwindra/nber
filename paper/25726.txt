                                 NBER WORKING PAPER SERIES




                    FAST, "ROBUST", AND APPROXIMATELY CORRECT:
                         ESTIMATING MIXED DEMAND SYSTEMS

                                             Bernard SalaniÃ©
                                             Frank A. Wolak

                                          Working Paper 25726
                                  http://www.nber.org/papers/w25726


                       NATIONAL BUREAU OF ECONOMIC RESEARCH
                                1050 Massachusetts Avenue
                                  Cambridge, MA 02138
                                       April 2019




We are grateful to Dan Ackerberg, John Asker, Steve Berry, Xiaohong Chen, Chris Conlon, Pierre
Dubois, Jeremy Fox, Han Hong, Guy Laroque, Simon Lee, Arthur Lewbel, Thierry Magnac, Lars
Nesheim, Ariel Pakes, Mathias Reynaert, Tobias Salz, Richard Smith, Pedro Souza, Frank Verboven,
Martin Weidner, and Ken Wolpin for their useful comments, as well as to seminar audiences at NYU,
Rice, UCL, and the Stanford Institute for Theoretical Economics (SITE). We also thank Zeyu Wang
for excellent research assistance. The views expressed herein are those of the authors and do not necessarily
reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

Â© 2019 by Bernard SalaniÃ© and Frank A. Wolak. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
Â© notice, is given to the source.
Fast, "Robust", and Approximately Correct: Estimating Mixed Demand Systems
Bernard SalaniÃ© and Frank A. Wolak
NBER Working Paper No. 25726
April 2019
JEL No. L00,L13

                                          ABSTRACT

Many econometric models used in applied work integrate over unobserved heterogeneity. We
show that a class of these models that includes many random coefficients demand systems can be
approximated by a â€œsmall-Ïƒâ€ expansion that yields a linear two-stage least squares estimator. We
study in detail the models of product market shares and prices popular in empirical IO. Our
estimator is only approximately correct, but it performs very well in practice. It is extremely fast
and easy to implement, and it is â€œrobustâ€ to changes in the higher moments of the distribution of
the random coefficients. At the very least, it provides excellent starting values for more
commonly used estimators of these models.


Bernard SalaniÃ©
Department of Economics
Columbia University
1033 International Affairs Building, MC 3308
420 West 118th Street
New York, NY 10027
and CEPR
bs2237@columbia.edu

Frank A. Wolak
Department of Economics
Stanford University
Stanford, CA 94305-6072
and NBER
wolak@zia.stanford.edu
Introduction
Many econometric models are estimated from conditional moment conditions that
express the mean independence of random unobservable terms Î· and instruments Z:

                                      E pÎ·|Zq â€œ 0.

In structural models, the unobservable term is usually obtained by solving a set of
equationsâ€”often a set of first-order conditionsâ€”that define the observed endogenous
variables as functions of the observed exogenous variables and unobservables. That
is, we start from
                                    Gpy, Î·, Î¸0 q â€œ 0                            (1)
where y is the vector of observed endogenous variables and Î¸0 is the true value of the
vector of unknown parameters. The parametric function G is assumed to be known
and can depend on a vector of observed exogenous variables. Then (assuming that
the solution exists and is unique) we invert this system into

                                      Î· â€œ F py, Î¸0 q

and we seek an estimator of Î¸0 by minimizing an empirical analog of a norm

                                  kE pF py, Î¸qmpZqqk

where mpZq is a vector of measurable functions of Z.
    Unless F py, Î¸0 q exists in closed form, inversion often is a step fraught with diffi-
culties. Even when a simple inversion algorithm exists, it is still costly and must be
done with a high degree of numerical precision, as errors may jeopardize the â€œouterâ€
minimization problem. One alternative is to minimize an empirical analog of the
norm
                                       kE pÎ·mpZqqk
subject to the structural constraints (1). This â€œMPEC approachâ€ has met with
some success in dynamic programming and empirical industrial organization (Su and
Judd 2012, DubeÌ et al 2012). It still requires solving a nonlinearly constrained,
nonlinear objective function minimization problem; convergence to a solution can
be a challenging task in the absence of very good initial values. This is especially

                                            2
galling when the model has be estimated many times, for instance within models of
bargaining like those of Crawford and Yurokoglu (2012) or Ho and Lee (2017).
    We propose an alternative that derives a linear model from a very simple series
expansion. To fix ideas, suppose that Î¸0 can be decomposed into a pair pÎ²0 , Ïƒ0 q, where
Ïƒ0 is a scalar that we have reasons to think is not too far from zero. We rewrite (1)
as
                             Gpy, F py, Î²0 , Ïƒ0 q, Î²0 , Ïƒ0 q â€œ 0.
We expand Ïƒ Ã‘ F py, Î²0 , Ïƒq in a Taylor series around 0 and re-write F py, Î²0 , Ïƒ0 q as:

                                                                                      Ïƒ0L
   F py, Î²0 , Ïƒ0 q â€œ F py, Î²0 , 0q ` FÏƒ py, Î²0 , 0qÏƒ0 ` . . . ` FÏƒÏƒ...Ïƒ py, Î²0 , 0q       ` OpÏƒ0L`1 q,
                                                                                      L!
where the subscript Ïƒ denotes a partial derivative with respect to the argument Ïƒ.
   This suggests a sequence of â€œapproximate estimatorsâ€ that minimize the empirical
analogs of the following norms

                    kE pF py, Î², 0qmpZqqk
                    kE ppF py, Î², 0q ` FÏƒ py, Î², 0qÏƒq mpZqqk
                    â€º Ë†Ë†                                              Ïƒ2
                                                                         Ë™     Ë™â€º
                    â€ºE    F py, Î², 0q ` FÏƒ py, Î², 0qÏƒ ` FÏƒÏƒ py, Î², 0q      mpZq â€º
                    â€º                                                           â€º
                                                                      2
              ...

If the true value Ïƒ0 is not too large, one may hope to obtain a satisfactory estimator
with the third of these â€œapproximate estimators.â€ In general, this still requires solving
a nonlinear minimization problem. However, suppose that the function F satisfies
the following three conditions:

C1: FÏƒ py, Î²0 , 0q â€ 0

C2: F py, Î², 0q â€ f0 pyq Â´ f1 pyqÎ² is affine in Î² for known functions f0 pÂ¨q and f1 pÂ¨q.

C3: the second derivative FÏƒÏƒ py, Î², 0q does not depend on Î².

Denote f2 pyq â€ Â´FÏƒÏƒ py, Î², 0q. Under C1â€“C3, we would minimize
                    â€º Ë†Ë†                              Ïƒ2
                                                         Ë™     Ë™â€º
                    â€ºE      f0 pyq Â´ f1 pyqÎ² Â´ f2 pyq      mpZq â€º.
                    â€º                                           â€º
                                                      2

                                                   3
Taking the parameters of interest to be pÎ²0 , Ïƒ02 q, this is simply a two-stage least
squares regression of f0 pyq on f1 pyq and f2 pyq with instruments mpZq. As this is
a linear problem, the optimal1 instruments mpZq associated with the conditional
moment restrictions EpÎ·|Zq â€œ 0 are simply

                          mpZq â€œ Z Ëš â€œ pE pf1 pyq|Zq , E pf2 pyq|Zqq .

These optimal instuments could be estimated directly from the data using nonpara-
metric regressions. Or more simply, we can include flexible functions of the columns
of Z in the instruments used to compute the 2SLS estimates.
   The resulting estimators of Î²0 and Ïƒ02 are only approximately correct, because
they consistently estimate an approximation of the original model. On the other
hand, they can be estimated in closed form using linear 2SLS. Moreover, because
they only rely on limited features of the data generating process, they are â€œrobustâ€
in ways that we will explore later.
   Conditions C1â€“C3 extend directly to a multivariate parameter Ïƒ0 . They may
seem very demanding. Yet as we will show, under very weak conditions the Berry,
Levinsohn, and Pakes (1995) (macro-BLP) model that is the workhorse of empirical
IO satisfies all three. In this application, Ïƒ0 is taken to be the square root of the
varianceâ€“covariance matrix Î£ of the random coefficients in the mixed demand model.
More generally, we will characterize in Section 6.1 a general class of models with
unobserved heterogeneity to which conditions C1â€“C3 apply.
    Our approach builds on â€œsmall-Î£â€ approximations to construct successive approx-
imations to the inverse mapping (from market shares to product effects). Kadane
(1971) pioneered the â€œsmall-Ïƒâ€ method. He applied it to a linear, normal simulta-
neous equation system and studied the properties of k-class estimators2 when the
number of observations n is fixed and Ïƒ goes to zero. He showed that when the num-
ber of observations is large, under these â€œsmall-Ïƒ asymptoticsâ€ the k-class estimators
have biases in Ïƒ 2 , and that their mean-squared errors differ by terms of order Ïƒ 4 .
Kadane argued that small Ïƒ, fixed n asymptotics are often a good approximation to
finite-sample distributions when the estimation sample is large enough.
      The small-Ïƒ approach was used by Chesher (1991) in models with measurement er-
  1
      In the sense of Amemiya (1975).
  2
      Which include OLS and 2SLS.

                                               4
ror. Most directly related to us, Chesher and Santos-Silva (2002) used a second-order
approximation argument to reduce a mixed multinomial logit model to a â€œheterogene-
ity adjustedâ€ unmixed multinomial logit model in which mean utilities have additional
terms3 . They suggested estimating the unmixed logit and using a score statistic based
on these additional covariates to test for the null of no random variation in preferences.
Like them, we introduce additional covariates. Unlike them, we develop a method to
estimate jointly the mean preference coefficients and parameters characterizing their
random variation; and we only use linear instrumental variables estimators. To some
degree, our method is also related to that of Harding and Hausman 2007, who use a
Laplace approximation of the integral over the random coefficients in a mixed logit
model without choice-specific random effects. Unlike them, we allow for endogeneous
prices; our approach is also much simpler to implement.
    Section 1 presents the model popularized by Berryâ€“Levinsohnâ€“Pakes (1995) and
discusses some of the difficulties that practitioners have encountered when taking it
to data. We give a detailed description of our algorithm in section 2; readers not in-
terested in the derivation of our formulÃ¦ in fact can jump directly to our Monte Carlo
simulations in section 7. The rest of the paper justifies our algorithm (sections 3 and
4); studies its properties (section 5); and discusses a variety of extensions (section 6).



1       The macro-BLP model
Our leading example is taken from empirical IO. Much work in this area is based on
market share and price data. It has followed Berry et al (1995â€”hereafter BLP) in
specifying a mixed multinomial logit model with product-level random effects. To
deal with the endogeneity of prices implied by these product-level random effects,
BLP use a Generalized Method Moments (GMM) estimator that relies on the mean
independence of the product-level random effects and a set of instruments.
   To fix ideas, we define â€œthe standard modelâ€ as follows4 . Let J products be
available on each of T markets. Each market contains an infinity of consumers who
    3
     Ketz (2018) builds on a quadratic expansion in Ïƒ0 â€œ 0 to derive asymptotic distributions when
the true Ïƒ0 is on the boundary.
   4
     While some of our exposition relies on it for simplicity, our methods apply to a more general
modelâ€” see section 6.1.


                                                5
choose one of J products. Consumer i in market t derives a conditional indirect utility
from consuming product j equal to

                                   Xjt pÎ²0 ` i q ` Î¾jt ` uijt .

There is also a good 0, the â€œoutside goodâ€, whose utility for consumer i is typically
normalized to equal ui0t . The random variables  represent individual variation in
tastes for observed product characteristics, while the u stand for unobserved hetero-
geneity observed by the individual, but not by the econometrician. The vector  and
u are independent of each other, and of the covariates X and product random effects
Î¾. Berry et al. (1995) assume that the vector uit â€œ pui0t , ui1t , . . . , uiJt q is indepen-
dently and identically distributed (iid) as standard type-I Extreme Value (EV); the
product effects Î¾jt are unknown mean zero random variables conditional on a set of
instruments; and the random variation in preferences i has a mean-zero distribution
which is known up to its variance-covariance matrix Î£0 . The i are often taken to be
independent, identically distributed N p0, Î£0 q random vectors with a diagonal Î£0 .
    Some of the covariates in Xjt may be correlated with the product-specific random
effects. The usual example is a model of imperfect price competition where the prices
firms set in market t depend on the value of the vector of unobservable product
characteristics, Î¾t , some of which the firms observe.
    The parameters to be estimated are the mean coefficients Î²0 and the variance-
covariance matrix of the random coefficients Î£0 . We collect them in Î¸0 â€œ pÎ²0 , Î£0 q.
The data available consists of the market shares ps1t , . . . , sJt q and prices pp1t , . . . , pJt q1
of the J varieties of the good, of the covariates Xt , and of additional instruments
Zt , all for market t. Note that the market shares do not include information on
the proportion S0t of consumers who choose to buy good 0. Typically the analyst
estimates this from other sources. Let us assume that this is done, so that we can deal
with the augmented vector of market shares pS0t , S1t , . . . , SJt q, with Sjt â€œ p1 Â´ S0t qsjt
for j P J â€œ t1, . . . , Ju.
   The market shares for market t are obtained by integration over the variation in
preferences : for good j P J ,
                               â€                                    ïš¾
                                     exp pXjt pÎ² ` q ` Î¾jt q
                     Sjt â€œ E                                                   (2)
                                 1 ` Î£Jkâ€œ1 exp pXkt pÎ² ` q ` Î¾kt q
and S0t â€œ 1 Â´ Jjâ€œ1 Sjt .
               Å™


                                                  6
       Berry et al. (1995) assume that

                                         E pÎ¾jt |Zjt q â€œ 0

for all j P J and t. The instruments Zjt may for instance be the characteristics
of competing products, or cost-side variables. The procedure is operationalized by
showing that for given values of Î¸, the system (2) defines an invertible mapping5 in
IRJ . Call ÎpSt , Xt , Î¸q its inverse; a GMM estimator obtains by choosing functions
  Ëš
Zjt  of the instruments and minimizing a well-chosen quadratic norm of the sample
analogue of:
                                      `              Ëš
                                                        Ë˜
                                    E ÎpSt , Xt , Î¸qZjt
over Î¸.
    These models have proved very popular; but their implementation has faced a
number of problems. Much recent literature has focused on the sensitivity of the
estimates to the instruments used in GMM estimation of the mixed multinomial
logit model. Reynaertâ€“Verboven (2014) showed that using linear combinations of
the instruments can lead to unreliable estimates of the parameters of interest. They
recommend using the optimal instruments given by the Amemiya (1975) formula:
                                   Ë†                       Ë™
                            Ëš        BÎ
                          Zjt â€œ E       pSt , Xt , Î¸0 q|Zjt .
                                     BÎ¸
As implementing the Amemiya formula relies on a consistent first-step estimate of Î¸0 ,
this is still problematic. Gandhi and Houde (2016) propose â€œdifferentiation IVsâ€ to
approximate the optimal instruments for the parameters Î£ of the distribution of the
random preferences . They also suggest a simple regression to detect weak instru-
ments. An alternative is to use the Continuously Updating Estimator to build up the
optimal instruments as minimization progresses. Armstrong (2016) points out that
instruments based on the characteristics of competing products achieve identification
through correlation with markups. But when the number of products is large, many
models of the cost-side of the market yield markups that just do not have enough
variation, relative to sampling error. This can give inconsistent or just uninformative
estimates6 .
   5
    See Berry (1994).
   6
    Instruments that shift marginal cost directly (if available) do not need variation in the markup
to shift prices, and therefore do not suffer from these issues. Variation in the number of products
per market may also be used to restore identification, data permitting.

                                                 7
    Computation has also been a serious issue. The original BLP approach used a
â€œnested fixed pointâ€ (NFP) approach: every time the objective function to be mini-
mized was evaluated for the current parameter values, a contraction mapping/fixed-
point algorithm must be employed to compute the implied product effects Î¾t from the
observed market shares St and for the current value of Î¸. This was both very costly
and prone to numerical errors that propagate from the nested fixed point algorithm
to the minimization algorithm. DubeÌ et al (2012) proposed a nonlinearly-constrained,
nonlinear optimization problem to estimate Î¸. Their simulations suggest that this
â€œMPECâ€ approach often outperforms the NFP method, sometimes by a large factor.
Lee and Seo (2015) proposed an â€œapproximate BLPâ€ method that inverts a linearized
approximation of the mapping from Î¾t to St . They argue that this can be even faster
than the MPEC approach to estimation. Petrin and Train (2010) have proposed a
maximum likelihood estimator that replaces endogeneous regressors with a control
function. This circumvents the need to compute the implied value of Î¾ for each
value of Î¸, but still requires solving a nonlinear optimization problem to compute an
estimate of Î¸0 .
    Solving a nonlinear optimization problem for a potentially large set of parame-
ters is time-consuming. It typically requires starting values in the neighborhood of
the optimal solution; closed-form gradients; and careful monitoring of the optimiza-
tion algorithm by the analyst, as the objective function is not globally concave. The
method we propose in this paper completely circumvents the need to solve a non-
linear optimization problem. Our estimator relies on an approximate model that is
exactly valid when there is no random variation in preferences, and becomes a coarser
approximation as the amplitude of the random variation in elements of Î² ` i grows.
As such, our estimator is not a consistent estimator of the parameters of the BLP
model. On the other hand, it has some very real advantages that may tip the scale
in its favor. First, it requires a single linear 2SLS regression that can be computed
in microseconds with off-the-shelf software7 . Second, our estimator needs to assume
very little about the form of the distribution of the random variation in preferences 
(beyond its small scale), justifying the â€œrobustâ€ in our titleâ€”where the quotes reflect
   7
    Fox et al. (2011) discretize the distribution of the random coefficients on a grid and estimate the
corresponding probability masses. This also results in a least-squares estimatorâ€”but it is constrained
by linear inequalities and may be sensitive to the choice of the grid points.



                                                  8
our awareness that we are taking some liberties with the definition of robustness. Fi-
nally, because our estimating equation is linear, computing the â€œoptimalâ€ instruments
for our estimator is also straightforward.
    Some readers may find the â€œapproximate correctnessâ€ of our estimator unsatis-
fying. It at least yields â€œnearly consistentâ€ starting values for the classical nested-
fixed point and MPEC nonlinear optimization procedures at a minimal cost. This
can address a major challenge associated with successfully implementing the MPEC
estimation procedure. It also provides useful diagnoses about how well different pa-
rameters can be identified with a particular model and dataset; and a simple way to
select between models, as we discuss below.



2         2SLS Estimation in the Standard BLP Model
For the reader primarily interested in applying our method, this section provides a
step-by-step guide to implementing the estimator in the standard macro-BLP model.
This requires some notation. The dimensions of the vectors and matrices are as
follows:

    â€¢ for each j P J and t, Xjt is a row vector with nX components

    â€¢ Î² is a column vector with nX components

    â€¢ for each i, i is a row vector with ne components; in the standard model,
      ne Ä nX .

We denote I as the set of pairs of ordered indices pm, n Ä mq such that the variance-
covariance element Î£mn â€œ covpim , in q is not restricted to be zero8 . For notational
simplicity, we also assume that we use all J Ë† T conditional moment restrictions:

                                             E pÎ¾jt |Zjt q â€œ 0.

Adapting our procedure to subsets of moment restrictions is straightforward.

        Our procedure runs as follows:
    8
        E.g. if ne â€œ nX and Î£ is assumed to be diagonal, I â€œ tp1, 1q, . . . , pnX , nX qu.


                                                       9
Algorithm 1. Fast Robust and Approximately Correct (FRAC) estima-
tion of the standard BLP model

  1. For every market t, augment the market shares from ps1t , . . . , sJt q to pS0t , S1t , . . . , SJt q

  2. For every product-market pair pj P J , tq :
                                                                                Å™J
         (a) compute the market-share weighted covariate vector et â€œ              kâ€œ1   Skt Xkt ;
                                                                        jt
         (b) for every pm, nq in I, compute the â€œartificial regressorâ€ Kmn as
                                    Â´           Â¯
                              jt      Xjtm
               â€¢ if n â€œ m: Kmm    â€œ    2
                                           Â´ etm Xjtm ;
                             jt
                â€¢ if n Äƒ m: Kmn â€œ Xjtm Xjtn Â´ etm Xjtn Â´ etn Xjtm .
         (c) for every j â€œ 1, . . . , J, define yjt â€œ logpSjt {S0t q

  3. Run a two-stage least squares regression of y on X and K, taking as instru-
     ments a flexible set of functions of the columns of Z. Define Î²Ì‚ to be the es-
     timated coefficients associated with X and (the nonzero part of ) Î£Ì‚ to be the
     estimated coefficients associated with K.

  4. (optional9 ) Run a three-stage least squares (3SLS) regression across the T mar-
     kets stacking the J equations for each product with a weighting matrix equal to
     the inverse of the sample variance of the residuals from step 3.

    Consistent estimates of the covariance matrix of the asymptotic distribution of
?
  T JpÎ¸Ì‚ Â´ plimpÎ¸Ì‚qq can be obtained from the expressions for the heteroskedasticity
consistent covariance matrix for the 2SLS estimator given in White (1982).
   Ideally, the â€œflexible set of functions of the columns of Zâ€ in step 3 should be
able to span the space of the optimal instruments EpX|Zq and EpK|Zq for our
approximate model. Alternatively, these optimal instruments can be estimated by a
nonparametric regressions of each the column of X on the columns of Z.
    As is well-known, misspecification of one equation of the model can lead to incon-
sistency in 3SLS parameter estimates of all equations of the model. It is therefore
unclear whether Step 4 is worth the additional effort. We intend to explore this
question in future work.
  9
      This step should only be considered when T " J.


                                                10
    It is important to note here that e is not a simple weighted average, as the weights
do not sum to one, but only to p1Â´S0t q. To illustrate, if Xjtm â€ 1 is the constant, then
etm is p1 Â´ S0t q and the artificial regressor that identifies the corresponding variance
parameter is
                                        jt           1
                                      Kmm   â€œ S0t Â´ .
                                                     2
More generally, if Xjtn â€œ 1 pj P J0 q is a dummy that reflects whether variety j belongs
to group J0 Ä‚ J , then it is easy to see that the corresponding variance parameter is
the coefficient of the artificial regressor
                                                 Ë†           Ë™
                                  jt               1
                               Knn â€œ 1 pj P J0 q     Â´ SJ0 t
                                                   2

where SJ0 t is the market share of group J0 on market t.



3      Second-order Expansions
The rest of the paper justifies algorithm 1 and discusses extensions. We first derive
the small-Ïƒ expansions of the introduction.
   We start from a specification of the conditional indirect utility of variety j for
consumer i on market t as

                                Xjt Î² ` g pXjt , i q ` Î¾jt ` uijt                    (3)

for j P J ; and Ui0t â€œ ui0t . Define the vectors uit â€œ pui0t , ui1t , . . . , uiJt q; Xt â€œ
pX1t , . . . , XJt q; and Î¾t â€œ pÎ¾1t , . . . , Î¾Jt q. We assume that

    1. the random terms i are i.i.d. across i with finite variance;

    2. they are distributed independently of pXt , Î¾t q;

    3. EgpXjt , i q â€œ 0 for all Xjt ;

    4. the random vectors uit are i.i.d. across i and t; and they are distributed inde-
       pendently of pi , Xt , Î¾t q.




                                               11
These assumptions are all standard, except for the third one which is only a mild
extension of the usual normalization Ei â€œ 0. They allow for any type of codepen-
dence between the product effects Î¾t and the covariates Xt . Note that the additive
separability between Î² and  is not as strict as it seems. If for instance we start from
a multiplicative model with utilities
                                   nX
                                   Ã¿
                                         Xjtk Î²k Î¶ki ` Î¾jt ` uijt
                                   kâ€œ1

we can always redefine ki â€œ Î²k pÎ¶ki Â´ 1q to recover (3).
    Our crucial assumption, which we maintain throughout, is that the utilities are
affine in Î² and additive in the product effects Î¾ and in the idiosyncratic terms u. On
the other hand, we allow for any kind of distribution for i and uit . This encompasses
most empirical specifications used, as well as many more. We will refer to three special
cases for illustrative purposes:

   1. The standard model, also known as the mixed multinomial logit model, has
      g pX, q â€œ X; and the vector uit is distributed as iid standard type-I Extreme
      Value random variables.

   2. The standard binary model (or mixed logit model) further imposes J â€œ 1.

   3. The standard symmetric model is a standard model with  distributed symmet-
      rically around 0;

   4. The standard Gaussian model is a standard model with  normal with a diag-
      noral covariance matrix. It is probably the most commonly used in applications
      of the macro-BLP method.

   5. Finally, the standard Gaussian binary model imposes both 2 and 4.

    In order to do small-Ïƒ expansions, we need to introduce a scale parameter Ïƒ.
We do this with Assumption 1, which fits the usual understanding of what a scale
parameter is10 and also imposes the restriction that all moments of  are finite-valued.
The most common specification of the â€œmacro-BLPâ€ model has a Gaussian  and
satisfies Assumption 1.
  10
    In principle it should be possible to use several scale parameters, say Ïƒ1 for one part of the
variance-covariance matrix and Ïƒ2 for another one.

                                                 12
Assumption 1 (Finite moments). For some integer L Ä› 2, all moments of order
1 Ä l Ä L ` 1 of the vector  are finite; they are of order l in some non-negative scalar
Ïƒ. The first moment is zero: E â€œ 0. We denote Î£ â€œ E1 the variance-covariance
matrix of , and Âµl (for l Ä› 3) its (uncentered) higher order moments.

   It will be convenient to write  â€ ÏƒBv with v a random vector of mean zero
and covariance matrix equal to the identity matrix, so that V arpq â€ Î£ â€œ Ïƒ 2 BB 1 .
We only use this decomposition for intermediate results. Note that B is an ne Ë† nv
matrix pne Ä› nx q, where v is a row vector with nv components. This allows for Î£
to have a factor structure. Our final expansions do not depend on how Ïƒ and B are
normalized, and we wonâ€™t need to specify it.
    We drop the index t from the notation in most of this section as we will only need
to deal with one market at a time.


3.1       Second-order Expansions in the Standard Model
Much of the rest of the remainder of the paper focuses on the standard model, where
the idiosyncratic error terms u have iid Type I extreme value distributions. We will
show in section 6.4 how to extend our results to more general distributions.
   Recall that in the standard model, market shares are given by (2). If the scale
parameter Ïƒ was zero, inverting (2) would simply give us
                                                Sj
                                   Î¾j â€œ log        Â´ Xj Î² for j P J ,                (4)
                                                S0
which is standard multinomial logit model without random coefficients. This is the
starting point of the contraction algorithm described in Berry (1994).
   Now let Ïƒ be positive. With  â€œ ÏƒBv, a Taylor expansion of (4) at Ïƒ â€œ 0 would
give (assuming that the expansion is valid11 )
                                Sj                             Ïƒl
                     Î¾j â€œ log      Â´ Xj Î² ` Î£Llâ€œ1 alj pS, X, Î²q ` OpÏƒ L`1 q,         (5)
                                S0                             l!
where the alj pS, X, Î²q are defined below. In this equation, X collects the covariates
of all products and S is the vector of market shares. Market-share weighted sums
will play a crucial role in what follows:
 11
      We return to this point in section 5.1.

                                                    13
Definition 1 (Market-share weighting). For any J-dimensional vector T of J com-
ponents, we define the scalar
                                       Ã¿J
                               eS T â€œ     Sk Tk .
                                            kâ€œ1

By extension, if m is a pJ Ë† Jq matrix with J columns pm1 , . . . , mJ q, we define the
vector
                                        Ã¿J
                                eS m â€œ      Sk mk .
                                            kâ€œ1

Finally, we denote TÌ‚j â€œ Tj Â´ eS T and mÌ‚j â€œ mj Â´ eS m.

    Note that we are using the observed market shares of the J goods, so that these
weighted sums are very easy to compute from the data. It is important to emphasize
that the operator eS is not an average, as the augmented market shares Sk for k P J
do not sum to one but to p1 Â´ S0 q. Similarly, the TÌ‚j terms are not residuals, and
eS TÌ‚ â€° 0 in general.
   Our first goal is to find explicit formulÃ¦ for the coefficients alj in (5). While this
can be done at a high level of generality, let us start with a result that covers a large
majority of applications.
   In the standard model, g pXj , q is simply Xj . Using the identity  â€ ÏƒBv,
denote xj â€œ pXj Bq1 , a vector of nv components; and x the matrix whose J columns
are px1 , . . . , xJ q. Then
                                  g pXj , q â€œ Ïƒx1j v.
We now use this notation to derive the second-order expansion in Ïƒ in the standard
model.

Theorem 1 (Intermediate expansion in the standard model). In the standard model,

  (i) the alj coefficients only depend on S and on x;

 (ii) the first-order coefficients are zero: a1j â€ 0 for all j;

(iii) the second-order coefficients are given by
                                                        Ëœ                       Â¸
                                                                  J
                                                                  Ã¿
                    a2j â€œ 2xj Â¨ eS x Â´ kxj k2 â€œ Â´xj Â¨    xj Â´ 2         Sk xk       ;   (6)
                                                                  kâ€œ1


                                            14
 (iv) in the standard symmetric model, alj â€œ 0 for all j and odd l Ä L. Therefore if
      L Ä› 3,
                                   Sj           a2j 2
                          Î¾j â€œ log    Â´ Xj Î² `     Ïƒ ` OpÏƒ 4 q.                 (7)
                                   S0            2
Proof. See Appendix A.


3.2    The Artificial Regressors in the Standard Model
When truncated of its remainder term, equation (7) becomes linear in the parame-
ters pÎ², Ïƒ 2 q. The coefficients a2j , however, are quadratic combinations of the vectors
xj , which are themselves linear in the unknown coefficients of the matrix B. Fortu-
nately, the formula that gives a2j can be transformed so that it becomes linear in the
coefficients of the variance-covariance matrix Î£ of .
   To see this, note that since xk â€œ B 1 Xk1 ,

                                            x1k xl â€œ Xk BB 1 Xl1 .

But because Î£ â€œ Ïƒ 2 BB 1 , we have
                                            nX
                                            Ã¿
                         2
                     Ïƒ       x1k xl   â€œ            Î£mn Xkm Xln â€œ Tr pÎ£Xl Xk1 q
                                           m,nâ€œ1

where TrpÂ¨q is the trace operator.
   Plugging this into (6) gives
                                      Ë† Ë†          Ë™ Ë™
                                2 a2j           Xj
                              Ïƒ   â€œ Tr Î£ eS X Â´     Xj1 .
                                2               2
Define the nX Ë† nX matrices Kj by
                                                   Ë†            Ë™
                                            j          Xj
                                          K â€œ             Â´ eS X Xj1
                                                       2
                                      a
so that we can also write Ïƒ 2 22j â€œ Â´ TrrÎ£Kj s. The matrices Kj can be constructed
straightforwardly from the covariates X and the market shares S. Given that Î£ is
symmetric,
                                      nX
                                      Ã¿                        Ã¿         ` j        Ë˜
                TrrÎ£Kj s â€œ                      j
                                           Î£mm Kmm `                  Î£mn Kmn    j
                                                                              ` Knm   .   (8)
                                  mâ€œ1                         m,nÄƒm

Define the lower-triangular matrices K j by

                                                         15
                        j
    â€¢ on the diagonal: Kmm â€œ Kjmm
                                     j
    â€¢ off the diagonal (for n Äƒ m): Kmn â€œ Kjmn ` Kjnm .

We call their elements the â€œartificial regressorsâ€, for reasons that will soon become
clear.
     Additional a priori restrictions can be accommodated very easily. For instance,
it is common to restrict Î£ to be diagonal, as is the case in Berry et al. (1995). Then
only nX terms enter in the sum (8); moreover,
                                   Ëœ                  Â¸
                                             J
                            j        Xjm    Ã¿
                          Kmm    â€œ        Â´     Sk Xkm Xjm .
                                       2    kâ€œ1

If Î£ is not diagonal, then we need to also use additional terms
                                Ëœ           Â¸        Ëœ           Â¸
                                  J
                                  Ã¿                    Ã¿J
                j
              Kmn  â€œ Xjm Xjn Â´       Sj Xkm Xjn Â´          Sj Xkn Xjm .
                                  kâ€œ1                  kâ€œ1


    To summarize, we have:

Theorem 2 (Final expansion in the standard model). In the standard model,
                                 nX
                     Sj          Ã¿
                                          j
                                                Ã¿
                                                        j
          Î¾j â€œ log      Â´ Xj Î² Â´     Î£mm Kmm Â´     Î£mn Kmn ` OpkÎ£kk{2 q,           (9)
                     S0          mâ€œ1           nÄƒm

where the integer k Ä› 3; and k â€œ 4 if the model is symmetric.
    The artificial regressors are given by
                       Ëœ                    Â¸
                                  J
                j        X jm
                                 Ã¿
              Kmm    â€œ        Â´      Sk Xkm Xjm
                           2     kâ€œ1
                                   Ëœ          Â¸      Ëœ         Â¸
                                     Ã¿J                J
                                                       Ã¿
                 j
               Kmn   â€œ Xjm Xjn Â´        Sj Xkm Xjn Â´     Sj Xkn Xjm .
                                  kâ€œ1                  kâ€œ1



4     2SLS Estimation
Equation (9) is linear in the parameters of interest Î¸ â€œ pÎ², Î£q, up to the remainder
term. This immediately suggests neglecting the remainder term and estimating the
                             S
approximate model Î¾j â€œ log S0j Â´ Xj Î² Â´ TrrÎ£K j s.

                                         16
   More precisely, assume we are given a sample of T markets, and instruments
Zjt such that E pÎ¾jt |Zjt q for all j and t. Then our proposed estimator Î¸Ì‚ fits the
approximate linear set of conditional moment restrictions:
                      Ë† â€ ïš¾                                Ë™
                              Sjt     `             jt
                                                       Ë˜
                  E log            Â´ Xjt Î² ` TrrÎ£K s |Zjt â€œ 0                   (10)
                              S0t

which only differs from the original model by a term of order Ïƒ 3 (or Ïƒ 4 if the distri-
bution of  is symmetric)12 . This can simply be done by choosing vector functions
  Ëš
Zjt of the instruments and running two-stage least squares: for each j â€œ 1, . . . , J
and t â€œ 1, . . . , T , we run a 2SLS regression of logpSjt {S0t q on Xjt and the relevant13
variables K jt , using the instruments Zjt Ëš
                                             .


5      Pros and Cons of the 2SLS Estimation Approach
The drawback of our method is obvious: because this is only an approximate model,
the resulting estimator Î¸Ì‚ will not converge to Î¸0 as the number of markets T goes
to infinity. We discuss this in much more detail in section 5.1. For now, let us
note that this drawback is tempered by several considerations. First, the number of
markets available in empirical IO is typically small; finite-sample performance of the
estimator is what matters, and we will examine that in Section 7. More importantly,
our estimator has several useful features. Let us list six of them:

    1. Because the estimator employs linear 2SLS, computing it is extremely fast and
       can be done in microseconds with any of-the-shelf software.

    2. We do not have to assume any distributional form for the random variation
       in preferences . This is a notable advantage over other methods: while they
       yield inconsistent estimates if the distribution of  is misspecified, our estimator
       remains consistent for the parameters of the approximate model.
  12
      Note that because our model is only an approximation, there may not exist a value of Î¸ that
satisfies the conditional moment restrictions (10) in the population. Nevertheless, our 2SLS estima-
tion procedure will still yield an estimate of the value of Î¸ that minimizes the probability limit of
the 2SLS objective function. We explore this further in our Monte Carlo study in section 7.
   13                               jt
      E.g. only the nX variables Kmm    if Î£ is restricted to be diagonal, or even a subset if some
coefficients are non-random.

                                                 17
  3. Computing the optimal instruments does not require any first-step estimate
     because the estimating equation is linear. We can just use a flexible set of
     functions of the columns of Z that span the space of the optimal instruments
     EpX|Zq and EpK|Zq.

  4. Even if the econometrician decides to go for a different estimation method,
     our proposed 2SLS estimates obtained should provide a set of very good initial
     parameter values for a nonlinear optimization algorithm.

  5. The confidence regions on the estimates will give useful diagnoses about the
     strength of identification of the parameters, both mean coefficients Î² and their
     random variation Î£. This would be very hard to obtain otherwise, except by
     trying different specifications.

  6. There has been much interest in systematic specification searches in recent
     years; see e.g. Horowitzâ€“Nesheim 2018 for a Lasso-based selection approach in
     discrete choice models. With our method any number of variants can be tried
     in seconds, and model selection is drastically simplified.


5.1    The Quality of the Approximation
Ideally, we would be able to bound the approximation error in the expansion of Î¾j ,
and use this bound to majorize the error in our estimator in the manner described in
Kristensen and SalanieÌ (2017). While we have not gone that far, we can justify the
local-to-zero validity of the expansion in the usual way. We are taking a mapping

                                    S â€œ G pÎ¾, X, Ïƒq

that is differentiable in both Î¾ and Ïƒ; inverting it to Î¾ â€œ Î pS, X, Ïƒq; and taking
an expansion to the right of Ïƒ â€œ 0 for fixed market shares S and covariates X. The
validity of the expansion for small Ïƒ and fixed pX, Sq depends on the invertibility of
the Jacobian GÎ¾ .
    First consider the standard model. It follows from Berry 1994 that GÎ¾ is invertible
if no observed market share hits zero or one. Applying the Implicit Function Theorem
repeatedly shows that in fact the Taylor series of Î¾ converges over some interval r0, ÏƒÌ„s


                                           18
if all moments of  are finite; and that the expansion is valid at order L if the moments
of  are bounded to order pL ` 1q.
    Characterizing this range of validity is trickier. Figure 1 uses formulÃ¦ derived
in Appendix B to plot the first four coefficients of the expansion in Î£11 X12 for the
standard Gaussian binary model (that is, the Gaussian mixed logit) with one covariate
X1 :
                                        4
                          S1           Ã¿          `       Ë˜k
                 Î¾1 â€œ log    Â´ Î²X1 `      tk pS1 q Î£11 X12 ` OpÏƒ 10 q.
                          S0          kâ€œ1

Each curve plots the function tk as market shares vary between zero and one. The
visual impression is clear: the coefficients damp quickly. Beyond the first term (which
corresponds to our 2SLS method), the coefficients are always smaller than 0.05 in
absolute value. Of course, the approximation error also depends on the values taken
by the covariates.


  0.15


  0.10

                                                                               t1(S1 )
  0.05
                                                                               t2(S1 )

                 0.2         0.4         0.6           0.8         1.0
                                                                         S1    t3(S1 )
                                                                               t4(S1 )
 -0.05


 -0.10


 -0.15


                           Figure 1: Coefficients t1,2,3,4 pS1 q

    While this simple example can only be illustrative, we find the figure encouraging
as to the practical range of validity of the approximation. We have not determined
the extent to which these results generalize to the multinomial logit model.




                                            19
5.2       â€œRobustnessâ€
Our expansions only rely on the properties of the derivatives of the logistic cdf Lptq â€œ
    1
1`exppÂ´tq
          and on the first two moments of . This has a distinct advantage over
competing methods: the lower-order moments of  can be estimated by 2SLS, and
nothing more needs to be known about its distribution.
    Suppose for instance that the analyst does not want to assume that  has a
symmetric distribution. Then the coefficients a1j are still zero, and the coefficients
a2j are unchanged. In the absence of symmetry, the approximate model is only valid
up to OpÏƒ 3 q; but running Algorithm 1 may still provide very useful estimators of the
elements of Î£0 .



6       Extensions
Our technique can be extended to other random coefficient models as long as the
conditional indirect utility remains additive in the product-specific random effects Î¾.
This is shown in section 6.1. We follow with a modification of our multinomial logit
random coefficients modeling framework to account for the third and fourth moments
of . We then turn to methods for improving the quality of our 2SLS estimates14 .
Finally, section 6.4 presents a nonlinear 2SLS estimation procedure for the nested
logit model.


6.1       Quasi-linear Random Coefficients Models
Consider the following class of models, whose defining characteristic is that the error
term Î· and the mean coefficients Î² only enter via a linear combination Î· Â´ f1 pyqÎ²:

                    Gpy, Î·, Î², Ïƒq â€ GËš py, Ev AËš py, Î· Â´ f1 pyqÎ², ÏƒBvqq .                        (11)

where v is unobserved heterogeneity distributed independently of y and Î· and nor-
malized by Ev â€œ 0 and V v â€œ I; and both functions GËš and AËš are assumed to be
known.
 14
      We explore the small sample properties of several of these corrections in our Monte Carlo study.



                                                  20
   Note that the macroâ€“BLP model takes this form, with y â€œ pS, Xq; f1 pyq â€œ Â´X;
Î· â€œ Î¾; and
                    Ë†                                            Ë™
              Ëš
            Aj â€œ Pr j â€œ arg max pXk Î² ` Î¾k ` ÏƒXk Bvq |X, Î¾, v
                                   Jâ€œ0,1,...,J


so that, denoting aj â€ Xj and bj â€œ Xj Î² ` Î¾j ,

                                               exp pbj ` aj cq
                          AËš pa, b, cq â€      Å™J                   ;
                                           1 ` kâ€œ1 exp pbk ` ak cq

and GËšj â€ Sj Â´ Ev AËšj .
    We continue to assume that E pÎ·|Zq â€œ 0. The quasi-linear structure in (11)
allows this class of models to be approximately estimated by 2SLS. Denoting partial
derivatives with subscripts, we have:

Theorem 3 (Expansions for quasi-linear random coefficients models). Consider a
model of the class defined by (11) and assume that

   â€¢ GËš is twice differentiable with respect to its second argument

   â€¢ AËš is twice differentiable with respect to its last two arguments

   â€¢ the matrices GËš2 py, AËš py, Î·Â´f1 pyqÎ², 0qq and AËš2 py, Î·Â´f1 pyqÎ², 0q are invertible
     for all py, Î·, Î²q.

Any such model satisfies the conditions C1â€“C3 in the introduction. Moreover,

   â€¢ f1 pyq appears directly in (11)

   â€¢ the variables f0 pyq are defined by the system of equations

                                    GËš py, AËš py, f0 pyq, 0qq â€œ 0

   â€¢ and the variables f2 pyq solve the linear system

                          AËš33 py, f0 pyq, 0qq f2 pyq â€œ Â´AËš2 py, f0 pyq, 0qq.




                                                 21
   Proof: See Appendix E.

    As explained in the introduction, these models can be estimated by regressing
f0 pyq on f1 pyq and f2 pyq with a set of flexible functions of Z as instruments. As
the macroâ€“BLP model belongs to this class, this confirms that conditions C1â€“C3
hold in the BLP model; we had shown it implicitly in section 3 by deriving the
expansions. Note also that we did not use any distributional assumption on the
random coefficients and the idiosyncratic shocksâ€”although of course the terms in
the expansions do depend on these distributions. We give an illustration for a one-
covariate mixed binary model without any distributional assumption in Appendix B.3.


6.1.1   Examples

It is easy to generate models in the quasi-linear class (11). Starting from any Genes-
ralized Linear Model gpyq â€œ XÎ² ` Î·, we can for instance transform the right-hand
side by adding additive unobserved heterogeneity and another link function:

                              gpyq â€œ EÎµ h pXÎ² ` Î·, ÏƒÎµq .

When the link functions g and h are both assumed to be known, all such models obey
conditions C1â€“C3 and can therefore be studied with our method. Note that in these
models f1 pyq â€ Â´X and f2 pyq â€œ Â´ph1 {h22 qpf0 pyq, 0q where

                                f0 pyq â€œ hpÂ¨, 0qÂ´1 pgpyqq

(assuming the inverse is well-defined.)
   The nested logit of section 6.4 shows that our method remains useful beyond
the class of quasi-linear models, at the cost of breaking conditions C2 and C3 and
requiring (simple) numerical optimization.


6.2     Higher-order terms
In Appendix B, we study in more detail the standard binary model. For this simpler
case, calculations are easily done by hand for lower orders of approximation, or using
symbolic software for higher orders.

                                           22
    More generally, return to the standard model and assume (as is often done in
practice) that the m are independent across the covariates m â€œ 1, . . . , nX . We
denote as before Î£mm â€œ Ep2m q, and Âµlm the expected value of lm for l Ä› 3. Tedious
calculations15 show that the second- to fourth-order terms of the expansion in Ïƒ are
                                                     4
                                        Sj          Ã¿
                               Î¾j â€œ log    Â´ Xj Î² `     Alj ` OpÏƒ 5 q
                                        S0          lâ€œ2

with
                                      Ã¿
                             A2j â€œ        Xjm peS Xm Â´ Xjm {2q Î£mm ;
                                      m
                               Ë†                   2    2              Ë™
                     Ã¿                 eS Xm eS pXm  q Xjm           2
             A3j â€œ       Xjm       Xjm      `         Â´    Â´ peS Xm q Âµ3m ;
                     m
                                          2      2      6
and
                             Ë†                                                  3
                Ã¿
                                          3                 2      peS Xm q2 Xjm
        A4j â€œ     Âµ4m Xjm peS Xm q Â´          peS Xm qpeS pXm
                                                           Â´ Xjm
                                                              qq            Â´
              m
                                                                       2       24
                                3               2
                                                                 Ë™
                          eS pXm  q       eS pXm  q    2 e S Xm
                        `           ` Xjm           ` Xjm
                              6               4              6
                2                   Ë†                     Ë†                  Ë™Ë™
              A2j    Ã¿                                       Xjm
            `      ` Î£mm Xjm eS pA2 Xm q ` peS A2 q               Â´ 2peS Xm q   .
               2      m
                                                               2

First consider the third-order term A3j . It is a linear function of the unknown skew-
nesses Âµ3m ; in fact it can be rewritten as
                                         Ã¿
                                       Â´ Tmj Âµ3m
                                                m

where we introduced new artificial regressors
                       Ë† 2                                  2
                                                                Ë™
               j
                        Xjm               2     eS Xm eS pXm  q
             Tm â€ Xjm          ` peS Xm q Â´ Xjm      Â´            .
                          6                        2      2

Algorithm 1 can be adapted in the obvious way to take possible skewness of  into
account. Note that the procedure remains linear in the parameters pÎ², Î£, Âµ3 q, for
which it generates approximate estimates by 2SLS.
 15
      Available from the authors.



                                                    23
    The fourth-order term, on the other hand, contains terms that are linear in the
Âµ4m (the first two lines of the formula) as well as terms that are quadratic in Î£ (the
last line). The first group suggests introducing more artificial regressors
                     Ë†
           j                         2                      peS Xm q2
         Qm â€ Xjm peS Xm qpeS pXm      qq Â´ peS Xm q3 ` Xjm               3
                                                                      ` Xjm {24
                                                                2
                     3               2
                                                      Ë™
               eS pXm  q       eS pXm  q     2 e S Xm
             Â´           Â´ Xjm           Â´ Xjm          ,
                   6               4               6
whose coefficients are the Âµ4m . The second group yields
                                    Ã¿
                                                  j
                                 Â´      Î£mm Î£nn Wmn
                                  m,nâ€œ1

where new artificial regressors W are assigned products of the elements of Î£. Esti-
mating the resulting regression requires nonlinear optimization (albeit a very simple
one).


6.3     Correcting the 2SLS estimates
If the analyst is willing to make more distributional assumptions, she can resort to
bootstrap or asymptotic corrections to improve the accuracy of our 2SLS estimators.


6.3.1   Bootstrapping

Once we have approximate estimators Î²Ì‚ and Î£Ì‚, we can use them to solve the market
shares equations for estimates of the product effects Î¾ and bootstrap them, provided
that we are willing to impose a distribution for v (beyond the normalization of its
first two moments.)
   We use Berry inversion to solve for Î¾Ì‚t in the system
                                   Â´       Â´           Â¯   Â¯
                               exp Xjt Î²Ì‚ ` Î£Ì‚1{2 v ` Î¾Ë†jt
                Sjt â€œ Ev                 Â´     Â´         Â¯      Â¯,
                          1 ` Î£Jkâ€œ1 exp Xkt Î²Ì‚ ` Î£Ì‚1{2 v ` Î¾Ë†kt
where Ev pÂ¨q denotes the expectation taken with respect to the assumed distribution
of v. For any resample Î¾ Ëš of the Î¾Ì‚, we simulate the market shares from
                                     Â´     Â´          Â¯      Â¯
                                exp Xjt Î²Ì‚ ` Î£Ì‚1{2 v ` Î¾jt Ëš
                   Ëš
                  Sjt â€œ Ev               Â´     Â´          Â¯      Â¯
                           1 ` Î£Jkâ€œ1 exp Xkt Î²Ì‚ ` Î£Ì‚1{2 v ` Î¾ktËš



                                          24
and we use our 2SLS method to get new estimates Î² Ëš , Î£Ëš . Finally, we compute
bias-corrected estimates by e.g.
                                                           B
                                                        1 Ã¿ Ëš
                                        Î² C â€œ 2Î²Ì‚ Â´          Î² .
                                                        B bâ€œ1 b
More generally, the resampled estimates can be used to estimate the distribution of
Î²Ì‚ and Î£Ì‚ in the usual manner.


6.3.2      Asymptotic Correction

Another way to use the third- and fourth-order terms in our expansion is as a cor-
rective term: that is, we run 2SLS on the second-order expansion and we use the
formulÃ¦ for the higher-order terms to correct for the effects of the approximation.
    Denote Î¸ â€œ pÎ£, Î²q, and Î¸0 its true value. Let Î¸Ì‚2 be our 2SLS estimator based on
a second-order expansion. That is, we estimate the approximate model EpÎ¾2 Zq â€œ 0
with instruments Z and weighting matrix W , where
                                              Sj
                                  Î¾2j â€œ log      Â´ Xj Î² Â´ Tr Î£K j .                                (12)
                                              S0
As the number of markets T gets large, Î¸Ì‚2 converges to the solution Î¸2 of Ef2 pÎ¸2 q â€œ 0,
with
                               BÎ¾2
                      f2 pÎ¸q â€     pÎ¸, X, Sq1 ZW Z 1 Î¾2 pÎ¸, X, Sq.
                               BÎ¸
Alternatively, we could have estimated the model using inversion or MPEC, with an
â€œexactâ€ Î¾8 . Let Î»0 denote additional parameters of the model (such as higher-order
moments of the distribution of ) that are identified using the exact Î¾8 but not16
with our approximate Î¾2 .
      Since by assumption E pÎ¾8 pÎ¸0 , Î»0 , X, SqZq â€œ 0, a fortiori Ef8 pÎ¸0 ; Î»0 q â€œ 0 with
                             BÎ¾8
                    f8 pÎ¸; Î»0 q â€ pÎ¸, Î»0 , X, Sq1 ZW Z 1 Î¾8 pÎ¸, Î»0 , X, Sq.
                              BÎ¸
The dominant term in the asymptotic bias is given by expanding Ef8 pÎ¸; Î»0 q around
Î¸ â€œ Î¸2 , keeping Î»0 fixed. It is
                                 Ë†                   Ë™Â´1
                                     Bf8
                      Î¸2 Â´ Î¸0 Â» E          pÎ¸2 ; Î»0 q    Ef8 pÎ¸2 ; Î»0 q.
                                      BÎ¸
 16
      If the only free parameters of the distribution of  are the elements of Î£, then Î»0 will be empty.

                                                   25
    Denote X the matrix with terms Xjm and K the matrix whose row j â€œ 1, . . . , J
                                    j
contains the artificial regressors Kmn . We define e2 pÎ¸; Î»0 q â€œ Î¾8 pÎ¸; Î»0 q Â´ Î¾2 pÎ¸q, the
approximation error on Î¾. Under any assumption about the parameters in Î»0 , we
can compute the higher-order terms Î¾3 , Î¾4 , . . . to approximate e2 . If for instance we
maintain the assumption that the model is symmetric, we can approximate e2 Â»
Î¾4 Â´ Î¾2 . Results in Robinson (1988) and in Kristensen and SalanieÌ (2017) could be
adapted to show that the resulting corrected estimator not only has a smaller bias;
it is in fact asymptotically equivalent to the estimator Î¸Ì‚4 based on a fourth-order
expansion.
   Let us suppose then that we have a reliable estimator eÌ‚2 pÎ¸; Î»0 q of e2 pÎ¸; Î»0 q. Define
V by the Cholesky decomposition ZW Z 1 â€œ V V 1 , so that V is a pJ Ë† Jq matrix. We
prove in Appendix D that this asymptotic correction yields the following formula:
           Ëœ                                 Â¸Â´1 Ëœ                                        Â¸
             E pX 1 V V 1 Xq E pX 1 V V 1 Kq                 E pX 1 V V 1 eÌ‚2 q
Î¸0 Â» Î¸2 `                                                                 `              Ë˜ .
            E pK 1 V V 1 Xq E pK 1 V V 1 Kq        E pK 1 V V 1 eÌ‚2 q Â´ E BeÌ‚
                                                                            BÎ£
                                                                              2
                                                                                V V 1 Î¾2

To interpret this formula, note that if eÌ‚2 did not depend on Î£ the corrective term
on the right-hand-side would simply be the 2SLS estimate of the regression of eÌ‚2
on pX, Kq with instruments V . In fact, if we are only interested in correcting the
estimators of the mean coefficients Î²2 , we can simply keep the corresponding part
of the 2SLS estimate. The correction on Î£2 has an additional term as higher order
terms in the expansion of Î¾ typically depend on Î£. (Recall from Theorem 1.(i) that
they do not depend on Î².)


6.3.3   Two-Step Estimator Based on the Asymptotic Correction

In practice, we use the estimate of Î¾8 pÎ¸Ì‚2 q computed using our 2SLS estimator and
the Berry inversion as well as the residual from our 2SLS estimate
                                             Sj
                          Î¾2j pÎ¸Ì‚2 q â€œ log      Â´ Xj Î²Ì‚2 Â´ Tr Î£Ì‚2 K j
                                             S0
to correct our initial 2SLS estimate. The difference between Î¾2j pÎ¸0 q and Î¾8 pÎ¸0 q is
equal to the higher-order terms in equation (5). Computing a new dependent variable
                                  â€ ïš¾
                                    Sj
                         yj â€œ log      Â´ pÎ¾2j pÎ¸0 q Â´ Î¾8 pÎ¸0 qq
                                    S0

                                                26
and applying 2SLS would yield a consistent estimate of Î¸0 . Using our 2SLS estimate
Î¸Ì‚2 and computing               â€ ïš¾
                        Ëš        Sj
                      yj â€œ log        Â´ pÎ¾2j pÎ¸Ì‚2 q Â´ Î¾8 pÎ¸Ì‚2 qq
                                 S0
and applying 2SLS with this dependent variable is a feasible version of this correc-
tion17 . In our Monte Carlo study we will explore the small sample properties of this
procedure.


6.4       The Two-level Nested Logit
Campioni (2018) applies a nonparametric approach to the choice among a very large
set of products. He shows that the mixed logit specification forces the price elasticity
to become â€œtoo smallâ€ at high price levels. This raises the question of the appropriate
choice of a distribution for the idiosyncratic terms uijt .
    For the mixed logit (J â€œ 1), it is very easy to compute the artificial regressors
for any distribution of the idiosyncratic terms; we give the formulÃ¦ in Appendix B.3.
When J Ä… 1, the space of possible distributions increases dramatically. The compu-
tations also become more complicated. Finally, estimating the additional parameters
of the distribution of u requires (simple) nonlinear optimization.
    For illustrative purposes, we give the estimating equation for the two-level nested
logit model. Assume that there is a nest for good 0, and K nests N1 , . . . , NK for the
varieties of the good. For k â€œ 1, . . . , K, we denote Î»k the corresponding distribution
parameterâ€”with the usual interpretation that p1 Â´ Î»k q proxies for the correlation
between choices within nest k, and that the multinomial logit model obtains when all
Î»k â€œ 1.
                                                              Å™
    We denote the market share of nest k by SNk â€œ jPNk Sj . Take any variable
T â€œ pT0 , T1 , . . . , TJ q. We define the within-nest-k share-weighted average as
                                                  Ã¿ Sj
                                         TÌ„k â€œ            Tj .
                                                 jPN
                                                     S Nk
                                                     k

                                      Å™K
Note in particular that eS T â€œ          kâ€œ1   SNk TÌ„k .
 17
      Note that this procedure could be iterated.



                                                    27
   Appendix C derives the equivalent of (6): for j P Nk ,
                        Ë†                          Ë™
                                 xj      1 Â´ Î»k         1 Â´ Î»k
             a2j â€œ xj Â¨ 2eS x Â´      `2         xÌ„k Â´          kxÌ„k k2 .
                                 Î»k        Î»k             Î»k
Reintroducing the market index t, the corresponding artificial regressors are
             Ë†                      Ë™                       Ë†                 Ë™
       jt      Xjt,m 1 Â´ S0t Î»k       Xjt,m 1 Â´ Î»k                     2Xjt,m
     Kmm â€œ          Â´           etm         `        XÌ„kt,m XÌ„kt,m Â´
                 2      1 Â´ S0t         Î»k      Î»k                       Î»k
and for any off-diagonal term n Äƒ m,
                                                                           Ë†                                                  Ë™
 jt                 1 Â´ S0t Î»k etm Xjt,n ` etn Xjt,m 1 Â´ Î»k                                     XÌ„kt,m Xjt,n ` XÌ„kt,n Xjt,m
Kmn â€œ Xjt,m Xjt,n Â´                                 `2                          XÌ„kt,m XÌ„kt,n Â´
                     1 Â´ S0t             Î»k            Î»k                                                    Î»k

where as in section 3, etm â€œ Jjâ€œ1 Sjt Xjtm .
                              Å™

   If the Î»k parameters are known, then our procedure becomes:

Algorithm 2. FRAC estimation of the two-level nested logit BLP model

  1. on every market t, augment the market shares from ps1t , . . . , sJt q to pS0t , S1t , . . . , SJt q

  2. for every product-market pair pj P J , tq :
                                                                                         Å™J
      (a) compute the market-share weighted covariate vector et â€œ                           lâ€œ1   Slt Xlt and
          the within-nest weighted average covariate vector
                                                      Ã¿          Slt
                                        XÌ„kpjq,t â€œ                        Xlt
                                                     lPNkpjq
                                                               SNkpjq,t

           where kpjq is the nest that variety j belongs to.
      (b) for every pm, nq in I, compute the â€œartificial regressorâ€
                  Ë†                         Ë™                            Ë†                    Ë™
            jt      Xjt,m 1 Â´ S0t Î»kpjq       Xjt,m 1 Â´ Î»kpjq                          2Xjt,m
          Kmm â€œ           Â´             etm         `          XÌ„kpjq,t,m XÌ„kpjq,t,m Â´
                      2        1 Â´ S0t        Î»kpjq     Î»kpjq                           Î»kpjq

           and for any off-diagonal term n Äƒ m,

               jt                 1 Â´ S0t Î»kpjq etm Xjt,n ` etn Xjt,m
              Kmn â€œ Xjt,m Xjt,n Â´
                                     1 Â´ S0t               Î»kpjq
                               Ë†                                                             Ë™
                     1 Â´ Î»kpjq                           XÌ„kpjq,t,m Xjt,n ` XÌ„kpjq,t,n Xjt,m
                  `2             XÌ„kpjq,t,m XÌ„kpjq,t,n Â´                                       .
                       Î»kpjq                                            Î»kpjq

                                                28
         (c) define
                                                 SNkpjq ,t              Sjt
                                     yjt â€œ log             ` Î»kpjq log
                                                  S0t                  SNkpjq,t

    3. run a two-stage least squares regression of y on X and K, taking as instruments
       a flexible set of functions of Z

    4. (optional) run a three-stage least squares (3SLS) regression across the T markets
       stacking the J equations for each product with a weighting matrix equal to the
       inverse of the sample variance of the residuals from step 43.

   If the parameters Î» are not known, then things are slightly more complicated:
the formulÃ¦ cannot be made linear in Î», and there are no corresponding artificial
regressors. Estimation of pÎ², Î£, Î»q requires numerical minimization over the Î».
    More general distributions in the GEV family could also be accommodated. As
the nested logit example illustrates, there is a cost to it: the approximate model
becomes nonlinear in some parameters18 . Note however that if there is reason to
believe that the true distribution is close to the multinomial logit (say Î» Â» 1 in the
example above), then one can take expansions in the same way we did for the random
coefficients and use a 2SLS estimate again.



7       Monte Carlo Analysis of the Small-Sample Per-
        formance of our Estimator
This section presents the results of a Monte Carlo study of an aggregate discrete choice
demand system with random coefficients. It compares the finite sample performance
of our estimator of the parameters to estimators computed using the mathematical
programming with equilibrium constraints (MPEC) approach recommended by DubeÌ,
Fox and Su (2012). We also show results demonstrating some of the â€œrobustnessâ€
of our estimation procedure to assumptions about the distribution of the random
coefficients. Specifically, we find that even if the distribution of random coefficients is
misspecified, our procedure still yields very good estimates of the means and variances
of the random coefficients.
 18
      Technically, condition C1 in the introduction still holds, but conditions C2 and C3 do not.

                                                  29
    The basic set-up of our Monte Carlo study follows that in DubeÌ, Fox and Su
(2012). It is a standard static aggregate discrete choice random coefficients demand
system with T â€œ 50 markets and J â€œ 25 products in each market, and K â€œ 3
observed product characteristics. Following DubeÌ, Fox, and Su (2012), let Mt denote
the mass of consumers in market t â€œ 1, 2, . . . , T . Each product is characterized by the
             1
vector pXjt    , Î¾jt , pjt q1 , where Xjt is a K Ë† 1 vector of observable attributes of product
j â€œ 1, 2, . . . , J in market t, Î¾jt is the vertical product characteristic of product j
in market t that is observed by producers and consumers, but unobserved by the
econometrician, and pjt is the price of product j in market t. Collect these variables
                                                                                   1            1 1
for each product into the following market-specific variables: Xt â€œ pX1t             , . . . , XJt q,
                               1                                     1
Î¾t â€œ pÎ¾1t , Î¾2t , . . . , Î¾Jt q , and pt â€œ pp1t , p2t , . . . , pJt q .
       The conditional indirect utility of consumer i in market t from purchasing product
j is
                                            1
                               uijt â€œ Î²0 ` Xjt Î²ix Â´ Î²ip pjt ` Î¾jt ` ijt
The utility of the j â€œ 0 good, the â€œoutsideâ€ good, is equal to u0jt â€œ i0t . Each
element of Î²ix â€œ pÎ²i1 x             x 1
                         , . . . , Î²iK q is assumed to be drawn independently from N pÎ²Â¯kx , Ïƒk2 q
distributions, and each Î²ip is assumed to be drawn independently from N pÎ²Â¯p , Ïƒp2 q. We
denote Î²i â€œ pÎ²ix 1 , Î²ip q1 .
       We collect all parameters into

                             Î¸ â€œ pÎ²0 , Î²Â¯1x , . . . , Î²Â¯K
                                                        x Â¯
                                                          , Î²p , Ïƒ12 , . . . , ÏƒK
                                                                                2
                                                                                  , Ïƒp2 q1 .

Our simulations all have mean coefficients

                           pÎ²0 , Î²Â¯1x , Î²Â¯2x , Î²Â¯3x , Î²Â¯p q â€œ pÂ´1, 1.5, 1.5, 0.5, Â´1q.

and varying variances pÏƒ12 , Ïƒ22 , Ïƒ32 , Ïƒp2 q. We also experiment with varying Î²Ì„p .
   To compute the market shares for the J products, we assume that the ijt are
independently and identically distributed Type I extreme value random variables, so
that the probability that consumer i with random preferences Î²i purchases good j in
market t is equal to

                                                exppÎ² 0 ` Xjt
                                                            1
                                                              Î²ix Â´ Î²ip pjt ` Î¾jt q
               sijt pXt , pt , Î¾t |Î²i q â€œ
                                            1 ` Jkâ€œ1 exppÎ²0 ` Xkt   Î²ix Â´ Î²ip pkt ` Î¾kt q
                                               Å™                  1




                                                          30
We compute the observed market shares for all goods in market t by drawing ns â€œ
1, 000 draws pÎ¶ikt q from four N p0, 1q random variables and constructing 1, 000 draws
from Î²i |Î¸ as follows:
                           x
                          Î²ikt â€œ Î²Â¯kx ` Ïƒk Î¶ikt and Î²itp â€œ Î²Â¯p ` Ïƒp Î¶ipt .

We then use these draws to compute the observed market share of good j in market
t as:                                           ns
                                            1 Ã¿
                    sjt pXt , pt , Î¾t |Î¸q â€œ        sijt pXt , pt , Î¾t |Î²i q
                                            ns iâ€œ1
given the vectors Xt , pt , and Î¾t for each market t.
   Consistent with the experimental design in DubeÌ, Fox and Su (2012), we generate
the values of Xt , pt , Î¾t and a vector of 6 instruments Zjt as follows. First we draw
Xt for all markets t â€œ 1, 2, . . . , T from a multivariate normal distribution:
                        Â» fi           Â¨Â» fi Â»                  fiË›
                          x1j             0      1     Â´0.8 0.3
                        â€“x2j fl â€ N Ëâ€“0fl , â€“Â´0.8        1   0.3flâ€š
                        â€” ffi          Ëšâ€” ffi â€”                 ffiâ€¹

                          x3j             0     0.3     0.3   1
The price of good j in market t is equal to

                          pjt â€œ |0.5Î¾jt ` ejt ` 1.1px1j ` x2j ` x3j q|,

where ejt â€ N p0, 1q, distributed independently across products and markets. The
Î¾jt are N p0, ÏƒÎ¾2 q random variables drawn independently across products and markets
for different values of ÏƒÎ¾2 described below. The data generating process for the vector
of instruments is:

                      zjtd â€ U p0, 1q ` 0.25pejt ` 1.1px1j ` x2j ` x3j qq

where d â€œ 1, . . . , 6.
  For a specified value of the parameter vector Î¸, following this process for T â€œ 50
markets yields the dataset for one Monte Carlo draw.


7.1     MPEC Approach
The MPEC approach solves a nonlinear minimization problem subject to nonlin-
ear equilibrium constraints. The first step of the estimation process constructs the

                                                31
following instrumental variables for all the products in all the markets. There are
42 instruments in total; they are constructed from product characteristics xj and
excluded instruments zjt :
                                                                                               6
                                                                                               Åº
             1, xkj , x2kj , x3kj , x1j x2j x3j , zjtd , zjtd
                                                          2      3
                                                              , zjtd , zjtd x1j , zjtd x2j ,         zjtd
                                                                                               dâ€œ1

Let W denote this pJ Ë† T q Ë† 42 matrix of instruments. In our case J Ë† T â€œ 1, 250
since J â€œ 25 and T â€œ 50.
   The MPEC approach solves for Î¸ by minimizing

                                           Î· 1 W pW 1 W qÂ´1 W 1 Î·

subject to the â€œequilibrium constraintsâ€

                                                 spÎ·, Î¸q â€œ S

where S is the vector of observed market shares computed as described above given
the values of xt , pt and Î¾t and Î· is a pJ Ë† T q Ë† 1 vector defined by the following
equation:
                       N
                   1 Ã¿s                     x
                              exppÎ¸1 ` x1j Î²1i        x
                                               ` x2j Î²2i        x
                                                         ` x3j Î²3i ` pjt Î²ip ` Î·jt q
    sjt pÎ·, Î¸q â€œ
                   Ns iâ€œ1 1 ` Jkâ€œ1 exppÎ¸1 ` x1k Î²1i                     ` pkt Î²ip ` Î·kt q
                             Å™                   x         x          x
                                                    ` x2k Î²2i ` x3k Î²3i

where each pÎ²ix , Î²ip q is a random draw from the following normal distribution:
                                    Â¨Â» fi Â»             fiË›
                                      Î¸2       Î¸6 0 0 0
                                    Ëšâ€”Î¸ ffi â€” 0 Î¸ 0 0 ffiâ€¹
                                    Ëšâ€” 3 ffi â€”     7
                                  N Ëšâ€” ffi , â€”
                                                        ffiâ€¹
                                                        ffiâ€¹
                                    Ëâ€“Î¸4 fl â€“ 0 0 Î¸8 0 flâ€š
                                      Î¸5       0 0 0 Î¸9

Note that Î¸1 (like Î²0 ) is not allowed to be random. For purposes of estimation, we
set Ns â€œ 1, 000. For each Monte Carlo simulation, we start the optimization with
the following initial point: true values for Î¸, and a vector of zeros for the Î· vector.
Clearly, these starting values are not feasible for empirical researchers; we use them
to maximize the chances that the MPEC estimation will converge to a solution.



                                                        32
7.2     Our 2SLS Approach
Our 2SLS approach resorts to a slight modification of the standard linear 2SLS esti-
mator to account for the fact that the estimates of the Ïƒk2 and Ïƒp2 cannot be negative.
First, we construct the instrumental variables as the MPEC approach. We then con-
struct the artificial regressors K1 , K2 , K3 , Kp of Theorem 2 for each product in each
market by applying
                                            J
                                            Ã¿
                                   XÌ„it â€œ         xik Skt
                                            kâ€œ1

                                  Kijt   â€œ xij pxij {2 Â´ XÌ„it q

for i â€œ 1, 2, 3, p.
                                                                                      Sjt
    The next step performs an instrumental variable regression of yjt â€œ logp S0t          q on
1, x1 , x2 , x3 , x4 , K1 , K2 , K3 , Kp using all 42 instruments. If any coefficient for the
last four variables is negative, we set that coefficient to 0 and rerun the regression
without that variable. We iterate this process until all the coefficients are positive,
or all four variables are excluded from the instrumental variables regression.
   In addition to this standard 2SLS estimator, we compute a corrected estimator as
explained in section 6.3.3. To evaluate it, we replace yjt , the dependent variables for
2SLS estimates, with yjt Â´ pÎ¾2,jt Â´ Î¾8,jt q, where

    â€¢ Î¾2,jt is the residual from our initial 2SLS estimation procedure

    â€¢ Î¾8,jt is the value of Î¾jt that results from solving the equation st pÎ¾t , Î¸Ì‚q â€œ St ,
      where Î¸Ì‚ is the initial 2SLS estimate of Î¸.

We found that it worked as well as and often better than the bootstrap, at a con-
siderably lower computational cost. We also experimented with using the opti-
mal instruments, obtained by a kernel regression of X and of K on the variables
x1 , x2 , x3 , z1 , . . . , z6 .


7.3     Pseudo True Values for the 2SLS Approach
As explained earlier, the 2SLS estimator is not consistent for the true parameter
values, as it estimates an approximate model. We constructed estimates of the pseudo

                                                  33
true values to which our 2SLS estimators converge by simulating their probability
limit. A first approach increases the number of markets and computes our 2SLS
estimates for this large number of markets. The second approach computes estimates
of the population values of the moments of our 2SLS estimator.


7.3.1   Increasing-number-of-markets Approach

For each simulation, we keep the size and distribution of product characteristics for
each market fixed, but increase the number of markets. For each scenario, we cal-
culate the pseudo true value (and its standard error) by 20 simulations of 100,000
markets. Note that across different simulations, we generate different product char-
acteristics. Also, when calculating market shares, we use different random draws of
Î²i across different simulations, but the same random draws of Î²i within a simulation.
Estimates are calculated by the sample mean of the 20 simulations. Standard errors
are calculated by the sample standard errors of the 20 simulations.


7.3.2   Moment-based Approach

We can also calculate the pseudo true values in a different way. We first run the first
stage projection: Î Ì‚ â€œ pW 1 W qÂ´1 W 1 X for each simulation, where W is our matrix of
instruments and X is our matrix of regressors. We then take the average across all
the simulations to get our estimate of the population value of Î . Then in the second
stage, we calculate pW Î q1 X and pW Î q1 Y for each simulation, and then take averages
across all the simulations to get two matrices A and B. The final estimate is then
AÂ´1 B. In short, we have

                          Î  â€œ Eall simulations rpW 1 W qÂ´1 W 1 Xs
                             A â€œ Eall simulations rpW Î q1 Xs
                             B â€œ Eall simulations rpW Î q1 Y s
                                   Estimate â€œ AÂ´1 B

With this method, we only have the estimates but cannot get the standard errors.
We used 1000 simulations of 10,000 markets.



                                            34
7.4     Monte Carlo Simulation Results
We used the SNOPT optimization package available from the Stanford Systems Op-
timization Laboratory to solve the nonlinear optimization problems for the MPEC
estimator. The software employs a sparse sequential quadratic programming (SQP)
algorithm with limited-memory quasi-Newton approximations to the Hessian of the
Lagrangian.
    We run simulations for 9 scenarios obtained by setting three values for the variance
of the product random effects: ÏƒÎ¾2 â€œ VarpÎ¾q â€œ 0.1, 0.5, 1 and three values for the vector
                                             x
of variances of the coefficients Î²i â€œ pÎ²0 , Î²1i    x
                                                , Î²2i    x
                                                      , Î²3i , Î²ip q1 :

        VarpÎ²i q â€œ p0, 0.1, 0.1, 0.1, 0.05q, p0, 0.2, 0.2, 0.2, 0.1q, p0, 0.5, 0.5, 0.5, 0.2q.

Note that the square roots of the elements of VarpÎ²j q represent the relative values of
the scale parameter Ïƒ of models 1, 2, and 5.
    It is worth noting here that we explored other scenarii in which MPEC often failed
to converge, even though we are starting it from the true values of the parameters. In
particular, larger variances of Î¾ are problematic. It is also the reason why we reduced
the highest value of Ïƒp2 from 0.25 to 0.2.
   All the other parameter specifications are as described above.


7.4.1    Distribution of the Estimates

We summarize the estimation results in Tables 1 to 9, where density plots are grouped
by parameter for all scenarii. These plots suggest that if the researcher is interested
in a precise estimate of the mean of the random coefficients, then using our 2SLS
approach does not imply any significant bias or loss in efficiency relative to the MPEC
approach.
   The MPEC approach appears to dominate the 2SLS approach for the variance of
the random coefficients. The 2SLS estimators of the variances have a downward bias
that increases with the variance of the random coefficients. However, larger values of
the variance of Î¾jt do seem to improve the performance of the 2SLS estimator of the
variance of the random coefficients.
   We also implemented the Petrin and Train (2010) control function approach. To

                                                 35
do this, we first run a linear regression of the price pjt on all 42 instruments. We
denote the residuals from this regression by ÎµÌ‚jt and we include them as an additional
covariate in the mean utility of product j. The middle panel of Tables 1 to 9 presents
the distribution of estimated parameters for the case that VarpÎ¾q â€œ 0.5. The middle
panels of Tables 8 and 9 shows the results of this experiment for the mean and variance
of the price coefficients in the central case VarpÎ¾q â€œ 0.5. The control function approach
exhibits substantial bias in the estimates of both means and variances of the random
price coefficients19 .


7.4.2    Starting Values

We are giving a big advantage to MPEC in our comparisons, since we allow the
algorithm to start from the true values of the parameters. This is of course infeasible
in practice. With this initial boost, MPEC converges 100% of the time, after 1,030
iterations on average; the minimization takes 110 seconds on average. Our 2SLS
approach provides a more realistic alternative, in which we start MPEC from the
results of our 2SLS regression. This appears to work very well: MPEC converges
after an average 125 seconds and 1,280 iterations, again with a 100% success rate.
The resulting estimates are very close to those obtained when staring from the true
values: the difference is between 10Â´6 and 10Â´7 .
    These results are very encouraging for the use of our approach to find very good
starting values for the MPEC and nested-fixed point estimation procedures. Given
that 2SLS takes no time at all, we would strongly recommend running it before a
more sophisticated algorithm.


7.4.3    Pseudo-true Values

Tables 10 and 11 demonstrate that for most scenarii and coefficients, the pseudo true
values implied by our 2SLS procedure are not substantially different from the true
values. Based on these results, it is difficult to argue that a researcher would draw
conclusions from 2SLS estimates that differ in an economically or even statistically
meaningful way from those obtained with MPEC estimates.
  19
    Other experiments (not reported here) show that this bias increases with the variance of Î¾jt , as
one would expect since the control function is inexact.


                                                 36
7.4.4    Price Elasticities

Based on the parameter estimates, we can estimate the own price elasticity of the
demand for each product. The graphs in table 12 plot the distribution of the difference
between the true price elasticity and the estimated price elasticity for the MPEC
approach, our standard 2SLS approach, and our corrected 2SLS estimator. For space
reasons, we only presents the results for five products: numbers 5, 10, 15, 20, 25.
Our simulations have variances pÏƒ12 , Ïƒ22 , Ïƒ32 , Ïƒp2 q â€œ p0, 0.4, 0.4, 0.4, 0.2q with VarpÎ¾q â€œ 1.
Table 12 demonstrates that our procedure recovers nearly identical mean own-price
elasticities for products as the MPEC approach, although the spread for our estimates
is slightly larger than in the MPEC approach.
    We also performed a set of simulations (with the same variances) to determine
if changing the true value of the price coefficient Î²Â¯p changes the performance of the
estimators. The results in tables 13 and 14 reinforce our previous conclusions about
the 2SLS approach. For a range of values of the mean value of the price coefficient,
our approach introduces minimal bias in the estimates of the means of the random
coefficients. The estimates of the variances of the random coefficients for our 2SLS
estimate continue to be downwards biased in general, but the bias is smaller for larger
price coefficients.


7.4.5    Variable Selection Tests

Researchers in empirical IO have little guidance on the list of characteristics X they
should include, or how to specify the matrix Î£. Experimenting with different speci-
fications is costly with the usual estimators. Our 2SLS approach, on the other hand,
makes variable selection very easy. We can decide whether a characteristic simply
by testing whether the corresponding covariate can be dropped from the estimat-
ing equation; and to decide whether we should allow for a random coefficient, we
only need to test whether the associated artificial regressor can be dropped from the
equation. We experimented with this approach to detecting random coefficients by
setting Î²Ì„1x â€œ Ïƒ1x â€œ 0 in the data generating process and applying standard tests that
that the covariate x1 and/or the artificial regressor K1 has a zero coefficient in the
2SLS regression. We also performed this test using our corrected 2SLS estimates.
Tables 15 to 20 give the probability that the null hypothesis is not rejected, where

                                               37
the null hypothesis is

   â€¢ Î²Ì„1x â€œ 0 (Tables 15 and 16)

   â€¢ Ïƒ1x â€œ 0 (Tables 17 and 18)

   â€¢ Î²Ì„1x â€œ Ïƒ1x â€œ 0 (Tables 19 and 20).

The row labelled â€œ2SLS with heteroskedasticity-robust standard errorâ€ is our 2SLS
estimate, using a standard heteroskedasticity-robust covariance matrix to compute
standard errors. The row labelled â€œGLS estimator and standard errorsâ€ uses Craggâ€™s
(1983) generalized least squares estimator and his recommended standard error es-
timates. The row labelled â€œ2SLS with clustered standard errorsâ€ uses our 2SLS
estimates with standard errors clustered at market level.
    Since the null hypothesis is true, each row in Tables 15-20 would ideally contain
0.99, 0.95, and 0.90. Clearly, our test rejects the null too often. In this particu-
lar application, this is probably better than the alternative: better to include more
variables and lose some efficiency than to incur bias by leaving them out. The size
distortion is smaller for tests on the means (Tables 15 and 16); it is also smaller when
we use corrected estimates. The clustered standard error estimates appear to have
the largest size distortions. On the whole, we take this to suggest that demonstrate
that our estimator can be used to good effect in order to decide which coefficients
should be modelled as random.


7.5    Lognormal Distribution for Î²
As explained in section 5.2, our estimating equation is the same whether the dis-
tribution of the random coefficients is normal or not. To illustrate this, we modify
the data-generating process so that the consumer preference parameters Î²i have a
lognormal distribution:

                                   Î²i â€œ Î²Ì„i i
                                   Î²Ì„i â€œ p1, 1.5, 1.5, 0.5, 1q
                              lnpi q â€ N pÂ´0.5Ïƒ 2 , Ïƒ 2 q.


                                             38
We study several cases, with Ïƒ â€œ 0.3, 0.4, 0.5 and Î¾jt â€ N p0, 0.1q. The rest of the
specification is as before. Lognormality induces significant skewness and kurtosis into
the distribution of the random coefficients. The standard 2SLS approach gives us
estimates of the first and second moments. We can also introduce the additional
artificial regressors T of section 6.2, either to control for skewness or to estimate it.
We experimented with both possibilities. Each plot in Tables 21, 22, and 23 shows
the distributions when we use only X and K (â€œonly include 2nd momentâ€) and when
we add T (â€œinclude third momentâ€). These three tables report the distributions of
the estimates of the first, second, and third moments of Î²1 , Î²2 , Î²3 , and Î²p . Table 24
provides the corresponding summary statistics.
    For a variety of values for the parameter Ïƒ of our lognormal distribution, the
2SLS estimates are just as good as they were in the normal setup. The additional
information in the third moment of the random coefficients does not appreciably
increase the precision in our estimates of the means and variances of the random
coefficients. In fact, for some of the coefficients, including the third-order artificial
regressors T leads to significantly less efficient estimates. This is likely due to the fact
that our procedure has a difficult time estimating the third moment of the random
coefficients, as Table 23 shows.


7.5.1   Correction and Kernel

Our paper suggested two potential improvements to the standard 2SLS regression:
our two-step asymptotic correction described in section 6.3.3, and using a kernel
regression to estmate the optimal instruments EpX|Zq and EpK|Zq. We compare
both methods, when coefficients are normal with variances p0, 0.5, 0.5, 0.5, 0.2q and
when they are lognormally distributed with Ïƒ â€œ 0.4 for lnpi q â€ N pÂ´0.5Ïƒ 2 , Ïƒ 2 q. In
both cases we took V arpÎ¾q â€œ 0.1.
   Tables 25 and 26 plot the distributions of the estimators. They suggest that our
correction helps reduce the bias, both for the means and the variances. This holds
whether the random coefficients are normally or log-normally distributed. Using
kernel regressions to approximate the optimal instruments appears to slightly reduce
both the bias and the variance of some of the estimates.



                                            39
Concluding Comments
Our FRAC estimation procedure applies directly to the random coefficients demand
models commonly used in empirical IO. For the most part, our Monte Carlo results
confirm the findings from the expansions. The 2SLS approach yields reliable estimates
of the parameters of the model and of economically meaningful quantities such as price
elasticities; and it does so at a very minimal cost. It is â€œrobustâ€ to variations on the
distribution of the random coefficients. In addition, it provides straightforward tests
that help in variable selection, especially as a guide to determine which coefficients
in the demand system should be modeled as random.
   Some of our simulation results point to directions for future research. We hope to
report more general analytical results that illuminate these findings.



References



    Amemiya, T. (1975), â€œThe nonlinear limited-information maximum-likelihood
estimator and the modified nonlinear two-stage least-squares estimator,â€ Journal of
Econometrics, 3, 375â€“386.
  Armstrong, T. (2016), â€œLarge Market Asymptotics for Differentiated Product
Demand Estimators With Economic Models of Supplyâ€, Econometrica, 84, 1961-1980.
    Berry, S. (1994), â€œEstimating Discrete Choice Models of Product Differentia-
tionâ€, Rand Journal of Economics, 23, 242-262.
   Berry, S., Levinsohn, J., and A. Pakes (1995), â€œAutomobile Prices in Market
Equilibriumâ€, Econometrica, 60, 889-917.
   Campioni, G. (2018), â€œNonparametric Demand Estimation in Differentiated
Products Marketsâ€, mimeo Yale.
   Chesher, A. (1991), â€œThe Effect of Measurement Errorâ€, Biometrika, 78, 451â€“
462.




                                          40
  Chesher, A. and J. Santos-Silva (2002), â€œTaste Variation in Discrete Choice
Modelsâ€, Review of Economic Studies, 69, 147â€“168.
    Cragg, J.G. (1983), â€œMore Efficient Estimation in the Presence of Heteroscedas-
ticity of Unknown Formâ€, Econometrica, 51, 3, 751â€“763.
   Crawford, G., and A. Yurukoglu (2012), â€œThe Welfare Effects of Bundling
in Multichannel Television Markets,â€ American Economic Review, 102, 643-685.
   DubeÌ, J.-P., Fox, J., and C.-L. Su (2012), â€œImproving the Numerical Per-
formance of BLP Static and Dynamic Discrete Choice Random Coefficients Demand
Estimationâ€, Econometrica, 80, 2231-2267.
   Fox, J., Kim, K., Ryan, S.. and P. Bajari (2011), â€œA simple estimator for
the distribution of random coefficientsâ€, Quantitative Economics, 2, 381â€“418.
    Gandhi, A. and J.-F. Houde (2016), â€œMeasuring Substitution Patterns in
Differentiated Products Industriesâ€, mimeo.
   Harding, M. and J. Hausman (2007), â€œUsing a Laplace Approximation to
Estimate the Random Coefficients Logit Model by Nonlinear Least Squaresâ€, Inter-
national Economic Review, 48, 1311â€“1328.
   Ho, K. and R. Lee (2017), â€œInsurer Competition in Health Care Marketsâ€,
Econometrica, 85, 379-417.
   Horowitz, J. and L. Nesheim (2018), â€œUsing Penalized Likelihood to Select
Parameters in a Random Coefficients Multinomial Logit Modelâ€, mimeo UCL.
  Kadane, J. (1971), â€œComparison of k-class Estimators when the Variance is
Smallâ€, Econometrica, 39, 723â€“737.
   Ketz, P. (2018), â€œOn Asymptotic Size Distortions in the Random Coefficients
Logit Modelâ€, mimeo Paris School of Economics.
  Kristensen, D. and B. SalanieÌ (2017), â€œHigher-order Properties of Approxi-
mate Estimatorsâ€, Journal of Econometrics, 189â€“208.
    Lee, J. and K. Seo (2015), â€œA computationally fast estimator for random
coefficients logit demand models using aggregate dataâ€, Rand Journal of Economics,
46, 86-102.
   Reynaert, M. and F. Verboven (2014), â€œImproving the Performance of Ran-
dom Coefficients Demand Models: The Role of Optimal Instrumentsâ€, Journal of

                                        41
Econometrics, 179, 83-98.
    Robinson, P. (1988), â€œThe Stochastic Difference between Econometric Statis-
ticsâ€, Econometrica, 56, 531â€“548.
   Su, C.-L. and K. Judd (2012), â€œConstrained Optimization Approaches to
Estimation of Structural Modelsâ€, Econometrica, 80, 2213-2230.
    White, H. (1982), â€œInstrumental Variables Regression with Independent Ob-
servationsâ€, Econometrica, 50(2), 483-499.




                                      42
A        Proof of Theorem 1
Proof. We start from (2); we drop the market index t and the bold letters. Since we
now denote X â€œ Ïƒx Â¨ v, we can rewrite (2) in the standard model as
                                      exppXj Î² ` Ïƒxj Â¨ v ` Î¾j q
                       S j â€œ Ev      Å™J                                .
                                  1 ` kâ€œ1 Sk exppXk Î² ` Ïƒxk Â¨ v ` Î¾k q
Given that
                                   Sj                     Ïƒ2
                        Î¾j â€œ log      Â´ Xj Î² ` a1j Ïƒ ` a2j ` OpÏƒ 3 q,
                                   S0                     2
we get                          Â´                                   Â¯
                                                          2
                         Sj exp Ïƒ pxj Â¨ v ` a1j q ` a2j Ïƒ2 ` OpÏƒ 3 q
            Sj â€œ Ev
                     S0 ` Jkâ€œ1 Sk exp Ïƒ pxk Â¨ v ` a1k q ` a2k Ïƒ2 ` OpÏƒ 3 q
                         Å™           `                          2         Ë˜

Eliminating Sj gives
                              Â´ `            Ë˜     2
                                                                  Â¯
                          exp Ïƒ x1j v ` a1j ` Ïƒ2 a2j ` OpÏƒ 3 q
             1 â€œ Ev                                                      Ë˜.                 (13)
                     S0 ` Jkâ€œ1 Sk exp Ïƒ px1k v ` a1k q ` Ïƒ2 a2k ` OpÏƒ 3 q
                         Å™           `                      2



In this form, Theorem 1.(i) is obvious since only the vectors xk and market shares Sk
enter the system of equations.
    Now use the notation eS Z â€ Jkâ€œ1 Sk Zk and ZÌ‚j â€œ Zj Â´ eS Z to rewrite (13) as
                                  Å™
                                         Ëœ          Â¸
                                              VÌ‚j
                                 0 â€œ Ev                                          (14)
                                           1 ` eS V
where Vj â€ fj ` Î±j ` fj Î±j , with
                                                         Ïƒ2
                  fj â€ exppÏƒxj Â¨ vq Â´ 1 â€œ Ïƒxj Â¨ v `         pxj Â¨ vq2 ` OP pÏƒ 3 q
                                                         2
and
                                                                  Ïƒ2
         Î±j â€œ exppa1j Ïƒ ` a2j Ïƒ 2 {2 ` OpÏƒ 3 qq Â´ 1 â€œ a1j Ïƒ ` pa2j ` a21j q
                                                                     ` OpÏƒ 3 q.
                                                                   2
We note that fj is OP pÏƒq and Î±j is OpÏƒq, so that Vj is also OP pÏƒq.
    Now expanding (14) gives
                              Â´           Â¯
         OpÏƒ 3 q â€œ Ev VÌ‚j Â´ Ev VÌ‚j peS V q                                                  (15)
                  â€œ Ev fË†j ` Î±Ì‚j                                                            (16)
                  ` Î±{                  Ë†                                     Ë†
                     j Ev fj Â´ peS Î±qEv fj Â´ Î±Ì‚j Ev peS f q Â´ Î±Ì‚j peS Î±q Â´ Ev fj peS f q.   (17)

                                                43
Only the terms on line (16) can be of order 1 in Ïƒ. But using Ev v â€œ 0 and Ev pxj Â¨
                                        2
vqpxk Â¨ vq â€œ xj Â¨ xk gives us Ev fj â€œ Ïƒ2 kxj k2 ` OpÏƒ 3 q. Therefore the only term of order
1 is in Î±Ì‚j â€œ ap1j Ïƒ ` OpÏƒ 2 q, and we must have ap1j â€œ 0. We note that the â€œhatâ€ operator
is linear and invertible:
Lemma 1. If ZÌ‚j â€œ WÌ‚j for all j and S0 Äƒ 1, then Zj â€œ Wj .

Proof. Zj Â´ eS Z â€œ Wj Â´ eS W implies Zj â€œ Wj ` Î», where Î» â€œ eS Z Â´ eS W . But then
eS Z â€œ eS W ` eS Î» â€œ p1 Â´ S0 qÎ», so that Î» â€œ p1 Â´ S0 qÎ» â€œ 0.

    Applying the lemma gives a1j â€œ 0. As a consequence, Î±j â€œ a2j Ïƒ 2 {2 ` OpÏƒ 3 q; and
all terms on line (17) except the last one are of order at least 3 in Ïƒ. Since

                                        Ïƒ 2 z2 Ïƒ 2
                          Ev fË†j ` Î±Ì‚j â€œ kx jk `   ap2j ` OpÏƒ 3 q
                                        2        2
and

        Ev fË†j peS f q â€œ Ïƒ 2 Ev pxÌ‚j Â¨ vqppeS xq Â¨ vq ` OpÏƒ 3 q â€œ Ïƒ 2 pxÌ‚j Â¨ peS xqq ` OpÏƒ 3 q

applying the lemma again gives us pkxj k2 ` a2j q{2 Â´ xj Â¨ peS xq â€œ 0.
   Finally, if the distribution of v is symmetric around 0 changing Ïƒ to Â´Ïƒ in (2)
must leave all market shares unchanged; therefore all expansions can only contain
even-degree terms in Ïƒ.



B     Detailed Examination of the Mixed Logit
The standard binary model is simply a mixed logit. Applying Theorem 1 with J â€œ 1
and using S0 ` S1 â€œ 1, we obtain

                                     a21 â€œ p2S1 Â´ 1qkx1 k2

and K 1 â€œ p1{2 Â´ S1 qX1 X11 . Therefore
                                      Ë†       Ë™
                        S1              1
               Î¾1 â€œ log      Â´ X1 Î² Â´     Â´ S1 Tr Î£X1 X11 ` OpÏƒ k q
                        S0              2

where k â€œ 3 in general, and k â€œ 4 if the distribution of  is symmetric around zero.

                                                 44
    The presence of the term p1{2 Â´ S1 q in this formula is a consequence of the sym-
metry of the distribution of v around 0 and of the logistic distribution around 0.
Taken together, this implies that market shares around one half vary very little with
Ïƒ. The random variation in tastes can only be identified from nonlinearities in the
market shares; but since the cdf of the logistic has an inflexion point when its value
is one half, market shares are essentially linear around that point. It is easy to check
that this is specific to the one-product case; when J Ä… 1, the mixed multinomial logit
does not face any such difficulty.
   Let us focus for simplicity on the case when random variation in preferences is
uncorrelated across covariates: Î£ is the nX Ë† nX diagonal matrix with elements Î£mm .
Then given instruments such that E pÎ¾1 |Zq â€œ 0, the approximate model is
                 Ëœ                  Ë†          nX
                                              Ë™Ã¿               Â¸
                       S1              1                 2
                                                           Ë‡
               E log      Â´ X1 Î² Â´       Â´ S1      Î£mm X1m Ë‡ Z â€œ 0.             (18)
                                                           Ë‡
                       S0              2       mâ€œ1



B.1     Identification
The form of the estimating equation holds interesting insights about identification.
First note that the optimal instruments are
                                            Ë†Ë†       Ë™      Ë™
                                               1        2
                      f pZq â€œ E pX1 |Zq , E      Â´ S1 X1 |Z
                                               2
where X12 is the vector with components X1m  2
                                               . The asymptotic variance-covariance
matrix of our estimator Î¸Ì‚ is given by the usual formula:

                            T Vas Î¸Ì‚ Â» J Â´1 V pÎ¾1 f pZqqJ Â´1 ,

where                          Ë†Ë†      Ë†       Ë™      Ë™     Ë™
                                        1           2
                        J â€œE      X1 ,    Â´ S1 X1 f pZq .
                                        2
The identifying power of the (approximate) model relies on the full-rank of the ma-
trix J . Suppose for instance that after projecting (via nonparametric regression)
the regressors on the instruments, the residual variation in the artificial regressor
             2
p1{2 Â´ S1 qX1m  is very well explained in a linear regression on the other covariates.
Then the estimate of Î£mm will be very imprecise, and random taste variation on the
characteristic X1m is probably best left out of the model. Of course, this can be
diagnosed immediately by looking at the precision of the 2SLS estimates.

                                           45
B.2     Higher-order terms
It is easy to program a symbolic algebra system to compute higher-order terms alj for
l Ä… 2. We show here how to compute the fourth-order term in the mixed logit model
by hand. This will also illustrate the â€œrobustnessâ€ of our method to distributional
assumptions.
    Assume that  has a distribution that is symmetric around zero, and that its com-
ponents are independent of each other with variances Î£mm and fourth-order moments
km . As before, we assume that Î£mm is of order Ïƒ 2 and km is of order Ïƒ 4 . We also
assume that we can take expansions to order L Ä› 5.
   Since the distribution is symmetric, we already know that
                                S1          a21 2 a41 4
                     Î¾1 â€œ log      Â´ X1 Î² `    Ïƒ `    Ïƒ ` OpÏƒ 6 q.
                                S0           2     24
Define Lptq â€œ 1{p1 ` exppÂ´tqq the cdf of the logistic distribution. Note that L1 â€œ
Lp1 Â´ Lq, and that higher-order derivatives follow easily:

                        L2 â€œ Lp1 Â´ Lqp1 Â´ 2Lq
                      Lp3q â€œ Lp1 Â´ Lqp1 Â´ 6L ` 6L2 q
                      Lp4q â€œ Lp1 Â´ Lqp1 Â´ 2Lqp1 Â´ 12L ` 12L2 q.

   Since the market share of good 1 is

                                S1 â€œ E L pX1 pÎ² ` q ` Î¾1 q

we obtain, much as in Appendix A,
                         Ë†                                   Ë™
                              S1              2      4     6
                S1 â€œ E L log    ` X1  ` Î±2 Ïƒ ` Î±4 Ïƒ ` OpÏƒ q
                              S0

where we defined Î±l â€œ al1 {l! for l â€œ 2, 4.
    Let a 0 subscript indicate that we take the value and derivatives of Lptq at t â€œ
logpS1 {S0 q. Defining upq â€œ X1  ` Î±2 Ïƒ 2 ` Î±4 Ïƒ 4 ` OpÏƒ 6 q and expanding gives
             Ë†            Ë™                           p3q      p4q
                 S1                            L20 2 L0 3 L0 4
           L log    `u        â€œ L0 ` L10 u `      u `   u `    u ` Opu5 q.
                 S0                            2      6     24


                                               46
                                                           p4q
Incorporating L0 â€œ S1 , L10 â€œ S1 p1 Â´ S1 q, up to L0 gives
         Ë†
                                                       upq2
S1 â€œ E S1 ` S1 p1 Â´ S1 qupq ` S1 p1 Â´ S1 qp1 Â´ 2S1 q
                                                         2
                                     3                                               4
                                                                                                 Ë™
                              2 upq                                          2 upq           5
   ` S1 p1 Â´ S1 qp1 Â´ 6S1 ` 6S1 q      ` S1 p1 Â´ S1 qp1 Â´ 2S1 qp1 Â´ 12S1 ` 12S1 q      ` Opupq q ;
                                   6                                              24
dividing by S1 p1 Â´ S1 q yields

E u`p1Â´2S1 qE u2 {2`p1Â´6S1 `6S12 qE u3 {6`p1Â´2S1 qp1Â´12S1 `12S12 qE u4 {24 â€œ E Opu5 q.
                                                                                 (19)
   Finally, up to order 6 in Ïƒ:

                   E u â€œ Î± 2 Ïƒ 2 ` Î± 4 Ïƒ 4
                                                          nX
                                                          Ã¿
                  E u2 â€œ Ïƒ 2 E pX1 q2 ` Î±22 Ïƒ 4 â€œ             Î£mm x21m ` Î±22 Ïƒ 4
                                                        mâ€œ1
                                                        nX
                                                        Ã¿
                  E u3 â€œ 3Î±2 Ïƒ 4 E pX1 q2 â€œ 3Î±2               Î£mm x21m
                                                        mâ€œ1
                                              nX
                                              Ã¿
                  E u4 â€œ Ïƒ 4 E pX1 q4 â€œ          km x41m .
                                              mâ€œ1
                         2
Regrouping terms in Ïƒ in (19) confirms that
                                                     nX
                                                     Ã¿
                             Î±2 Ïƒ 2 â€œ pS1 Â´ 1{2q           Î£mm x21m ,
                                                     mâ€œ1

which we knew from Theorem 1. The terms in Ïƒ 4 give us
                                                                         nX
                                                                         Ã¿
            Î±4 Ïƒ 4 â€œ Î±22 Ïƒ 4 pS1 Â´ 1{2q Â´ Î±2 Ïƒ 2 p1 Â´ 6S1 ` 6S12 q              Î£mm x21m {2
                                                                         mâ€œ1
                                                        nX
                                                        Ã¿
                  Â´ p1 Â´ 2S1 qp1 Â´ 12S1 ` 12S12 q                km x41m {24.
                                                       mâ€œ1

This simplifies to
          Ë†        Ë™
      4      1
  Î±4 Ïƒ â€œ       Â´ S1 Ë†
             2
        Â¨                                Â¸2 Ë†                                Ë›
         Ë†                 Ë™ËœÃ¿
                             nX                                  nX
                                                                Ë™Ã¿
        Ë 1 Â´ 2S1 p1 Â´ S1 q      Î£mm x21m Â´
                                              1
                                                 Â´ S1 p1 Â´ S1 q      km x41m â€š.
            4                mâ€œ1
                                              12                 mâ€œ1



                                              47
   This formula may not seem especially enlightening, but it shows several impor-
tant points. First, terms of higher orders can be computed without much difficulty.
                                                                            2
Second, each additional term adds information on lower-order moments (here Ïƒm ), as
well as on the moments of higher order (here km ). The model remains linear in the
highest order moments; here for km we have new artificial regressors
                         Ë†        Ë™Ë†                   Ë™
                           1           1
                             Â´ S1        Â´ S1 p1 Â´ S1 q x41m .
                           2          12
On the other hand, the higher-order expansions introduce nonlinear functions of the
lower-order moments, here represented by
                Ë†         Ë™Ë†                  Ë™ËœÃ¿ nX
                                                             Â¸2
                   1         1
                     Â´ S1      Â´ 2S1 p1 Â´ S1 q       Î£mm x21m ,
                   2         4                   mâ€œ1

and the model is not linear in these parameters any more. This could be dealt with
in several ways: by nonlinear optimization (of a very simple kind), or by iterative
methods. In any case, we will see in our simulations that stopping with the second-
order expansion often gives results that are already very reliable.
    Finally, while the estimator based on the second-order expansion is â€œrobustâ€ to
any (well-behaved) distribution, the estimator based on this fourth-order expansion
also assumes symmetry: a skewed distribution would generate terms in Ïƒ 3 . Making
more assumptions changes the form of the artificial regressors. To illustrate this,
consider a mixed logit with one covariate only (nX â€œ 1). The expansion to order 2L
can be written
                                      L
                          S1         Ã¿           `       Ë˜k
                 Î¾1 â€œ log    Â´ Î²X1 `     tk pS1 q Î£11 X12 ` OpÏƒ 2L`2 q.
                          S0         kâ€œ1

Assume that  has normal kurtosis. Then k1 â€œ 3Î£211 and we find the simpler formula
                                   Ë†       Ë™
                                     1
                        t2 â€œ Î±4 â€œ      Â´ S1 S1 p1 Â´ S1 q.
                                     2
    Specializing further, Figure 1 in the main text plots the terms tk pS1 q for k â€œ
1, 2, 3, 4 as the market share goes from zero to one when  is Gaussian.
     If the components of  are independently distributed and have third moments
ps1 , . . . , snX q, then it is easy to see that an additional term
                                    Ë†                 Ë™ nX
                                                     1 Ã¿        3
                                      S1 p1 Â´ S1 q Â´       sm X1m
                                                     6 mâ€œ1

                                        48
enters the expansion. To test for skewness on covariate m, one could simply test that
              `                Ë˜ 3
the regressor S1 p1 Â´ S1 q Â´ 61 X1m  can be omitted.


B.3     Beyond Logit and Gaussian
The properties of the logistic function may seem to have been more central to our
calculations; but in fact they are quite ancillary. Suppose that ui1t Â´ ui0t has some
distribution with cdf Q instead of L. While the derivatives of Q may not obey the
nice polynomial formulÃ¦ we used for L, it is still true that if Q is invertible and
smooth then we can define functions Fk by

                                    Qpkq ptq â€œ Fk pQptqq.

This is all we need to carry out the expansions. One can show for instance that the
factor pS1 Â´ 12 q that appears in (18) just needs to be replaced with

                                             F2 pS1 q
                                         Â´             .
                                             2F1 pS1 q

Take for instance a mixed binary model with such a general distribution for u1 Â´
u0 , and a distribution of the random coefficient on the single covariate X1 that has
successive moments 0, Î£, Âµ3 , Âµ4 . Then it is easy to derive the following fourth-order
expansion, which could perhaps serve as the basis for a semiparametric estimator:

                S1              F2 pS1 q 2
        Î¾2 â€œ log     Â´ Î²X1 Â´            X Î£
                S0              F1 pS1 q 1
            F3 pS1 q 3
          Â´          X Âµ3
            F1 pS1 q 1
                     Ëœ              Ë†           Ë™2 Â¸
            F2 pS1 q     F3 pS1 q      F2 pS1 q               F4 pS1 q
          `            3          Â´                  X14 Î£2 Â´          Âµ4 X14 ` OpÏƒ 5 q.
            F1 pS1 q     F1 pS1 q      F1 pS1 q               F1 pS1 q

This can be extended in the obvious way to make v heteroskedastic (just replace Î£
with Ep2 |X1 q and Âµm with EpÎµm |X1 q in the above formula.)




                                               49
C        The Two-level Nested Logit
In the unmixed model (Ïƒ â€œ 0) the mean utility of alternative j is Uj â€œ Ik `Î»k log Sj|Nk
if j P Nk , with Ik â€ logpSNk {S0 q and Sj|Nk â€ Sj {SNk .This gives

                          Î¾j0 â€œ Â´Xj Î² ` logpSNk {S0 q ` Î»k log Sj|Nk .

We write (imposing a1j â€œ 0 from the start as this is a general property of models
with Ev â€œ 0)

                                                                              Ïƒ2
                   Uj pvq â€œ logpSNk {S0 q ` Î»k log Sj|Nk ` Ïƒxj Â¨ v `             a2j
                                                                              2
and
                                        Ã¿
               exppIk pvq{Î»k q â€œ               exppUj pvq{Î»k q â€œ pSNk {S0 q1{Î»k fÂ¯k pvq
                                        jPNk
                            Å™
where we denote XÌ„k â€œ           jPNk   Sj|Nk Xj and
                   Ë†                      Ë™
                       Ïƒ Â´           a2j Â¯       Ïƒ           Ïƒ2 `                 Ë˜
    fj pvq â€œ exp          xj Â¨ v ` Ïƒ        Â» 1 ` pxj Â¨ vq ` 2 Î»k a2j ` pxj Â¨ vq2
                       Î»k             2          Î»k         2Î»k

so that
                                    Ïƒ           Ïƒ2 `
                       fÂ¯k pvq Â» 1 ` x
                                                                      Ë˜
                                       sk Â¨ v ` 2 Î»k aÌ„2k ` px
                                                             Ä Â¨ vq2 k .
                                    Î»k         2Î»k
Now using
                                                                   exppIk pvqq
                   Sj â€œ Ev expppUj pvq Â´ Ik pvqq{Î»k q              Å™K
                                                                1 ` lâ€œ1 exppIl pvqq
we get                             Ëœ                                   Â¸
                                                            Ë˜Î»k
                                                      Â¯
                                                          `
                                        fj pvq       f k pvq
                          1 â€œ Ev        Â¯                           Ë˜Î» .
                                                         SNl fÂ¯l pvq l
                                                    Å™K
                                        fk pvq S0 `
                                                             `
                                                          lâ€œ1

We note that
            1 ` aÏƒ ` bÏƒ 2
                          â€œ 1 ` pa Â´ cqÏƒ ` pb Â´ d Â´ cpa Â´ cqqÏƒ 2 ` OpÏƒ 3 q.               (20)
            1 ` cÏƒ ` dÏƒ 2

Denote AÌ‚j|k â€œ Aj Â´ AÌ„k . Applying (20) gives

                              fj pvq        Ïƒ             Ïƒ2
                                      Â» 1 `    C j pvq `      Dj pvq.
                              fÂ¯k pvq       Î»k           2Î»2k


                                                     50
with
                                          Cj pvq â€œ xÌ‚j|k Â¨ v
and
                      Dj pvq â€œ Î»k ap2j|k ` px
                                            { Â¨ vq2 j|k Â´ 2pxÌ„k Â¨ vqpxÌ‚j|k Â¨ vq.
Moreover,
                                                 Ëœ                                           Â¸
                       Ë˜Î»                   Ïƒ2       Î»l Â´ 1                     px
                                                                                 Ä Â¨ vq2 l
                fÂ¯l pvq l Â» 1 ` Ïƒ xÌ„l Â¨ v `
            `
                                                            pxÌ„l Â¨ vq2 ` aÌ„2l `
                                            2          Î»l                          Î»l

and
                                                   Â´                             Ä2
                                                                                        Â¯
             Ë˜Î»k                                Ïƒ2       Î»k Â´1            2     pxÂ¨vq
      fÂ¯k pvq                   1 ` Ïƒ xÌ„k Â¨ v ` 2 aÌ„2k ` Î»k pxÌ„k Â¨ vq ` Î»k            k
       `
                    Ë˜Î»l Â»                     Â´                 Â´                             Â¯Â¯
S0 ` lâ€œ1 SNl fÂ¯l pvq
    Å™K        `                                                                         Ä2
                          1 ` ÏƒeS x Â¨ v ` Ïƒ2 eS a2 ` K
                                            2                     Î»l Â´1           2 ` pxÂ¨vq l
                                                     Å™
                                                       lâ€œ1 SN l    Î»l
                                                                        pxÌ„ l Â¨ vq        Î»l
                            Å™J                Å™K
where as usual eS T â€œ         jâ€œ1   Sj Tj â€œ    kâ€œ1   SNk TÌ„k .
   Then, using (20) again,
                                 Ë˜Î»
                          fÂ¯k pvq k
                        `
                                                         Ïƒ2
                                      Ë˜Î»l Â» 1 ` ÏƒEk pvq ` Fk pvq
                 S0 ` lâ€œ1 SNl fÂ¯l pvq
                      Å™K          `
                                                         2

with
                                      Ek pvq â€œ pxÌ„k Â´ eS xq Â¨ v
and

                       Fk pvq â€œ aÌ„2k Â´ eS a2
                                                    K
                                Î»k Â´ 1          2
                                                   Ã¿       Î»l Â´ 1
                              `        pxÌ„k Â¨ vq Â´     SNl        pxÌ„l Â¨ vq2
                                  Î»k               lâ€œ1
                                                             Î»l
                                             K
                                px
                                 Ä Â¨ vq2 k Ã¿        px
                                                     Ä Â¨ vq2 l
                              `           Â´     SNl
                                   Î»k       lâ€œ1
                                                       Î»l
                              Â´ 2peS x Â¨ vqppxÌ„k Â´ eS xq Â¨ vq.

This allows us to write
                   Ë†                 Ë™Ë†                Ë™
                        Ïƒ       Ïƒ2                  Ïƒ2
            1 Â» Ev 1 ` C j ` 2 D j       1 ` ÏƒEk ` Fk
                        Î»k     2Î»k                  2
                   Ë†     Ë†         Ë™     2 `
                                                               Ë™
                           Cj          Ïƒ          2
                                                             Ë˜
              Â» Ev 1 ` Ïƒ      ` Ek ` 2 Dj ` Î»k Fk ` 2Î»k Cj Ek .
                           Î»k         2Î»k

                                                     51
We have Ev Cj â€œ Ev Ek â€œ 0; also,

                         EDj â€œ Î»k ap2j|k ` kxj k2 Â´ kxk
                                                    Ä˜2 Â´ 2xÌ„k Â¨ xÌ‚j|k
                                                        k

                         EFk â€œ aÌ„2k Â´ eS a2
                                                 K
                                Î»k Â´ 1       2
                                                Ã¿       Î»l Â´ 1
                              `        kxÌ„k k Â´     SNl        kxÌ„l k2
                                  Î»k            lâ€œ1
                                                          Î»l

                                Ä˜2       K      Ä˜2
                                kxk k
                                        Ã¿       kxk  l
                              `       Â´     SNl
                                 Î»k     lâ€œ1
                                                 Î» l

                              Â´ 2peS xq Â¨ pxÌ„k Â´ eS xq
                    EpCj Ek q â€œ xÌ‚j|k Â¨ pxÌ„k Â´ eS xq.

Writing EpDj ` Î»2k Fk ` 2Î»k Cj Ek q â€œ 0 gives us an equation of the form

                   Î»k pa2j Â´ aÌ„2k q ` Î»2k paÌ„2k Â´ eS a2 q â€œ Î»2k M ` Î½k ` Âµj

where
     K                      K      Ä˜2
     Ã¿     Î»l Â´ 1       2
                           Ã¿       kxk
Mâ€œ     SNl        kxÌ„l k `     SNl     l
                                         Â´ 2keS xk2
   lâ€œ1
             Î»l            lâ€œ1
                                    Î»l

     Ä˜2 Â´ 2kxÌ„k k2 Â´ Î»k pÎ»k Â´ 1qkxÌ„k k2 Â´ Î»k kxk
Î½k â€œ kxk                                     Ä˜2 ` 2Î»2 eS x Â¨ xÌ„k ` 2Î»k kxÌ„k k2 Â´ 2Î»k xÌ„k Â¨ eS x
          k                                        k   k
              `                     2
                                                     Ë˜
                Ä˜2 Â´ p2 Â´ Î»k qkxÌ„k k Â´ 2Î»k xÌ„k Â¨ eS x
   â€œ p1 Â´ Î»k q kxk                                                               (21)
                     k
              2
Âµj â€œ Â´kxj k ` 2xj Â¨ xÌ„k Â´ 2Î»k xj Â¨ pxÌ„k Â´ eS xq
   â€œ xj Â¨ p2Î»k eS x Â´ xj ` 2p1 Â´ Î»k qxÌ„k q .                                        (22)

It is easy to aggregate from a2j â€œ p1 Â´ Î»k qaÌ„2k ` Î»k eS a2 ` Î»k M ` pÎ½k ` Âµj q{Î»k to
                                                        Î½k ` ÂµÌ„k
                                aÌ„2k â€œ eS a2 ` M `
                                                           Î»2k

and then to
                                                    K
                                                    Ã¿           Î½k ` ÂµÌ„k
                         S0 eS a2 â€œ p1 Â´ S0 qM `          SNk            ,
                                                    kâ€œ1
                                                                   Î»2k




                                               52
which gives
                                            Î½k ` ÂµÌ„k Î½k ` Âµj
              a2j â€œ eS a2 ` M ` p1 Â´ Î»k q           `
                                               Î»2k      Î»k
                            K
                     M   1 Ã¿       Î½l ` ÂµÌ„l             Î½k ` ÂµÌ„k Î½k ` Âµj
                 â€œ     `       SNl     2
                                            ` p1 Â´ Î»k q         `
                     S0 S0 lâ€œ1        Î»l                   Î»2k      Î»k
                            K
                     M   1 Ã¿       Î½l ` ÂµÌ„l Î½k ` p1 Â´ Î»k qÂµÌ„k Âµj
                 â€œ     `       SNl         `                 ` .
                     S0 S0 lâ€œ1        Î»2l          Î»2k        Î»k

Finally, using equations (21) and (22) we aggregate

                     ÂµÌ„k â€œ 2Î»k xÌ„k Â¨ eS x ` 2p1 Â´ Î»k qkxÌ„k k2 Â´ kxk
                                                                Ä˜2 ,
                                                                    k


which gives
                Î½k ` ÂµÌ„k â€œ 2Î»2k xÌ„k Â¨ eS x ` Î»k p1 Â´ Î»k qkxÌ„k k2 Â´ Î»k kxk
                                                                      Ä˜2
                                                                          k

and
                        Î½k ` p1 Â´ Î»k qÂµÌ„k â€œ Â´Î»k p1 Â´ Î»k qkxÌ„k k2 .
Putting everything together, we get
                              K
                    M    1 Ã¿        Î½l ` ÂµÌ„l Î½k ` p1 Â´ Î»k qÂµÌ„k Âµj
              a2j â€œ   `        SNl            `                   `
                    S0 S0 lâ€œ1          Î»2l             Î»2k           Î»k
                      Ëœ                                                    Â¸
                        K                         K           2
                    1   Ã¿      Î»l Â´ 1            Ã¿       kxk l
                                                         Ä˜
                  â€œ        SNl         kxÌ„l k2 `     SNl        Â´ 2keS xk2
                    S0 lâ€œ1       Î»l              lâ€œ1
                                                           Î»l
                                   K       Ä˜2 ` p1 Â´ Î»l qkxÌ„l k2
                   2       2    1 Ã¿       Â´kxk  l
                 ` keS xk `           SNl
                   S0          S0 lâ€œ1               Î»l
                       Ë†                          Ë™
                                xj     1 Â´ Î»k         1 Â´ Î»k
                 â€œ xj Â¨ 2eS x Â´     `2        xÌ„k Â´          kxÌ„k k2 .
                                Î»k        Î»k            Î»k


D     Our Correction Formula
Remember from section 6.3.2 that
                              Ë†                Ë™Â´1
                                 Bf8
                   Î¸0 Â» Î¸2 Â´ E       pÎ¸2 ; Î»0 q    f8 pÎ¸2 ; Î»0 q.              (23)
                                  BÎ¸


                                            53
The term in the inverse is easily proxied:
                                                    Ë†            Ë™1
                 Bf8                Bf2                 BÎ¾2             BÎ¾2
               E     pÎ¸2 ; Î»0 q Â» E     pÎ¸2 q â€œ E           pÎ¸2 q V V 1     pÎ¸2 q,
                  BÎ¸                BÎ¸                  BÎ¸              BÎ¸
since Î¾2 is linear in Î¸. Note that this is EX 1 X , where
                                      BÎ¾2
                             X â€V1        pÎ¸2 q â€œ Â´V 1 pX, Kq
                                      BÎ¸
and row j â€œ 1, . . . , J of pX, Kq lists the covariates and artificial regressors for this
product. It follows that
                                      Ëœ                                Â¸
                                             1     1         1     1
                    Bf8                 E pX V V Xq E pX V V Kq
                 E       pÎ¸2 ; Î»0 q Â»                                    .
                     BÎ¸                 E pK 1 V V 1 Xq E pK 1 V V 1 Kq

To the second-order in e2 , Ef8 pÎ¸2 ; Î»0 q equals
             Ë†                             Ë™    Ë†                             Ë™
               BÎ¾2      1    1                    BeÌ‚2           1    1
           E       pÎ¸2 q V V eÌ‚2 pÎ¸2 ; Î»0 q ` E        pÎ¸2 ; Î»0 q V V Î¾2 pÎ¸2 q .        (24)
               BÎ¸                                 BÎ¸
The first term in (24) is simply E pX 1 V 1 eÌ‚2 q. Going back to (23), we get
            Ëœ                                   Â¸Â´1 Ë†
                                                                         B eÌ‚2 1
                                                                       Ë†                Ë™Ë™
              E pX 1 V V 1 Xq E pX 1 V V 1 Kq              1 1                       1
Î¸0 Â» Î¸2 Â´                                             E pX V eÌ‚2 q ` E           V V Î¾2    .
             E pK 1 V V 1 Xq E pK 1 V V 1 Kq                             BÎ¸

Finally, using Theorem 1(i), we know that BeÌ‚
                                          BÎ²
                                             2
                                               â€œ 0. Therefore
                                    Ë™ Ëœ                                          Â¸
                      B eÌ‚2 1                    Â´E pX 1 V VÂ´1 eÌ‚2 q
                    Ë†
         1 1                      1
   E pX V eÌ‚2 q ` E           V V Î¾2 â€œ                               1
                                                                                Â¯  .
                      BÎ¸               Â´E pK 1 V V 1 eÌ‚2 q ` E BeÌ‚
                                                                BÎ£
                                                                   2
                                                                       V V 1 Î¾2


E     Proof of Theorem 3
We drop the bold letters in this proof to alleviate the notation, and without loss of
generality we normalize B â€œ 1.
    Remember that Gpy, F py, Î², Ïƒq, Î², Ïƒq â€œ 0, so that Gpy, F py, Î², 0q, Î², 0q â€œ 0. Given (11),
this gives GËš py, AËš py, F py, Î², 0q Â´ f1 pyqÎ², 0q â€œ 0 for all Î². This can only hold if
F py, Î², 0q Â´ f1 pyqÎ² does not depend on Î², which implies condition C2. Denoting
f0 pyq â€œ F py, Î², 0q Â´ f1 pyqÎ², we obtain

                                GËš py, AËš py, f0 pyq, 0qq â€œ 0.

                                             54
Now writing GËš py, Ev AËš py, F py, Î², ÏƒqÂ´f1 pyqÎ², Ïƒvqq â€œ 0 as an identity in Ïƒ and taking
derivatives with respect to Ïƒ, we get

                                                       GËš2 Ev pAËš2 FÏƒ ` AËš3 vq â€œ 0
                              GËš22 rEv pAËš2 FÏƒ ` AËš3 vqsrEv pAËš2 FÏƒ ` AËš3 vqs
             `GËš2 Ev pAËš2 FÏƒÏƒ ` AËš22 rFÏƒ , FÏƒ s ` 2AËš23 rFÏƒ , vs ` AËš33 rv, vsq â€œ 0.

Fortunately, this simplifies greatly at Ïƒ â€œ 0. The first equation gives

                             GËš2 Ev pAËš2 FÏƒ py, Î², 0q ` AËš3 vq â€œ 0,

where the derivatives AËš2 and AËš3 do not depend on v since Ïƒ â€œ 0. It follows that
GËš2 AËš2 FÏƒ py, Î²0 , 0q â€œ 0 since Ev â€œ 0. Given our invertibility assumption, condition C1
also holds. Using the second equation at Ïƒ â€œ 0, and given that FÏƒ py, Î²0 , 0q â€œ 0, we
get
                              GËš2 Ev pAËš22 rFÏƒ , FÏƒ s ` 2AËš23 rFÏƒ , vsq â€œ 0
so that
                                   GËš2 pAËš2 FÏƒÏƒ ` AËš33 q â€œ 0.
Given that GËš2 is invertible, this gives (reintroducing the arguments)

                    AËš2 py, f0 pyq, 0qFÏƒÏƒ py, Î², 0q ` AËš33 py, f0 pyq, 0q â€œ 0.

Therefore FÏƒÏƒ py, Î², 0q is independent of Î² and condition C3 holds. Noting that
f2 pyq â€œ Â´FÏƒÏƒ py, Î², 0q completes the proof.




                                               55
List of Tables
  1    Distribution of the Estimates of Î²0 . . . . . . . . . . . . . . . . . . .         57
  2    Distribution of the Estimates of Î²Â¯1x . . . . . . . . . . . . . . . . . . .       58
  3    Distribution of the Estimates of   VarpÎ²1x q    . . . . . . . . . . . . . . . .   59
  4    Distribution of the Estimates of   Î²Â¯2x . . .   . . . . . . . . . . . . . . . .   60
  5    Distribution of the Estimates of   VarpÎ²2x q    . . . . . . . . . . . . . . . .   61
  6    Distribution of the Estimates of   Î²Â¯3x . . .   . . . . . . . . . . . . . . . .   62
  7    Distribution of the Estimates of   VarpÎ²3x q    . . . . . . . . . . . . . . . .   63
  8    Distribution of the Estimates of   Î²Â¯p . . .    . . . . . . . . . . . . . . . .   64
                                                 p
  9    Distribution of the Estimates of VarpÎ² q . . . . . . . . . . . . . . . .          65
  10   Pseudo True Value: Increasing-number-of-markets Approach . . . . .                66
  11   Pseudo True Value: Moment-based Approach . . . . . . . . . . . . .                67
  12   Distribution of the Difference between True and Estimated Elasticity              68
  13   Distribution of the Estimates of the Means â€” Different Î²Â¯p . . . . . .            69
  14   Distribution of the Estimates of the Variances â€” Different Î²Â¯p . . . .            70
  15   Testing for Zero Means â€” Standard 2SLS . . . . . . . . . . . . . . .              71
  16   Testing for Zero Means â€” Corrected 2SLS . . . . . . . . . . . . . . .             72
  17   Testing for Zero Variances â€” Standard 2SLS . . . . . . . . . . . . . .            73
  18   Testing for Zero Variances â€” Corrected 2SLS . . . . . . . . . . . . .             74
  19   Joint Test of Zero Means and Variances â€” Standard 2SLS . . . . . .                75
  20   Joint Test of Zero Means and Variances â€” Corrected 2SLS . . . . . .               76
  21   Distribution of the Estimates of the Means (Lognormal Case) . . . .               77
  22   Distribution of the Estimates of the Variances (Lognormal Case) . . .             78
  23   Distribution of the Estimates of the Third-order Moments (Lognormal
       Case) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     79
  24   Summary Statistics for the Lognormal Case . . . . . . . . . . . . . .             80
  25   Distribution of Three Estimates of the Means â€” Normal and Lognormal 81
  26   Distribution of Three Estimates of the Variances â€” Normal and Log-
       normal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      82




                                          56
                  Table 1: Distribution of the Estimates of Î²0

       Var(Î¾)=0.1                   Var(Î¾)=0.1                    Var(Î¾)=0.1
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)    Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=0.5                   Var(Î¾)=0.5                    Var(Î¾)=0.5
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)    Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=1.0                   Var(Î¾)=1.0                    Var(Î¾)=1.0
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)    Var(Î²)=(0,0.5,0.5,0.5,0.2)




                                      57
                  Table 2: Distribution of the Estimates of Î²Â¯1x

       Var(Î¾)=0.1                   Var(Î¾)=0.1                      Var(Î¾)=0.1
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)      Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=0.5                   Var(Î¾)=0.5                      Var(Î¾)=0.5
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)      Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=1.0                   Var(Î¾)=1.0                      Var(Î¾)=1.0
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)      Var(Î²)=(0,0.5,0.5,0.5,0.2)




                                       58
               Table 3: Distribution of the Estimates of VarpÎ²1x q

       Var(Î¾)=0.1                   Var(Î¾)=0.1                     Var(Î¾)=0.1
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=0.5                   Var(Î¾)=0.5                     Var(Î¾)=0.5
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=1.0                   Var(Î¾)=1.0                     Var(Î¾)=1.0
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




                                       59
                  Table 4: Distribution of the Estimates of Î²Â¯2x

       Var(Î¾)=0.1                   Var(Î¾)=0.1                      Var(Î¾)=0.1
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)      Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=0.5                   Var(Î¾)=0.5                      Var(Î¾)=0.5
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)      Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=1.0                   Var(Î¾)=1.0                      Var(Î¾)=1.0
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)      Var(Î²)=(0,0.5,0.5,0.5,0.2)




                                       60
               Table 5: Distribution of the Estimates of VarpÎ²2x q

       Var(Î¾)=0.1                   Var(Î¾)=0.1                     Var(Î¾)=0.1
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=0.5                   Var(Î¾)=0.5                     Var(Î¾)=0.5
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=1.0                   Var(Î¾)=1.0                     Var(Î¾)=1.0
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




                                       61
                  Table 6: Distribution of the Estimates of Î²Â¯3x

       Var(Î¾)=0.1                   Var(Î¾)=0.1                      Var(Î¾)=0.1
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)      Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=0.5                   Var(Î¾)=0.5                      Var(Î¾)=0.5
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)      Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=1.0                   Var(Î¾)=1.0                      Var(Î¾)=1.0
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)      Var(Î²)=(0,0.5,0.5,0.5,0.2)




                                       62
               Table 7: Distribution of the Estimates of VarpÎ²3x q

       Var(Î¾)=0.1                   Var(Î¾)=0.1                     Var(Î¾)=0.1
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=0.5                   Var(Î¾)=0.5                     Var(Î¾)=0.5
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=1.0                   Var(Î¾)=1.0                     Var(Î¾)=1.0
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




                                       63
                  Table 8: Distribution of the Estimates of Î²Â¯p

       Var(Î¾)=0.1                   Var(Î¾)=0.1                     Var(Î¾)=0.1
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=0.5                   Var(Î¾)=0.5                     Var(Î¾)=0.5
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=1.0                   Var(Î¾)=1.0                     Var(Î¾)=1.0
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




                                       64
               Table 9: Distribution of the Estimates of VarpÎ² p q

       Var(Î¾)=0.1                   Var(Î¾)=0.1                     Var(Î¾)=0.1
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=0.5                   Var(Î¾)=0.5                     Var(Î¾)=0.5
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




       Var(Î¾)=1.0                   Var(Î¾)=1.0                     Var(Î¾)=1.0
Var(Î²)=(0,0.1,0.1,0.1,0.05)   Var(Î²)=(0,0.2,0.2,0.2,0.1)     Var(Î²)=(0,0.5,0.5,0.5,0.2)




                                       65
     Table 10: Pseudo True Value: Increasing-number-of-markets Approach

 Parameter                                            Scenarios
True VarpÎ²q :       p0, 0.1, 0.1, 0.1, 0.05q      p0, 0.2, 0.2, 0.2, 0.1q        p0, 0.5, 0.5, 0.5, 0.2q
True VarpÎ¾q :      0.1        0.5           1    0.1        0.5          1      0.1        0.5         1
                  -1.00      -1.00       -1.00 -1.00       -1.00      -1.00    -1.02 -1.03 -1.03
  Î²0 â€œ Â´1
                (.0043) (.0050) (.0058) (.011) (.012) (.013)                  (.032) (.035) (.038)
                  1.51        1.51        1.51  1.53       1.53        1.53     1.56      1.57       1.57
  Î²Â¯1x â€œ 1.5
                 (.022) (.023) (.024) (.050) (.050) (.050)                     (.13)      (.13)     (.13)
                  1.51        1.51        1.51  1.52       1.52        1.52     1.55      1.56       1.56
  Î²Â¯2x â€œ 1.5
                 (.023) (.024) (.025) (.048) (.049) (.049)                     (.12)      (.12)     (.12)
                 0.487       0.487       0.487 0.465      0.465       0.464   0.403 0.400 0.398
 Î²Â¯3x q â€œ 0.5
                 (.022) (.022) (.022) (.048) (.047) (.047)                     (.12)      (.12)     (.11)
                 -0.999 -0.999 -0.999 -0.990 -0.990 -0.990                    -0.954 -0.955 -0.956
  Î²Â¯p â€œ Â´1
                (.0086) (.0088) (.0090) (.0184) (.0186) (.0188)               (.043) (.044) (.045)
                0.0857 0.0856 0.0856           0.152      0.152       0.152   0.288 0.290 0.291
  VarpÎ²1x q
                 (.011) (.011) (.011) (.028) (.027) (.027)                    (.078) (.076) (.075)
                0.0863 0.0865 0.0866           0.152      0.152       0.153   0.284 0.286 0.288
  VarpÎ²2x q
                (.0086) (.0086) (.0087) (.0205) (.020) (.020)                 (.059) (.057) (.056)
                0.0952 0.0949 0.0946           0.182      0.181       0.181   0.400 0.399 0.397
  VarpÎ²3x q
                (.0097) (.010) (.010) (.024) (.023) (.023)                    (.063) (.063) (.062)
                0.0480 0.0479 0.0478 0.0888               0.088       0.088    0.148 0.147 0.147
  VarpÎ² p q
                (.0056) (.0057) (.0059) (.013) (.013) (.014)                  (.031) (.032) (.033)




                                          66
            Table 11: Pseudo True Value: Moment-based Approach

 Parameter                                          Scenarios
True VarpÎ²q :     p0, 0.1, 0.1, 0.1, 0.05q    p0, 0.2, 0.2, 0.2, 0.1q        p0, 0.5, 0.5, 0.5, 0.2q
True VarpÎ¾q :     0.1        0.5         1   0.1        0.5         1       0.1        0.5         1
  Î²0 â€œ Â´1        -1.01     -1.01      -1.01 -1.04      -1.04      -1.04    -1.11      -1.11      -1.12
  Î²Â¯1x â€œ 1.5     1.49       1.49       1.49 1.48       1.48        1.48     1.43       1.43       1.43
  Î²Â¯2x â€œ 1.5     1.49       1.49       1.49 1.48       1.48        1.48     1.43       1.43       1.43
  Î²Â¯3x â€œ 0.5    0.496 0.496 0.496 0.486 0.486 0.486                       0.455 0.455 0.455
  Î²Â¯p â€œ Â´1      -0.989 -0.988 -0.988 -0.958 -0.957 -0.955                 -0.873 -0.869 -0.864
   VarpÎ²1x q    0.0854 0.0855 0.0855 0.149 0.149 0.149                     0.275 0.275 0.276
   VarpÎ²2x q    0.0855 0.0855 0.0856 0.149 0.149 0.149                     0.273 0.274 0.274
   VarpÎ²3x q    0.0938 0.0938 0.0937 0.176 0.175 0.175                     0.369 0.368 0.366
   VarpÎ² p q    0.0421 0.0421 0.0419 0.0685 0.0681 0.0676                 0.0920 0.0906 0.0888




                                          67
Table 12: Distribution of the Difference between True and Estimated Elasticity

       Product 5                    Product 10                    Product 15




      Product 20                    Product 25                     Legend




                                     68
 Table 13: Distribution of the Estimates of the Means â€” Different Î²Â¯p

     Î²Â¯p â€œ Â´1                     Î²Â¯p â€œ Â´2                     Î²Â¯p â€œ Â´3
Distribution of Î²0           Distribution of Î²0           Distribution of Î²0




Distribution of Î²Â¯1x         Distribution of Î²Â¯1x         Distribution of Î²Â¯1x




Distribution of Î²Â¯2x         Distribution of Î²Â¯2x         Distribution of Î²Â¯2x




Distribution of Î²Â¯3x         Distribution of Î²Â¯3x         Distribution of Î²Â¯3x




Distribution of Î²Â¯p          Distribution of Î²Â¯p          Distribution of Î²Â¯p




                                  69
  Table 14: Distribution of the Estimates of the Variances â€” Different Î²Â¯p

       Î²Â¯p â€œ Â´1                     Î²Â¯p â€œ Â´2                      Î²Â¯p â€œ Â´3
Distribution of VarpÎ²1x q    Distribution of VarpÎ²1x q     Distribution of VarpÎ²1x q




Distribution of VarpÎ²2x q    Distribution of VarpÎ²2x q     Distribution of VarpÎ²2x q




Distribution of VarpÎ²3x q    Distribution of VarpÎ²3x q     Distribution of VarpÎ²3x q




Distribution of VarpÎ² p q    Distribution of VarpÎ² p q     Distribution of VarpÎ² p q




                                     70
         Table 15: Testing for Zero Means â€” Standard 2SLS

Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.904 0.793 0.711
GLS estimator and standard errors                  0.889 0.765 0.678
2SLS with clustered standard error                 0.904 0.780 0.702




                                71
         Table 16: Testing for Zero Means â€” Corrected 2SLS

Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.915 0.819 0.725
GLS estimator and standard errors                  0.882 0.767 0.669
2SLS with clustered standard error                 0.906 0.809 0.731




                                72
        Table 17: Testing for Zero Variances â€” Standard 2SLS

Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.746 0.625 0.556
GLS estimator and standard errors                  0.738 0.618 0.542
2SLS with clustered standard error                 0.740 0.617 0.547




                                73
        Table 18: Testing for Zero Variances â€” Corrected 2SLS

Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.792 0.688 0.626
GLS estimator and standard errors                  0.773 0.680 0.613
2SLS with clustered standard error                 0.775 0.679 0.620




                                 74
  Table 19: Joint Test of Zero Means and Variances â€” Standard 2SLS
Significance level                                  1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.731 0.578 0.507
GLS estimator and standard errors                  0.699 0.545 0.483
2SLS with clustered standard error                 0.702 0.565 0.498




                                75
 Table 20: Joint Test of Zero Means and Variances â€” Corrected 2SLS
Significant level                                   1%    5%   10%
2SLS with heteroskedasticity-robust standard error 0.756 0.642 0.568
GLS estimator and standard errors                  0.738 0.631 0.548
2SLS with clustered standard error                 0.749 0.617 0.546




                                76
Table 21: Distribution of the Estimates of the Means (Lognormal Case)

     Ïƒ â€œ 0.3                     Ïƒ â€œ 0.4                      Ïƒ â€œ 0.5
Distribution of Î²0          Distribution of Î²0           Distribution of Î²0




Distribution of Î²Â¯1x        Distribution of Î²Â¯1x         Distribution of Î²Â¯1x




Distribution of Î²Â¯2x        Distribution of Î²Â¯2x         Distribution of Î²Â¯2x




Distribution of Î²Â¯3x        Distribution of Î²Â¯3x         Distribution of Î²Â¯3x




Distribution of Î²Â¯p         Distribution of Î²Â¯p          Distribution of Î²Â¯p




                                 77
 Table 22: Distribution of the Estimates of the Variances (Lognormal Case)

        Ïƒ â€œ 0.3                      Ïƒ â€œ 0.4                      Ïƒ â€œ 0.5
Distribution of VarpÎ²1x q    Distribution of VarpÎ²1 q     Distribution of VarpÎ²1 q




Distribution of VarpÎ²2x q    Distribution of VarpÎ²2 q     Distribution of VarpÎ²2 q




Distribution of VarpÎ²3x q    Distribution of VarpÎ²3 q     Distribution of VarpÎ²3 q




Distribution of VarpÎ² p q    Distribution of VarpÎ² p q    Distribution of VarpÎ² p q




                                     78
Table 23: Distribution of the Estimates of the Third-order Moments (Lognormal
Case)

          Ïƒ â€œ 0.3                     Ïƒ â€œ 0.4                    Ïƒ â€œ 0.5
  Distribution of Î²1x 3rdM    Distribution of Î²1x 3rdM   Distribution of Î²1x 3rdM




  Distribution of Î²2x 3rdM    Distribution of Î²2x 3rdM   Distribution of Î²2x 3rdM




  Distribution of Î²3x 3rdM    Distribution of Î²3x 3rdM   Distribution of Î²3x 3rdM




  Distribution of Î² p 3rdM    Distribution of Î² p 3rdM   Distribution of Î² p 3rdM




                                     79
             Table 24: Summary Statistics for the Lognormal Case

            Parameter                                           Scenarios
                Ïƒ                             Ïƒ â€œ 0.3            Ïƒ â€œ 0.4            Ïƒ â€œ 0.5
         Moments included:                   2          3      2          3         2         3
                                           -1.03      -1.01  -1.06      -1.03     -1.10     -1.06
              Î²0 â€œ Â´1
                                         (0.013)    (0.080) (0.021) (0.092)     (0.031)   (0.107)
                                           1.49       1.49    1.47       1.48      1.44      1.45
              Î²Â¯1x â€œ 1.5
                                         (0.032)    (0.057) (0.056) (0.072)     (0.086)   (0.096)
                                           1.49       1.49    1.47       1.47      1.44      1.45
              Î²Â¯2x â€œ 1.5
                                         (0.033)    (0.054) (0.057) (0.070)     (0.088)   (0.093)
                                          0.499       0.502  0.497      0.502    0.496      0.501
              Î²Â¯3x â€œ 0.5
                                         (0.020)    (0.039) (0.036) (0.047)     (0.056)   (0.060)
                                          -0.976     -0.991 -0.946 -0.972        -0.906    -0.940
              Î²Â¯p â€œ Â´1
                                         (0.014)    (0.076) (0.020) (0.084)     (0.027)   (0.095)
                                          0.153       0.159  0.229      0.241    0.295      0.316
    VarpÎ²1x q â€œ 0.212{0.390{0.639
                                         (0.020)    (0.030) (0.039) (0.048)     (0.062)   (0.072)
                                          0.153       0.160  0.228      0.244    0.294      0.319
    VarpÎ²2x q â€œ 0.212{0.390{0.639
                                         (0.020)    (0.028) (0.038) (0.045)     (0.060)   (0.067)
                                          0.025       0.030  0.045      0.036    0.068      0.056
     VarpÎ²3x q â€œ 0.04{0.043{0.071
                                         (0.014)    (0.033) (0.025) (0.041)     (0.038)   (0.059)
                                          0.0579      0.077  0.081      0.115     0.099     0.145
    VarpÎ² p q â€œ 0.094{0.174{0.284
                                         (0.007 )   (0.095) (0.011) (0.11)      (0.016)    (0.12)
                                                      0.044            0.096               0.160
   3rdM pÎ²1x q â€œ 0.093{0.322{0.894
                                                    (0.067)           (0.091)             (0.135)
                                                      0.043            0.097               0.161
   3rdM pÎ²2x q â€œ 0.093{0.322{0.894
                                                    (0.082)           (0.101)             (0.130)
                                                    -0.0073            -0.012              -0.015
   3rdM pÎ²3x q â€œ 0.003{0.012{0.033
                                                    (0.070)           (0.084)             (0.120)
                                                    -0.0095            -0.018              -0.024
3rdM pÎ² p q â€œ Â´0.027{ Â´ 0.096{ Â´ 0.265
                                                    (0.050)           (0.058)             (0.066)




                                         80
Table 25: Distribution of Three Estimates of the Means â€” Normal and Lognormal

          Normal                    Lognormal
     Distribution of Î²0          Distribution of Î²0




     Distribution of Î²Â¯1x       Distribution of Î²Â¯1x




     Distribution of Î²Â¯2x       Distribution of Î²Â¯2x




     Distribution of Î²Â¯3x       Distribution of Î²Â¯3x




     Distribution of Î²Â¯p        Distribution of Î²Â¯p




                                     81
Table 26: Distribution of Three Estimates of the Variances â€” Normal and Lognormal

           Normal                     Lognormal
   Distribution of VarpÎ²1x q    Distribution of VarpÎ²1x q




   Distribution of VarpÎ²2x q    Distribution of VarpÎ²2x q




   Distribution of VarpÎ²3x q    Distribution of VarpÎ²3x q




   Distribution of VarpÎ² p q    Distribution of VarpÎ² p q




                                        82
