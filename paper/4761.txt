NBER WORKII4G PAPER SERS

TOWARD A MODERN MACROECONOMIC
MODEL USABLE FOR POLICY ANALYSIS

Eric M. L.eeper
Christopher A. Sims

Working Paper No. 4761

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
June 1994

Forthcoming in Fischer, Stanley and Rotemberg, Julio J.. eds., NBER Macroeconomics

Annual 1994. MIT Press, Cambridge. MA. This paper is part of NBER's research program in
International Trade and Investment. Any opinions expressed are those of the authors and not
those of the National Bureau of Economic Research.

NBER Working Paper #4761
June 1994
TOWARD A MODERN MACROECONOMIC
MODEL USABLE FOR POLICY ANALYSIS

ABS'rRAa
This paper presents a macroeconomic model that is both a completely specified dynamic

general equilibrium model and a pmbabilistic model for time series data. We view the model

as a potential competitor to existing ISLM-based models that continue to be used for actual
policy analysis. Our approach is also an alternative to recent efforts to calibrate real business
cycle models. In contrast to these existing models, the one we present embodies all the following
important characteristics:

1) It generates a complete multivariate stochastic process model for the data it aims to
explain, and the full specification is used in the maximum likelihood estimation of the model;

ii) It integrates modeling of nominal variables -- money stock, price level, wage level, and
nominal interest rate -- with modeling real vthables iii) It contains a Keynesian investment
function, breaking the tight relationship of the return on investment with the capital-output ratio;

iv) It treats both monetary and fiscal policy explicitly; v) It is based on dynamic optimizing
behavior of the private agents in the model.

Flexible-price and sticky-price versions of the model are estimated and their fits are
evaluated relative to a naive model of no-change in the variables and to an unrestricted VAR.

The paper displays the model's implications for the dynamic responses to structural shocks,
including policy shocks, and evaluates the relative importance of various shocks for determining
economic fluctuations.

Eric M. Leeper
Research Department
Federal Reserve Bank of Atlanta
104 Marietta Street
Atlanta, GA 30309

Christopher A. Sims
Depanment of Economics
Yale University
37 Hillhouse Avenue
New Haven. CF 06520

Towiw A

MODERN MACROECONOMIC MODEL USABLE FOR POLICY ANALYSIS

Introduction

This paper presents a macroeconomic model that is both a completely specified
dynamic general equilibrium and a probabilistic model for time series data. We view
the model, perhaps with future refinements, as a potential competitor to existing
ISL.M-based models that continue to be used for actual policy analysis in Institutions such as the Federal Reserve Board, the Congressional Budget Office, or the
International Monetary Fund. Our approach is also an alternative to recent efforts to
calibrate real business cycle models. In contrast to these existing models, the one
we present embodies all the following characteristics:

I) It generates a complete muitivariate stochastic process model for the data
it aims to explain, and the full specification is used In fitting the model.
Ii) It integrates modeling of nominal variables —— money stock, price level,
wage level, and nominal Interest rate —— with modeling real variables.
iii) It allows for increasing costs in the production of capital

goods,

breaking the tight relationship of the return on investment with the capitaloutput ratio.
iv) It treats both monetary and fiscal policy explicitly.

v) It is based on dynamic optimizing behavior of the private agents in

the

model.

The paper displays results of fitting the model that are encouraging, though
still highly preliminary. A restricted version of the model fit only to data on 3
real variables performs approximately as well as an unrestricted VAR, attributing
most cyclical variability to real shocks, but not to shocks in the "Solow resldual.
has not yet been successfully
A 10—variable flexible price version of the model
fitted, possibly because of fundamental problems with matching data on prices and
real variables together with that type of model. A 10-variable sticky price model
fits worse than an unrestricted VAR, though the structural model is much more tightly

It fits about as well as a naive no-change model. The estimated
structure Implies extreme price stickiness and effective monetary policy, but attributes little of observed cyclical variability to monetary policy shocks (and none to
parameterized.

fiscal shocks).
1

t4ow we discuss in more detail each of the five aspects of the model listed
above.

regard the fact that we have a complete stochastic process model as
important because it allows us to bring all aspects of the data to bear in generating
estimates, Improving efficiency relative to Instrumental variables approaches that in
effect use a narrow band of the available information in estimation. We also can
investigate any desired aspect of the discrepancy between our model's Implications
and the behavior of the data, because we can simulate solutions of it. Any use of a
model to trace out the Impacts of policy intervention will require use of its full
set of dynamic implications. If the model has been estimated by single-equation
methods, important aspects of Its dynamic structure may never have been confronted
with the data, and Its policy implications may be correspondingly unreliable. Of
course all these remarks apply a forttort to a comparison of this model with ones
1) We

that are largely calibrated informally rather than estimated.

ii) Though our model in many respects follows the spirit of real business cycle
(RBC) modeling exercises as pioneered by Kydland and Prescott (1982), we do not

follow that literature in paying attention almost exclusively to the behavior of
real, rather than nominal aggregates. The RBC approach may be motivated in part by

the fact that it has usually started from models In which real and price behavior
dichotomizes, so that a complete model for the real variables in the system is
possible without any reference to nominal variables. Of course such models have few
Interesting implications for monetary policy —— Indeed are in a sense aimed at
showing that monetary policy is unimportant. Thus one reason for our emphasis on

Including nominal variables is our aim of eventually exploring specifications in
which price stickiness generates stronger nominal-real interactions. But even for
models that do dichotomize, there Is information about the model structure in price
as well as real data. RBC modelers sometimes Invoke the idea that nominal aggregates

are less likely to correspond to the theoretical constructs that appear In their
models than are real aggregates. The wage, for example, Is claimed not to be a true
market—clearing price as assumed In the theory. But since It is left unspecified
what mechanisms produce the results of a market—clearing model in a world where
measured price variables are not market—clearing prices, we find the argument for

2

There are, after all, strong reasons to
Ignoring price variables unconvincing.
suppose that measured quantity variables are distant from the corresponding theoretical constructs as well.

lii) The curvature In the transformation curve relating

output of Investment to

output of capital is Important in matching some aspects of observed cyclical behavior. We would like the technology to be able to generate fluctuations In real rates

of return without corresponding large shifts In the level of the capItal stock or
output. With a relative price of C and I goods In the model, this can occur, with
high rates of return generating above-normal I, but only smooth growth In K.

In

sticky—price models, we would like to allow the possibility that a monetary of fiscal

expansion could lower nominal Interest rates. Increasing costs of capital goods
production provide a mechanism that could contribute to this behavior in the model.
Also, as recently shown by Nakornthob (1993). models that make prices and wages
sticky in the sense that they cannot move discontinuously may easily have no equilibrium solution unless they include a variable relative price of C and I.

lv) Those few models In the RUC style that have considered "aggregate demand"
policy variables have generally focused on monetary or fiscal policy alone. In fact,
monetary and fiscal policy are intimately related, as Leeper (19911 and Sims (l992.a)
show theoretically and as becomes evident in the estimation of a model like this. As

we explain below, the parameters of monetary and fiscal policy equations must, in
order to guarantee existence of a unique equilibrium, lie in a set of complicated
geometry. While there are certain conditions under which monetary and fiscal policy
dichotomize, with most of the equilibrium derivable from the monetary policy specif i-

cation alone, these conditions are not generic. Even a model that aims mainly at
guiding analysis of monetary or fiscal policy alone needs to treat both together to
give reliable results. This Is especially true in a period like the recent history
of the U.S. • in which there have probably been changes In belief's about the political
feasibility of keeping taxation in line with government commitments to spending and
debt service, at all levels of government. Such shifts In beliefs could have impacts
on prices. and, in a model with sluggish prices, on real variables, not captured by
models that ignore monetary—fiscal Interactions.

3

v) Recognizing the Implications of dynamic optimizing behavior Is important in
macroeconomic modeling. But because this point has been emphasized first by natural
rate theorists, who aimed to show that dynamic optimization by private agents undermined the effectiveness of aggregate demand management policies, and then by RBC
modelers, who model a world without demand managers, Keynesians and monetarists have

not embraced It. To be sure, Keynesian textbooks now Include treatments of forward-

looking theories of consumption and (sometimes) Investment accelerator effects
arising out of expectations, but these treatments of forward-looking behavior are In
Furthermore they are seldom Integrated into the generalthemselves incomplete.
equilibrium versions of Keynesian models, which usually tall back Instead on the ISLM

framework, without even a clear distinction of real and nominal interest rates.
Expectational elements are, paradoxically, emphasized more In the wage and pricesetting components of such models than In their asset accumulation sectors. Yet the
Keynesian and monetarist notion of nominal aggregate demand is at its root a theory
about the relation of supply to demand for nominal government assets —- debt and

Demand for these assets depends critically on the public's beliefs about
future government monetary and fiscal policy. Any model that Is to be used to trace
out effects of monetary and fiscal policy needs to consider the implications of
dynamic optimization, and this is especially true of a model that intends to explore
money.

the implications of price stickiness.

Previous successful attempts to use maximum likelihood to estimate a maximizing

equilibrium model, such as Altug (1989). dealt with much simpler theoretical structures. Altug estimates a version of Kydland and Prescott's (1982) model, for which
the simpler social planner's solution can be supported by a competitive equilibrium.
The introduction of money requires solving the decentralized problem and brings with
it

the difficulties inherent in ensuring that a determinate equilibrium exists.

Though we. like Altug, impose stationarity, her approach is to extract deterministic
trends from the data, while ours is to allow unit and near—unit roots that imply
long-term deviations from steady state. Most Importantly, her approach postulates
the existence of a measurement error in the data with well—defined stochastic properties but no economic Interpretation. Our model's stochastic disturbances are all
structuz-al. This means in particular that we have a higher—dimensional vector of
structural disturbances than In any previous model of this type. While we may
4

error In

will
The allocation of any substantial part of observed variation to
do so reluctantly.
such an uninterpreted source raises serious difficulties in using the model f or
eventually need to introduce something like measurement

the model, we

prediction and policy analysis.

McGrattan (1992) and McGrattan. Rogerson. and Wright (1993) use maximum likelihood to estimate general equilibrium models with distorting taxes, so they also
cannot rely on solving the social planner's problem. Like Altug, these authors
Introduce measurement errors as additional sources of uncertainty.

Watson (1993) adds measurement error that Is by construction a linear function
of the systematic component of the model. Though this unattractive perfect collinearity between error and systematic components Is Introduced In order to minimize the
size of the error. Watson finds that in matching a standard RUG model to data. 40 to
60 percent of the variance must be attributed to economically uninterpreted sources.
The Model
Consumers

Consumers are endowed each period with one unit of time, which they divide
between work, 1.., and leisure. They derive utility from consumption net of transactions costs. C, and leisure. i—I.,, and discount utility at the rate of time preference, S. Consumers can hold two types of nominal assets, non-interest bearing money.
IC.
Income
14, and interest bearing government debt. B. and one real asset, capital.
is earned from the capital and labor they rent to firms (which together make up
factor income 1') and from the interest received from holding government debt, lB/P.
In the sticky—price version of the model firms make temporary pure profits and
losses, and these are assumed to be returned to consumers as dividends. We assume
they maximize:1

'Note that, though 5

and

(as we will see below) x vary through time, this objective

function does not run afoul of the well—known result that time-varying discount rates
generate time—Inconsistency. The usual Inconsistency result depends on the discount

rate being thought of as indexed by the number of periods into the future at which

the discounting is done. We think of consumers as understanding in advance that they

will at the absolute date s discount at rate fl(s), so no time-inconsistency
Involved.

5

Is

(C(l-.L)')

(1)

£ [iexP[_i(s)ds]
dtl
subject to
A:
UP:

C:

XC+Ql+t+"Y+
XC +

• XC

that it
constraint is listed

(3)

Kal-ÔK

(4)

Y—rK+wL+S

(5)

v—!J.
We assume

(2)

(5)

> 0. The Lagrange multiplier associated with each
(0.1) and
at the left.
The agents' choice variables are C , C, L, 1. M,

B, K, Y and V.
The first equation is the consumers' budget constraint, where C Is gross con-

sumption, I is investment, and t Is the level of lump-sum taxes. Consumption goods
and investment goods are distinct from the output good, so they have prices X and Q
relative to the output good. Government bonds earn the nominal rate of return i and
P is the general price level. i.e. the price in dollars of the output good.
The second equation defines consumption net of transactions costs, with total
output sening as a measure of the level of transactions at a given point in time.
Costs are assumed to be increasing In the volume of transactions and in velocity. V.
This simple transactions technology implies that costs approach zero as the level of
real money balances approaches infinity, pushing C toward C. As the level of real
balances approaches zero, transactions costs are unbounded, which Implies that no
non-monetary equilibrium exists. This feature of the model could easily be modified

6

by specifying a transactions technology that places an upper bound on transactions
costs,
Firms

The third and fourth equations are standard. Equation (4) specifies the law of
qiotlon for capital, and (5) defines Income as the sum of dividends $ with factor
payments to capital and labor, which consumers receive from the firm. Equation (6)
defines the Income velocity of money.

Firms rent factor Inputs from consumers, transform them Into "output" according
to a production technology, convert "output" Into salable C-goods and I-goods accord-

big to another part of the production technology, and sell the C and I goods to
consumers and the government. Because the technology Is linear bomogenous, there are
no profits in a market-clearing competitive equilibrium and we need not keep track of

who receives the profits in the flexible price version of the model. In the stickyprice version we need treat profits as part of consumer Income and must keep track of
the difference between income and factor payments. The net profits from the firms'
buying, selling and transforming activities are their maxirnand, S (written assuming
that the number of firms matches the number of consumers so we can again avoid
separate explicit market—clearing equations):

max {x(c+)

+ Ql +

A(aKt+L')' -

it - wL -

(7)

((C+g)M.olM)lM}

with C"g, I (defined below), K, and L as choice variables. To keep the production
technology concave, c"l, and to keep the costs-of-adjustment convex, pal. The
borderline case c=O corresponds to Cobb-Douglas production and c<O corresponds to low

elasticity of substitution. The g term that appears In this problem Is the level of
government purchases, which shares a relative price with the consumption good. Since

government purchases are exogenous, their appearance does not affect the firms'
optimization problem.2

2The first-order conditions for the consumers and the firms appear in an appendix.

7

Government

The government uses consumption goods in amount g for a purpose that yields no
utility to individuals. It also takes responsibility for endowing newly born agents

with the same wealth as existing agents and for redistributing equally to living
agents the wealth of all those that die. Population rows at the (possibly varying)
proportional rate ii.

With

this device, the population can fluctuate while the model

an maintain the assumption that there Is an infinitely lived representative agent.
Thus the government operates with the budget constraint

+

r

—

+ gX + QnK ,

where QnK is net transfers of capital denominated in terms of output goods.

(8)

The

variables in this equation are In per capita terms to avoid having to introduce
additional market—clearing equations for the policy variables. Aggregate money and

debt row or shrink not only through the U and B terms In this equation, but also
through net issuance of government paper to newborns. Thus only the real capital
that is demanded by the newly born (or turned over by the newly dead) creates a
resource drain (or inflow) for the government. Wealth in the form of government
paper can be created or destroyed without any effect on per capita PA and B simply by
the government's issuing or retiring new paper.
Investment goods produced by the firm, I • are both those bought by the existing
population and those pw-chased by the government for distribution to the newborns.
Thus a market-clearing condition is
S

I

—l+nX.

(9)

The social resource constraint, which is redundant If both firm and government
budget constraints are in the system, is
X(C+g) + Q(l+nlC)

8

.

(10)

We treat g and ii as determined exogenously and P. IC and I as determined by
market conditions. Thus the government has as choice variables 14, B, and r. Besides
its constraints, it needs two policy equations.
The monetary and tax policy rules are motivated by two considerations: the need
to satisfy the Intertemporal government budget Identity and the pursuit of counterIn addition, monetary and fiscal policies must interact
cyclical policy objectives.
determine
to
the price level. Leeper (1991) and Sims (1992a) use simple rules in
models
to compute the regions of the policy parameter space where unique
equilibrium
equilibria exist. In those models, the monetary authority obeys a nominal Interest
rate rule that depends on Inflation or money growth and the fiscal authority adjusts

lump—swn taxes In response to the level of real debt held by the public.
With
systematic policy behavior summarized by the two parameters In the policy rules,
there are four qualitatively distinct regions of the policy parameter space: In two
of these there exist unique equilibria, In one real debt explodes generically to
violate transversality or feasibility, and in one the price level Is Indeterminate.

If monetary policy fixes the stock of high—powered money or raises the nominal

interest rate strongly In response to increases In nominal variables like inflation
or money growth, then a unique equilibrium exists only if fiscal policy behaves
compatibly by matching any increase in real debt with an equivalent increase in the
present value of direct taxes. This policy environment produces the usual monetaristlRicardian propositions that underlie many economists' views of monetary and

tax policy effects. Of course, given the hypothesized monetary policy behavior,
refusal of the fiscal authority to Increase direct taxes when real debt rises violates the government's Intertemporal budget Identity, placing the model In the region
of the parameter space where no equilibrium exists.

Alternatively, when the fiscal authority refuses to change taxes in the face of
real debt expansions, then monetary policy must prevent nominal interest rates from
rising and generating explosive growth in government debt. Such a policy mix Implies
that total government liabilities —— high—powered money plus government debt —determine the price level, while the ratio of money—to—debt Is determined by the

9

nominal Interest rate. Shocks to lump—sum taxes influence nominal magnitudes when
prices are flexible and both real and nominal variables when prices are sticky. The
choice between debt— or tax-financing of government spending, therefore, is relevant

although the models are "Ricardian in the sense that private agents fully discount
the future tax —- direct and Inflation —— liabilities associated with Increases in
government debt.

This result emerges because future lump—sum taxation and future

Inflation taxation represent different marginal sources of debt financing and elicit

different private responses to a given cut in current direct taxes. Moreover,
monetary policy shocks can have unexpected effects in this region of the policy
For example, a disturbance that unexpectedly raises the nominal
interest rate can be deflationary or inflationary, dependIng on the assumed fiscal
parameter space.
behavior.3

Finally, a policy combination where taxes respond strongly to real debt and
nominal interest rates respond weakly to nominal variables leaves the price level
indeterminate.

Although the ratio of money—to—bonds is determined, their sum is not:

at each date, many different levels of total government liabilities are consistent
with an equilibrium and associated with each level Is a different price level.
Explicitly modeling fiscal behavior generalizes the well—known Wicksellian view that
a pegged nominal Interest rate does not determine prices. A pegged rate coupled with

direct taxes that do not rise sufficiently when real debt Increases can uniquely
determine the price level.

Actual policy behavior also contains strong countercyclical components. Monetary policy tends to lower Interest rates as unemployment rises and employment falls
and to raise rates when Inflationary pressures mount. Fiscal policy, through both

discretionary tax changes and automatic stabilizers, tends to lower revenues when
employment and Inflation decline.

The policy rules are parameterlzed to embody both
of these reasons for monetary and tax policy changes. In addition to these system-

atic responses of policy to observable data, the rules contain disturbances reflecting policy responses to unmodelled economic or political developments.

3Leeper (1993) simulates a wide range of policy effects in a simple model with
interest rate and tax rules.

10

The policy rules in this model an more complex than those studied In earlier
papers, so the relevant regions of the parameter space cannot be derived analytically. The underlying economic Intuition carries over, however. The monetary policy
rule is

—

and

alog(P/P)

+

a

+

a1log(i/) + a.jog(LIL) + Cj.

(11)

the tax policy rule is

S

b[! - I] + blOg(L/E) + b + bx

-

+c

(12)

overscored variables denote steady state values. Note that the steady state
price level, P. and the steady state debt—to—CUP level, B/Y are free parameters of
(11) and (12) Just as are the a's and b's.
The

Policy Analysis

The specification of policy behavior In equations (11) and (12) leads to two
types of policy experiments one might conduct with the fitted model: interventions

on the out-of-sample paths of the disturbances, c and c, and once-and-for-all
changes in the policy parameters, the a's and b's. We view the first type of experiment as useful for policy analysis of the sort conducted in preparation for Federal
Open Market Committee meetings or CongressIonal debates about fiscal policy.

The

analyst would choose several candidate paths for, say, c1, and present the model's
predictions for each path to the policy—makers. Most commonly, the Ci paths themselves would be generated to provide responses to questions such as, What would
happen if the Federal Reserve held interest rates below 4% for the next six months?'
To do so. one would solve for c sequences that make the time path of Interest rates
behave as desired. Because the model implies that there are many potential stochastic influences on Interest rates, this kind of projection is generally quite different from simply forecasting conditional on a given time path of the interest rate.
11

A menu of options generated along these lines would form the basis for the
policy discussion. Osoices of paths of such shocks In this mariner is not a trivial
exercise,

and the process closely mimics actual policy practices, as has been argued

in detail elsewhere [Sims 1982, 1987; Cooley, LeRoy. Raymon 1984, and LeRoy 19931.

Of course, repeated use of the model In this way for policy choice could eventually
be seen by the public as changing the a's and b's. but this Is not necessarily true
and Is In any case likely to take a long time. Even If policy-makers announce that
they are making permanent changes In the way policy variables are set, the public
will for good reason wait for the announcement to be backed by sustained action
before accepting the change as even approximately permanent. And In any case for
systematic use of the model to eventually change the a's and b's, It would have to be
true both that users of the model had a strong Impact on policy debates and that the
use of the model changed their conclusions about good policy, rather than simply
letting them reach those conclusions more quickly and cheaply.

fly construction, changes In policy parameters are rare and permanent events -the
stuff of regular cyclical policy debates. Thus while It may be interesting to
not
use the model to see whether an alternative set of the a's and b's would deliver a
better equilibrium than the historically estimated one, it Is internally contradictory to evaluate policy "rules" In this way as if the result were a contribution to
the usual year-to—year or decade—to-decade ebb and flow of macroeconomic policy
arguments.

Sticky-Price Version

The sticky-price formulation we are working with does not have the flexible
price model as a special case or even as a limit point. In particular, we drop
equations (A4) and (AlE in the appendix, corresponding to the workers' matching of
marginal utility of leisure to the wage and the firm's matching of marginal productivity of labor to the wage. In their place we postulate differential equations
relating the rates of growth of I'/P and W to the discrepancies between the left— and
right-hand sides of (A4) and (Al4):

—
dt

log(P)

- xlog

1-c

At

12

'w1 .

(A4')

—-

-

,.

J

O4

(*14')
J

These can be thought of as markup and Phlliips Curve equations. They have
no rational expectations elements and can therefore be criticized as old—fashioned.

However, any element of stickiness Is going to violate canons of purity on the
rationality front. We az-c using a modeling framework in which the inflation rate is
stationary and do not regard it as unrealistic to treat price dynamics formulated as

in (A4') and (A14') as "structural" relative to policy disturbances that come as a
stationary time series of shocks. Note also that because (*4') is formulated with
the second derivative on the left, it implies that when Inflation Is at a constant
rate, there is no persistent gap between the wage and the marginal product of labor.
And since (A14') is in terms of the real wage w, it again does not require any gap
between the wage and the marginal utility of labor In the presence of steady inflation-

can be positive or
negative. When negative, they fit an interpretation that requires P and w to have
continuous time paths and be predetermined. When positive they are interpreted as
allowing discontinuity in P and w paths, with the levels of these variables set to
match average expected future values of the right—hand—side forcing variables. We
actually do not impose these Interpretations directly, but instead simply treat (*4')
and (Al4') as containing endogenous expectationai disturbance terms or exogenous
disturbances, respectively, according to whether the model as a whole has the right
In principle the speed—of—adjustment parameters x,,,

and

number and location of unstable roots to support the Interpretation.4

We do not tie (A4') and (AlE) to a particular Institutional story. They are
consistent with many of the stories that have been told in the literature about price
adjustment mechanisms. For example, If we denote the right-hand side of (*4') as
4As the number of unstable roots in the system formed by the constraints and first
order conditions increases, there are correspondingly increased numbers of stability
conditions Imposed on the model. A new exact relation among the variables required

to suppress an additional unstable root will In general conflict with the

specification of the model, unless an additional equation is specified as having an
endogenously determined disturbance.
13

zCt), then a continuous-time adaptation of John Taylor's5 overlapping-contracts
pricing mechanism would postulate

• e(f—p) ,
when

(13)

p Is log price. B is the rate at which the stock of firms are selected, to

adjust per unit time and p Is the value to which they adjust p when the adjust.
Then, recognizing that the price they set now will hold over a long future span, they
form I? as

a0

E

e(p(s)-;Z(s))

(14)

dsl.

Is reasonable if adjusters average over a time span in the future that
corresponds to their actual hazard rate for being selected for price adjustment.

If eq.

as

exactly (A4') emerges, though with z,<O.

Another possible story to back up the price adjustment mechanism would be built
on search. By explicitly modeling choice of search activity levels by firms and
workers we could create a foundation for modeling unemployment and job vacancy data
along with the rest of the model.

Notice that our formulation, though It treats the price adjustment mechanism as
an institutional datum. Involves no ad hoc or suboptimal behavior by firms or work-

The firms and workers simply treat both the price and quantity of labor as
beyond their control. Given the prices and quantities turned out by the labor market
ers.

mechanism, savings and Investment are carried out completely optimally, and with full
accounting for expected future paths of inflation and output.

One might think that as the speed with which P and w react to gaps between the
left- and right—hand sides of (A4) and (AlE Increases, we would smoothly move from a

(l979l.

Another example of a rational expectations sticky—price model is that in

Money and Business Cycles" [1991) by Robert King. He uses a Taylor-style adjustment
mechanism for wages, but assumes firms always to be on their labor demand curves and

observes that this creates a strongly positive reaction of the nominal Interest rate
to demand expansions.

14

sticky-price world to a flexIble-price world. However, as the speeds of adjustment
of P and w increase, we converge to flexible-price behavior In a complicated way.
This point was shown a year ago by Don Nakornthab In a somewhat simpler version of
this model. Its intuitive economic explanation traces back to a point that can be
found in Keynes General Theory: making wages or prices more flexible will not
eliminate Keynesian unemployment. Indeed it may make it worse. In a sticky-price

that we have set up here, quantities respond strongly to fiscal and
monetary policy shifts and to technological shocks. But if prices cannot jump
model like

downward In response to an initial contractionary demand shock, for example, the
effect on real Interest rates of a more rapid deflationary response of prices may

actually make the initial Impact on quantities greater. The result Is that as the
speed of adjustment of wages and prices increases, the sensitivity of quantities to
shocks increases rather than decreases. What Nakornthab shows that is not obvious
from Keynes's informal discussion Is that the duration of the quantity responses
decreases as their speed increases. The amount of time quantity variables spend away
from steady state following a shock decreases with increased price and wage flexibil-

ity. but the amplitude of their movement stays about the same and the initial speed
of their reaction increases.6
Stochastic SDecification

The model contains 11 sources of uncertainty in its neo—classical version and 11
to 13 (depending on whether the price-adjustment equations are backward or forward-

BesIdes the serially uncorrelated policy
looking) In the sticky—price version.
disturbances. c. and c. the model is driven by stochastic behavior of some of the
parameters and exogenous variables: n, g, u. 8. 6, 8, a, A, and 0. Each of these is
a logarithmic first—order AR In continuous time, except for 5, which is a logarithmic
first-order AR in unlogged form. Each has a steady—state parameter7, a decay parame-

ter and a variance parameter associated with it. The shocks c1, c, n. g. 6. and 0
are Independent, the preference shocks s and 5 are correlated with each other, but

61n a sticky-price model that differs from that here in a number of important ways.
DeLong and Summers (1986) have also made the point that Increasing rates of price
adjustment does not necessarily reduce volatility of real quantities.

7As we see below, these "steady-state" parameters need not, If dynamics are nonstationary, correspond to actual means or steady—states.
15

not with any other shocks, and the technoloa disturbances 8, a, and A are correlated
with each other, but Independent of the other shocks.

Our approach is to carry out inference as an exploration of the shape of the
likelihood function. We do not follow the procedure, common in time series infer-

ence, of using the likelihood conditional on initIal observations, as that loses
information and may generate fits that embody large 'transients" at the beginning of
the sample that are attributed to Initial conditions. (See Sims (1992b1). Models
that build in non-stationarity may easily absorb into the statistical 'trenC much of
what economists think of as business cycle variation. This Is true whether the trend
is modeled as difference—stationarity, determinIstic trend components, or as what Is
removed by some high-pass filter. At an earlier stage of this work we used unconditional likelihood based on a stationarity assumption, arguing that a stationary model
near enough to the non-stationary boundary of the parameter space could generate

arbitrarily strong persistence and thus fit even apparently non-stationary data.
However, numerical instabilities near the non-stationary boundary were difficult to
handle, so we now generate likelihood conditional on an assumption that the model

started up from its steady—state" value 100 years before the initial date of the
sample. Well away from the nonstatlonary boundary, the likelihood formed this way is
essentially identical to the unconditional likelihood formed under an assumption of

stationarity, but at the boundary it does not have the sharp singularity of the
It can still show numerical problems If the parameters imply
rapidly explosive behavior, but the problems are rarer and easier to deal with than
stationary likelihood.

those of the stationary unconditional likelihood.

If the coefficient a on the price level in the monetary policy equation is
zero, it becomes possible to write the entIre model and its first—order conditions as
functions of real money balances M/P, real debt B,'?, and the inflation rate in place
of the variables M, B and P. In this form, of course, P is by construction nonstationary. In earlier work we did not force a1, to zero and fit to levels of the
data. While we have recently switched over to relying mainly on the a1,—0 version,

the results we report below for a 3—variable fit are for the a1,*O version of the
model. The results reported for the 10—variable sticky—price model are for the a,tO
version.

16

Solving

Estimating 1k Model

The first—order conditions, the constraints, and the policy rules of the model
form a system of nonlinear stochastic differential equations. We set the model's
disturbances to zero and solve for the steady state as a function of the fixed
parameters and the mean values of the disturbances. Since the stochastic specification does not require stationarlty, this "steady state" may not correspond to an

actual steady state of the deterministic version of the model. It is what the steady
state would be it all the exogenously evolving stochastic parameters were fixed at
their "steady—state" values, despite the possibility that the dynamic specification
may imply that these exogenous parameters do not tend to stay near their "steadystate" values. We then compute a first—order Taylor expansion of the first-order
conditions and the constraints around the steady state.

From the resulting system of first—order linear differential equations, we
calculate the eigenvalues and eigenvectors. For a determinate equilibrium to exist.
the system must have a number of unstable roots8 that matches the number of forward-

The left elgenvectors associated with the unstable
linear constraints on the model's variables that must
hold to suppress the unstable component of the solution. Combining these relationships with the remaining equations In the model produces a complete linearized
solution. Our algorithm allows Interpreting the price and wage adjustment equations
in the sticky-price version to be either forward or backward looking, depending on
looking first—order conditions.
roots are the coefficients of

the number of unstable roots.
Once the linearized model has been solved, the matrix-valued autocovarlance
function can be derived as a function of the matrices of coefficients In the lin-

earized system. The autocovariance function is aggregated over time from the continuous time theoretical structure to correspond with quarterly time series observa-

tions. The likelihood function for the data can then be computed and estimated over
the detailed structure of
still be consistent with
We allow "slightly" explosive roots and hope that our
transversality conditions.
guess at a dividing line between stable and unstable regions does not affect results.
8What qualifies as unstable here In principle depends on
Roots that are slightly explosive may
the model.

If we had guessed wrong, we might have expected to find maxima at the boundary of the
allowable region of the parameter space, and this seems not to have occurred.
17

the tnt parameters of the model. The model consists of 17 endogenous variables and
U to 13 exogenous shocks. Private and policy behavior and the statistical properties of the exogenous disturbances are summarized by 46 parameters in the flexibleprice model and 50 parameters In the sticky—price version. Before being sent through

the estimation algorithm, the parameters are transformed to ensure the estimates

satisfy some simple a priori bounds (mostly log transformations to ensure
positivity).

The model is fit to two different sets of quarterly data over the sample period
1959:1 to 1992:3. InItially, three U.S. time series on real personal consumption
expenditures, bows, and real gross private domestic Investment less Inventory
accumulation are used to estimate a subset of the parameters associated with preferences, technologies, and the real exogenous disturbances. These estimates are used
to Judge the model's ability to replicate some of the calibration exercises performed
on real business cycle models. The present work, however, applies a more stringent
set of measures of fit to the data than is typically employed in calibration exercises.

The second data set adds to these three series real wages, the rate of inflation
in the GD? deflator, real total government purchases, real total government revenues,
the real monetary base, the three-month Treasury bill rate, and working age popula-

At this stage the estimation Is extended to Include all the model's parameters, Including those In the policy equations. All series are converted into per
capita terms and in the current version of the model, all series are logged except
tion.

tax revenues, the inflation rate and the Interest rate.9
Our approach to inference is based on the likelihood principle —— that Is, on
the idea that the information In the data about the model Is completely captured in
the shape of the likelihood function. From this point of view, we need not be
concerned with the fact that for parameter values close to the nonstatlonary boundary
of the parameter space the distribution of maximum likelihood estimators is not well
approximated by the usual asymptotic theory for stationary models. The maximum of
the log likelihood function and the second—order Taylor expansion of It around the
9betailed descriptions appear in a data appendix.
18

maximum carry the same sort of Information about the function's shape regardless of
the presence of' near-non—stationary behavior.
nample model In Sims and Ublig 119911.

This point is elaborated in a simple

Numerical Considerations

Estimation of this model presents some numerical difficulties, In addition to
the usual requirement that most economic variables be positive, a determinate soiulion requires there to be the proper number of unstable roots In the linearized
system of differential equations and that they generate a well—behaved mapping
between exogenous disturbances and expectational error tenns. These requirements
create complex and generally unknown boundaries to the set or feasible parameters in
the parameter space. The model, and particularly the policy behavior, is specified
to make the boundaries as simple as possible. But without Implausibly simple policy
rules and a dichotomy between the real and nominal sectors of the economy, it is
difficult to characterize the boundaries analytically.

The economics of the model implies that when parameters fall outside the feasible boundaries, either no equilibrium exists (too many unstable roots) or the equilibrium is underdetermined (too few unstable roots).1° In either case, the likelihood

function is not defined. This puts the numerical optimization problem into somewhat
uncharted waters because It Is neither an unconstrained maximization nor a con-

strained maximization with • separate routine that checks if the constraint is
satisfied.

Penalty function methods associated with constrained optimization typi-

cally assume the objective function Is defined In bad" regions of the parameter
space and tack on to that function a smooth, continuously differentiable function
that penalizes the objective function when the algorithm tries parameter values that
violate the constraint. if, as In our case, bad" parameter values Imply that the
t0More precisely, when there are too many unstable roots, an equilibrium exists only

if the exogenous stochastic processes are linearly related, which violates the

maintained assumption that they are uncorrelated. When there are too few unstable
roots the price level is not determined. We can sometimes pick an equilibrium in

this case by treating the largest stable root as If It were unstable. Then we can
use penalty-function methods by using this equilibrium to calculate the Implied
distribution of the data, adding on to the likelihood a penalty term that depends
monotonely on the degree to which the largest stable root falls below zero.
19

likelihood function

Is not defined, the

usual

penalty function methods cannot be

applied.

As an alternative, our likelihood-evaluation algorithm checks whether the candidate parameter vector implies nonsensical steady state values or the wrong number of

unstable roots and returns a large negative likelihood value In these cases. This
approach introduces discontinulties In the likelihood function, which can create
difficulties for some gradient—based algorithms. There are two kinds of difficulties. Some algorithms may try to take a gradient of the objective function at points
where the function value is worse than that at the beginning of the iteratIon. If
such a point happens to be in the region where the objective function Is flat at a
large negative value because of non—existence of equilibrium, this obviously creates
Other routines attempt sophisticated line searches that Interpolate
a problem.
polynomials across function values obtained in the line search. When the function is
discontinuous, these methods may not only fail to be useful, they may fail to find an
improved value even when a less sophisticated line search would easily find it. Our
routine that avoids these difficulties Is available as a Matlab m—f lie.

Our procedure is to linearize the model's first—order conditions and constraints

about the steady state and to use the resulting model to generate first and second
moments I or the full data matrix. With 134 (when we use filtered data) observations
and 3-10 variables, the covariance matrix we are using is of order 402-1340. Such a
matrix cannot even be stored on PC's with the most common memory endowments. We are

nonetheless able to compute the likelihood by using recursive methods to factor and
invert the covariance matrix, therefore never having to store the entire matrix.

At early stages of our work, numerical difficulties in evaluating the likelihood
led us to focus attention on fitting to quasi—differences, i.e. Y(t)—pY(t—l) for some

p In (0.1). where these difficulties lessened. This allows deviation of Initial
levels from steady—state to influence the fit, though the Influence Is diminished
relative to use of levels data.11 Note that this does not mean we are modeling in
11We have greatly reduced our model's numerical difficulties by four main techniques.
We have found a way to double the accuracy of Matlab's Ricatti equation solver lyap.m
by essentially applying It twice, We have truncated the triangular orthogonalization

generated by the block Levinson algorithm at a fixed lag length in every function
20

quasi-differences. constraining the model to Imply nonstationarity as would be the
case If we fit a VAR in quasi—differences. We are modeling in levels, using the
result to generate Implications about the second—order moments of the time series of
quasi—differences. It is true that as p approaches unity, using the differenced data
will make the fit emphasize higher—frequency characteristics of the data, and In

particular will pay less attention to the distance of the Initial value from the
steady—state.

Our method for solving the linearized model, based on the QZ decomposition, is
somewhat non—standard and probably competitive or superior In efficiency with standard methods. It makes no use of the fact that the model comes from optimization
problems, working only off boundary conditions limiting the size of unstable roots.

It handles singularities In the matrix of coefficients on derivatives automatically
and does not require explicit casting of the model into state-space form. (The
algorithm must be told how many unstable roots to squash, however.) Though we can
make the code available, this part of our code Is model—specific In its current form.
Results

In Judging the model's goodness of fit, we are not simply testing the model to

see whether It is true. We do use statistical measures of fit and compare, with
likelihood-based test statistics, our model to others, including naive no—change
predictions and VAR s. Like many REC researchers, we are not ready to cast our model

aside as soon as we find a VAR that clearly fits better. On the other hand, we are
also not ready to make excuses for our model that Imply we are ready to rely on It
for policy conclusions even though It clearly misses or contradicts patterns of
behavior observed in the data. Our model has been formulated with enough sources of
random disturbanCe and enough free parameters that It Is not implausible that it
could match the observed time-series variation In the 10 variables we aim at explaining. While the model does yet f it quite as well as an unrestricted VAR with a
evaluation.

At some stages of our work we used a Bayeisan hill-climbing algorithm

that can be adjusted to make It Insensitive to modest levels of rounding error in
likelihood evaluation. And finally we have used the likelihood conditioned on the
data being at steady state" at a distant past point, as described in the text. A
separate paper or papers describing these numerical Innovations Is in preparation.
21

similar number of parameters, It comes close enough to suggest that with a little
more work and a few judicious modifications of the model structure a I' It as good as a
VAR

fl

fit may be attainable.
Three-VariabI Qa 5cS

First we report the parameter estimates Implied by fitting the flexible-price
model to quasi-differences of time series on consumption, hours, and Investment.

Table 1 reports the estimated values for the 33 free parameters associated with the
version of the model that Is formulated In the levels of nominal variables.12 Because
at this stage we are not using data on 7 of the 10 variables and policy shocks are
neutral In the flexible—price model, the values for the remaining 12 parameters from
the policy rules have no effect on the likelihood value and are not reported. Most
of the estimated parameters look reasonable. The risk aversion parameter. ;. is
estimated to be 8.82, which is in tine with some previous estimates. There is some
slight convexity to the technology that transforms output goods into consumption and
investment goods (M — 1.025) and the elasticity of substitution between labor and
capital is estimated to be quite low Ce • —.32). Point estimates for two Important
parameters —— the discount rate. $, and the depreciation rate, 6 —— are less reasonable. but so Imprecisely estimated that they are within one standard error of plausible values. Most of the exogenous processes have roots away from the unit circle:
only the shock to total factor productivity has a root above .95.
Table 2 compares the forecast errors from the estimated model with those implied
by naively assuming no change In the data and by fitting an unrestricted VAR to the
three quasi-differenced time series. The VAR is estimated with 2 lags of each

variable and a constant term. The number of free parameters in the VAR, then.
(Including the 6 rree parameters in the covariance matrix of the Innovations) is 27.
12we actually maximize the likelihood concentrated with respect to a scale factor for
the variances. Thus we can fix the variance of one of the exogenous processes as a

normalization, and the standard errors we compute on the variances are actually
standard errors on their ratios to the pegged variance parameter. This follows from

the fact that the concentrated likelihood Is almost exactly the same as the
likelihood Integrated over the same parameter, so that the concentrated likelihood
can be interpreted as an approximate marginal posterior p.d.f. The standard errors
of the estimates are computed using a BItS—update of the inverse of the Hessian
matrix.
22

By the log determinant criterion, the model fits the data about 22% better than does
the assumption of no change and It fits only 1.8% worse than the unrestricted VAR.13
The model also comes close to matching the contemporaneous correlations among the VAR
innovations.

On the other hand, the log determinant criterion Is proportional to the maxiniized likelihood for the VAR and for the naive no-change model for which the covariance matrix of first differences Is the innovation covariance matrix. For our
structural model, the log determinant criterion Is not proportional to the maximized
likelihood.

For direct comparison of likelihood, we should compare -.5x134x(log det

covarlance matrix)-.51343 for the two naive models to our fitted log likelihood or
The two naive models have, by this calculation, Jog likelihoods of 1469.04
1545.48.
and 1563.71, respectively. The likelihood" reported in the table for the model is
computed from the log det covariance matrix just as It was for the two naive models.
The model's actual likelihood is slightly lower. This Is partly because unlike the
reduced-form models, It does not leave the covarlances among the Innovations unrestricted. However the model fits the data unconditionally, while the VAR is fitting
only conditional on the initial observations, so the two likelihoods are not strictly
comparable.14

For convenience, the table also reports Choleski decompositions of the covariance matrices, which make comparisons by variable across models easier. The model
Improves on a random walk for consumption, hours, and investment. In the case of
investment the model produces an error variance below that of the VAR.

The estimated parameters Imply a steady state that matches the means of the data
in some but not all respects. Except for consumption, the steady state values of the

three series are near their means, as are the ratios of hours and investment to
see this, divide one half of the difference in the relevant log determinants by
For the likelihood values, the same sort of estimate of
average percentage difference In standard errors of forecast" Is the difference In

the number of variables.

likelihoods divided by sample size times number of variables.

14lt Is also true that the model Implies that the covariance matrix of Innovations
varies over time, so that In computing its likelihood, errors at different dates are
weighted differently. But it appears that this effect Is not strong here compared to
the effect of restrictions on the covariances at a point in time.
23

consumption. The model also Implies a labor share of Income equal to .90, which is
higher than the two-thirds typically cited.

Figure 1 shows time series charts that compare the model's output with the
innovations from the VAR. The shaded areas correspond to NBER business cycle peaks
and troughs. For consumption, the model keeps pace with the VAR in the middle of the
sample and during most recessions, but performs less well than the VAR in the l960s
and outperforms the VAR In the 1980s. In the second panel, the model performs

remarkably well in predicting hours fluctuations, though It has a slight tendency to
exaggerate declines In hours during recessions. The model also predicts Investment
is well as the VAR.

FIgure 2 contrasts the reduced-form moving average representations over 20
quarters from the VAR with those from the model. The covariance matrices are orthogonaliaed in the order consumption, hours, and Investment. Estimating the VAR in
quasi-differences eliminates most of the dynamics in the response functions, with the
responses of the three series dying out quickly following their own disturbance. The
model appears to reproduce the contemporaneous correlations among Innovations well,
but implies more persistent responses in consumption to consumption and hours innovations than observed in the data.

Once estimated, the model can be used to evaluate the underlying exogenous
sources of fluctuations In consumption, hours, and Investment over the sample period.

It turns out that shocks to only three of the 11 exogenous processes in the model
account for all the fluctuations In the three endogenous series. The three important
disturbances are w, a shock to consumption's share In utility, G, a shock to Investment in the cost-of-adjustment technology, and a, a shock to the marginal product of
capital in the production function.

Table 3 reports the percentage of each variable's forecast error variance due to
the three shocks in the short- and medium runs. In the short run, all three shocks
contribute to consumption fluctuations, with w and 0 accounting for over one—third of
the variance each and a accounting for one—quarter. As the forecast horizon extends,
however, e

becomes

the dominant source of fluctuations in consumption. Two—thirds of

the error variance of hours Is due to the preference shock and one—third is due to
24

the cost-of-adjustment shock, regardless of the forecast horizon.

Finally, fluctua-

tions in investment arise almost entirely from shocks to the technology that transforms output goods into consumption and Investment goods.

For the model's implications to be credible, the estimates must produce sensible
Figure 3 reports the responses of condynamic responses to the exogenous shocks.
sumption, hours, and investment to the three Important structural disturbances.35 The

first row shows that a transitory preference shock that makes consumption more
desirable raises consumption and lowers leisure contemporaneously. Investment rises
gradually, reaching a peak after about one year before declining smoothly. Responses

to a shock to 0 appear in the second row. Higher 0, making investment goods rela-

tively more expensive, can be thought of as a decline In the productivity of the
capital goods sector.

Because 6 is serially uncorrelated (p0 —

-3191

Implying almost

no persistence). the shock lowers the one-period return to investment, causing
Investment to fall precipitously and return Immediately to Its normal level. Hours
worked fall with the caital stock, but then rise to compensate for the decline In
capital. Consumption drops to a permanently lower level consistent with the one-time
decline in the capital stock. The shock to the marginal product of capital, a,

raises the rental price of capital and drives down investment and hours worked
initially. The serial correlation of the shock (estimated root of .72) generates a
smooth decline in consumption, which bottoms out after about two years. As noted in

Table 3, disturbances to a are unimportant sources of fluctuations in hours and
investment, but relatively important for short—run movements in consumption.

fl

10-Variable
We

Q

had substantial numerical difficulties with the 10—variable neo—classical

version of the model, and did not achieve a respectable fit with it. The fit remained particularly bad for the price variables. We do not take these difficulties
as proof that this version of the model cannot fit the data —— we had substantial
numerical difficulties with each version of the model at one point or another, so

'5The responses are plotted over a five—year period as monthly samplings of the
underlying continuous response function, not as responses of the time-aggregated
actual data.
25

that the fact that we do not have results to display for this version of the model at
the deadline for this manuscript could be simple misfortune. But the difficulty with
the nec—classical model fit to price data does accord with speculation by one of the
authors (Sims (1989)). and may yet turn out to be a robust result.
For the sticky—price model we obtained apparently converged results.
likelihood

The

value at the best fit is 5348.91, quIte a bit lower than the 5742.26

with a first-order VAR containing constant terms and with an unrestricted
covariance matrix of disturbances. The VAR in this case, however, has 165 parameters
compared to 49 for the model. The difference In likelihoods Is still large relative
to degrees of freedom, but again we must recognize that we cannot draw finn conclusions from comparing conditional and unconditional likelihoods. The log determinants
of the residual covariance matrices are —95.705 for the VAR and -92.651 for the
model, and these are on a comparable sample. The difference corresponds to an
average improvement by the VAR over the model of 15% of the standard error of forecast. The model has almost the same log determinant of the covariance matrix of
errors as does the naive no-change forecast. Table 7 shows that, In constrast to the
obtainable

3-variable data set, here the model's fit is closer to that of a naive no-change
forecast than to that of a VAR.

While the fit achieved here leaves plenty of room for Improvement, It is still
Interesting to explore what son of economic interpretation this model supplies.
Tables S and 6 give most of the story. The model is converged to a parameter value
in the purely Ricardian region of the parameter space. Shocks to the tax equation
(the Fpolicy column) have no effect on any of the 10 variables other than taxes
themselves. Monetary policy looks weak, Judged by the size of the entries in column
However this reflects a low estimate of the variance of shocks to monetary
1.
policy, and the tendency of larger shocks to dominate variance decompositions, where
squared responses matter. In Table 6, which shows cumulative per cent responses to
sustained one-time shifts of one "standard error unit"16, we see that responses of C.

6Slnce this is a continuous time model, a sustained shift In the disturbance (which
is modeled as white noise) Is even more atypical of realizations of the model than It
would be In a discrete model. The sizes of the responses look large because the

typical disturbance is so little sustained that It never builds nearly this much
cumulative response.

26

I.., and I to a monetary contraction are substantial and of reasonable signs. Because
the model is so tightly parameterized. It does not produce fancy dynamics in the
impulse responses. Figures 4-6 show four typical response shapes. In all three
raphs the x axis is In monthly units. In Figures 5 and 6 the responses have not yet
begun to die away after three years.

Inflation responds to nothing but the price adjustment shock, while wages
respond to nothing but the wage adjustment shock.

This is a model In which most

fluctuations in prices are persistent, generating no expectation of Inflation or
deflation, and In which the rest of the economy has little impact on price movements.
In this sense the fitted model Is showing extremely sticky prices, with none of the
expecational instability we have noted Is In principle possible In these models.

An open question is whether in this fitted model the extreme price stickiness
and strong influence of real shocks is dependent on the particular monetary and
fiscal policy rules that are estimated here. We could simulate the model with
alternative policy rules to check this.

27

Table 1. Three-VarIable Data Set: Estimated Parameters for flexible Price Model
(Standard errors In parentheses)

Preferences

a 0.341
(.060)
—

0.165

(.228)
—

a 3.961e-4

p1 a

-2.912
(.194)
-0.635
Pp —

(.153)

(2.730e-1)
— -4.019

Pg

(.206)

—

5.277e-6

(3.209e-2)

8.882

(.145)

Technologies
B

— 0.528
(.082)

a a 0.352

p9 — -3. 191e+3

(.240)
p

— -0.324

a9 • 1.186

(.126)

(.088)

(.092)

a -8.715r5

A — 96.794
(.105)
— 2.137e-2 -

Cr2

a

(369)
— 2.934e—4

(.182)
6

a 0.250
—

1.995e-5

S

p

—0.613

1.025

a a -0.315

A9 — 0.559
(.205)

a1 — 6.847e-6
(.216)

a2 — 1.962e-2

(.105)

(.135)

Government

(.147)

(0.363)

p5 a -1.821
(.232)

(.324)

A — 2.520

(.094)

(3.194e—2)

— 8.169e-2

(fixed)

(.037)

Spending and Population

j/Y a 7.731e-2
(.292)

a a 1.400e-2
(.292)

pg a -5.428e-2
(0.114)

'

p • —5.930
(.047)

a2 a 4.823e-6
g

(3.165e-2)

a2
I' a 4.894e-6
(3.186e-2)

Log Likelihood Value a 1545.48

Is the first-order

AR

coefficient on the continuous-time process x.

Is the

determining how y depends on x, and 2 Is the variance of the process.PoIicy parameters do not affect the likelihood and were fixed arbitrarily to
coefficient

ensure a unique equilibrium exists.

22

Table 2. Three-Variable Data Set: Fit of the Flexible Price Model
Covariarace/Correlatlon Matrices

First Difference of Data
-24.926 LIkelihood a 1469.04

Log Oct

cboleskl decomposition
C

I..

I

C .i69
L .0026 .0106
1

.0072 .0048 .0215

VAR Innovations (2 lags plus constant. 21 free parameters; estimated using quasi—
dir fercaced data)
Log Oct — -26.339 LIkelihood a 1563.71

Qwleski decomposition
I
C
L

C .0124
L .0016 .0099

I .0080 .0048 .0173

Model Residuals (33 free parameters)
Log Det

-26.229 tlkelihood" — 1556.34

choleski decomposition
C

L

I

C .0128
L .0015 .0092

1 .0081 .0045 .0171
The Model's Steady State Versus Means of the Data
U.S. Data Means

Model

Steady State

C

13.96

20.48

L

0.353
2.292

3.110

L/C
I/C

0.025
0.164

0.350
0.011
0.152

29

Table 3. Three-VariabLe Data Set: Variance Decomposition
Percentage of Forecast

Error Variance Attributable

After 6 months to
C

L

n

B

a

a5
6B

42
32

22

After 3 years to

0

30

K

$

U

B

B

83

67

32

10

85

0
2

Table 4

Estited Parameters for 10-variable sticky Price
0.0681

2 Pg

—0.00147

0.0149

—9.63

51
7$

4p

0.614
0.06

6 p,1

—0.000285

8p

—0.000488

9 $w
11 p9

—1.56

10 0

1.81

—1.28

12 a

—0.000754

14 aS

0.624
0.412

107

16 'A

—0.000306

—0.612

18 AG

—1.089

0.00532

20 p9
22 p6

—0.00165

i

17?

3

13 p
15 A
17 Aa

19 0
21

23 p

25 r
27 ar
29

b

31

b

33

P

x

35

0.465
25.8
5.65

24 a

26 a1.

mpol

Padj
g

41

43

jI

45 9

47 A
49

8

0.00748
1.66

5.47
2.41

30 bL

1.36
1.82

0.563

34 BGNP

0.0188

in!

36

Equation
39

—25.0

28 aL

0.008041

37

—0.0107

—1.06

32 b

0.118
0.00019

Variances

38 (pci

0.0283 40 wadj
0.0175 42 n
0.00196
44 fi
0.00786
0.00301
0.401

Model

46 a

48 •

Note:

0.0209

0.00166
0.00372
0.00107

0.00722
0.0986

The parameters are as they appear in the text, except that bars over
the parameters indicate steady—state values and the double—greek—letter
parameters refer to responses In the exogenous dynamics. For exampi, Ia is
the coefficient on the level of a in the differential equation with A on the
left.

3'

s. a as a a a
6000000000

aaaS.S
00
66000000

ass

do 6000 0000
a,

sassaa

d 6 6 6 00 0000
—
o
'100000
6666660600

C)

Cl)

a,

.0
Ct

I.C,

* 90
C
N Q , r0 -00
0C
r 0000

666666

6

gflg3S8
00000 0 0 000

r

S
00 o 00 0
0 G3
0 00
00000
0
9
00—
dodo 00 0 9
C

0
In
—o
.0

0 0 0 0 00
000
oo00000.90.9
000000000

'-F0 00000C000
0 00
0. 0
C,

C)

a,

C
a,
C.)

C
C,

I-

0• 0 0.
oooooooo66

000.-000000
ooOfloqqOO.0
0000 —0 o dod

C,

> g$sr.8asaSS
oooo 600000

aaaflgJ.
000000 0 000
U

_a_.5Ssc

C

32

(0

aa)
(UI-ct
E
C-)

33

Table 7

Standard deviations of errors
Model

C

L

Dirt. s

0.0130

0.0119

0.0114

0.0084

0.0201

0.0102
0.0238
0.0108
0.0174

0.0047

0.0048

0.0146

0.0111

0.0126

0.0161

0.0090
0.0154

0.0146

0.0029

0.0029

0.0026

0.0101

I
0.0234
tx/V 0.0119
liii!

W

Mi?

0.0092
g

VAR

34

0.0195
0.0096
0.0143

0.0045

0.0084

Appendix: The Ffnt-Ordcr Conditions
The first—order conditions for the firm are

X

CM)

[JilL
1-p

r

(ft)

(A3)

—

Ata{J
(A4)

W—

A°[J
For the agent they are
liZ

8L:

1-L '

r

—vX

(l_li)Z_7[]L] a

(AS)

(A6)

(where ZC (I-L)
(A7)

81:
A—v

(AS)

(A9)

35

1'

-tI_i_s_p

88:

(MO)

(All)

DY:

OK:

•Yv — -w

(AlZ)

—t
0 —6—5
0

(A13)

Here are the agent FOC's manipulated to get rid of Lagrange Multipliers:

.

I-i C

(l2*V) -j- nti

(A14)

I

(AIS)

- w(l—y))j +(l—ir)(I-n)jJj — I

—

5—

x — Pp

C

i-—.iz(l-.

(l-2#V)

+

-a—I-

36

(Al61

p

(Al?)

Data Appendix
consumption: Personal Consumption Expenditures. NIPA, deflated by the PCE defla-

tor.

Investment: Residential plus Non—Residential Fixed Investment, NIPA. deflated by
their respective deflators. NIPA.

Civilian employment times weekly hours index for total goods production, scaled to lie in the unit interval.
-.

Employment:

Real

wages: Index of compensation per hour in the nonf arm business sector,

deflated by the GDP deflator.
Price level: GOP deflator, NIPA.

Government purchases: Total federal plus state and local purchases. NIPA.
deflated by the GDP deflator.

Government tax revenues: Total federal plus state and local tax receipts, less
transfer payments, with federal grants to state and local governments netted out,
NIPA. deflated by the GNP deflator.

Money: The Federal Reserve Board's monetary base, not adjusted for reserve requirement changes.

Interest rate; Three-month Treasury bill rate, secondary market, In basis
points.

Population; Quarterly growth rate of the civilian non—institutional population.
at annual rates.

Per capita series are obatined by deflating by the population. All series
seasonally adjusted except the Interest rate and population. Monthly series are
time—averaged to quarterly frequencies.

37

References

Altug. Sumn (1989), "Time-to-Build and Aggregate Fluctuations: Some New Evidence," International Economic Review, 30, November. 889-920.

Thomas F., Stephen F. LeRoy. and Nell Raymon (1984), "Econometric Policy
Evaluation: Note," America-n Economic RevIew 74, June, 467-70.
Cooley,

lIe Long, i. Bradford and Lawrence H. Summers (1986), Is Increased Price Flexibility Stabilizing'?" American Economic RevIew 76, December, 1031-44.

King. Robert (1991). "Money and Business Cycles".
Journal of Monetary Economics.

Processed.

To appear.

Kydland, Finn and E. Prescott (1982). "TIme—to—Build and Aggregate Fluctuations," Econometrica 50, November. 1345—1370.

Eric M. (1991), "Equilibria Under 'Active' and 'Passive' Monetary And
Fiscal Policies," Journal of Monetary Economics 27, February, 129-47.
Leeper,

Leeper, Eric M. (1993), "The Policy Tango: Toward a Holistic View of Monetary
and Fiscal Effects," Federal Reserve Bank of Atlanta Economic Review 78, July/August,
1-27.

LeRoy, Stephen F. (1993), "On Policy Regimes," Manuscript, University of Minnesota, August.

McCrattan, Ellen R. (1992), "The Macroeconomic Effects of Distortionary Taxation," forthcoming Journal of Monetary Economics.
McGratten, Ellen R., Rogerson, Richard, and Randall Wright (1993), "Household
production and Taxation in the Stochastic Growth Model," Federal Reserve Bank or
Minneapolis Working Paper 521, October.

Nakornthob, Don (1993). "The Study of a Deterministic Keynesian Model with
Perfect Foresight in Asset Markets," Yale College Senior Essay, April.

38

Sims. Christopher A. (1982). "Policy Analysis with Econometric Models," Brooktngs Papers on Economic Activity 1, 107-64.
______________________

(1987). "A Rational Expectations Framework for Short-Run

Policy Analysis." in: Barnett, W.A. and K.J. Singleton. eds., New Approaches to
Monetary Economics (Cambridge, England: Cambridge University Press).
________________________(1989), "Models and their Uses," American Journal

of

Agricultural Economics.
________________________(1992a). "A Simple Model for Study of the Determination of

the Price Level and the Interaction of Monetary and Fiscal Policy." forthcoming
Economic Theory.
__________________________(1992b). "Bayesian Inference for Multlvariate Time Series

Models with Trend. - mimeo, November.

Sims. Christopher A. and Harald Uhlig (1991). "UnderstandIng Unit Rooters: A
Helicopter Tour," Econcimetrlca 59, November, 1591-99.

Taylor. John (1979).

"Staggered Wage Setting in a Macro Model," American

Economic Review, 69. May. 108-113.

Watson, Mark (1993), "Measures of Fit for Calibrated Models," Journal of Poll Itcat Economy. 101. December. 1011-41.

39

CUBIT$IC4'P LNCYOtkIS (solid) Vt MI

R60AGIS (fled)

0.03

01110.00
-001

-

I63 66?607I

74

77

00

60

60

60

00

ISal hmotbs (solid) Vt Moil R60ktnb (fled)
0.01

0n2
orn
0.00
-001

-0

-om
-0.01

63

62

66

60

71

71

77

00

lmavotlcns (solid) vs. Mol R60kN.cls (fled)
0.061

0010

0016
0000
-01110
-01110

-01
63

62

66

68

71

71

7?

00

FIGURE 2

I

g

I-.
C

I

•

—

— II.

S

S

6

Is
a
—

r
S

c

I

•1

;,
C
4'

41

I

a

ii

!

a

L

a

a

uintI

U

plan ott

a

Spoil

a

ci

!'

t

1

an

m0-

we

a

t,

a

a a

a

a

mallet,

&

ii

zi

a

U

ci

t

wee-

wee-

Dee

a

etaa eta

Qulpota

a

we

a

a

ave

a

a

no

me

lee-

me

Ia,

Inn

lee-

a

FIGURE 4

Response of C to M Policy

40

43

FIGURE 5

Response of C to theta Shock

0.0767

0

5

10

20

15

44

25

30

35

40

FIGURE 6

Response of r to RI Shock

-0.01

0

5

10

20

15

45

25

30

35

40

