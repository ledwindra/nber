NBER WORKING PAPER SERIES

PARTISAN BIAS IN FACTUAL BELIEFS ABOUT POLITICS
John G. Bullock
Alan S. Gerber
Seth J. Hill
Gregory A. Huber
Working Paper 19080
http://www.nber.org/papers/w19080

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
May 2013

The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2013 by John G. Bullock, Alan S. Gerber, Seth J. Hill, and Gregory A. Huber. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.

Partisan Bias in Factual Beliefs about Politics
John G. Bullock, Alan S. Gerber, Seth J. Hill, and Gregory A. Huber
NBER Working Paper No. 19080
May 2013
JEL No. H0,H1
ABSTRACT
Partisanship seems to affect factual beliefs about politics. For example, Republicans are more likely
than Democrats to say that the deficit rose during the Clinton administration; Democrats are more
likely to say that inflation rose under Reagan. We investigate whether such patterns reflect differing
beliefs among partisans or instead reflect a desire to praise one party or criticize another. We develop
a model of partisan survey response and report two experiments that are based on the model. The experiments
show that small payments for correct and “don't know” responses sharply diminish the gap between
Democrats and Republicans in responses to “partisan” factual questions. The results suggest that the
apparent differences in factual beliefs between members of different parties may be more illusory
than real.

John G. Bullock
Institution for Social and Policy Studies
Yale University
PO Box 208209
New Haven, CT 06520-8209
john.bullock@aya.yale.edu

Seth J. Hill
University of California, San Diego
Department of Political Science
9500 Gilman Drive, #0521
La Jolla, CA 92093
sjhill@ucsd.edu

Alan S. Gerber
Yale University
Institution for Social and Policy Studies
77 Prospect Street
New Haven, CT 06520
and NBER
alan.gerber@yale.edu

Gregory A. Huber
Yale University
Institution for Social and Policy Studies
77 Prospect Street
New Haven, CT 06520
gregory.huber@yale.edu

A persistent pattern in American public opinion is the presence of large differences between Democrats
and Republicans in stated attitudes about factual matters. For example, in 2010, Harris Interactive
surveyed U.S. adults to determine their beliefs about whether Barack Obama was born in the United
States. Forty-five percent of Republicans stated that he was born abroad, compared to only eight percent
of Democrats (Harris Interactive 2010). This partisan divide appears in many other surveys and on other
topics, e.g., whether weapons of mass destruction had been found in Iraq (Harris Interactive 2006; see
also Duelfer 2004). Partisan divisions would be expected for questions about political attitudes or tastes,
but they extend even to evaluations of economic trends during a president’s tenure (Bartels 2002,
133-38).
Systematic partisan differences in responses to factual questions about politics are not mere
curiosities. Some of the strongest defenses of democracy rest on theories of retrospective voting: even if
voters know little, these theories maintain, they can administer “rough justice” by rewarding or punishing
incumbents for things that have happened during their terms (Fiorina 1981, 4). But partisan differences in
responses to factual questions call into question voters’ abilities to vote retrospectively (see also Healy
and Malhotra 2009). And partisan bias in voters’ memories of incumbent performance may give
incumbents weaker incentives to seek true improvements in voter welfare.
More generally, both recent and classic research supports the notion that partisanship has a
powerful influence on attitudes and behaviors (e.g., Gerber, Huber, Washington 2010; Campbell et al.
1960). In light of this tradition, it is not surprising that scholars often take survey respondents’ statements
of beliefs at face value (e.g., Bartels 2002; Shapiro and Bloch-Elkon 2008; Jerit and Barabas 2012). If
partisanship is a “perceptual screen,” partisan differences in factual beliefs are a natural consequence of
the influence of partisanship on information acquisition and processing. This paper, however, considers a
distinct alternative: partisan differences in survey responses may not solely indicate differences in true
beliefs. Instead, they may also reflect the expressive value of offering survey responses that portray one’s
party in a favorable light. A partisan pattern of survey response follows when this expressive value
outweighs the utility that survey respondents would receive from stating their sincere beliefs. Partisan
3

divergence may therefore reflect the joy of partisan “cheerleading” rather than sincere differences in
beliefs about the truth. Despite the reality that survey respondents have limited incentives to respond
accurately to survey questions, almost no research has attempted to determine the extent to which partisan
divergence in responses to factual questions with partisan implications reflects sincere beliefs.
This paper reports results from two novel experiments designed to distinguish sincere from
expressive partisan differences in responses to factual questions. In both experiments, all subjects were
asked factual questions, but some were given financial incentives to answer correctly. In both
experiments, we find that the incentives reduce partisan divergence substantially—on average, by about
55% and 60% across all of the questions for which partisan gaps appear when subjects are not
incentivized. But offering an incentive for accurate responses will not deter cheerleading among those
who are unsure of the correct factual response, because such people stand to gain little by forgoing it. In
our second experiment, we therefore implement a treatment in which subjects were offered incentives
both for correct responses and for admitting that they did not know the correct response. We find that
partisan gaps are even smaller in this condition—about 80% smaller than for unincentivized responses.
This finding suggests that partisan divergence is driven by both expressive behavior and by respondents’
knowledge that they do not actually know the correct answers.
These results have important implications for our understanding of public opinion. Most
importantly, they call into question the claim that partisan divergence in beliefs about factual questions is
ground for concern about voters’ abilities to hold incumbents accountable for their performance. Partisans
may disagree in surveys, but we should not take these differences at face value. 1
These results may also affect our understanding of partisan polarization in the mass electorate. At
least in the realm of factual evaluations, partisan differences exist, but our results suggest that they are not
as large as naïve analysis of survey data suggests. Just as people enjoy rooting for their favorite sports
team and arguing that their team’s players are superior, even when they are not, surveys give citizens an
1

These results also have implications for the literature on economic voting, because our results confirm
concerns (e.g., Ansolabehere, Meredith, and Snowberg 2013) that survey reports of economic conditions
may be contaminated by expressive partisan responding.
4

opportunity to cheer for their partisan team (Green, Palmquist, and Schickler 2002). Deep down,
however, individuals understand the true merits of different teams and players—or, at the minimum, they
understand that they don’t know enough to support their expressive responding as correct.
Our work also has important methodological implications. In particular, how should one interpret
experiments that show partisan cues increase partisan divisions in survey response? Such results are
commonly taken to show that partisanship affects attitudes. Our results suggest, however, that partisan
cues may merely remind participants about the expressive utility that they gain from offering partisanfriendly survey responses. A key task for researchers is thus to understand when survey responses reflect
real attitudes and when they reflect these more expressive tendencies. Such an understanding is essential
if survey attitudes are used to explain vote choices or other salient political outcomes, a topic we take up
in the conclusion.
The remainder of this paper proceeds as follows. We begin by reviewing prior theory and
evidence about partisan response patterns in answers to factual questions. We then present a model of
survey response that incorporates the possibility of expressive benefits to offering partisan responses. This
model informs the design of our two experiments, which we present in the next sections. The final section
considers the implications of our results and avenues for future research.

THEORY AND PRIOR EVIDENCE
Prior research documents partisan differences in expressed factual beliefs (e.g., Jerit and Barabas 2012;
Gaines et al. 2007; Jacobson 2006), and some of it focuses on differences in evaluations of retrospective
economic conditions (e.g., Conover, Feldman, and Knight 1986, 1987; Bartels 2002, 133-38). 2 Many of
these partisan gaps arise because members of one party issue retrospective economic assessments that
deviate starkly from objective conditions. For example, despite the large improvement in unemployment
and inflation during Ronald Reagan’s presidency, Bartels (2002) shows that, in 1988, Democrats were
more likely than Republicans to incorrectly report that unemployment and inflation had increased since
2

A related but distinct literature concerns partisan differences in responses to non-factual questions (e.g.,
is the president evil?) or to questions whose answers are not readily observable (see, e.g., Berinsky 2012).
5

1980. This pattern was reversed in 2000, at the end of the Clinton presidency, when Republicans were
more likely to offer negative retrospective evaluations. 3
The key question is how to interpret these partisan gaps. Bartels presents the common view when
he argues that partisans likely believe their divergent assessments: “Absent some complicated just-so
story involving stark differences in the meaning of ‘unemployment’ and inflation…among Democrats and
Republicans, these large differences can only be interpreted as evidence of partisan biases in perceptions”
(Bartels 2002, 136-37). An alternative view is that differences in survey responses are the result of a
combination of motivations. Individuals may express responses consistent with their partisanship not
because they believe those responses, but because doing so gives them opportunity to support their
“team” (e.g., Gerber and Huber 2010; Green, Schickler, and Palmquist 2002).
Of course, many social scientists have wrestled with the problem of insincere survey responses
(e.g., Berinsky 2005; Kuklinski, Cobb, and Gilens 1997; Noelle-Neumann 1993). But they typically focus
on responses to sensitive topics (e.g., race) rather than on problems that may be caused by “expressive
benefits” in survey response. 4 And the methods used to overcome problems associated with responses to
sensitive topics—for example, “list experiments” (Kuklinski, Cobb, and Gilens 1997)—may not apply to
the problem of eliciting sincere responses when people derive expressive benefits from answering
insincerely.
Instead, researchers have long turned to incentives to induce honest responses or rational
behavior. In a review of experiments involving incentives, Morton and Williams (2010, 358-61) argue
that incentives often reduce the size and frequency of decision-making errors. But almost all of the studies
that they review are apolitical and do not involve tests of factual knowledge. Prior and Lupia (2008) do
3

Additional work examines conditions that can exacerbate apparent partisan gaps. Asking political
questions prior to economic ones increases the correlation between partisanship and subjective economic
evaluations (Lau, Sears, and Jessor 1990; Palmer and Duch 2001; Sears and Lau 1983; Wilcox and
Wlezien 1993), and partisan gaps are larger when elections are more salient (Lavine, Johnston, and
Steenbergen 2012, ch. 4; see also Stroud 2008). What is unclear in this work, however, is how to interpret
these patterns. Do circumstances that make partisanship more salient provide factual information to
survey respondents or do they simply increase the expressive value of partisan responses?

4

An exception to this characterization is the literature on economic voting discussed above.
6

study the effects of financial incentives on responses to factual questions about politics, and they find that
the effects of offering incentives are real but weak. 5 However, they do not examine the effects of
incentives on partisan patterns in responding.
To date, only one study has examined the effects of incentives on partisan response patterns to
factual questions about politics: Prior (2007). Subjects in the study were asked 14 questions about
politics; some were assigned at random to receive $1 for each correct answer. The results were mixed, but
they suggest that $1 incentives can reduce party differences in responses to such questions. 6 One
unanswered question in that work, however, is how respondents who do not know the correct answers
should be expected to behave in the presence and absence of incentives. It may be, for example, that
partisan responses are insincere, but that respondents continue to offer them when given incentives to
provide a correct response because they do not know what other answer might be correct.
To address these questions, we present a model of survey response that incorporates the
possibility that individuals (a) receive utility from offering partisan-tinged responses and (b) differ in their
underlying knowledge of the truth. We use this model to understand the effect of incentives on a
respondent’s tendency to answer questions in a manner that reflects either her partisan affinity or her
sincere beliefs about the truth. We also show that our model can be used to understand the proportion of
partisan differences that arise because individuals are uncertain about the truth.

A THEORY OF EXPRESSIVE SURVEY RESPONSE
To understand the role that expressive survey response plays in the partisan polarization of survey
responses, and to motivate our experimental design, we present a model of survey response in the
presence and absence of financial incentives. Our objective is to provide intuitions about how sincere

5

All subjects in the Prior and Lupia (2008) study were asked 14 factual questions about politics. Subjects
in a control condition averaged 4.5 correct answers, while those who were paid $1 for each correct answer
averaged 5.0 correct answers (Prior and Lupia 2008, 175).

6

Incentives reduced partisan gaps in responses to four items. Results on a fifth item were mixed. Results
were null for two other items. There was no partisan gap in the control group for three further items, and
results for the remaining four items were not reported.
7

differences in beliefs about the truth, the expressive value of partisan responding, and incentives may
interact to shape polarization in survey responses. The model allows us to clarify expectations about our
experimental findings. As in our experimental design, financial incentives in the model take two forms:
Respondents may be paid for offering a correct response or for admitting that they “don’t know” the
correct answer.
We begin by focusing on incentives for a correct response. Our model shows that incentives
allow us to assess the degree to which partisan divergence arises because people simultaneously
(1) understand that they would otherwise provide inaccurate partisan responses, (2) give low value to this
“cheerleading” relative to the size of the offered incentive, and (3) maintain strong beliefs about the
correct answer that are common to members of both parties. Under these conditions, a surveyor can
reduce partisan divergence by offering incentives for correct responses.
A consequential aspect of our models is the third condition: a shared bipartisan belief about the
truth. Holding all else constant, if members of different parties have different beliefs about the truth, then
financial incentives will not cause their survey responses to converge. This, of course, is what most
existing research assumes is true: that partisan differences in factual beliefs are sincere. Alternatively,
however, competing partisans may not differ in their beliefs about the truth, but they may still be
uncertain enough about the truth that a reward will not cause them to deviate from their partisan-tinged
response. Put another way, if I’m being paid for a correct response but I believe that my preferred partisan
response is as likely to be correct as any other (i.e., I am completely uniformed), then my expected utility
is maximized by continuing to offer the response that gives me the most expressive partisan utility.
In light of this ambiguity, we extend the model by incorporating incentives for admitting one’s
lack of knowledge. When respondents are offered incentives for both correct and “don’t know” answers,
our analysis shows that the proportion of respondents choosing “don’t know” is increasing in the
proportion of respondents who (1) give low value to partisan cheerleading relative to the size of the
incentive for choosing “don’t know,” and (2) have sufficiently weak beliefs about the truth that they are
better off choosing “don’t know” than any other option. Overall, incentives for “don’t know” responses
8

allow us to estimate the proportion of partisan divergence that arises because respondents know that they
don’t know the truth and instead offer expressive partisan responses in the absence of incentives for
choosing “don’t know.”

Basic Model
We begin with a model in which respondents derive utility from their survey responses in three ways: by
offering answers that cast their party in a favorable light, by expressing their sincere beliefs, and by
earning financial rewards. For now, we set aside the possibility that people can choose to say “don’t
know.” For simplicity, we focus on the case in which there are two survey responses, r1 and r2.
Individuals, indexed by the subscript i, are either Democrats (T = D) or Republicans (T = R). Individuals
differ in their taste for partisan cheerleading and their beliefs about the truth.
Turning first to expressive benefits, individual i’s taste for partisan cheerleading is denoted by the
parameter ci, for cheerleading, which ranges from 0 (no taste for it) to any positive number. Beliefs about
the truth are described by the function pi(rj), which is the probability that i believes response rj, j = 1 or 2,
is correct. In this example, we assume that response r1 portrays Democrats most favorably, that response
r2 portrays Republicans most favorably, and that these assumptions are shared by respondents from both
parties. Specifically, the expressive function e(T, rj) maps an individual’s partisanship T to the personal
benefit of offering response rj, and is defined as e(T = D, r1) = e(T = R, r2) =1 and e(T = D, r2) =
e(T = R, r1) = 0. That is, Democrats and Republicans receive an expressive partisan utility boost from
offering the response that portrays their party in a favorable light, and they receive no partisan utility from
offering the response that is inconsistent with their partisan leanings.
The utility associated with providing a sincere response is measured by the “honesty” function
hi(rj). For simplicity, we assume hi(rj) = pi(rj), i.e., the honesty value of offering response rj is the
probability that the respondent believes it is true. Finally, some respondents may also receive an
incentive, I > 0, which is the additional reward for a correct response. We assume utility is linear in I.
These assumptions allow us to describe a respondent’s expected utility for offering response rj as
the sum of three terms. We omit the individual subscript i for clarity:
9

(1) EU(rj|.) = h(rj) + I × p(rj) + c × e(T,rj).
The first term is simply the honesty value of response rj. The second term is the additional value of
providing response j in the presence of incentive I (realized with the probability that response is correct).
The third term is the partisan value of offering response rj weighted by the respondent’s value of
expressive partisan responding, c. Using the assumption that h() is equivalent to p(), we rewrite (1) as:
(2) EU(rj|.) = (1+I) × p(rj) + c × e(T,rj),
which is the form of the expected utility we focus on here. A respondent will offer the response rj from
(r1,r2) that maximizes (2).
To make the exposition as clear as possible, we suppose that the respondent is a Democrat
(T = D). The analysis for the Republican partisan mirrors that for the Democratic partisan and is omitted.
Recall that r1 is the partisan Democratic response, and so e(D, r1) = 1 and e(D, r2) = 0.
First, consider how our model predicts that partisans will respond to a survey in the absence of
incentives for correct responses. In this case, equation (2) reduces to
(3) EU(rj|.) = p(rj) + c × e(T,rj).
Using (3), the utility from reporting response r1 is p(r1) + c, and the utility from reporting r2 is
p(r2) = 1 – p(r1). Therefore the Democrat will report r1 whenever c ≥ c* = 1– 2p(r1).
As c is weakly positive, whenever p(r1) > .5 (that is, the Democrat believes response r1 is at least
as likely to be correct as r2), the Democrat will offer the partisan response r1 even in the absence of
expressive returns (i.e., even if c = 0). By contrast, as p(r1) grows small (i.e., as the Democrat becomes
increasingly likely to believe the pro-Republican response is correct), larger values of c are required to
cause her to offer r1. To produce a response of r1, the partisan expressive return must be larger to offset
the greater cost of providing an answer that is likely to be untrue.
This relationship is displayed graphically in Panel A of Figure 1, which shows that for each value
of p(r1) there is a value of expressive partisan responding such that, for those Democrats with c at least
this large, r1 will be their survey response. Democrats offering r1 are therefore composed of two groups.
The first group consists of those who believe that r1 is more likely to be correct than r2; this group is
10

represented by the right-hand side of the panel, for which p(r1) > .5. The second group consists of those
who believe that r2 is more likely to be correct, but for whom that belief is offset by a larger return from
offering an expressive partisan response. This group is represented by the upper segment of the left-hand
side of the panel, which is labeled “insincere choice of r1.”
To link expressive returns to polarization of partisan responses, consider Panels B and C. Panel B
shows the response pattern for Republicans, which is a mirror image of Panel A. And Panel C displays
both partisan response patterns at once. It shows that in the presence of expressive returns, Democrats and
Republicans who share common beliefs about the truth (are at the same position on the horizontal axis)
can nonetheless offer polarized survey responses if their value of expressive partisan responding is large
enough. When beliefs about the truth are shared, polarization is most prevalent when beliefs are most
uncertain, i.e., when p(r1) = p(r2) =.5. Polarization will also arise, even in the absence of returns to
expressive partisan responding (i.e., when c = 0), if Democrats and Republicans hold different beliefs
about the truth.
We next consider what happens when incentives are offered for correct responses, i.e., when
I > 0. From equation (2), for a given value of I, there is a unique c*’= (1+I)(1 – 2p(r1)) such that all
Democrats with an expressive responding parameter greater than c*’ will offer r1. As before, incentives
have no effect on the responses of Democrats who believe that response r1 is correct (i.e., p(r1) > .5). But
for Democrats who believe response r2 is more likely to be correct, a larger return to cheerleading is now
required to offset the earnings that are likely to be lost by offering response r1. Formally, c*’ = c* + (I ×
(1 – 2p(r1)). This relationship is shown in Panel A of Figure 2. (For simplicity, we assume throughout
Figure 2 that I = 1.)
Comparison of Panel A in Figure 1 and Panel A in Figure 2 draws out a basic but important
result: incentives for correct responses reduce expressive partisan responding by causing some of those
who know that response r1 is less likely to be true to offer response r2 instead. In Figure 2, these
respondents are represented by the region that is labeled “induced choice of r2.”
Figure 2 draws out a second important result: when a Democrat believes that r2 is more likely to
11

be correct, the additional value of expressive returns (c) that is required to make her offer response r1
increases in her belief that r2 is correct. Formally, c*’ – c* is increasing in p(r2). To see this result
graphically, note that the vertical gap between the dashed and solid lines increases as one approaches the
left side of the x-axis. This gap increases because the difference between c*’ and c* is a function of p(r1).
In other words, for those who are more uncertain (p(r1) is closer to .5), incentives have smaller effects. At
the extreme, an individual who believes that r1 and r2 are equally likely to be true—that is, she knows that
she doesn’t know the truth—continues to offer r1 regardless of incentives for correct responses because
she won’t (in expectation) do better by giving up the certain benefit of a partisan response.
To illustrate the effect of incentives on polarization, Panel B of Figure 2 shows the effect of
incentives for Republican partisans, and Panel C displays both partisan response patterns at once.
Comparison of Panel C in Figure 1 to Panel C in Figure 2 shows that increasing incentives decreases
polarization. In particular, incentives reduce the frequency with which Democrats and Republicans who
share common beliefs about the truth offer different survey responses, apart from the case in which
p(r1) = p(r2) = .5.
This exposition leads us to two conclusions. First, incentives for correct answers reduce partisan
divergence in the presence of shared beliefs about the truth. Second, partisan divergence may persist in
the face of incentives. It is clear that if partisan groups have different sincere beliefs about which response
is most likely to be true, paying respondents for correct responses will not reduce polarization. However,
although it may seem intuitive that persistent partisan divergence in the presence of incentives for correct
responses implies underlying differences in beliefs about the truth, our analysis suggests partisan
divergence may nonetheless persist for two other reasons. First, the taste for expressive partisan
cheerleading (c) may be large. Second, even if that taste is small, individuals may be uncertain about the
truth. In that case, they will offer partisan responses even in the face of large incentives for correct
responding.
We have considered respondents who must provide either a partisan-consistent or a partisaninconsistent response. But giving respondents the option to decline to provide a response may reduce
12

observed polarization. To explore this possibility, we consider a model with an additional response
option: “don’t know.”

Incorporating “Don’t Know” Responses
To incorporate a “don’t know” response option, we must specify the utility that a respondent receives
from selecting “don’t know.” For simplicity, we assume that a “don’t know” response (rdk) yields some
fixed positive psychological benefit Vdk > 0 plus whatever financial incentive is offered for giving that
response (Idk). Specified this way, U(rdk) = Vdk + Idk. One can think of Vdk as the honesty value of
choosing “don’t know” relative to an incorrect response. As before, the individual is offered an incentive I
for providing a correct response.
When will a respondent choose “don’t know”? Note that the value of “don’t know” is unaffected
by c or p(), so a respondent chooses “don’t know” when the values of c and p() make both r1 and r2 less
attractive than “don’t know.” Recall from the previous analysis (illustrated in Panel A of Figure 2) that a
Democrat’s selection of r1 or r2 depends on whether c is greater or less than c*’ = (1+I)(1 –2p(r1)).
Consider first a Democrat who would otherwise choose the “Republican” response, r2. Her
expected utility for choosing this response is (1+I) × (1 – p(r1)). This utility is greater than the utility
associated with selecting “don’t know” when p(r1) < p*(r1) = 1 – (Vdk + Idk) / (1+I). This p*(r1) is the
lowest probability that the Democratic response (r1) is correct for which the Democrat will select “don’t
know” rather than the Republican response. When p(r1) is below this critical value, the Democrat prefers
to report the Republican response. Note that this critical value of p*(r1) is unaffected by the expressive
value of partisan responding c, because the return to r2 is unaffected by c.
Figure 3 illustrates this logic. For presentation, we assume that I = 1, Idk = .75, and Vdk = .5. 7 The
value of p*(r1) is thus 1 – (.5 + .75) / (1 + 1) = .375. Graphically, this solution is represented in Panel A
by the leftmost line that defines the “induced don’t know” region. Substantively, the point is that when
7

We choose a relatively high level of Idk because Figure 3 illustrates the logic of our model when there
are only two survey responses (in addition to “don’t know”). Given only two responses, even complete
uncertainty means that one is, in expectation, correct half of the time. In a model with more response
options, the value of Idk necessary to sustain don’t know responses would be smaller.
13

p(r1) exceeds the critical value p*(r1), all cases in which the Democrat would have offered the Republican
response are replaced by “don’t know” answers.
We next examine how a Democrat who otherwise would have chosen the “Democratic” response,
r1, behaves in the presence of incentives for “don’t know.” We have already shown that if c = c*’, the
Democrat is indifferent between the Democratic and the Republican responses, and that if p(r1) = p*(r1),
she is also indifferent between those responses and “don’t know.” However, as p(r1) rises above p*(r1),
the expected return from choosing the “Democratic” response increases. This means that as the
Democratic response becomes more likely to be true, smaller returns to expressive responding are
required to keep the Democratic response more attractive than “don’t know.” In Panel A of Figure 3, this
condition is illustrated by the downward-sloping line that defines the top of the region labeled “induced
don’t know.” Formally, c = c*’’ = (Vdk + Idk) / (p(r1)(1+I)) is the critical value, such that when c > c*’’
(and c > c*’), the Democrat chooses the Democratic response over “don’t know.”
Parallel analysis for Republicans appears in Panel B of Figure 3. For both Democrats and
Republicans, the subjects who offer “don’t know” responses are drawn from those who are most uncertain
about which answer is correct, i.e., from subjects for whom p(r1) is close to .5. Our analysis above
establishes that it is this uncertainty that makes incentives for correct answers least likely to affect survey
responses. Accordingly, for these uncertain respondents, the “sure thing” of a “don’t know” payment is a
more effective inducement than the smaller probability of earning a potentially larger payment for a
correct response.
Combining these analyses, as we do in Panel C, and comparing that plot to panel C of Figure 2
allows us to assess the effect on observed polarization of offering incentives for both correct and “don’t
know” responses. Relative to simply offering incentives for correct responses, adding incentives for
“don’t know” responses decreases the frequency with which Democrats and Republicans who share
common beliefs about the correct response provide divergent (non-“don’t know”) survey responses.
We now describe the design of our two experiments. The first experiment focuses on differences
in survey responses in the presence and absence of incentives for correct responses. The second
14

experiment also incorporates incentives for “don’t know” responses.

EXPERIMENT 1: THE EFFECT OF FINANCIAL INCENTIVES FOR
CORRECT RESPONSES ON PARTISAN DIVERGENCE
Our first experiment was fielded on the 2008 Cooperative Congressional Election Study, an
Internet survey of U.S. citizens that was conducted by YouGov/Polimetrix in October 2008.
YouGov/Polimetrix uses sampling and matching techniques to generate a sample that approximates the
demographic composition of the adult U.S. population. (See Appendix A for further information about the
construction of the 2008 CCES sample.) Six hundred and twenty-six participants were randomly assigned
to the control group (N = 312) or the treatment group (N = 314). 8 We restrict our analysis to the 419
participants who identified as either Democrats or Republicans. 9
We told control-group subjects that they would be asked questions about politics, that they would
have 20 seconds to answer each question, and that their scores would not be shared with anyone. Treated
subjects received the same instructions and were also told that answering correctly would increase their
chance of winning a prize:
For each question that you answer correctly, your name will be entered in a drawing for a $200
Amazon.com gift certificate. For example, if you answer 10 questions correctly you will be
entered 10 times. The average chance of winning is about 1 in 100, but if you answer many
questions correctly, your chance of winning will be much higher.
After receiving their instructions, all subjects were asked the twelve factual questions shown in
Table 1. The first ten items had closed response options and were similar to questions for which other
research has found partisan differences. No “don’t know” option was offered. Each question included a
reference to a salient partisan issue, e.g., the war in Afghanistan under a Republican president. The last
two “placebo” questions required participants to enter numerical responses and were fielded to ascertain

8

Of these 419 partisans, 81% were white, 7% were black, 8% were Hispanic and 54% were female. Their
mean age was 48 years old, their median level of educational attainment was “some college,” and 67%
were married or in a domestic partnership

9

We defined partisanship as responding either “Democrat” or “Republican” to the first question in the
standard party identification stem question, “Generally speaking, do you think of yourself as a …?” We
present question wording for both experiments in the appendix.
15

whether participants were using their allotted 20 seconds to look up answers using outside references.
These questions demanded knowledge of obscure historical facts. Using these questions, we find little
evidence that participants “cheated”: rates of correct responding in the control and payment conditions
were statistically indistinguishable.10
This experiment allows us to understand whether some partisan divergence in responses to factual
questions arises because of the expressive benefit of providing partisan responses. We can do so by
comparing divergence in the treatment and control conditions. As we discussed earlier, divergence will
decrease only if our incentives are large enough to overcome the expressive value of partisan responding
and if participants have beliefs about the correct responses that are both common across parties and
sufficiently strong. Given the modest size of the incentives that we offered, we view these estimates as
providing a lower bound on the importance of expressive partisan responding in explaining partisan
divergence.
To measure partisan divergence, we create scale scores by coding all responses to range linearly
from 0 to 1. The most Republican responses to each question are coded 0; the most Democratic responses
are coded 1. If partisans are answering in a manner consistent with their partisanship, Democrats should
offer “larger” responses than Republicans.
Table 1 shows the average partisan difference, by question, for those in the control group.
Questions were asked in fixed order, but in Table 1 they are ordered by the size of the partisan gap
observed in the control group, with the two placebo questions at the bottom. For nine of the ten nonplacebo questions, we find positive partisan gaps that are consistent with our expectations about patterns
of partisan responding. 11 Eight of those differences are significant at p < .10 (one-tailed). The gaps vary
substantially in size, with the largest gaps for the questions about changes in causalities in Iraq between

10

Correct-response rates in the control and treatment groups were 3% and 3% (Bangladesh) and 1% and
1% (price of gold), respectively. We also fielded an open-ended question about the offices held by George
von L. Meyer. No participant answered this question correctly.

11

The exception is the question about the change in the deficit under George W. Bush. There was almost
no partisan divergence in responses to this question: Fully 88% of Democrats and 90% of Republicans
correctly reported that the deficit increased under Bush.
16

2007 and 2008 and Bush’s economic performance. For the placebo question about the price of gold we
find an unexpected partisan gap, whereas for the question about Bangladesh’s date of independence there
is no gap. Those placebo questions are not included in our remaining analyses.
What effect do incentives for correct responses have on observed partisan divergence? To
measure these effects, we estimate a model in which we predict differences in scale score R for individual
i and question j:
Rij = b0 + b1Democrati + b2PayCorrecti + b3(Democrati × PayCorrecti) + Questionj + ei,
where Democrat takes on the value 1 for Democratic participants and 0 for Republicans, PayCorrect = 1
for those assigned to the incentive condition, and Question is a vector of question-specific fixed effects. b1
is therefore the average party difference in mean scale scores in the control condition, while b1 + b3 is the
average party difference in the incentive condition. Prior research suggests b1 > 0, while the theoretical
model introduced above predicts that b3 will be negative if partisans offer partisan-tinged responses in the
absence of incentives, share common and sufficiently strong beliefs about the truth, and give less weight
to partisan responding than to the expected value of the incentive. OLS estimates, with standard errors
clustered at the respondent level, appear in Table 2.
Pooling across the eight non-placebo questions for which we observe statistically significant
partisan gaps in the control condition, column (1) provides estimates of the average effect of incentives on
responses. 12 The .118 (p < .001) coefficient for Democrat (b1) is the average gap between Democrats and
Republicans in the control condition. The –.065 (p < .001) coefficient for Democrat × PayCorrect (b3)
means that this gap is reduced to .053 (.118 – .065), or by 55%, when incentives are offered. In column
(2), we add demographic controls; the results are nearly unchanged.
These results show that even modest incentives can substantially reduce partisan divergence in
factual assessments. In this experiment, participants were told that answering correctly would improve
12

This analysis excludes cases in which participants didn’t provide a response, which occurs 3% of the
time in both treatment and control conditions. Replacing those responses with party averages for that
question produces substantively similar results. Analysis is available upon request. In Table A1 of the
appendix, we repeat these analyses for each questions individually. In all eight cases, the estimate for b3 is
negative, although it is not usually statistically significant.
17

their chances of earning a $200 gift certificate, and that the baseline chance of winning was around 1 out
of 100. Assuming that participants estimate that answering all questions correctly would double their
chances of winning this prize, the expected value of answering any given question correctly is $0.167. 13
In turn, the finding that incentives reduced partisan gaps by more than 50% implies that more than half of
the party gap may be generated by participants for whom partisan responding is worth less than $0.17.
Of course, we cannot ascertain why the remaining proportion (about 45%) of the partisan gap
remains. Following our model, the individuals responsible for this gap may value partisan cheerleading
more highly (high values of expressiveness, c), disagree about which response is correct (difference in
p(rj) across parties), or be sufficiently uncertain about which response is correct that they cannot improve
their chances of earning the incentive by deviating from their partisan response (uncertainty). To
understand the role of awareness of one’s own lack of knowledge, we now turn to our second experiment.

EXPERIMENT 2: THE EFFECT OF FINANCIAL INCENTIVES FOR
CORRECT AND “DON’T KNOW” RESPONSES ON PARTISAN
DIVERGENCE
We fielded our second experiment in 2012 with subjects whom we recruited from Amazon.com’s
Mechanical Turk marketplace. 14 Subjects were required to pass a two-question attention screener and
were then randomly assigned to a control group (N = 156) or to one of three treatment groups, two of
which we examine here. 15 In the first treatment group, participants were paid for correct responses
(N = 534). In the second treatment group, participants were paid for both correct and “don’t know”
responses (N = 660). Below, we restrict our analysis to the 795 individuals in these three groups who

13

(1/100 × $200)/12 questions = $0.167 per question.

14

We recruited 1,506 participants for the MTurk study over the web from March 29, 2012 to April 16,
2012. See Appendix A for details. Because MTurk samples tend be more Democratic than the general
population, we invited equal numbers of Democrats and Republicans who had previously taken our
surveys to participate in this study. Of the 795 partisans analyzed, age ranged from 19 to 75 with a mean
of 33, 54 percent were female, and 46 percent had at least a four-year college degree.
15

In a third treatment, we paid participants a flat fee to answer questions post-treatment, the same
incentive provided to the control group. However, we also allowed respondents to express “don’t know”
answers.
18

identified as either Democrats or Republicans. 16
There are three major differences between this experiment and Experiment 1. First, and of
greatest importance theoretically, we introduce a new treatment here, in which we offer subjects the
opportunity to select “don’t know” and incentives for both correct and “don’t know” responses. Doing so
allows us to estimate the degree to which partisan responding arises because participants are unaware 17 of
correct responses, but are aware of their lack of knowledge. Therefore, unlike Experiment 1, Experiment
2 permits us to assess the extent to which partisan divergence that persists in the face of incentives for
correct responses reflects knowing ignorance, rather than partisan cheerleading (as driven by large
expressive returns) or sincere differences in beliefs.
Second, in both treatment conditions, we randomly vary the amount offered for correct responses.
In the treatment that includes payment for “don’t know” responses, we also vary the amount offered for
that response. These randomizations allow us to assess the degree to which partisan divergence is affected
by the size of incentives.
Third, for all of the questions asked in this experiment, we used a novel graphical input device to
measure participants’ attitudes. Figure 4 displays an example of the “slider” that we used to gather
answers to each of the questions we asked. After we trained participants to use this interface (complete
instructions appear in Appendix B), we asked them to respond to each question by manipulating the
slider. As before, we gave subjects 20 seconds to answer each question to limit opportunities for
consulting outside information sources. Additionally, in the conditions in which participants were paid for
correct responses, subjects were informed that a response would be scored as correct if the slider
overlapped the correct answer. The primary advantage of this input device is that it allows individuals to
16

As in our first experiment, we defined partisanship as responding either “Democrat” or “Republican” to
the first question in the standard party identification stem question, “Generally speaking, do you usually
think of yourself as a Democrat, a Republican, an Independent, or what?” Of the 795 partisans in our
sample, 65% were Democrats, 89 were assigned to the control group, 327 to the pay-for-correct-response
group, and 379 to the pay-for-correct-and-“don’t know” group.

17

We use “unaware” loosely: “unaware” subjects include those who believe that one response option is
most likely to be true, but for whom there is only a small difference in assessments of which answer is
true (i.e., |p(r1) – p(r2)| is small).
19

provide responses continuously across the entire range of possible responses instead of requiring
categorical responses.
In all conditions, participants were initially asked five questions that were selected at random
from a larger list that we describe below. All questions had a closed response format and we did not
present a “don’t know” response option. Their treatment was then introduced, and they were then asked
seven more questions: two new questions followed by the same five questions that they had previously
been asked. (See Appendix B for the text that we used to introduce each treatment.) In the control
condition, participants were paid a flat $0.50 bonus to answer those seven post-treatment questions. In the
pay-for-correct condition, participants were informed that they would be paid for each correct response,
and the amount offered for each correct response was randomly assigned to be $0.10 with probability .25,
$0.25 with probability .25, $0.50 with probability .25, $0.75 with probability .15, and $1.00 with
probability .10.
Finally, in the pay-for-correct-and-“don’t know” condition, participants were again informed they
would be paid for each correct response, and the amount offered for each correct response was assigned
as in the prior treatment. Participants in this treatment were also given “don’t know” response options and
told that they would be paid for selecting “don’t know.” The amount paid for “don’t know” responses was
also assigned randomly, and was a fraction of the amount offered for a correct response: 20% of the
payment for a correct response with probability .33, 25% with probability .33, and 33% with
probability .33.
We list the 12 questions that we fielded in this experiment in Table 3, which also shows the
correct response and the range of the response options (i.e., the lower and upper bound of the labeled
horizontal line) that we offered to participants. These questions were chosen so that correct responses
varied across the entire range of potential answers: they were not concentrated at either end of the scale or
in the middle. The direction of partisan responding also varied: sometimes, higher responses favored the
Democratic Party; sometimes, they favored the Republican Party. As before, we used a placebo question

20

designed to assess whether or not participants were consulting outside references. 18
As with Experiment 1, we recoded all responses to range from 0 to 1, with 1 indicating the most
Democratic response. The right-hand column of Table 3 reports, for each question, the observed pretreatment difference in mean scale scores between Democrat and Republican participants. (Recall that
each participant was asked five pre-treatment questions.) We find statistically significant (p < .10, onetailed) partisan gaps for ten of these questions, with the largest gaps for questions about unemployment
under Bush and Obama, and the smallest gaps for a question about the proportion of the population that is
foreign-born and for the placebo question about baseball. Our subsequent analysis is restricted to the ten
non-placebo questions for which there are statistically significant pre-treatment partisan gaps.

The Effect of Incentives for Correct and “Don’t Know” Responses
We begin by reporting the effect of the treatments on the frequency of selecting “don’t know.” Our model
suggests that the rate at which participants select “don’t know” when offered a payment for doing so
indicates the degree to which they understand that they don’t know the correct response to these
questions. In particular, if beliefs about correct responses are sufficiently weak (i.e., uninformative about
the truth) and preferences for expressive partisan responding are not too large, then choosing “don’t
know” when paid to do so will yield greater expected utility than either expressive or sincere responses.
Pooling across the non-placebo questions for which we found pre-treatment partisan gaps, we
find that 47.8% of responses in the payment-for-correct-and-“don’t know” condition are “don’t know.” 19
That is, nearly half of participants forgo a response that would allow them to support their party or give
them a chance to earn a larger monetary incentive for a correct response.
Recall that for “don’t know” responses, participants were randomly assigned to receive 20%,
25%, or 33% of the payment that they received for correct responses. Across these conditions, the
frequency of “don’t know” is 46%, 47%, and 50%, respectively. These differences are ordered as the
18

See Appendix A for an analysis of reported efforts to look up correct responses using outside sources.

19

By contrast, in the other treatment that we fielded, in which participants were paid a flat fee for their
responses and also offered a “don’t know” option (see note 15), only 14.8% of responses were “don’t
know.”
21

theoretical model predicts, but only the difference in means between the 20% and 33% conditions
approaches statistical significance (p < .07, one-tailed).
This pattern—frequent “don’t know” responses when subjects are paid to say “don’t know,” even
though they are also offered more for correct responses—implies that participants are sufficiently
uncertain about the truth that they expect to earn more by selecting “don’t know.” For example, given that
46% of participants selected “don’t know” when paid 20% of the correct-answer payment for “don’t
know” responses, and given that the slider covers 20% of the response space, we infer that participants’
beliefs about the probability that a response is correct are nearly equal across responses. By contrast, if
participants were confident that even 20% of the range of the scale did not include the correct answer,
they would do better in expectation by simply guessing randomly from among the remaining 80% of the
scale range. 20
This great willingness to select “don’t know” has important implications for our understanding of
partisan divergence. In particular, participants who offer “don’t know” responses behave in a manner that
is consistent with this hypothesis: they know that their responses are otherwise partisan and that they
don’t know the truth. In the absence of incentives for “don’t know” responses, they would offer insincere
partisan responses, even if paid for correct ones, because they are both uninformed about the truth and
aware of their ignorance. Overall, absent sufficiently strong beliefs about what the truth is, they cannot
expect to earn more if paid only for correct responses.
To incorporate “don’t know” responses into our analysis of partisan divergence, we must decide
where to place those responses on the 0-1 scale that we use to analyze other responses. Because
participants who admit that they don’t know thereby forgo the opportunity to express support for their
party, we treat these responses as being non-polarized. That is, we assign both Democrats and
Republicans who choose “don’t know” to the same position on the 0-1 scale. In particular, we assign

20

This is so because the slider can cover up to 1/6th of the response range, so random guessing from the
entire range would be correct about 16.7% of the time. If 20% of the scale range is eliminated, randomly
guessing from the remaining 80% of the scale would be correct about 20.8% of the time. This calculation
assumes risk neutrality.
22

“don’t know” responses for a given question to the average pre-treatment response that participants
offered to that question. In practice, the particular value makes little difference to our analyses; the
important point is that Democrats and Republicans are assigned to the same position on the scale if they
say “don’t know.” One implication of this choice is that if all partisans chose “don’t know,” we would
find no divergences between the parties.
As in the previous section, we study the effect of the treatments on party polarization by
examining whether post-treatment partisan gaps differ between the control and treatment conditions. 21
Our analysis initially takes the following form:
Rij = b0 + b1PayCorrecti + b2PayCorrectDKi + b3(Controli × Democrati) + b4(PayCorrecti ×
Democrati) + b5(PayCorrectDKi × Democrati) + Questionj + ei,
where Democrat = 1 for Democratic participants and 0 for Republicans, Control = 1 for those assigned to
the control condition, PayCorrect = 1 for those assigned to the pay-for-correct-response condition,
PayCorrectDK = 1 for those assigned to the pay-for-correct-and-“don’t know” condition, and Question is
a vector of question-specific fixed effects. In this specification, b3, b4, and b5, are the amount of partisan
divergence in the control condition and the two treatment conditions. (This specification does not exploit,
within treatment, variation in the amounts that we paid for correct and “don’t know” responses.) Our
expectation is that b3 > b4 and b3 > b5, that is that each treatment will reduce partisan divergence relative to
the control (flat fee) condition. Additionally, our theoretical model suggests that some partisans who will
not respond to incentives for correct responses will nonetheless respond to incentives for “don’t know”
responses. For this reason, we also predict b4 > b5.
The first column of Table 4 reports OLS estimates of the equation. (Parallel analysis for each
individual question appears in Table A2 of Appendix A.) The estimate of b3 is .145 (p < .01), which
means that, on average, Democrats and Republicans in the control condition differ in their answers by
about 15% of the range of the scale. The estimate of b4 is only .058, and the difference between these two
21

One alternative would be to compare pre-treatment responses to post-treatment responses in a given
treatment condition. That analysis, however, would conflate the effect of the treatment with the effect of
answering some questions a second time.
23

estimates is significant at p < .01. (This test statistic is reported in the second-to-last row of the table.) The
difference indicates that only 40% of the previously observed party gap remains when participants are
paid small amounts for correct responses. Despite the differences in subject pools, questions, and details
of the experiment, this effect is similar in size to what we find in our analysis of Experiment 1. And, like
Experiment 1, this result shows that analysis of survey responses as sincere expressions of factual beliefs
likely overstates true partisan polarization.
This experiment also allows us to estimate the effect on apparent partisan polarization of
incentives for “don’t know” responses. The estimate of b5 is .028, or about 80% smaller than the
corresponding control-condition estimate and about 50% smaller than the pay-only-for-correct-response
estimate. Both differences are significant at p < .05. In practical terms, this means that whereas the
baseline (control-group difference) between Democrats and Republicans was about 15% of the scale
range, it shrinks to 3% of the range when we offer incentives for both correct and “don’t know”
responses. To give a more concrete understanding of the importance of these differences, in the case of
the question about change in unemployment under Bush, the response scale ranged from –2% to 4%, and
the estimates imply that control-group Democrats offered a response that was about .9 percentage points
higher than control-group Republicans. Among those paid for a correct response, these estimates suggest
that gap was only .4 percentage points, and among those paid for both a correct and don’t know response,
it was only .2 percentage points.
From the perspective of voters trying to evaluate a president, an annual change in the
unemployment rate of .2 points or greater happens more than 80 percent of the time, and a change of .4
points or greater happens 65 percent of the time. By contrast, a change of .9 points or greater happens
only about 25 percent of the time. 22 Partisan gaps in survey response in the presence of incentives are
therefore far more appropriately calibrated to year-to-year variation than the gaps suggested by
unincentivized responses.
22

Annual change in the unemployment rate from 1970 to 2012 retrieved from the BLS at
http://data.bls.gov/timeseries/LNU04000000?years_option=all_years&periods_option=specific_periods&
periods=Annual+Data.
24

We consider the robustness of these results in columns (2) through (4). In column (2) we estimate
a Tobit specification, which allows us to account for the fact that our response scales were finite and
therefore unable to accommodate extreme responses. The Tobit estimates are similar to those shown in
column (1) and indications of statistical significance do not change. In columns (3) and (4) we examine
different subsets of the questions that each respondent answered. Column 3 reports results only for those
questions that respondents had also been asked at the pre-treatment stage. Column 4 reports results only
for those questions that respondents had not been asked at the pre-treatment stage. Dividing the analysis
in this way reduces the size of the sample in each column, but the pattern of effects remains: The partisan
gap is largest in the control condition, around 60% smaller in the pay-for-correct-response condition, and
around 80% smaller in the pay-for-correct-and-“don’t know” condition. Indications of statistical
significance are marginal in the column (4) specification, which is restricted to only two questions per
respondent.
We can also leverage the variation in the incentive amounts provided to assess more fully the
effect of differences in correct and “don’t know” payments on observed divergence. A specification that
exploits this variation appears in column (5). It includes indicators for each level of payment. These
indicators appear separately and are interacted with partisanship. The resulting specification is highly
flexible because it does not make any assumptions about the functional form of the effects of changes in
payments.
The estimated .145 (p < .05) coefficient for Democrat is the average difference between
Democrats and Republicans in the control condition. As expected, of the five interactions between
amount paid for a correct response and Democrat, all are negative and statistically significant at p < .10,
which means that party gaps are smaller when participants are offered incentives for correct responses.
With one exception, they are also ordered in decreasing fashion. That is, larger payments are associated
with smaller partisan gaps relative to the control group. The exception is the $0.75 payment for a correct
response, which is associated with a decrease in the partisan gap of .06 (p < .10), smaller than the .08

25

decrease associated with a $0.10 payment for a correct response.23 Focusing just on the comparison of the
effect of the $1.00 and $.10 payment and acknowledging concerns about multiple comparisons, we
estimate that partisan gaps are 56% smaller in the $0.10 payment condition than in the control group and
80% smaller in the $1.00 payment condition. The difference between the two coefficients (Amount
correct = $0.10 × Democrat and Amount correct = 1.00 × Democrat) is marginally significant (p < .10,
one-tailed test).
We can also assess the effects of variation in the amount paid for “don’t know” responses. All of
the interactions between the fractional payment amounts and partisanship are in the expected negative
direction, meaning that adding incentives for “don’t know” reduces partisan gaps. For payments that are
20% or 33% as large as the payments for correct responses, the estimates are statistically significant at
p < .10 (two-tailed), and the pooled estimate of the effect of “don’t know” payments is significant at
p < .05. To interpret these coefficients, one can fix the payment for a correct response at $0.10, in which
case the estimated partisan gap is .063 (.145 – .082, p < .01). Adding the “don’t know” payment is
estimated to reduce this party gap by between .02 (a 25% reduction for a “don’t know” payment of
$0.025) and .04 (a 65% reduction for a payment of $0.033).
The ordering of the effects for the proportional payments is mixed. The largest reduction in
partisan divergence is associated with the 33% payment for “don’t know” responses, the next-largest
reduction is associated with the 20% payment, and the smallest reduction is associated with the 25%
payment. None of these coefficients are statistically distinguishable from one another, perhaps reflecting
the relatively small sample sizes in each condition. 24 At the same time, the point estimates imply that the
combination of a $1.00 payment for a correct response and a $0.33 payment for a “don’t know” response
will eliminate the entire partisan gap between Democrats and Republicans in responses to partisan factual

23

We note that this is one of two treatment conditions, the other being the $1.00 payment, which we
undersampled for reasons of cost. There are 574 respondent-answers in the $0.75 payment condition,
compared to 1000 on average in the $0.10, $0.25, and $0.50 payment conditions.

24

For example, among those 1130 cases assigned to the $0.50 payment for a correct response, there are
571 [50%] in which no payment was offered for “don’t know.” The remaining cases were split almost
equally across the three levels of proportional “don’t know” payments.
26

questions. 25
Taken as a whole, these results have two implications. First, as in Experiment 1, modest
incentives substantially reduce partisan gaps, which is consistent with some portion of these gaps being
due to expressive responding rather than to sincere differences in beliefs. Second, at least half of the
partisan divergence that remains even in the presence of incentives for correct responses arises because
people know that they do not know the correct response. On average, payments for correct responses in
this experiment reduce partisan gaps by 60%. Adding “don’t know” payments reduces partisan gaps by an
additional 20 percentage points, leaving only 20% of the original gap. This implies that fully half of the
remaining gap arose because participants were unaware of the correct response and understood their lack
of knowledge. Indeed, the relatively high rate of selecting “don’t know” (about 48% across “don’t know”
payment rates) suggests a great deal of self-aware lack of knowledge about the world. When offered a
payment both for a correct response and a “don’t know” response, nearly half of participants chose “don’t
know.” In doing so, they gave up both the chance to engage in expressive partisan responding and the
opportunity to earn a larger payment by offering a correct response.

DISCUSSION AND IMPLICATIONS
Our results have important implications for both political science and understandings of contemporary
public opinion. Regarding the former, persistent partisan gaps, if sincere, suggest important limitations to
democratic accountability. If Democrats and Republicans perceive different realities, then the incentives
for incumbent politicians to pursue policies that generate objectively good policies may be reduced. Our
results imply that such concerns are overstated. Democrats and Republicans may diverge in their survey
reports of facts, but such responses should not be taken at face value as sincere expressions of partisan
worldviews.
To make the magnitude of this concern clear, we use Experiment 1 to assess the correlation
between survey assessments of factual matters and reported candidate preference. By comparing the
25

This calculation is .145 – .116 – .041, which is actually slightly smaller than 0.
27

correlations in the control and treatment conditions, we can understand whether the use of cross-sectional
survey measures to predict vote choice is biased when those measures are affected by partisan
cheerleading. In particular, we estimate
PresVotei = b0 + b1FactualAssessmentsi + b2PayCorrecti + b3(PayCorrecti × FactualAssessmentsi) + ei,
where PresVote = 1 indicates an intended vote for Obama and PresVote = 0 indicates an intended vote for
McCain. (We exclude from the analyses those who aren’t registered, prefer other candidates, or report
that they won’t vote.) FactualAssessments is a mean scale created from the eight items included in our
earlier analysis of the experiment, with each item coded so that 1 is the most Democratic response and 0
is the most Republican response. PayCorrect is an indicator for assignment to the pay-for-correctresponse condition. Existing research suggest b1 > 0, that is, more Democratic assessments are associated
with voting for the Democratic candidate. If those factual assessments are affected by partisan-consistent
cheerleading when incentives are not offered, then that correlation should be reduced in the treatment
condition, implying b3 < 0.
We present OLS estimates with clustered standard errors in Table 5.26 Per these estimates, a onestandard-deviation increase (.124) in the factual assessments scale is associated with a 22-percentagepoint increase in the likelihood of voting for Obama (p < .01). Among those assigned to the treatment
group, however, the negative estimate for b3 means that this effect is substantially reduced (p < .05). In
particular, the same shift in the assessments scale now increases the probability of voting for Obama by
only 13 percentage points, a decrease of more than 40% in the effect of those assessments on voting. This
means that the observed correlation between normal (unincentivized) survey reports of factual
assessments and voting is exaggerated by the partisan “contamination” of those responses. Analysts
should therefore craft research designs that allow them to distinguish the true relationship between sincere
factual assessments and political choices from the apparent relationship that exists when factual
assessments are affected by partisan expressive responding. Failure to do so likely biases inference about

26

In this sample, the mean FactualAssessments score is .59 and 50% of the sample prefers Obama. Probit
results are substantively similar.
28

the effect of the “perceptual screen” on retrospective voting.
Extending beyond political science, our results also inform understandings of contemporary
public opinion. Scholarly and popular analysts alike frequently take survey responses at face value,
assuming that what individuals choose in a survey context reflects their true underlying beliefs. While this
assumption has been called into question for sensitive topics, our results suggest that the concern should
be far more widespread. Indeed, in light of this concern, ongoing efforts to assess the dynamics of public
opinion must grapple with the possibility that changes in partisan responses to many questions may not
reflect changes in factual beliefs but changes in the degree to which different responses are understood as
conveying support for one’s party. (They may also reflect changes in the social returns to partisan
cheerleading: see Iyengar, Sood, and Lelkes 2012). Further, if our results about factual questions extend
to responses to non-factual survey items, our findings have important implications for partisan divergence
in attitudinal self-reports.

CONCLUSION
A common feature of American politics is the existence of differences between Democrats and
Republicans in survey assessments of factual beliefs. How should those differences be interpreted? One
view is that they represent the stark reality of partisan bias, in which Democrats and Republicans perceive
different realities. Another possibility, highlighted in this paper, is that differences in survey responses
arise because surveys offer partisans low-cost opportunities to express their partisan affinities.
To highlight the distinction between sincere beliefs about the truth and survey responses, we have
presented a model of survey response that incorporates the possibility of expressive partisan responding.
The model shows that incentives for correct responses can be used to distinguish sincere from insincere
partisan responding. It also shows that incentives will fail to reduce partisan responding if respondents
understand that they are unaware of the truth. However, by providing incentives for both correct and
“don’t know” responses, one can estimate the proportion of partisan responding that arises because of
either informed or uninformed partisan cheerleading.

29

Building from this model, we designed and fielded two novel experiments. In the first
experiment, some participants were paid for correct responses to factual questions. The payments reduced
observed partisan gaps by about 55%. In the second experiment, we also paid some participants for “don’t
know” responses. In this experiment, incentives for correct responses reduced partisan gaps by 60%, and
incentives for “don’t know” did so by an additional 20%, yielding partisan gaps that were 80% smaller
than those that we observed in the absence of incentives. Taken together, these results provide a lowerbound estimate on the proportion of partisan divergence that arises because of the combination of
expressive partisan returns and self-aware ignorance of the truth. Extending our analysis, we found that
paying people for correct responses sharply reduces the power of factual assessments to predict vote
choice.
Our work also highlights areas for subsequent research. The imprecision of our estimates about
the effects of increasing incentives, for example, suggests the value of conducting additional experiments
with larger samples. The apparent increasing effect of those incentives also implies that it would be
desirable to ascertain whether even larger incentives can further reduce apparent bias by overcoming the
tendency for individuals to engage in expressive partisan cheerleading. There is also the question of
whether individual-level factors can explain which individuals are more likely to engage in expressive
cheerleading, and which individuals are most responsive to these financial inducements. 27
These questions aside, the core of our paper is the exposition of a model of expressive survey
response and the implementation of a pair of experiments designed to distinguish that cheerleading
behavior from sincere partisan divergence. We find that small financial inducements for correct responses
can substantially reduce partisan divergence, and that these reductions are even larger when inducements
are also provided for “don’t know” answers. In light of these results, survey responses that indicate
partisan polarization with respect to factual matters should not be taken at face value. Researchers and
general analysts of public opinion should consider the possibility that the appearance of polarization is to
a great extent an artifact of survey measurement rather than evidence of real differences in beliefs.
27

See Appendix C for a preliminary discussion of accuracy.
30

REFERENCES
Ansolabehere, Stephen, Marc Meredith, and Erik Snowberg. 2013. “Asking about Numbers:
Why and How.” Political Analysis 21 (January): 48-69.
Bartels, Larry M. 2002. “Beyond the Running Tally: Partisan Bias in Political Perceptions.”
Political Behavior 24 (June): 117-50.
Berinsky, Adam J. 2005. Silent Voices: Public Opinion and Participation in America. Princeton,
NJ: Princeton University Press.
Berinsky, Adam J. 2012. “Rumors, Truths, and Reality: A Study of Political Misinformation.”
Massachusetts Institute of Technology. Manuscript.
Campbell, Angus, Philip E. Converse, Warren E. Miller, and Donald E. Stokes. 1960. The
American Voter. Chicago: University of Chicago Press.
Conover, Pamela J., Stanley Feldman, and Kathleen Knight. 1986. “Judging Inﬂation and
Unemployment: The Origins of Retrospective Evaluations.” Journal of Politics 48 (3): 565-88.
Conover, Pamela J., Stanley Feldman, and Kathleen Knight. 1987. “The Personal and Political
Underpinnings of Economic Forecasts.” American Journal of Political Science 31 (August): 559583.
Duelfer, Charles. 2004. “Comprehensive Report of the Special Advisor to the Director of Central
Intelligence on Iraq’s Weapons of Mass Destruction.”
http://www.gpoaccess.gov/duelfer/index.html (accessed October 1, 2009).

31

Fiorina, Morris P. 1981. Retrospective Voting in American National Elections. New Haven, CT:
Yale University Press.
Gaines, Brian J., James H. Kuklinski, Paul J. Quirk, Buddy Peyton, and Jay Verkuilen. 2007.
“Same Facts, Different Interpretations: Partisan Motivation and Opinion on Iraq.” Journal of
Politics 69 (November): 957-74.
Gerber, Alan S., and Gregory A. Huber. 2010. “Partisanship, Political Control, and Economic
Assessments.” American Journal of Political Science 54 (January): 153-73.
Gerber, Alan S., Gregory A. Huber, Ebonya Washington. 2010. “Party Affiliation, Partisanship,
and Political Beliefs: A Field Experiment.” American Political Science Review 104 (4
November): 720-744.
Green, Donald, Bradley Palmquist, and Eric Schickler. 2002. Partisan Hearts and Minds:
Political Parties and the Social Identities of Voters. New Haven, CT: Yale University Press.
Harris Interactive. 2006 July 21. “Belief that Iraq Had Weapons of Mass Destruction Has
Increased Substantially.” http://news.
harrisinteractive.com/proﬁles/investor/ResLibraryView.asp?
ResLibraryID=34134&GoTopage=4&Category=1777&BzID=1963&t=8.
Harris Interactive. 2010 March 24. “‘Wingnuts’ and President Obama.”
http://news.harrisinteractive.com/proles/investor/ResLibraryView.asp?ResLibraryID=37050&C
ategory=1777&BzID=1963.
Healy, Andrew, and Neil Malhotra. 2009. “Myopic Voters and Natural Disaster Policy.”

32

American Political Science Review 103 (3): 387-406.
Iyengar, Shanto, Gaurav Sood, and Yphtach Lelkes. 2012. “Affect, Not Ideology: A Social
Identity Perspective on Polarization.” Public Opinion Quarterly 76 (Fall): 405-31.
Jacobson, Gary C. 2006. A Divider, Not a Uniter: George W. Bush and the American People.
Upper Saddle River, NJ: Pearson.
Jerit, Jennifer, and Jason Barabas. 2012. “Partisan Perceptual Bias and the Information
Environment.” Journal of Politics 74: 672-684.
Kuklinski, James H., Michael D. Cobb, and Martin Gilens. 1997. “Racial Attitudes and the ‘New
South’.” Journal of Politics 59 (May): 323-49.
Lau, Richard R., David O. Sears, and Tom Jessor. 1990. “Fact or Artifact Revisited: Survey
Instrument Effects and Pocketbook Politics.” Political Behavior 12 (3): 217-242.
Lavine, Howard G., Christopher D. Johnston, and Marco R. Steenbergen. 2012. The Ambivalent
Partisan: How Critical Loyalty Promotes Democracy. Oxford University Press.
Morton, Rebecca C., and Kenneth C. Williams. 2010. Experimental Political Science and the
Study of Causality: From Nature to the Lab. New York: Cambridge University Press.
Noelle-Neumann, Elisabeth. 1993. The Spiral of Silence: Public Opinion, Our Social Skin.
Chicago: University of Chicago Press.
Palmer, Harvey D., and Raymond M. Duch. 2001. “Do Surveys Provide Representative or
Whimsical Assessments of the Economy?” Political Analysis 9 (1): 58-77.
33

Prior, Markus. 2007. “Is Partisan Bias in Perceptions of Objective Conditions Real? The Effect
of an Accuracy Incentive on the Stated Beliefs of Partisans.” Presented at the Annual Conference
of the Midwest Political Science Association, Chicago.
Prior, Markus, and Arthur Lupia. 2008. “Money, Time, and Political Knowledge: Distinguishing
Quick Recall and Political Learning Skills.” American Journal of Political Science 52 (January):
169-83.
Sears, David O., and Richard R. Lau. 1983. “Inducing Apparently Self-Interested Political
Preferences.” American Journal of Political Science 27 (May): 223-252.
Shapiro, Robert Y., and Yaeli Bloch-Elkon. 2008. “Do the Facts Speak for Themselves? Partisan
Disagreement as a Challenge to Democratic Competence.” Critical Review 20 (1): 115-39.
Stroud, Natalie J. 2008. “Media Use and Political Predispositions: Revisiting the Concept of
Selective Exposure.” Political Behavior 30 (3): 341-366.
Wilcox, Nathaniel, and Christopher Wlezien. 1993. “The Contamination of Responses to Survey
Items: Economic Perceptions and Political Judgments.” Political Analysis 5 (1): 181-213.

34

Figure 1: Patterns of Survey Response in the Absence of Incentives
by Value of Expressive Partisan Responding and Beliefs about Correct Responses
B) Republicans’ Survey Responses

2

2

1.8

1.8

c, Value of Expressive Partisan Responding

c, Value of Expressive Partisan Responding

A) Democrats’ Survey Responses

1.6
1.4
1.2

Insincere
(expressive)
choice of r1

1

Sincere
choice of r1

0.8
0.6
0.4
Sincere
choice of r2

0.2
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.6
1.4
1.2
1
0.8
0.6
0.4
Sincere
choice of r1

0.2
0

1

Insincere
(expressive)
choice of r2

Sincere
choice of r2

0

p(r1), Belief Democratic-Expressive Response r1 is Correct

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

p(r1), Belief Democratic-Expressive Response r1 is Correct

C) Observed Polarization
2

c, Value of Expressive Partisan Responding

1.8
1.6
1.4

Polarization region
(Democrats choose r1,
Republicans choose r2)

1.2
1
0.8
0.6
0.4

0

Democrats and
Republicans
choose r1

Democrats and
Republicans
choose r2

0.2

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

p(r1), Belief Democratic-Expressive Response r1 is Correct

Note: Panel A displays Democrats’ survey responses in the absence of incentives for different levels of returns to expressive
partisan responding and beliefs about whether response r1 is correct. Panel B displays responses for the same parameters for
Republicans. Finally, the grey area in panel C is the range of parameters for which Democrats and Republicans offer different
survey responses despite common beliefs about which response is correct.

1

Figure 2: Patterns of Survey Response Given Incentives for Correct Responses (I=1)
by Value of Expressive Partisan Responding and Beliefs about Correct Responses
B) Republicans’ Survey Responses

2

2

1.8

1.8

1.6

c, Value of Expressive Partisan Responding

c, Value of Expressive Partisan Responding

A) Democrats’ Survey Responses

Insincere
(expressive)
choice of r1

1.4
1.2

Induced
choice of r2

1

Sincere
choice of r1

0.8
0.6
0.4
Sincere
choice of r2

0.2
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.6
1.4
1.2

Sincere
choice of r2

1

Induced
choice of r1

0.8
0.6
0.4
Sincere
choice of r1

0.2
0

1

Insincere
(expressive)
choice of r2

0

p(r1), Belief Democratic-Expressive Response r1 is Correct

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

p(r1), Belief Democratic-Expressive Response r1 is Correct

C) Observed Polarization
2

c, Value of Expressive Partisan Responding

1.8
1.6
1.4

Polarization region
(Democrats choose r1,
Republicans choose r2)

1.2
1
0.8
0.6
0.4

0

Democrats and
Republicans
choose r1

Democrats and
Republicans
choose r2

0.2

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

p(r1), Belief Democratic-Expressive Response r1 is Correct

Note: Panel A displays Democrats’ survey responses given incentives I=1 for correct responses for different levels of returns to
expressive partisan responding and beliefs about whether response r1 is correct. Panel B displays responses for the same parameters for Republicans. Finally, the grey area in panel C is the range of parameters for which Democrats and Republicans offer
different survey responses despite common beliefs about which response is correct.

1

Figure 3: Patterns of Survey Response Given Incentives for Correct (I=1) and Don’t Know (Idk=.75) Responses
by Value of Expressive Partisan Responding and Beliefs about Correct Responses
B) Republicans’ Survey Responses

2

2

1.8

1.8

1.6

c, Value of Expressive Partisan Responding

c, Value of Expressive Partisan Responding

A) Democrats’ Survey Responses

Insincere
(expressive)
choice of r1

1.4
1.2

Induced
choice of r2

1

Sincere
choice of r1

0.8
0.6
0.4
Sincere
choice of r2

0.2
0

0

0.1

0.2

Induced
don’t know
0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.6
1.4
1.2

Sincere
choice of r2

1

Induced
choice of r1

0.8
0.6
0.4
0.2
0

1

Insincere
(expressive)
choice of r2

Sincere
choice of r1

Induced
don’t know
0

p(r1), Belief Democratic-Expressive Response r1 is Correct

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

p(r1), Belief Democratic-Expressive Response r1 is Correct

C) Observed Polarization
2

c, Value of Expressive Partisan Responding

1.8
1.6
1.4

Polarization region
(Democrats choose r1,
Republicans choose r2)

1.2
1
0.8
0.6
0.4
Democrats and
Republicans
choose r2

0.2
0

0

0.1

0.2

Democrats and
Republicans
choose r1

Dem. and/or Rep.
choose
don’t know

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

p(r1), Belief Democratic-Expressive Response r1 is Correct

Note: Panel A displays Democrats’ survey responses given incentives for correct (I=1) and don’t know (Idk=.75) responses for
different levels of returns to expressive partisan responding and beliefs about whether response r1 is correct. In all panels, Vdk=.5.
Panel B displays responses for the same parameters for Republicans. Finally, the grey area in panel C is the range of parameters for
which Democrats and Republicans offer different non-don’t know survey responses despite common beliefs about which
response is correct.

Figure 4: Example of Graphical Input Slider for Experiment #2

Table 1: Experiment 1, Question Wording and Baseline Partisan Differences in Scale Scores

Control Group, Control Group,
Mean
Mean
Republican
Democratic
Response
Correct response
Response
Lower (0), About the same (.5), Higher (1)
0.416
0.177

Control Group
P-value of
Difference in
Scale Scores, Difference of
Democrats - party means, 1Republicans
tailed test
0.239
0.000

Question
Iraq 07 to 08 Change Casualties

Question wording
Was the number of U.S. soldiers killed in Iraq in the first half of 2008 lower,
about the same, or higher than the number who were killed in the second half
of 2007?

Bush Inflation Change

Compared to January 2001, when President Bush first took office, has the
level of inflation in the country increased, stayed the same, or decreased?

Increased (1), Stayed about the same
(.5), Decreased (0)

0.894

0.694

0.201

0.000

207

Bush Unemployment Change

Compared to January 2001, when President Bush first took office, has the
level of unemployment in the country increased, stayed the same, or
decreased?

Increased (1), Stayed about the same
(.5), Decreased (0)

0.766

0.598

0.168

0.002

208

Est. Bush Approval

About what percentage of Americans approve of the way that George W.
Bush is handling his job as President?
About how many U.S. soldiers have been killed in Iraq since the invasion in
March 2003?
About what percentage of Republicans approve of the way that George W.
Bush is handling his job as President?
How old is Barack Obama?
How old is John McCain?
Was the number of U.S. soldiers killed in Afghanistan in the first half of 2008
lower, about the same, or higher than the number who were killed in the
second half of 2007?

20% (1), 30% (.75), 40% (.5), 50% (.25),
60% (0)
4,000 (0), 8,000 (.25), 12,000 (.5), 16,000
(.75), 20,000 (1)
40% (1), 50% (.75), 60% (.5), 70% (.25),
80% (0)
37 (0), 42 (.33), 47 (.66), 52 (1)
62 (0), 67 (.33), 72 (.66), 77 (1)
Lower (0), About the same (.5), Higher (1)

0.909

0.817

0.092

0.000

216

0.200

0.114

0.087

0.013

210

0.794

0.724

0.070

0.039

211

0.558
0.681
0.608

0.508
0.637
0.598

0.050
0.044
0.010

0.055
0.035
0.430

213
215
208

Bush Deficit Change

Compared to January 2001, when President Bush first took office, has the
federal budget deficit in the country increased, stayed the same, or
decreased?

Increased (1), Stayed about the same
(.5), Decreased (0)

0.938

0.944

-0.006

0.589

212

Placebo, Gold Price 1980

What was the price of gold, in dollars per ounce, on January 18, 1980?

0.005

128

0.755

123

Iraq Total Casualties
Est. Bush Approval Among Reps.
Obama Age
McCain Age
Afgh. 07 to 08 Change Casualties

In dollars, 0=0, 1000=1, Correct is
0.791
0.680
0.111
between $800 and $900
Placebo, Bangladeshi Indpc. Date
In what year did Bangladesh become independent of Pakistan?
In years, 1800=0, 2000=1, Correct is
0.151
0.185
-0.034
1971
Note: Source: 2008 CCES Study. Questions are ordered by size of partisan gap in Control Group responses, with placebo questions at the bottom. All responses scaled 0 to 1, with 1 the most Democratic response.

N
212

Table 2: Experiment 1: Effect of Payment for Correct Responses on Partisan Divergence in Scale Scores
(1)

Democrat (1=Yes, 0=Republican)
Payment for Correct Response * Democrat
Payment for Correct Response
Knowledge (0-1)
Race: White (1=yes)
Race: Hispanic (1=yes)
Race: Other Race (1=yes)
Female (1=yes)
Age (Years)
Age-squared/100
Region: Northeast
Region: Midwest
Region: South
Income (1=<10k; 14=>150k; 15=RF/Missing)
Income Missing
Education (1=No HS; 6=Post-grad)
Education: No HS
Education: Some college
Education: 2-year college
Education: 4-year college
Married/Domestic Partnership (1=yes)
Religious Attendance (1-6)
Constant
Observations
R-squared

(2)

Mean Scale Score (0 to 1)
(Pooled for 8 questions with partisan gap, p<.10, among control
cases)
0.118
0.105
[0.015]***
[0.016]***
-0.065
-0.059
[0.022]***
[0.022]***
0.038
0.031
[0.016]**
[0.016]*
0.013
[0.015]
0.017
[0.024]
0.040
[0.028]
0.051
[0.030]*
0.016
[0.012]
0.001
[0.002]
-0.001
[0.002]
0.043
[0.017]***
0.042
[0.016]***
0.014
[0.014]
0.005
[0.002]**
-0.046
[0.024]*
0.000
[0.006]
0.006
[0.024]
0.019
[0.014]
0.032
[0.026]
-0.003
[0.019]
-0.007
[0.013]
-0.002
[0.004]
0.239
0.160
[0.021]***
[0.059]***
3321
3299
0.398
0.407

Note: Source: 2008 CCES study. Includes only Democrats and Republicans. Cases included are from control and paid for correct
response condition. OLS Coefficients with robust standard errors, clustered by respondent. Question fixed effects not reported. *
significant at 10%; ** significant at 5%; *** significant at 1% (two-tailed tests).

Table 3: Question Wording and Baseline Partisan Differences in Scale Scores, 2012 MTURK Study

Question
Obama Unemployment
Bush II Unemployment

Defense Spending
Obama Vote 08

Iraq deaths % Black
Medicaid Spending

Question wording
From January 2009, when President Obama first took office, to February
2012, how had the unemployment rate in the country changed?
From January 2001, when President Bush first took office, to January 2009,
when President Bush left office, how had the unemployment rate in the country
changed?

Range of response line
Correct response
-2 (Unemployment decreased) to increased by 0.5 %
4% (Unemployment increased)
-2 (Unemployment decreased) to increased by 3.6 %
4% (Unemployment increased)

For every dollar the federal government spent in fiscal year 2011, about how
much went to the Department of Defense (US Military)?
In the 2008 Presidential Election, Barack Obama defeated his Republican
challenger John McCain. In the nation as a whole, of all the votes cast for
Obama and McCain, what percentage went to Obama?

3 to 27 cents

Approximately 12 to 13% of the US population is Black. What percentage of
US Soldiers killed in Iraq since the invasion in 2003 are Black?
Medicaid is a jointly funded, Federal-State health insurance program for lowincome and needy people. For every dollar the federal government spent in
fiscal year 2011, about how much went to Medicaid?

Pre-treatment, Pre-treatment,
Mean
Mean
Democratic
Republican
Response
Response
0.552
0.378

Pre-Treatment
Difference in
P-value of
Scale Scores, Difference of
Democrats - party means, 1Republicans
tailed test
0.174
0.000

N
389

0.715

0.583

0.132

0.000

383

19.4 cents

0.731

0.631

0.101

0.000

355

50 to 62%

53.70%

0.544

0.444

0.100

0.001

366

9 to 21%

9.90%

0.430

0.344

0.085

0.006

373

3 to 27 cents

7.5 cents

0.577

0.502

0.075

0.013

343

TARP % Paid Back

The Treasury Department initiated TARP (the first bailout) during the financial 1 (Less repaid) to 100 (More
crisis of 2008. TARP involved loans to banks, insurance companies, and auto repaid)
companies. Of the $414 billion spent, what percentage had been repaid, as of
March 15, 2012?

69.56%

0.391

0.324

0.068

0.027

349

Global Warming Amount

According to NASA, by how much did annual average global temperatures, in -1 (Temperatures cooler) to 2
degrees Fahrenheit, differ in 2010 from the average annual global temperature (Temperatures warmer)
between 1951 and 1980?

increased by 1.1
degrees

0.685

0.640

0.045

0.013

382

Iraq deaths

About how many U.S. soldiers were killed in Iraq between the invasion in 2003 1000 to 7000
and the withdrawal of troops in December 2011?
The Treasury Department finances U.S. Government debt by selling bonds
3 to 27 cents
and other financial products. For every dollar the federal government spent in
fiscal year 2011, about how much went to pay interest on those Treasury
securities?

4,486

0.549

0.504

0.044

0.072

382

6.2 cents

0.501

0.458

0.043

0.095

360

Foreign Born %

According to the Census Bureau, in 2010 what percentage of the total
1 to 100%
population of the United States was born outside of the United States (foreignborn)?

12.92%

0.785

0.772

0.013

0.239

388

Placebo: Mantle home runs '61

In 1961, Roger Maris broke Babe Ruth's record for most home runs hit in a
major league baseball season. He hit 61 home runs that year. How many
home runs did his Yankees teammate Mickey Mantle hit in that year?

54

0.339

0.319

0.019

0.236

362

Debt Service Spending

36 to 60

Note: Source 2012 Mturk Study. Questions are ordered by size of partisan gap in pre-treatment responses with placebo question at the bottom. All responses scaled 0 to 1, with 1 the most Democratic response.

Table 4: Experiment #2: Effect of Payment for Correct Responses on Partisan Divergence in Scale Scores
(1)

Sample

Specification
Flat fee * Democrat (1=Yes, 0=Republican)
Payment Correct * Democrat
Payment DK and Correct * Democrat
Payment for Correct Response
Payment for DK and Correct Response

(2)

All 10 non-placebo questions
with partisan-gaps (p<.10) pretreatment

OLS
0.145
[0.028]***
0.058
[0.012]***
0.028
[0.009]***
0.018
[0.025]
0.049
[0.024]**

Tobit
0.152
[0.029]***
0.061
[0.013]***
0.029
[0.009]***
0.018
[0.026]
0.052
[0.025]**

(3)

(4)

(5)

Second time
answering any
of 10 nonplacebo
questions with
partisan-gaps
(p<.10) pretreatment

First time
answering any
of 10 nonplacebo
questions with
partisan-gaps
(p<.10) pretreatment

All 10 nonplacebo
questions with
partisan-gaps
(p<.10) pretreatment

OLS
0.160
[0.029]***
0.062
[0.015]***
0.033
[0.011]***
0.022
[0.027]
0.056
[0.026]**

OLS
0.109
[0.052]**
0.047
[0.025]*
0.015
[0.015]
0.007
[0.048]
0.033
[0.046]

Democrat (1=Yes, 0=Republican)
Amount correct = 0.10 * Democrat
Amount correct = 0.25 * Democrat
Amount correct = 0.50 * Democrat
Amount correct = 0.75 * Democrat
Amount correct = 1.00 * Democrat
Prop. payment for DK=.20 * Democrat
Prop. payment for DK=.25 * Democrat
Prop. payment for DK=.33 * Democrat
Amount correct = 0.10
Amount correct = 0.25
Amount correct = 0.50
Amount correct = 0.75
Amount correct = 1.00
Prop. payment for DK=.20
Prop. payment for DK=.25
Prop. payment for DK=.33
Constant
Observations
R-squared
F-test, Flat fee * Dem. > Pay Correct * Dem.
F-test, Pay Correct * Dem. > Pay DK and Correct * Dem.

0.614
[0.026]***
4608
0.179
0.000
0.020

0.617
[0.026]***
4608
0.000
0.020

0.608
[0.027]***
3275
0.190
0.000
0.060

0.628
[0.048]***
1333
0.157
0.140
0.130

OLS

0.145
[0.028]***
-0.082
[0.033]**
-0.092
[0.033]***
-0.096
[0.033]***
-0.061
[0.036]*
-0.116
[0.036]***
-0.031
[0.018]*
-0.016
[0.020]
-0.041
[0.020]**
0.010
[0.027]
0.028
[0.027]
0.020
[0.027]
0.005
[0.029]
0.042
[0.029]
0.023
[0.013]*
0.030
[0.017]*
0.034
[0.016]**
0.614
[0.026]***
4608
0.181

Source: 2012 MTURK study. Includes only Democrats and Republicans. Comparison of post-treatment responses in control, pay correct, and pay
correct and don't know conditions. OLS Coefficients with robust standard errors in columns (1) and (3)-(5). Tobit results shown in column (2). Standard
errors are clustered by respondent. Question fixed effects not reported. * significant at 10%; ** significant at 5%; *** significant at 1% (two-tailed tests).

Table 5: Experiment #1: Vote Choice by Average Factual Assessments Scale Score in Control and Pay Correct Conditions
Presidential Vote
(1=Dem., 0=Rep., .=Else)
Average factual assessments scale score (0=Most Republican, 1=Most Democratic)
1.770
[0.222]***
Pay Correct * Average factual assessments scale score
-0.741
[0.367]**
Pay Correct condition
0.418
[0.224]*
Constant
-0.548
[0.135]***
Observations
373
R-squared
0.130
Percentage of Scale Score Effect on Vote Eliminated in Pay Correct Condition
41.9%
Note: Source: 2008 CCES. Includes only Democrats and Republicans. Scale score is pooled across 8 non-placebo questions with
partisan gaps in control condition. * significant at 10%; ** significant at 5%; *** significant at 1%

APPENDIX A
Experiment 1: Construction of the 2008 CCES Sample
The survey sample was part of a private module on the 2008 CCES, with a target population sample of
1,800 individuals. These questions were asked of a subset, drawn at random, of 626 of the 1,800
individuals in the full sample. The full sample is based on the 2005-06 American Community Study,
November 2008 Current Population Survey, and the 2007 Pew Religious Life Survey. Thus, this target
sample is representative of the general population on a broad range of characteristics including a variety
of geographic (state, region and metropolitan statistical area), demographic (age, race, income, education
and gender), and other measures (born-again status, employment, interest in news, party identification,
ideology and turnout). Polimetrix invited a sample of their opt-in panel of 1.4 million survey respondents
to participate in the study. Invitations were stratified based on age, race, gender, education and by simple
random sampling within strata. For more detailed information on this type of survey and sampling
technique see Vavreck and Rivers (2008). More broadly, see Baker et al. (2010) for a report on the
potential strengths and limitations of online panels.

Experiment 2: Construction of MTurk Sample
Subjects for the experiment were recruited with an advertisement for “A quick survey to see what you
know and how you learn.” We also invited 868 previous participants in our surveys, 115 each strong
Democrats and Republicans, 208 each Democrats and Republicans, and 111 each weak Democrats and
Republicans, in an attempt to attract more Republicans than are ordinarily found in MTurk samples.

Experiment 2: Instructions to Subjects
The experiment had three conditions: a control condition, the pay-for-correct condition, and the pay-forcorrect-and-“don’t know” condition. (It also had a fourth condition that we do not analyze here: see
Footnote 15.)
Instructions in the control condition: “Once again, your answers will be timed. By answering

2

these questions, you will earn an additional 50 cent bonus.”
Instructions in the pay-for-correct condition: “Once again, your answers will be timed.
Additionally, we are now going to give you a [X] cent bonus for each question you answer correctly.
We'll tell you how many questions you get right at the end of the survey. You'll get credit for answering a
question correctly if the thick horizontal bar underneath your arrow covers the correct answer. So, for
example, in the picture below, the arrow is at 5. If the correct answer were 5.25, which is under the bar,
you would earn the bonus. If the correct answer was 7, however, you would not earn the bonus.”
Instructions in the pay-for-correct-and-“don’t know” condition: “Once again, your answers will
be timed. Additionally, we are now going to give you a X cent bonus for each question you answer
correctly. We'll tell you how many questions you get right at the end of the survey. You'll get credit for
answering a question correctly if the thick horizontal bar underneath your arrow covers the correct
answer. So, for example, in the picture below, the arrow is at 5. If the correct answer were 5.25, which is
under the bar, you would earn the bonus. If the correct answer was 7, however, you would not earn the
bonus. As an alternative to being paid for a correct answer, you can instead earn a X × Y cent bonus for
each question you tell us you don't know the answer to. We'll pay you for saying ‘don’t know’ if you
click the check box next to ‘don’t know,’ but when you do so, the location of your arrow, whether correct
or incorrect, does not affect your payment. Because the payment for ‘don’t know’ is (Y × 100)% of the
payment for getting an answer correct, you will on average earn more by selecting don't know than your
best guess if you are less than (Y × 100)% sure that the bar underneath the arrow covers the correct
answer.”

Experiment 2: Analysis of Consultation of Outside References
After the survey was over, we asked participants if they had looked up the answers to each question that
they were asked, noting explicitly that “Your bonus is already determined, and we won't change your
bonus in any way on the basis of your answer to these questions.” Of our 795 partisan participants, only
20 (2.5 percent) reported looking up the answer to 41 questions (0.74 percent of all questions asked). The
percentages of user-questions by treatment assignment are 0.32 percent (control), 0.96 percent (pay for
3

correct), and 0.64 percent (pay for correct and pay for don’t know).

Appendix References
Baker, Reg, Stephen J. Blumberg, J. Michael Brick, Mick P. Couper, Melanie Courtright, J.
Michael Dennis, Don Dillman, Martin R. Frankel, Philip Garland, Robert M. Groves, Courtney
Kennedy, Jon Krosnick, Paul J. Lavrakas, Sunghee Lee, Michael Link, Linda Piekarski, Kumar
Rao, Randall K. Thomas, and Dan Zahs. 2010. “Research Synthesis: AAPOR Report on Online
Panels.” Public Opinion Quarterly 74 (4): 711-81.
Vavreck, Lynn, and Douglas Rivers. 2008. “The 2006 Cooperative Congressional Election
Study.” Journal of Elections, Public Opinion and Parties 18 (4): 355-66.

4

APPENDIX B
(Begins on following page)

5

You are being asked to complete an online research survey that will take approximately 7-9 minutes. The survey is
conducted by researchers at Yale University to study how people learn. This page describes your consent.

Findings from this study may be reported in scholarly journals, at academic seminars, and at research association
meetings. The data will be stored at a secured location and retained indefinitely. No identifying information about you will
be made public and all of your choices will be kept completely confidential. Your participation is voluntary. You are free to
stop the survey at any time without penalty.

There are no known risks associated with this study beyond those associated with everyday life. Although this study will
not benefit you personally, we hope that our results will add to the knowledge about how people learn. You will receive
$0.50 for completing the survey, paid through Amazon Mechanical Turk. You will also have the opportunity to earn a
bonus of $0.50 or more, although not everyone will receive a bonus.

To participate in the study, you must be at least 18 years old and a United States resident. JavaScript must be activated
on your browser so that the graphics in the survey will work properly. The next page will test your browser.

If you have any questions about the research, you can contact Seth Hill at seth.hill@yale.edu. If you have any questions
about your rights as a research participant or concerns about the conduct of this study, you may contact the Yale
University Human Subjects Committee, Box 208010, New Haven, CT 06520-8010, 203-785-4688,
human.subjects@yale.edu.

When you are ready to begin, please elect to participate and press the Submit button. You will then be taken to the first
page of the survey.

I agree to participate.
I do not agree to participate.

To confirm that our survey graphics will work with your browser, please follow the instructions in the graphic
below. You have 20 seconds to complete this task. After 20 seconds, your browser will automatically proceed
to the next page.

Please drag the arrow as far as you can to the right. You can move the arrow by
clicking on the arrowhead and dragging.

Arrow

You have 16 seconds to submit your answer before your current answer is automatically submitted.

Please read carefully and answer t he following quest ions.
Here are two personality traits that may or may not apply to you. Please check the box to indicate the extent to
which you agree or disagree with each statement. You should rate the extent to which the pair of traits applies
to you, even if one characteristic applies more strongly than the other. To demonstrate that you've read this
much, just go ahead and select both disagree strongly and agree strongly for both questions below, no matter
how you would actually answer each question. In other words, to confirm that you are paying attention, for
each question please check both of these two boxes.

I see myself as: Dependable, self-disciplined.
Agree strongly.
Agree moderately.
Agree a little.
Neither agree nor disagree.
Disagree a little.
Disagree moderately.
Disagree strongly.

I see myself as: Disorganized, careless.
Agree strongly.
Agree moderately.
Agree a little.
Neither agree nor disagree.
Disagree a little.
Disagree moderately.
Disagree strongly.

Please read carefully and answer t he following quest ions.
What is the highest level of education that you have achieved?
No high school diploma.
High school diploma or equivalent.
Some college.
Two year degree.
Four year college graduate.
Post-graduate.

What is the year of your birth?

What is your gender?
Female.
Male.

What is your state of residence?

Generally speaking, do you usually think of yourself as a Democrat, a Republican, an Independent, or what?
Democrat.
Republican.
Independent.
Other.

Please read carefully and answer t he following quest ions.
Some people seem to follow what's going on in government and public affairs most of the time, whether
there's an election going on or not. Others aren't that interested. Would you say you follow what's going on in
government and public affairs...?
Most of the time.
Some of the time.
Only now and then.
Hardly at all.

We are interested in the kinds of things people do when they use the internet. What kinds of things have you
used the internet for in the LAST SEVEN DAYS? (Choose as many as apply to you)
Get directions.
Plan vacations.
Keep in touch with friends.
Look at sports highlights.
Find restaurants.
Pay bills.
Look up movie times.
Shopping.
Read the news.
Read about politics.

Do you happen to know how much of a majority is required for the United States Senate and House to
override a Presidential veto?
A majority (fifty percent plus one vote).
Two-thirds (sixty-seven percent).
Three-fourths (seventy-five percent).
Ninety percent.
Don't know.

Do you think most professional athletes are good role models for children today?
Yes.
No.
Don't know.

In this study, we'd like you to tell us what you think the correct answer is to a number of questions. We don't
want you to look up those answers or talk to someone else, so even if you don't know please just give us your
best guess. For each question, we'll give you a short period of time -- 30 seconds -- to answer the question
before we automatically take you to the next question.

To indicate your answer, we will ask you to slide the arrow on a line like that below to the point that is closest
to your answer. You can slide that arrow by clicking your mouse on the arrowhead and dragging it to the left or
right.

How tall is the average NBA player?

Your guess

3ft

4ft

5ft

Shorter

6ft

7ft
Taller

For example, in the above graphic, if you though the correct answer was 6 feet 6 inches, you would slide the
arrow to the point midway between the lines marked 6 and 7 ft.

Give it a try! Once you are happy with where the arrow is located, you can click "Next." On the next page, we'll
give you a timed example with another question.

How tall is the Statue of Liberty, in feet, from the base of the feet to the top of the
torch?

Your guess

140ft

180ft

220ft

260ft

300ft

Shorter

Taller

In this example, we are asking you to indicate your best guess as to how tall the Statue of Liberty is. You can
also see how the countdown timer works -- you have 45 seconds to indicate your answer (see below). After
you've indicated your best guess, click "Next" or just wait to go to the next page. When the timer is up, you will
automatically be routed to the next page.

You have 45 seconds to submit your answer before your current answer is automatically submitted.

We're almost ready to begin. Before we proceed, we just want to make sure we've been clear about what we
are asking you to do.

Dave has two dozen apples. He eats half of them, and then eight more. How many
apples are left?

A guess

-1

1

3

5

7

In the graph above, we've placed the arrow at a certain point to indicate somebody's response to the question.
Which of the following has that person indicated is their best guess?

Their best guess is...
1.
2.
3.
4.
5.
None of the above.

Dave has two dozen apples. He eats half of them, and then eight more. How many
apples are left?

A guess

-1

1

3

5

The arrow is located midway between 3 and 5, so the person's response is 4.

7

We will now send you to the actual survey. On the next screen, you will be presented with your first question
and will only have a limited amount of time to respond.

Please do not use the back button in your browser during this survey. Any questions your answer a second
time by using the back button will not be recorded. When you are ready, please click Next.

Please drag the slider to your best guess to the following

About how many U.S. soldiers were killed in Iraq between the invasion in 2003 and
the withdrawal of troops in December 2011?

Your guess

2000

3000

4000

5000

6000

You have 27 seconds to submit your answer before your current answer is automatically submitted.

Please drag the slider to your best guess to the following

According to the Census Bureau, in 2010 what percentage of the total population of
the United States was born outside of the United States (foreign-born)?

Your guess

18%

34%

50%

67%

84%

You have 28 seconds to submit your answer before your current answer is automatically submitted.

Thank you for answering those questions, we'd now like you to answer a few more questions.

Once again, your answers will be timed.

By answering these questions, you will earn an additional 50¢ bonus.

Again, please do not use the back button in your browser. Any questions your answer a second time by
using the back button will not be recorded. When you are ready to proceed, please click Next.

Please drag the slider to your best guess to the following

In the 2008 Presidential Election, Barack Obama defeated his Republican
challenger John McCain. In the nation as a whole, of all the votes cast for Obama
and McCain, what percentage went to Obama?
Your guess

52.0%

54.0%

56.0%

58.0%

60.0%

You have 28 seconds to submit your answer before your current answer is automatically submitted.

Please drag the slider to your best guess to the following

For every dollar the federal government spent in fiscal year 2011, about how much
went to the Department of Defense (US Military)?

Your guess

7 cents

11 cents

15 cents

19 cents

23 cents

You have 26 seconds to submit your answer before your current answer is automatically submitted.

Thank you for your part icipat ion!
Your bonus is already determined, and we won't change your bonus in any way on the basis of your answer to
these questions. For research purposes...

Did you look up the answer to this question?
In the 2008 Presidential Election, Barack Obama defeated his Republican challenger John McCain. In the
nation as a whole, of all the votes cast for Obama and McCain, what percentage went to Obama?
No, I did not look up th answer to this question.
Yes, I did look up the answer to this question.

Did you look up the answer to this question?
For every dollar the federal government spent in fiscal year 2011, about how much went to the Department of
Defense (US Military)?
No, I did not look up th answer to this question.
Yes, I did look up the answer to this question.

Did you look up the answer to this question?
About how many U.S. soldiers were killed in Iraq between the invasion in 2003 and the withdrawal of troops in
December 2011?
No, I did not look up th answer to this question.
Yes, I did look up the answer to this question.

Did you look up the answer to this question?
According to the Census Bureau, in 2010 what percentage of the total population of the United States was
born outside of the United States (foreign-born)?
No, I did not look up th answer to this question.
Yes, I did look up the answer to this question.

Did you look up the answer to this question?
Compared to January 2001, when President Bush first took office, how had the level of unemployment, as
measured using the unemployment rate, in the country changed by the time he left office in January 2009?
No, I did not look up th answer to this question.
Yes, I did look up the answer to this question.

Did you look up the answer to this question?
The Treasury Department initiated TARP (the first bailout) during the financial crisis of 2008. TARP involved
loans to banks, insurance companies, and auto companies. Of the $414 billion spent, what percentage had
been repaid, as of March 15, 2012?
No, I did not look up th answer to this question.
Yes, I did look up the answer to this question.

Did you look up the answer to this question?
Medicaid is a jointly funded, Federal-State health insurance program for low-income and needy people. For
every dollar the federal government spent in fiscal year 2011, about how much went to Medicaid?
No, I did not look up th answer to this question.
Yes, I did look up the answer to this question.

Thank you for your part icipat ion!
What is the total number of Mechanical Turk surveys you have taken about current events or politics?

What is the total number of Mechanical Turk surveys you have taken about current events or politics in the
last month?

If you would like to be contacted when we have future studies, please leave your email here. If not, leave
blank:

If you would like to leave any comments or feedback, please do so here (up to 500 characters):

Pleast continue to the next page to retrieve your code for payment!

Thank you for your participation!

You have now completed the survey.
If you have any questions, please contact seth.hill@yale.edu. If you have any questions about your rights as a
research participant or concerns about the conduct of this study, you may contact the Yale University Human
Subjects Committee at human.subjects@yale.edu.

For the purposes of getting paid on Mechanical Turk, please enter the following code into the box on the
survey's Mechanical Turk HIT page. You must enter this code to get your bonus.

If you are curious about the sources we used to score your answers, please contact us through the
Mechanical Turk interface and we are happy to provide references to you. Thank you!

APPENDIX C: ACCURACY
While the analysis reported in the main text focuses on polarization, a distinct question is whether or not
offering incentives affects accuracy, or the degree to which expressed survey responses are correct. In the
second experiment, we can examine the absolute distance between participants’ survey responses across
treatment conditions to assess the effect of incentives on accuracy. (That is, in the same 0 to 1 scale, we
can calculate the difference between a respondent’s slider placement and the correct answer in that 0 to 1
scale and then take the absolute value of the difference between those two numbers.) As before, however,
we have to decide how to treat responses from those who select “don’t know.” We code those responses
as accurate in this analysis, which means that selecting “don’t know” will mechanically (if weakly)
increase accuracy.
We find that there is no difference between the control (flat fee) condition and the pay for correct
condition in accuracy: The average distance from the truth across questions and treatments is .28 in both
cases. 1 Our earlier results show that offering incentives for correct responses substantially reduces
partisan divergence. The analysis here suggests that convergence is, on average, no more likely to be
toward the truth than away from it. Such a pattern is consistent either with shared bipartisan but
inaccurate beliefs or, alternatively, very weak beliefs and small returns to expressive partisan responding
such that individuals are effectively guessing across some range of the scale. Indeed, the relatively high
frequency of selecting “don’t know” in the pay for correct and don’t know condition implies that many
participants understand they don’t have well-informed beliefs about the truth. Not surprisingly, therefore,
we find substantially higher accuracy in that treatment condition: The average distance from the truth in
the Pay for Correct and Don’t Know treatment is .13, or 55% smaller than in either of the other conditions
(p<.01 for tests of differences). Overall, then, when given the option to choose don’t know, it appears that
those who do so may understand that they systematically know less than those who do not.
1

Regression results estimated from the model Absolute Value of Distance from the Truthi = b0 + b1
PayCorrecti + b2 PayCorrectDKi + Question + ei produce results substantively similar to simple
difference in average Absolute Value of Distance from the Truth scores across treatments and are
available upon request.

6

This assertion leads naturally to our last analysis: Among those assigned to the Pay for Correct
and Don’t Know condition, how are the pre-treatment survey responses of those who will be induced to
select don’t know different from those who will continue to give a response when offered a payment both
for a correct and don’t know answer? Are those who will select don’t know less accurate, before being
treated, then those who will continue to offer a response? Is this lack of knowledge, if it exists, associated
with more or less partisan divergence? In order to understand this question, we can examine the pretreatment survey responses, both in terms of absolute value of distance from the truth and partisan
divergence. For the former, we estimate
Distance from the Truthijt=0 = b0 + b1 Will Say Don’t Know ijt=1 + Question + ei,
where Distance from the Truth is the pre-treatment (t=0) absolute value of the distance from the correct
response for question j and Will Say Don’t Know=1 if the participants will select don’t know the second
time (t=1) they answer that question when offered incentives for doing so. If individuals who will later
select “don’t know” know less and understand that lack of knowledge, then we would predict b1 > 0. OLS
estimates employing this specification appear in column (1) of Appendix C Table C1. The distance from
the truth among those who will continue to offer a response rather than selecting don’t know is .185.
Among those who will select don’t know, this difference increases by .024, or about 13% (p<.05, onetailed), indicating that those participants who say they don’t know post-treatment were less accurate when
answering the questions pre-treatment.
If these participants are less knowledgeable, is this associated with different levels of pretreatment polarization? To answer this question, we estimate
Rijt=0 = b0 + b1 Democrati + b2 Will Say Don’t Know ijt=1 + b3 Democrati × Will Say Don’t Knowijt=1
+ Question + ei,
which is similar to our earlier models for assessing partisan divergence but instead examines pretreatment responses by whether the participant will subsequently choose “don’t know.” In this model, b1
is the baseline level of pre-treatment polarization between Democrats and Republicans, and b3 is how
much larger or smaller that estimate is among those who will later choose don’t know. OLS estimates
7

appear in column (2) of Table C1. Here, we find that those who will later choose don’t know are no more
or less polarized than those who will continue to offer a response. Put differently, an apparent lack of
knowledge (as demonstrated by a willingness to choose “don’t know”) is a marker of lack of knowledge
about the truth, but those partisans still diverge by a similar amount that their more informed partisan
counterparts. Divergence therefore persists but is centered on a different (less true) response than among
those who appear to have greater knowledge.

8

Appendix Table A1: Experiment #1: Effect of Payment for Correct Responses on Partisan Divergence in Scale Scores by Question
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
Iraq 07 to 08
Bush
Est. Bush
Bush Inflation Unemployment
Est. Bush
Iraq Total
Change
Approval
McCain Age
Casualties
Change
Change
Approval
Casualties
Among Reps.
Obama Age
Democrat (1=Yes, 0=Republican)
0.239
0.201
0.168
0.092
0.087
0.070
0.050
0.044
[0.052]***
[0.044]***
[0.056]***
[0.023]***
[0.038]**
[0.039]*
[0.031]
[0.025]*
Payment for Correct Response * Democrat
-0.078
-0.026
-0.074
-0.100
-0.064
-0.072
-0.048
-0.053
[0.077]
[0.061]
[0.079]
[0.034]***
[0.054]
[0.055]
[0.045]
[0.033]
Payment for Correct Response
0.043
0.059
0.091
0.018
0.051
0.026
0.005
0.010
[0.051]
[0.052]
[0.058]
[0.024]
[0.036]
[0.039]
[0.034]
[0.024]
Constant
0.177
0.694
0.598
0.818
0.114
0.724
0.508
0.637
[0.033]***
[0.036]***
[0.042]***
[0.016]***
[0.024]***
[0.029]***
[0.023]***
[0.019]***
Observations
415
409
407
421
412
416
419
422
R-squared
0.064
0.093
0.032
0.044
0.014
0.008
0.008
0.012
Source: 2008 CCES study. Includes only Democrats and Republicans. Cases included are from control and paid for correct response condition. OLS Coefficients with robust standard errors. * significant at
10%; ** significant at 5%; *** significant at 1% (two-tailed tests).

Appendix Table A2: Experiment #2: Effect of Payment for Correct and Don't Know Responses on Partisan Divergence in Scale Scores by Question
(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)
(9)
(10)
Global
Obama
Bush II
Defense
Iraq deaths %
Medicaid
TARP % Paid
Warming
Debt Service
Unemployment Unemployment
Spending
Obama Vote 08
Black
Spending
Back
Amount
Iraq deaths
Spending
Flat fee * Democrat (1=Yes, 0=Republican)
0.293
0.239
0.118
0.126
0.219
0.136
0.107
0.133
0.051
0.010
[0.065]***
[0.068]***
[0.085]
[0.086]
[0.081]***
[0.086]
[0.091]
[0.057]**
[0.072]
[0.089]
Payment Correct * Democrat
0.083
0.142
0.097
0.035
0.013
0.048
0.048
0.026
0.009
0.073
[0.042]**
[0.036]***
[0.038]**
[0.039]
[0.045]
[0.042]
[0.043]
[0.027]
[0.037]
[0.044]
Payment DK and Correct * Democrat
0.109
0.037
0.026
0.070
0.011
-0.003
0.016
0.039
-0.001
-0.025
[0.032]***
[0.026]
[0.024]
[0.033]**
[0.030]
[0.025]
[0.032]
[0.019]**
[0.028]
[0.024]
Payment for Correct Response
0.021
-0.049
-0.053
-0.028
0.113
0.081
-0.019
0.058
-0.009
0.042
[0.057]
[0.072]
[0.080]
[0.080]
[0.069]
[0.079]
[0.073]
[0.055]
[0.064]
[0.082]
Payment for DK and Correct Response
-0.019
0.079
0.059
-0.031
0.158
0.067
0.053
0.038
0.013
0.039
[0.054]
[0.068]
[0.076]
[0.078]
[0.064]**
[0.073]
[0.071]
[0.053]
[0.062]
[0.076]
Constant
0.401
0.586
0.630
0.467
0.241
0.489
0.346
0.605
0.522
0.490
[0.048]***
[0.066]***
[0.073]***
[0.073]***
[0.060]***
[0.071]***
[0.066]***
[0.050]***
[0.057]***
[0.074]***
Observations
444
485
446
457
470
442
452
466
479
467
R-squared
0.077
0.099
0.050
0.029
0.023
0.022
0.017
0.028
0.005
0.029
F-test, Flat fee * Dem. > Pay Correct * Dem.
0.000
0.100
0.410
0.170
0.010
0.180
0.280
0.050
0.300
0.260
F-test, Pay Correct * Dem. > Pay DK and Correct * Dem.
0.310
0.010
0.060
0.250
0.490
0.150
0.280
0.340
0.420
0.030
Source: 2012 MTURK study. Includes only Democrats and Republicans. Comparison of post-treatment responses in control, pay correct, and pay correct and don't know conditions. OLS Coefficients with robust standard errors. * significant
at 10%; ** significant at 5%; *** significant at 1% (two-tailed tests).

Appendix Table C1: Experiment #2: Are Individuals who will be induced to select "Don't Know" more Polarized or less Knowledable?
(1)
(2)
Pre-treatment scale score
Pre-treatment absolute value of
(+= More Democratic)
distance from correct answer
Democrat (1=Yes, 0=Republican)
0.078
[0.021]***
Will say Don't Know * Democrat
-0.011
[0.031]
Will say Don't Know Post-treatment
-0.033
0.024
[0.026]
[0.012]*
Constant
0.657
0.185
[0.025]***
[0.013]***
Observations
1547
1547
R-squared
0.146
0.157
Note: Source: 2012 MTURK study. Includes only Democrats and Republicans in pay correct and don't know condition. Robust standard errors,
clustered by respondent. Question fixed effects not reported. Number of participants is 379.

