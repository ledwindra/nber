{
    "id": 21124,
    "citation_title": "Improving Policy Functions in High-Dimensional Dynamic Games",
    "citation_author": [
        "Carlos A. Manzanares",
        "Ying Jiang",
        "Patrick Bajari"
    ],
    "citation_publication_date": "2015-05-04",
    "issue_date": "2015-04-30",
    "revision_date": "None",
    "topics": [
        "\n",
        "Econometrics",
        "\n",
        "Estimation Methods",
        "\n",
        "Microeconomics",
        "\n",
        "Game Theory",
        "\n",
        "Industrial Organization",
        "\n",
        "Market Structure and Firm Performance",
        "\n"
    ],
    "program": [
        "\n",
        "Industrial Organization",
        "\n",
        "Technical Working Papers",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nIn this paper, we propose a method for finding policy function improvements for a single agent in high-dimensional Markov dynamic optimization problems, focusing in particular on dynamic games. Our approach combines ideas from literatures in Machine Learning and the econometric analysis of games to derive a one-step improvement policy over any given benchmark policy. In order to reduce the dimensionality of the game, our method selects a parsimonious subset of state variables in a data-driven manner using a Machine Learning estimator. This one-step improvement policy can in turn be improved upon until a suitable stopping rule is met as in the classical policy function iteration approach. We illustrate our algorithm in a high-dimensional entry game similar to that studied by Holmes (2011) and show that it results in a nearly 300 percent improvement in expected profits as compared with a benchmark policy.\n\n",
    "acknowledgement": "\nThe views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}