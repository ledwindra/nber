NBER WORNG PAPER SERIES

ThE SINGULAR VALUE ANALYSIS
IN MATRIX COMRJTATION

Richard Becker*
Neil I<.aden*
Virginia KLema*

Working Paper No. 6

COMPUFER RESEARCH (INTER FOR ECONOMICS AND MANAGEMENT SCIENCE

National Beau of Economic Research, Inc.
575 Tethnolo, Square
Cambridge, Massachusetts 02139
July l97L

Pre]±ninary: not for quotation
NBER working papers are distr.thuted informally and in ]±ited
numbers for coimnts only. They should not be quoted without
written permission.
This report has not undergone the review accorded official NBER

publications; in particular, it has not yet been submitted for
approval by the Board of Directors.

NBER Coirputer Research Center. Research supported in part by
National Science Foundation Grent GJ_1l5LX3 to the National
Bureau of Economic Research, Inc.

Abstract

This paper discusses the robusthess and the conutational stability of the
singular value decxmposition algorithm used at the NBER Caruter Research

Cneter. The effect of perturtaticris on input data is explored. Suggestions
are made for using the algorithm to get information about the rank of a

real square or rectangular matrix. The algorithm can also be used to
compute the best approximate solution of linear systerr of equations in the

least squares sense, to solve linear systens of equations with equality
constraints, and to determine dependencies or near dependencies anong the

rows or coLzrns of a matrix.

A copy of the subroutine that is used and sai examples on which it has
been tested axe included in the appendixes.

Contents

The Singular Value Analysis in Matrix Computation .
References

1

15

Appendixes

A. Listing of the Fortran IV Program M]IJFIT

Al

B. TROLL Imp1enntation of MINFIT and Associated Output

Bi

C. Selected Matrices, Computed Solutions, and Illustrative
Thcamples

Cl

The singular value decomposition of a matrix is one of the nost elegant algorithms

in nunerical algebra for exposing quantitative inforntion about the structure of
a system of linear equations. It can be used to get information about the rank of
a square or rectangular matrix, to compute the best approxiiite solution of a
linear system of equations in the least squares sense, to solve systems of linear
equations

with equality constraints, and

to

determine dependencies or near-

dependencies anng the rows or coliiris of a matrix. Occasionally the singular
decomposition is used in the iterations of linear systems that tend toward

value

solution of nonlinear systems of equations. The condition ranther of a matrix

the

with respect to the solution of a linear system of equations is a by-product of
the singular value decomposition as is the production of the pseudo-inverse and
the solution of homogeneous systems of equations.

The condition nither of a matrix with respect to the solution of a linear
system of equations shows how well the vector x is defined by the transformation

Axb. The condition nunber K(A) of the nonsingular matrix A is the ratio
where

of A

and c11 are, respectively, the maxiunun and

minimum singular values

(i.e., the non-negative square roots of the eigenvalues of ATA where AT denotes

the transpose of A). For example, if K(A)1O6, a perturbation of 2_20 in the eleirents

to

of A can change the computed solution x by a factor of 2—20 •l06 , that

is

say, even the leading digit may be changed. For a more rigorously detailed

explanation, see [9J.

Nunera1s in square brackets refer to entries in the Pference section, p. 15.

—2—

In the discussion that follows, we seek to corrpute directly the best

solution to

approximate

system

the possibly over-determined or under-determined

of equations
Ax

The singular

value

b.

deconosition is used to obtain this solution.

Frequently a user, or a problem originator, poses a
he wants to obtain a solution vector x in the sense
from

the

system

of

problem from

which

of least squares

equations

ATAxATb.

Possibly

he thinks the

information he needs cones from the

solution

x= (ATA) IATb.

Classically,

is

data matrix A and the vector b are exact

(that

to say, there is no uncertainty in the data A and b), (2) if the

precision

of

and stored

be

(1) if the

the arithjitic of the iradhine is such that ATA can be ford

exactly, and (3) if ATA is of full rank, the soli.rtion x

obtained from (ATA)IATb. However, given

are

seldom attainable in

that

these three

conditions

practice, the solution should not be conuted

in this way because of the extra precision that is required.
unless

could

Further!rcre,

there is a priori exact information known about the rank of A, the

solution x cannot be obtained from the pseudo-inverse of A with any nre

—3—

authenticity than fran (ATA)I. That is to say the rank should be detenrilned

during the course of computing the singular value decomposition. Reliable
inforniation

about rank

deficiency cannot be obtained from triangular

factorization.

Sylvester wrote an article on the singular value decanposition of real
mm matrices in 1889 [10]. Eckert and

Young

extended the %.ork to general

matrices in 1936 [1]. The definitive paper on calculating the singular
value decomposition was written by Golub and Kahan [2]. Though the paper
was

published

in 1965, it is fair to say that its use as a robust tool of

mathematical software is recent and, as of now, is not very widespread

[L] and

(see

[5].

The singular values of the matrix A and the non-negative square roots
of the eigenvalues of the symmetric matrix ATA are mathematically equal,

but may be different computationally. Singular values correct to working
accuracy

for the matrix A can often be computed when certain small

elgenvalues cannot be computed for ATA. This fact is not startling. It
is caused by the perturbation of an exact
of AT by A. There are

many

introduced

the multiplication
examples of such matrices, one of which is
ATA

in

illustrated in [9], assuming a 4-deciina1-place machine, as
A

1.005

0.995

.995

1.005

L

having singular values 2.0 and .01. The matrix ATA in '4-decimal arithmetic

is
ATA

2.000

2.000

2.000

2.000

——

with eigenvalues i..0 and 0.0. Attrition in forming ATA has obscured all

information about the smaller singular value.
The subroutine I'1I1'IFIT, using the notation in [2], reduces the system

of equations
Ax

b

where A has m rows and n columns (m can be less than, equAl to, or greater

than n) to the form

UEvTxb
T
T
VxUb.

giving

The columns of V are the orrthonormal eigenvectors of ATA. The transforration

UTb is formed directly --

of

U is

not computed explicitly. The columns

If

U are the orthonormal eigenvectors of PAT

one needs the explicit
to the right-hand side b.

columns of U he should append the identity matrix

There is no restriction, at the subroutine level, on the number of columns of b;
it can be zero.

values

The diagonal matrix, E, contains the singular
transformations

of A. The

used to obtain the decomposition preserve unitarily invariant

norms, thereby assuring that the norm of Z is that of A. The diagonal
elements of Z, when ordered, are a]

not order the singular values

a2

a3

.

.

.

a 0. MflFIT does

Given information about the certainty of the

data A and b, one can choose the best approxiniatin matrix Ar of full rank

that

is nearest, in the norm sense, to the matrix A. From Ar the best

candidate solution x for Axb can be computed. If ar is chosen

such

that

—5—

a1

a2

•

•

effectively

are

ratio, a1— .

ar

O

>

a2.

a, whereby a1 ,

•

•a

.

considered to be zero, the condition number of A is the

If the matrix A is equilibrated, i.e., scaled, so that

or

the square root of the
a constant representing the uncertainty in the data,

To

be arbitrary about the

a1l, Gr should be not less than

choice of

machine

precision,

is larger.
ar relative to a1 is difficult. At the
whichever

NEER Conputer Psearch Center we have chosen a rank tolerance equal to the

floating point representation of 2_26, the square root of the machine
precision, 2_52. There is an obvious danger that this range rolerance may be
inadequate for sone problesm. For example suppose that AU z vT such that
1
2

2

2
—26
2
2

26

227

where 2_52

e

<

2_26,

The arbitrary rank
Thus

say.

tolerance

u1d leave a4 unchanged but set a5 to zero.

Aa would be deemed to have full rank whereas a ncre judicious choice of rank is 3.

This example,

though artificial,

is given to encc'a all usnc to display

the diagonal matrix, E, to see his Dart icular Drohiem' s distribution of the

a..
1

—6—

Given

an

appropriate choice of

hAil

whe

- llI

(z 2)l/2
ir+1

indicates the Frobenius no, i.e.

lAP

(E
il,m

(a1.

)2)112

j:l,n
Noting tha

uTu

vTv

vvT

I

and that the

pseudo-inverse of E is

the diagonal matrix

a1
1
a2

1

ar
0

0

the

pseudo-inverse of A is

A4 v uT.
There

is seldom any reason to form a pseudo-inverse explicitly. MINPIT

accumulates Householder transformations to produce a bidiagonal matrix

having the same singular values as A, and continues, by a variant of the
QR algorithm (see [3]), to diagonalize the bidiagonaJL form to give

—7—

T
T
EVxUb=c

from which

x VZ+C.

Various candidate solutions x can be provided by different choices of a rank

tolerance to fix ar. See [6], chapters 25 and 26.
For suitably chosen ar, consider those columns of V associated with
•

.

as

Vs,, namely the columns of V that span the null space of A. Then
AV

0.

When such columns V exist, they constitute the non-trivial solutions of
the homogenous system of equations

AX 0.
The elements of the columns of V can be inspected to reveal dependencies

or near dependencies among the columns, i.e., the variables of the
coefficient matrix A. Analogously, the columns of U can reveal dependencies
among the equations,

e., the rows of A.

In using MINFIT, and providing it to other users, we are concerned with

three distinct but related items, (1) the stability of the algorithm
fran the standpoint of numerical algebra, (2) the robustness of the
mathematical software that inplements the algorithm, and (3) the
documentation that provides information on the use of the mathematical software.

The numerical stability of an algorithm usually means that the solution
that is computed is the exact solution of a neighboring problem and that
the neighboring problem can be defined in the sense of a backward error

analysis. Such analysis for the singular value decomposition has been

—8—

published

in [2], [11], [12], and

is stable in the sense that

the

[13].

The singular

value

decomposition

computation of eigensystems of Hermitiari

matrices is stable. In general, we expect

JJA-

UEVTII

HAIl

in

of

precision, as is corroborated for the matrices
Appendix C. If this criterion should not be net for sane matrix, A,

to be the order

machine

the authors xuld like to }a-iow about it. For computational convenience we
computed

I

I

JAI I -

IUEVTI

for

the test matrices.

hAil

Robustness

of this mathematical software is established to the extent

of exposing test matrices on which the algorittm
Professor Gene Golub
1)

suggested

has performed

two additional tests.

Decompose A to give uzvT. Permute

correctly.

are

These

a, reform AUVT,

and recompute the decomposition. This gives the
effect of a perturbation

on A in the sense that the

resulting decomposition will show a permutation of
the columns of U and V, yet give the same singular

va:Iues of A. As additional tests we have taken ortho-

normal matrices U and V, particular a, formed uvTA
and computed AUVT. Denote the maximum singular value by
and the minimum singular value by nin If emin is
amax

less

than the relative machine precision, the computed

°nin may

machine

not

on

be less than the relative precision of the

which it is computed, i.e. 2_52 for long

—9--

precision, 220 for short precision, on the IBM

360/370

machines.

2) Calculate the residuals r= Ax—b to observe the error

between the
From

the

true solution x and

Golub' s

computed solution x.

formulation

CK(A) + CK2(A)

ff-ff
a

in which

the condition number K(A)
amjn

The second term on the right-hand side is daminant
for least
solution

squares

problems. In seeking

the

candidate

of least norm we compute

UK

k

K(A)
HxkH

for different

choices of k. We could compute 11k

directly by forming ç and rk. However, taking
advantage of

ui I

= I lvi

1

it follows that

-

- 10

i/ m

21

2i\1/2
C.

E

() (k

1

'

where

2\/2

T)

c uTb. This formulation permits the

appropriate choice of the best approximating matrix
Ar UErVT from the minimum

without explicitly

computing the candidate solutions xk. The best approximate

solution is

obtained when 11k S minimum.

Frequently the question is raised alxut using
iterative methods for computing the singular

value

decomposition. There is an excellent discussion of
such issues in [8] along with suggestions for
constructing matrices with exact singular values.
Informally,

we suggest certain guidelines for

using MINFIT. Whenevçr
forming

the product

of

possible one should avoid

a matrix by its transpose.

Note that the eigenvalues A and eigerivectors X for the real
syrrinetric matrix

eigenproblem
AX = XA

are inunediately available from MINFIT

forming ATA. However, if
to

the

without ever

original problem is

obtain the eigensystem of a real symmetric

positive definite, negative definite, or indefinite

- 11

matrix, SYMEIG

(see

[7])

—

should be used. One should,

however, be warned that the appearance of zero or

believed to be

negative

elgenvalues for a matrix

positive

definite signals the need to analyze the

original

data

or the construction of the problem

nre carefully by obtaining the singular value
decomposition of the original data matrix.
MINFIT can be used

of

a

to

obtain the solution

linear system of equations. However, if the

matrix of coefficients is ]iown

to

have full rank, and,

if

the

to

the uncertainty in the data, one of the matrix

condition number of this matrix is small relative

factorization methods should be used. Such matrix

factorization methods
2)

the LU

are 1) the Choleski factorization,

decomposition with partial

or

complete pivoting

where the elementary transformations have been stabilized
by

row and/or coluim- interchanges, arid 3) the orthogonal

factorization with column pivoting. However, such

factorizations

cannot

be guaranteed to

give definitive

information about the condition number of a matrix.

Consider from [lL] the bidiagonal matrix of order 100
r5ol
I

—l

.502

—l

.509

—l
.600

- 12

-

This matrix is extremely ill-conditioned
of

with respect

to

the solution

a linear system of equations. Its smallest singular value is approximately

io_22 despite

the fact that its smallest eigenvalue is .501. This

matrix

also

shows that computation of the rllest eigenvalue is limited by the relative
finite precision of the machine on which it is computed. That is to say,
the small singular value, io_22 will appear computationally to be no smaller

than the order of machine precision. This result is not attributable to the
construction of the algorithm, but rather to the finite precision of the
'machine's arithmetic.

We suggest everywhere the use of long precision on the IBM 360/370
machines to compute the solutions of linear systems of equations, eigensystems,

and the

singular

value decomposition. Even so, we urge extreme

caution wherever the number of rows, m, or the number of columns, n, of a

matrix is of more than modest size, say 200, if the matrix is dense.
A - ,,
/
should be the order of machine precision.
The quantity
IIAIIuInax(m,n)
However,

the computational algorithms are, in general,

0(n3) or 0(mn2)

processes. We advise a rigorous analysis of the structure of a matrix

of

high dinens ions before

See

any of

the numerical algebra

algorithms are used.

Appendix C for son timing results on random matrices.

The singular values of a matrix can be substantially altered by scaling

the

original data matrix as is shown by the examples in Appendix C.

Deliberately, MThFIT does not include scaling of the rows or columns

of the matrix A or right-band sides b. For the best perfonnance of the
algorithm we suggest that columns of A be equili]riated such that the sums

of their elements be as nearly equal as possible. Exact powers of

— 13 —

16 for the 360/370 machines should be used for scaling

the

factors

so that

scaling will have the
effect of introducing weights on the data in a least squares problem and
therefore should be done at a user's discretion. An excellent discussion
data is not perturbed in trailing digits. Row

of scaling is in 16].

further points out in [6] that it is important to take advantage
of infoniation about the certainty of data. For example, if data is known
Lawson

to have uncertainty in the third decimal place, that digit and all that
follow are arbitrary. The matrix
11.02

[1.05
if

1.09
1.01

uncertain in the third figure could lead to
[1.00
1.00

1.00
1.00

L
The eigenvectors of a syrmietric matrix, and therefore, the singular

vectors

U and V from MINFIT

are known only to

within

a constant multiplier

of modulus 1. If anyone should attempt to recompute the results in
Appendix C

the

I1vI

on a machine

.fnose aritblrEtic is different from that of•

360/67 he may observe a change in sign on

The Fortran IV subroutine

the columns of U or V.

MINFIT, imbedded in TROLL (see [7]), that

forms the singular value decomposition and obtains a best approximate solution
vector x is an adaptation of ANLF233S from the Argonne National Laboratory.
ANLF233S written by Burton Garbow, ANL, is a Fortran IV translation, with

certain modifications, of the Algol 60 procedure MINFIT [3]. We have aunented

— lL

—

ANLF233S by adding cainnents and producing the numerically
best approxinate solution x based on a particular rank tolerance chosen for

the I1 360/370 long precision arithnetic. The machine epsilon, that is, the
smallest number, c >

0,

for which 1 + c > 1 is the floating point representation

of 16_13 = 2_52 for th IRI 360/370 machines. The comments and the Fortran IV

listing of the subroutine used at the Center is given in Appendix A. The
description of the parameters for the TROLL interface is given in Appendix B.

Appendix C contains selected ntrices, computed solutions, and residual
norm checks obtained fr driver programs that use the singular value

decomposition. These results were computed on

questions,

attention

the

IHI 360/67. Comments,

or criticims of this subroutine should be brought
of the authors of this working paper.

to the

— 15 —

References

1.

Eckert, C., arid Young, G. (1936) "The Approxiiration of One Matrix
by Another of Lower Rank," Psychometrika 1, 211-218.

2. Golub, G. and Kahan, W., "Calculating the Singular Values and
Pseudo-inverse of a Matrix," in J. SIAM. Numer. Anal. SER
B 2, 205—224 (1965).

3. Golub,

G. H. and Reinsch,

C., "Singular

Least Squares Solutions," in J.

Value

Decomposition and
C. Reinsch (eds.)

H. WiJidnson arid

for Automatic Computation, Volume II: Linear Algebra,
Springer Verlag, 134-151 (1971); prepublished in Numer. Math.

Handbook

14, 403—420 (1970).

4.

Hanson, R. and Lawson, C. L., "Extensions and Applications of
the Householder Algorithn for Solving Linear Least Squares Problems,"
Mathematics of Computation, vol. 23, no. 108, 787-812 (1969).

5.

Lawson, C. L.. "Applications of Singular Value Analysis" in John Rice
(ed.), Mathematical Software, Academic Press, Chapter 25 (1971).

6. Lawson, C. L., and Hanson, R. J., Solving Least Squares Problems,
Prentice-Hall, (1974).
7. Natiorial Bureau of conic Research, ThOLL Experimental Programs:
Numerical Methods,

8.

(1974).

Soderstrom, Torsten and Stewart, G. W., "On the Numerical Properties

of an Iterative Method for Computing the Moore-Penrose Generalized
Vol. 11, No. 1 (1974), 61-74.
Inverse, SIAM J. Numer. Anal

9. Stewart, G. W., Introduction to Matrix Computations, Academic Press,
380—387 (1973).

10.

Sylvester, J. J., Messenger of

Math 19, 42 (1889).

11. Wedin, Per-Ake, "Perturbation Bounds in Connection with Singular Value
Decomposition," BIT 12

(1972), 99—111.

12. Wedin, Per-Ake, "Perturbation Theory for Pseudo-Inverses," BIT
217—232.
13.

Wedin, Per-Ake, "On the Almost Rank Deficient Case of the Least Squares
Problem,"

14.

13 (1973),

BIT (1973) 344—354.

Wilkinson, J. H., The Algebraic Eigenvalue Problem, Clarendon Press
(1965), 195.

APPENDIX A: Listing of the Fortren IV Program MINFIT

C

SUBROUTINE MINFIT(NM,M,N,A,W,IP,B,IERR,RV1,RETX)

INTEGER I,J,K,L,M,N,II,IP,I1,KK,Kj,LL,L1,M1,NM,ITS,IERR
REAL*8 A(NM,N),W(N),B(NM,IP),RV1(N)
REAL*8 C,F,G,H, S, X,Y, Z ,EPS,SCALE,MACHP,RKTOL

REAL*8 DSQRT,DMAX1,DABS,DSIGN
LOGICAL RETX
C

C

THIS SUBROUTINE DETERMINES, TOWARDS THE SULUTION OF THE LINEAR

C

C
C
C
C
C
C
C

T

SYSTEM AXB, THE SINGULAR VALUE DECOMPOSITION A=USV OF 4 REAL
T

C

M BY N RECTANGULAR MATRIX, FORMING U B REATHER THAN U, HUUSEHOLDER
BIDIAGONALIZATION AND A VARIANT OF THE OR ALGORITHM ARE USED.
THIS SUBROUTINE COMPUTES A CANDIDATE SOLUTION X WHEN THE
LOGICAL INPUT PARAMETER RETX IS SET .TRUE. THIS CANDIDATE
SOLUTION IS BASED ON THE RANK TOLERANCE SET TO
2.ODO**(—26), THE SQUARE ROOT OF THE MACHINE PRECISION
2.ODO**(—52).

C
C

ON INPUT:

C

C
C

C
C
C
C
C

NM MUST BE SET TO THE ROW DIMENSION OF THE TWO—DIMENSIONAL
ARRAY PARAMETERS AS DECLARED IN THE CALLING PROGRAM
DIMENSION STATEMENT. NOTE THAT NM MUST BE AT LEAST
AS LARGE AS THE MAXIMUM UF M AND N;
M IS THE NUMBER OF ROWS OF A AND B;

C
C
C

N IS THE NUMBER OF COLUMNS OF A AND THE ORDER OF V;

C

A CONTAINS THE RECTANGULAR COEFFICIENT MATRIX OF THE SYSTEM;

C

C
C
C
C
C
C

C
C
C
C
C

C
C
C
C
C

IP IS THE NUMBER OF COLUMNS OF B. IP CAN BE ZERO;

B CONTAINS THE CONSTANT COLUMN MATRIX OF THE SYSTEM
IF IP IS NOT ZERO. OTHERWISE B IS NOT REFERENCED.
RETX MUST BE SET .TRUE. IF THE CANDIDATE SOLUTION X IS TO
BE COMPUTED. IF ONLY THE SINGULAR VALUE DECOMPOSITION IS
DESIRED, SET RETX .FALSE.
ON OUTPUT:

A HAS BEEN OVERWRITTEN BY THE MATRIX V (ORTHOGONAL) O THE
DECOMPOSITION IN ITS FIRST N ROWS AND COLUMNS. IF AN
ERROR EXIT IS MADE, THE COLUMNS OF V CORRESPONDING TO
INDICES OF CORRECT SINGULAR VALUES SHOULD BE CORRECT;

C
C

C
C
C

C

W CONTAINS THE N (NON—NEGATIVE) SINGULAR VALUES OF A (THE
DIAGONAL ELEMENTS OF S). THEY ARE UNORDERED. IF AN
ERROR EXIT IS MADE, THE SINGULAR VALUES SHOULD BE CORRECT
FOR INDICES IERR+1,IERR+2,.,,,N;

-A2-

C

T

B HAS BEEN OVERWRITTEN BY U B. IF AN ERROR EXIT IS MADE,

C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C

T

THE ROWS OF U B CORRESPONDING IL) INDICES OF CORRECT
SINGULAR VALUES SHOULD BE CORRECT;

IF RETX IS TRUE, W WILL CONTAIN THE DIAGONAL OF THE PSEUDOINVERSE
OF THE DIAGONAL MATRIX S. ANY SINGULAR VALUES THAT
ARE LESS THAN RKTOL TIMES THE LARGEST SINGLUAR VALUE ARE
SET TO ZERO IN THE PSEUDUINVERSE.
ALSO, THE SOLUTION X IS RETURNED IN B, REPLACING U B.
IERR IS SET TO
ZERO
FOR NORMAL RETURN,
K
IF THE K—TH SINGULAR VALUE HAS NOT BEEN
DETERMINED AFTER 30 ITERATIONS,
—1
IF THE MAXIMUM SINGULAR VALUE IS ZERO (INDICATING
A ZERO A MATRIX ON INPUT). ONLY SET IF
RETX IS .TRUE..

RV1 IS A TEMPORARY STORAGE ARRAY.

C

C
C
C

MACHEP IS A MACHINE DEPENDENT PARAMETER SPECIFYING
THE RELATIVE PRECISION OF FLOATING POINT ARITHMETIC
MACHEP = 1b.000**(—13) FOR LONG FORM ARITHMETIC

:::;::::::

C

C
C
C

DATA MACHEP/Z3410000000000000/
::::::::::
RKTOL, FOR THESE APPLICATIONS, IS THE SQUARE

C

ROOTOFMACHEP::::::::::::::

ç

DATA RKTOL/Z3A40000000000000/
;:w::::: HOUSEHOLDER REDUCTI[JN TO BIDIAGUNAL FORM ::::::::::
IERR = 0
G = 0.ODO
SCALE = 0.000

C

X = O.ODO
C

00 300 1= 1, N

L=I+1

RVLII) = SCALE * G
G
0.000
S
0.000
SCALE = O.ODO
IF (I .GT. M) GO TO 210
C
120

00 120 K
I, M
SCALE = SCALE + DABS(A(K,I))

C

IF (SCALE .EQ. 0.000) GO TO 210
C

DO 130 K =
A(K,I) =
= S +

I,

M

A(K,I) /
A(K,I)**2

SCALE

-A3-

130
C

CONTINUE
F a A(I,I)
Ga —DSIGN(L)SQRT(S),F)
H a F * G — S
F — G
A(I,I)
IF (I .EO. N) GO TO 160

C

L,
0.000

00 150 J =
S =

N

C

DO 140 K
I, M
S = S + A(K,I) *

140
C

A(K,J)

F=S/H

C

DO 10 K =
150

A(K,J) =
CONTINUE

I,

M

A(K,J)

*

+ F

A(K,I)

C

160 IF (IP .EQ. 0) GO TO 190
C

00 180 J =
S =

1,IP

0.000

C
170
C

I,

M
DO 170 K =
S = S + A(K,j) *

B(K,J)

F=S/H

C

00 180 K =
B(K,J) =
180 CONTINUE
C
190

200

I,

M

B(K,J)

I,

M
DO 200 K =
SCALE *
A(K,I)

+ F *

A(K,I)

A(K,I)

C
210

W(I) = SCALE *
G = 0.000
S =

G

0.000

SCALE = 0.000
IF (I .GT. M .OR. I .EQ. N) GO TO 290
C

220

DO 220 K = L, N
SCALE = SCALE + DABS(A(I,K))

C

IF (SCALE .EQ. 0.ODO) GO TO 290
C

00 230 K = L, N
A(I,K) = A(I,K) / SCALE
S =

230

S

+

CONTINUE

A(I,K)**2

C
F

A(I,L)

G = —DSIGN(DSQRT(S),F)

-A'-

H —F *G— S

A(I,L)

F —

G

C

240

00 240 K = L, N
RV1(K) = A(I,K) / H

C

IF (I .EQ. M) GO TO 270
C

L,
0.000

DO 260 J =
S -

M

C

DO 250 K = L, N
S = S + A(J,K) *

250

A(I,K)

C

260

00 260 K = L, N
A(J,K) = A(J,K)
CONTINUE

+ S

* RV1(K)

C

270
280

00 280 K z L, N
A(I,K) = SCALE *

A(I,K)

C

C
C

290
X=DMAX1(X,DABS(W(I))+[)AbS(Rv1(j)))
300 CONTINUE
::::w::: ACCUMULATION OF RIGHT—HAND TRANSFORMATIONS ::::::::::
:::::::::: FOR I=N STEP —1 UNTIL 1 DO ——
DO 400 II - 1, N
I =

N

+

1

—

II

IF (I .EQ. N) GO TO 390
IF (G .E0. O.ODO) GO TO 360
H = A(I,L) * G
C

320

DO 320 J = L, N
A(J,I) = A(I,J) / H

C

DO 350 J = L, N
S = 0.ODO
C

340

DO 340 K
L, N
S = S + A(I,K) *

A(K,J)

C

350

350 K = L, N
A(K,J) = A(K,J) + S *
CONTINUE

360

D0380J=L,N

380

A(I,J) = O.ODO
A(J,I) = O.ODO
CONTINUE

DO

A(K,I)

C

C

390

All,!) = 1.000
G = RV1(I)

L=I

400 CONTINUE
C

IF (M .GE. N .OR. IP .E0. 0) GO TO 510

-A5-

Ml •

M +

1

C

DO 500 I

Ml, N

C

00 500 J — 1, IP
B(I,J) = 0.000
500 CONTINUE

::::z:::8:DIAGONALIZATIONOFTHEBIDIAGONALFORM::::::::::

C

510 EPS • MACHEP * X
:::::::::: FOR K=N STEP —1 UNTIL 1 DO ——
C
DO 700 KK
1, N
N — KK
Ki
K • Ki + 1
ITS = 0
:::::::::: TEST FOR SPLITTING.
C
C

::::::::::

FOR L=K STEP —1 UNTIL 1 1)1) ——

520

C

00 530 LL • 1, K
Li = K — LL
L = Li + 1
IF (DABS(RV1(L)) .LE. EPS) GO TO 565
:::::::::: RV1(i) IS ALWAYS ZEKO, SO THERE IS NO EXIT
THRUUGH

C

THE BOTTOM UP- THE LUOP :::::::

IF (DABS(W(L1)) .LE. EPS) GO TO 540
CONTINUE
:::::::::: CANCELLATION OF RV1(L) IF L GREATER THAN 1 :::::::
C = 0.000
540
S
1.000
530
C

C

DO 560 1 = L, K
F = S * RV1(I)
RV1(I) = C * RVI(I)
IF (DABSIF) .LE. EPS) GO TO 565
G = W(I)
H = DSQRT(F*F+G*G)
W(I) = H

C=G/H

S = —F / H
IF (IP .EQ. 0) GO TO 560
C

550

DO 550 J = 1, IP
V = B(L1,J)
Z = B(I,J)
B(L1,J) = V *
B(I,J) = —Y *
CONTINUE

C

S

+ Z * 5
+ Z * C

C

560
565
C

CONTINUE

::::::::::TESTFORCONvERGENcE::::::::::

C

Z — W(K)
IF (L .EQ. K) GO TO 650
:::::::::: SHIFT FROM BOTTOM 2 BY 2 MINOR ::::::::::
IF (ITS .E0. 30) GO TO 1000
ITS = ITS + 1
X = W(L)

Y = W(K1)

-AS-

RV1(K1)
RV1(K)

G

H

F =

— Z) * (V + Z) + (G — H) * (G +

((V

G z DSQRT(F*F+1.000)
((X — Z) * (X + Z)
F

H)) / (2.ODO *

H

+ H * (V / (F + USIGN(G,F)) — H)) / X
::::::::::NEXTQRTRANSFORMATIUN::::::::::
C = 1.000

C

S = 1.01)0
C

DO 600 Il = L, Ki
Ii + 1
I
G =

V =

RV1(I)
W(I)

H=S*G

G =C *G
Z = DSQRT(F*F+H*H)

RV1(I1) =

Z

C=F/Z
S=H/Z
F=X*C+G*S
*
G = —x * S +
H=V*S
V=Y*c
G

C

C

570

DO 570 J = 1, N
X
A(J,I1)
Z = A(J,!)
A(J,I1) = X * C +
A(J,I) = —x * S +
CONTINUE

Z * S
Z * C

C

1 = DSQRT(F*F+H*H)
W(I1) = Z
ROTATION CAN BE ARBITRARY IF Z IS ZERO ::::::::::
IF (Z .EQ. 0.ODO) GO TO 580

::::::::::

C

580

C=F/Z
S=H/Z
F=C*G+S*Y
*
*
X =
+

y
—s
G
C
IF (IP .EQ. 0) GO TO 600

C

590

DO 590 J = 1, IP
V = B(I1,J)
Z = B(I,J)
B(I1,J) = V *
B(I,J) = —Y *
CONTINUE

C

+ Z *

S

S

+ 1

C

*

C

600

CONTINUE

C

C

* Y)

RV1(L) = 0.000
RV1(K) = F
W(K) = X
GO TO 520
::::::::::CONVERGENCE::::::::::

-A7-

IF (Z .GE. 0.ODO) GO TO 700

650
C

W(K) =
C

DO 690 J
1, N
A(J,K) = —A(J,K)

690
C

700 CONTINUE
IF (.NOT. RETX) GO TO 1001
C
Z

= O.ODO

DO 750 J = 1, N
X = W(J)
IF (X .LE. 1)

Z=X

GO

TO 70

750 CONTINUE
IF (Z .E0. 0) GO TO 999
C

800 J = 1, N
X
W(J) / Z
IF (X .LE. RKTOL) GO TO 7Y0
W(J) = 1.ODO / W(J)
GO TO 800
W(J) = 0.000
790
800 CONTINUE
DO

::::::::::FORMX(RETURNED!NB)::::::::::

C

DO900J=1,IP
C

DO 810 I = 1, N
RV1(I) = W(I) *
CONTINUE

810

B(I,J)

C

DO 890 I = 1, N
C
X = O.ODO

00 850 Ii = 1, N
X = X + A(I,I1)
CONTINUE

850

*

RV1(I1)

C

B(I,J) =

X

C

890

CONTINUE

C

900 CONTINUE
C

GO TO 1001
:::::::::: ERROR IF MAX SINGULAR VALUE = 0 ::::::::::
999 K=—1
C
:::::::::: SET ERROR —— NO CONVERGENCE TO A
C
SINGULAR VALUE AFTE( 30 ItERATIONS ::::::::::::
1000 IERR = K
1001 RETURN
C

C

END

APPENDIX B: TROLL Impleiintatiom of ?T\IFIT and Associated Output

The calling sequence for using the singular value decomposition
within the TROLL envirornnent is considerably different than that

the Fortran

for

subroutine listed in Appendix A. This is a consequence

of the basic design features of TROLL.

However

all computations are

actually performed by the routine listed in Appendix A.

The TROLL version of the singular value decomposition is a function

named

MINFIT. Since it

result,

is a function, it returns a single data file as its

and by TROLL convention it may not modify any of its arguments.

The format of the TROLL call to MINFIT is

result =
where

MINFIT

(A-matrix <, B-matrix <, code >>)

the <> indicate optional arguments.

Since we may desire several matrices as output from MThTFIT, the data

file returned as result may be made up of several matrices. The precise
result returned by Mfl\IFIT is controlled by the code parameter as described
in the following table for the linear system:

- B2 -

X B

A
1mm nxp

whereAUZVT
and W diagonal of

B-matrix omitted

B-matrix present

.0

illegal

X (nxp) (default)

1

V (nxri)

2

W (nxl)

Code

(default)

W (nxl)

.

3

V (nxn)

T

U B

(U B) 'pxn

The

correspondence between the TROLL parameters

and the

Fortran

is as follows:

parameters

Inrniediately

prior to TROLL call to Fortran routine

TROlL

Fortran parameter

(number of rows of A-matrix,
number of columns A-matrix)

Max

Number

of rows of A-matrix

NM

M

Number of columns of A-matrix

N

A-matrix

A

free storage
if B-matrix omitted then 0

W

else number

colunis of B-matrix

set

not
free

if

of

storage

code
0 or code omitted and B-matrix is
present then .TRUE. else •FALSE.

IP
IERR
RV1

— B3 -

After call of Fortran routine

If

IERR

is not zero then print appropriate error messag; otherwise

Fortran variable to be used as result

Code
0

B

1

A

2

w

3

B

{

(the solution X is formed

]

in B)

.W

lxn

or if B-matrix was specific [A]mm
B nxp

For more details on the use of the ThOLL function, see [7].
The following output is

of

the

result

of

perfoniiing the TROLL version

IvNFIT on the Longley data described in Appendix C. Row 1 of the

matrix contains W, rs 2 through 8 contain the V matrix, and row 9 is
(UTb)T.

M161T(IIIMc,,jyx,Iu,1(;i-yy,4)

CIJUIMN I

CLI.N ?

1 .,371-+06
—?. 3417—h

M.39OO+O4

?
4

—9.6034—0l

'}W
1

I

',
6
7
8
9

—?.4376F—04
—7.7734F—04

—6.2675—(J

CI;.IiN 3

CtILIIM'J 4

4.47-+U3

1 .4M-+t)3

CULIIMN ,

41 .65420fl
—. 1O3M—(4

I .u7—o

—.731 M—06

—. f7M6—0I
I .43'F—0?

—?.t1 1-—I)
7.844I—o1

—4.'4-—I)4

—1 .43706—03

—6.18 6,-—u1

I.34??6—O?

—h• 166-—01
—4.1466—0
—l .S461-—0?
—?.4A3?6+03

—7.854R1-—01

S.07??#—03
2.13R16—O?
—4.941?'-—Ol
—1 .773tSE+fli

6. t"6Yf—O4

—?.7M616—t)1

Y..991—01

—4.57946—03
—?.5151)6+05

?.08926—0?

4.6093F+04

.16O—6

4.h4—()
I.8H3F—0?

4.4) -—03
1 .h() 491-+0A

1ILIIMN 6

343?4-—fl4

CtI.IIM

1

3 6'fl4f-+Ofl

—t .OflOOF+()

3.41 I —)5

—1 .I0—O3 —4.flR6—)N

—Q.94,4F—(1
1 ."411 F—4

—1 .7fl4—fl? —4•,77-—fl7

?. '3'I-—fl3

—l .40946—07

6.06176—04

-1 .044?6—O1

1.fl76—fl',
l.0430k—07
' • I 1346—04
1 • 18906+03

—l.60."6—03
1 .)4'n-—r)1
?1 0.q'0001

APPENDIX C:

Selected Matrices, Computed Soluticris, and Illustrative Examples

This appendix displays a representative sample of matrices on which

the subroutine IT'TFIT has perford satisfactorily. The input matrices
and the output computathns have been retained on magnetic tape. The
format

the

of the printing was

full fifteen decimal

chosen

place

for convenience

output that

and does

not include

was produced by the long

ecisn computation on the machine. If anyone should attempt to reproduce

results on a machine whose

these

arithmetic or relative precision is different

fran that of the IBM 3 60/67 he may get output that is different from that

correct to the order

which we display. However, such results should be

of

machine

precision

on which the computation is perfonned.

Though we include certain

encoage

matrices of

the Hubert

senents,

we do not

their use as test matrices for software validation. The Hilbert

senents are not representable cactly in a computing machine unless

appropriate multipliers are used to preclude a perturbation on input of the
data. We have used such multipliers.

Other matrices exhibited are a 3x3 matrix that is contrived to display
infor!ation about near dependencies of rows or columns, a test matrix from
[l]* and [2] and a matrix suggested by Ed Kuh. The
representable

matrix from

[1]

is exactly

in the machine though it is ill conditioned with respect to

the solution of linear systems of equations. The matrix in [J shows the
dependence of the solution vector x on the rank tolerance that is chosen.

On the output that is

displayed, V

has its usual meaning, W contains

unordered singular values fran ML1FIT, P is an integer vector that

the

indicates the descending order of the singular values, MU contains

for

i=l, 2,. .

. ,n for each

right-hand side

the candidate solution of Axb.

and

c contains

indicator

IERR is the error

non-zero if the computation of any

singular value

iterations or if the maximum singular

value

uTb. X contains
from MINFTT;

it is

requires more than 30

is zero.

*NmEraJs in square brackets refer to entries in the Reference section, p. C114.

_C2 -

This 3x3 matrix shows output that indicates rank 2 if the smallest

singular value is treated as zero. Given this interpretation, columns

1 and 2 are linearly dependent. This infotion is contained in column 2
of the V matrix,
A.
4108

1

(1 • 1009A001) 01 0.98000000 0))

0.10101 001) 01

2 ):

ROW

0.10104001) 01

0.98000001) 00

0 • 9)4000000 00 I) .9800000)) 1)11

0 • 101 00001) 01

0.10098000 01
3

ROW

C0?.UMN

1)

(COLUMN

7)

(COLUMN

3)

0.0

0.1000000)) 01 0.0
0.0
0.0

v=
(COLUMN

1)

CIII. INN

2I

0.1000000)) 01 0.0

0.0

0.1000000)) 01

—11.57977491) 00 —0 .5793330)1 0>1 —0. 5734230)) 00

—0 • 70)401 190 00 0.70619)43)) 00 0.176066 1l)—0
3)
(COLUMN
—0.40393051) 00 —0.4)170101)) 00 11.81925761) 00
8=

0.7990101) 01 0.44980760—03 0. 39948830—tI 1
C.

I
CtIl_IIMN
—0.57927491) 00 —0. 70801 191) 0)) —0.40 493051) III)
COLUMN

7

(I. 70619831) 0)) —11 • 40701 Iii)) 00

—0.5793311)11 00

3)

(COLUMN

—0.5734? 11))) 00 (1.1 761)441 0—02

3

I

1). 8192576)) 00

2

Nil.
I COLUMN

1)

CI)I_IJUN

7)

COlUMN

3)

0.46(501 51) 02

11.418407601)

01) 11.54410011(1—1)2

0.461455 11) 02 0.41773891)) 00 11.56945950—02

0.466711))) 02 0.43594980 01)

0.4282218)) 00

IJS 1510 MAC.HSP, X=
C)i(.UMN
1)
0.1118630)) 04 —0.1107352)) 04 —0.10943611) 1)2
7)
CUIJJMN

—0.110735711 04 11.11129911) 1)4 —1). 5471803>) 01
(COLUMN

3)

—0.10943610 02 —11.5471803)) 01 0.16917921) 1)2

((SING RKIUL, X=
1)
(COLUMN
0.1 118631)>)
(COLUMN
2)

06 —0.11073521) 04 —0.1094361)1 11

—0.11073520 04 0.11129910 04 —)).54718031) 01
(COLUMN
3)
—0.1094361)> 02 —0.54718030 01 0.16917921) 02

- C3 -

The matrix whose data is displayed on the following page was suggested
by Ed Ith. The matrix is 32x10 and has singular values, to 4 decimal places,
4921, '41.89, 30.33, 18.71, 8.573, 2.491,

4.763, 5.532, 6.162, 6.091.

The

if

the

indicated rank determination

is

that

data is certain in all digits, of rank

the matrix is of rank 10
1 if the

third

digit is

doubtful.
The residual checks for the decomposition are

MAX—ROW—SliM lSjDtJAL =

UCLIDIAN RESIDUAL =
1AX—C(JL—SlJM RtSIDlJAL =

O.18182413271)—14
U .240?6Y7593D—14

O.1378022275D—14

Thuncation of the data to integers 234,231,... 3l1 gives singular values
to 4 decimal places.

'4911, '41.10, 30.07, 18.59, 8.356, 3,403,
6.299, 5.727, 4.963, 5.198

- C4

Data

for A

346.6
342.1
337.9
337.9
331.2
326.7
321.8
314.5
312.2
311.7
311.6
307.4
303.8
300.8
294.6
290.7
286.4
283.2
278.9
272.6
266.2
262.4
257.3
254.7
255.3
254.0
253.8
253.4
249.2
245.8
240.9
231.7
231.2
227.9
226.0
220.8
214.7
209.0
201.5
202.2

Right-hand

Row 32

234.4 ow 1

-

ow_31
iow 30

214.6
216.7
225.0
228.4
230.1
231.0
230.3
232.3
234.6
237.3
241.8
247.7
252.7
256.8
260.4
262.0
264.4
267.5
272.8
277.2
279.3
283.8
285.4
284.5
287.4
292.2
296.2
304.0
309.8
314.8
316.3
321.1

side

B

- C5

The

—

Hubert matrix of order 7, generated in long precision, 7 digits

of which are given for each element, is inexact in the machine.

a=
ROW

1

):

0.10000000 01
ROW

2

0.50000000 00
ROW

3

):
00
):

0.33333330
4
ROW
0.25000000 00

5 ):
( ROW
0.20000000 00
6 I:
I ROW

0.16666671) 00
ROW

7

0.50000000 00

0.33333330 00

0.2500000() 00

O.2000000D 00

0.1666667D 00

00

0.33333330 00

0.25000000 00

0.2000000') 00

O.1h666671) 00

0.142R571D 00

00

0.25000000 00

0.20000000 00

0.1,6h6671) 00

0.14285710 00

0.12500000 00

00

0.20000000 00

0.16666670 00

0.1428571') 00

0.12500000 00

0.11111111) 00

00

0.16666670 00

0.14285710 00

0.12500000 00

0.11111110 00

0.10000000 00

0.14285710 00

0.12500000 00

0.11111111) 00

0.11)000001) 00

0.90909090—01

):

):

Its singular values are

0.166088,I) 01 0.27192021)00 0.212875D—O1 0.100R588)—02

0.2R637I)—O4 0.4M5h74S31)—06 0.34937440—08

•

-

- CE

Multiplication of the Hubert matrix of order 7 by the constant
360360 allows a machine representation that is exact.

4=
1 ):
I ROW
0.36036000 06

ROW
2 I:
0.18018000 06
I ROW

0.72072000 05

0.60060000 05

0.51480000 05

0.7207200)) 05

0.6006000)) 05

0.51480000 05

0.45045000 05

0.72072001) 05

0.60060001) 05

0.5148000)) 05

0.45045000 05

0.40040000 05

0.18018000 06

0.12012000 06

0.12012000 06

0.90090000 05

0.90090001) 05

3 I:

0.12012001) 06

0.9009000)) 05

4

):

0.90090000

Q5

0.72072001) 05

0.60060000 05

0.5148000') 05

0.4504500)) 05

0.40040000 05

0.36036000 05

0.7207200)) 05

0.60060000 05

0.51480001) 05

0.4504500)) 05

0.4004000)) 05

0.3603600)) 05

0.32760000 05

0.51480000 05

0.45045000 05

0.400400(1')

05

0.3603600)) 05

0.32760001) 05
.

0.3003000)) 05

0.45045000 05

0.40040001) 05

0.3603600') 05

0.32760000 05

0.10030000 05

0.27720001) 05

I ROW
ROW

S

I:

6 ):
I ROW
0.60060000 05
7 I:
I ROW
0.51480000 05

S
Its singular values are

0.59851661) 06 0.97989161) 05 0.76719761) 04 0.36345461) 03 0.10589670 02 0.17501830 00 0.12590610—0?

.

- C7

The

I ROW

1

2
3

4
5

I:

6

I:

0.10000000 01
I ROW

7

1)4

0.15900001) 04

0.10760801) 06

0.19470001) ((4

0.88500000 02

0.25942600 04,

0.2325000')

(14

0.1456000)) 04

0.1086320)) 06

0.19480001) 04

0.8820000D 02

0.25805400 06

0.36820001)

1)4

)).1616000)) 04

0.109773(11) 06

0.19490001) 04

0.89500000 02

0.2845990)) 1)6

0.34510000 04

((.16500001) 04

0.1109290))

0.1950000))

0. 96200001) 02

0.32897501) 06

0.20990001)

1)4

0.3099000(1 04

0.1120750)) 06

0.1951000)) 04

0.98100000

02

0.3469990(1 06

0.1932000') 04

0.3594000)) 04

0.11327(10)) 06

0.1952(10(10 04

0.99000000
•

•

0.10000000 01
(ROW

0.2356)1000

'I:

0.10000000 01
I ROW

0.23428900 06

I:

0.10000000 01
I 80W

0.83000000 02

I:

0.10000000 01
I ROW'

Longley data matrix [3J with its associated output is

I:

0.10000000 01
I ROW

-

I

0.10000000 01

.

0,5

(14

02

0.3653850)) 06

0.18700000 04

0.3547000)) 04

0.1150940)) 06

0.19530000 04

0.10000000 03

0.36311200 06

0.35780000 04

0.43500001) 04

0.11621900 06

0.1954000)) 04

0.10120000 03

0.39746900 06

0.29041)001) ((4

l).'4048000)) 04

0.11738800 06

0.1955000)) 04

0.10460001) 03

0.41918000 06

0.28220001) 04

0.2857000)) 04

0.1187340)) 06

0.1956(100(1 04

0.10840000 03

0.44276901) 06

0.293601(0)) (4

0.2798000)) 04

0.1204450)) 06

0.1957000)) ((4

0.10000000 01

0.1108000D

03

0.4445460)) 06

0.468101)01)

0.1219500))

04,

0.1958000(1 04

I ROW
13 I:
0.1000000)) 01

0.11260000 03

0.4827040006

0.3813000(1 04

0.2552000)) 04

0.12336600 06

0.19590000 04

0.12536800 06

0.1960000)) 04

I ROW

8

I:

0.1000000)) 01
I ROW

9

I:

0.10000000 01
I NOW

10 I:

0.1000000)) 01
I ROW

ii

II
I:

0.1000000)) 01
I ROW

1?

14 I:
I ROW
0.10000000 01

04

((.26370001)

04

0.11420001) 03

0.50260101) 04

0.49130001) ((4

0.2514000))

0.11570000 03

0.5181730(1 06

0.4806000!) 04

0.25720001) 04

0.12785200 06

0.1961000D 04

0.10000000 01

0.11690000 03

0.55489401) 08

0.40071)001) ))4

0.28210000 04

0.13008100 06

0.19620000 04

8.
(COLUMN
1)
0.60323000 05
0.63761000 05
0.6933100D 05

0.61122000 05
0.6601900D 05
0.70S51000 05

0.60171001) 05 0.611870011 (iS 0.63221000 05 0.63639000 05 0.64989000 05
0.67857000 05 0.68169001) 1)5 0.66513000 05 0.6865500)) 05 0.6956400D 05

I ROW

15

I ROW

(ERR •

16

04

I:

0.10000000 01
I:

0

V.
(COLUMN
1)
—0.2341720D—05 —0.24375680—03 —0.96034010 00 —0.17733770—02 —0.62675480—02 —0.27861480 00 —0.45794090—02
(COLUMN
2)
0.10797810—04 0.61668600—03 —0.27878070 00 0.10334770—01 0.13421660—01 0.95997790 00 0.20891970—01
COLUMN
3)
—0.97316110—05 —0.49653610—03 —0.22179310—02 0.78543941) 00 —0.6186589)) 00 —0.48045890—04 —0.18466820—01
(COLUMN
4)
0.28160440—OS —0.14370220—02 0.46323930—02 —0.61856340 00 —O.7854783D 00 0.188828211—01 0.48025910—02
(COLUMN
5)
—0.5 103 8040—03 —0.10441850 00 —0.13504740—02 —0.17083690—01 0.80721570—02
0.2138102D—01 —0.99412300 00
(COLUMN
6)
—0.99999990 00 0.19307120—04 —0.30687980—07 —0.45777720—06 —0.13094330—06 0.10429930—06 0.51137880—03
(COLUMN
7)
0.34 180980—04 —0.99453210 00 0.19871480—03 0.230360811—02 O.60617261)—03 —0.16085630—02 0.10439200 00
W.

0.16636680 07

0.83899620 05 0.34056741) 04 0.15847881) 04 0.41654200 02 0.34322890—03 0.36503800 01

C.

(COLUMN
11
—0.15328510 00

0.45378580 00 0.89857260—01 0.26517570 00 0.30558470 00 0.11393900—01 0.15302070 00

—0. 4 84 652 90—0 1

—0.4020948D—02 —0.19433730 00 —0.34302910 00 —U.8976866))—O1 —0.11988820 00 —0.29862080 00
—0.29740200 00 —0.46626980 00
2

3

4

5

7

6

MU.

(COLUMN

1)

0.19577650 20 0.33345700 18 0.66973430 17 0.10643310 17 0.24581320 15 0.42386460 14 0.54338150 11
USING MACHEP, 8.
(COLUMN
1)
—0.3464269D 07 0.13849520 02 —0.35218390—01 —0.20094190 01 —0.10251330 01 —0.52347820—01 O.181994a0 04

USING AKTOI., 8.

(COLUMN
1)
0.23724110—01 —0.53035530 02 0.71033030—01 —0.42355560 00 —0.5715101D 00 —0.41366870 00 0.48394360 02

-C8The Bauer matrix

A.
ROW

I

0.8000000)) 02 0.18000001) I)? —0.11 000)0)1)

—0. 74000001) 02

ROW

2

I ROW

3

ROW

4

with its associated output is

0.14000000 02 —0.6900000)) 02 0.2100000)) 1?

12

01 —0.8000000!) 01

0.0

0.70000001) 01

01 0.7000000) III

0.1(100000))

0)

0.40000000 (11

0.66000001) 02 —0.3000000)) 12 —0.23000001) 0?

0.3000000))

01

—0.30000000 01

0.66000000 02 —0. 72000000 02 —0. 50000001)
—0.1 2000001) 02

0.281)001)01)

12 —O • 4)10)10000

5
I ROW
0.30000000 01 0.80000000 01 —0. 7000000)) ii —0.60000001) 1)1
6
ROW
0.40000000 01 —0.12000000 02 0.401)00000 I)) I) .4000001il) 01

0 •

11)000001) 01

0 • I)

0.0
0.10000000 01

B.
(COLUMN

1)

I CDL))MN

7

0.51000000 02 —0.6100001)!) 0? —0.5600000))

—0.56000001) 02
3)
(COLUMN

0.52000000 02

—0.50000000 01 —0.900000011
(1)88 •

02 0.69000001) 02 0.10000001) 02 —0.12000000

0.76400001) 03 0.409600!)!) 04 —0.1327600!) 05

0.8421000!) 04

.4165000)' 04 —0.13266000 05

0.84090000 04

0.70800001) 03

1)1

0?

1)

0

V.
1
COLUMN
0.53159590 00 —0.82429841) 00 0.38242860—01 0.17q49251) 00 0.10,76910—01 0.64390190—01
2)
(COLUMN
—0.62509500 00 —0.2981574)) 00 0.62845081) 00 0.3481670)) 1)0 —0.6)9569011—01 0.1049137)7—01
3)
(COLUMN
00 —0.33892800 00
0.33696201) 00 0. 1042175)) 00 0.65658481) 00 —O.'i?3)l)901) 0)) —0 • 2 3501 73!)
4)
(COLUMN
—0.40A2480 00 —0.4082483)) 00 —0.40824830 01) —0.408248311 (0) —0.41)824831) 00 —0.40874830 00
5)
(COLUMN
0.21539230 00 0.2325(74)) 00 —0.704591911—01 0.6062083)) 00 —0.6389627)) 00 —0.34469610 00
6)
(COLUMN
0.76299760—02 —0.64905330—02 —0.29192670—01 0.1949887!) 00 0 • 60467951) 00 —0.7716144)) 00

.W.

0.64861870 02 0.10667160 02 0.100000)))) 1)1

0.17383930 03
C.
(COLUMN

1)

(COLUMN

2)

0.1752477)) 00 0.47441820—04

—0.11457290 03—0.3566961002 —0.79211710 01 —0.4082483)) 00 0.891420800 00 —0.85865440—04
0.37040050—03 0.16174950—03 —0.31773030—01 —0.4082483)) 00 —0.19669080 01 —0.16264440 05

3)
(COLUMN
—0.11457260 03 —0.35669450 02 —0.79529440 01 —0.81649661) 00 —0.10687000 01 —0.16264440 05

P.

2

1

3

4

5

6

MU.
(COLUMN

1)

0.55143120 07 0.42339880 07 0.32020640 07
2)
(COLUMN
0.73068101) 15 0.14984950 15 0.16501570 12
3)
(COLUMN
0.74578680 09 0.57262870 09 0.4323217D 09

0 •

3013079)7 07 0.69023290 06 0.65274640 06

0.120392 51)
0 •

10 0.43763960 08 0.1433698)) 01

3511 520)) 09 0.78558700 08 0.14337370 01

USING MACHEP, 8.
1)
COLUMN

0.10000000 01 0.20000000 01 —0.10000000 01 0 • 3000000)) 01 —O.4000000D 01 —0.12249380—09
2)
(COLUMN
—0.26157640 07 0.2225142)) 07 0.1000810D OR —0.66847850 08 —0.2073018D 09 0.26453220 09
3)
(COLUMN
—0.26157630 07 0.22251440 07 0.1000810D 08 —0.66847850 08 —0.20730180 09 0.26453220 09

USING RKTOL, 8—
1)
(COLUMN

0.10000000 01 0.2000000D 01 —0.10000000 01 0 • 30000000 01 —0.40000000 01 —0.12249380—09
2)
(COLUMN
—0.26157640 07 0.22251421) 07 O.1000810D 08 —0.66847850 08 —0.20730180 09 0.26453720 09
3)
(COLUMN
—0.26157630 07 0.22251440 07 0.10008100 08 —0.668478 50 08 —0.20730180 09 0.26453220 09

.

- CS

The

—

condition number of a nonsingular matrix may be improved by

row or column scaling. The Bauer matrix, scaled as

4=
I ROW

1

1:

—0.74000000 02
8IJW

2

0.80000001) 02

):

0.14000000 02 —0.69000001) 02
ROW

3

O.3600000u 02 —0.33000000 02 —0.40000000 02 —0.80000000 02
0.42000000 02

0.84000000 02

0W

5

):

0.24000000' 02

I UW

6

0.70000001) 02

):

0.66000000 02 —0.72000000 02 —0.I000000D 02 0.21000001) 02
I ROW
4 ):
—0.12000000 02 0.66000000 02 —0.60000000 02 —0.6900000!) 02

)

0.64000001) 02 —0.11200000 03 —0.ThQO0OO) 02

0.28000000 02 —0.84000000 02

02

0.40000000 02

02 —0.30000000 02

0.80000000 02

0.0

0.56000001)

with singular values

0.29594490 03 0.18165700 03 0.48937800 02 0.12882170 02 O.70Q59950 00 0.13971070—02

— C].O —

singular value decomposition provides uzvT as the decomposition
of a matrix A. Given the orthononiial columns U and V one can form another
The

matrix uzvT for arbitrary E. Using U and V from the inexact Hubert matrix
of order 7, the reformed matrix

THE REFORMED A
I ROW
1 I:
0.20106490 02 —0.1119 1230
I ROW
2 I:
—0.11191230 0 0.13209940
.1 ROW
3 1:
0.23250410 03 —0 .472 8 00 10
I ROW
4 I:
—0.24893840 03 0.78498420
ROW
5 ):
0.1477433D 03 —0.67653190
6 I:
I ROW
—0.46373370 02 0.294907 10
1 1:
I ROW
—0 .5 155 58 10
0.60402080 01

03 0.23250410 03 —0.24893840 03 0.1411430 03 —0.46373310 02 0.60402080 01
04 —0.41280010 04 0.78498420 04 —0.67653190 04 0.29490710 04 —0.51555810 03

04 0.25364010 05 —0.58889481) 05 0.6799662D 05 —0.38558890 05 0.85824910 04
04 —0.58889480 05 0.18005690 06 —0.26360290 06 0.18470440 06 —0.49868680 05

04 0.61996620 05 —0.26360291) 06 0.47139410 06 —0.39302090 06 0.1238620!) 06
04 —0.38558090 05 0.18470440 06 —0.39302090 06 0.37926500 06 —0.13550680 06

03 0.8582491D 04 —0.49868680 05 0.12386200 06 —0.13550680 06 0.53689920 05

where the a1 are io8,

io_6, 1O, lO, lO and io_2.

The computed a1 from the reformed A are

0.10000000 07 0.10000000 06 0.10000000 05 0.10000000 04 0.10000000 03 0.10000000 01 0.10000000 02
MAX—ROW—SUM RESIDUAL =
EUCLIDEAN RESIDUAL =
MAX—COL—SUM RESIDUAL

0.12283894900—14
0.99904912580—15
0.12263894901)—14

-cli -

However, choosing aj

io21, io20, io16, io12, ion, iou, 100 gives

w.
0.10000000 2 0.10000000 21 0.10000000 17 0.2608)441) 08 0.10000011) 13
4AX—RUW—SUM 8ESIOUAL •
EUCI.10E*N RESIDUAL
4A*—COL—SUM RESIDUAL

The

O.21R9020

08 0.354464650 07

0.604675626030—16
0.4697620457L)—15
0.4045041 f35D—1

singular values

rial1er

tn io12

are

effected

by the order

of machine precision relative to amax
Choosing

0.

10000110 lii

ai

io0, io, io8, l0_12, io6, io20, l0_2 gives

U. 10000001)—Il

8AX140W—SIIM R4SjIIIi. •
EIlClII)iA4 RESIDUAl.
8AX—C(IL—Slh., 4,-Silpi •

u. 10000000—07 0.04 1711-12 0.1 1040820—15 1. lM6?77(lU—14, 0.?307j40—l 7

(I.1i160610?01)—1'.
0.681417H41531)—15

0.26 1)1071391)—I,

- C12 -

The order 100 matrix
.501

—l
.502

—l

—l

.600

has a maxinum singular

value

l.587 arid a miniuim singular value l022.

The minimi.nn singular value computed on the IBM 360/67 is .3 329410 xl05.

Using long precision on the IBM 360/195 at Argonne National Laboratory,
Jack Dongerra computed the same singular values as those from the 67
except for the rniniiinim singular value which was .33292721xl05. The
arithiretic of the 195 is not the sane as that of the 67. Multiplying
this matrix by 1O3 (so that the input was internally representable as

exact integers) gave the smallest singular value .3329095xl012.
Brian Smith suggested riiining this matrix on the 195 using short

precision

from which the sn11est singular

.l323073xl0

—2

value was .l287991xl05 and

.
for the matrix
scaled by 10 3

We have done some -timing tests on the singular value decomposition.

In general, accessing data is more costly than

computing

the singular

value

decomposition, so we u1d expect the use of Fortran H (opt:2) to reduce the
computation times listed below by about 50%. From a Fortran IV G compilation
on the 360 / 67 computer, the computation time for U, V, and using SVD from
[2]

on random square

matrices

of dimension N is as follows:

- C13

.1

Tiji

-

in seconds

5

.0714

10

.14614

20

3.1490

140

25.010

60

79.353

80

185.653

These times were obtained from the interval timer on the 67 which
gives approximate microseconds at 13 microsecond intervals. These
timings were obtained at the NB. Computer Research Center by Harry Bochner.
The time required by ML'JFIT is approximately that of SVD if U, V, and
E are computed. However, in general, U is not needed. The time that is
used to form V, , and UTb is therefore reduced by alinost 50% of the times

listed here.
The time for computation of the singular value decomposition will be
matrix dependent in that fewer iterations may be required when there are
multiplicities or clusters of singular values.

- Cl'4 -

References

F. L., "Elimination with Weighted Row Canbinations for
Solving Linear Equations arid Least Squares Problems," in
J. H. Wilkinson and C. Reinsch (eds.) Handbook for Autariatic

1. Bauer,

Corrutation, Volume II: Linear Algebra, Springer Verlag, 7,
338—362 (1965).

G. H. and Reinsch, C., "Singular Value Decomposition and
Squares Solutions," in J. H. Wilkinson and C. Reinsch (eds.)
Linear Algebra,
Handbook
Autaiatic Computation, Volume
Verlag,
134-151
(1971);
prepubli.shed
in
Numer. Math.
Springer
14, 403—420 (1970).

2. Goluh,

Least

3. Lcngley,

II:

for

James W., "An Appraisal

of Least

Squares

Programs

for

the E1econic Computer from the Point of View of the User,"
JASA 62, 819_8141, 1967.

.

.

