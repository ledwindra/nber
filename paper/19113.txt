NBER WORKING PAPER SERIES

INFORMATION AND STUDENT ACHIEVEMENT:
EVIDENCE FROM A CELLULAR PHONE EXPERIMENT
Roland G. Fryer, Jr
Working Paper 19113
http://www.nber.org/papers/w19113

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
June 2013

Special thanks to Karl Springer, superintendent of Oklahoma City Public Schools, for his support and
leadership during this experiment. I am grateful to my colleagues Lawrence Katz, Andrei Shleifer,
and Robert Jensen for helpful comments and suggestions. Brad Allan, Matt Davis, and Blake Heller
provided exceptional research assistance and project management support. Financial and in-kind support
from the Sandridge Foundation, Droga5, and TracFone Wireless Inc. is gratefully acknowledged. Correspondence
can be addressed to the author by email at rfryer@fas.harvard.edu. The usual caveat applies. The views
expressed herein are those of the author and do not necessarily reflect the views of the National Bureau
of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
¬© 2013 by Roland G. Fryer, Jr. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including ¬© notice, is given to
the source.

Information and Student Achievement: Evidence from a Cellular Phone Experiment
Roland G. Fryer, Jr
NBER Working Paper No. 19113
June 2013
JEL No. I20,J01
ABSTRACT
This paper describes a field experiment in Oklahoma City Public Schools in which students were provided
with free cellular phones and daily information about the link between human capital and future outcomes
via text message. Students‚Äô reported beliefs about the relationship between education and outcomes
were influenced by treatment, and treatment students also report being more focused and working
harder in school. However, there were no measureable changes in attendance, behavioral incidents,
or test scores. The patterns in the data appear most consistent with a model in which students cannot
translate effort into measureable output, though other explanations are possible.

Roland G. Fryer, Jr
Department of Economics
Harvard University
Littauer Center 208
Cambridge, MA 02138
and NBER
rfryer@fas.harvard.edu

In an effort to increase student achievement, a wide variety of innovative reforms have
been put forth by school districts across America. One particularly cost-effective strategy, not yet
tested in American urban public schools, is providing frequent information about the returns to
schooling. 1 Theoretically, providing such information could have one of three effects. If, as
Wilson (1987) argues, students lack accurate information and their expectations are lower than
the true returns, then providing information could motivate students to increase effort and
achievement. 2 Conversely, if students are more optimistic than historical returns suggest they
should be ‚Äì as Smith and Powell (1990), Avery and Kane (2004), and Rouse (2004) argue ‚Äì
providing information could lead to reduced effort and achievement. Finally, providing
information will likely have no effect on effort or achievement if students do not know the
production function, heavily discount the future, or already hold accurate beliefs about the
returns to schooling (Mickelson 1990, Fryer 2011b).
In the 2010-2011 school year, we conducted a randomized field experiment in Oklahoma
City Public Schools (1,470 treatment and 437 control students) that provided information to
students on the link between human capital and future outcomes such as unemployment,
incarceration, and wages. 3 In partnership with the largest pre-paid mobile phone provider in the
US and an internationally recognized advertising firm, we launched a campaign entitled ‚ÄúThe
Million,‚Äù designed to provide accurate information to students about the importance of education

1

Informational programs have been attempted in the United States to motivate students by providing accurate
information on the returns to schooling or ‚Äúrebranding‚Äù achievement. Since 1972, The United Negro College Fund
has run a series of PSAs promoting educational among low-income students with the ‚ÄúA Mind is a Terrible Thing to
Waste‚Äù campaign. Since 2000, with the lauch of ‚ÄúOperation Graduation,‚Äù the U.S. Army has sponsored Ad Council
media campaigns to encourage students to stay in school. Their most recent collaboration, Boost Up, follows the
lead of non-profit organizations like the Gates Foundation, using interactive web sites and online video in addition
to traditional visual and print media to engage youth and promote academic achievement among vulnerable
populations. The Gates Foundation‚Äôs ‚ÄúGet Schooled‚Äù campaign utilizes the influence of celebrities, partnering with
MTV, DefJam, and others to generate excitement around school improvement and implore students to stay in school
to reach their potential. While government agencies and non-profit organizations continue to invest millions of
dollars to engage youth through these informational campaigns and others, no rigorous evaluation of their effect on
student learning or other educational outcomes has been attempted.
2
Neal and Johnson (1996) argue that, if anything, the returns to test scores are higher for blacks than whites.
3
Throughout the text, I depart from custom by using the terms ‚Äúwe,‚Äù ‚Äúour,‚Äù and so on. While this is sole-authored
work, it took a team of dedicated project and finance managers to implement the experiment. Using ‚ÄúI‚Äù seems
disingenuous.

2

on future outcomes. 4 The key element of the experiment was a cellular telephone, pictured in
Appendix Figure 1.
Students in three treatment groups were given cellular phones free of charge, which came
pre-loaded with 300 credits that could be used to make calls or send text messages. Students in
our main treatment arm received 200 credits per month to use as they wanted and received one
text message per day delivered at approximately 6:00 P.M. 5 A second treatment arm provided
the same information on the link between human capital and future outcomes as well as nonfinancial incentives ‚Äì credits to talk and text were earned by reading books outside of school. A
third treatment allowed students to earn credits by reading books and included no information.
There is also a pure control group that received neither free cellular phones, information, nor
incentives. 6
On direct outcomes for students in the informational treatments, we examine students‚Äô
ability to answer specific questions about the relationship between human capital and outcomes
such as income and incarceration whose answers were sent to treatment students in text messages
during the year. Treatment effects are uniformly positive. Pooling across both informational
treatments, treatment students were 4.9 (2.7) percentage points more likely to correctly identify
the wage gap between college graduates and college dropouts, 17.9 (3.8) percentage points more
likely to correctly identify the relationship between schooling and incarceration, and 17.8 (3.8)
percentage points more likely to answer both questions correctly. As a robustness test, we
included a ‚Äúplacebo‚Äù question on the unemployment rate of college graduates, about which
students never received information. The difference in the probability of answering this question
correctly between informational treatments and the control group was trivial and statistically
insignificant. Moreover, 54 percent of control students believe that incarceration rates for high
school graduates and dropouts are ‚Äúno differen[t]‚Äù or ‚Äúreally close‚Äù, suggesting that students in
Oklahoma Public Schools do not have accurate knowledge of the returns to schooling.
4

Given the complexities involved in the field experiment, an operational pilot program was conducted in seven
public schools in New York City in the Spring of 2008.
5
When to send the text messages was an important experimental design question, for which theory provided little
guidance. We chose 6 P.M. because it was likely after students‚Äô extracurricular activities, but before dinner and bed
time. We chose not to send messages in the morning because the corresponding time window was less obvious.
6
The inclusion of the information and incentive treatment was to understand whether there might be important
complementarities between the two. If indeed the interaction were positive, it would be impossible to tell if this was
due to complementarities or the inclusion of incentives. The third treatment was designed to disentangle these
effects. In what follows, we combine the two information treatments for the purposes of exposition. Summary
statistics and results for each individual treatment arm can be found in the Online Appendix.

3

For indirect outcomes, such as state test scores, attendance, and self-reported effort,
results are mixed. Across the treatment arms, ITT estimates of the effect of treatment on selfreported effort are positive and statistically significant for both incentives and information arms.
For instance, students in the information treatment are 15.1 (3.7) percentage points more likely to
report feeling more ‚Äúfocused‚Äù or excited about doing well in school and 7.0 (3.7) percentage
points more likely to believe that students are working harder in school.
In stark contrast, on all administrative outcomes ‚Äì math or English Language Arts (ELA)
test scores, student attendance, or behavioral incidence ‚Äì there is no evidence that any treatment
had a statistically significant impact, though due to imprecise estimates we cannot rule out small
to moderate effects which might have a positive return on investment.
We demonstrate that our three facts ‚Äì providing students information on the returns to
schooling changes their beliefs, increases self-reported but not administrative measure of effort,
and has no impact on state test scores ‚Äì is robust to sample attrition and bounding, as well as
adjusting the standard errors on the treatment effects to account for the family-wise error rate.
The paper concludes with a simple model of human capital investment. In the model,
exerting effort in school incurs costs, but yields long-term benefits that increase with the return
to educational production. This yields simple equilibrium conditions from which we derive
comparative statics. The magnitude of our identified treatment effects depends on two features
of the model: the responsiveness of effort to the change in beliefs, and the shape of the
production technology around the pre-treatment equilibrium. We use this setup to frame our
empirical results and attempt to understand why beliefs changed, effort seemingly increased, yet
there were no tangible academic benefits.
We provide speculative evidence that the data is most consistent with a model in which
students do not know the education production function and thus are not sophisticated enough to
translate effort into measureable output. Moreover, a lack of knowledge of the particulars of the
education production function may also reconcile our results from those gleaned in developing
countries. In stark contrast to our results, Jensen (2010) and Nguyen (2008) report significant
treatment effects on educational attainment and achievement from implementing informational
experiments in the Dominican Republic and Madagascar, respectively. In our framework, higher
costs of investment lead to higher marginal productivity in equilibrium, following directly from
the first-order conditions. If the costs of investing in education are higher in less developed
4

countries, then under certain conditions investment will be more sensitive to changes in the
perceived return to education. The key assumption is that there is more ‚Äúlow-hanging fruit‚Äù for
students in developing countries.
Other theories such as high discount rates or complementarities in production all seem to
contradict the data in important ways. High discount rates are inconsistent with self-reported
effort increasing and we find no evidence of complementarities between information and teacher
quality, neighborhood quality (measured by poverty rates), or residential segregation.
In the end, however, we cannot provide definitive evidence on the underlying mechanisms that
produce the set of results. Much depends on the reliability of self-reported measures of effort.
The next section provides a brief review of the literature on how much students know
about the returns to schooling. Section II describes details of our field experiment aimed at
providing accurate information regarding the link between education and future outcomes.
Section III outlines our research design and details the data used in our analysis. The main
statistical results are presented in Section IV. Section V attempts to reconcile our results and the
data gleaned from similar experiments in developing countries with a range of potential theories.
The final section concludes. There are two online appendices. Online Appendix A is an
implementation supplement that provides details on the timing of our experimental roll-out and
critical milestones reached. Online Appendix B is a data appendix that provides details on how
we construct our covariates and our samples from the school district administrative files and
survey data used in our analysis.
I. A Brief Review of Related Literature
A growing body of research examines student perceptions of the value of education in the
US and abroad (Dominitz and Manski 1996, Avery and Kane 2004, Rouse 2004, Harris 2008,
Kaufmann 2009, Attanasio and Kaufmann 2009), as well as the effects of informational
treatments on educational outcomes in the developing world (Jensen 2010, Nguyen 2008).
Below, we describe each of these literatures in turn.
Survey Data on Attitudes and Beliefs
The anthropology and sociology literatures are divided on whether and the extent to
which minority or low-income students know the link between educational achievement and
5

future outcomes. Ogbu (1978) and Lieberson (1980) suggest that the historically discriminatory
job ceiling has led educated members of the black community to provide negative feedback
regarding returns to education. They hypothesize that this causes black students and their parents
to lower their expectations about the returns to educational attainment and question its
instrumental value. Using data from the 1990 National Education Longitudinal Study (NELS),
Ainsworth-Darnell and Downey (1998) question Ogbu‚Äôs (1978) oppositional culture explanation
reporting that black students are more likely than their white peers to report that education is
important to getting a job later on. 7
Economists have also documented similarities in the expected costs and benefits of
education across racial and income groups. Surveying a group of low-income, mainly minority
youth in Boston and a group of relatively affluent, white suburban students from a nearby
suburb, Avery and Kane (2004) find striking similarities between the perceived costs and payoffs
from attending college among members of these two groups. Similarly, Rouse (2004) finds little
evidence of differential expected returns to education between racial or socioeconomic groups,
but notes that high expectations in the low-income group are not as strongly correlated with
actual college enrollment as in the higher-income group.
Field Experiments in Developing Countries
The papers most closely related to the current project are from field experiments
conducted in the Dominican Republic (Jensen 2010) and Madagascar (Nguyen 2008). 8 Jensen
(2010) considers the role that the perceived returns to education play in students‚Äô schooling
choices. Jensen demonstrates that the eighth grade boys in his sample dramatically
underestimate measured returns to education. While the mean earnings of Dominicans who
finish secondary school are 40% higher than those who don‚Äôt, the typical student perceives that
his earnings will increase by only 9.2% if he completes secondary school. More importantly, a
7

To explain why blacks report more optimistic beliefs about the returns to human capital investment than their
white counterparts, Mickelson (1990) distinguishes between ‚Äúabstract‚Äù and ‚Äúconcrete‚Äù attitudes toward education.
‚ÄúAbstract‚Äù attitudes are defined as a respondent‚Äôs expressed beliefs about the general value of education in society.
‚ÄúConcrete‚Äù attitudes relate to a respondent‚Äôs expressed beliefs about the value of education and barriers to enjoying
its full value for themselves, personally. Consistent with Ainsworth-Darnell and Downey‚Äôs (1998) analysis,
Mickelson (1990) notes that in survey results, black respondents have ‚Äúabstract‚Äù attitudes toward education that are
similar to that of their white peers, but relatively less positive ‚Äúconcrete‚Äù attitudes, that are rooted in life experience..
8
See also Wiswall and Zafar (2012), who inform college students of the true income distribution by college
major/degree status. The authors find that this influences their beliefs about future earnings and intended major, but
they do not observe whether students actually change their behavior (e.g. switching majors).

6

random subset of students who received information on the real returns to education enrolled in
an additional 0.20 ‚Äì 0.35 years of high school, on average. 9
Nguyen (2008) also shows that providing information about returns to education to
parents and students can have a positive impact on academic outcomes, especially when parents
underestimate the value of schooling. Teachers in 80 randomly selected treatment schools
presented parents and students with information about the distribution of jobs and the expected
earnings of 25 year-old males and females in Madagascar by educational attainment level.
Nguyen (2008) finds that providing accurate statistics on the value of additional schooling to
parents and students in Madagascar raised test scores by 0.202 (0.106) standard deviations
(hereafter œÉ) and improved attendance by 3.5 percentage points. Test scores increased by 0.365 œÉ
(0.156) among those who underestimated the returns to education during a baseline survey.
Our paper makes three contributions to the current literature. Perhaps most importantly,
we conduct the first field experiment aimed at exploring the role of information on student
achievement in the US ‚Äì where the survey evidence is ambivalent as to whether minorities know
the true returns to human capital. Second, our message technology potentially improves on the
previous literature. While past efforts have relied upon pamphlets or one-time conferences to
distribute information, mobile technology allowed us to provide a multi-faceted stream of
information directly to students over the course of a school year. 10 Third, we inform students of
a variety outcomes that are correlated with educational attainment and achievement ‚Äì
unemployment, probability of incarceration, life expectancy ‚Äì rather than concentrating solely on
labor market returns. Conceptually, this may provide even more impetus to invest in human
capital.
II. Field Experiment Details

9

It is unclear whether Jensen‚Äôs treatment or the current approach is ‚Äústronger.‚Äù Treated students in Jensen‚Äôs sample
were read a single paragraph that cited the average salary earned by Dominican men with a primary education, a
high school education, and a college education. Our treatment provided daily messages over the school year on a
wider variety of returns (i.e. incarceration, unemployment, etc.) While it is possible that delivering the message in
person results in a larger change in beliefs, Karlan et al (2010) show that text messages can lead to measurable
changes in behavior in a different setting.
10
Karlan et al. (2010) use text message reminders to promote and incentivize monthly savings among bank
customers in Peru and Bolivia. They find that reminders coupled with incentives based upon account interest rates
increases amount saved and likelihood of reaching a savings goal.

7

Oklahoma City Public Schools (OKCPS) is a typical medium-sized urban school district - serving 42,567 students in eighty-nine schools. Seventy-seven percent of OKCPS students are
black, Hispanic, or Native American. Roughly 85 percent of all students are eligible for free or
reduced-price lunch and twenty-eight percent of students are English language learners. There is
a large racial achievement gap in OKCPS by 6th grade; within the twenty-two experimental
schools, black and Hispanic students‚Äô 2009-2010 test scores are 0.404œÉ (0.042) and 0.317œÉ
(0.044) behind their white peers in reading and math respectively, controlling for socioeconomic
status, free lunch eligibility, English Language Learner status, Special Education status, and
gender. This is consistent with overall national trends (Jencks and Phillips 1998, Fryer 2011a).
A. Description Of Treatment
Table 1 provides a bird‚Äôs eye view of the experiment. First, we ‚Äì together with local
philanthropists, TracFone (the mobile device provider), and Droga5 (an internationallyrecognized advertising agency) ‚Äì first garnered support from the district superintendent.
Following the superintendent‚Äôs approval, we held an information session for the principals and
instructional leaders of all twenty-two district schools with sixth and/or seventh grade students
that were not designated ‚Äúalternative education academies‚Äù to provide an overview of the
proposed experiment. All twenty-two eligible schools signed up to participate. At the end of
September 2010, information packets (containing a letter about the program to families and a
parent consent form) were distributed to principals and library media specialists (LMS) from the
twenty-two elementary and secondary schools. The LMS had been jointly determined to act as
school-based coordinators and help oversee implementation for a small stipend that was not tied
to performance.
Sixth and seventh grade students attending the twenty-two elementary and secondary
schools in OKCPS who signed up for the program were eligible to participate. 11 Students
received information packets on September 28, 2010 and were required to return a signed
consent form by October 1, 2010 in order to be eligible for the lottery that determined
participation. We received 1,907 student consent forms (out of a possible 4,810) and randomized
students into one of four groups: (Treatment 1) 490 students received a cell phone (pre-loaded
11

We chose sixth and seventh grade because they were old enough to have a cellular phone, but only 39% of
students in OKC had them. This number is almost double in urban centers such as New York City (where we
conducted the operational pilot), which makes OKC an ideal location on this dimension.

8

with 300 minutes) with daily informational text messages and a fixed allocation (i.e. nonperformance-based) of 200 credits on a monthly schedule; (Treatment 2) 490 students received a
cell phone (pre-loaded with 300 minutes) and daily informational text messages and were
required to read books and complete quizzes to confirm their understanding of those books in
order to receive additional credits; (Treatment 3) 490 students received a cell phone (pre-loaded
with 300 minutes) and were required to read books and complete quizzes about those books in
order to receive additional credits on a biweekly schedule; and (Control) 437 students did not
receive a phone, informational messages, or non-financial incentives. Sending three outgoing
text messages or talking on the phone for one minute or a fraction of a minute deducted one
credit from the student‚Äôs balance. Incoming text messages were free of charge.
Phones were distributed to each of the twenty-two schools on the morning of October 8,
2010. Students in treatments (2) and (3) were eligible to earn credits by reading books. Upon
finishing a book, each student took an Accelerated Reader (AR) computer-based comprehension
quiz, which provided evidence as to whether the student read the book. Each book in AR is
assigned a point value based on length and difficulty. Students were allowed to select and read
books of their choice and at their leisure, not as a classroom assignment. The books came from
the existing stock available at their school (in the library or in the classroom), though additional
copies of books that proved to be particularly popular were ordered during the year. This is
almost identical to the reading incentive program described in Fryer (2011b).
For those students required to read books in order to receive additional credits, the
incentive scheme was strictly linear: each point earned during each biweekly reward period
translated to ten credits which could be used to talk or text. Because credits could only be
distributed (i.e. uploaded electronically) in increments of 200, point earnings in excess of a
multiple of 20 were banked and carried over to subsequent reward periods. Once a student
reached or passed any 20 point interval, blocks of 200 credits were uploaded at the next
scheduled ‚Äúpayday‚Äù according to the predetermined biweekly reward schedule.
Text messages were sent to students in the appropriate treatment groups on a daily basis,
including weekends, at approximately 6:00 p.m. We worked closely with Droga5, an advertising
firm based in New York City, to determine the messaging and branding components of the
program. We met initially to discuss the types of text messages that would be written and sent to
students on a daily basis. Writing text messages throughout the year was a collaborative and
9

iterative process. Drawing upon advertising research suggesting that consumers respond to both
informative and persuasive messages (Nelson 1974; Mullainathan, Schwartzstein, and Schleifer
2008; Shapiro 2006) and recognizing our comparative advantage, Droga5 created the persuasive
messages and we created the informative messages based on information from the Bureau of
Labor Statistics, the National Center for Education Statistics, the Census Bureau, and other
sources. 12 Project teams met monthly to finalize upcoming text messages. Approximately 25% of
the sent messages were informational and 75% were designed to be persuasive. Approved
messages were sent to TracFone for distribution to students in Treatments 1 and 2.
Implementation Monitoring
Implementation of experimental protocols was monitored along several dimensions. First,
each school was visited and project managers reviewed the basics of the program with treatment
students to reinforce their understanding of the program details. To diagnose specific
misunderstandings of the reward algorithm or distribution system, brief quizzes were
administered to check for student understanding, covering topics including the incentive
structure, reward schedule, and how to report phone problems. After the first three months of
implementation, students answered 79% of quiz questions correctly. Second, administrative
access to the AR program enabled us to follow student usage on a daily basis for students in the
incentive treatments and produce and deliver program-, school-, and student-level dashboards
weekly. Third, every month, project managers conducted site visits to schools.
By the end of the experiment, 77 percent of students who received a phone and were
required to earn AR points in order to receive credits had earned at least a fraction of a point. 13
Twelve of the twenty-two schools had a rate of participation of at least 90 percent. The largest
and second largest schools (in terms of number of students with cell phones in incentivized
treatment groups) had participation rates of 65 percent and 75 percent, respectively.
In total, incentive and hardware costs were $230,365 for a program with 1,470 subjects in
treatment. Administrative costs were approximately $139,000, which includes AR registration
12

Examples of informational texts include ‚ÄúEach year, H.S. dropouts make $21,023. College graduates make
$58,613. Do the math‚Äù (United States Census Bureau 2011) and ‚ÄúHigh school dropouts are more than three times as
likely to be unemployed as college graduates‚Äù (Bureau of Labor Statistics 2011). Persuasive examples include
‚ÄúPeople don't look down on someone for being too educated‚Äù and ‚ÄúGraduates never regret staying in school, but
dropouts often regret leaving it.‚Äù
13
This figure includes the approximately 11 percent of students who exited the experiment during the year for a
variety of reasons: lost phone, moved out of district, etc.

10

fees, software installation, and a district-based program manager. Total cost of implementation
was approximately $369,365 ‚Äì or $251.27 per student (this does not include potentially billable
hours of the advertising firm.)

III. Data, Research Design, and Econometrics
A. Data
We collected both administrative data from all schools in OKCPS and survey data from
students in experimental schools. We begin with an overview of the administrative data.
Administrative Data
The administrative data includes first and last name, birth date, race, gender, free lunch
eligibility, behavioral incidents, daily attendance, matriculation with course grades, special
education status, English language learner (ELL) status, and Oklahoma Core Curriculum
Criterion Referenced Test (CRT) assessment data for math and ELA. We use administrative data
from 2008-09 and 2009-10 (pre-treatment) to construct baseline controls and 2010-11 (posttreatment) for outcome measures.
We observe results from the Oklahoma Core Curriculum Criterion Referenced Tests
(CRT) in math and ELA. For ease of interpretation, we normalize raw scores to have a mean of
zero and a standard deviation of one within grades and subjects (across all schools) for 20102011 scores, when they are used as outcomes in our analysis. Raw and controlled regressions
control for non-normalized scale scores from the two prior years as well as their squares. We do
not report testing results for 7% of students who take Oklahoma Modified Alternative Assessment
Program. Pooling the results for the two tests together does not change our findings, however.

Individual attendance rates account for all presences and absences for each student,
regardless of which school the student had enrolled in when the absence occurred, as long as the
student was enrolled in OKCPS. The attendance rate is calculated by dividing the number of
days present by the number of days a student was enrolled in the district during the 2010-2011
school year. 14

14

Oklahoma law requires that absences be recorded daily for both the morning and afternoon portions of the school
day. If a student misses more than one hour of school in the morning, he incurs a half-day‚Äôs absence. If he also
misses more than one hour of the afternoon, he is marked as absent for the day.

11

Behavioral incidents and (if applicable) suspensions are recorded individually by date of
infraction. Our measure of behavior is the total number of suspensions each student incurs during
the year, regardless of the length of the suspension or the nature of the infraction. Using the total
number of recorded infractions yields identical results.
We use a parsimonious set of controls to aid in precision and correct for any potential
imbalance between treatment and control. The most important controls are reading and math
achievement scores from the previous two years, as well as their squares, which we include in all
regressions. Previous years‚Äô test scores are available for most students who were in the district in
the previous year (See Table 2 for exact percentages of experimental group students with valid
test scores from the previous year). We also include a set of indicator variables that take a value
of one if a student is missing a given test score from the previous year and zero otherwise.
Other individual-level controls include a mutually exclusive and collectively exhaustive
set of race dummies extracted from each school‚Äôs district administrative files, indicators for free
lunch eligibility, special education status, and whether a student is an English Language Learner
(ELL). 15 Special education and ELL status are determined by the OKCPS Special Services office
and the OKCPS Language and Cultural Services Office, respectively.
Survey Data
To supplement each district‚Äôs administrative data, we administered a survey to all
students in the experimental group in each school. In total, 66 percent of student surveys were
completed and returned in experimental schools; 61 percent of control students and 68 percent of
treatment students completed and returned a survey. 16 We consider the possible implications of
differential attrition for our results in Section IV.
The data from the student survey includes questions about student motivations for
entering the experiment, phone use, phone problems and troubleshooting, student perceptions of
15

A student is income-eligible for free lunch if her family income is below 130 percent of the federal poverty
guidelines, or categorically eligible if (1) the student‚Äôs household receives assistance under the Food Stamp
Program, the Food Distribution Program on Indian Reservations (FDPIR), or the Temporary Assistance for Needy
Families Program (TANF); (2) the student was enrolled in Head Start on the basis of meeting that program‚Äôs lowincome criteria; (3) the student is homeless; (4) the student is a migrant child; (5) the student is identified by the
local educational liaison as a runaway child receiving assistance from a program under the Runaway Youth and
Home Youth Act.
16
More specifically, 70 percent of students in the information only treatment, 69 percent in the information plus
non-financial incentives treatment, and 65 percent in the non-financial incentives only treatment completed and
returned student surveys.

12

school-wide impact, and homework completion. In addition, the survey included questions that
quizzed students on specific facts about the importance of education that were delivered via text
message to students in the informational treatment arms during the year. For instance, we asked
students ‚ÄúAre high school dropouts more likely to go to prison than high school graduates?‚Äù,
which referenced the text messages ‚Äúmale high school dropouts go to prison four times more
often than men who went to college‚Äù and ‚Äúhigh school dropouts are 3-4 times more likely to go
to prison than high school graduates.‚Äù The survey also asked ‚ÄúTrue or false: college graduates
make 54% more money than college dropouts‚Äù ‚Äì a statistic pulled directly from an earlier text
message. The last question asked for the unemployment rate of college graduates. This figure
was not referenced in any text message, and is therefore a placebo question for which we expect
zero effect.
Table 2 provides descriptive statistics of all 6th and 7th grade students in OKCPS,
divided (not mutually exclusively) into five columns: students in eligible schools who did not
choose to participate in the experiment (column 1); students who opted into the experiment
(column 2), students randomly selected into the informational treatments (column 4); students
randomly selected into the incentive only treatment (column 5); and a pure control group
(column 6). 17 Each column provides the mean and standard deviation for each variable used in
our analysis (see Online Appendix B for details of how each variable was constructed).
As students could opt in to the randomization, there are some statistically significant
differences between participants and non-participants. Participating students are 3.5 percentage
points more likely to be female and 3.7 percentage points more likely to be white. They are also
poorer on average ‚Äì 91.7% of participating students are eligible for free or reduced price lunch,
relative to 85.7% of non-participants ‚Äì and roughly 10 percentage points more likely to have
valid baseline testing data.
Within the experimental group, the treatment groups and the control group are wellbalanced, although the control group has more male students (p = 0.03). A joint significance test
yields a p-value of 0.436, suggesting that the randomization is collectively balanced along the
observable dimensions we can consider.
B. Research Design
17

Descriptive statistics for each individual treatment group can be found in Appendix Table 1.

13

There is an active debate as to which randomization procedures have the best properties
under different circumstances (e.g. Greevy et al. 2004, Bruhn and McKenzie 2009, Imai et al.
2009, Imbens 2011, Kasy 2012). In samples with more than 300 units, Bruhn and McKenzie
(2009) provide evidence that there is little gain from different methods of randomization over a
pure single draw. Consistent with this, we used a pure single random draw to sort the 1,907
students who turned in consent forms into treatment and control.
C. Econometric Model
To estimate the causal impact of each treatment, we estimate Intent-To-Treat (ITT)
effects, i.e. differences between treatment and control group means for each treatment arm. Let
Zi be an indicator for assignment to a given treatment arm that takes a value of one if a student is
in that treatment group and a value of zero if a student is in the control group. Let Xi be a vector
of baseline covariates measured at the individual level; Xi and a school fixed effect Œ≥i comprise
our set of controls. Given our research design, results with or without controls are virtually
identical. Controls are included to aid in precision. All regressions without controls are available
from the author by request.
The ITT effect, œÄ, is estimated from the equation below:
ùëúùë¢ùë°ùëêùëúùëöùëíùëñ = ùõº + ùëãùëñ ùõΩ + ùõæùë† + ùëçùëñ ùúã + ùúñùëñ,ùë† .

Each ITT estimate is an average of the causal effects for students who were randomly selected
into a given arm of treatment at the beginning of the year and students who signed up for
treatment but were not chosen. In other words, ITT provides an estimate of the impact of being
offered a chance to participate in a given arm of the experiment. All student mobility and
disruptions in phone service due to theft, loss, or malfunction is ignored. 18 We only include
students who were enrolled in OKCPS as of the date of randomization, October 4, 2010. In
OKCPS, school began on August 19, 2010; students in the incentive treatment were eligible to
earn credits as of October 11, 2010.

18

Roughly 27% of our sample either lost their phone or experienced technical problems that prevented them from
receiving text messages for part of the year. Hence, there is some variation in the treatment dosage after random
assignment. As a separate specification, we also estimate two-state least squares models in which we use the
treatment assignment to instrument for the percentage of the year in which a student had a working phone. We
report only ITT estimates in the text and put 2SLS results in Appendix Table 2.

14

IV. Results
In this section, we describe the main results of our experiment across three domains.
First, using survey data, we investigate the effect of daily text messages about the link between
human capital and outcomes on the average students‚Äô knowledge of similar correlations as well
as heterogeneity of treatment effects for various predetermined subgroups. Second, we examine
two additional survey outcomes meant to capture effort. Finally, we estimate the effect of
providing more information on test scores, behavior, and attendance collected from the district‚Äôs
administrative files. 19
A. Direct Outcomes
Knowledge of the Link Between Human Capital and Future Outcomes
Recall, to assess whether students better understood the link between human capital and
outcomes, we asked them questions for which students in the informational treatments received
multiple text messages with the answers throughout the year and a ‚Äúplacebo‚Äù question designed
to test whether treatment students became generally more knowledgeable about returns to
education or whether they only retained knowledge about the specific information they were
provided. The two questions students in the information treatments were provided information
about via text message were: (1) ‚ÄúTrue or false? College graduates make 54% more money than
college dropouts.‚Äù and (2) ‚ÄúAre high school dropouts more likely to go to prison than high
school graduates?‚Äù The placebo question was ‚Äú15.5% of high school dropouts are unemployed.
What percentage of college graduates are unemployed?‚Äù
Table 3 presents treatment effects on students‚Äô ability to correctly identify links between
human capital and life outcomes, which are positive for the informational treatment arms.
Students were 4.9 (2.7) percentage points more likely to correctly identify the wage gap between
college graduates and college dropouts [control mean = 81.9 percent], 17.9 (3.8) percentage
points more likely to correctly identify the relationship between schooling and incarceration
[control mean = 45.9 percent], and 17.8 (3.8) percentage points more likely to answer both
questions correctly [control mean= 39.4 percent]. Students in the information treatments were no
more likely to answer the placebo question correctly, further suggesting that improved
19

For expositional purposes, we focus our discussion in the text on the regressions that pool the information
treatments together and include our parsimonious set of controls. ITT estimates for each treatment arm can be found
in Appendix Table 3. Results without controls are displayed in Appendix Table 4. All findings are unchanged.

15

knowledge is a result of the experiment. Moreover, 54.1 percent of students underestimated the
relationship between educational attainment and incarceration, which implies that students in
OKCPS do not have accurate information about the returns to schooling.
B. Indirect Outcomes
Survey Outcomes
We gleaned two measures of effort from our survey. The results reported in Table 4
assess the impact of each treatment on students‚Äô self-reported measures of engagement and
academic behavior. Students were asked questions about the impact of the program, such as
‚ÄúSince the Million program started, do you think you are more focused on or excited about doing
well in school?‚Äù and ‚ÄúWhat impact do you think the Million program has had at your school?
(check all that apply).‚Äù
Students in the information treatments are 15.1 (3.7) percentage points more likely to
report feeling more focused or excited about doing well in school and 7.0 (3.7) percentage points
more likely to believe that students are working harder in school as a result of the treatment.
Similarly, students in the incentives only treatment were 15.2 (4.3) percentage points more likely
to report feeling more focused or excited about doing well in school and 7.7 (4.4) percentage
points more likely to believe that students are working harder in school as a result of the
treatment. Put together, students self-report being ‚Äúmore focused‚Äù and working harder across all
treatments, with no significant differences across the information or incentive arms. 20
Administrative Data Outcomes
Panel B of Table 4 presents ITT estimates of the effect of each treatment on state math
and ELA standardized test scores, attendance, and behavioral incidence. Test scores are
normalized by grade level and subject to have a mean of zero and a standard deviation (œÉ) of one

20

Ideally, we would like to disentangle the effects of the informative and persuasive text messages by regressing
outcomes on the separate counts of each type of message. For students who received all the texts, these measures
are perfectly collinear and hence not identified. However, among students who lost or broke their phone during the
experiment, there is some variation in the portion of messages received due to the (plausibly random) timing of these
interruptions. In Appendix Table 5, we regress our main outcomes on the percentage of each type of message
received, limiting the sample to students who missed at least one message. The results are very imprecise, but they
suggest that the information text messages had their intended effect. The effect of the information dose is larger
than that of the persuasion dose on all non-placebo quiz outcomes, for instance, though none of the coefficients are
statistically differentiable.

16

within the full OKCPS sample. Treatment effects are reported in œÉ units and standard errors are
presented in parentheses below each estimate. Attendance is measured as a proportion of days
present in OKCPS divided by days enrolled and is then normalized to have a mean of zero and a
standard deviation of one. Total suspensions are counted and summed for each student.
Across the three treatment arms, there are no statistically significant treatment effects on
any administrative outcomes, though due to imprecise estimates we cannot rule out small to
moderate effects which might have a positive return on investment (the experiment was designed
to detect 0.15œÉ effects with eighty percent power). The effect on ELA achievement 0.040œÉ
(0.041) for the information treatment and 0.023 (0.050) for the incentive treatment. The ITT
effects on math achievement are -0.027œÉ (0.039) and -0.023 œÉ (0.050) for the information and
incentive treatments, respectively. Similar results obtain for attendance and behavioral incidence.
To assess heterogeneity in treatment effects across subgroups of students, Table 5 reports
treatment effects for the information treatment on a subset of direct and indirect outcomes for a
number of predetermined subgroups. 21 For ease of comparison, the first row of Table 5 shows
the ITT estimate for the sample for whom we observe the demographic data used to create the
subgroups. These estimates are nearly identical to the full-sample estimates in Tables 3 and 4.
The final row in each panel reports a p-value on the null hypothesis of equal treatment effects
within the panel.
There are few consistent patterns of heterogeneity. Male students show a much larger
increase in the probability of answering both quiz questions correctly (25.2 (5.4) percentage
points vs. 8.5 (5.4) for females.) However the treatment seems to reduce males‚Äô math scores by
0.123œÉ (0.059). Students who are not eligible for special education accommodations are 19.8
(4.0) percentage points more likely to provide two correct quiz answers, while students who are
eligible are 4.8 (13.6) percentage points less likely. There is no observable heterogeneity along
measures of baseline ability.
C. Robustness Checks
Sample Attrition and Bounding
If students selectively exit the sample, then the treatment effects we reported above may
be biased. A standard test for attrition bias is to check for differential response rates among
21

Subgroup results for the incentive-only treatment are available from the author upon request.

17

treatment and control groups. In Table 6, we regress an indicator for obtaining a response on our
main outcome measure on treatment dummies and our full set of controls. While we find no
evidence of differential attrition on test score outcomes, students in the information treatments
are 5.8 (2.1) percentage points more likely to provide valid survey data. Similarly, students in
the incentive treatment are 7.0 (2.4) percentage points more likely to respond.
Conceptually, the direction of the potential attrition-induced bias is unclear. If the
students in the treatment who gleaned more valuable information are more likely to respond to
our survey, then the estimates in Table 3 may be biased upward. If, on the other hand, these
students naturally absorb more information and put forth more effort, then our estimates would
be too low.
In Table 7, we use two methods to explore the extent to which differential survey attrition
between treatment and control can account for our set of results: (1) by calculating Lee (2009)
bounds and (2) by imputing missing outcomes for students who did not respond to the survey.
Given we have flat priors on the direction of the bias, we present both upper and lower bounds
using the methods described in Lee (2009).
The bounds in Columns (2) and (4) are generated by trimming the sample to equalize
response rates between the treatment and control groups. To estimate a lower bound, the sample
is trimmed by dropping the fraction of treatment students who have the largest predicted
residuals from a regression of the survey outcome of interest on baseline test scores and
demographics. Samples for upper bounds are created analogously. We then re-estimate our main
ITT specification on the resulting sample.
Column (6) of Table 7 reports the treatment coefficients after imputing outcomes for
students in the experimental group who did not respond to a given survey question. We impute
missing outcomes for all non-respondents using the full set of baseline data and any available
outcome variables. If attrition is uncorrelated with unobservable characteristics, this method is
equivalent to imputing a treatment effect of zero for any unobserved outcomes.
Both exercises confirm the robustness of our results. In the information treatments, the
Lee lower bounds for two coefficients ‚Äì knowing the wage gap and believing that Million makes
students work harder ‚Äì are no longer statistically significant. The other three survey estimates all
maintain p-values below 0.01. As expected, imputing unobserved values shrinks treatment
effects towards zero, but all remain statistically significant. Throughout, none of the attrition18

adjusted coefficients are statistically distinguishable from the main ITT results for all direct
outcomes, suggesting that differential survey attrition is not an important factor for our results.
A final concern is that our single-comparison tests do not correct for biases introduced by
testing multiple hypotheses. The p-values on our main outcomes with positive treatment effects
‚Äì answering both quiz questions correctly and self-reported focus‚Äì are both less than 0.001, and
hence survive even the most conservative methods to adjust for multiple-comparisons bias.
V. Discussion and Speculation
The experimental results provide us with three facts. First, receiving information via text
message causes students to update their beliefs about the returns to education and their updated
beliefs are more ‚Äúcorrect.‚Äù Second, students report that they increased their effort by working
harder and remaining more focused in school. Third, there was no measurable increase in
educational attainment or achievement.
To better understand what mechanisms might lead to these conclusions, we propose a
simple two-period model of human capital investment and consider the conditions that could
generate these facts. This section is, by necessity, more speculative than our previous analysis.
Consider the problem of a representative student choosing the optimal level of effort E to
invest in her studies. 22 The production function for academic achievement follows A=F(E,K)
where K is an n-dimensional vector of school, neighborhood, and family ‚Äúcapital‚Äù levels that are
fixed prior to the student‚Äôs decision. We impose the following restrictions: (a) F() is twice
continuously differentiable in all inputs (b) production exhibits diminishing marginal returns to
effort ‚Äì i.e. ùúïùêπ/ùúïùê∏ > 0 and ùúï 2 ùêπ/ùúïùê∏ 2 < 0 -- and (c) capital and effort are complements ‚Äì i.e.

ùúï 2 ùêπ/ùúïùê∏ùúïùëòùëñ > 0, where ki is the ith element of the vector K.

Academic achievement yields long-term benefits in the forms of higher wages, increased

employment opportunities, and other social opportunities. Let V(A;r) denote the long-run
benefits of achievement, where r is a parameter that measures the student‚Äôs perceived return to
achievement. We assume that ùúïùëâ/ùúïùê¥ > 0 and ùúï 2 ùëâ/ùúïùê¥2 < 0. Increases in r increase payoffs at
all levels of A: ùúïùëâ/ùúïùëü > 0.

22

Here we do not differentiate between academic achievement and attainment. This is in part due to empirical
necessity, as we will not know whether the intervention encouraged students to stay in school longer for several
more years. As a theoretical matter, the intuition provided in this section still holds so long as students do not
substitute academic effort for additional years in school.

19

The student‚Äôs problem can then be summarized as:
maxE ùõΩùëâ(ùê¥; ùëü) ‚àí ùê∂(ùê∏)

where C(E) is the cost of effort and Œ≤ is a standard discount factor. Assume that C‚Äô(0) = 0 and
F‚Äô(0, K) > 0 to ensure an interior solution. The equilibrium level of effort is then defined by the
value E* that solves:
ùõΩ

ùúïùëâ
ùúïùê¥
ùúïùëâ
(ùëü) = ùõΩ
(ùëü) ‚àó
= ùê∂ ‚Ä≤ (ùê∏).
ùúïùê¥
ùúïùê∏
ùúïùê∏

In what follows, we use this simple model to frame a discussion of explanations for our set of
facts. In this admittedly limited framework, there are three potential mechanisms to generate a
change in beliefs without a change in achievement: discount rates, complementarities in
production, and uncertainty about the production function.

A. High Discount Rates
The key challenge in interpreting our results is explaining why academic achievement did not
increase despite the change in perceived returns. If the benefits of education occur primarily in
the future, then excessive discounting could explain this paradox. In other words, even if the
information treatment causes students to foresee additional rewards for investing in their
education, the payoff arrives so far in the future that it is not worth expending effort in the
current period. 23 In our framework, this is equivalent to having Œ≤ small enough that ùúïùê∏ ‚àó /ùúïùëü is

roughly zero.

The data in favor of this hypothesis is mixed. While high-discount rates are consistent
with student achievement remaining flat even after an increase in r, it is inconsistent with survey
results that indicate treatment students expended additional effort as a result of the field
experiment. Recall that treatment students reported being ‚Äúmore focused‚Äù and were more likely
to believe that the intervention caused students to work hard. Taken at face value, these results
indicate that students increased their effort due to the information intervention that is inconsistent
with explanations driven by high discount rates.

23

A slightly different interpretation is that students lack self-control ‚Äì i.e. they recognize that effort will result in
large benefits in the future, but cannot commit to studying, going to class, etc. The empirical predictions of this
model are identical to the discount-rate explanation.

20

Conversely, other (administrative) proxies for effort ‚Äì such as attendance ‚Äì show no
treatment effects. Importantly, whether one believes that present-bias can explain all or a portion
of the results depends on the reliability of self-reported measures of effort in surveys. How much
should believe self-reported measures of effort and is it an interesting outcome?
The answer to the first question is exceedingly difficult without a ‚Äútrue‚Äù measure as a
comparison, though the evidence in the health literature is mixed (Clarke and Ryan 2006,
Johnston, Propper, and Shields 2009). 24 To provide some evidence on the importance of selfreported academic effort as an outcome, we turn to the National Longitudinal Survey of
Adolescent Health (Add Health). The baseline survey collected rich baseline data on students in
grades 7-12 during the 1994-1995 school year, including an in-school effort that elicited attitudes
about academics and school. The final wave re-surveyed these students as adults (between the
ages of 24 and 32), allowing us to correlate self reported effort of middle and high school
students with longer-term economic and social outcomes.
Our measure of self-reported effort draws on students‚Äô responses to the question ‚ÄúIn general,
how hard to you try to do your school work well?‚Äù Students responded on a 1-4 scale, with 4
indicating ‚ÄúI try very hard to do my best‚Äù and 1 ‚ÄúI never try at all.‚Äù We standardize this measure
to have mean zero and standard deviation one.
In Appendix Table 6, we regress various adult outcomes on self-reported effort. Column (2)
reports raw correlations that include only fixed effects for school and grade of enrollment at the
time of the survey. Column (3) adds controls for race, gender, mother‚Äôs education, father‚Äôs
education, the number of biological parents living with the student, and the student‚Äôs score on the
Add Health Picture Vocabulary Test (AHPVT), an abridged version of the Peabody Picture
Vocabulary Test.
The results in Appendix Table 6 demonstrate a fairly robust correlation between self-reported
effort and adult outcomes. In our controlled specification, students with one standard deviation
higher reported effort are l.3 (0.5) percentage points more likely to be employed, 1.6 (0.5)
percentage points less likely to receive welfare or public assistance, 5.2 (0.6) percentage points
less likely to have ever been arrested, 3.2 percentage points less likely to have ever been
24

Dunifon and Duncan (1998) provide consistent evidence for an adult population using the Panel Study of Income
Dynamics. Their effort measure is constructed from a series of questions that solicit preferences for ‚Äúchallenges‚Äù or
‚Äúaffiliation.‚Äù Those who prefer challenges earned higher wages during follow-up surveys five and twenty years
later, even when controlling for baseline earnings.

21

incarcerated, and 1.7 (0.7) percentage points more likely to be married at the time of the followup survey. All of these results are statistically significant. The effect on annual income is only
marginally significant: $1,131 (667), relative to the sample mean of $34,021.
While these results cannot speak directly to the reliability of self-reported effort and do not
necessarily identify a causal relationship, they suggest that self-reported effort captures
something that may be informative beyond test scores. Even after controlling for standardized
test scores and family background, these responses strongly predict a wide variety of economic
and social outcomes.
B. Complementary Inputs
A second interpretation that may explain our findings is that the educational production
function has important complementarities that are out of the student‚Äôs control. For instance,
student effort may need to be coupled with effective teachers, an engaging curriculum, safe
neighborhoods, involved parents, or other inputs in order to yield increased achievement. In the
parlance of our model, if capital levels K are so low that there is a very small return to effort,
then students have little reason to work hard. In symbols: for small enough ki,

ùúïùê¥

|
ùúïùê∏ ùê∏=ùê∏‚àó

‚âà 0.

For intuition, consider a special case that lends itself to graphical exposition. Let the

production technology be Cobb-Douglas with a single capital input, such that F(E,K) = aEŒ±K1-Œ±,
and assume that the long-run benefits are linear in units of achievement: V(A) = rA. This allows
us to use units of academic achievement as the numeraire and represent benefits and achievement
on the same axes.
Figure 1 considers how achievement A responds to changes in returns r for different levels of
capital K. The gray lines show the marginal product of effort at low levels of capital, and thin
black lines depict the high-capital scenario. For each capital level, the solid curve represents the
base case, in which we normalize the return r to one. The dashed lines show marginal payoffs
after an increase in r.
The graph clarifies the two channels through which missing complements reduce treatment
effects. First, because labor and capital are complements, the marginal return to a unit of effort

22

is lower in equilibrium when capital levels are lower. Second, an increase in r results in a larger
increase in equilibrium effort at higher levels of marginal productivity.
There are several (admittedly weak) tests of elements of this model that are possible with our
data. If effective teachers or environmental factors are an important complementary input to
student incentives in producing test scores, we should notice a correlation between these inputs
and the impact of providing information on achievement. To test this hypothesis, we partition
our sample on three measures of external ‚Äúcapital‚Äù that are plausible complements of academic
effort: (1) Teacher Quality (measured by teacher value-added (TVA) estimates calculated for the
ELA or math teacher of roughly 85% of our sample), (2) Neighborhood Quality (measured by
the zip-code level poverty rates recorded in the American Community Survey), Neighborhood
Segregation (measured by zip codes‚Äô Black Dissimilarity Indices,). See Online Appendix B for
the precise details of how we calculate each of these measures.
To create subgroups, we rank all students in the experimental group and split the sample
at the median. Table 8 presents treatment effects for our information treatment within each of
these groups on our four main outcome measures. 25
If anything, the resulting estimates demonstrate the opposite of what one might expect if
complementarities in production were a driving force. Students from more segregated
neighborhoods show larger increases in both math and reading scores. Similarly, students
assigned to low-TVA teachers show treatment effects of 0.121œÉ (0.063) in reading, relative to a 0.033œÉ (0.063) effect in high-TVA classrooms. The effects on math scores are not statistically
differentiable by teacher quality. Both of these differences point in the opposite direction than
the theory of complementarities predicts.
C. Lack of Knowledge of the Production Function
The standard economic model implicitly assumes that students know their production
functions ‚Äì that is, the precise relationship between the vector of inputs and the corresponding
output. If students only have a vague idea of how to increase achievement, then there may be
25

In Appendix Tables 7a, 7b, and 7c, we report covariate means and balance tests within each of these subgroups. In
the low-dissimilarity group the p-value on a joint significance test is 0.085; the other five subgroups are all wellbalanced. Results for race, gender, special education, and ability subgroups are similar and are available from the
author upon request.

23

little reason for them to increase effort in response to new information or their effort may not
result in measureable output. In our framework, one might imagine that F represents the
students‚Äô beliefs about the production function, though not necessarily the true relationship.
In this scenario, the informational treatment changed beliefs, students put in more effort,
but the effort was not effective at producing test scores given their lack of knowledge of how to
translate effort into output. This explanation may also reconcile our set of facts with those
presented in Nguyen (2008) and Jensen (2010). Less than half of the parents in Nguyen‚Äôs
sample finished their primary education, and 45% of the eighth graders in Jensen‚Äôs control group
do not enroll in high school the following year. This suggests that these populations are
investing extremely little in their education at baseline, leaving significant ‚Äúlow-hanging fruit‚Äù
unclaimed.
This is not the first time that similar educational interventions have shown much larger
effects in the developing world than the United States. For instance, series of experiments in
India (Duflo, Hanna, and Ryan 2012, Muralidharan and Sundararaman 2011) and Kenya
(Glewwe et al. 2010) have revealed important achievement gains after the introduction of teacher
incentives. Comparable merit pay initiatives have been ineffective in the United States (Fryer
forthcoming, Springer et al. 2010, Fryer et al 2012). A frequent explanation for these
differences is that, in the absence of incentives, teachers do not pursue simple measures to
improve student achievement (for instance, unannounced visits revealed 35% of the schools in
Duflo, Hanna, and Ryan‚Äôs sample were closed due to teacher absenteeism).
Intuitively, the mapping from effort to academic success ought to be clear at low levels of
investment. The decision to attend school or drop out, for instance, has a clear relationship to
academic achievement. At higher levels of investment, however, the ways in which different
kinds of effort produce achievement is less clear. Once students are in school, they have to
choose not just how much to study, but which particular types of studying to invest in.
If one takes the self-reported effort results at face value, then this sort of uncertainty is
necessary to explain why students report higher effort but do not achieve at higher levels. After
all, if students understood that their efforts would not lead to increased achievement, then there is
no reason for them to work harder in our model. We have argued that self-reported effort is a
24

meaningful measure, but, given the usual caveats of survey data, we urge caution in interpreting
these results. 26
VI. Conclusion
In an effort to increase achievement and narrow achievement gaps, school districts have
become incubators of innovative reforms. One potentially cost effective and imminently scalable
strategy, not yet tested in American public schools, is to teach students about the returns to
human capital.
This paper reports estimates of the impact of providing this type of information from a
field experiment in Oklahoma City Public Schools during the 2010-2011 school year. Three facts
emerge: (1) students update their beliefs about the returns to education in response to the text
messages (2) students report that they are putting more effort into their work, and (3) there are no
detectable changes in academic achievement. How to interpret these facts in a model of human
capital acquisition is less clear. We argue that a model in which students do not fully understand
the education production function best explains our findings, though other explanations are
possible. Much depends on how much faith one has in self-reported measures of effort. If they
are unreliable, then high discount rates may also explain our results.
Providing information on the returns to schooling in urban schools in America seems
important. What to combine it with to effect student achievement is less clear. In future work, it
may be important to couple information treatments with teaching of the production function,
provide students with non-cognitive treatments designed to influence students‚Äô ‚Äúmindsets,‚Äù or
both (Dweck 2008).

26

Theoretically, systematic differences in discount rates between the populations could also explain why similar
treatments are more successful in developing countries. Since we do not directly observe discounting behavior in
any of these experiments, evaluating this claim is difficult. Wang, Rieger, and Hens (2010) analyze survey data
from 45 countries and find that citizen of poorer countries do have higher discount rates. However, Lawrance
(1991) shows that low-income Americans in the Panel Study of Income Dynamics exhibit higher-than-average
discounting behavior, suggesting that the national average may not be a good proxy for our population. Given the
paucity of clear evidence, we can neither confirm nor rule out that discount rates explain the divergent findings.

25

References

Ainsworth-Darnell, James W., and Douglas B. Downey. 1998. ‚ÄúAssessing the Oppositional
Culture Explanation for Racial/Ethnic Differences in School Performance.‚Äù American
Sociological Review, 63: 536-553.
Attanasio, Orazio, and Katja Kaufmann. 2009. ‚ÄúEducational Choices, Subjective Expectations,
and Credit Constraints.‚Äù NBER Working Paper No. 15087.
Avery, Christopher, and Thomas J. Kane. 2004. ‚ÄúStudent Perceptions of College Opportunities:
The Boston COACH Program.‚Äù In College Choices: The Economics of Where to Go, When to
Go, and How to Pay for It, Caroline M. Hoxby, ed., Chicago: University of Chicago Press.
Bruhn, Miriam, and David McKenzie. 2009. ‚ÄúIn Pursuit of Balance: Randomization in Practice
in Development Field Experiments." American Economic Journal: Applied Economics,1(4):
200-232.
Bureau of Labor Statistics. 2011. ‚ÄúEconomic News Release: Employment Status of the Civilian
Population 25 Years and Over by Educational Attainment.‚Äù
http://www.bls.gov/news.release/empsit.t04.htm, accessed January 2011.
Clarke, Philip M., and Chris Ryan. 2006. ‚ÄúSelf‚ÄêReported Health: Reliability and Consequences
for Health Inequality Measurement.‚Äù Health Economics, 15(6): 645-652.
Dominitz, Jeff, and Charles F. Manski. 1996. ‚ÄúEliciting Student Expectations of the Returns to
Schooling.‚Äù Journal of Human Resources, 31: 1‚Äì26.
Duflo, Esther, Rema Hanna, and Stephen P. Ryan. 2012. ‚ÄúIncentives Work:
Getting Teachers to Come to School.‚Äù American Economic Review, 102(4): 1241-78.
Dunifon, Rachel and Greg J. Duncan. 1998. ‚ÄúLong-Run Effects of Motivation on Labor-Market
Success.‚Äù Social Psychology Quarterly, 61(1): 33-48.
Dweck, Carol. 2008. Mindset: The New Psychology of Success. Ballantine Books.
Fryer Jr., Roland G. 2011a. ‚ÄúRacial Inequality in the 21st Century: The Declining Significance of
Discrimination.‚Äù In Handbook of Labor Economics, vol. 4, Orley Ashenfelter and David Card,
eds. Amsterdam: Elsevier Science/North-Holland.
Fryer Jr., Roland G. 2011b. ‚ÄúFinancial Incentives and Student Achievement: Evidence from
Randomized Trials.‚Äù The Quarterly Journal of Economics, 126: 1755-1798.
Fryer Jr., Roland G. Forthcoming. ‚ÄúTeacher Incentives and Student Achievement: Evidence
from New York City Public Schools. Journal of Labor Economics (2013): 31(2).

26

Fryer Jr, Roland G., Steven D. Levitt, John List, and Sally Sadoff. 2012. ‚ÄúEnhancing the
Efficacy of Teacher Incentives through Loss Aversion: A Field Experiment.‚Äù NBER Working
Paper No. 18237.
Glewwe, Paul, Nauman Ilias, and Michael Kremer. 2010. ‚ÄúTeacher Incentives.‚Äù
American Economic Journal: Applied Economics, 2(3): 205-227.
Greevy, Robert, Bo Lu, and Jeffrey H. Silber. 2004. ‚ÄúOptimal Multivariate Matching before
Randomization." Biostatistics, 5: 263-275.
Harris Angel L. 2008. ‚ÄúOptimism in the Face of Despair: Black-White Differences in Beliefs
About Achool as a Means for Upward Social Mobility.‚Äù Social Science Quarterly, 89(3):608‚Äì
630.
Hernstein, R. J., and C. Murray. 1994. The Bell Curve: Intelligence and Class Structure in
American Life. New York: Free Press.
Imai, Kosuke, Gary King, and Clayton Nall. 2009. ‚ÄúThe Essential Role of Pair Matching in
Cluster Randomized Experiments." Statistical Science, 24(1): 29-53.
Imbens, Guido. 2011. ‚ÄúExperimental Design for Unit and Cluster Randomized Trials.‚Äù
Conference Paper, International Initiative for Impact Evaluation.
Jahn, Julius A., Calvin F. Schmid, and Clarence Schrag. 1947. ‚ÄúThe Measurement of
Ecological Segregation.‚Äù American Sociological Review, CIII, 293-303.
Jencks, Christopher, and Meredith Phillips, eds. 1998. The Black-White Test Score Gap.
Washington D.C.: Brookings Institution Press.
Jensen, Robert. 2010. ‚ÄúThe (Perceived) Returns to Education and the Demand for Schooling,‚Äù
Quarterly Journal of Economics, 125(2):515-48.
Johnston, David W., Carol Propper, and Michael A. Shields. ‚ÄúComparing Subjective and
Objective Measures of Health: Evidence from Hypertension for the Income/Health Gradient.‚Äù
Journal of Health Economics, 28(3): 540-552.
Karlan, Dean, Margaret McConnell, Sendhil Mullainathan, and Jonathan Zinman. 2010. ‚ÄúGetting
To the Top of Mind: How Reminders Increase Saving.‚Äù NBER Working Paper No. 16205.
Kasy, Maximilian. 2012. "Why Experimenters Should not Randomize, and What They Should
do Instead." Unpublished Manuscript.
Kaufmann, Katja. 2009. ‚ÄúUnderstanding the Income Gradient in College Attendance in Mexico:
The Role of Heterogeneity in Expected Returns to College.‚Äù Mimeo. Bocconi University.

27

Lawrance, Emily C. 1991. ‚ÄúPoverty and the Rate of Time Preference: Evidence from Panel
Data.‚Äù Journal of Political Economy, 99(1): 54-77.
Lee, David S. 2009. ‚ÄúTraining, Wages, and Sample Selection: Estimating Sharp Bounds on
Treatment Effects.‚Äù Review of Economic Studies, 76(3): 1071-1102.
Lieberson, Stanley. 1980. A Piece of the Pie: Blacks and White Immigrants Since 1880.
Berkeley: University of California Press.
Mickelson, Roslyn A. 1990. ‚ÄúThe Attitude-Achievement Paradox among Black Adolescents.‚Äù
Sociology of Education, 63: 44-61.
Mullainathan, Sendhil, Joshua Schwartzstein, and Andrei Shleifer. 2008. ‚ÄúCoarse Thinking and
Persuasion.‚Äù Quarterly Journal of Economics, 123: 577‚Äì619.
Muralidharan, Karthik and Venkatesh Sundararaman. 2011. ‚ÄúTeacher Performance Pay:
Experimental Evidence from India.‚Äù Journal of Political Economy, 119 (1).
Neal, Derek, and William Johnson. 1996. ‚ÄúThe Role of Premarket Factors in Black-White Wage
Differentials.‚Äù Journal of Political Economy, 104: 869‚Äì95.
Nelson, Phillip. 1974. ‚ÄúAdvertising as Information.‚Äù Journal of Political Economy, 81: 729-754.
Nguyen, Trang. 2008. ‚ÄúInformation, Role Models and Perceived Returns to Education:
Experimental Evidence from Madagascar,‚Äù MIT Working Paper.
Ogbu, John U. 1978. Minority Education and Caste: The American System in Cross-Cultural
Perspective. New York: Academic Press.
Rouse, Cecelia E. 2004. ‚ÄúLow-Income Students and College Attendance: An Exploration of
Income Expectations.‚Äù Social Science Quarterly, 85: 1299‚Äì1317.
Shapiro, Jesse. 2006. ‚ÄúA ‚ÄòMemory-Jamming‚Äô Theory of Advertising,‚Äù mimeo, University of
Chicago.
Smith, Herbert L., and Brian Powell. 1990. ‚ÄúGreat Expectations: Variations in Income
Expectations among College Seniors.‚Äù Sociology of Education, 63: 194‚Äì207.
Springer, Matthew G., Dale Ballou, Laura S. Hamilton, Vi-Nhuan Le, J.R. Lockwood,
Daniel F. McCaffrey, Matthew Pepper, and Brian M. Stecher. 2010. ‚ÄúTeacher Pay for
Performance: Experimental Evidence from the Project on Incentives in Teaching.‚Äù
Conference paper, National Center on Performance Incentives.
United States Census Bureau. 2011. ‚ÄúCurrent Population Survey Data on Educational
Attainment.‚Äù http://www.census.gov/hhes/socdemo/education/data/cps/index.html, accessed May
2011.

28

Wang, M., Rieger, M. O. and Hens, T. 2010. ‚ÄúHow Time Preferences Differ: Evidence From 45
Countries.‚Äù SFI Working Paper 09-47.
Wilson, William Julius. 1987. ‚ÄúThe Truly Disadvantaged: The Inner City, the Underclass, and
Social Policy.‚Äù Chicago: University of Chicago Press.
Wiswall, Matthew and Basit Zafar. 2012. ‚ÄúDeterminants of College Major Choice:
Identification using an Information Experiment.‚Äù Available at SSRN:
http://ssrn.com/abstract=1919670

29

Figure 1: Treatment Effects Under High and Low Capital Endowments
Notes: The figure depicts how achievement changes with an increase perceived returns r in a low-capital and highcapital scenario. The model is described in Section V of the text and is parameterized as follows: a=1, Œ±=0.5,
Khigh=30, Klow=1, and C(E) = 4E2.

1

Table 1: Summary of The Million Experiment
A. Overview
Schools
Treatment Group
Control Group
Outcomes of Interest
Test Dates
Operations
B. Treatments

All 22 non-alternative OKCPS schools serving grades 6 and 7 opted in to participate. All experimental schools were
provided complete Accelerated Reading software, training, and implementation materials. All treatment students
received a Samsung t401g mobile phone.
1,470 6th and 7th grade students: 31.1% black, 44.3% Hispanic, 91.7% free lunch eligible
437 6th and 7th grade students: 30.9% black, 43.5% Hispanic, 91.8% free lunch eligible
Student Knowledge of Returns to Education, Oklahoma Core Curriculum Criterion Referenced Test (CRT), Measures
of Student Effort and Motivation, Attendance, Suspensions
CRT: April 11-26, 2011
$230,365 worth of hardware and incentives distributed to treatment students, 34.3% consent rate. 1 dedicated project
managers.
(1) Information Only

(2) Information & Incentives

(3) Non-Financial Incentives Only

Phone

Free Samsung t401g mobile phone

Free Samsung t401g mobile phone

Free Samsung t401g mobile phone

Basic Reward Structure

Fixed allotment of 200 minutes per
month

Students earned 10 cell phone
minutes per Accelerated Reader point
earned, distributed in blocks of 200
minutes

Students earned 10 cell phone
minutes per Accelerated Reader point
earned, distributed in blocks of 200
minutes

Informational Campaign

Students received one informational
or persuasive message per day

Students received one informational
or persuasive message per day

None

Monthly, unconditional

Bi-weekly, contingent upon AR points
earned

Bi-weekly, contingent upon AR points
earned

Reward Frequency

Notes. In panel A, each row describes an aspect of treatment indicated in the first column. In panel B, each column represents a different arm of treatment. Entries
are descriptions of the schools, students, outcomes of interest, testing dates, and basic operations of each phase of the incentive treatment. See Online Appendix A for more
details. The numbers of treatment and control students given are for those students who have non-missing reading or math test scores.

Student Characteristics
Male
White
Black
Hispanic
Asian
Other Race
Special Education Services
English Language Learner
Free Lunch
Economically Disadvantaged
Baseline Math
Baseline Reading
Missing: Baseline Math
Missing: Baseline Reading
p-value from joint F-test
Observations

Table 2: Student Baseline Characteristics
Non
p-value
Pooled
Participating Participating (1) = (2) Information
(1)
(2)
(3)
(4)
0.521
0.486
0.019
0.479
(0.500)
(0.500)
(0.500)
0.200
0.163
0.002
0.158
(0.400)
(0.369)
(0.365)
0.290
0.311
0.125
0.310
(0.454)
(0.463)
(0.463)
0.435
0.443
0.634
0.447
(0.496)
(0.497)
(0.497)
0.025
0.026
0.824
0.024
(0.155)
(0.158)
(0.155)
0.051
0.058
0.288
0.060
(0.220)
(0.234)
(0.238)
0.149
0.139
0.326
0.136
(0.356)
(0.346)
(0.343)
0.154
0.159
0.612
0.159
(0.361)
(0.366)
(0.366)
0.857
0.917
0.000
0.921
(0.351)
(0.276)
(0.269)
0.741
0.915
0.000
0.918
(0.438)
(0.279)
(0.274)
0.010
0.030
0.565
0.009
(1.022)
(0.983)
(1.006)
0.037
-0.021
0.098
-0.041
(1.015)
(1.010)
(1.060)
0.319
0.216
0.000
0.197
(0.466)
(0.411)
(0.398)
0.326
0.219
0.000
0.203
(0.469)
(0.414)
(0.402)
0.000
2903

1907

4810

980

Non-Financial
Incentives
(5)
0.453
(0.498)
0.163
(0.370)
0.314
(0.465)
0.441
(0.497)
0.018
(0.134)
0.063
(0.244)
0.147
(0.354)
0.159
(0.366)
0.908
(0.289)
0.908
(0.289)
0.006
(0.993)
-0.053
(0.989)
0.237
(0.426)
0.231
(0.422)

Control
(6)
0.538
(0.499)
0.172
(0.377)
0.309
(0.463)
0.435
(0.496)
0.037
(0.188)
0.048
(0.214)
0.137
(0.345)
0.160
(0.367)
0.918
(0.275)
0.915
(0.279)
0.108
(0.916)
0.062
(0.905)
0.233
(0.423)
0.243
(0.429)

p-value
(4)=(5)=(6)
(7)
0.030
0.817
0.982
0.910
0.203
0.571
0.837
0.999
0.685
0.804
0.263
0.230
0.127
0.196
0.436

490

437

1907

Notes: This table reports summary statistics for the field experiment. Columns (1), (2), (4), (5), and (6) represent
the sample means of the variable indicated in each row for the group indicated in each column The treatment groups
are restricted to randomly selected 6th and 7th grade students in Oklahoma City Public Schools experimental schools
who opted into the randomization for the field experiment. Columns (3) and (7) report the p-value from a test of
equality across treatment indicators (or experimental group indicators) from a regression of the variable in each row
on indicators for each treatment group and the control group (or experimental group status). The joint F-tests report
the p-value from a test of equality across treatment indicators (or experimental group indicators) from a multi-variate
regression testing the overall quality of the lottery.

Table 3 - Mean Effect Sizes (Intent-to-Treat) on Direct Outcomes
Non-Financial
Information
Incentives
A. Treatment Questions
Knows Wage Gap btw BA and Dropouts
0.049‚àó
0.017
(0.027)
(0.033)
902
589
Knows Prison Rates
0.179‚àó‚àó‚àó
-0.046
(0.038)
(0.043)
891
585
Both Quiz Questions Correct
0.178‚àó‚àó‚àó
-0.023
(0.038)
(0.043)
880
576
B. Placebo Question
Knows Unemployment Rate of College Grads
0.022
0.047
(0.036)
(0.043)
903
590

p-value
0.458

0.000

0.000

0.653

Notes: This table reports ITT estimates for the effect of being offered a chance to participate in the field experiment
on students‚Äô ability to correctly answer questions about human capital development. Questions are coded as a 1
if the student answered the question correctly and a 0 otherwise. All regressions include school fixed effects and
controls for student grade, gender, race, SES, special education status, and English language learner status, as well
as 2009 state test scores, 2010 state test scores, and their squares. The sample is restricted to randomly selected 6th
and 7th grade students in Oklahoma City Public Schools. Randomization was done at the student level. Treatment
is defined as returning a signed consent form to participate and being lotteried into the specified treatment group.
Heteroskedasticity-robust errors are reported in parentheses below each estimate. The number of observations in each
regression is reported directly below the standard errors. *** = significant at 1 percent level, ** = significant at 5
percent level, * = significant at 10 percent level.

Table 4 - Mean Effect Sizes (Intent-to-Treat) on Indirect Outcomes
Non-Financial
Information
Incentives
A. Survey Outcomes
More Focused Since Million
0.151‚àó‚àó‚àó
0.152‚àó‚àó‚àó
(0.037)
(0.043)
910
592
Million Makes Students Work Harder
0.070‚àó
0.077‚àó
(0.037)
(0.044)
916
599
B. Administrative Data Outcomes
OK State Math Test Post-Treatment
-0.027
-0.023
(0.039)
(0.047)
1211
782
OK State Reading Test Post-Treatment
0.040
0.023
(0.041)
(0.050)
1202
780
Attendance Rate
-0.007
0.034
(0.056)
(0.063)
1310
861
Number of Suspensions
0.021
0.025
(0.061)
(0.073)
1417
927

p-value
0.988

0.897

0.947

0.794

0.623

0.966

Notes: This table reports ITT estimates for the effect of being offered a chance to participate in the field experiment
on survey and administrative data outcomes. Survey measures are coded as a 1 if the student answered a question
indicating that he or she agreed with the statement in the corresponding row and a 0 otherwise. Test scores are
standardized to have mean zero and standard deviation one by grade in the full OKCPS 6th and 7th grade samples. All
regressions include school fixed effects and controls for student grade, gender, race, SES, special education status, and
English language learner status, as well as 2009 state test scores, 2010 state test scores, and their squares. The sample
is restricted to randomly selected 6th and 7th grade students in Oklahoma City Public Schools. Randomization was
done at the student level. Treatment is defined as returning a signed consent form to participate and being lotteried
into the specified treatment group. Heteroskedasticity-robust errors are reported in parentheses below each estimate.
The number of observations in each regression is reported directly below the standard errors. *** = significant at 1
percent level, ** = significant at 5 percent level, * = significant at 10 percent level.

Table 5 - Analysis of Subsamples for Pooled Information Treatments
Both Quiz
Reports Being
Questions Correct More Focused State Math State Reading
Common Sample
0.178‚àó‚àó‚àó
0.151‚àó‚àó‚àó
-0.027
0.040
(0.038)
(0.037)
(0.039)
(0.041)
880
910
1211
1202
A. Gender
Male
0.252‚àó‚àó‚àó
0.154‚àó‚àó‚àó
-0.123‚àó‚àó
0.091
(0.054)
(0.053)
(0.059)
(0.064)
428
441
589
584
Female
0.085
0.127‚àó‚àó
0.062
-0.025
(0.054)
(0.055)
(0.055)
(0.056)
452
469
622
618
p-value
0.021
0.713
0.018
0.159
B. Race
Black
0.109
0.181‚àó‚àó
-0.118
0.056
(0.077)
(0.075)
(0.079)
(0.075)
223
232
349
347
Hispanic
0.201‚àó‚àó‚àó
0.186‚àó‚àó‚àó
0.044
0.006
(0.054)
(0.055)
(0.054)
(0.060)
428
445
572
569
White
0.120
0.006
-0.042
-0.002
(0.103)
(0.108)
(0.123)
(0.128)
147
153
186
184
p-value
0.509
0.228
0.203
0.839
C. Special Education
Yes
-0.048
0.124
0.003
0.183
(0.136)
(0.131)
(0.273)
(0.256)
111
115
86
77
No
0.198‚àó‚àó‚àó
0.160‚àó‚àó‚àó
-0.033
0.023
(0.040)
(0.040)
(0.038)
(0.040)
769
795
1125
1125
p-value
0.036
0.753
0.872
0.433
D. Baseline Scores
Above Median
0.197‚àó‚àó‚àó
0.136‚àó‚àó
-0.034
0.051
(0.058)
(0.060)
(0.047)
(0.053)
382
391
505
506
Below Median
0.184‚àó‚àó‚àó
0.121‚àó
-0.055
0.017
(0.066)
(0.066)
(0.061)
(0.059)
335
348
507
506
Missing
0.041
0.216‚àó‚àó
0.000
0.095
(0.096)
(0.087)
(0.118)
(0.135)
163
171
199
190
p-value
0.270
0.602
0.896
0.809
Notes: This table reports ITT estimates for the effect of being offered a chance to participate in the field experiment on a subset of direct and
indirect outcomes for a variety of subgroups. Columns indicate outcome measure, and rows indicate the subgroup to which the regression sample
is limited. All regressions compare the informational treatment groups with the control group. Regressions follow the same specification as Tables
3 and 4. The first row reports ITT estimates for the common sample with valid demographic information for all the subgroups we consider. Within
the racial subgroups, we limit our analysis to racial groups represented by at least 100 students in the common sample. being lotteried into the
specified treatment group and returning a signed consent form to participate. Heteroskedasticity-robust errors are reported in parentheses below
each estimate. *** = significant at 1 percent level, ** = significant at 5 percent level, * = significant at 10 percent level.

Table 6 - Mean Effect Size on Attrition
Control
Differential Follow-up
Response
Pooled
Non-Financial
Rate
Information
Incentives
(1)
(2)
(3)
Reading
0.931
-0.002
0.001
(0.004)
(0.005)
Mathematics
0.931
0.000
-0.001
(0.004)
(0.005)
Survey
0.611
0.058‚àó‚àó‚àó
0.070‚àó‚àó‚àó
(0.021)
(0.024)
Number of Observations
1417
927
Notes: This table reports differential rates of attrition for individuals in the field experiment‚Äôs experimental group.
Column (1) reports the share control students with non-missing values for the post-treatment outcomes indicated in
each row. Columns (2)and (3) report coefficients from regressions of an indicator variable equal to one if the outcome
in the same row is non-missing on an indicator for being randomly selected into the indicated treatment group. All
regressions includes the full set of covariates and fixed effects used in the preceding tables. Heteroskedasticity-robust
errors are reported in parentheses below each estimate. The number of observations in each regression is reported in
the final row. *** = significant at 1 percent level, ** = significant at 5 percent level, * = significant at 10 percent level.

Table 7 - Bounding
Lee Lower
ITT
Bound
(1)
(2)
A. Information Treatment versus Control
Knows Wage Gap btw BA and Dropouts

Knows Prison Rates

Both Quiz Questions Correct

More Focused Since Million

Million Makes Students Work Harder

B. Non-financial Incentives Treatment versus Control
Knows Wage Gap btw BA and Dropouts

Knows Prison Rates

Both Quiz Questions Correct

More Focused Since Million

Million Makes Students Work Harder

0.049‚àó
(0.027)
902
0.179‚àó‚àó‚àó
(0.038)
891
0.178‚àó‚àó‚àó
(0.038)
880
0.151‚àó‚àó‚àó
(0.037)
910
0.070‚àó
(0.037)
916

0.032
(0.027)
862
0.146‚àó‚àó‚àó
(0.038)
854
0.150‚àó‚àó‚àó
(0.038)
844
0.096‚àó‚àó‚àó
(0.037)
854
0.037
(0.037)
872

0.017
(0.033)
589
-0.046
(0.043)
585
-0.023
(0.043)
576
0.152‚àó‚àó‚àó
(0.043)
592
0.077‚àó
(0.044)
599

-0.008
(0.033)
568
-0.102‚àó‚àó
(0.043)
564
-0.065
(0.043)
558
0.091‚àó‚àó
(0.043)
563
0.033
(0.044)
575

p-value
(1)=(2)
(3)

Lee Upper
Bound
(4)

p-value
(1)=(4)
(5)

0.662

0.101‚àó‚àó‚àó
(0.026)
862
0.216‚àó‚àó‚àó
(0.037)
854
0.218‚àó‚àó‚àó
(0.037)
844
0.210‚àó‚àó‚àó
(0.036)
854
0.128‚àó‚àó‚àó
(0.037)
872

0.163

0.083‚àó‚àó‚àó
(0.031)
568
-0.017
(0.043)
564
0.002
(0.043)
558
0.213‚àó‚àó‚àó
(0.042)
563
0.126‚àó‚àó‚àó
(0.044)
575

0.144

0.536

0.594

0.293

0.529

0.606

0.354

0.483

0.311

0.473

0.487

0.444

0.259

0.272

0.629

0.683

0.308

0.437

Imputed
(6)

p-value
(1)=(6)

0.034‚àó
(0.019)
1257
0.141‚àó‚àó‚àó
(0.026)
1257
0.145‚àó‚àó‚àó
(0.026)
1256
0.114‚àó‚àó‚àó
(0.026)
1258
0.063‚àó‚àó
(0.026)
1259

0.648

-0.001
(0.024)
828
-0.043
(0.030)
828
-0.024
(0.030)
826
0.133‚àó‚àó‚àó
(0.034)
830
0.060‚àó
(0.031)
831

0.671

Notes: This table reports upper and lower Lee bounds and regression estimates using imputed missing outcomes to account for survey attrition.
Controlling for baseline test scores, demographics, and school fixed effects, students in the informational treatment groups are 7.3 percentage points
more likely to respond to the survey, and treatment students who received only non-financial incentives are 9.9 percentage points more likely to
response to the survey. For ease of comparison, Column (1) reproduces the survey results from Tables 3 and 4. Column (2) reports lower Lee Bounds.
These bounds are generated by predicting the residuals from a regression of the survey outcome of interest on baseline test scores, demographics,
and treatment-year test scores within the control group only. The treatment group is then sorted and individuals with the largest residuals from the
regressions are removed from the regression to equate response rates between treatment and control. The resulting Lee lower bounds are from an
OLS regression identical to our main specification after trimming the sample in this way. Column (4) reports upper Lee Bounds. These bounds
are generated by the same process as lower Lee Bounds, except individuals with the smallest residuals are removed from the regression to equate
response rates between treatment and control. To generate the results in Column (6), the full set of baseline characteristics and year of treatment
test scores are used to impute missing data for attriters in the treatment and control groups. Otherwise, regressions use the same covariates as Table
3. Columns (3), (5), and (7) report p-values on the null hypothesis that the treatment coefficients from theLEE bound and imputed regressions are
equal to the treatment coefficient from the main ITT specification for the treatment group indicated in the panel title. Heteroskedasticity-robust
errors are reported in parentheses below each estimate. The number of observations in each regression is reported directly below the standard errors.
*** = significant at 1 percent level, ** = significant at 5 percent level, * = significant at 10 percent level.

0.393

0.466

0.409

0.874

0.958

0.984

0.720

0.750

Table 8 - Analysis of Subsamples for Pooled Information Treatments
Both Quiz
Reports Being
Questions Correct More Focused State Math State Reading
A. Black Dissimilarity Index
Above Median
0.172‚àó‚àó‚àó
0.127‚àó‚àó
0.084
0.141‚àó‚àó
(0.053)
(0.054)
(0.054)
(0.056)
440
455
645
645
Below Median
0.194‚àó‚àó‚àó
0.177‚àó‚àó‚àó
-0.135‚àó‚àó
-0.067
(0.054)
(0.054)
(0.060)
(0.064)
440
455
566
557
p-value
0.757
0.487
0.005
0.012
B. Zip Code Poverty Rate
Above Median
0.149‚àó‚àó
0.095
-0.069
0.082
(0.074)
(0.072)
(0.057)
(0.070)
296
304
471
462
Below Median
0.203‚àó‚àó‚àó
0.174‚àó‚àó‚àó
-0.007
0.024
(0.045)
(0.046)
(0.053)
(0.053)
584
606
740
740
p-value
0.504
0.327
0.414
0.498
C. Teacher Value-Added
Above Median
0.167‚àó‚àó‚àó
0.091‚àó
-0.016
-0.033
(0.053)
(0.054)
(0.057)
(0.063)
442
452
523
521
Below Median
0.210‚àó‚àó‚àó
0.235‚àó‚àó‚àó
-0.056
0.121‚àó
(0.065)
(0.063)
(0.064)
(0.063)
315
328
517
518
Missing
0.012
0.273‚àó‚àó‚àó
-0.010
-0.092
(0.122)
(0.104)
(0.092)
(0.110)
123
130
171
163
p-value (High=Low)
0.590
0.072
0.631
0.074

Notes: This table reports ITT estimates for the effect of being offered a chance to participate in the field experiment
on a subset of direct and indirect outcomes for a variety of subgroups. Columns indicate outcome measure, and rows
indicate the subgroup to which the regression sample is limited. All regressions compare the informational treatment
groups with the control group. Regressions follow the same specification as Tables 3 and 4. The first row reports
ITT estimates for the common sample with valid demographic information for all the subgroups we consider. Panel
A presents ITT estimates for students based upon the Black Dissimilarity Index score of their zip code relative to the
rest of the experimental group. Panel B presents ITT estimates for students based upon the poverty rate of their zip
code relative to the rest of the experimental group. Panel C presents ITT estimates based upon the average Teacher
Value-Added score of each student‚Äôs math and reading/ELA teachers relative to the rest of the experimental group.
See Online Appendix B for details about the construction of the Black Dissimilarity Index, zip code poverty rates, and
TVA scores. The last row in each panel reports a p-value on the null hypothesis that treatment coefficients across the
subgroups in that panel are equal for the indicated outcome. Randomization was done at the student level. Treatment
is defined as being lotteried into the specified treatment group and returning a signed consent form to participate.
Heteroskedasticity-robust errors are reported in parentheses below each estimate. The number of observations in each
regression is reported directly below the standard errors. *** = significant at 1 percent level, ** = significant at 5
percent level, * = significant at 10 percent level.

Online Appendix For
Information and Student Achievement:
Evidence from a Cellular Phone Experiment

Roland G. Fryer, Jr.
June 2013

Not for Publication

1

Online Appendix A: Implementation Manual (Not For Publication)

The experiment was implemented and managed by the Education Innovation Laboratory
(EdLabs) at Harvard University.

SCHOOLS
EdLabs first presented the basics of the program to OKCPS district leaders on July 27,
2010, at which point it was decided to offer participation to schools with sixth and seventh grade
students. District leaders informally provided schools with additional details as part of the
recruitment process over the summer. On August 16, 2010, EdLabs presented the research design
and program details to the OKCPS Board of Education, spurring further internal discussions
about exactly which schools would be eligible to participate.
On August 25, 2010, the district identified all non-alternative district schools that served
the 6th or 7th grade students. On September 27, 2010, the principals and library media specialists
(LMS) from those schools were invited to an introductory meeting to review the basics of the
program and to prepare the process of starting the experiment in the subsequent weeks. Schools
were also able to ‚Äúopt out‚Äù of participating; however, all twenty-two schools elected to
participate and allow consenting students to be randomized into treatment and control groups.

STUDENTS
Sixth and seventh grade students attending twenty-two elementary and secondary schools
in OKCPS were eligible to participate in the experiment. Students were required to obtain
parental consent to be a part of the study. Students received information packets on September

2

28, 2010 and were required to return a signed parental consent form by October 1, 2010 in order
to be eligible for the lottery to determine participation. We received 1,907 student consent forms
and randomized students into one of three treatment groups and a control group: (1) 490 students
received a cell phone and were required to read books and complete quizzes about those books in
order to receive phone credits on a biweekly schedule; (2) 490 students received a cell phone and
daily text messages and were required to read books and complete quizzes in order to receive
credits; (3) 490 students received a cell phone with daily text messages and a fixed number (i.e.
non-performance-based) of credits on a monthly schedule; and (C) 437 students did not receive a
phone. Phones pre-loaded with 300 airtime credits were distributed to schools on the morning of
October 8, 2010. Students in treatments (1) and (2) were eligible to earn credits by reading books
starting on October 11, 2010. Students last received credits on May 18, 2011. Students or their
parents could opt to return the phone or discontinue active participation in the program at any
time.

TEXT MESSAGING
We worked closely with Droga5, an advertising firm based in New York City, to
determine the messaging and branding components of the program. We met initially with Droga5
to discuss the types of text messages that would be written and sent to students on a daily basis.
Writing text messages throughout the year was a collaborative and iterative process. Text
messages were sent to students in the appropriate treatment groups on a daily basis, including
weekends, at approximately 6:00 p.m. Messages were divided between ‚Äúinformative‚Äù and
‚Äúpersuasive‚Äù messages. Through the duration of the program, Droga5 drafted persuasive
messages and sent to us for review; concurrently, we drafted informative messages based on our

3

understanding of the relationship between educational attainment and relevant life outcomes
gleaned from national data sets and sent potential messages to Droga5 for review. Approved
messages were sent to TracFone for distribution.

SOFTWARE AND INCENTIVE STRUCTURE
The Accelerated Reader platform allows students to select from a vast library of popular
literature to demonstrate their knowledge of its plot. Upon finishing a book, each student took an
Accelerated Reader (AR) computer-based comprehension quiz, which provided evidence as to
whether the student read the book. Each book in AR is assigned a point value based on length
and difficulty. Students were allowed to select and read books of their choice and at their leisure,
not as a classroom assignment. The books came from the existing stock available at their school
(in the library or in the classroom), though additional copies of books that proved to be
particularly popular were ordered during the year.
For those students required to read books in order to receive credits, the incentive scheme
was strictly linear: each point earned during each biweekly reward period translated to ten phone
credits. Because phone credits could only be distributed (i.e. uploaded electronically) in
increments of 200, point earnings of less than or greater than a multiple of 20 were banked and
carried over to subsequent reward periods. Once a student reached or passed any 20 point
interval, blocks of 200 credits were uploaded at the next scheduled ‚Äúpayday‚Äù according to the
predetermined biweekly reward schedule.

REWARD PROCESS

4

For those students required to read books in order to receive credits, the incentive scheme
was strictly linear: each point earned during each biweekly reward period translated to ten phone
credits. Because phone credits could only be distributed (i.e. uploaded electronically) in
increments of 200, point earnings of less than or greater than a multiple of 20 were banked and
carried over to subsequent reward periods. Once a student reached or passed any 20 point
interval, blocks of 200 credits were uploaded by EdLabs at the next scheduled ‚Äúpayday‚Äù
according to the predetermined biweekly reward schedule. For students who received a fixed
stipend of credits, 200 credits were uploaded to their account by EdLabs according to a predetermined monthly schedule.

PHONE PROBLEMS
A spreadsheet was established to track all student phone issues throughout the
program. Once per week, the Project Manager would update the spreadsheet and send it to
Droga5. Droga5 would then communicate all phone issues to the Million
TracFone representative. TracFone troubleshot phones, and remedies would be communicated
back to Droga5, then the Project Manager, and then the LMS if appropriate.
The most common phone issue was blocked SIM cards, which occurred when students
attempted to lock their phones with a four-digit passcode, then forgot the passcodes and entered
incorrect passcodes three times. A blocked SIM would require a new SIM to be shipped from
TracFone to the student's school, where LMS would have to replace the card. Typically the
SIMS were pre-activated, so they required no further action from the LMS other than adding the
new SIM to the correct students' phones. However, there were a few cases toward the end of the
year in which it was possible to expedite fixing phones by shipping un-activated SIMs, and

5

having LMSs call TracFone to complete the activation. The first 10-15 students who reported
their phones stolen had them replaced. Subsequently, students who reported their phone as lost
or stolen had their SIM-card deactivated and no longer received informational text messages,
monthly uploads of credits, or credits in exchange for accumulating Accelerate Reader points.
All other issues were addressed remotely by TracFone, or via instructions emailed to the LMS to
resolve the problem.

SITE VISITS AND PROGRAM MONITORING
In an effort to gather extensive qualitative data on the implementation of the experiment,
EdLabs conducted brief site visits to all twenty-two experimental schools. EdLabs observed
classrooms and interviewed students, teachers, and school leaders. These visits helped to ensure
fidelity of implementation and allowed EdLabs to share best practices among LMS to improve
program implementation. Starting in November and continuing into January, we visited schools
and reviewed the basics of the program with treatment students to reinforce their understanding.
To diagnose specific misunderstandings of the reward algorithm or distribution system, we also
administered brief quizzes to check for student understanding. We revisited schools with
particularly low quiz scores to target specific areas of misunderstanding. By the end of this cycle,
students scored an average of 79 percent on the quiz, in response to questions about the basics of
the program, including the incentive structure, reward schedule, and how to report phone

6

Online Appendix B: Data Appendix (Not For Publication)

OKC Public School Administrative Data

Attendance Rates
Individual attendance rates account for all presences and absences for each student,
regardless of which school the student had enrolled in when the absence occurred, as long as the
student was enrolled in OKCPS. The attendance rate is calculated by dividing the number of
days present by the number of days a student was enrolled in the district during the 2010-2011
school year.

Free Lunch Status
Controlled regressions include a dummy variable equal to one if a student is eligible for
free or reduced-price lunch and zero otherwise. Free lunch status is recorded in the district
enrollment files.

Socioeconomic Status
Controlled regressions include a dummy variable equal to one if a student is identified as
economically disadvantaged and zero otherwise. Socioeconomic status is recorded in the district
enrollment files.

Special Education Services
Controlled regressions include a dummy variable equal to one if a student has an
Individualized Education Program (IEP) and is eligible to receive special education services. IEP
7

status is recorded in the district enrollment files. Whether a student Whether a student is eligible
to receive special education services as part of an IEP is determined by the OKCPS Special
Services Office.

English Language Learner Status
Controlled regressions include a dummy variable equal to one if a student is designated
as an English Language Learner. English Language Learner status is recorded in the district
enrollment files. Whether a student is designated as an English Language Learner is determined
by the OKCPS Language and Cultural Services Office.

Behavioral Incidents
Behavioral incidents are recorded in the district behavior file, counted, and summed for
each student by student id. Behavioral incidents are recorded individually by date of infraction,
as well as cumulatively, as a count of the total number of times a student was involved in a
behavioral incident throughout the year, regardless of the length or nature of the incident.

Suspensions
Suspensions are recorded in the district behavior file, counted, and summed for each
student by student id. Suspensions are recorded individually by date of infraction, as well as
cumulatively, as a count of the total number of times a student was suspended throughout the
year, regardless of the length or nature of the suspension.

Race/Ethnicity

8

We code the race variables such that the five categories -- white, black, Hispanic, Asian
and other -- are collectively exhaustive and mutually exclusive. Hispanic ethnicity is an
absorbing state. Hence ‚Äúwhite‚Äù implies non-Hispanic white, ‚Äúblack‚Äù non-Hispanic black, and so
on.

State Test Scores
We observe results from the Oklahoma Core Curriculum Criterion Referenced Tests
(CRT) in math and ELA. For ease of interpretation, we normalize raw scores to have a mean of
zero and a standard deviation of one within grades and subjects for 2010-2011 scores, when they
are used as outcomes in our analysis and for 2009-2010 scores when they are reported in the
summary statistics. Raw and controlled regressions control for non-normalized 2009-09 and
2009-2010 scale scores from district testing files as well as their squares.

Treatment
Our randomization files record which students were randomized into each treatment arm
and the control group. Each treatment is recorded as a binary variable equal to one if the student
was randomized into that arm of treatment and zero if a student was randomized into the control
group. When regressions are run on multiple treatment groups, an additional binary variable was
created that is equal to one if a student was randomized into any of the treatment arms being
analyzed and zero if the student was randomized into the control group.

Teacher Value-Added

9

Teacher value-added scores are a measure of the independent impact of teachers on student
growth. The construction of Teacher Value Added estimates follow Chetty, Friedman, and
Rockoff (2011). We use the test data from OKCPS 6th, 7th, and 8th grade students from 20062010 to regress students test scores on lagged scores and observable characteristics to generate
score residuals for each student. We then compute the mean of residuals for each student taught
by a given teacher. We then use the empirical Bayes procedure outlined in Chetty, Friedman, and
Rockoff (2011) to reduce noise by shrinking estimate towards mean based on number of students
that are observed for each teacher. Students are linked to teachers using district course grade
administrative files. The analysis code used to generate the estimates in Chetty, Friedman, and
Rockoff (2011) that we base our estimates on is publicly available at
http://obs.rc.fas.harvard.edu/chetty/va_bias_code.zip

Survey Data
Some of the indirect outcomes reported in the paper include survey responses from a
student survey administered to all students in the experimental group. We include responses to
several survey questions as outcome variables:
For the question ‚ÄúSince the Million Program started, do you think you are more focused
on or excited about doing well in school?‚Äù we code student responses as a binary variable equal
to one if the student responded ‚ÄúDefinitely, I am much more focused/excited since the Million‚Äù
or ‚ÄúYes, I am more focused/excited since the Million‚Äù and zero if the student responded ‚ÄúMaybe,
I am somewhat more focused since the Million‚Äù or ‚ÄúNo, I was just as focused/excited before the
Million.‚Äù

10

For the question ‚ÄúWhat impact do you think the Million Program has had at your school?
(check all that apply)‚Äù we coded each possible response as a separate binary variable equal to
one if the student checked that response and zero if a student checked at least one other response
but left that one blank. The outcomes include: ‚ÄúStudents are working harder,‚Äù ‚ÄúStudents are
studying more together,‚Äù ‚ÄúStudents are more competitive with each other in a good way,‚Äù
‚ÄúStudents are more competitive with each other in a bad way,‚Äù ‚ÄúStudents and teachers interact
more,‚Äù or ‚ÄúNo difference.‚Äù We code a binary variable equal to one if students respond ‚Äústudents
are working harder‚Äù and zero otherwise.
The students were also asked quiz questions about the importance of educational
attainment based upon text messages that students in the information treatment groups received.
We use the following questions in our analysis (correct answers are in italics):
(1) ‚ÄúAre high school dropouts more likely to go to prison than high school graduates?‚Äù
A. Yes, much more likely
B. Yes, but it‚Äôs really close
C. No, there‚Äôs no difference
(2) ‚ÄúTrue or false: college graduates makes 54% more money than college dropouts.‚Äù
A. True
B. False
(3) ‚Äú15.5% of high school students are unemployed. What percentage of college graduates
are unemployed?‚Äù
A. 1%
B. 4.8%

11

C. 20%
D. 25%
Student responses to each question are recorded as binary variable equal to one if their answer is
correct and a zero if their answer is incorrect. In addition, we analyze a binary variable equal to
one if a student answered questions (1) and (2) correctly and a zero if a student answered at least
one incorrectly.
Question (3) was not referenced in any text message during the year; hence, we consider it a
placebo question.

US Census Data

Black Dissimilarity Index
The Black Dissimilarity Index is a measure of neighborhood segregation relative to the
full city (Jahn, Schmid, and Schrag 1947). The racial composition of each zip code of taken from
the 2000 United States Census, available at http://www.census.gov/epcd/www/zipstats.html. The
dissimilarity index is defined as follows:

ùêµùëôùëéùëêùëò ùê∑ùëñùë†ùë†ùëñùëöùëñùëôùëéùëüùëñùë°ùë¶ ùêºùëõùëëùëíùë• =

ùëõùëúùëõùëèùëôùëéùëêùëòùëßùëñùëù
1 ùëèùëôùëéùëêùëòùëßùëñùëù
ÔøΩ
‚àí
ÔøΩ
2 ùëèùëôùëéùëêùëòùëêùëñùë°ùë¶ ùëõùëúùëõùëèùëôùëéùëêùëòùëêùëñùë°ùë¶

The Black Dissimilarity Index score for a given neighborhood is the absolute difference between
the ratio of the percentage of black individuals who reside in a given zip code to the percentage
of black individuals who live in the city and the ratio of the percentage of non-black individuals
who reside in that zip code to the percentage of non-black individuals who live in the city.
12

Aggregating across zip codes, the dissimilarity index measures the percentage of the city‚Äôs
population that would have to change zip codes for each section to have the same percentage of
black individuals as the city.

Poverty Rates
Poverty rate data by zip code was taken from the 2000 United States Census, available at
http://www.census.gov/epcd/www/zipstats.html and merged to pre-treatment students address
records from district enrollment administrative files.

Add Health
Wave 1 School Interview
Wave 1 was conducted during the 1994-1995 school year. From the school interview, we
collect a variety of baseline information about student demographics, family background, and
academic attitudes.
Each student‚Äôs gender, race, and grade of enrollment at the survey date are provided in
the Wave 1 data. We create dummy variables for female gender and each grade level. We also
code a mutually exclusive and collectively exhaustive set of race dummies: black, white, Asian,
American Indian, Hispanic, and other. Hispanic ethnicity is the absorbing state; that is, student‚Äôs
who respond ‚ÄúYes‚Äù to question S4 ‚ÄúAre you of Hispanic or Spanish origin?‚Äù are categorized as
Hispanic regardless of what they indicated on question S6 ‚ÄúWhat is your race?‚Äù

13

Question S11 [S17] asks ‚Äúdo you live with your biological mother [father], stepmother
[stepfather], foster mother [father], or adoptive mother [father]?‚Äù If a student answers yes to one
and only one of these questions, we enter a value of one for a single-parent-home indicator
variable. If she answers yes to both, we enter a value of one in a two-parent-home indicator.
Question S12 [S18] asks ‚ÄúHow far in school did she [he] go?‚Äù Based on these responses,
we create indicator variables for having a mother who graduated from high school, father who
graduated from high school, mother who graduated from college, and father who graduated from
college. Responses that indicate graduating from college are: ‚Äúgraduated from a college or
university‚Äù and ‚Äúprofessional training beyond college.‚Äù Responses that indicate graduating from
high school include both college responses as well as: ‚Äúwent to college but did not graduate,‚Äù
‚Äúwent to a business, trade, or vocational school after high school,‚Äù ‚Äúcompleted a GED,‚Äù and
‚Äúhigh school graduate.‚Äù Students who respond ‚Äúshe went to school but I don‚Äôt know what level‚Äù
or ‚ÄúI don‚Äôt know if she went to school‚Äù are coded as missing.
Our effort measure is calculated based on students‚Äô responses to question S48: ‚ÄúIn
general, how hard do you try to do your school work well?‚Äù Students select one of the following
responses:
‚Ä¢

I try very hard to do my best

‚Ä¢

I try hard enough, but not as hard as I could

‚Ä¢

I don‚Äôt try very hard

‚Ä¢

I don‚Äôt try at all

We code responses on a 1-4 scale, with 4 indicating ‚ÄúI try very hard to do my best‚Äù and 1
corresponding to ‚ÄúI don‚Äôt try at all.‚Äù We then subtract the sample mean and divide by the
standard deviation to express the index in standard deviation units.

14

Wave 1: Add Health Picture Vocabulary Test
During the first wave, students took the Add-Health Picture Vocabulary Test, a shortened
version of the Peabody Picture Vocabulary Test. The test is structured as follows. For each
question, the student is presented with a set of four illustrations. The administrator reads the
student a word, and the student must select which illustration is best described by the word.
There are 87 questions in all, and the raw scores in the data set have been standardized by age.
To express scores in standard deviation units, we subtract the mean and divide by the standard
deviation within each enrolled grade.

Wave 4: Adult Survey
The fourth and final survey wave was conducted in 2007 and 2008, when the original
respondents were between 24 and 32 years old. We code a variety of outcome measures
responses to this survey.
Respondents are considered employed if they answer ‚ÄúYes‚Äù to the question H4LM11,
‚ÄúAre you currently working for pay at least 10 hours per week?‚Äù They are considered not to be
employed if: they respond ‚ÄúNo‚Äù to the question ‚ÄúHave you ever worked for 9 weeks or more at a
paying job that was at least 10 hours a week? Do not include military service.‚Äù; they respond
‚ÄúNo‚Äù to the question ‚ÄúAre you currently working for pay at least 10 hours per week?‚Äù; or the
respondent is in prison.
Income is derived from question H4EC2: ‚ÄúNow think about your personal earnings. In
{2006/2007/2008}, how much income did you receive from personal earnings before taxes, that
is, wages or salaries, including tips, bonuses, and overtime pay, and income from self-

15

employment?‚Äù Respondents who answer ‚Äúdon‚Äôt know‚Äù are asked to provide their ‚Äúbest guess‚Äù
by choosing from a series of intervals ranging from ‚Äúless than $5,000‚Äù to ‚Äú$150,000 or more.‚Äù
We impute the midpoint of each bounded interval, and impute $200,000 if the respondent
answers ‚Äú$150,000 or more.‚Äù
Question H4RD6 asks ‚Äúwhat is the current status of your marriage to {initials}?‚Äù We
code a married indicator equal to one if the respondent answers ‚Äúliving together,‚Äù ‚Äúliving apart
due to separation,‚Äù ‚Äúliving apart because of other reason such as career, military service, family
illness, etc.‚Äù or refused. We code the indicator as zero if the value is stored as ‚Äúlegitimate skip,‚Äù
indicating that the respondent is not married.
We use responses to question H4TR1 ‚Äì ‚ÄúHow many persons have you ever married? Be
sure to include your current spouse if you are married now.‚Äù ‚Äì to create an indicator for having
ever been married. A response of zero is coded as 0; all values greater than zero are coded as 1.
We use responses to question H4CJ1 ‚Äì ‚ÄúHave you ever been arrested?‚Äù ‚Äì to code an
indicator variable for having ever been arrested. ‚ÄúYes‚Äù responses are coded as one, and ‚ÄúNo‚Äù as
zero. If the interview took place in a prison (determined by the variable PRISON4) we also code
the person to have been arrested.
We use responses to question H4CJ17 ‚Äì ‚ÄúHave you ever spent time in a jail, prison,
juvenile detention center or other correctional facility?‚Äù ‚Äì to create indicators for having ever
been incarcerated. ‚ÄúYes‚Äù responses are coded as one, and ‚ÄúNo‚Äù as zero. If the interview took
place in a prison (determined by the variable PRISON4) we also code the person as incarcerated.
We use responses to question H4EC18 ‚Äì ‚ÄúBetween {1995/2002} and {2006/2007/2008},
did you or others in your household receive any public assistance, welfare payments, or food
stamps?‚Äù ‚Äì to code an indicator for having received public assistance (the first year in brackets is

16

the year the respondent turned 18; the second is the year the survey was administered). ‚ÄúYes‚Äù
responses are coded as one, and ‚ÄúNo‚Äù as zero.
We use responses to question H4EC4 ‚Äì ‚Äú4. Is your house, apartment, or residence owned
or being bought by {YOU AND/OR YOUR SPOUSE/PARTNER}?‚Äù ‚Äì to code an indicator for
owning a home. ‚ÄúYes‚Äù responses are coded as one, and ‚ÄúNo‚Äù as zero.
We use responses to question H4LM21A ‚Äì ‚ÄúDoes Does/Did your employer make the
following available to you: health insurance?‚Äù ‚Äì to code an indicator for receiving health
insurance from one‚Äôs current or most recent position. ‚ÄúYes‚Äù responses are coded as one, and
‚ÄúNo‚Äù as zero.

Wave 4: Weights
In all regressions, we use the grand sample weights calculated for longitudinal analyses
of students interviewed in Waves 1 and 4. The variable in the data is called GSWGT4 _2.

17

Appendix Figure 1: Samsung t401g Cell Phone
a) Closed View b) Open View

Appendix Table 1: Student Baseline Characteristics
Non
Information Information & Non-Financial
Student Characteristics
Participating
Only
Incentives
Incentives
Male
0.521
0.453
0.504
0.453
(0.500)
(0.498)
(0.500)
(0.498)
White
0.200
0.149
0.167
0.163
(0.400)
(0.356)
(0.374)
(0.370)
Black
0.290
0.294
0.327
0.314
(0.454)
(0.456)
(0.469)
(0.465)
Hispanic
0.435
0.469
0.424
0.441
(0.496)
(0.500)
(0.495)
(0.497)
Asian
0.025
0.018
0.031
0.018
(0.155)
(0.134)
(0.172)
(0.134)
Other Race
0.051
0.069
0.051
0.063
(0.220)
(0.254)
(0.220)
(0.244)
Special Education Services
0.149
0.131
0.141
0.147
(0.356)
(0.337)
(0.348)
(0.354)
English Language Learner
0.154
0.165
0.153
0.159
(0.361)
(0.372)
(0.360)
(0.366)
Free Lunch
0.857
0.922
0.920
0.908
(0.351)
(0.268)
(0.271)
(0.289)
Economically Disadvantaged
0.741
0.922
0.914
0.908
(0.438)
(0.268)
(0.280)
(0.289)
Baseline Math
0.010
-0.009
0.028
0.006
(1.022)
(0.978)
(1.035)
(0.993)
Baseline Reading
0.037
-0.086
0.007
-0.053
(1.015)
(1.071)
(1.049)
(0.989)
Missing: Baseline Math
0.319
0.169
0.224
0.237
(0.466)
(0.375)
(0.418)
(0.426)
Missing: Baseline Reading
0.326
0.176
0.231
0.231
(0.469)
(0.381)
(0.422)
(0.422)
p-value from joint F-test
Observations

2903

490

490

490

Control
0.538
(0.499)
0.172
(0.377)
0.309
(0.463)
0.435
(0.496)
0.037
(0.188)
0.048
(0.214)
0.137
(0.345)
0.160
(0.367)
0.918
(0.275)
0.915
(0.279)
0.108
(0.916)
0.062
(0.905)
0.233
(0.423)
0.243
(0.429)

T vs. C.
p-value
0.022
0.799
0.740
0.534
0.199
0.453
0.903
0.964
0.857
0.886
0.400
0.204
0.036
0.055
0.651

437

1907

Notes: This table reports summary statistics for the field experiment. The first 5 columns represent the sample means of
the variable indicated in each row for the group indicated in each column The first column, labeled OKCPS, represents
the mean for 6th and 7th grade students in Oklahoma City Public Schools who are not a part of the experimental group.
The treatment groups are restricted to randomly selected 6th & 7th grade students in Oklahoma City Public Schools
experimental schools who opted into the randomization for the field experiment. The final column represents the pvalue from a test of equality across treatment indicators from a regression of the variable in each row on indicators for
each treatment group and the control group. The joint F-test reports the p-value from a test of equality across treatment
indicators from a multi-variate regression testing the overall quality of the lottery.

Appendix Table 2 - Mean Effect Size (2SLS Estimates) on Direct and Indirect Outcomes
First Stage Reduced Form
2SLS
(1)
(2)
(3)
A. Direct Outcomes
Knows Wage Gap btw BA and Dropouts - Information
0.937‚àó‚àó‚àó
0.049‚àó
0.052‚àó
(0.007)
(0.027)
(0.029)
902
902
902
Knows Prison Rates - Information
0.937‚àó‚àó‚àó
0.179‚àó‚àó‚àó
0.191‚àó‚àó‚àó
(0.007)
(0.038)
(0.040)
891
891
891
Both Quiz Questions Correct - Information
0.937‚àó‚àó‚àó
0.178‚àó‚àó‚àó
0.190‚àó‚àó‚àó
(0.007)
(0.038)
(0.040)
880
880
880
Knows Wage Gap btw BA and Dropouts - Incentives
0.927‚àó‚àó‚àó
0.017
0.018
(0.010)
(0.033)
(0.036)
589
589
589
Knows Prison Rates - Incentives
0.924‚àó‚àó‚àó
-0.046
-0.050
(0.010)
(0.043)
(0.047)
585
585
585
Both Quiz Questions Correct - Incentives
0.926‚àó‚àó‚àó
-0.023
-0.024
(0.010)
(0.043)
(0.046)
576
576
576
B. Indirect Survey Outcomes
More Focused Since Million - Information
0.939‚àó‚àó‚àó
0.151‚àó‚àó‚àó
0.161‚àó‚àó‚àó
(0.007)
(0.037)
(0.039)
910
910
910
Million Makes Students Work Harder - Information
0.938‚àó‚àó‚àó
0.070‚àó
0.075‚àó
(0.007)
(0.037)
(0.040)
916
916
916
More Focused Since Million - Incentives
0.925‚àó‚àó‚àó
0.152‚àó‚àó‚àó
0.165‚àó‚àó‚àó
(0.009)
(0.043)
(0.046)
592
592
592
Million Makes Students Work Harder - Incentives
0.930‚àó‚àó‚àó
0.077‚àó
0.083‚àó
(0.009)
(0.044)
(0.047)
599
599
599
C. Indirect Administrative Data Outcomes
OK State Math Test Post-Treatment - Information
0.941‚àó‚àó‚àó
-0.027
-0.029
(0.005)
(0.039)
(0.042)
1211
1211
1211
OK State Reading Test Post-Treatment - Information
0.939‚àó‚àó‚àó
0.040
0.043
(0.006)
(0.041)
(0.044)
1202
1202
1202
OK State Math Test Post-Treatment - Incentives
0.934‚àó‚àó‚àó
-0.023
-0.025
(0.008)
(0.047)
(0.050)
782
782
782
OK State Reading Test Post-Treatment - Incentives
0.933‚àó‚àó‚àó
0.023
0.025
(0.008)
(0.050)
(0.053)
780
780
780

Notes: This table reports first stage, reduced form, and 2SLS estimates for participation on a variety of outcomes.
First stage estimates report the causal effect of treatment on the percentage of the year each student had access to a
functioning Million cellular phone (number of days without a reported phone problem divided by 225), controlling for
our full set of covariates. Reduced form estimates mirror the ITT estimates presented in earlier tables. 2SLS estimates
use randomized assignment to a treatment group to instrument for time spent with access to a functioning phone; the
estimates can be interpreted as the effect of spending a full year with phone access for treated individuals in each
treatment group. Heteroskedasticity-robust errors are reported in parentheses below each estimate. The number of
observations in each regression is reported directly below the standard errors. *** = significant at 1 percent level, **
= significant at 5 percent level, * = significant at 10 percent level.

Appendix Table 3 - Mean Effect Sizes (Intent-to-Treat) on Direct and Indirect Outcomes
Information Incentives & Non-Financial
Only
Information
Incentives
p-value
A. Treatment Questions
Knows Wage Gap btw BA and Dropouts
0.056‚àó
0.043
0.017
0.686
(0.032)
(0.031)
(0.033)
569
592
589
Knows Prison Rates
0.177‚àó‚àó‚àó
0.172‚àó‚àó‚àó
-0.046
0.000
(0.045)
(0.042)
(0.043)
561
587
585
Both Quiz Questions Correct
0.172‚àó‚àó‚àó
0.175‚àó‚àó‚àó
-0.023
0.001
(0.044)
(0.043)
(0.043)
554
580
576
B. Placebo Question
Knows Unemployment Rate of College Grads
0.035
-0.011
0.047
0.578
(0.042)
(0.041)
(0.043)
573
590
590
C. Survey Questions
More Focused Since Million
0.165‚àó‚àó‚àó
0.132‚àó‚àó‚àó
0.152‚àó‚àó‚àó
0.861
(0.044)
(0.042)
(0.043)
571
594
592
Million Makes Students Work Harder
0.018
0.113‚àó‚àó‚àó
0.077‚àó
0.299
(0.044)
(0.043)
(0.044)
579
599
599
D. Administrative Data Outcomes
OK State Math Test Post-Treatment
0.014
-0.057
-0.023
0.351
(0.046)
(0.045)
(0.047)
794
790
782
OK State Reading Test Post-Treatment
0.071
0.013
0.023
0.623
(0.047)
(0.047)
(0.050)
786
790
780
Attendance Rate
0.004
-0.007
0.034
0.892
(0.063)
(0.064)
(0.063)
856
863
861
Number of Suspensions
0.028
0.031
0.025
0.998
(0.069)
(0.074)
(0.073)
927
927
927

Notes: This table reports ITT estimates for the effect of being offered a chance to participate in the field experiment
on students‚Äô ability to correctly answer questions about human capital development. Questions are coded as a 1
if the student answered the question correctly and a 0 otherwise. All regressions include school fixed effects and
controls for student grade, gender, race, SES, special education status, and English language learner status, as well
as 2009 state test scorse, 2010 state test scores, and their squares. The sample is restricted to randomly selected 6th
and 7th grade students in Oklahoma City Public Schools. Randomization was done at the student level. Treatment
is defined as returning a signed consent form to participate and being lotteried into the specified treatment group.
Heteroskedasticity-robust errors are reported in parentheses below each estimate. The number of observations in each
regression is reported directly below the standard errors. *** = significant at 1 percent level, ** = significant at 5
percent level, * = significant at 10 percent level.

Appendix Table 4 - Mean Effect Sizes (Intent-to-Treat) without Demographic Controls
Non-Financial
Information
Incentives
p-value
A. Treatment Questions
Knows Wage Gap btw BA and Dropouts
0.046‚àó
0.013
0.448
(0.028)
(0.033)
902
589
Knows Prison Rates
0.172‚àó‚àó‚àó
-0.044
0.000
(0.038)
(0.043)
891
585
Both Quiz Questions Correct
0.171‚àó‚àó‚àó
-0.021
0.001
(0.037)
(0.042)
880
576
B. Placebo Question
Knows Unemployment Rate of College Grads
0.017
0.040
0.683
(0.036)
(0.042)
903
590
C. Survey Questions
More Focused Since Million
0.148‚àó‚àó‚àó
0.153‚àó‚àó‚àó
0.934
(0.037)
(0.042)
910
592
Million Makes Students Work Harder
0.069‚àó
0.080‚àó
0.847
(0.037)
(0.043)
916
599
D. Administrative Data Outcomes
OK State Math Test Post-Treatment
-0.031
-0.008
0.717
(0.042)
(0.048)
1211
782
OK State Reading Test Post-Treatment
0.047
0.046
0.987
(0.043)
(0.051)
1202
780
Attendance Rate
-0.012
0.022
0.696
(0.058)
(0.064)
1310
861
Number of Suspensions
0.024
0.024
0.998
(0.063)
(0.074)
1417
927

Notes: This table reports ITT estimates for the effect of being offered a chance to participate in the field experiment on
a variety of outcomes. All regressions include school fixed effects and controls for 2009 state test scores, 2010 state
test scores, and their squares. The sample is restricted to randomly selected 6th and 7th grade students in Oklahoma
City Public Schools. Randomization was done at the student level. Treatment is defined as returning a signed consent
form to participate and being lotteried into the specified treatment group. Heteroskedasticity-robust errors are reported
in parentheses below each estimate. The number of observations in each regression is reported directly below the
standard errors. *** = significant at 1 percent level, ** = significant at 5 percent level, * = significant at 10 percent
level.

Table A5 - Differences in Outcomes by Message Dosage
Information Persuasion
Dose
Dose
(1)
(2)
A. Treatment Questions
Knows Wage Gap btw BA and Dropouts
0.338
-0.086
(0.309)
(0.344)
171
171
Knows Prison Rates
0.416
-0.368
(0.318)
(0.369)
168
168
Both Quiz Questions Correct
0.595‚àó
-0.325
(0.312)
(0.369)
166
166
B. Placebo Question
Knows Unemployment Rate of College Grads
-0.441
0.448
(0.335)
(0.381)
172
172
C. Survey Questions
More Focused Since Million
0.359
-0.316
(0.343)
(0.419)
173
173
Million Makes Students Work Harder
-0.235
0.398
(0.375)
(0.418)
173
173
D. Administrative Data Outcomes
OK State Math Test Post-Treatment
0.538
-0.348
(0.491)
(0.558)
206
206
OK State Reading Test Post-Treatment
0.058
0.397
(0.478)
(0.511)
205
205

p-value
(1)=(2)
(3)
0.503

0.234

0.163

0.200

0.362

0.410

0.388

0.722

Notes: This table reports OLS estimates for the effect of receiving a full dose of informational and persuasive informational texts for individuals in the informational treatment groups who experienced some period of time without access
to a functioning phone. Columns (1) and (2) respectively report the coefficient on the proportion of informational and
persuasive texts a student received. A student is considered to have received a given informational or persuasive text
if he or she was randomly assigned to an informational treatment group and did not report a problem with his or her
phone (e.g., technical problems, stolen phone, lost phone, etc. Column (3) All regressions include school fixed effects
and controls for 2009 state test scores, 2010 state test scores, and their squares. The sample is restricted to individuals
in the informational treatment groups who experienced some period of time without access to a functioning phone.
Heteroskedasticity-robust errors are reported in parentheses below each estimate. The number of observations in each
regression is reported directly below the standard errors. *** = significant at 1 percent level, ** = significant at 5
percent level, * = significant at 10 percent level.

Appendix Table 6: Correlation of Self-Reported
Effort With Adult Outcomes in Add Health
Sample
Raw
Controlled
Mean
Correlation Correlation
Employed
0.801
0.008‚àó
0.013‚àó‚àó‚àó
(0.005)
(0.005)
10889
10889
Earnings (Annual)

34021

244.096
(513.538)
10709

1131.129‚àó‚àó
(511.680)
10709

Job Provides Health Insurance

0.699

0.008
(0.006)
10910

0.008
(0.006)
10910

Receives Public Assistance

0.247

-0.010‚àó
(0.005)
10918

-0.016‚àó‚àó‚àó
(0.005)
10918

Married

0.446

0.020‚àó‚àó‚àó
(0.007)
10616

0.017‚àó‚àó
(0.007)
10616

Ever Arrested

0.300

-0.068‚àó‚àó‚àó
(0.007)
10904

-0.052‚àó‚àó‚àó
(0.006)
10904

Ever Incarcerated

0.162

-0.042‚àó‚àó‚àó
(0.006)
10858

-0.032‚àó‚àó‚àó
(0.006)
10858

Homeowner

0.413

0.002
(0.007)
10915

0.000
(0.007)
10915

NOTES: This table reports raw and controlled correlations between self-reported effort and various adult outcomes in the National Longitudinal
Survey of Adolescent Health (Add Health). The dependent variable is a standardized index based on the student‚Äôs response to the question ‚ÄùIn
general, how hard to you try to do your school work well?‚Äù See the Online Appendix for definitions and sources of all other variables. All
regressions include fixed effects for the student‚Äôs school and enrolled grade at the time of the baseline survey. Controlled regressions also control for
race, gender, mother‚Äôs education, father‚Äôs education, number of biological parents living with the student, and the student‚Äôs score on the Add Health
Picture Vocabulary Test (AHPVT), taken during the baseline interview. All estimates use the post-stratification grand sample weights provided in
the dataset. Robust standard errors are reported in parentheses.

Appendix Table 7a: Student Baseline Characteristics for Black Dissimilarity Index Subgroups
Above Median Black Dissimilarity Index
Below Median Black Dissimilarity Index
Pooled
Non-Financial
p-value
Pooled
Non-Financial
p-value
Information
Incentives
Control (1)=(2)=(3) Information
Incentives
Control (5)=(6)=(7)
Student Characteristics
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
Male
0.475
0.480
0.502
0.790
0.483
0.419
0.574
0.005
(0.500)
(0.501)
(0.501)
(0.500)
(0.494)
(0.496)
White
0.104
0.138
0.122
0.358
0.219
0.195
0.222
0.745
(0.306)
(0.346)
(0.328)
(0.414)
(0.397)
(0.417)
Black
0.342
0.327
0.308
0.662
0.275
0.298
0.310
0.608
(0.475)
(0.470)
(0.463)
(0.447)
(0.458)
(0.464)
Hispanic
0.504
0.458
0.516
0.362
0.383
0.419
0.352
0.363
(0.500)
(0.499)
(0.501)
(0.487)
(0.494)
(0.479)
Asian
0.006
0.007
0.009
0.884
0.045
0.033
0.065
0.279
(0.076)
(0.085)
(0.095)
(0.209)
(0.178)
(0.247)
Other Race
0.044
0.069
0.045
0.293
0.078
0.056
0.051
0.330
(0.206)
(0.254)
(0.208)
(0.268)
(0.230)
(0.220)
Special Education
0.122
0.138
0.140
0.709
0.152
0.158
0.134
0.766
(0.327)
(0.346)
(0.348)
(0.359)
(0.366)
(0.342)
ELL
0.178
0.185
0.208
0.621
0.139
0.126
0.111
0.604
(0.383)
(0.389)
(0.407)
(0.346)
(0.332)
(0.315)
Free Lunch
0.969
0.935
0.964
0.061
0.868
0.874
0.870
0.973
(0.173)
(0.248)
(0.187)
(0.339)
(0.332)
(0.337)
Low SES
0.965
0.949
0.977
0.238
0.866
0.856
0.852
0.870
(0.183)
(0.220)
(0.149)
(0.341)
(0.352)
(0.356)
Baseline Math
-0.108
-0.110
-0.023
0.544
0.147
0.164
0.272
0.447
(0.924)
(0.981)
(0.886)
(1.080)
(0.991)
(0.930)
Baseline Reading
-0.137
-0.148
-0.108
0.905
0.074
0.076
0.270
0.161
(0.955)
(0.906)
(0.823)
(1.165)
(1.082)
(0.958)
Missing: Math
0.176
0.215
0.154
0.195
0.221
0.265
0.315
0.029
(0.381)
(0.411)
(0.362)
(0.415)
(0.442)
(0.466)
Missing: Reading
0.181
0.207
0.176
0.605
0.227
0.260
0.310
0.069
(0.386)
(0.406)
(0.382)
(0.420)
(0.440)
(0.464)
p-value from joint F-test
0.571
0.085
Observations

518

275

221

1014

462

215

216

Notes: This table reports summary statistics for students in each Black Dissimilarity Index subgroup. Columns (1),
(2), (3), (5), (6), and (7) represent the sample means of the variable indicated in each row for the group indicated in
each column The treatment groups are restricted to randomly selected 6th and 7th grade students in Oklahoma City
Public Schools experimental schools who opted into the randomization for the field experiment. Columns (4) and (8)
reports the p-value from a test of equality across treatment indicators from a regression of the variable in each row on
indicators for each treatment group and the control group within each subgroup. The joint F-tests report the p-value
from a test of equality across treatment indicators from a multi-variate regression testing the overall quality of the
lottery within each subgroup.

893

Appendix Table 7b: Student Baseline Characteristics for Zip Code Poverty Rate Subgroups
Above Median Zip Code Poverty Rate
Below Median Zip Code Poverty Rate
Pooled
Non-Financial
p-value
Pooled
Non-Financial
p-value
Information
Incentives
Control (1)=(2)=(3) Information
Incentives
Control (5)=(6)=(7)
Student Characteristics
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
Male
0.469
0.423
0.531
0.115
0.485
0.477
0.542
0.221
(0.500)
(0.495)
(0.501)
(0.500)
(0.500)
(0.499)
White
0.090
0.131
0.117
0.257
0.203
0.188
0.204
0.850
(0.286)
(0.339)
(0.323)
(0.403)
(0.391)
(0.403)
Black
0.336
0.291
0.278
0.309
0.293
0.332
0.327
0.410
(0.473)
(0.455)
(0.449)
(0.456)
(0.472)
(0.470)
Hispanic
0.500
0.512
0.525
0.865
0.412
0.386
0.382
0.628
(0.501)
(0.501)
(0.501)
(0.493)
(0.488)
(0.487)
Asian
0.008
0.019
0.019
0.411
0.036
0.018
0.047
0.162
(0.087)
(0.136)
(0.135)
(0.185)
(0.133)
(0.213)
Other Race
0.067
0.047
0.062
0.621
0.056
0.076
0.040
0.191
(0.250)
(0.212)
(0.241)
(0.230)
(0.265)
(0.196)
Special Education
0.115
0.117
0.160
0.318
0.149
0.170
0.124
0.312
(0.320)
(0.323)
(0.368)
(0.357)
(0.376)
(0.330)
ELL
0.162
0.155
0.185
0.716
0.158
0.162
0.145
0.847
(0.368)
(0.363)
(0.390)
(0.365)
(0.370)
(0.353)
Free Lunch
0.959
0.939
0.969
0.335
0.897
0.884
0.887
0.839
(0.199)
(0.240)
(0.173)
(0.305)
(0.320)
(0.317)
Low SES
0.951
0.944
0.969
0.503
0.897
0.881
0.884
0.738
(0.216)
(0.231)
(0.173)
(0.305)
(0.325)
(0.321)
Baseline Math
-0.106
-0.098
0.199
0.008
0.078
0.079
0.056
0.960
(0.949)
(0.953)
(0.918)
(1.033)
(1.016)
(0.914)
Baseline Reading
-0.192
-0.158
0.141
0.005
0.053
0.020
0.019
0.886
(1.006)
(0.853)
(0.951)
(1.083)
(1.070)
(0.878)
Missing: Math
0.238
0.277
0.259
0.574
0.169
0.206
0.218
0.176
(0.427)
(0.449)
(0.440)
(0.376)
(0.405)
(0.414)
Missing: Reading
0.233
0.272
0.278
0.420
0.183
0.199
0.222
0.407
(0.423)
(0.446)
(0.449)
(0.387)
(0.400)
(0.416)
p-value from joint F-test
0.123
0.437
Observations

390

213

162

765

590

277

275

Notes: This table reports summary statistics for students in each Zip Code Poverty Rate subgroup. Columns (1),
(2), (3), (5), (6), and (7) represent the sample means of the variable indicated in each row for the group indicated in
each column The treatment groups are restricted to randomly selected 6th and 7th grade students in Oklahoma City
Public Schools experimental schools who opted into the randomization for the field experiment. Columns (4) and (8)
reports the p-value from a test of equality across treatment indicators from a regression of the variable in each row on
indicators for each treatment group and the control group within each subgroup. The joint F-tests report the p-value
from a test of equality across treatment indicators from a multi-variate regression testing the overall quality of the
lottery within each subgroup.

1142

Student Characteristics
Male
White
Black
Hispanic
Asian
Other Race
Special Education
ELL
Free Lunch
Low SES
Baseline Math
Baseline Reading
Missing: Math
Missing: Reading
p-value from joint F-test
Observations

Appendix Table 7c: Student Baseline Characteristics for TVA Subgroups
Above Median TVA
Below Median TVA
Pooled
Non-Financial
p-value
Pooled
Non-Financial
Information
Incentives
Control (1)=(2)=(3) Information
Incentives
Control
(1)
(2)
(3)
(4)
(5)
(6)
(7)
0.490
0.489
0.541
0.482
0.474
0.438
0.541
(0.501)
(0.501)
(0.500)
(0.500)
(0.497)
(0.500)
0.219
0.226
0.213
0.953
0.117
0.146
0.138
(0.414)
(0.420)
(0.411)
(0.322)
(0.354)
(0.346)
0.219
0.253
0.284
0.216
0.399
0.349
0.343
(0.414)
(0.436)
(0.452)
(0.490)
(0.478)
(0.476)
0.462
0.416
0.383
0.173
0.411
0.438
0.453
(0.499)
(0.494)
(0.487)
(0.493)
(0.497)
(0.499)
0.045
0.037
0.066
0.403
0.010
0.005
0.017
(0.208)
(0.189)
(0.248)
(0.098)
(0.072)
(0.128)
0.055
0.068
0.055
0.793
0.063
0.063
0.050
(0.229)
(0.253)
(0.228)
(0.244)
(0.243)
(0.218)
0.108
0.084
0.120
0.507
0.122
0.146
0.122
(0.311)
(0.278)
(0.326)
(0.327)
(0.354)
(0.328)
0.166
0.132
0.126
0.345
0.134
0.151
0.155
(0.372)
(0.339)
(0.332)
(0.341)
(0.359)
(0.363)
0.849
0.816
0.874
0.288
0.971
0.958
0.945
(0.358)
(0.389)
(0.332)
(0.169)
(0.200)
(0.229)
0.852
0.832
0.863
0.682
0.964
0.964
0.950
(0.356)
(0.375)
(0.344)
(0.188)
(0.188)
(0.218)
0.289
0.301
0.276
0.977
-0.175
-0.121
-0.040
(0.998)
(0.952)
(0.935)
(0.918)
(0.946)
(0.860)
0.261
0.323
0.262
0.808
-0.239
-0.212
-0.112
(1.076)
(1.007)
(0.893)
(0.967)
(0.832)
(0.884)
0.148
0.189
0.180
0.381
0.187
0.193
0.227
(0.356)
(0.393)
(0.386)
(0.391)
(0.395)
(0.420)
0.161
0.189
0.186
0.615
0.192
0.188
0.243
(0.368)
(0.393)
(0.390)
(0.395)
(0.391)
(0.430)
0.815
398

190

183

771

411

192

181

Notes: This table reports summary statistics for students in each TVA subgroup. Columns (1), (2), (3), (5), (6), and (7)
represent the sample means of the variable indicated in each row for the group indicated in each column The treatment
groups are restricted to randomly selected 6th and 7th grade students in Oklahoma City Public Schools experimental
schools who opted into the randomization for the field experiment. Columns (4) and (8) reports the p-value from a test
of equality across treatment indicators from a regression of the variable in each row on indicators for each treatment
group and the control group within each subgroup. The joint F-tests report the p-value from a test of equality across
treatment indicators from a multi-variate regression testing the overall quality of the lottery within each subgroup.

p-value
(5)=(6)=(7)
(8)
0.124
0.560
0.305
0.605
0.547
0.804
0.683
0.747
0.302
0.726
0.334
0.397
0.535
0.305
0.642
784

