NBER WORKING PAPER SERIES

MEASURING TEST MEASUREMENT ERROR:
A GENERAL APPROACH
Donald Boyd
Hamilton Lankford
Susanna Loeb
James Wyckoff
Working Paper 18010
http://www.nber.org/papers/w18010
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
April 2012

We gratefully acknowledge support from the National Science Foundation and the Center for Analysis
of Longitudinal Data in Education Research (CALDER). Thanks also to Dale Ballou, Daniel McCaffrey
and Jeffery Zabel for their helpful comments. The authors are solely responsible for the content of
this paper. The views expressed herein are those of the authors and do not necessarily reflect the views
of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2012 by Donald Boyd, Hamilton Lankford, Susanna Loeb, and James Wyckoff. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.

Measuring Test Measurement Error: A General Approach
Donald Boyd, Hamilton Lankford, Susanna Loeb, and James Wyckoff
NBER Working Paper No. 18010
April 2012
JEL No. I21
ABSTRACT
Test-based accountability including value-added assessments and experimental and quasi-experimental
research in education rely on achievement tests to measure student skills and knowledge. Yet we know
little regarding important properties of these tests, an important example being the extent of test measurement
error and its implications for educational policy and practice. While test vendors provide estimates
of split-test reliability, these measures do not account for potentially important day-to-day differences
in student performance.
We show there is a credible, low-cost approach for estimating the total test measurement error that
can be applied when one or more cohorts of students take three or more tests in the subject of interest
(e.g., state assessments in three consecutive grades). Our method generalizes the test-retest framework
allowing for either growth or decay in knowledge and skills between tests as well as variation in the
degree of measurement error across tests. The approach maintains relatively unrestrictive, testable
assumptions regarding the structure of student achievement growth. Estimation only requires descriptive
statistics (e.g., correlations) for the tests. When student-level test-score data are available, the extent
and pattern of measurement error heteroskedasticity also can be estimated. Utilizing math and ELA
test data from New York City, we estimate the overall extent of test measurement error is more than
twice as large as that reported by the test vendor and demonstrate how using estimates of the total
measurement error and the degree of heteroskedasticity along with observed scores can yield meaningful
improvements in the precision of student achievement and achievement-gain estimates.
Donald Boyd
The Center for Policy Research
University of Albany
135 Western Ave.
Albany, NY 12222
donboyd5@gmail.com

Susanna Loeb
524 CERAS, 520 Galvez Mall
Stanford University
Stanford, CA 94305
and NBER
sloeb@stanford.edu

Hamilton Lankford
School of Education, ED 317
University at Albany
State University of New York
Albany, NY 12222
hamp@albany.edu

James Wyckoff
Curry School of Education
University of Virginia
P.O. Box 400277
Charlottesville, VA 22904-4277
wyckoff@virginia.edu

Recent educational policies such as test-based accountability, teacher evaluation and
experimental and quasi-experimental research in education rely on achievement tests as an
important metric to assess student skills and knowledge. Yet we know little regarding the
properties of these tests that bear directly on their use and interpretation. For example, evidence
is often scarce regarding the extent to which standardized tests are aligned with educational
standards or the outcomes of interest to policymakers or analysts. Similarly, we know little about
the extent of test measurement error and the implications of such error for educational policy and
practice. While test vendors provide estimates of reliability, these estimates capture only one of a
number of different sources of error.
This paper focuses on test measurement error and demonstrates a credible approach for
estimating the overall extent of error. For the tests we analyze, the measurement error is at least
twice as large as that indicated in the technical reports provided by test vendors. Such error in
measuring student performance results in measurement error in the estimation of teacher
effectiveness, school effectiveness and other measures based on student test scores. The
relevance of test measurement error in assessing the usefulness of measures such as teacher
value-added or schools’ adequate yearly progress often is noted but not addressed, due to the
lack of easily implemented methods for quantifying the overall extent of measurement error.
This paper demonstrates a technique for estimating the total measurement error and provides
evidence of the importance of doing so
Thorndike (1951) articulates a variety of factors which can result in a test score being a
noisy measure of student achievement. Technical reports produced by test vendors provide
information regarding test measurement error as defined in classical test theory and the IRT
framework. For both, the focus is on the measurement error associated with the test instrument

1

(e.g., randomness in the selection of test items and the raw-score to scale-score conversion). This
information is useful, but provides no information regarding the measurement error from other
sources (e.g., students having particularly good or bad days).
Reliability coefficients based on the test-retest approach using parallel test forms is
recognized in the psychometric literature as the gold standard for quantifying measurement error
from all sources. Students take alternative, but parallel (i.e., interchangeable), tests on two or
more occurrences sufficiently separated in time so as to allow for the “random variation within
each individual in health, motivation, mental efficiency, concentration, forgetfulness,
carelessness, subjectivity or impulsiveness in response and luck in random guessing”1 but
sufficiently close in time that the knowledge, skills and abilities of individuals taking the tests are
unchanged. However, there are relatively few examples of this approach to measurement error
estimation in practice, especially in the analysis of student achievement tests used in high-stakes
settings.
Rather than analyzing the consistency of student test scores over occurrences, the
standard approach used by test vendors is to divide the test taken at a single point in time into
what is hoped to be parallel parts. Reliability measured with respect to the consistency (i.e.,
correlation) of students’ scores across these parts only accounts for the measurement error
resulting from the random selection of a set of test items from the relevant population of items.
As Feldt and Brennan (1989) note, this approach “frequently present[s] a biased picture” in that
“reported reliability coefficients tend to overstate the trustworthiness of educational
measurement, and standard errors underestimate within-person variability.” The problem is that
measures based on a single test occurrence ignore potentially important day-to-day differences in
student performance.
1

Feldt and Brennan (1989).

2

In this paper we show that there is a credible approach for measuring the overall extent of
test measurement error that can be applied in a wide variety of settings. Estimation is
straightforward and only requires estimates of the correlation or covariance of test scores in the
subject of interest at several points in time (e.g., the correlations between third-, fourth- and fifthgrade math scores for one cohort of students).2 Note that student-level test-score data is not
needed, provided that estimates of test-score correlations or covariances are available. Our
approach generalizes the test-retest framework to allow for either growth or decay in the
knowledge, skills and abilities of students between the test administrations as well as variation
across tests in the extent of measurement error. Utilizing the estimated test-score covariance or
correlation matrix and a few assumptions regarding the structure of student achievement growth,
it is possible to estimate the overall extent of test measurement error and decompose the variance
of test scores into the part attributable to real differences in academic achievement and the part
attributable to measurement error.
In the following section we briefly introduce generalizability theory, a framework for
characterizing multiple sources of test measurement error, and show how the total measurement
error is reflected in the covariance structure of observed test scores. This is followed by an
explanation of our statistical approach. In turn, we report estimates of the overall extent of
measurement error associated with New York State assessments in math and English language
arts (ELA) and how the extent of test measurement error varies across ability levels. We
conclude with a summary and a brief discussion of ways in which information regarding the

2

As discussed below, it is necessary that the underlying knowledge one is attempting to measure is measurable
using a vertical scale. However, each test instrument employed need only be measured on an interval scale. The
interval scales can differ as long as they are linear transformations of the underlying vertical scale (even if this linear
transformation is unknown).

3

extent of test measurement error can be informative in analyses related to educational practice
and policy.
1.0 Defining Test Measurement Error
From the perspective of classical test theory, an individual’s observed test score is the
sum of two components: the true score representing the expected value of test scores over some
set of test replications, and the residual difference, or random error, associated with test
measurement error.3 Generalizability theory, which we draw upon here, extends test theory to
explicitly account for multiple sources of measurement error.4
Consider the case where a student takes a test consisting of a set of tasks (e.g., questions)
administered at a particular point in time. Each task, t, is assumed to be drawn from some
universe of similar conditions of measurement with the student doing that task at some point in
time. The universe of possible occurrences is such that the student’s knowledge/skills/ability is
the same for all feasible times. Here students are the object of measurement and are assumed to
be drawn from some population. As is typical, we assume the numbers of students, tasks and
occurrences that could be observed are infinite. The case where each pupil, i, might be asked to
complete each task at each of the possible occurrences is represented by i X t X o where the
symbol “ X ” is read “crossed with.”
Let Sito represent the ith student’s score on task t carried out at occurrence o, which can
be decomposed using the random-effects specification in Equation 1.

Sito  i t o it io to   ito (1)

3

Classical test theory is the focus of many books and articles. For example, see Haertel (2006).
See Brennan (2001) for a detailed development of Generalizability Theory. The basic structure of the framework is
outlined in Conbach, Linn, Brennan and Haertel (1997) as well as Feldt and Brennan (1988).
4

4

The universe score of a student,  i    i , equals the expected value of Sito over the universe of
generalization, here the universes of possible tasks and occurrences. The universe score is
comparable to the true score as defined in classical test theory. In our case,  i measures the
student’s underlying academic achievement, e.g., ability, knowledge and skills. The  ’s
represent a set of uncorrelated random effects which, along with  ito and the student’s universe
score, sum to Sito . Here t and  o , respectively, reflect the random effect, common to all testtakers, associated with scores for a particular task and a particular occurrence differing from the
population mean,  . it reflects the fact that a student might do especially well or poorly on a
particular task.  io is the measurement error associated with a student’s performance not being
temporally stable even when his or her underlying ability is unchanged (e.g., a student having a
particularly good or bad day, possibly due to illness or fatigue).  to reflects the possibility that
the performance of all students on a particular task might vary across occurrences.  ito reflects
the three-way interaction and other random effects. Even though there are other potential sources
of measurement error, we limit the number here to simplify the exposition.5
The observed score for a particular individual completing a task will differ from the
individual’s universe score because of the components of measurement error shown in Equation
2. In turn, the measurement error variance decomposition for a particular student and a single
task is shown in Equation 3.

ito   Sito  i   t  o  it   po to   ito

(2)

 2 ito    2  t    2  o    2  it    2 io    2 to    2  ito 

5

(3)

As noted above, Thorndike (1951, p. 568) provides a taxonomy characterizing different sources of measurement
error. The above framework also can be generalized to reflect students being grouped within schools and classrooms
and there being common random components of measurement error at those levels.

5

Now consider a test defined in terms of its timing (occurrence) and the NT tasks making up the
examination. The student’s actual score, SiT , will equal  i iT as shown in Equation 4, where

iT is a composite measure reflecting the errors in test measurement from all sources.6
SiT   t Sit NT    i  o  io   t t  it  to   ito  NT   i  iT (4)

The variance of iT for student i equals

2   2  o    2  io    2  t    2  it    2  to    2  ito  / NT .
iT

2.0 Test-Score Covariance Structure
We generalize the notation in Equation 4 to allow for multiple tests, for exposition here
assumed to be in multiple grades. In Equation 5 Sij is the ith student’s score on a test for a

Sij   ij  ij

(5)

particular subject taken in the jth tested grade.7  ij is the ith student’s true academic achievement
in that subject and grade. We drop subscript “T’ to simplify notation, but maintain that a
different test in a single occurrence is given in each grade and period. ij is the corresponding
test measurement error from all sources, where Eij  0 . Allowing for the possibility of
heteroskedasticity across grades and students, Eij2  2ij . Let  2 j equal 2ij for all pupils in
the homoskedastic case or, more generally, the mean value of 2ij for the universe of students in

6

Here we represent the score as the mean over the set of test items. An alternative would be to employ SiT   t Sit ,

e.g., the number of correct items.
In general, time intervals between tests need not be annual nor constant. For example, from a randomized control
trial one might know test-score correlations for tests administered at the start and end of the experiment as well as a
test given at some point during the experiment.

7

6

grade j. The  in Equation 1 being uncorrelated implies that Eijik  0,  j  k and

Eij ik  0, j, k .
Using vector notation, Si   i i where Si   Si1 Si 2

i  i1 i 2

SiJ  ,  i   i1  i 2

 iJ  , and

iJ  for the first (j=1) through the Jth tested grades8. Equation 6 defines  (i ) to

be the auto-covariance matrix for the ith student’s observed test scores.  is the auto-covariance
matrix for the universe scores in the population of students. i is the diagonal matrix with the
measurement-error variances across grades for the ith student (e.g.,  2ij ) on the diagonal.
(i )  E  Si  ESi  Si  ESi    E  i  E i  i  E ii   




i11 i12

i 22
  i 21


iJ 1 iJ 2

E (ii' )

 2i ,1

 0
 



 0
 JJ 


i1J 
i 2 J 

  11  12

 22
  21




iJJ 
 J 1  J 2

 1J 
 2 J 

0

 2

i ,2

0

0

0 

0 
    i
0 
 2i ,J 

(6) For the population of all students, 2 j  E2ij and   E(i)     where  is the
diagonal matrix with 21 , 22 ,..., 2 J on the diagonal. Note that  (i ) differs from (i ') only
because of possible heteroskedastic measurement error across test-takers.
For a variety of reasons, researchers and policymakers are interested in the
decomposition of the overall variance of observed scores for students in a particular grade,  jj ,
into the variance in universe scores across the student population,  jj , and the measurement-error
variance;  jj   jj   2 j . The corresponding generalizability coefficient, G j   jj  jj , measures
the portion of the total variation in observed scores that is explained by the variance of universe
8

For example, the third grade might be the first tested grade. To simplify exposition, we often will not distinguish
between the ith grade and the ith tested grade, even though we will mean the latter. Again, the assessments need not
be annual; the situation might be one in which several tests are given during a particular year.

7

scores. The reliability coefficient is the comparable measure in classical test theory. This
measure implies the characterization of  shown in Equation 7.
11 12 13

22 23
  

33



 12
  11 G1
 
 22 G2

 
 
 

 13
 23
 33 G3








(7)

 can be estimated using its empirical counterpart    i Si Si' N S where N S is the number
of students for whom test scores are observed.9
Let  jk represent the correlation between the universe scores in grades j and k;

 jk   jk

 jj  kk . This notation along with Equation 7 yields the test-score correlation matrix

 shown in Equation 8. Note that the presence of test measurement error (e.g., G j  1 ) implies

1 r12

1





r13
r23
1

1
 
 

 
 
 


G1G2 12

G1G3 13

G1G4 14

1

G2G3  23

G2G4  24

1

G3G4 34
1




 (8)





that each correlation of test scores is smaller than the correlation of the corresponding universe
scores (e.g., rjk   jk ). In contrast,  jk   jk , j  k , so that estimates of the off-diagonal
elements of the covariance matrix   (i.e., ˆ ij ) directly imply estimates of the off-diagonal
elements of  in Equation 7, i.e., ˆij . However, we are primarily interested in separate estimates

9

This corresponds to the case where one or more student cohorts are tracked through all J grades, a key assumption
being that the values of the  jk are constant across the cohorts. A subset of the  jk can be estimated when the
scores for individual students only span a subset of the grades included; a particular  jk can be estimated provided
one has test score data for students in both grades j and k. For example,  j , j 1 , j  1,2,3 can be estimated using
test-score data for first-, second- and third-grade students in year-one and the same students in the next grade a year
later.

8

of the universe-score and measurement-error variances, both of which enter the diagonal
elements of   . Test-score data can be used to estimate the  jk , but these estimates by
themselves are not sufficient to infer estimates of the  jj and G j .
Assuming nothing more than one of the structures typically maintained by researchers
estimating models of student achievement growth, the parameters in Equation 7 characterizing
the covariance of universe scores (i.e., the  jk ) can be expressed as functions of a smaller set of
elemental parameters (e.g.,  22 frequently is a function of  11 and other elemental parameters).
Taking advantage of such structure, estimates of the  jk can be used to infer estimates of these
elemental parameters, including  11 and G j . In general, the structure maintained needs to imply
that the values of at least J of the parameters on the right-hand side of Equation 7 are implied by
the values of the remaining parameters.10 In a similar way, the representation of  in Equation 8
can be used to estimate the G j ; the structures of underlying growth models imply restrictions on
the structure of the  jk so that test-score correlations can be used to infer estimates of the
underlying parameters and the G j . The central point of our paper is that these methods allow the
overall extent of test measurement error to be easily estimated.
Our estimation strategy is closely linked to frameworks laid out by Joreskog (1971, 1978)
and Abowd and Card (1989). Abowd and Card develop a framework for studying the covariance
structure of individual- and household-level earnings, hours worked and other time-series
variables. Their approach falls within the general framework for the analysis of covariance
structures developed by Joreskog (1978). Joreskog (1971) employs the kernel of this approach to
10

Suppose m parameters on the right-hand side of (7) are known functions of the elemental parameters. If m  J ,
the number of moments, J ( J  1) / 2 , will equal or exceed the number of elemental parameters,  J ( J  3) / 2   m ,
one needs to estimate.

9

analyze the covariance of congeneric tests. In classical test theory, the set of K tests
Sik   ik  ik , k  1, 2,

, K , is said to be congenetric if the true scores,  ik , are such that there

is a common  i where  ik  0k  1k i , k , i ; here the true scores across tests are perfectly
correlated. We consider the case where the  ik need only be correlated to some degree,
following some systematic pattern. This is a generalization of both the test-retest approach and
the somewhat more general framework of Joreskog (1971) for estimating the extent of test
measurement error and falls within his general approach for the analysis of covariances
(Joreskog, 1978).
3.0 Estimation Strategy
3.1 General Approach
We assume that academic achievement, measured by universe scores, is cumulative:

 ij   j 1 i , j 1  ij

(9)

This first-order autoregressive structure models student attainment in grade j as depending upon
the level of knowledge and skills in the prior grade11 possibly subject to decay (if  j 1  1 )
where the rate of decay can differ across grades. A key assumption is that decay is not complete,
as would be the case if  j  0 .  j   for all j is a special case. The further simplification

 j  1 is maintained in many value-added analyses. ij is the gain in student achievement in
grade j, gross of any decay. 12
For empirical growth models to actually measure growth in the underlying achievement
of students, the test(s) used to measure achievement must reflect a single interval scale, meaning
11
12

Todd and Wolpin (2003) discuss the conditions under which this will be the case.
In the special case where  j  1 ,  ij is the student’s gain in achievement while in grade j. However, we will refer

to  ig as the student’s achievement gain even when  j  1 .

10

that "equal-sized gains at all points on the scale represent the same increment of learning".13 For
example, the tests used to estimate  i1 ,  i 2 ,

in Equation 9 must all reflect a common vertical

scale. As discussed by Ballou (2009): (1) the underlying assumptions regarding test items and
test takers needed to assure interval scaling are quite restrictive, (2) those employing test scores
in empirical work typically cannot test those assumptions and (3) descriptive statistics for tests of
venders claiming their tests are vertically scaled often have properties that bring into question
whether this is actually the case. A set of exams not being vertically scaled could be the result of
the knowledge and skills being tested not being measurable on a single interval scale. However,
the lack of vertical scaling instead could be the result problems in test construction.
The prevalence of questions regarding whether test scales are the same across grades and
years explains why analysts often standardize test scores by grade and year to have zero means
and unit standard deviations. Empirical analysis employing standardized scores can only provide
information regarding the movements of students within the achievement distribution from grade
to grade. Fortunately, our approach need only employ test-score correlations. Thus, the
individual tests each need to reflect an interval scale, but the scales can differ from grade to
grade.14 Whether or not the tests are vertically scaled, the extent of test measurement error for the
individual tests, as measured by the G j , can be inferred. After first considering situations in
which tests are vertically scaled, we discuss the more general and simpler approach that can be
employed even when the tests are not necessarily vertically scaled.
Equation 9 can be used to infer Equation 10, which shows that each  ij reflects the

 ij  ij   j 1i , j 1   j 1 j 2i , j 2 

 ( j 1 j 2

13

 j ( s 1)i , j ( s 1) )  ( j 1 j 2

 j  s i , j  s ) (10)

Ballou (2009).
One can think of the underlying model corresponding to the case where actually achievement across grades falls
on the same interval scale, but the tests instruments employed need not have that property.
14

11

accumulation of decayed values of prior ij . As is true in other time-series models, one can
assume that the sum in Equation 10 extends back to an infinite past (i.e., s   ). A more
attractive alternative in our application is to assume that the pertinent time-series for each student
begins at a specified point in time (e.g., when she first enters school or the grade in which she is
first tested) and employ initial conditions to measure the knowledge and skills of each student at
that point in time (e.g.,  i , j  s for student i where j  s is the starting point). These initial
conditions together with Equation 10 and the statistical structure of the ij determine the
dynamic pattern of universe scores reflected in the parameterization of   E ( i  i' ) and  .
Two approaches can be used to characterize the statistical structure of the ij . One
approach is to fully specify the relationship of achievement gains across grades. For example, in
one specification discussed in Appendix A, we assume that ij  i   ij where i is a studentlevel random effect and  ij is white noise. An alternative approach is to assume nothing more
than that the joint distribution of i, j 1 and  ij is such that the conditional mean E (i, j 1 |  ij ) is
a linear function of  ij . Because of its simplicity and generality, we focus on the reduced-form
framework. Several structural models are discussed in Appendix A.
3.2 A Reduced-Form Model
Note that i , j 1  E i , j 1 |  ij   ui , j 1 where ui , j 1  i , j 1  E i , j 1 |  ij  and E ui , j 1 ij  0 .
The assumption that such conditional mean functions are linear in parameters is at the core of
regression analysis. We go a step further and assume that E i , j 1 |  ij  is a linear function of  ij ,
or more generally that such a linear relationship is a reasonably good approximation;

12

E i , j 1 |  ij   a j  b j  ij where a j and b j are parameters.15 For example,  ij and i , j 1 having a
bivariate normal distribution is sufficient, but not necessary, to assure linearity in  ij . In
particular, if the random variables  i 0 , i1 , i 2 ,

 ij , i , j 1 , i , j  2 ,
 i 0 , i1 , i 2 ,

, ij , i , j 1 ,

are multivariate normal,

will also be multivariate normal, since  ij is a linear function of

, ij , as shown in Equation 10. For this distribution,





E i , j 1 |  ij   Ei , j 1  Cov  ij ,i , j 1   jj  ij  E ij  , which is linear in  ij .

The assumption of linearity implies that i , j 1  a j  b j  ij  ui , j 1 . This along with

 i , j 1   j ij  i , j 1 implies that  i , j 1  a j  c j ij  ui , j 1 where c j   j  b j ; a student's universe
score in grade j+1 is a linear function of the universe score in the prior grade. This implies that

 j 1, j 1  c 2j  jj   u , as well as that  j , j 1  c j jj ,  j , j  2  c j 1 c j jj and, more generally,
j 1

 j , j  s  c j ( s 1) c j ( s 2)

c j jj . These equations along with Equation 7 imply the moments shown

in Equation 11 where  j 1, j 1  c 2j  jj   u j1 . This structure follows from only assuming that







E  i , j 1  ij is a linear function of  ij (e.g.,  i , j 1   j ij  i , j 1 and E i , j 1  ij



is a linear

function of  ij ).

 11 12 13 14

22 23 24

  
33 34

44



15

   11 G1
 
 

 
 
 

c1 11
 22 G2

c2 c1 11
c2 22
 33 G3



c3c2 c1 11
c3c2 22
c3 33
 44 G4



This linear specification is a first-order Taylor series approximation of E i , j 1 |  ij .

13









(11)

When test-score data for students span J grades, the parameters of the reduced-form
covariance structure will include the 2 J parameters  11 , c1 , c2 ,

, cJ 1 , G1 , G2 ,

, GJ and either

 u2 ,  u2 , ,  u2 or a smaller number of parameters that imply the values of  u2 , ,  u2 (e.g.,
2

3

2

J

J

 u2   u2 , j ). As an example of how such parameters can be estimated, suppose that G j  G
j

and that test-score data for J=3 grades yields estimates of 11 , 12 , 13 , 22 , 23 , and 33 . The
corresponding moment conditions are shown in Equation 12. Substitution of the ˆ jk for  jk and
manipulation of the six moments yields the estimators for the elemental parameters shown in
(13). As is the case here, a few back-of-the-envelope calculations often can yield estimates of the
overall extent of test measurement error.

c1 11
c2 c1 11
11 12 13   11 G





2
2
2
2
22 23   
(c1  11   u ) G
c2 (c1  11   u )

 (12)
33  
[c22c12 11  (c22  1) u2 ] G 


ˆ13
ˆ12
ˆ
cˆ1  12
ˆ11

cˆ2 

ˆ ˆ
Gˆ  12 23
ˆ13ˆ 22

ˆ11  ˆ11Gˆ

ˆ  ˆ 22Gˆ  c ˆ
2
u1

2
1 11

ˆ  (ˆ 33  c ˆ 22 )Gˆ
2
u2

(13)

2
2

This example illustrates that an estimate of the covariance of observed test scores
together with assumptions regarding the structure of student achievement growth are sufficient to
estimate the variance(s) of test measurement error from all sources, as well as the variances in
universe scores measuring the dispersion in student achievement in each tested grade. In general,
this is possible if student achievement is to some extent cumulative (e.g., 1  0 ) and one has an
estimate of  -- the covariance matrix for a sequence of exams measuring student achievement
over time (e.g., math test scores of students in three consecutive grades). Achievement being
cumulative implies that the universe score variances enter expressions characterizing the off14

diagonal elements of   . For instance,  11 enters the expressions for 12   12 and 13   13
shown in Equation 12. Thus, estimates of the  jk , j  k , can be used to infer estimates of the
universe score variances. In turn, the extent of test measurement error can be inferred utilizing
the diagonal elements of   (e.g., 11   11 G1 ).
A similar, but simpler, approach can be employed whether or not the tests utilized are
vertically scaled, provided that each is interval scaled. The reduced-form model

 i , j 1  a j  c j ij  ui , j 1 and the formulae in Equation 11 imply the following empirical
relationships:  i*, j 1   j , j 1 ij*  ui*, j 1 where ui*, j 1  ui , j 1

 j 1, j 1 , E ij* ui*, j 1  0 and  ij* and  i*, j 1

are standardized universe scores having correlation  j , j 1  c j  jj  j 1, j 1 (e.g.,
12  c1  11 /  22 ). In addition,  j , j  2   j , j 1  j 1, j  2 (e.g.,
13  c2 c1  11 /  33  c1  11 /  22 c2  22 /  33  12 23 ),  j , j 3   j , j 1  j 1, j  2  j  2, j 3 , etc.. This

structure along with Equation 8 implies the moment conditions in Equation 14, where rjk is the
 r12








r13

r14

r15

r23

r24

r25

r34

r35
r45

  G1G2 12
 
 

 
 
 
 

G1G3 12  23

G1G4 12  23 34

G1G5 12  23 34  45

G2 G3  23

G2 G4  23 34

G2 G5  23 34  45

G3G4 34

G3G5 34  45
G4 G5  45










(14)

test-score correlation for grades j and k . Because G1 and 12 only appear as a multiplicative
pair, the two parameters cannot be identified separately, but 12*  G1 12 can. The same is true
for  J* 1, J  GJ  J 1, J where J is the last grade for which one has test scores. After substituting
the expressions for 12* and  J* 1, J , the Nm  J  J  1 / 2 moments in Equation 14 are functions
of the N  2J  3 parameters in   G2 G3

GJ 1 12* 23
15

 J 2, J 1  J* 1, J  , which can be

identified provided that J  4 . With one or more additional parameter restriction (e.g.,

G1  G2  G3 or 23  34 ), J  3 is sufficient for identification.
Whether the moment conditions in Equations 11 or 14 are employed in estimation, the
parameters can be estimated using a minimum-distance estimator. For example, suppose the
elements of the column vector r ( ) are the moment conditions on the right-hand-side of
Equation 14, after having substituted the expressions for 12* and  J* 1, J . With r̂ representing the
corresponding vector of N m test-score correlations for a sample of students, the minimumdistance estimator is argmin [rˆ  r ( )]'  [rˆ  r ( )] where  is any positive semi-definite
matrix.16 We employ the identity matrix so that ˆ MD  argmin [rˆ  r ( )]' [rˆ  r ( )] .17,18 The
estimated generalizability coefficients, in turn, can be used to infer estimates of the prenormalized universe-score variance, ˆ jj  Gˆ j ˆ jj , as well as the measurement-error variances

2   jj (1  G j ) G j  (1  G j )  jj and 2*  1  G j .19
j
j

3.3 Additional Points
Before turning to our empirical analysis, consider six important points. First, noted in the
introduction, the test-retest approach is a commonly-discussed, but infrequently-employed,
16

If

P

B  B0 and Rank[ B0 r ( )  ]  N ,  is locally identified. In the case of a strict equality, the parameters

are exactly identified with rˆ  r (ˆ ) implicitly defining the estimator, which is the same for all  . See Cameron
and Trivedi (2005).
17
ˆ MD , the equally-weighted minimum-distant estimator is consistent, but less efficient than the estimator
corresponding to the optimally chosen  . However,

ˆ MD

does not have the finite-sample bias problem that arises

from the inclusion of second moments. See Altonji and Segal (1996).
18
r̂ having the limit distribution NS  rˆ  r0  d N[0,V (ˆ )] implies that the variance of the minimum-distance
estimator is V ˆ MD   [Q ' Q]1 Q ' V (ˆ ) Q [Q ' Q]1 where Q is the matrix of derivatives
19

Q  r ( )  .

Sij   ij  ij implies that Sij*   jj  jj  ij*  ij  jj  G j  ij*  ij* where the normalized test- and universe-

scores having unit variances. It follows that 1  G j  2* and, in turn, 2*  1  G j .
j

j

16

method for estimating the overall extent of test measurement error. To see that this approach is a
special case of our framework, consider the subset of elements in Equation 14 shown in (15),
initially focusing on the first equation. The test-retest approach requires that (i) the time between
r12  G1G2 12

r23  G2G3  23 (15)

r13  G1G3 12 23

the test and retest is sufficiently short that the skills and knowledge of those tested are unchanged
so that 12  1 and (ii) the tests are administered under identical conditions so that the overall
extent of measurement error is the same for the two tests (e.g., G1  G2 ). Under these conditions,
the first equation in (15) reduces to r12  G ; the estimated correlation of scores from the two tests
is an estimate of the generalizability (reliability) coefficient for the tests. Joreskog (1971)
maintains the assumption that the universe scores are perfectly correlated but allows the extent of
measurement error to differ across the tests. (In this case the expressions in (15) imply that

G1  r12 r13 r23 , G2  r12 r23 r13 , and G3  r13 r23 r12 .) Our approach goes meaningfully further in
allowing the universe-score correlations to be less than one and different between test pairs.
Second, to estimate the overall extent of measurement error for a population of students
one only needs descriptive statistics of scores on each test and test-score correlations, an
attractive feature of our approach. However, additional inferences are possible when studentlevel test-score data are available.
Third, our approach is applicable whether the measurement-error variance is constant
across students in each grade (i.e., 2  2 , i ) or there is heteroskedasticity, where 2 is
ij

j

j

the mean variance for the population of students (i.e., 2  E2 ). In the latter case, it is
j

ij

possible to explore the extent and pattern of heteroskedasticity, provided one has student-level
test scores. Consider the case where   G2 G3 G4
17

12*  23 34

 has been estimated. The

relationship  i*, j 1   j , j 1 ij*  ui*, j 1 for the standardized universe scores  ij* and  i*, j 1 implies that

 u2

*
i , j 1

 1   2j , j 1 .20 Note that Sij*  G j  ij*  ij* and, in turn,  ij*  ( Sij*  ij* )

G j . These

expressions imply that Si*, j 1   j , j 1 G j 1 G j Sij*  G j 1 ui*, j 1  i*, j 1   j , j 1 G j 1 G j ij* . It
follows that the variances of the expressions before and after the equality are also equal, which
implies that 2*

i , j 1













  2j , j 1 G j 1 G j 2*  V Si*, j 1   j , j 1 G j 1 G j Sij*  G j 1 1   2j , j 1 .
ij

To provide some needed structure, suppose that  2*   i  2* ; the ratio of a student's
j

ij

measurement-error variance to the population mean variance is constant across grades. If so,
Equation 16 follows, which suggests that  can be estimated for a group of students, C, having
the same (unknown) value C , as shown in Equation 17.21 Rather than grouping students based
upon one or more observed attributes, student-level values of  i can be estimated using a
regression approach described below where we estimate the extent to which  i varies with the
level of student achievement.

i 





2

*
 , j 1

ˆC 

1
NC



V Si*, j 1   j , j 1 G j 1 G j Sij*  G j 1 1   2j , j 1 




i C



2
j , j 1

G

j 1

G j  2*

Si*, j 1  ˆ j , j 1 Gˆ j 1 Gˆ j Sij*

ˆ2

*
 , j 1



(16)

j

  Gˆ
2



j 1

1  ˆ 
2
j , j 1

 ˆ 2j , j 1 Gˆ j 1 Gˆ j ˆ2*

(17)

j

The reduced-form framework provides a useful tool for estimating the extent of test
measurement error from all sources. Estimation is straightforward and the key assumptions

20

This follows because E ui , j 1 ij  0 implies that E ui*, j 1 ij*  0 .

21

The formula in (16) only maintains that  i for each student is constant across grades j and j+1. The formula easily

can be generalized to reflect  i being constant across more than two adjacent grades.

18



underlying the empirical model (i.e.,  i , j 1   j ij  i , j 1 with  j  0 and E i , j 1  ij



is a linear

function of  ij ) appear to be quite reasonable. A fourth point is that the assumptions need not be
accepted as an article of faith; together they imply that  i , j 1 is a linear function of  ij (i.e.,

 i , j 1  a j  c j ij  ui , j 1 where E  ij ui , j 1  0 ), which can be tested utilizing test-score data, as
demonstrated below.
The fifth point is that even though the following empirical analysis utilizes the reducedform model, our general approach is not dependent on this particular specification. One can carry
out empirical analyses employing fully-specified statistical structures for the ij . A variety of
specifications can be employed, provided the specifications imply covariance structures where
the number of moment conditions are sufficient to estimate the number of parameters, including
the generalizability coefficients. The estimation strategy for the structural approach is the same
as above, the only difference being that the moment conditions employed will include the full set
of structural parameters, not a subset of the structural parameters and a set of reduced-form
parameters. We discuss the structural approach and alternative model specifications in more
detail in Appendix A.
Finally, the parameters entering the covariance structure can be estimated without
specifying the distributions of  ij and ij . However, additional inferences are possible when one
assumes particular functional forms and has student-level test scores. When needed, we assume
that  ij and ij are normally distributed. When ij is either homoskedastic or heteroskedastic
with  2ij not varying with the level of ability,  ij and Sij will be bivariate normal, implying that





the conditional distribution of  ij given Sij will be normal with moments E  ij Sij 

19







(1  Gij )  j  Gij Sij and V  ij Sij  (1  Gij ) jj where  j  E ij  ESij and Gij   jj



jj



 2ij .



Here E  ij Sij is the Bayesian posterior mean of  ij given Sij – the best linear unbiased





predictor (BLUP) of the student's actual ability. V  ij Sij and easily computed Bayesian
confidence (credible) intervals can be employed to measure the precision of the BLUP estimator
for each student.
When the extent of test measurement error systematically varies across ability levels (i.e.,





ij   j ( ij ) ) – as is the case in our application – the normal density of ij is g j ij  ij 





 ij 2 ( ij )  ( ij ) where  ( ) is the standard-normal density. The joint density of  ij and
j

j





ij is h j ij , ij   g j ij  ij f j ( ij ) 



 

1
 ij  2j ( ij )  ( ij   j )
  j ( ij )  jj



 jj which is

not bivariate normal, due to ij being a function of  ij . (In this case Sij is a mixture of normal
random variables.) The conditional density of  ij given Sij is h j  Sij   ij , ij  k j  Sij  . Here





k j  Sij    h j  Sij   ij , ij  d ij   g j Sij   ij  ij f j ( ij ) d ij is the density of Sij . Given any








particular function  2ij   2j ( ij ) , this integral can be calculated using Monte Carlo integration



*
*
 mj
with importance sampling; k j  Sij    m1 g j Sij   mj
M



*
M where  mj
, m  1, 2,

, M , is a

sufficiently large set of random draws from the distribution f j ( ij ) . Similarly, the posterior





mean ability level given any particular score is E  ij Sij 

k

j

1
Sij M

 

 m1  mj* g j  Sij   mj*  mj*  . Also, P  ij  a Sij  
M

20

k

j

1
Sij M

 



*
 mj
a





*
*
 mj
g j Sij   mj
is

the cumulative posterior distribution of  ij which can be used to infer Bayesian confidence
intervals (i.e., credible intervals). For example, the 80 percent credible interval is (L, U) such that





P L   ij  U Sij =0.80. Here we choose the lower- and upper-bounds corresponding to the









values of a such that P  ij  a Sij  0.10 and P  ij  a Sij  0.90 .22
4.0 An Empirical Application
We estimate the parameters in the reduced-form model employing moments defined in
terms of the correlations of scores on the third- through eighth-grade New York State math and
ELA tests for the cohort of New York City students who were in the third grade in the 20042005 school year. Students making normal grade progression were in the eighth grade in 20092010. The exams, developed by CTB-McGraw Hill, are aligned to the New York State learning
standards and are given to all registered students, with limited accommodations and exclusions.
Table 1 reports descriptive statistics for the cohort of students studied. Correlations for ELA and
Math are shown below the diagonals in Tables 3 and 4.23
4.1 Testing Model Assumptions

22

A common alternative is to define the credible/confidence interval to be the narrowest interval (L, U) for which







 we employ estimates of  , , and the
) , but do not adjust the formula for P   a S  to account for this imprecision.
2

P L   ij  U Sij  0.80 . In the computation of P  ij  a Sij

parameters in the function 2j ( ij

j

ij

23

j

ij

There are a nontrivial number of missing test scores. For example, consider the percent of students having scores
in the data for a particular grade but missing score for the next grade. The percentage of missing scores in the
following grade averages seven percent across grades in each subject. The extent to which this is a problem depends
upon the reasons for the missing data. There is little problem if scores are missing completely at random. (See Rubin
(1987) and Schafer (1997).) However, this does not appear to be the case for the NY tests. In particular, we find
evidence that students having missing scores typically score relatively low in the grades where scores are present.
The exception is that there are some missing scores for otherwise high-scoring students who skip the next grade. To
avoid statistical problems associated with this systematic pattern of missing scores, we impute values of missing
scores using SAS Proc MI. The Markov Chain Monte Carlo procedure is used to impute missing-score gaps (e.g., a
missing fourth grade score for a student having scores for grades three and five). This yielded an imputed database
with only monotone missing data (e.g., scores included for grades three through five and missing in all grades
thereafter). The monotone missing data were then imputed using the parametric regression method.

21

We first explore whether the data is consistent with the assumptions which imply that

E  i , j 1 |  ij  is a linear function of  ij . While the two assumptions are sufficient to assure the
linearity of E  i , j 1 |  ij  , it is this linearity that implies the structure of correlations shown in
(14). It is fortunate that we are able to assess whether E  i , j 1 |  ij  is in fact linear.
The lines in Figures 1 and 2 are empirical, nonparametric estimates of the function

E  Si , j 1 | Sij  for ELA and math, respectively, showing how the observed scores of students in
the eighth grade are related to scores in the prior grade. The bubbles with white fill show the
actual combinations of observed seventh- and eighth-grade scores; the area of each bubble
reflects the relative number of students with that score combination.
The dark bubbles toward the bottoms of Figures 1 and 2 show the IRT standard errors of
measurement (SEMs) for the seventh grade tests (in reference to the right vertical axis) reported
in the test technical reports.24 Note that the extent of measurement error associated with the test
instrument is meaningfully larger for both low and high scores, reflecting the nonlinear mapping
between raw and scale scores. Each point of the standard errors of measurement plot corresponds
to a particular scale score as well as a corresponding raw score; movements from one dot to the
next (left to right) reflect a one-point increase in the raw score (e.g., one additional question
being answered correctly), with the scale-score change shown on the horizontal axis. For
example, starting at an ELA scale score of 709, a one point raw-score increase corresponds to a
20 point increase in the scale score to 729. In contrast, starting from a scale score of 641, a one
point increase in the raw score corresponds to a two point increase in the scale score. This
varying coarseness of the raw- to scale-score mappings – reflected in the varying spacing of

24

CTB/McGraw-Hill (2006, 2007, etc.).

22

points aligned in rows and columns in the bubble plot – explains why the reported scale-score
standard errors of measurement are substantially higher for both low and high scores. Even if the
variance were constant across the range of raw scores – as assumed in classical test theory used
to produce the reliability coefficient estimates in the technical reports – the same would not be
the case for scale scores.
The fitted nonparametric curves in Figures 1 and 2, as well as very similar results for
other grades, provide strong evidence that E  Si , j 1 | Sij  is not a linear function of Sij . Even so,
this does not contradict our assumption that E  i , j 1 |  ij  is a linear function of  ij ; test measure
error can explain E  Si , j 1 | Sij  being S-shaped even when E  i , j 1 |  ij  is linear in  ij . It is not
measurement error per se that implies E  Si , j 1 | Sij  will be an S-shaped function of Sij ;

E  Si , j 1 | Sij  will be linear in Sij if the measurement-error variance is constant (i.e.,

2  2 , i ). However, E  Si , j 1 | Sij  will be a S-shaped function of Sij when ij is
ij
j
heteroskedastic with ij   j ( ij ) having a U-shape (e.g., the SEM patterns shown in Figures
1 and 2). The explanation and an example are included in Appendix B.
Appendix B also includes a discussion of how information regarding the pattern of test
measurement error can be used to obtain consistent estimates of the parameters in a
corresponding polynomial specification of E  i , j 1 |  ij  . We utilize this approach to eliminate
the inconsistency of the parameter estimates associated with the measurement error reflected in
the SEMs reported in the technical reports. Even though this does not eliminate any

23

inconsistency of parameter estimates resulting from other sources of measurement error, we are
able to adjust for the meaningful heteroskedasticity reflected in the reported SEMs.25
Results from using this approach to analyze the NY test data are shown in Figures 3 and
4 for ELA and math, respectively. As an example, consider graph (d) in either figure. The
thicker, S-shaped curve corresponds to the OLS estimation of Si 5 regressed on Si 4 using a cubic
specification. We employ a third-order polynomial because it is the lowest-order specification
that can capture the general features of the nonparametric estimates of E  Si , j 1 | Sij  in Figures 1
and 2. The dashed line is a cubic estimate of E  i , j 1 |  ij  obtained using the approach described
in Appendix B to avoid parameter-estimate inconsistency associated with that part of test
measurement error reflected in the SEMs reported in the technical reports. For comparison, the
straight line is the estimate of E  i , j 1 |  ij  employing this approach and a linear specification.
Across the grade-pair graphs, it is striking how close the consistent cubic estimates of

E  i , j 1 |  ij  are to being linear.26 Overall, the assumption that E  i , j 1 |  ij  is a linear function
of  ij appears to be quite reasonable in our application.
4.2 Estimated Model
Parameter estimates for the reduced-form model and their standard errors are reported in

25

As discussed below, how the reported SEMs vary with the level of ability is quite similar to our estimates of how
the standard deviations of the measurement-error from all sources vary with ability. If true, by accounting for the
heteroskedasticity in the measurement error associated with the test instrument, we are able to roughly account for
the effect of heteroskedasticity, increasing our confidence in the estimated curvature of E  i , j 1 |  ij for each grade





and subject. At the same time, not accounting for other sources of measurement error will result in the estimated
cubic specification generally being flatter than E  i , j 1 |  ij .
26





The cubic estimates of E  i , j 1 |  ij



 in the graphs might be even closer to linear if we had accounted for all

measurement error. This was not done to avoid possible circularity; one could question results where the estimates
of the overall measurement-error variances are predicated maintaining linearity and the estimated variances are then
used to assess whether E  i , j 1 |  ij is in fact linear.





24

Table 2. First consider how well the estimated models fit the observed score correlations. The
empirical correlations for ELA and math, respectively, are shown below the diagonals in Tables
3 and 4. The predicted correlations implied by the estimated models are above the diagonals. To
evaluate goodness of fit, consider the absolute differences between the empirical and predicted
correlations. The average, and average percentage, absolute differences for ELA are 0.001 and
one-fifth of one percent, respectively. For math, the differences are 0.003 and one-half of one
percent. Thus, the estimated reduced-form models fits the New York data quite well.
Returning to Table 2, the estimated generalizability coefficients for math are
meaningfully larger than those for ELA, and the estimates for ELA are higher in some grades
compared to others. These differences are of sufficient size that one could reasonably question
whether they reflect underlying differences in the extent of test measurement error. Instead the
pattern could reflect estimation error or a fundamental shortcoming of our approach, or both.
Fortunately, we can compare these estimates to the reliability measures reported in the technical
reports for the New York tests, to see whether the reliability coefficients differ in similar ways.
The top two lines in Figure 5 marked with squares show the reported reliability coefficients for
math (solid line) and ELA (dashed line). The lower two lines marked with diamonds show the
generalizability coefficient estimates reported in Table 2. It is not surprising that the estimated
generalizability coefficient are smaller than the corresponding reported reliability coefficients, as
the latter statistics do not account for all sources of measurement error. However, consistencies
in the patterns are striking. The differences between the reliability and generalizability
coefficients vary little across grades and subjects, averaging 0.117. Reflecting this result, the
generalizability coefficient estimates for math are higher than those for ELA, mirroring
corresponding difference between the reliability coefficients reported in the technical reports.

25

Also, in each subject the variation in the generalizability coefficient estimates across grades
closely mirrors the corresponding across-grade variation in the reported reliability coefficients.
This is especially noteworthy given the marked differences between math and ELA in the
patterns across grades.
The primary motivation for this paper is the desire to estimate the overall extent of
measurement error motivated by concern that the measurement error in total is much larger than
that reported in test technical reports. The estimates of the overall extent of test measurement
error on the NY math exams, on average, are over twice as large as that indicated by the reported
reliability coefficients. For the NY ELA tests, the estimates of the overall extent of measurement
error average 130 percent higher than that indicated by the reported reliability coefficients. The
extent of measurement error from other sources appears to be at least as large as that associated
with the construction of the test instrument.
Estimates of the variances in actual student achievement can be obtained employing
estimates of the overall extent of test measurement error together with the test-score variances.
Universe-score variance estimates for our application are reported in column (3) of Table 5. It is
also possible to infer estimates of the variances of universe-score gains shown in column (6).
Because these values are much smaller than the variances of test-score gains, the implied
generalizability coefficient estimates in column (7) are quite small, especially for ELA.
Estimation of the overall extent of measurement error for a population of students only
requires descriptive statistics of scores for each test and test-score correlations. However,
additional inferences are possible when student-level test-score data are available. In particular,
student-level data and the formula in Equation 16 can be used to estimate  j ( i )  2 ( i ) 2
j

j

characterizing how the variance of measurement error varies with student ability. For example,

26

for grade pairs 4-5 and 6-7 we compute ˆ i for each student and grade pair using Equation 16.
Assuming that a student's mean (normalized) universe score across the grade pair,  i* , is the
relevant measure of ability in  j ( i ) ,  i* can be estimated using a student's average normalized
test score for the adjacent grades, Si* . Here we estimate  j ( i* ) employing a fourth-order
polynomial. However, regressing ˆ i on Si* would yield inconsistent parameter estimates as a





result of Si* measuring  i* with error. If ik  E ( i* )k Si* , k  1, 2,3, 4 , were know for each
student, consistent estimates of polynomial parameters could be obtained by regressing ˆ i on

i1 , i 2 , i 3 , and i 4 .27 The problem is that computation of ik requires knowledge of  j ( i* ) –
the function we are trying to estimate.
This circularity suggests the following iterative solution. (1) Obtain an initial estimate of
the parameters in  j ( i* ) by regressing ˆ i on Si* . (2) Use the estimated function ˆ j ( i* ) to
compute estimates of the ik , i.e., ˆik .28 (3) Regress ˆ i on ˆi1 , ˆi 2 , ˆi 3 , and ˆi 4 to obtain an
updated estimate of ˆ j ( i* ) . Steps two and three can be repeated until estimates of the
polynomial parameters converge (a dozen or so repetitions in our analyses). In this way we
estimate how the variance of measurement error from all sources varies across the range of
universe scores; ˆ2 ( i )  ˆ j ( i ) ˆ2  ˆ j ( i ) (1  Gˆ j ) ˆ jj .
j

j

The solid lines in Figures 6 and 7 are our estimates of ˆ ( i ) . The dashed lines show the
j

IRT SEMs reported in the test technical reports for the grade. The shapes of the two curves in
27

For example, see the discussion of the "structural least squares" polynomial estimator in Kukush et. al (2005) .

28

Extending the approach for computing E  ij Sij







ˆik  E  ij  Sij  k j  Sij  M 
k





1

 m1   mj* 
M

k

 discussed above,





*
*
*
 Sij   mj
2j ( mj
)  j ( mj
).

27

each graph indicate that our estimates of how the overall measurement-error standard deviations
vary over the range of universe scores is very similar to the patterns reported in the technical
reports. These results together with those shown in Figure 5 are striking. Our estimates of the
overall extent of measurement error, as expected, are systematically higher than those associated
with the test instrument. The estimated standard deviation of the overall measure error for each
test varies over the range of abilities in a way quite similar to the pattern seen for the reported
IRT SEMs. In addition, as shown in Figure 5, the variation in generalizability coefficient
estimates across grades mirror across-grade differences in the reliability coefficients and the
differences between the math and ELA generalizability coefficient estimates mirror the
corresponding differences between the reported math and ELA reliability coefficients. These
similarities do not prove the accuracy of our technique for estimating the overall extent of test
measurement error, but they do greatly increase our confidence that the technique is able to
identify at times subtle differences in the extent of measurement error.
4.3 Inferences Regarding Universe Scores and Universe Score Gains
Observed test scores typically are used to directly estimate students' abilities and ability
gains. More precise estimates of universe scores and/or universe-score gains for individual
students can be obtained employing the observed scores along with the parameter estimates in
Table 2 and the estimated measurement-error heteroskedasticity measured by ˆ ( i ) . As an
j





example, the solid S-shaped line in Figure 8(a) shows the values of Eˆ  ij Sij for fifth-grade
ELA. Results for grades five and seven math are shown in Figure 9. Results for both subjects in
other grades are quite similar. Referencing the 45o straight line, the estimated posterior-mean
ability levels for higher-scoring students are substantially below the observed scores while
predicted ability levels for low-scoring students are above the observed scores. This Bayes
28

"shrinkage" is largest for the highest and lowest scores due to the estimated pattern of
measurement-error heteroskedasticity. The dashed lines show 80-percent Bayesian credible
(confidence) bounds for ability conditional on the observed score. For example, the BLUP of the
universe-score for fifth-grade students scoring 775 in ELA is 731, 44 point below the observed
score. We estimate that 80 percent of students scoring 775 have universe scores in the range 717-





747; P 717.1 ij  747.2 Sij  775  0.80 . In this case, the observed score is 28 points higher
than the upper bound of the 80-percent credible interval. Midrange scores are somewhat more
informative, reflecting the smaller standard deviation of test measurement error. For an observed
score of 650, the estimated posterior mean and 80 percent Bayesian confidence interval are 652
and (639,665), respectively. The credible bounds for a 775 score is 15 percent larger than that for
a score of 650.
As Figures 8 and 9 make clear, utilizing test scores to directly estimate students' abilities
is problematic for high- and, to a lesser extent, low-scoring students. To explore further, consider
the root of the expected mean squared errors (RMSE) associated with estimating student ability
using (i) observed scores and (ii) estimated posterior mean abilities conditional on observed
scores.29 In the case of the fifth-grade math exam shown in Figure 9(a), the RMSE associated





with using Eˆ  ij Sij to estimate students' abilities is 15.5 scale-score points. In contrast, the
RMSE associated with using Sij is 18.4, 19 percent larger. The magnitude of this difference is





meaningful given that Eˆ  ij Sij differs little from Sij over the range of scores for which there
are relatively more students. Over the range of actual abilities between 620 and 710 in Figure

29

The expected values are computed using Monte Carlo simulation and assuming our parameter estimates are
correct.

29





9(a), the RMSE for Eˆ  ij Sij and Sij are 15.5 and 15.6, respectively. For ability levels below
620 the RMSEs are 16.8 and 21.8, respectively, the latter being 30 percent larger. For students





whose ability levels are greater than 710, the RMSE of employing Eˆ  ij Sij to estimate  ij is
14.4, smaller than the overall RMSE for this estimator. In contrast, the RMSE associated with
using Sij to estimate  ij is 31.3 for students whose actual abilities are greater than 710 -- over





twice as large as the corresponding RMSE for Eˆ  ij Sij . By estimating the overall extent and
pattern of test measurement error from all sources, it is possible to compute estimates of universe
scores that have statistical properties superior to those corresponding to merely using the
observed scores of students as estimates of their ability levels.
Turning to the measurement of ability gains, the solid S-shaped curve in Figure 10 shows
the posterior-mean universe-score change in math between grades five and six conditional on the
observed score change. Again, the dashed lines show 80-percent credible bounds. For example,
among students observed to have a 40-point score increase between the fifth and sixth grades,
their actual universe score changes are estimated to average 13.5. Eighty-percent of all students
having a 40-point score increase are estimated to have actual universe score changes falling in
the interval -1.1 to 27.4. It is noteworthy that for the full range of score change shown (  50
points), the 80-percent credible bounds include there actually being no change in ability.
Note that there are many different combinations of scores that yield a given change in
observed scores; a score increase from 590 to 630 implies a 40-point change as does an increase
from 710 to 750. Figure 10 corresponds to the case where one knows the score change but not

30

the pre- and post-scores.30 However, for a given score change, the mean universe-score change
and credible bounds will vary across known score levels because of the pattern of measurement
error heteroskedasticity. For example, Figure 11 shows the posterior-mean universe score change
and credible bounds conditional on particular combinations of scores that correspond to a 40point increase. For example, students scoring 710 on the grade-five exam and 750 on the gradesix exam are estimated to have a 10.2 point universe-score increase on average, with 80 percent
of such students having actual changes in ability in the interval (-12.4, 31.0). (Note that a 40
point score increase is relatively large in that the standard deviation of the score change between
the fifth- and sixth-grades is 26.0.) For students having a 40-point score increase, there actually
being no change in ability falls outside the credible bounds only when the score in grade five is
approximately between 615 and 658.
A striking result in Figure 11 is that the posterior mean universe-score change,

Eˆ  6   5 S5 , S6   Eˆ  6 S5 , S6   Eˆ  5 S5 , S6  , is substantially smaller than the corresponding
observed-score change, warranting further explanation. Again consider

Eˆ  6   5 S5  710, S6  750   10.3 . Figure 12 illustrates why this is substantially smaller in

30













The joint density of  ij , i , j 1 ,ij , and i , j 1 is h j  ij , i , j 1 ,ij ,i , j 1  g j ij  ij g j 1 i , j 1  i , j 1 f ( ij , i , j 1 ) .

With    j 1   j and D  S j 1  S j     j 1   j , the joint density of  ij ,  ,ij , and D is





h j  ij , ij   ,ij , D    ij . Integrating over  ij and ij yields the joint density of  and D ;

z  , D   







 

g

j 1

 D   

ij







 i , j 1 f 2 ( ij    ij ) g j ij  ij f 1 ( ij ) dij d ij where f 1 ( ij ) is the marginal

density of  ij and f ( i , j 1  ij ) is the conditional density of  i , j 1 given  ij . This integral can be computed using
2





z  , D   1 J   j 1 g j 1 D    ij*  ij*   f 2 ( ij*    ij* ) where ( ij* ,ij* ), j  1, 2,
J

, J , is a sufficiently large

number of draws from the joint distribution of ( ij ,ij ) . In turn, the density of the posterior distribution of  given





D is z  D   z  , D  / l  D  where l  D   1 J   j 1 g j 1 D   i*, j 1   ij*  ij*  i*, j 1 is the density of D . The
J

cumulative posterior distribution is P   a S   1 J l ( D)   *

*
i , j 1  ij  a



posterior mean ability given D is E  D   1 J l ( D)   j 1 
J

31

*
i , j 1







g j 1 D   i*, j 1   ij*  ij*  i*, j 1 . Finally, the

*
ij

 g  D 
j 1

*
i , j 1

  
*
ij

*
ij

*
i , j 1

.

magnitude compared to the 40-point increase in score. First, Eˆ ( 6 S6  750)  733.0 is 17 points
below the observed score due to the Bayes shrinkage toward the mean 6  667.8 .

Eˆ  6 S5  710, S6  750   729.9 is even smaller; because S6 is a noisy estimate of  6 and  5 is
correlated with  6 , the value of S5 provides information regarding the distribution of  6 that
goes beyond the information gained by observing S6 . Note that E  6 S5 , S6  would equal

E ( 6 S6 ) if either 2 6  0 or 56  0 . Eˆ  6 S5 , S6  is less than Eˆ ( 6 S6 ) because S5 is
substantially below S6 . Similar logic holds for the fifth grade. Eˆ ( 5 S5  710)  707.5 is less than
710 because the latter is substantially above 5 . However, Eˆ ( 5 S5 , S6 )  719.6 is meaningfully
larger than Eˆ ( 5 S5 )  707.5 and larger than S5  710 , reflecting that S6  750 is substantially
larger than S5 . In summary, among New York City students scoring 710 on the fifth-grade math
exam and 40 points higher on the sixth grade exam, we estimate the mean gain in ability is little
more than one-forth as large as the actual score change; Eˆ  6 S5 , S6   Eˆ  5 S5 , S6  
729.9  719.6  10.3 . The importance of accounting for the estimated correlation between ability

levels in grades five and six is reflected in the fact that the mean ability increase would be two
and one-half times large were the ability levels uncorrelated,

Eˆ  6 S6   Eˆ  5 S5   733.0  707.5  25.5 .
5.0 Conclusion
In this paper we show that there is a credible and feasible approach for estimating the
total extent of test measurement error utilizing estimates of the empirical correlation or
covariance matrix for three or more interval-scaled tests. The scales can differ across the tests

32

provided that they are linear transformations of an underlying common vertical scale that need
not be known. Our approach maintains relatively unrestrictive assumptions regarding the
structure of student achievement growth. We assume that academic achievement is cumulative
following a first-order autoregressive process;  ij   j 1 i , j 1  ij where there is at least some
persistence (i.e.,  j 1  0 ) and the possibility of decay (  j 1  1 ), which can differ across grades.
Even though derivations would be more complicated, one could employ some other structure
(e.g., a second-order autoregressive process). With  ij   j 1 i , j 1  ij , an additional assumption
is needed regarding the stochastic properties of ij . A reduced-form specification is employed in
the paper and, to illustrate the generality of the approach, three examples of fully specified
structural models are outlined in Appendix A,
Our approach is a meaningful generalization of the test-retest method, providing a useful,
more generally applicable tool for estimating the extent of test measurement error from all
sources. Estimation is straightforward and the key assumptions underlying the empirical model



(i.e.,  i , j 1   j ij  i , j 1 with  j  0 and E i , j 1  ij



is a linear function of  ij ) appear to be

quite reasonable. Furthermore, these assumptions imply that  i , j 1 is a linear function of  ij
which can be tested.
Estimation of the overall extent of measurement error for a population of students only
requires computed test-score descriptive statistics and correlations. However, when student-level
test-score data are available, one can explore the extent and pattern of measurement error
heteroskedasticity. Results for New York make clear that heteroskedasticity can be important in
that variation in the extent of test measurement error across ability levels is quite large.

33

The overall extent of test measurement error can be estimated without specifying
particular functional forms for the distribution of either abilities or test measurement error.
However, by maintaining standard assumptions (e.g., normality), one can make inferences
regarding universe scores and universe score gains. In particular, for a student with a given score,









the Bayesian posterior mean and variance of  ij given Sij , E  ij Sij and V  ij Sij are easily
computed where the former is the best linear unbiased predictor (BLUP) of the student's actual
ability. Similar statistics for test score gains can also be computed. We show that using the
observed score as an estimate of a student's underlying ability can be quite misleading for
relatively low- or high-ability students. However, this is not the case when the posterior mean is
employed.
Estimates of the overall extent of test measurement error have a variety of uses that go





beyond merely assessing the reliability of various assessments. Employing E  ij Sij , rather
than Sij , to estimate  ij is one example. Another is the computation of effect sizes where the
magnitudes of the effects of different causal factors can be judged relative to either the standard
deviation of ability or the standard deviation of ability gains. Bloom et al. (2008) discuss the
desirability of taking into account the dispersion of ability or ability gains rather than test scores
or test-score gains but note that analysts often have little if any information regarding the extent
of test measurement error.
As demonstrated above, the same types of data researchers often employ to estimate how
various factors affect educational outcomes can be used to estimate the overall extent of test
measurement error. Based on the variance estimates shown in columns (1) and (3) of Table 5, for
the tests we analyze effect sizes measures relative to the standard deviation of ability will be ten
to 18 percent larger than effect sizes measured relative to the standard deviation of test scores. In
34

cases where it is pertinent to judge the magnitudes of effects in terms of achievement gains,
effect sizes measured relative to the standard deviation of ability gains will be two to over three
times larger compared to those measured relative to the standard deviation of test-score gains.
Another use of the estimated extent of test measurement error pertains to the common
practice of entering student test scores as right-hand-side variables in regression equations, as is
often done in value-added modeling. A concern is that the measurement error associated with the
prior tests can bias other coefficient estimates. However, any such problem can be avoided by
employing E  ij | Sij  rather than S ij as a proxy for ability in the regression.31 Doing so has a
second benefit. Even if the dependent variable in such a regression is a linear function of ability,
a problem of nonlinearity is introduced when S ij is used to proxy ability. This problem arises
when E  ij | Sij  is a nonlinear function of S ij , as we demonstrate in our application. In an effort
to deal with this issue, one could include the square and cube of S ij in the regression. However,
the problem can be avoided by employing Eˆ  ij | Sij  rather than S ij in the regression.
Finally, by estimating the extent and pattern of test measurement error one can assess the
precision of a variety of measures that are computed based upon test scores. These include
indicators of student proficiency (e.g., AYP), teacher- and school-effect estimates and
accountability measures more generally. As we have shown, it is possible to measure the
reliability of such measures as well as employ the estimated extent of test measurement error to
calculate more accurate measures, information which should be employed in policy applications
based on student achievement tests.
Overall, this paper has both methodological and substantive implications.
Methodologically it shows that the full extent of test measurement error can be estimated without
31

See Sullivan (2001).

35

employing the costly test-retest strategy. Substantively, it shows that the overall measurement
error is substantially greater than reported split-test measurement error and this difference
suggests that much empirical work has been underestimating the effect sizes of interventions
(e.g., programs or teachers) that affect student learning.

36

References
Aaronson, Daniel, Lisa Barrow, and William Sander. 2007. “Teachers and Student Achievement
in the Chicago Public High Schools.” Journal of Labor Economics 25(1): 95–135.
Abowd, J.M. and D. Card (1989) “On the Covariance Structure of Earnings and Hours
Changes,” Econometrica 57(2), 411-445.
Altonji, J.G. and L.M. Segal (1996) "Small Sample Bias in GMM Estimation of Covariance
Structures," Journal of Business and Economic Statistics 14, 353-366.
Ballou, D. (2002) “Sizing Up Test Scores,” Education Next 2(2), 10-15.
Baltagi, B. H. (2005) Econometric Analysis of Panel Data, West Sussex, England: John Wiley &
Sons, Ltd.
Bloom, H.S., C.J. Hill, A.R. Black and M.W. Lipsey (2008) "Performance Trajectories and
Performance Gaps as Achievement Effect-Size Benchmarks for Educational Interventions,"
Journal of Research on Educational Effectiveness (1) 289-328.
Brennan, R. L. (2001) Generalizability Theory, New York: Sprnger-Verlag.
Cameron, A.C. and P.K. Trivedi (2005) Microeconometrics: Methods and Applications, New
York: Cambridge University Press.
Carlin, B. P. and T. A. Louis (1996) Bayes and Empirical Bayes Methods for Data Analysis,
Boca Raton: Chapman & Hall/CRC.
Conbach, L.J., R.L. Linn, R.L. Brennan and E.H. Haertel (1997) "Generalizability Analysis for
Performance Assessments of Student Achievement or School Effectiveness," Educational and
Psychological Measurement, 57(3), 373-399.
CTB/McGraw-Hill (2006) “New York State Testing Program 2006: Mathematics, Grades 3-8:
Technical Report”, Monterey, CA.
CTB/McGraw-Hill (2007) “New York State Testing Program 2007: Mathematics, Grades 3-8:
Technical Report”, Monterey, CA.
Feldt, L. S. and R. L. Brennan (1989) “Reliability,” in Educational Measurement 3rd ed., New
York: American Council on Education.
Goldhaber, D. and E. Anthony (2007) “Can Teacher Quality Be Effectively Assessed? National
Board Certification as Signal of Effective Teaching,” The Review of Economics and Statistics
89(1), 134-150.
Goldhaber, D., and M. Hansen. 2010 “Assessing the Potential of Using Value-Added Estimates
of Teacher Job Performance for Making Tenure Decisions.” CALDER working paper.
Haertel, E. H. (2006) “Reliability,” in Educational Measurement, fourth edition, R. L. Brennan,
ed., Praeger.
Hill, C., H. Bloom, A. Black, M. Lipsey, “Empirical Benchmarks for Interpreting Effect Sizes in
Research” MDRC Working Paper, July 2007.

37

Koedel, Cory, and Julian R. Betts. 2007. “Re-Examining the Role of Teacher Quality in the
Educational Production Function.” Working Paper 0708. University of Missouri, Department
of Economics.
Kukush, A., H. Schneesweiss and R. Wolf (2005) "Relative Efficiency of Three Estimators in a
Polynomial Regression with Measurement Errors," Journal of Statistical Planning and
Inference 127, 179-203.
Lee, W. C., R. L. Brennan and M. J. Kolen (2000) “Estimators of Conditional Scale-Score
Standard Errors of Measurement: A Simulation Study,” Journal of Educational Measurement
37(1), 1-20.
McCaffrey, D. F., J. R. Lockwood, D. Koretz, T. A. Louis and L Hamilton (2004) “Models for
Value-Added Modeling of Teacher Effects,” Journal of Educational and Behavioral Statistics
29(1), 67-101.
McCaffrey, Daniel F., Tim R. Sass, J. R. Lockwood, and Kata Mihaly. 2009. “The Intertemporal
Variability of Teacher Effect Estimates.” Education Finance and Policy 4(4):572–606.
Rogosa, D.R. and J. B. Willett (1983) “Demonstrating the Reliability of Difference Scores in the
Measurement of Change,” Journal of Educational Measurement 20(4) 335-343.
Rubin, D. B. (1987) Multiple Imputation for Nonresponse in Surveys, New York: J. Wiley &
Sons.
Sanders, W. and J. Rivers (1996) “Cumulative and Residual Effects of Teachers on Future
Student Academic Achievement,” working paper, University of Tennessee Value-Added
Research and Assessment Center.
Sanford, E. E. (1996) “North Carolina End-of-Grade Tests: Reading Comprehension,
Mathematics,” Technical Report #1. Division of Accountability/Testing, Office of Instruction
and Accountability Services, North Carolina Department of Public Instruction.
Schafer, J. L. (1997) Analysis of Incomplete Multivariate Data, London: Chapman & Hall.
Shen, W. and T. A. Louis (1998) “Triple-goal Estimates in Two-Stage Hierarchical Models,”
Journal of the Royal Statistical Society 60(2), 455-471.
Sullivan, D. G. (2001) “A Note on the Estimation of Linear Regression Models with
Heteroskedastic Measurement Errors,” Federal Reserve Bank of Chicago working paper WP
2001-23.
Thorndike, R. L. (1951) “Reliability,” in Educational Measurement, E.F. Lindquist, ed.,
Washington, D.C.: American Council on Education.
Todd, P.E. and K.I. Wolpin (2003) "On the Specification and Estimation of the Production
Function for Cognitive Achievement," The Economic Journal 113, F3-F33.
Wright, S. P. and W. L. Sanders ( ) “Decomposition of Estimates in a Layered Value-Added
Assessment Model,” Value-Added Assessment

38

Grade 3
Grade 4
Grade 5
Grade 6
Grade 7
Grade 8

Table 1
Descriptive Statistics for Cohort
ELA
Math
standard
standard
mean deviation
mean deviation
626.8
37.3
616.5
42.3
657.9
39.0
665.8
36.0
659.3
36.1
665.7
37.5
658.0
28.8
667.8
37.5
661.7
24.4
671.0
32.5
660.5
26.0
672.2
31.9
N = 67,528
N = 74,700

Table 2 Correlation and Generalizability
Coefficient Estimates, New York City
Parameters+
 34*

Math

ELA

0.8144
(0.0016)

0.8369
(0.0016)

0.9581
(0.0012)

0.9785
(0.0013)

0.9331
(0.0011)

0.9644
(0.0012)

0.9647
(0.0011)

0.9817
(0.0012)

0.8711
(0.0013)

0.8168
(0.0013)

G4

0.8005
(0.0024)

0.7853
(0.0025)

G5

0.8057
(0.0020)

0.7169
(0.0018)

G6

0.8227
(0.0019)

0.7716
(0.0019)

G7

0.8284
(0.0020)

0.7184
(0.0019)

 45
56
67
*
78

+ The parameter subscripts here correspond to the
*
grade tested. For example, 34
is the correlation of
universe scores of students in grades three and four

39

Table 3 Correlations of Scores on the NYS ELA Examinations
in Grades Three Through Eight (Computed values below
the diagonal and fitted-values above)
Grade 3

Grade 3
Grade 4
Grade 5
Grade 6
Grade 7
Grade 8

0.7416
0.6949
0.6899
0.6573
0.6356

Grade 4
0.7416

Grade 5
0.6934
0.7342

0.7328
0.7357
0.6958
0.6709

0.7198
0.6800
0.6514

Grade 6
0.6937
0.7346
0.7173

Grade 7
0.6571
0.6958
0.6794
0.7309

0.7303
0.7050

Grade 8
0.6332
0.6705
0.6548
0.7044
0.6923

0.6923

Table 4 Correlations of Scores on the NYS Math Examinations
in Grades Three Through Eight (Computed values below
the diagonal and fitted-values above)
Grade 3

Grade 3
Grade 4
Grade 5
Grade 6
Grade 7
Grade 8

0.7286
0.6936
0.6616
0.6480
0.6091

Grade 4
0.7286

Grade 5
0.7003
0.7694

0.7755
0.7248
0.6998
0.6685

0.7592
0.7323
0.7077

Grade 6
0.6603
0.7254
0.7597

Grade 7
0.6393
0.7023
0.7355
0.7964

0.7944
0.7643

0.7929

Grade 8
0.6119
0.6722
0.7039
0.7623
0.7929

Table 5: Variances of Test Scores, Test Measurement Error, Universe Scores, Test-Score Gains,
Measurement Error for Gains, and Universe Score Gains and Generalizabiltity Coefficient for
Test-Score Gain, ELA and Math
(1)
(2)
(3)
(4)
(5)
(6)
(7)

ˆ2

ˆ jj  Gˆ j  S2

ˆ 2S

ˆ 2

ˆ 2

Gˆ j  ˆ 2

ELA

 S2

grade 7
grade 6
grade 5
grade 4

1520.8
1303.0
832.1
595.1

326.5
368.8
190.0
167.6

1194.3
934.2
642.1
427.5

763.8
646.2
407.4

695.3
558.9
357.6

68.4
87.3
49.8

0.090
0.135
0.122

Math
grade 7
grade 6
grade 5
grade 4

1297.6
1409.5
1409.5
1054.9

259.0
273.8
250.0
181.0

1038.6
1135.7
1159.5
873.9

661.9
677.9
527.8

532.8
523.8
431.0

129.1
154.1
96.8

0.195
0.227
0.183

j

j

j

40

j

j

j

j

ˆ 2S

j

800

200

775

180
160

grade 8 score

750

140

725

120

700

100

675

80
60

650

40

625

reported SEM for grade 7

Figure 1: Nonparametric Regression of Grade 8 ELA Scores on Scores in Grade 7,
Bubble Graph Showing the Joint Distribution of Scores and
Standard-Error of Measurement for 7th Grade Scores

20

600

0
600

625

650

675

700

725

750

775

800

grade 7 score

800

200

775

180
160

grade 8 score

750

140

725

120

700

100

675

80
60

650

40

625

20

600

0
600 625

650 675

700 725

grade 7 score

41

750 775 800

reported SEM for grade 7

Figure 2: Nonparametric Regression of Grade 8 Math Scores on Scores in Grade 7,
Bubble Graph Showing the Joint Distribution of Scores and
Standard-Error of Measurement for 7th Grade Scores

Figure 3: Cubic Regression Estimates of E  Si , j 1 | Sij  as well as consistent
estimates of cubic and linear specifications of E  i , j 1 |  ij  , ELA

(b) Sixth and fifth grade scores

775

775

750

750

725

725

700

700

score

score

(a) Fifth and fourth grade scores

675

675

650

650

625

625

600

600
600

625

650

675

700

725

750

600

775

625

650

"consistent" cubic

725

750

775

"consistent" linear

(c) Seventh and sixth grade scores

(d) Eigth and seventh grade scores

775

775
750

750

725

725

700

700

score

score

700

prior score

prior score
cubic

675

675

675

650

650

625

625

600

600
600

625

650

675

700

725

750

775

prior score

600

625

650

675

700

prior score

42

725

750

775

Figure 4: Cubic Regression Estimates of E  Si , j 1 | Sij  as well as consistent
estimates of cubic and linear specifications of E  i , j 1 |  ij  , Math

(b) Sixth and fifth grade scores

775

775

750

750

725

725

700

700

score

score

(a) Fifth and fourth grade scores

675

675

650

650

625

625

600

600

600

625

650

675 700
prior score

725

750

600

775

625

650

(c) Seventh and sixth grade scores

750

725

725

700

700

score

score

775

750

675

650

625

625

600
625

650

675

700

725

750

775

750

775

prior score

(d) Eigth and seventh grade scores

675

650

600

725

unadjusted cubic
"consistent" cubic
"consistent" linear

unadjusted cubic
"consistent" cubic
"consistent" linear

775

675 700
prior score

600
600

625

650

675

700

prior score

43

725

750

775

Figure 5: Generalizability and Reliability Coefficient
Estimates for New York Math and ELA Exams by Grade

generalizability coefficient

0.95
0.90
ELA Feldt-Raju Alpha

0.85

Math Feldt-Raju Alpha
ELA - G

0.80

Math - G

0.75
0.70
4

5

grade

6

44

7

Figure 6: Standard Errors of Measurement Reported in Technical Reports (dashed lines) and
Estimated Using the Reduced-Form Model, ELA by Grade Pairs
(b) grade five

50

measuerment error SE

measuerment error SE

(a) grade four
40
30
20
10
0

50
40
30
20
10
0

575

600

625

650

675

700

725

750

575

600

level of achievement

40
30
20
10
0
625

650

675

675

700

725

750

725

750

(d) grade seven

measuerment error SE

measuerment error SE

(c) grade six

600

650

level of achievement

50

575

625

700

725

50
40
30
20
10
0

750

575

600

level of achievement

625

650

675

700

level of achievement

Figure 7: Standard Errors of Measurement Reported in Technical Reports (dashed lines) and
Estimated Using the Reduced-Form Model, Mathematics by Grade Pairs
(b) grade five

measuerment error SE

measuerment error SE

(a) grade four
50
40
30
20
10
0
575

600

625

650

675

700

725

50
40
30
20
10
0

750

575

600

level of achievement

measuerment error SE

measuerment error SE

40
30
20
10
0
625

650

675

675

700

725

750

725

750

(d) grade seven

50

600

650

level of achievement

(c) grade six

575

625

700

level of achievement

725

750

45

50
40
30
20
10
0
575

600

625

650

675

700

level of achievement

Figure 8
Estimated Posterior Mean Ability Level Given the Observed Score
and 80-Percent Bayesian Confidence Bounds, Grades 5 and 7 ELA
(a) grade five
800

775

775

750

750

725

725

actual ability

actual ability

800

700
675

700
675

650

650

625

625

600

600

450 degree line

575

(b) grade seven

575

575 600 625 650 675 700 725 750 775 800
observed score

450 degree line

575 600 625 650 675 700 725 750 775 800
observed score

Figure 9
Estimated Posterior Mean Ability Level Given the Observed
Score and 80-Percent Bayesian Confidence Bounds, Grades 5 and 7 Math
(b) grade seven

(a) grade five
800

775

775

750

750

725

725

actual ability

actual ability

800

700

700

675

675
650

650

625

625

600
575

600

450 degree line

450 degree line

575
575 600 625 650 675 700 725 750 775 800

575 600 625 650 675 700 725 750 775 800
observed score

observed score

46

Figure 10
Estimated Posterior Mean Change in Ability Given the Change in Observed
Score and 80-Percent Credible Bounds, Grades 5 and 6 Mathematics
50
40
change in ability '

30
20
10
0

-10
-20
-30
-40
-50
-50 -40 -30 -20 -10 0 10 20 30 40 50
change in score

Figure 11
Estimated Posterior Mean Change in Ability for the Observed Scores in Grades
Five and Six Mathematics for S6 - S5 = 40 and 80-Percent Credible Bounds

change in ability '

50
40
30
20
10
0
-10
-20
575

600

625

650 675 700
grade 5 score

47

725

750

775

Figure 12
Example Showing Posterior means for a Forty-Point Score Increase, Grades 5 & 6 Mathematics

S5  710
700

S5  710
710

S6  750
730

720

E( 5 S5 )  707.5 E( 5 S5 , S6 )  719.6 E ( 6 S5 , S6 )  729.9

48

740

E ( 6 S6 )  733.0

750

APPENDIX A
We illustrate the structural approach for estimating the overall extent of test measurement
error using three alternative specifications for ij in  ij   j 1 i , j 1  ij , each of which fully
specifies the covariance structure of achievement gains across grades.
Before considering particular specifications, note that for any specification of ij ,

 12  Cov( i1 , i 2 )  Cov( i1 , 1 i1  i 2 )  1  11  Cov( i1 ,i 2 )  1  11  12 and, in general,
 jk  Cov( ij , ik )  Cov( ij ,  k 1 i ,k 1  ik )   k 1  j ,k 1  jk , for k > j, where  jk  Cov( ij ,ik ) .
These recursive equations imply the structure of  shown in Equation A1 and the moment
conditions in Equation A2. Equation A3 also holds (e.g.,  22  12  11  21 12   2 ).
2

11 12 13 14

22 23 24

  
33 34

44



  11 G1
 
 

 
 
 

1 11   12
 22 G2

 2 12   13
 2 22   23
 33 G3

 3 13   14
 3 23   24
3 33   34
 44 G4









11   11 G1
12  1 11  12
13   2 1 11   2 12  13
14  3  2 1 11  3  2 12  3 13  14
22   22 G2
23   2 22  23
24  3  2 22  3 23  24

(A2)

33   33 G3
34  3 33  34
44   44 G4

 jj  V ( ij )  V (  j 1 i , j 1  ij )   j21  j 1, j 1  2 j 1 j 1, j   2 . (A3)
j

1

(A1)

This covariance structure follows from only assuming  ij   j 1 i , j 1  ij . We consider three
specifications for ij , each of which implies formulae for  jk and  2 . In each case, we utilize a
j

random variable  ij having the following properties: Cov( ij ,  ik )  0 j  k , V ( ij )   2 ,
Cov(ij ,  i , j  m )  0 m  0 , and Cov( ij ,  i , j  m )  0 m  0 .32

Model 1 is the relatively simple, but frequently employed, specification ij  i   ij
where i is a student-level random effect with V ( i )   2 . It follows that 2  V (ij )   2   2
j

and Cov(ij ,ik )  Cov(i   ij , i   ik )   2 , j  k . Here the variance of student
achievement gains gross of any decay is constant across grades.
Model 2 is the specification ij  i, j 1   ij where 0    1 . Note that Cov(ij ,i , j 1 ) 
Cov(ij , ij   i , j 1 )   2j and, more generally, Cov(ij ,i , j  m )   m 2j m  0 .

Model 3 is the moving average ij   ij  1 i, j 1   2 i, j 2 . It follows that
V (ij )  (1  12  22 ) 2   2 is constant across grades. Note that Cov(ij , i , j 1 )  1 (1  2 ) 2 ,

Cov(ij ,i , j 2 )  2 2 and Cov(ij ,i , j  m )  0 m  3 .
The three models differ in the degree to which the achievement gains of students are
persistent over time, as shown in (A4). The expression for Model 2 is obtained through iterative
Model 1:

ij   ij  i

Model 2: ij   ij   i, j 1   2 i, j 2  i, j 3 (A4)
Model 3: ij   ij  1 i, j 1   2 i, j 2

32

We allow for the possibility that the mean of  i , j , say E i , j   j , is nonzero and varies across grades. This

generalizes the specification E i , j  0, j , E i , j ,  i , k  0 j  k , E i ,  i , j  0 j , and E i , j ,  i , k  0 k  j . Note
that the value of V ( ij )   2 can vary across the three models.

2

substitution. There is no diminution in the persistence of the ij over time in Model 1;
Cov(ij ,i, j  m )   2 is constant regardless of the grade span (i.e., for all m). The covariance of
i

ij and i , j  m in Model 2 diminishes as m increases; Cov(ij ,i , j  m )   m 2 m  0 . Even so,
j

Cov(ij ,i , j  m ) is greater than zero for all grade spans. In Model 3 there is no memory for spans

of three grades or larger; Cov (ij , ij 1 )  1 (1  2 ) 2 , Cov(ij , ij  2 )  2 2 and

Cov(ij ,i, j m )  0, m  3 . Memory would be limited to adjacent grades if 1  0 but 2  0 and
there would be no persistence if 1  2  0 . Even though there is no memory in Model 3 across
spans exceeding two grades, any pattern is possible for the first two years, as the values of 1
and  2 are not restricted. For example, if 1   and  2   2 , the first three terms on the right
hand side of the equations for Models 2 and 3 in (A4) would be identical. As a group, the three
models include a wide range of possibilities. However, the estimation strategy discussed below
can be extended to other specifications for ij as well. We largely focus on Model 1 to illustrate
the structural approach to estimation.
The specification ij  i   ij in Model 1 implies a relatively simple structure for the
 jk . For example, 1k  1 , k and  2  1 1   2 . In general,  jk  Cov( ij ,ik )

 Cov( ij , i )   j where  j  Cov( ij ,ik )  Cov(  j 1 i , j 1  i   ij , i   ik ) 

 j 1 j 1   2 . Thus, the value of  jk follows from the values of 1 , 2 , ,  j 1 , 1 , and  2 .
This structure, 2   2   2 , and Equations A2 and A3 imply the moment equations in (A5)
j

and (A6), where the formulae for  jj , j  1, in (A6) can be used to eliminate  jj in (A5).
Estimation of the remaining parameters is relatively a straightforward.
3

11   11 G1  0
12  1  11  1  0
13   2 1  11  (  2  1) 1  0
14  3  2 1  11  (  3  2   3  1) 1  0
15   4 3  2 1  11  (  4  3  2   4  3   4  1) 1  0
22   22 G2  0
23   2  22  [ 1 1   2 ]  0
24  3  2  22  (  3  1)[ 1 1   2 ]  0

(A5)

25   4 3  2  22  (  4  3   4  1)[ 1 1   2 ]  0
33   33 G3  0
34  3  33  [  2 1 1  (  2  1) 2 ]  0
35   4 3  33  (  4  1)[  2 1 1  (  2  1) 2 ]  0

44   44 G4  0
45   4  44  [ 3  2 1 1  ( 3  2  3  1) 2 ]  0
55   55 G5  0
 22  12 11  21  1   2   2
 33   22 22  2  2 1  1  (2 2  1) 2   2

( A6)

 44  32 33  2 3  2 1 1  (23  2  23  1) 2   2
 55   42 44  2  4 3  2 1 1  (2 4  3  2  2 4  3  2 4  1) 2   2
Suppose that the values of ˆ ij have been estimated employing student test scores
spanning J grades (i.e., i, j  1, 2,..., J ). Let ˆ c represent a column vector made up of the

Nm  J ( J  1) / 2 moment equations following the structure in (A5) with ˆ ij substituted for ij
and the formulae in (A6) substituted for  jj , j  1 . Here ˆ c is a function of the N p  2 J  3

4

parameters in    11  1  2  2 1  2

 J 1 G1 G2

GJ  , which can be estimated using the

generalized-method-of-moments estimator described toward the end of Section 3.2.
In background analysis we estimated the three structural models and found that estimates
of all three specifications yielded predicted covariance structures that fit the covariance of test
scores well, with little evidence that any one was a superior specification. Important given the
motivation for this paper, the estimated patterns of test measurement error are quite robust across
the three structural models and the reduced-form model discussed in the paper.

5

Appendix B
As noted in the paper, measurement error can, but need not, result in E  Si , j 1 | Sij  being
a nonlinear function of Sij even when E  i , j 1 |  ij  is linear in  ij . It is not measurement error
per se that implies the nonlinearity, as E  Si , j 1 | Sij  is linear in Sij if the measurement-error is
homoskedastic (i.e., 2  2 , i ). However, E  Si , j 1 | Sij  is nonlinear in Sij when
ij
j

E  i , j 1 |  ij  is linear but ij is heteroskedastic with the extent of measurement error varying
with the ability level (i.e., ij   j ( ij ) ). When  j ( ij ) is U-shaped, as in Figures 1 and 2,

E  Si , j 1 | Sij  is an S-shaped function of Sij . An explanation and example follow.





N (  j ,  2j ) , E  i , j 1 |  ij  0  1 ij and Sij   ij  ij .

Consider the case where  ij

Note that  i , j 1  0  1 ij  ij with E ij ij  0 so that Si , j 1  0  1Sij  1ij  i , j 1  ij . In

















turn, E Si , j 1 Sij  0  1Sij  1E ij Sij since both E ij Sij and E i , j 1 Sij are zero. In
the homoskedastic case ( 2  2 ), ij and Sij are bivariate normal, as shown in (B-1),
ij
j





implying that E ij Sij 

2
j

ij 
 
 Sij 



S

j

   
2

2

ij

  j   1  G j  Sij   j  where G j   2j



2

j

j


 2
  0    j
N  ,
2

  j  
 j





2  2  
j
j 

2

j

(B-1)



It follows that E Si , j 1 Sij  0  1 1  G j   j  1G j Sij is linear in Sij .
As discussed in the paper, Sij and ij are not bivariate normal when the extent of
measurement error varies across ability levels (i.e., ij   j ( ij ) ). Similar to the way
6



  2 j .









E  ij Sij can be computed, E ij Sij 

k

j

1
Sij M

 

 m1 ij  ij
M



*
*
2j ( mj
)   j ( mj
) where

*
 ( ) is the standard-normal density, k j  Sij  is the score density and  mj
is a random draw from









the distribution of  ij . In this way, we can compute E Si , j 1 Sij  0  1Sij  1E ij Sij . For
example, suppose that  ij





N 0, 2 ( ij ) with  n ( ij )   o   ( ij   j ) 2

N (670,30) and ij

and  n j  E j  n ( ij )   o   2j  15 . (These assumptions are roughly consistent with the
patterns found for the NYC test scores.) The three cases shown in Figure B.1 differ with respect
to the degree of heteroskedasticity: the homoskedastic case (  o  15 and   0 ), moderate
heteroskedasticity (  o  12 and   0.00333
(  o  3 and   0.01333



) and a more extreme heteroskedasticity





). Values of E Si , j 1 Sij for the three cases are shown in Figure B.2.







E Si , j 1 Sij is linear in the homoskedastic case and the degree to which E Si , j 1 Sij is S-

shaped depends upon the extent of this particular type of heteroskedasticity.





Knowing that the observed S-shape patterns of E Si , j 1 Sij can be consistent with

E  i , j 1 |  ij  being linear in  ij is useful, but of greater importance is whether E  i , j 1 |  ij  is in
fact linear for the tests of interest. This can be explored employing the cubic specification

 i , j 1  0  1 ij  2 ij2  3 ij3  i , j 1 where 2  3  0 implies linearity. Substituting
Sij   ij  ij and regressing Si , j 1 on Sij would yield biased parameter estimates. However, if





ik  E ( i* )k Si* , k  1, 2,3, 4 , were know for each student, regressing Si , j 1 on

i1 , i 2 , i 3 , and i 4 would yield consistent estimates.33

33

See the discussion of the "structural least squares" estimator in Kukush et. al (2005) .

7





Computing the ik  E ( i* ) k Si* , k  1, 2,3, 4 , for each student requires knowledge of
the overall extent and pattern of measurement error. It is the lack of such knowledge that motives





this paper. However, we are able to compute ˆik  Eˆ ( i* )k Si* accounting for the meaningful
measurement-error heteroskedasticity reflected in the reported SEMs34, even though this does not





account for other sources of measurement error. Computation of Eˆ ( i* ) k Si* also requires an
estimate of  2j which can be obtained by solving for ˆ2j implicitly defined in





ˆ2j  ˆ S2 j  ˆ2 j  ˆ S2 j   2   f  ˆ j , ˆ2j d . We solve for ˆ2j iteratively by computing





ˆ S2 j   2   f  ˆ j ,  2j d  ˆ S2 j 

    f 
2

1
M

2  mj*  .
M

Here the integral

m



ˆ j ,  2j d is computed using Monte Carlo integration with importance sampling





*
where the  mj
are random draws from the distribution N ˆ j ,  2j and  2j is an initial estimate

of  2j . This yielded an updated value of  2j which can be used to repeat the prior step.
Relatively few iterations are needed for converge to the fixed-point – our estimate of  2j The
estimate ˆ2j allows us to compute values of ˆik and, in turn, regress Sij +1 on

ˆi1 , ˆi 2 , ˆi 3 , and ˆi 4 .

.

Because SEM values are reported for a limited set of scores, a flexible functional form for 2   was fit to the
reported SEM. This function was then used in computation of moments.

34

8

Figure B.1
Examples Showing Different Degrees of Heteroskedastic Measurement Error

SD of measurement error

100
80
Extent of
heteroskedasticity:

60

none

40

moderate
more extreme

20
0
550

575

600

625

650

675

700

725

750

775

800

prior ability/skills

Figure B.2
How the Relationship Between E  Si Si  and Si
Varies with the Degree of Heteroskedasticity
800
775
750
725

score

700
675
650
625
600
575
550
550

575

600

625

650

675

700

725

750

775

800

prior score
Extent of heteroskedasticity:

none

moderate

9

more extreme

actual

