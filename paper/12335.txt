NBER WORKING PAPER SERIES

HOSPITAL COMPETITION, MANAGED CARE AND MORTALITY
AFTER HOSPITALIZATION FOR MEDICAL CONDITIONS:
EVIDENCE FROM THREE STATES
José J. Escarce
Arvind K. Jain
Jeannette Rogowski
Working Paper 12335
http://www.nber.org/papers/w12335
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
June 2006

This research was funded by grant number P01 HS10770-01 from the Agency for Healthcare Research and
Quality. We would like to thank Randy Hirscher for expert programming assistance, Elaine Quiter for project
management, and Kate Lee for administrative assistance. The views expressed herein are those of the
author(s) and do not necessarily reflect the views of the National Bureau of Economic Research.
©2006 by José J. Escarce, Arvind K. Jain and Jeannette Rogowski. All rights reserved. Short sections of
text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.

Hospital Competition, Managed Care and Mortality After Hospitalization for Medical
Conditions: Evidence From Three States
José J. Escarce, Arvind K. Jain and Jeannette Rogowski
NBER Working Paper No. 12335
June 2006
JEL No. I1
ABSTRACT
This study assessed the effect of hospital competition and HMO penetration on mortality after
hospitalization for six medical conditions in California, New York, and Wisconsin. We used linked
hospital discharge and vital statistics data to study adults hospitalized for myocardial infarction, hip
fracture, stroke, gastrointestinal hemorrhage, congestive heart failure, or diabetes. We estimated
logistic regression models with death within 30 days of admission as the dependent variable and
hospital competition, HMO penetration, and hospital and patient characteristics as explanatory
variables. Higher hospital competition was associated with lower mortality in California and New
York, but not Wisconsin. In addition, higher HMO penetration was associated with lower mortality
in California, but higher mortality in New York. In the context of the study states’ history with
managed care, these findings suggest that hospitals in highly competitive markets compete on quality
even in the absence of mature managed care markets. The findings also underscore the need to
consider geographic effects in studies of market structure and hospital quality.
José J. Escarce
Division of General Internal Medicine
Health Services Research
911 Broxton Plaza
Box 951736
Los Angeles, CA 90024
escarce@rand.org
Jeannette Rogowski
Department of Health Systems and Policy
University of Medicine and Dentistry of
New Jersey (UMDNJ)
335 George Street, Suite 2200
New Brunswick, NJ 08903
and NBER
rogowsje@umdnj.edu

BACKGROUND
Over the past two decades the structure of the U.S. health care industry has changed
dramatically. In particular, the growth of managed care—especially health maintenance
organizations (HMOs)—has led to the introduction of price competition among health care
providers. Numerous studies have assessed the effects of changes in health care market structure
on health care system performance. Studies of the hospital sector have confirmed that managed
care has led to the intensification of price competition among hospitals (e.g., Feldman et al.
1990), and that price competition has resulted in lower rates of cost growth, lower prices and
price-cost margins, and changes in the adoption and use of technology (e.g., Zwanziger and
Melnick 1988; Robinson 1991; Gaskin and Hadley 1997; Melnick et al. 1992; Keeler, Melnick,
and Zwanziger 1999; Baker and Phibbs 2002; Heidenreich et al. 2002; Bundorf et al. 2004).
However, the effects of changes in hospital market structure on the quality of care provided by
hospitals is less well understood. Only a handful of studies have addressed this issue and the
findings have not been consistent.
The literature provides mixed evidence on the effects of hospital competition and
managed care penetration on the quality of hospital care. In the earliest study, based on data
from the early 1980s, Shortell and Hughes (1988) found that in-hospital mortality rates for 16
clinical conditions were higher in market areas with higher HMO penetration, but the number of
competing hospitals in a market area was unassociated with mortality. More recently, Kessler
and McClellan (2000) studied 1-year mortality for Medicare patients with acute myocardial
infarction (AMI) in the 1980s and 1990s. They found that higher hospital competition was
associated with decreased mortality, especially after 1990. HMO penetration was not associated

3

with mortality, but the beneficial effects of hospital competition were stronger in high
penetration market areas. By contrast, Mukamel, Zwanziger, and Tomaszewski (2001), using
data from the 1990s, found no effects of hospital competition on 30-day mortality for Medicare
patients with a variety of conditions, but higher HMO penetration was associated with lower
mortality.
In a study based on hospital discharge data from 16 states in the 1990s, Sari (2002)
examined the effects of hospital competition and HMO penetration on the rates of in-hospital
mortality following common elective procedures, selected in-hospital complications, and
inappropriate surgery. They found that higher hospital competition and higher HMO penetration
were associated with lower rates of wound infections, iatrogenic complications, and
inappropriate surgery. Shen (2003) found that faster growth in non-Medicare HMO penetration
increased 7-day, 30-day, and 90-day mortality for Medicare patients with AMI, but did not affect
longer-term mortality.
While the five studies reviewed in the preceding paragraphs considered competition
globally, for all patients in a market, a recent study by Gowrisankaran and Town (2003) took a
different approach by assuming that hospitals compete for HMO and Medicare patients
separately. The investigators used data for Southern California in the early 1990s to study inhospital mortality for pneumonia and 30-day mortality for AMI. They found that competition
for HMO patients was associated with lower mortality, whereas competition for Medicare
patients was associated with higher mortality.
There are several potential reasons for the differences in findings across existing studies.
Some of the differences may be attributable to differences in the time periods studied. The
effects of hospital competition and managed care on hospital quality may have evolved over time

4

as hospitals adapted to the new regime of price competition. Methodological differences across
studies, including differences in measures of hospital quality or hospital competition, may
contribute to differences in findings as well. For example, since hospitals in more competitive
market areas and in areas with higher HMO penetration have shorter lengths of stay, and since
shorter length of stay may shift deaths from the hospital to the period following discharge (Baker
et al. 2002), use of in-hospital mortality (or complication rates) as the measure of quality may
lead to bias toward finding that higher hospital competition and HMO penetration are associated
with higher quality. The only studies that have been able to use mortality within a fixed time
interval as the quality measure are those that focus on Medicare patients. Last, the effects of
hospital competition and managed care may vary across geographic areas depending on the
maturity of managed care markets or other factors. No study has directly addressed this
possibility.
The goal of this study is to assess the effect of hospital competition and HMO penetration
on hospital quality of care for six medical conditions, contrasting the findings for three states:
California, New York, and Wisconsin. As shown in Figure 1, these states differ in their history
and experience with HMOs. California, the prototypical example of a state with mature
managed care markets, has had among the highest levels of HMO penetration since the early
1980s. HMO penetration in Wisconsin grew early, nearly equaling the level in California by
1985, but has since plateaued. New York had very low HMO penetration until about 1990, when
penetration began to rise and eventually surpassed the level in Wisconsin. Our study is based on
data for these three states between 1994 and 1999.

5

NEW CONTRIBUTION
Our study contributes to the literature on hospital market structure and the quality of
hospital care in three main ways. First, by linking hospital discharge and vital statistics data, we
are able to use 30-day mortality as the quality measure while including all patients in the
analysis, not just Medicare patients. Second, we use more recent data than prior studies. Finally,
and most important, we examine the experience of three states that differ in their history with
managed care and in the maturity of their managed care markets. Thus we assess the importance
of considering state-specific effects when studying the effects of managed care penetration and
hospital competition on hospital quality.

CONCEPTUAL FRAMEWORK
Hospital markets have evolved rapidly over the past two decades. In the 1970s, health
insurance consisted of fee-for-service plans that allowed their insured members to use any
hospital and paid for hospital care on a cost-reimbursement basis. Cost-based payment led
hospitals to compete through the services, amenities, and convenience they offered rather than
price. Under this form of nonprice competition, often called the “medical arms race,” hospitals
in more competitive markets had higher costs (Joskow 1980; Robinson and Luft 1985).
There are no studies of the impact of nonprice competition on the quality of hospital care.
However, one study of hospital choice that used data from the early 1980s found that patients
with a range of medical and surgical conditions were more likely to use hospitals having lower
in-patient mortality, other things equal (Luft et al., 1989). This suggests that in the days of the
“medical arms race” hospitals could compete for patients by offering higher quality of care.

6

Changes in the types of health insurance plans and in hospital payment beginning in the
early 1980s led to dramatic changes in the nature of competition among hospitals. Rapid
escalation in hospital costs led to the introduction of a prospective payment system for hospitals
by Medicare, reducing incentives for hospitals to increase costs. More important, the emergence
and rapid growth of managed care plans, especially HMOs, were enormously consequential.
Managed care plans bargained with hospitals to obtain price discounts and selectively contracted
with some of them to create networks of hospitals that would provide care to their insured
members. Selective contracting by managed care plans introduced price competition into
hospital markets and began to erode the “medical arms race” model of competition (e.g.,
Feldman et al. 1990). Of note, unique among the three study states, New York had an all-payer,
prospective rate-setting system for hospitals until 1997. However, New York’s system allowed
HMOs to negotiate lower prices with hospitals beginning in 1988 (McDonough 1997).
Documented effects of price competition among hospitals include lower rates of hospital
cost growth and lower rates of use of costly services and technologies (e.g., Zwanziger and
Melnick 1988; Robinson 1991; Gaskin and Hadley 1997; Baker and Phibbs 2002; Heidenreich et
al. 2002; Bundorf et al. 2004). Therefore, price competition, left unchecked, might be expected
to adversely affect hospital quality of care by inducing hospitals to skimp on the resources used
to care for patients. However, there is considerable indirect evidence that HMOs may have
fostered quality competition among hospitals as well, by making quality a factor in their
contracting decisions, at least in mature managed care markets like those in California.
The first line of indirect evidence comes from interviews and surveys of HMO executives
in California, who report that they consider information on quality when choosing hospitals for
contracts, even if the information is based on surrogate quality measures (Schulman et al. 1997;

7

Rainwater and Romano 2003). Notably, Schulman at al. (1997) found that the use of quality
information was infrequent in less mature managed care markets in Florida and Pennsylvania.
The second line of evidence comes from studies that have assessed quality of care in hospitals
used by large numbers of HMO patients compared with hospitals used by large numbers of
patients with fee-for-service insurance. Escarce et al. (1999) found that HMO patients who
received coronary artery bypass surgery were more likely than their peers with fee-for-service
coverage to use low-mortality hospitals in California, but not in Florida. Erickson et al. (2000)
found that patients in New York with managed care insurance were less likely than fee-forservice patients to use low-mortality hospitals for coronary artery bypass surgery. The final line
of evidence comes from an econometric study of the determinants of contracts between HMOs
and hospitals for bypass surgery, which found that HMOs are more likely to have contracts with
hospitals that have low mortality rates, other things equal (Gaskin et al. 2002).
This discussion suggests that the effects of hospital competition and HMO penetration on
the quality of hospital care are theoretically ambiguous. Although price competition might be
expected to affect hospital quality adversely, quality competition would tend to offset this effect.
Thus the net impact of hospital competition and HMO penetration on hospital quality is likely to
depend on the relative strength of these counterbalancing influences. The discussion also
suggests that the relative strength of price and quality competition may hinge on the maturity of
managed care markets, with quality competition being strongest in the most mature markets.

8

DATA AND METHODS
Data and Study Sample
The main source of data for this study consisted of linked hospital discharge and vital
statistics data for three states: New York (1995-1999), Wisconsin (1994-1999) and California
(1994-1999). (Linked data for 1994 were unavailable for New York.) The hospital discharge
data contained detailed information on all discharges from short-term general hospitals in these
states during the period in question, including admitting hospital; source of admission; patient
age, sex, race/ethnicity, and zip code of residence; principal diagnosis and secondary diagnoses
(up to 24 in California, 14 in New York, and 8 in Wisconsin); principal procedure secondary
procedures (up to 24 in California, 14 in New York, and 5 in Wisconsin); and type of health
insurance. The vital statistics data contained information on all deaths in these states during the
period in question.
We identified adult residents of the study states who were admitted to hospitals in
metropolitan areas from the community or from nursing homes for acute myocardial infarction
(AMI), hip fracture (HIP), stroke (CVA), gastrointestinal hemorrhage (GIH), congestive heart
failure (CHF), or diabetes mellitus (DM). We chose these six conditions because at least one
study of geographic variation in hospital admission rates found that each was a low variation
condition (e.g., Wennberg, McPherson, and Caper 1984; Gittlesohn and Powe 1995; McMahon,
Wolf, and Tedeschi 1989; Chassin et al. 1986). In the framework developed by Wennberg
(1987), low variation conditions tend to be those in which the criteria for hospitalizing patients
are narrowly defined and for which there is a relatively high degree of professional consensus
regarding the need for hospital admission. We wished to assess quality of care for conditions
with these characteristics because hospital competition and HMO penetration may influence

9

admission decisions for conditions in which hospital admission is discretionary. Thus, for
discretionary conditions, hospital competition and HMO penetration are more likely to be
correlated with unmeasured severity of illness, which may lead to biased estimates of their
effects on health outcomes. Nonetheless, there are differences in the degree of consensus
regarding the need for hospital admission even among the six study conditions. Whereas most
clinicians agree that all patients with AMI, HIP, or CVA and most patients with GIH require
hospitalization, admission decisions for CHF and DM are more discretionary. We excluded
patients who lived in other states because vital statistics data are more likely to record deaths for
state residents. We excluded admissions to hospitals in nonmetropolitan areas because we did
not have data on HMO penetration.
We refined the study sample further in four ways. First, we excluded admissions for
certain clinical variants of the study conditions to ensure more clinically homogenous samples or
samples with a high likelihood of needing hospital admission. For example, we excluded HIP
admissions where the fracture was due to primary or metastatic bone cancer or to major multiple
trauma because these cases are very different from typical fractures, and we excluded GIH
admissions where the hemorrhage was due to esophagitis or a Mallory-Weiss tear because these
cases generally do not require admission.
Second, we included only first admissions in an “episode of care,” defined as admissions
where another admission for the same condition had not occurred within the preceding 90 days.
For CHF, in particular, two or more admissions often occurred in close succession. Third, for
California, we included only admissions that began between April 1, 1994 and November 30,
1999; for Wisconsin, we included admissions that began between April 1, 1994 and December
31, 1999; and for New York, we included admissions that began between April 1, 1995 and

10

December 31, 1999. We excluded admissions in the first quarter of the first year of data for each
state so we could identify first admissions in an episode of care. In addition, we excluded
admissions in December 1999 for California so we could assess 30-day mortality (see below)
without censoring, because the California vital statistics data only included deaths through 1999.
(The Wisconsin and New York vital statistics data included deaths through 2000.) Fourth, for
each study condition, we excluded admissions to hospitals that had fewer than 25 admissions for
that condition during the period of the study.
The final study sample consisted of N=1,321,531 patients in California, N=876,704
patients in New York, and N=200,872 patients in Wisconsin. These admissions were to 363
different hospitals in California, 190 in New York, and 61 in Wisconsin. The study hospitals
were located in 25 different metropolitan areas in California, 13 in New York, and 13 in
Wisconsin.
Other data sources used in the study were the American Hospital Association Annual
Surveys of Hospitals, Medicare Cost Reports, and Medicare PPS Impact Files for 1994-1999.

Empirical Analyses
The measure of hospital quality in the study was 30-day mortality. We assessed the
effect of hospital competition and HMO penetration on quality by estimating admission-level
logistic regression models for each study condition within each state, using death within 30 days
of admission as the dependent variable and hospital competition, HMO penetration, hospital
characteristics1 and patient severity measures as explanatory variables.

1

We used the characteristics of the hospital to which the patient was initially admitted even if the patient was
subsequently transferred to a different hospital (e.g., Kessler and McClellan 2000; Gowrisankaran and Town 2003).
Transfer rates were low for all study conditions except AMI (for California: AMI, 22.0 percent; HIP, 1.6. percent;
CVA, 3.4 percent; GIH, 1.6 percent; CHF, 3.4 percent; and DM, 1.4 percent; for New York: AMI, 26.4 percent;

11

The key explanatory variables in the models were hospital competition and HMO
penetration. We assessed the degree of competition facing each hospital using the predicted 75
percent and 90 percent radii for the hospital, obtained from Gresenz, Rogowski, and Escarce
(2004), to define the hospital’s local market area.2 After identifying all the other hospitals in
each hospital’s local market area, we derived the following competition measures: (1) a
competition index calculated as one minus the Herfindahl index based on bed shares, and (2) the
square root of the number of hospitals. We used one minus the Herfindahl index based on the 90
percent radius in our main analyses, and conducted sensitivity analyses using the alternate
measures. HMO penetration in each metropolitan area was measured as the fraction of the
population enrolled in an HMO, obtained from the InterStudy Regional Market Analysis
database for 1994-1999.
The hospital characteristics included in the models were teaching status, categorized as
none, minor, or major based on the intern- and resident-to-bed ratio;3 ownership, categorized as
public (i.e., city or county-owned), private nonprofit, or private for-profit;4 and bed size,
categorized as less than 100 beds, 100-199 beds, 200-399 beds, or 400 or more beds.
HIP, 3.1 percent; CVA, 2.6 percent; GIH, 1.0 percent; CHF, 3.7 percent; and DM, 0.9 percent; for Wisconsin: AMI,
12.3 percent; HIP, 2.0 percent; CVA, 3.2 percent; GIH, 1.2 percent; CHF, 2.6 percent; and DM, 1.2 percent). In
effect, our approach holds the admitting hospital responsible for making transfers that would improve patient
outcomes.
2

Briefly, we used hospital discharge data for nine states in 1997 to determine, for each short-term general hospital
in those states, the distance from the hospital to patient zip codes required to account for 75 percent and 90 percent
of the admissions to the hospital. We then developed a regression model for the 75 and 90 percent radii as functions
of hospital and market area characteristics, and we used the estimated coefficients to predict the radii for every
metropolitan hospital in the U.S. (For details, see Gresenz, Rogowski, and Escarce [2004].)
3

Teaching hospitals were those with any interns or residents, and major teaching hospitals were those with more
than 0.25 interns and residents per bed. Data on interns and residents were obtained from Medicare Cost Reports.
4

District hospitals in California are tax supported, but they resemble private nonprofit hospitals in most other ways.
Because there are few district hospitals in metropolitan areas, we included them in the nonprofit category. All
hospitals in Wisconsin were private nonprofits except for one public hospital, so we did not include ownership in the
Wisconsin models.

12

To control for differences in patient severity, we also included a variety of severity
measures as covariates in the regression models, including patient age, sex, whether the patient
was admitted from a nursing home, chronic comorbidities, and a set of condition-specific
measures for each study condition. The chronic comorbidities used in the analyses were the
conditions identified by Iezzoni et al. (1994) as conditions that are nearly always present prior to
hospital admission; hence they are extremely unlikely to represent complications due to poor
care. They included primary cancer with a poor prognosis, metastatic cancer, chronic pulmonary
disease, coronary artery disease, congestive heart failure, peripheral vascular disease, severe
chronic liver disease, diabetes mellitus with end-organ damage, chronic renal failure, nutritional
deficiencies, dementia, and functional impairment.5 Examples of the condition-specific
measures for AMI include indicators for the location of the infarction and for the presence of
complete heart block; for CVA, indicators for hemorrhagic stroke and for different types of
ischemic stroke; for GIH, indicators for the source of the bleeding, such as esophageal varices,
different types of peptic ulcer with or without perforation or obstruction, arteriovenous
malformations, and diverticulosis; and for DM, indicators for the type of diabetes (type 1 or type
2), for the presence of ketoacidosis and nonketotic coma, and for different end-organ
complications.6 Finally, the models included indicator variables for year of admission.

5

Elixhauser et al. (1998) developed a more comprehensive list of comorbidities. However, this list includes
conditions, such as hypertension, paralysis, obesity, and uncomplicated diabetes, that have been found to be
underreported in hospital discharge data, especially for patients who die (Romano and Mark 1994; Iezzoni et al.
1992; Iezzoni et al. 1994).
6

To test the performance of these severity measures, we estimated logistic regression models for each study
condition using death within 30 days as the dependent variable and the severity measures, alone, as explanatory
variables. The c-statistics for these models ranged from 0.75 to 0.82 (with the exception of 0.66 to 0.69 for CHF),
indicating excellent discrimination. Additionally, comparisons of predicted and observed mortality across deciles of
risk showed good calibration.

13

Standard errors were corrected for clustering of admissions within hospitals using a
Huber-White sandwich estimator. We converted the odds ratios from the logistic regression
models to approximate relative risks using the method described by Zhang and Yu (1998).

RESULTS
Descriptive Data
Table 1 reports 30-day mortality for the six study conditions, by state. Mortality was
lowest for DM and highest for CVA and AMI in every state. Comparing states, mortality rates
were highest in Wisconsin, intermediate in California, and lowest in New York.
Patients in Wisconsin were slightly older than patients in the other states (data not
shown). Thus 71.8% of Wisconsin patients were 65 years or older across all the study
conditions, compared with 69.7% of patients in each of California and New York, and 18.8% of
Wisconsin patients were 85 years or older, compared with 17.3% of California patients and
18.4% of New York patients. The percentage of women patients across the study conditions
ranged from 51.3% in California, to 51.7% in Wisconsin, to 54.0% in New York. Patients in
Wisconsin averaged 1.02 chronic comorbidities, compared with 1.00 for patients in California
and 0.98 for patients in New York. However, patients in California were more likely than those
in Wisconsin or New York to be admitted to the hospital from a nursing home (4.7%, 1.3%, and
2.4%, respectively).
Table 2 reports the characteristics of the hospitals that contributed admissions to the
study sample in each state. The competition index (calculated as one minus the Herfindahl
index) based on the 90 percent radius averaged 0.79 across hospitals in California, 0.72 in New
York, and 0.68 in Wisconsin. On average, hospitals in California had 19.8 hospitals within their

14

90 percent radius, compared with 28.0 hospitals for hospitals in New York and 8.0 hospitals for
hospitals in Wisconsin. The competition measures based on the 75 percent radius exhibited
similar patterns across the three states. The average HMO penetration in the metropolitan areas
where the study hospitals were located was 0.43 in California, 0.34 in New York, and 0.32 in
Wisconsin.
There were large differences across the study states in hospital ownership (Table 2).
Hospitals in California were much more likely to be for-profit (27.0%) than hospitals in New
York (5.4%), and Wisconsin had no for-profit hospitals. By contrast, hospitals in New York
were more likely to be public (9.7%) than hospitals in California (5.9%) or Wisconsin (1.5%).
The teaching status of hospitals also differed across states. Only three in ten hospitals in
California had teaching programs, compared with half in New York and Wisconsin. On the
other hand, the proportion of hospitals categorized as major teaching hospitals was much larger
in New York (28.5%) than either California (7.6%) or Wisconsin (5.3%). The study states
differed in the distribution of hospital size as well. Wisconsin had the highest fraction of small
hospitals with fewer than 100 beds, while New York had the highest fraction of very large
hospitals with 400 or more beds.

Regression Results
Tables 3a-3c report the findings of our main analyses regarding the effects of hospital
competition, HMO penetration, and hospital characteristics on 30-day mortality for the study
conditions in California, New York, and Wisconsin, respectively. We found that hospital
competition was associated with lower mortality rates in California and New York. In
California, five conditions had significantly lower mortality in hospitals that faced greater

15

competition: HIP (relative risk [RR]=0.75, p<0.01), CVA (RR=0.71, p<0.001), GIH (RR=0.83,
p<0.05), CHF (RR=0.88, p<0.10), and DM (RR=0.82, p<0.01). In New York, three conditions
had significantly lower mortality in hospitals that faced more competition: AMI (RR=0.85,
p<0.01), GIH (RR=0.86, p<0.10), and DM (RR=0.86, p<0.10). The point estimates for the other
conditions in both states were consistent with a protective effect of competition, but these
estimates did not achieve statistical significance.7

Hospital competition was unassociated with

mortality in Wisconsin.
The relative risks summarized in the preceding paragraph are consistent with clinically
significant effects of hospital competition on mortality. For example, other things equal, a
California hospital at the 10th percentile of the competition index (one minus the Herfindahl
index based on the 90 percent radius) had a 30-day mortality rate of 7.1% for HIP, compared
with 6.2% in a hospital at the 90th percentile of the index. The corresponding figures for other
conditions were 18.4% and 15.6% for CVA, 6.2% and 5.6% for GIH, 9.0% and 8.4% for CHF,
and 3.9% and 3.2% for DM. Similarly, other things equal, a New York hospital at the 10th
percentile of the competition index had a 30-day mortality rate of 14.3% for AMI, compared
with 12.2% in a hospital at the 90th percentile of the index. The corresponding figures for other
conditions were 6.6% and 5.7% for GIH, and 3.2% and 2.7% for DM.
The effects of HMO penetration differed strikingly across the study states (Tables 3a-3c).
In California, higher HMO penetration reduced mortality for GIH (RR=0.76, p<0.05) and CHF
(RR=0.78, p<0.01), and the point estimates for most of the remaining conditions were consistent
7

Kessler and McClellan (2000) and Gowrisankaran and Town (2003) have argued that measures of hospital
competition based on patient flows may be endogenous to hospital quality, since hospitals with higher quality may
draw patients from longer distances. We based our competition measures on predicted rather than observed radii, as
described earlier, in order to avoid endogeneity, and we tested whether this strategy worked by estimating additional
logistic models for 30-day mortality that included the hospital’s predicted radius as an explanatory variable. The
coefficient of the predicted radius was close to zero for all the study conditions in all states, and the coefficients of
hospital competition did not change appreciably. Moreover, the statistical significance of the findings for
competition was strengthened in California and only slightly weakened in New York.

16

with a protective effect of HMO penetration, although these estimates did not achieve statistical
significance. Similarly, in Wisconsin, higher HMO penetration reduced mortality for CHF
(RR=0.66, p<0.01), and the point estimates for four of the five remaining conditions were
consistent with a protective effect of HMO penetration. By contrast, in New York, higher HMO
penetration was associated with higher mortality for HIP (RR=1.54, p<0.001), CVA (RR=1.33,
p<0.01), GIH (RR=1.54, p<0.01), and CHF (RR=1.43, p<0.01). Further, the point estimates for
AMI and DM suggested a harmful effect of HMO penetration as well, but these estimates were
not significant.
Because the estimated effects of hospital competition and HMO penetration were
qualitatively similar for the six study conditions within each state (Tables 3a-3c), we estimated a
model for each state where we pooled all the conditions and included interactions between
condition and all the explanatory variables except competition and penetration. These analyses
found significant protective effects of hospital competition in California (OR=0.80, p<0.001) and
New York (OR=0.89, p<0.05), but not Wisconsin (OR=1.02, p>0.05). The pooled analyses also
found significant protective effects of HMO penetration in California (OR=0.86, p<0.05) and
Wisconsin (O=0.78, p<0.05). However, in New York, higher HMO penetration was associated
with higher mortality (OR=1.39, p<0.001).
In addition, to assess whether the protective effect of hospital competition was greater in
metropolitan areas with high HMO penetration, we estimated models where we again pooled all
the study conditions but this time included an interaction between hospital competition and an
indicator variable for metropolitan areas in the top half of the distribution of HMO penetration
for each state. These analyses found a significant protective effect of hospital competition in
both high and low penetration metropolitan areas in California, but the protective effect of

17

competition was greater in high penetration areas (RR=0.81, p<0.001, in high penetration areas;
RR=0.91, p<0.10, in low penetration areas; p<0.001 for test of difference in odds ratios). The
protective effect of hospital competition did not vary by HMO penetration in New York.
Other hospital characteristics were also associated with mortality for medical conditions
(Tables 3a-3c), although the precise findings varied across the study states. Public hospitals in
California had higher mortality than private nonprofit hospitals for AMI and lower mortality for
CHF and DM, while public hospitals in New York had higher mortality for CVA. For-profit
hospitals in California had higher mortality for AMI and lower mortality for CVA, CHF and
DM, whereas for-profit hospitals in New York had lower mortality for HIP and CVA. In
California, major teaching hospitals had higher mortality than non-teaching hospitals for CVA,
GIH, and DM. By contrast, major teaching hospitals in New York had lower mortality than nonteaching hospitals for CVA, CHF, and DM, and major teaching hospitals in Wisconsin had lower
mortality for DM. In California, large hospitals of 400 beds or more tended to have lower
mortality than small hospitals with fewer than 100 beds. Large hospitals in New York had
higher mortality than small hospitals for GIH and CHF, whereas in Wisconsin the effect of
hospital size on mortality was mixed.

Alternate Measures of Hospital Competition
We explored the robustness of our findings for hospital competition using three alternate
competition measures: the competition index based on the 75 radius and the square root of the
number of hospitals based on both the 90 percent and 75 percent radii. As shown in Table 4, the
finding of protective effects of hospital competition in California and New York was robust. In
California, AMI was the only study condition for which higher competition did not reduce

18

mortality. In New York, every study condition exhibited a protective effect of hospital
competition for at least one competition measure, although within condition the level of
statistical significance varied across measures. We found no evidence that hospital competition
affected mortality in Wisconsin irrespective of the competition measure used. Our findings
regarding the effects of HMO penetration on mortality did not change as we varied the measure
of hospital competition (data not shown).

Effect of Hospital Competition on Medicare Versus Non-Medicare Patients
To assess whether the effect of hospital competition mortality differed for Medicare and
non-Medicare patients (Gowrisankaran and Town, 2003), we estimated models for 30-day
mortality that included the appropriate interaction terms. In California, the effect of competition
differed only for AMI. Higher competition reduced mortality for non-Medicare patients with
AMI (RR=0.89, p<0.10), but not for Medicare patients. In New York, the effect of competition
differed for four study conditions. Thus higher competition reduced mortality for non-Medicare
patients with HIP (RR=0.79, p<0.05), CVA (RR=0.87, p<0.05), CHF (RR=0.88, p<0.10), and
DM (RR=0.80, p<0.05), but not for Medicare patients with those conditions. For all other study
conditions, higher competition had the same effect (or lack thereof) on both Medicare and nonMedicare patients. In no case did higher competition lead to higher mortality for Medicare
patients.

Additional Sensitivity Analyses
We conducted additional sensitivity analyses to further assess the robustness of our
results. First, hospital discharge data have been criticized for lacking the clinical detail necessary

19

to capture illness severity adequately enough to assess hospital quality of care (e.g., Pine et al.
1997; Hannan et al. 1992). Therefore, we reestimated the models in our main analyses including
an indicator for uninsured patients and an indicator for patients with insurance coverage from
Medicaid or an indigent program under the rationale that these categories may capture
unobserved dimensions of health status (e.g., Parkerson et al. 2005; Hadley 2003). The findings
for hospital competition and HMO penetration in Tables 3a-3c were unchanged.
Second, due to the limited number of metropolitan areas in each study state, and because
we were concerned that HMO penetration could be endogenous to hospital quality, we estimated
models for 30-day mortality where we replaced the HMO penetration variable with metropolitanarea fixed effects. The findings for hospital competition were unchanged.
Third, because the New York City metropolitan area accounted for a large share of
hospital admissions in the state of New York (one-third to two-fifths across conditions), and
because some observers believe that the hospital market in New York City is atypical (Salit,
Fass, and Nowak 2002), we repeated the analyses for New York excluding admissions to
hospitals in New York City. The findings for hospital competition and HMO penetration were
again unchanged.
Fourth, because New York did not fully abolish hospital rate-setting until 1997, we
estimated models to test for differences in the effects of hospital competition and HMO
penetration between 1995-1996 and 1997-1999. We did not find appreciable differences.
Finally, we estimated models with 90-day and 180-day mortality, rather than 30-day
mortality, as the outcome. As Table 5 shows, the effects of hospital competition and HMO
penetration on 90-day and 180-day mortality were similar to their effects on 30-day mortality,

20

although the effects of competition were slightly attenuated in the analyses of 180-day mortality
in California.

DISCUSSION
This study examined the effects of hospital competition and HMO penetration on the
quality of hospital care for six medical conditions in California, New York, and Wisconsin. We
found that hospitals in California and New York that faced a higher degree of competition
generally had lower mortality rates within 30 days of hospital admission. Specifically, higher
hospital competition led to lower mortality for five of the six study conditions in California and
all six conditions in New York, although significance levels varied depending on the competition
measure used. Analyses where we pooled the six study conditions also found protective effects
of competition in California and New York. By contrast, hospital competition was unassociated
with mortality in Wisconsin. Our findings regarding hospital competition were robust to a wide
range of sensitivity analyses in which we employed alternate measures of hospital competition
and varied the explanatory variables in the regression models. Similarly, analyses using 90-day
or 180-day mortality as the outcome, rather than 30-day mortality, found beneficial effects of
hospital competition in California and New York but not Wisconsin. In contrast to
Gowrisankaran and Town (2003), we did not find that higher competition was associated with
higher mortality for Medicare patients.
The study also found that higher HMO penetration was associated with lower mortality
for gastrointestinal hemorrhage and congestive heart failure in California and for congestive
heart failure in Wisconsin, and the analyses where we pooled the six study conditions found a

21

beneficial effect of higher HMO penetration in California and Wisconsin as well. Moreover,
when we included an interaction between hospital competition and HMO penetration in the
regression models, we found that the protective effect of competition in California was stronger
in metropolitan areas with high penetration. By contrast, in New York higher HMO penetration
was associated with a substantial increase in mortality for four of the six study conditions and in
the pooled analyses. We interpret our results for HMO penetration with caution due to the fact
that penetration was measured at the level of metropolitan areas and each study state only had
between 13 and 25 metropolitan areas. The small numbers of observations on HMO penetration,
especially in New York and Wisconsin, coupled with the concern that HMO penetration could be
endogenous to hospital quality, argue for regarding these results as suggestive rather than
conclusive. Nonetheless, the difference across states in the findings for penetration is striking.
The results of this study are consistent with the thesis that hospitals competed on “true”
quality of care—i.e., processes and outcomes of care—rather than just on price or amenities in
California and New York, but not in Wisconsin. This observation raises two questions. First,
what is the source of the divergent results for California and New York, on one hand, compared
with Wisconsin, on the other? Although we cannot be certain, one possibility is that the effects
of hospital competition on quality of care emerge only at high levels of competition. As shown
in Table 2, hospitals in Wisconsin typically competed with many fewer hospitals than hospitals
in either California or New York.
Second, and more important, do our results regarding the effects of hospital competition
on hospital quality have anything to do with managed care? Our finding that in California
hospital competition had a stronger beneficial effect on mortality in metropolitan areas with high
HMO penetration suggests that HMOs in that state fostered quality competition among hospitals

22

and that they were at least partly responsible for the competition effects on quality that we found.
This interpretation is consistent with what is known about the maturity of managed care markets
in California and with the indirect evidence, reviewed earlier, that HMOs in California have
made hospital quality a factor in their contracting decisions. It is also consistent with our finding
that higher HMO penetration, per se, was associated with lower mortality in California. A
substantial presence in a market area of HMOs that take quality into account in contracting and
that induce hospitals to compete on quality would be expected to lead to overall improvements in
the quality of care in the area.
On the other hand, our finding that in New York the beneficial effect of hospital
competition on mortality was the same in high and low HMO penetration areas, coupled with the
finding that higher HMO penetration in a metropolitan area was associated with higher mortality,
suggests that HMOs in New York may not have played a prominent role in promoting quality
competition among hospitals. This interpretation is consistent with the observation that HMOs
in New York did not begin to grow rapidly until the 1990s, and that hospital markets were not
fully deregulated until 1997. It is also consistent with Erickson et al.’s (2000) finding that
managed care patients in New York were less likely than fee-for-service patients to use lowmortality hospitals for coronary artery bypass surgery, which suggests that managed care
organizations did not emphasize quality in hospital contracting. Rather, the findings for New
York raise the possibility that competition on “true” quality of care was a component of the
medical arms race model of hospital competition that prevailed in New York prior to the rapid
growth of managed care. Notably, studies have found that patients with fee-for-service
insurance are more likely to use hospitals with low mortality than hospitals with high mortality,
other things equal (e.g., Luft et al. 1989; Escarce et al. 1999), which gives hospitals a reason to

23

compete on quality even when individual patients and their physicians, and not managed care
organizations, are making the choices.
Additional findings of our study addressed to the effects on quality of hospital ownership,
teaching status, and bed size. Several studies have found similar or worse outcomes in public
hospitals compared with private hospitals (e.g., Shapiro et al. 1994; Kuhn et al. 1994).
Therefore, the finding of lower mortality for congestive heart failure in diabetes in public
hospitals in California was unexpected. To investigate this finding, we compared the
characteristics of patients admitted to public and private hospitals in California. We found that
compared with patients admitted to private hospitals, those admitted to public hospitals were
much younger, had fewer co-morbidities, and had substantially lower predicted probabilities of
death for all the study conditions. If our severity measures failed to capture all the differences in
illness severity between public and private hospital patients, the finding of lower mortality for
congestive heart failure and diabetes in public hospitals could reflect unobserved differences in
severity.
The finding that major teaching hospitals in California had higher mortality for three
conditions was also unexpected. Several studies have reported better quality of care and lower
mortality in major teaching hospitals compared with other hospitals (e.g., Keeler et al. 1992;
Rosenthal et al. 1997; Ayanian and Weissman 2002). On the other hand, reports developed by
California’s Office of Statewide Health Planning and Development (OSHPD) on hospital quality
of care for myocardial infarction and pneumonia in California are not consistent with published
studies. For myocardial infarction, major teaching hospitals in California were more likely than
other hospitals in metropolitan areas to be both low-mortality and high-mortality outliers
(OSHPD 2002). For pneumonia, major teaching hospitals were less likely than other hospitals to

24

be low-mortality outliers but more likely to be high-mortality outliers (OSHPD 2004). Our
findings regarding teaching status and 30-day mortality in New York and Wisconsin were
consistent with the published literature.
A noteworthy strength of our study is that we employed linked hospital discharge and
vital statistics data, which enabled us to use 30-day mortality, rather than in-hospital mortality, as
the measure of quality of care for all patients. Previous studies that used mortality within a fixed
time interval after hospital admission as the quality measure were limited to Medicare patients
(e.g., Kessler and McClellan 2000; Mukamel, Zwanziger, and Tomaszewski 2001; Shen 2003).
Using in-hospital mortality to assess quality may lead to biased estimates of the effects of market
structure on quality because market structure may affect hospital length of stay and,
consequently, the likelihood of dying in the hospital (e.g., Baker et al. 2002). An additional
strength is that we examined mortality for six medical conditions that vary in the degree of
professional consensus regarding the need for hospitalization, including several for which there
is a great deal of consensus. Further, our focus on a small number of conditions enabled us to
include a variety of carefully selected, condition-specific severity measures in our regression
models.
Our study also has several limitations. First, discharge data are inherently limited in their
ability to capture patient severity, since they lack clinical detail such as laboratory and
physiologic data (e.g., Pine et al. 1997). Although we used multiple and detailed conditionspecific measures to assess severity, we cannot rule out the possibility that the limitations of
discharge data influenced our findings.
Second, radius-based measures of hospital competition have been criticized because all
hospitals inside the radius count equally whereas hospitals just outside the radius do not count at

25

all. We addressed this concern by using competition measures based on two different radii—the
75 percent and 90 percent radii—which in practice led to sizable differences in the number of
hospitals that contributed to the competition measures. Our results did not change appreciably
based on the choice of radius. Radius-based competition measures have also been criticized for
being endogenous, but we addressed this concern by using predicted, rather than observed, radii.
Third, the small number of metropolitan areas in each study state precludes drawing
definitive conclusions about the impact of HMO penetration on hospital quality of care.
Nonetheless, the divergence in the findings for penetration across the three states lends support
to the notion that the effect of managed care on quality may vary geographically, possibly
depending on characteristics of the managed care market. The results for New York, in
particular, call for additional research into the circumstances where managed care may lead to
lower hospital quality. In addition, our study did not address the influence of hospital market
structure on quality in nonmetropolitan areas.
Fourth, because our study did not assess the impact of hospital competition on costs, the
implications of our findings for the welfare effects of competition are uncertain. In particular,
we don’t know whether competition reduces hospital costs in a state with few competition
hospitals, such as Wisconsin, to the same degree as in California.
To conclude, this study provides robust evidence that hospital competition can lead to
higher quality of care for a range of medical conditions, at least in states like California and New
York where hospitals typically compete with many other hospitals. However, the findings of the
study also suggest that quality competition among hospitals may not require the presence of
managed care. In particular, the results for New York raise the possibility that quality
competition may be a feature of competitive hospital markets irrespective of the characteristics

26

of managed care markets. The study findings also suggest that the influence of managed care on
hospital quality may vary across states. Taken together, our results underscore the need to
consider state-specific effects in studies of health care markets and quality of care. Given the
“backlash” against managed care, studies to assess the conditions under which hospitals compete
on quality even in the absence of mature managed care markets warrant particular attention.

27

REFERENCES
Ayanian, J.Z., and J.S. Weissman. 2002. “Teaching Hospitals and Quality of Care: A Review
of the Literature.” The Milbank Quarterly 80 (3): 569-93.

Baker, D.W., D. Einstadter, C.L. Thomas, S.S. Husak, N.H. Gordon, and R.D. Cebul. 2002.
“Mortality Trends During a Program that Publicly Reported Hospital Performance.” Medical
Care 40 (10): 879-90.

Baker, L.C., and C.S. Phibbs. 2002. “Managed Care, Technology Adoption, and Health Care:
The Adoption of Neonatal Intensive Care.” The RAND Journal of Economics 33 (3): 524-48.

Bundorf, M.K., K.A. Schulman, J.A. Stafford, D. Gaskin, J.G. Jollis, and J.J. Escarce. 2004.
“Impact of Managed Care on the Treatment, Costs, and Outcomes of Fee-for-Service Medicare
Patients with Acute Myocardial Infarction.” Health Services Research 39 (1): 131-52.

Chassin, M.R., R.H. Brook, R.E. Park, J. Keesey, A. Fink, J. Kosecoff, K. Kahn, N. Merrick, and
D.H. Solomon. 1986. “Variations in the Use of Medical and Surgical Services by the Medicare
Population.” New England Journal of Medicine 314 (5): 285-90.

Elixhauser, A., C. Steiner, D.R. Harris, and R.M. Coffey. 1998. “Comorbidity Measures for
Use with Administrative Data.” Medical Care 36 (1): 8-27.

28

Erickson, L.C., D.F. Torchiana, E.C. Schneider, J.W. Newburger, and E.L. Hannan. 2000. “The
Relationship between Managed Care Insurance and Use of Lower-Mortality Hospitals for CABG
Surgery.” Journal of the American Medical Association 283 (15): 1976-82.

Escarce, J.J., R.L. Van Horn, M.V. Pauly, S.V. Williams, J.A. Shea, and W. Chen. 1999.
“Health Maintenance Organizations and Hospital Quality for Coronary Artery Bypass Surgery.”
Medical Care Research and Review 56 (3): 340-62.

Feldman, R., H.C. Chan, J. Kralewski, B. Dowd, and J. Shapiro. 1990. “Effects of HMOs on the
Creation of Competitive Markets for Hospital Services.” Journal of Health Economics 9 (2):
207-22.

Gaskin, D.J., and J. Hadley. 1997. “The Impact of HMO Penetration on the Rate of Hospital
Cost Inflation, 1985-1993.” Inquiry 34 (3): 205-16.

Gaskin, D.J., J.J. Escarce, K. Schulman, and J. Hadley. 2002. “The Determinants of HMOs’
Contracting with Hospitals for Bypass Surgery.” Health Services Research 37 (4): 963-84.

Gittelsohn, A., and N.R. Powe. 1995. “Small Area Variations in Health Care Delivery in
Maryland.” Health Services Research 30 (2): 295-317.

Gowrisankaran, G., and R.J. Town. 2003. “Competition, Payers and Hospital Quality.” Health
Services Research 38 (6, part 1): 1403-21.

29

Gresenz, C.R., J. Rogowski, and J.J. Escarce. 2004. “Updated Variable-Radius Measures of
Hospital Competition.” Health Services Research 39 (2): 417-30.

Hadley, J. 2003. “Sicker and Poorer—the Consequences of Being Uninsured: A Review of the
Research on the Relationship Between Health Insurance, Medical Care Use, Health, Work, and
Income.” Medical Care Research and Review 60 (2) Suppl: 3S-75S.

Hannan, E.L., H. Kilburn Jr., M.L. Lindsey, and R. Lewis. 1992. “Clinical Versus
Administrative Data Bases for CABG Surgery. Does it Matter?” Medical Care 30 (10): 892907.

Heidenreich, P.A., M. McClellan, C. Frances, and L.C. Baker. 2002. “The Relation between
Managed Care Market Share and the Treatment of Elderly Fee-for-Service Patients with
Myocardial Infarction.” The American Journal of Medicine 112 (3): 176-82.

Iezzoni, L.I., T. Heeren, S.M. Foley, J. Daley, J. Hughes, and G.A. Coffman. 1994. “Chronic
Conditions and Risk of In-Hospital Death.” Health Services Research 29 (4): 435-60.

Iezzoni, L.I., S.M. Foley, J. Daley, J. Hughes, E.S. Fisher, and T. Heeren. 1992.
“Comorbidities, Complications, and Coding Bias. Does the Number of Diagnosis Codes Matter
in Predicting In-Hospital Mortality?” The Journal of the American Medical Association 267
(16): 2197-203.

30

Joskow, P.L. 1980. “The Effects of Competition and Regulation on Hospital Bed Supply and
the Reservation Quality of the Hospital.” The Bell Journal of Economics 11 (2): 421-47.

Keeler, E.B., L.V. Rubenstein, K.L. Kahn, D. Draper, E.R. Harrison, M.J. McGinty, W.H.
Rogers, and R.H. Brook. 1992. “Hospital Characteristics and Quality of Care.” The Journal of
the American Medical Association 268 (13): 1709-14.

Keeler, E.B., G. Melnick, and J. Zwanziger. 1999. “The Changing Effects of Competition on
Non-Profit and For-Profit Hospital Pricing Behavior.” Journal of Health Economics 18 (1): 6986.

Kessler, D.P., and M.B. McClellan. 2000. “Is Hospital Competition Socially Wasteful?” The
Quarterly Journal of Economics 115 (4): 577-615.

Kuhn, E.M., A.J. Hartz, H. Krakauer, R.C. Bailey, A.A. Rimm. 1994. “The Relationship of
Hospital Ownership and Teaching Status to 30- and 180-Day Adjusted Mortality Rates.”
Medical Care 32 (11): 1098-108.

McDonough, J.E. 1997. “Tracking The Demise of State Hospital Rate Setting.” Health Affairs
16 (1): 142-49.

31

McMahon Jr., L.F., R.A. Wolfe, P.J. Tedeschi. 1989. “Variation in Hospital Admissions among
Small Areas. A Comparison of Maine and Michigan.” Medical Care 27 (6): 623-31.

Melnick, G.A., J. Zwanziger, A. Bamezai, and R. Pattison. 1992. “The Effects of Market
Structure and Bargaining Position on Hospital Prices.” Journal of Health Economics 11 (3):
217-33.

Mukamel, D.B., J. Zwanziger, and K.J. Tomaszewski. 2001. “HMO Penetration, Competition,
and Risk-Adjusted Hospital Mortality.” Health Services Research 36 (6 part 1): 1019-35.

OSHPD Hospital Outcomes Center. 2004. Report on Hospital Outcomes for CommunityAcquired Pneumonia in California, 1999-2001. Sacramento, California: Healthcare Quality and
Analysis Division, California Office of Statewide Health Planning and Development. February.

OSHPD Healthcare Quality and Analysis Division. 2002. Report on Heart Attack Outcomes in
California 1996-1998, Volume 1: User’s Guide. Sacramento, CA: California Office of
Statewide Health Planning and Development. February.

Parkerson, G.R. Jr., W.E. Hammond, J.L. Michener, K.S. Yarnall, and J.L. Johnson. 2005.
“Risk Classification of Adult Primary Care Patients by Self-Reported Quality of Life.” Medical
Care 43 (2): 189-93.

32

Pine, M., M. Noriusis, B. Jones, and G.E. Rosenthal. 1997. “Predictions of Hospital Mortality
Rates: A Comparison of Data Sources.” Annals of Internal Medicine 126 (5): 347-54.

Rainwater, J.A., and P.S. Romano. 2003. “What Data do California HMOs Use to Select
Hospitals for Contracting?” The American Journal of Managed Care 9 (8): 553-61.

Romano, P.S., and D.H. Mark. 1994. “Bias in the Coding of Hospital Discharge Data and Its
Implications for Quality Assessment.” Medical Care 32 (1): 81-90.

Romano, P.S. and R. Mutter. 2004. “The Evolving Science of Quality Measurement for
Hospitals: Implications for Studies of Competition and Consolidation.” International Journal of
Health Care Finance and Economics 4 (2): 131-57.

Robinson, J.C. 1991. “HMO Market Penetration and Hospital Cost Inflation in California.”
The Journal of the American Medical Association 266 (19) 2719-23.

Robinson, J. C., and H. S. Luft. 1985. “The Impact of Hospital Market Structure on Patient
Volume, Average Length of Stay, and the Cost of Care.” Journal of Health Economics 4 (4):
333-56.

Rosenthal, G.E., D.L. Harper, L.M. Quinn, and G.S. Cooper. 1997. “Severity-Adjusted
Mortality and Length of Stay in Teaching and Nonteaching Hospitals. Results of a Regional
Study.” The Journal of the American Medical Association 278 (6): 485-90.

33

Salit, S., S. Fass, and M. Nowak. 2002. “Out of the Frying Pan: New York City Hospitals in an
Age of Deregulation.” Health Affairs 21 (1): 127-39.

Sari, N. 2002. “Do Competition and Managed Care Improve Quality?” Health Economics 11
(7): 571-84.
Schulman, K.A., L.E. Rubenstein, D.M. Seils, M. Harris, J. Hadley, and J.J. Escarce. 1997.
“Quality Assessment in Contracting for Tertiary Care Services by HMOs: A Case Study of
Three Markets.” The Joint Commission Journal on Quality Improvement 23 (2): 117-27.

Shapiro, M.F., R.E. Park, J. Keesey, and R.H. Brook. 1994. “The Effect of Alternative CaseMix Adjustments on Mortality Differences Between Municipal and Voluntary Hospitals in New
York City.” Health Services Research 29 (1): 95-112.

Shen, Y.C. 2003. “The Effect of Financial Pressure on the Quality of Care in Hospitals.”
Journal of Health Economics 22 (2): 243-69.

Shortell, S.M., and E.F. Hughes. 1988. The Effects of Regulation, Competition, and Ownership
on Mortality Rates among Hospital Inpatients. The New England Journal of Medicine 318 (17):
1100-7.

34

Wennberg, J.E., K. McPherson, and P. Caper. 1984. “Will Payment Based on DiagnosisRelated Groups Control Hospital Costs?” The New England Journal of Medicine 311 (5): 295300.

Wennberg, J.E. 1987. “Population Illness Rates Do Not Explain Population Hospitalization
Rates. A Comment on Mark Blumberg’s Thesis that Morbidity Adjusters are Needed to
Interpret Small Area Variations.” Medical Care 25 (4): 354-9.

Young, G.J., J.F. Burgess Jr., and D. Valley. 2002. “Competition among Hospitals for HMO
Business: Effect of Price and Nonprice Attributes.” Health Services Research 37 (5): 1267-89.

Zhang, M.B., and K.F. Yu. 1998. “What’s the Relative Risk? A Method for Correcting the
Odds Ratio in Cohort Studies of Common Outcomes. Journal of the American Medical
Association 280(19): 1690-1691.

Zwanziger, J., and G.A. Melnick. 1988. “The Effects of Hospital Competition and the Medicare
PPS Program on Hospital Cost Behavior in California.” Journal of Health Economics 7 (4):
301-20.

35

Percent of Population in HMOs

Figure 1. HMO Penetration in Three Study States
60
50
40

California
New York
Wisconsin

30
20
10
0
1980'

1985'

1990'

1995'

2000'

Year

36

Table 1: Thirty-Day Mortality for Study Conditions, By State.
AMI

HIP

CVA

GIH

CHF

DM

California
30-day Mortality
N

13.2%
6.3%
16.0%
5.7%
8.5%
3.3%
227,446 129,944 237,248 216,443 355,613 154,837

New York
30-day Mortality
N

12.5%
153,970

5.7%
15.3%
5.8%
8.0%
2.8%
75,567 134,016 121,733 262,844 128,574

Wisconsin
30-day Mortality
N

13.3%
37,693

7.2%
21,810

17.1%
34,037

6.0%
29,011

10.8%
55,475

3.3%
22,846

37

Table 2: Descriptive Data: Hospital Competition, HMO Penetration, and Other Hospital
Characteristics in Three States

Hospital Competition
90% Radius Measures
1 - Herfindahl Index
Number of Hospitals
75% Radius Measures
1- Herfindahl
Number of Hospitals
HMO Penetration
Hospital Characteristics
Hospital Ownership
Nonprofit
Public
For profit
Teaching Status
No
Minor
Major
Hospital Bed Size
<100 beds
100-199 beds
200-399 beds
400+ beds

California

Percentage or Mean (SD)
New York
Wisconsin

0.79 (0.25)
19.8 (25.6)

0.72 (0.31)
28.0 (44.4)

0.68 (0.29)
8.0 (7.7)

0.61 (0.33)
7.3 (11.1)

0.57 (0.37)
12.4 (22.7)

0.51 (0.30)
3.3 (4.0)

0.43 (0.13)

0.34 (0.14)

0.32 (0.13)

67.1%
5.9%
27.0%

84.9%
9.7%
5.4%

98.5%
1.5%
---

70.3%
22.1%
7.6%

46.6%
24.9%
28.5%

52.5%
42.2%
5.3%

25.5%
36.0%
32.1%
6.4%

11.7%
22.3%
34.3%
31.7%

32.8%
30.8%
28.5%
7.9%

38

Table 3a: Regression Results: Effects of Hospital Competition, HMO Penetration, and
Hospital Characteristics on 30-Day Mortality for Six Medical Conditions in California.
AMI
Relative
Risk
0.93

HIP
Relative
Risk
0.75***

CVA
Relative
Risk
0.71***

GIH
Relative
Risk
0.83**

CHF
Relative
Risk
0.88*

DM
Relative
Risk
0.82*

0.91

0.87

0.98

0.76**

0.78***

0.93

Hospital Ownership
Non-Profit (excluded)
Public
For Profit

1.00
1.35***
1.08**

1.00
0.98
1.05

1.00
1.06
0.92**

1.00
1.08
0.98

1.00
0.87*
0.93**

1.00
0.70***
0.85***

Teaching Status
No (excluded)
Minor
Major

1.00
0.99
0.93

1.00
1.07*
1.05

1.00
1.04
1.12**

1.00
0.99
1.19***

1.00
1.01
0.96

1.00
1.10*
1.15*

1.00
0.97
0.94*

1.00
1.01
1.02

1.00
0.98
0.92*

1.00
1.05
1.05

1.00
1.03
1.00

1.00
0.99
0.91

0.90**

0.94

0.89*

0.94

0.85***

0.72***

Explanatory Variable
Hospital Competition
(1-Herfindahl, 90% radius)
HMO Penetration

Hospital Bed Size
<100 Beds (excluded)
100-199 Beds
200-399 Beds
400+ Beds

Notes: Statistical significance is indicated as follows: *p<0.10, **p<0.05, ***p<0.01.
The regression models included patient severity measures and year indicators.

39

Table 3b: Regression Results: Effects of Hospital Competition, HMO Penetration, and
Hospital Characteristics on 30-Day Mortality for Six Medical Conditions in New York.
AMI
Relative
Risk
0.85***

HIP
Relative
Risk
0.91

CVA
Relative
Risk
0.92

GIH
Relative
Risk
0.86*

CHF
Relative
Risk
0.94

DM
Relative
Risk
0.86*

1.20

1.54***

1.33***

1.54***

1.43***

1.13

Hospital Ownership
Non-Profit (excluded)
Public
For Profit

1.00
1.03
0.98

1.00
1.00
0.61***

1.00
1.36***
0.90*

1.00
1.13
0.88

1.00
0.95
0.94

1.00
0.97
0.90

Teaching Status
No (excluded)
Minor
Major

1.00
1.03
0.98

1.00
0.98
0.91

1.00
1.03
0.89**

1.00
1.00
0.98

1.00
0.99
0.82***

1.00
1.03
0.81**

1.00
0.98
1.02

1.00
0.94
0.99

1.00
1.01
0.94

1.00
1.06
1.14

1.00
1.05
1.09

1.00
0.95
0.97

1.06

0.93

0.99

1.20*

1.12*

1.08

Explanatory Variable
Hospital Competition
(1-Herfindahl, 90% radius)
HMO Penetration

Hospital Bed Size
<100 Beds (excluded)
100-199 Beds
200-399 Beds
400+ Beds

Notes: Statistical significance is indicated as follows: *p<0.10, **p<0.05, ***p<0.01.
The regression models included patient severity measures and year indicators.

40

Table 3c: Regression Results: Effects of Hospital Competition, HMO Penetration, and
Hospital Characteristics on 30-Day Mortality for Six Medical Conditions in Wisconsin.

Explanatory Variable
Hospital Competition
(1-Herfindahl, 90% radius)
HMO Penetration
Teaching Status
No (excluded)
Minor
Major
Hospital Bed Size
<100 Beds (excluded)
100-199 Beds
200-399 Beds
400+ Beds

AMI
Relative
Risk
1.06

HIP
Relative
Risk
0.98

CVA
Relative
Risk
0.90

GIH
Relative
Risk
0.94

CHF
Relative
Risk
1.12

DM
Relative
Risk
1.02

0.78

1.03

0.95

0.78

0.66***

0.76

1.00
0.95
1.20*

1.00
0.98
1.03

1.00
1.04
1.07

1.00
0.98
0.95

1.00
1.10*
0.87*

1.00
0.95
0.55**

1.00
1.15
1.20*

1.00
0.94
0.93

1.00
0.91
0.85**

1.00
1.18*
1.15

1.00
0.90
0.83**

1.00
0.99
0.91

1.27**

0.86

0.85**

1.24*

0.84*

0.85

Notes: Statistical significance is indicated as follows: *p<0.10, **p<0.05, ***p<0.01.
The regression models included patient severity measures and year indicators. However, the
models for Wisconsin did not include hospital ownership.

41

Table 4: Regression Results: Effects of Hospital Competition Using Alternate Measures of
Competition, By State.
AMI
Relative
Risk

HIP
Relative
Risk

CVA
Relative
Risk

GIH
Relative
Risk

CHF
Relative
Risk

DM
Relative
Risk

90% Radius Measures
1-Herfindahl Index
Number of Hospitals

0.93
0.99

0.75***
0.97***

0.71***
0.96***

0.83**
0.98***

0.88*
0.96***

0.82*
0.97***

75% Radius Measures
1-Herfindahl Index
Number of Hospitals

1.02
0.99

0.87**
0.95***

0.77***
0.93***

0.94
0.97***

0.89**
0.94***

0.92
0.96***

90% Radius Measures
1-Herfindahl Index
Number of Hospitals

0.85***
0.97***

0.91
0.99*

0.92
0.97***

0.86*
0.97***

0.94
1.00

0.86*
1.00

75% Radius Measures
1-Herfindahl Index
Number of Hospitals

0.90*
0.96***

0.92
0.97***

0.94
0.97***

0.87**
0.95***

0.89**
0.99

0.83**
0.98

90% Radius Measures
1-Herfindahl Index
Number of Hospitals

1.06
1.01

0.98
1.01

0.90
0.99

0.94
0.95

1.12
0.98

1.02
0.97

75% Radius Measures
1-Herfindahl Index
Number of Hospitals

1.11
1.04

1.17
1.04

0.94
1.00

1.10
1.02

0.96
0.96*

1.01
1.00

State/
Competition Measure
California

New York

Wisconsin

Notes: Statistical significance is indicated as follows: *p<0.10, **p<0.05, ***p<0.01.
The regression models included HMO penetration, hospital ownership (except in Wisconsin),
teaching status, bed size, patient severity measures, and year indicators.

42

Table 5: Regression Results: Effects of Hospital Competition and HMO Penetration 90Day and 180-Day Mortality for Six Medical Conditions, By State.

State/Outcome/
Competition Measure

AMI
Relative
Risk

HIP
Relative
Risk

CVA
Relative
Risk

GIH
Relative
Risk

CHF
Relative
Risk

DM
Relative
Risk

0.93

0.84**

0.75***

0.89*

0.90*

0.84*

0.92

0.93

1.01

0.83**

0.81***

1.00

0.95

0.87**

0.77***

0.93

0.94

0.91

0.91

0.91

1.03

0.80***

0.85***

1.03

0.83***

0.89*

0.90*

0.82**

0.93

0.91

1.27**

1.47***

1.21**

1.36**

1.42***

1.42*

0.86***

0.89**

0.91*

0.85***

0.96

0.90

1.29***

1.43***

1.18**

1.24*

1.38***

1.39**

1.09

1.03

0.95

1.00

1.07

0.90

0.72

0.93

0.95

0.81

0.80

0.63

1.09

1.03

0.98

0.97

1.07

1.07

0.65*

0.99

0.96

0.77

0.78*

0.66

California
90-Day Mortality
Hospital Competition

(1-Herfindahl, 90% radius)

HMO Penetration

180-Day Mortality
Hospital Competition

(1-Herfindahl, 90% radius)

HMO Penetration
New York

90-Day Mortality
Hospital Competition

(1-Herfindahl, 90% radius)

HMO Penetration

180-Day Mortality
Hospital Competition

(1-Herfindahl, 90% radius)

HMO Penetration
Wisconsin

90-Day Mortality
Hospital Competition

(1-Herfindahl, 90% radius)

HMO Penetration

180-Day Mortality
Hospital Competition

(1-Herfindahl, 90% radius)

HMO Penetration

Notes: Statistical significance is indicated as follows: *p<0.10, **p<0.05, ***p<0.01.
The regression models included hospital ownership (except in Wisconsin), teaching status, bed
size, patient severity measures, and year indicators.
43

