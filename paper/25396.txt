NBER WORKING PAPER SERIES

EDUCATION FOR ALL? A NATIONWIDE AUDIT STUDY OF SCHOOL CHOICE
Peter Bergman
Isaac McFarlin Jr.
Working Paper 25396
http://www.nber.org/papers/w25396
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
December 2018, Revised January 2020
The authors are co-leaders in the production of research presented herein, and their names are
listed alphabetically. This research is supported by the University of Michigan Office of
Research, Center for the Education of Women, Center for Public Policy in Diverse Societies;
the Russell Sage Foundation and the Walton Family Foundation. We thank Joe Altonji,
David Card, Sarah Cohodes, Julie Cullen, John DiNardo, Will Dobbie, Maria Ferreya,
Caroline Hoxby, Brian Jacob, Larry Kotlikoff, Mike Lovenheim, Paco Martorell, Dick
Murnane, Jonah Rockoff, Rich Romano, David Sappington, Elizabeth Setren, Christopher
Walters, Seth Zimmerman for their detailed feedback. We also thank seminar participants at
the Columbia University, Harvard University, Princeton University, University of CaliforniaDavis, University of Chicago, University of Florida, University of Michigan and the
Tinbergen Institute as well as conference participants at APPAM, ASSA, CESifo, IZA,
NBER Labor Studies, Stanford GSE, and University of Wisconsin IRP. We thank the Bureau
of Economic and Business Research at the University of Florida for assisting with
implementation and Katie Brown for assisting with the content analysis. We also thank Maron
Alemu, Melis Balta, Magdalena Bennett, Emily Case, Frank Cousin, Maria Keller, Kelle Parsons,
Rachel Rickles, Sarah Rinehart, Susha Roy, Michael Spaeth, Katherine Wen, and Bing Zhao for
outstanding research assistance. The study was approved by University of Michigan Institutional
Review Board (HUM00080890), Teachers College, Columbia University Institutional Review
Board (15-118), and the University of Florida Institutional Review Board (IRB201702513). The
experiment is registered under ID AEARCTR-0005288. The views expressed herein are those of
the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2018 by Peter Bergman and Isaac McFarlin Jr.. All rights reserved. Short sections of text, not
to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.

Education for All? A Nationwide Audit Study of School Choice
Peter Bergman and Isaac McFarlin Jr.
NBER Working Paper No. 25396
December 2018, Revised January 2020
JEL No. I20,I21,I24,I28
ABSTRACT
School choice may allow schools to impede access to students perceived as costlier to educate.
To test this, we sent emails from fictitious parents to 6,452 charter schools and traditional public
schools subject to school choice in 29 states and Washington, D.C. The fictitious parent asked
whether any student is eligible to apply to the school and how to apply. Each email signaled a
randomly assigned attribute of the child. We find that schools are less likely to respond to
inquiries from students with poor behavior, low achievement, or a significant special need. Lower
response rates to students with this special need are driven by charter schools. Otherwise, these
results hold for traditional public schools, high value-added schools, including high-value added,
urban charter schools.

Peter Bergman
Columbia University
525 W. 120th Street
Box 174
New York, NY 10027
bergman@tc.columbia.edu
Isaac McFarlin Jr.
College of Education
University of Florida
2-230D Norman Hall
P.O. Box 117049
Gainesville, FL 32611
and NBER
imcfar@ufl.edu

1. Introduction
In the last 15 years, researchers have leveraged the randomization inherent in many schoolchoice admissions policies to demonstrate that certain schools have large impacts on test scores,
college enrollment, risky health behaviors, and criminality (Hoxby and Rockoff, 2004; Deming,
2011; Angrist et al., 2013; Wong et al., 2014; Dobbie and Fryer, 2015). These studies, which have
strong internal validity, examine the impacts on those who apply to a given set of schools. These
estimates may change as schools expand, especially if they have been strategically altering the
pool of applicants along hard-to-observe characteristics. We conduct a nationwide audit study to
test whether schools attempt to manipulate their applicant pool by providing less application
information to the parents of children perceived as more difficult or costlier to educate.
The concern that school choice enables schools to impede access for certain students is salient
to policymakers (Cohodes and Dynarski, 2016).1 To minimize this practice, regulators use lotteries
and common applications to control admissions. While the educational costs of students with
severe disabilities can exceed 10 times that of other students, the government provides financial
offsets and requires that all districts serve these students—and procure their required services
(Griffith, 2008). But frictions in the choice process may still allow schools to influence who applies.
Many families lack information about schools’ eligibility requirements, quality, and admissions
process (DeArmond et al., 2014; Hastings and Weinstein, 2008; Kapor et al., 2019; Bergman et
al., 2019). Qualitative research has found instances of schools taking advantage of these frictions
to hinder access for certain groups of students (Drame, 2011; Orfield et al., 2013; Welner, 2013).
This behavior is difficult to detect. Differences in how families choose schools may reflect
heterogeneous preferences rather than steering away applicants. Observational studies have
focused on specific contexts using administrative data, which has limited information on student

There is also controversy in popular press on access to choice schools. See “Are Charter Schools Cherry-Picking Students” in the New
York Times (Dec. 10, 2014), which features a debate by policymakers on charter school access. In an article in the Washington Post,
“The Masquerade of School Choice: A Parent’s Story” (April 1, 2017), a parent describes her experience with racial discrimination and
school choice.
1

1

behaviors and needs (Lacireno-Paquet et al., 2002; Bifulco et al., 2009; Zimmer et al., 2009; Hoxby
and Murarka, 2009; Zimmer and Guarino, 2013; Nichols-Barrer et al., 2015; Walters, 2018).2 The
findings are mixed.
We conduct a nationwide audit study to detect whether schools selectively provide
application information to families.3 Across two experiments, we sent emails from fictitious
parents to 6,452 charter and traditional public schools subject to school choice. The parent asked
whether any student is eligible to apply to the school and how to apply.4 Each email signaled one
of the following randomly-assigned attributes about the student: their disability status, poor
behavior, high or low prior academic achievement, or no indication of these characteristics. We
also randomly varied students’ implied race, household structure, and gender. In the first
experiment, we sent messages only to charter schools. Based on constructive feedback from
researchers, we ran a second experiment, where we sent messages to both charter schools and
traditional public schools to replicate our previous findings and to compare response rates between
these two types of schools.
Our results were the same across both experiments: We find that schools respond less often
to messages regarding students whom schools may perceive as more challenging to educate. The
baseline response rate was 53 percent. Messages signaling that a student has a potentially severe
special need were 5 percentage points less likely to receive a response than the baseline message.
Messages signaling a behavior problem and messages indicating low prior achievement were 7 and

Several studies also look at voucher systems (Epple et al., 2017). Altonji et al. (2015) find little evidence that vouchers negatively impact
students who remain in traditional public schools. Outside the United States, Hsieh and Urquiola (2006) show that the voucher system in
Chile caused high-achieving and high-income students to move to private schools. Muralidharan and Sundararaman (2015) use a twostage experiment in India to estimate whether the introduction of private-school vouchers negatively impacts non-voucher recipients;
they find no evidence of spillovers on student outcomes.

2

Prior studies using this design include Bertrand and Mullainathan (2004) and Oreopoulos (2011) to study racial and ethnic discrimination
in labor markets. Ayres and Siegelman (1995) investigate racial and gender discrimination in bargaining for a new car. In education
settings, Darolia et al. (2015) and Deming et al. (2016) examine the value of a credential from a for-profit postsecondary institution while
Baker et al. (2018) study bias in online learning environments. Giulietti et al. (2019) examine racial discrimination in the provision of
public services in the United States. Investigating the sharing economy, Edelman et al. (2017) find evidence of racial bias in the online
market for housing rentals.

3

4

Understanding eligibility requirements is the most commonly-cited barrier to selecting a school of choice (DeArmond et al, 2014).

2

2 percentage points less likely to receive a response, respectively. A message indicating good grades
and attendance, however, was neither more nor less likely to receive a response than the baseline
message.
A key question is whether these results differ between traditional publics subject to school
choice and charter schools. Charter schools represent the fastest growing form of school choice in
the country.5 To answer this question, we matched charter schools to nearby traditional public
schools subject to school choice. We find that, overall, traditional public schools’ response rates
are similar to the response rates from charter schools across treatment messages. However, there
is a different response rate to messages that signal a child has a significant special need.
Traditional public schools exhibit no differential response rate to these messages, but charter
schools are 7 percentage points less likely to respond to them than to the baseline message. This
result is important because the differential cost of serving students with severe special needs cited
above (Moore et al., 1988; Chambers, 1998; Collins and Zirkel, 1992, Griffith, 2008).6
These results hold for high-value-added schools, including urban, high-value added charter
schools. Prior research has also shown that strict (“no-excuses”) charter schools also tend to have
high value added (Abdulkadiroglu et al., 2011; Angrist et al., 2013, 2016; Chabrier et al., 2016;
Clark et al., 2015; Dobbie and Fryer, 2013; Dobbie and Fryer, 2015; Hoxby and Murarka, 2009).
We identified 272 such schools in our data; these schools have a value-added one-half standard
deviation above other charter schools. No-excuses charter schools are significantly less likely (10
percentage points) to respond to inquiries that signal a child’s potentially significant disability
than to the baseline message.
We also present evidence that the monetary cost of serving students matters. States fund
students with special needs in several different ways, including block grants designated for special
education, cost-reimbursements for services rendered, and formulae that provide additional

See the National Alliance for Public Charter Schools’ report.
Quality of response may matter as well. In a later section, we provide corroborating evidence that the messages traditional public
schools send to parents of special needs students are viewed as more encouraging.

5
6

3

general funds to schools (Griffith, 2008; Millard and Aragon, 2015).7 We find that charter schools
in states, such as Wisconsin and Michigan, that reimburse districts for a large share of the realized
cost of serving special-needs students exhibit no differential response rate to messages signaling a
potentially high-cost special need.
Allowing charter schools to integrate with another Local Education Agency (LEA) could
spread the risk of serving higher-cost students across multiple schools.8 The legal obligation to
provide services to students with special needs falls on the LEA. Integration between a charter
school and a traditional-public school LEA could enable the schools to pool resources. We coded
each charter school’s LEA status based on states’ LEA policies.9 We find that LEA status does
not moderate the differential response rate to messages signaling a disability.
We find exploratory evidence of differences in response rates by the implied race of the family,
but not by household structure. Schools may interpret these attributes as signals of families’ socioeconomic status. Overall, schools are 2 percentage points less likely to respond to emails signed
by Hispanic-sounding names than to other messages. There is weaker evidence of a differential
response rate for messages signed by Black-sounding names.
This paper makes several primary contributions to the literature on school choice. We provide
the first experimental evidence testing whether schools of choice provide less application
information to students whom schools may perceive as harder to educate. Second, we incorporate
signals of several student attributes—beyond race and gender—across a wide variety and a large
number of schools. These features increase the external validity of our study and allow us to
investigate an important dimension of heterogeneity comparing traditional public schools to
charter schools.

Per-student education costs have steadily risen over time. Average expenditure per pupil for the 1990-1991 school year was $9,936. In
2016 dollars, this increased to $13,119 in the 2014-2015 school year (NCES Table 236.69). About 20 percent of the growth in new
education spending is directed to special education services (Hanushek and Rivkin, 1997).

7

8

An LEA is equivalent to a school district in most instances.

9

LEA status varies within states because it can depend on what entity authorized the charter school (e.g. the state or a district).

4

Our results also have implications for interpreting studies that use evidence from schools’
admission lotteries to examine school effectiveness. Our findings do not undermine the internal
validity of lottery-based studies, but they underscore that lottery-based studies are conditional
on the set of students who apply.10 Hastings et al. (2006), Walters (2014) and Kline and Walters
(2016) find that students who may benefit the most in terms of academic achievement are also
the least likely to apply to high-performing schools or education programs. However, certain
students—even those who may benefit most—may also impose high costs or negative behavioral
spillovers (cf. Carrell and Hoekstra, 2010; Carrell et al., 2018) that reduce schools’ demand to
serve them. If the hardest-to-educate students were evenly distributed across schools, the impacts
of highly-effective schools could decrease due to negative behavioral or fiscal spillovers.
Lastly, our research highlights the importance of providing transparent information to
families to ensure all students have equal access in the choice process.11 DeArmond et al. (2014)
survey 4,000 parents across eight cities and find that the most common barriers in choosing a
school is understanding eligibility requirements, followed by transportation issues and obtaining
accurate information about school characteristics. Hastings and Weinstein (2008) show that
providing schools’ test score information to families increases their chances of selecting a higherscoring school, which increases achievement. Corcoran et al. (2018) also show that providing
information about nearby high schools’ selectiveness and graduation rates improved the quality
of middle-school students’ high-school choices, particularly for non-English speaking households.
Kapor et al. (2019) find that low-income families can misunderstand the school selection process
and are less likely to be placed into their preferred school. Impeding access to information about
how to apply could reduce opportunities for disadvantaged students even when there is, ostensibly,
equal access (“open enrollment”). More broadly, we document the use of one mechanism to reduce

Fryer (2014) finds that the practices of high-performing charter schools, when installed in traditional public schools, produce comparable
gains in achievement. Abdulkadiroğlu et al. (2016) find those with lower application costs to charter schools experience similar
achievement gains to other students.

10

For examples in the U.S. context, see Hoxby and Turner (2013) and Bergman (2015) in post-secondary and secondary education
contexts.

11

5

access, but schools may use a number of ordeal mechanisms to screen applicants and the scope
for choice, despite the intentions of policymakers.12
There is a normative question about what optimal policy should require of choice schools
with respect to whom they enroll. Our paper does not aim to identify an optimal policy, which
depends on a particular social welfare function. Instead, our results inform how a socially optimal
policy might be achieved. For instance, if a particular social welfare function implies that all
schools should have the capacity to serve any student, bolstering existing efforts to reimburse
schools for realized costs may ensure this opportunity for a variety of students. Other policies
could unify and simplify application processes to help families make informed decisions. Several
education agencies have undertaken audits like ours to monitor whether schools are providing
information equitably across families (Prothero, 2014).
The rest of our paper proceeds as follows. Section 2 provides background on school choice for
our sample. Sections 3 and 4 describe our intervention, experimental design and empirical analysis.
Section 5 presents our results and Section 6 concludes.

2. Background Information on School Choice
Our sample, which we describe in detail in the following section, covers traditional public
schools in districts with various forms of school choice and charter schools. Charter schools are
public, open-enrollment schools that have greater autonomy over their finances, staffing decisions,
and curricula than traditional public schools, but they must admit students by lottery if more
students apply to the school than can be accommodated.13 Charter schools are the fastest growing
form of school choice in the United States. Since 2010, more than 2,000 new charter schools have
opened (NCES, 2015). They enroll nearly 3 million students at nearly 7,000 schools in 43 states
and the District of Columbia. While their performance overall tends to be no better or worse than

Welner (2013) documents that schools use a variety of mechanisms, such as releasing applications only for short periods of time early
in the school year, demanding the presentation of social security cards and birth certificates, and documentation of disabilities.
12

13

Specifically, lottery admission is required if they receive federal funding.

6

traditional public schools, charter schools in urban areas, which are often, no-excuses charter
schools, have been shown to have large, positive impacts on student achievement (Abdulkadiroglu
et al., 2011; Angrist et al., 2013, 2016; Cohodes, 2018; Chabrier et al., 2016; Clark et al., 2015;
Dobbie and Fryer, 2013; Dobbie and Fryer, 2015; Hoxby and Murarka, 2009).
Among the traditional public schools in our sample, school choice operates in several ways.
The rules are governed by both state and local laws.14 For example, some states like Arizona
make school choice mandatory for all school districts in the state. In states where it is voluntary,
it is often practiced mostly by large urban school districts. At the local level, school districts
commonly establish attendance areas, and students’ default to neighborhood-based assigned
schools but allow applications to other schools within the district, subject to capacity constraints.
Other school districts have no neighborhood assignment and require that families apply to schools
as a requirement for enrollment. Schools may also offer priority enrollment to applicants based
on residence, having a sibling within a school, or a safety concern. Among schools with more
applicants than slots, schools may assign students based on a first-come, first-serve basis or a
lottery.
Both traditional public schools and charter schools must comply with a number of antidiscrimination laws, several of which have overlapping protections. The Civil Rights Act of 1964
prohibits discrimination on the basis of race, religion, and country of origin.15 The Rehabilitation
Act of 1973, Individuals with Disabilities Education Act (IDEA) and Americans with Disabilities
Act of 1990 require LEAs to provide all necessary services to students with physical or mental
disabilities. IDEA mandates the creation of an Individualized Education Plan (IEP) for students
with disabilities. The IEP, formulated with parents, school and other professional representatives,
dictates what services the LEA is legally obligated to provide a student to address their special

National Center for Education Statistics Table 4.2 outlines states with mandatory open-enrollment policies for their traditional public
schools and briefly describes the nature of each policy. For voluntary open-enrollment policies by state, we discovered inaccuracies within
Table 4.2 and do not rely upon this particular information. For example, it is reported that the state of New York does not have any
voluntary admittance of students from other schools when in fact New York City public schools have one of the largest open-enrollment
programs in the country.
14

15

The Civil Rights Act of 1964 further prohibits discrimination based on gender and sex, except for same-sex schools.

7

needs.16
This last requirement may have greater consequences for charter schools that are authorized
as their own LEA than for those authorized as part of an existing LEA or district (Heubert, 1997).
The latter arrangement implies that charter schools may be able to draw on resources from the
broader school district to help serve special-education students, while the former implies charter
schools may have to address this requirement entirely on their own.17 For this reason, charter
schools that serve as their own LEA may respond to different incentives during the application
process than those that are not their own LEA.
Traditional public and charter school funding comes from federal, state, and local
governments. The degree of funding parity between charter schools and traditional public schools
within the same state varies across states.18 Supplementary funds for students with disabilities
can also vary based on state and local policies. Special education funds overwhelmingly (90%)
come from state and local sources (Cullen and Rivkin, 2003; Rhim et al., 2015). As a point of
reference, the average cost of educating a child with special needs is roughly 2.3 to 2.5 times that
of a child without special needs (Moore et al., 1988; Chambers, 1998; Collins and Zirkel, 1992).
A point of controversy is whether charter schools serve the most disadvantaged or costliestto-educate students at similar rates to traditional public schools. Nationally, the Government
Accountability Office found that charter schools enroll a smaller proportion of students classified with severe
disabilities than traditional public schools (US Government Accountability Office, 2012). But evidence

We summarize the above requirements as background information, but whether or not our findings constitute legal or illegal behavior on the
part of a school is not germane to our first-order research question, which is to determine experimentally whether schools practice any form of
differential treatment during the application process with respect to specific student characteristics.

16

Akin to other social insurance programs such as Medicare and Food Stamps, the economic justification for special education services
is multi-tiered. First, it provides a form of insurance to protect families who have children that are expensive to educate due to a
disability; second, federal and state funding works as a form of insurance to protect local schools from the high cost of absorbing a
disproportionate number of disabled students (Cullen and Rivkin, 2003). IDEA also permits the allocation of funds for a statewide “risk
pool” to help LEAs serve students with high-cost disabilities (Rhim et al., 2015). States may also designate charter schools to be part of
a larger LEA specifically for IDEA purposes. There is some evidence that individual schools existing as their own LEA may form consortia
to pool resources, making it easier to establish economies of scale and provide appropriate services for all students (NCESCS, 2017).
18
For example, the typical Oregon charter school receives only about 60 percent of the level of funding that a typical traditional public
school receives while both charter and traditional public schools in Tennessee receive similar levels of funding (Batdorff et al., 2014; Epple
et al., 2016).
17

8

from specific locations is more nuanced. Setren (2015) finds that Boston-area charter schools
classify fewer students as special needs (irrespective of whether they have a disability).19 Hoxby
and Murarka (2009) find that New York City charter schools enroll more low-income and minority
students than traditional public schools as a percentage of total enrollment. Using data from
California and Texas, Booker et al. (2005) show that students who enroll in charter schools tend
to have lower achievement than the students in the traditional public schools they left.

3. Experimental Design and Data
Messages
The field experiment consisted of email messages sent to charter schools and traditional
public schools subject to school choice. We framed each message as coming from a parent looking
for a school. The parent contacts the school to ask about their child’s eligibility and how to apply.
We developed our messages in consultation with charter school and traditional-public school
administrators who have received application inquiries via email. Our conversations with
administrators at charter schools and traditional public schools found that parents do make
eligibility inquiries and provided examples.20 The baseline message indicated that the parent is
looking for a school for their son or daughter and they would like to know whether anyone can
apply to that school and how to apply. Each treatment message added a sentence to this baseline
message to signal a child’s potential cost to educate, disadvantage, or prior academic performance.
This sentence indicated the child has one of the following: an IEP requiring they be taught in a
classroom separate from mainstream students; poor behavior; bad grades; or good attendance and
good grades. We show examples of the exact wording of these treatments in Figure 1 and Figure
A1.

In part, this could be because the traditional public-school students were enrolled in prior to enrolling the charter school did not readily
transfer their IEPs.
19

For instance, parents frequently asked whether the school admits students with special needs or low grades. Administrators reported
other questions that parents ask as well, including whether parents have to volunteer, pay any fees, purchase school books or provide
particular documentation about their child, whether there is a lottery or preferences for neighborhood families, and lastly, whether a
charter school diploma is valid for college or university admission.

20

9

We chose these messages based on existing concerns about how schools may screen potential
applicants. For instance, there is a national discrepancy in enrollment rates of students with
severe disabilities between charter schools and traditional public schools. Students with severe
disabilities may require significant support services, which may require teaching in a separate classroom
from mainstream students. Note that this does not imply one-on-one instruction; instead, this is
often instruction by a certified Special Education teacher in a separate or “self-contained” classroom
with fewer students (e.g. a common set up is 12 students to one teacher with special-education
certification, plus any other “para-professionals” that may be specified in a student’s IEP to
provide further services). The poor-behavior message ties to a contention that some schools push
out or screen children with behavior issues (Zimmer and Guarino, 2013). The poor-grades message
and the good grades and good attendance message reflects concerns that schools may seek out
students or screen students based on their academic performance (Winters, Clayton, and
Carpenter, 2017).
Lastly, demographic characteristics of the parent and student were varied at random across all
messages. We randomized a signal of household structure by indicating that the parent and their
spouse were making the inquiry (e.g. “My husband and I…”) or that just one parent was making
the inquiry. Following Bertrand and Mullainathan (2004), the name of the parent signaled the gender
of the parent and whether they are Hispanic, Black, or white. 21 The choice to randomly vary demographic
signals also reflects concerns about how schools may screen potential applicants. Minority background
may signal socio-economic disadvantage and a child’s gender may signal disruptive behavior, as male
students tend to be more disruptive in class than female students (Bertrand and Pan, 2013).

Experimental Design and Sample Frames
We conducted the first experiment in late 2014, in which we sent messages only to charter
schools. Based on feedback we received in this first experiment, we replicated and extended the

We chose names based on New York state data that associated names with race and gender. We then chose names that were
overwhelmingly associated with one particular race and gender combination.

21

10

experiment in early 2018 with the goal of comparing the response rates of traditional public
schools in areas with school choice to the response rates of nearby charter schools. The three-year
gap between the two experiments provides a check on the consistency of the results across time.
We constructed the sample for each experiment using the Common Core Data from the
National Center for Education Statistics, which has information on the universe of both charter
and traditional public schools. In the first experiment, we chose the 17 states with the largest
number of charter schools. These 3,131 schools were roughly half of the charter schools in the
country at the time.
The initial sample for the second experiment was the 29 states (and the District of Columbia)
with the largest number of charter schools and some form of intra-district choice. We list these
states in Appendix Table A9. For every charter school in these states, we matched it to the
nearest traditional public school with the same entry-grade level and within the same district
boundaries. We then further restricted this sample to school districts in the top 40 in terms of
enrollment because of the fixed costs of data collection.22 The final sample consisted of 4,338
schools (1,016 of which were charter schools that were also in the first experiment) or 2,169
matched-pairs of traditional public and charter schools. Figure A2 shows states with charter
schools colored in white, states in our sample colored in dark blue, and states with no charter
schools colored in gray. The sample has broad geographic coverage across the United States.
Appendix B discusses our sample construction in further detail.
In each experiment, we sent two emails to each school three-to-four weeks apart. The
treatment messages were randomly assigned at the school level in the first experiment. For the
second experiment, treatment messages were clustered at the pair level; identical messages from
the same fictitious parent were sent to each charter school and its match-paired traditional public
school. Within each experiment, no school received the same message treatment twice and a
school was assigned a treatment without replacement. We also randomized the order in which

22

Specifically, we selected the largest districts because of the fixed costs of investigating district-specific school choice policies.

11

schools were contacted.
Table 1 shows the results of regressing school characteristics on an indicator for each
treatment. Each column restricts the sample to the baseline messages and the treatment message
indicated in the column header. The results and joint test of covariates within each column show
that randomization generated assignments uncorrelated with school characteristics.
Figure 1 shows the number of emails sent per treatment. More baseline and IEP messages
were sent than behavior- and grades-related messages so that we would have additional precision
with respect to those two treatments.23 Across the two experiments, we sent the same baseline,
IEP, poor behavior and low-grades messages.24 The second experiment added the good-grades and
attendance message to this list of treatments as well as the two-parent household signal.

Data
Data come from information on school websites, national databases of school demographic
information, the census, a school-rating non-profit organization, and the responses to our emails.
We hired research assistants to find and visit the website of every school in our sample frame.
We then coded several variables including whether the school has a website, and, if so, whether
the website includes a link to the school’s contact information on its landing page, a webform to
contact the school, or a requirement to add a phone number via the webform. We also used
information on the school websites (and schools’ handbooks on these websites) to identify the
“no-excuses” charter schools in the sample. We based this determination on a template of
characteristics common to such schools.

In our funding proposal, we pre-specified our primary treatments as the IEP, poor behavior, and poor grades in the first experiment,
and these plus the good grades and attendance treatment in the second experiment. A key difference specified in the second experiment
was to test for differences in response rates between the traditional public schools and charter schools. Sending one baseline message to
every school would have drastically reduced our power to detect treatment effects for each of these message types relative to the baseline
message. In our first experiment, we wanted added precision for the IEP message, so we sent out relatively more IEP messages. Our
primary specification, compares response rates to each treatment message to the response rate of the baseline message. A power analysis
showed that power in the second experiment was maximized by sending roughly twice as many baseline messages as treatment messages.
See Figure 1 for the exact message counts per treatment in each experiment.
24
Each message had randomly assigned wording variations as well. For instance, we randomly varied the subject line, the sign off
(“thanks” or “thank you” or just signing the name) and the greeting.
23

12

We supplemented these data with information on schools from two national databases: the
Common Core of Data (CCD) and the Civil Rights Data Collection (CRDC). We used the CCD
data to compute enrollment size, the share of students who are Black, Hispanic or white, and the
share of students who receive free or reduced-price lunch at each school. These data also recorded
the latitude and longitude of a school and whether it was located in an urban, suburban or rural
area. We used the location data to link each school to census tract information on the share of
individuals by race, education, income and disability in a tract. From the CRDC data, we used
the number of students with a disability for each school, which the CRDC breaks down by the
portion of the day students are not in a restricted environment (less than 40%, between 40% and
79%, and 80% or more). We translated these numbers into shares of enrollment. We also used
the CRDC to calculate the share of students who are chronically absent (missing 10% of days or
more), suspended, and have limited English proficiency.
Third, we used data provided by a nonprofit organization, GreatSchools, which collects
proficiency rates based on test scores for traditional public schools and charter schools across the
country. We average the proficiency rates across subjects (e.g. math and reading) and grade levels
for each school. We use this measure to estimate each school’s value added by measuring the
difference between its observed average proficiency rate and the rate predicted based on the
covariates specified above, state fixed effects, and an indicator for charter school or not. We then
standardize this measure of value added according to the mean and standard deviation within the
sample.
Fourth, we coded the responses of schools to our emails. We created an indicator for whether
or not a school responds to a message, which is our outcome variable. Some schools provide
automated responses (3% of emails receive an automated response) to our messages. Since each
treatment is as likely to receive an automated response as another, this practice only raises overall
response rates and our results are robust to excluding these messages (available upon request).
Finally, a response does not necessarily indicate encouragement to apply. Schools may
13

attempt to persuade a parent to either apply or not to apply, or it may contain other information
that differs by message content. Thus, a reply is not necessarily encouragement to apply. We
worked with a qualitative researcher to develop a codebook and to train a team of research
assistants to review each message and manually code message responses as either encouraging or
not encouraging. Appendix C discusses in further detail how we created this measure.
In Appendix Table 1, we show the characteristics of our sample schools. On average, sample
schools serve primarily students from low-income families; more than half of the students served
in these schools receive free or reduced-priced lunch. Demographically, there are relatively similar
shares of white, Black and Hispanic students across all sample schools. Over half the sample is
located in an urban area. We also show that school-level correlates of disadvantage predict lower
response rates, and that high-performing schools are less likely to respond overall (Table A2).

4. Empirical Strategy
Our main outcome is a binary variable for whether a school responds to our inquiry or not.
We estimate a linear-probability model as follows:
Ri = α + Treatmenti β + Demographicsi γ + Xi δ + Wavei θ + εi
In this regression, Ri is our binary outcome. Treatmenti is a vector of treatment indicators
for the mutually-exclusive treatments: the IEP treatment, the behavior treatment, the low-grades
treatment, and the good-grades and good-attendance treatment. Demographicsi is a vector of
indicators for the randomized implied race, gender of the child and parent, and whether the
message indicates a two-parent household. Xi is a vector of school covariates, including school
demographics and whether or not the school is a traditional public school. Any missing covariates
are imputed and an indicator for missing data is included in the regression. All regressions include
indicators for the wave of emails.25 Standard errors are clustered at the school or school-by-pair

25

There are four waves of emails: two in the first experiment and two in the second experiment.

14

level.26
Outcomes include binary indicators for whether or not a school of choice responded to the
email as well as indicators for the encouragement of the response. To ameliorate concerns about
selection into response, all outcomes are unconditional on whether or not a school responds. For
instance, if the outcome is an indicator for whether or not a response is “encouraging,” then a one
indicates that the school responded and wrote a message using an encouraging tone while a zero
indicates either that the school’s response was discouraging or that the school did not respond at
all.
To test whether traditional public schools respond at different rates to each type of message
than charter schools, we interact each treatment with an indicator for whether or not the school
is a traditional public school. The significance of this interaction effect tests whether the
traditional public school responded more or less frequently than the nearby charter school. To
conduct other heterogeneity analyses, we either restrict the sample according to a certain variable
or fully interact it with our treatment messages as specified in the text.
Our results replicate similarly across experiments, which may be unsurprising given that the
treatment messages were the same (except for the addition of new treatments) and the policy
environment was similar. For ease of presentation, we combine the two experiments in our analysis
of the results. However , the appendix Table A3 shows results from each experiment separately.
Lastly, we show our results are robust to different specifications and multiple-testing
corrections. We show whether controls for school characteristics from the CCD and CRDC, census
tract characteristics, and pair fixed effects affect our results. Given random assignment, none of
these additional covariates or fixed effects is required for identification. In the appendix, we adjust
our main results—the effects of each treatment message and the differential effect between charter

Specifically, clustering is at the school level for schools that are only in the first experiment, clustering is at the pair level for schools
only in the second experiment, and clustering is at the school and pair level for schools that are in both the first and second experiment.
In practice, the level of clustering does not alter the standard errors much and therefore the significance of the results; other versions of
clustering are available upon request.

26

15

schools and traditional public schools—for multiple-hypothesis testing (Table A5). We use Holm’s
step-down procedure for groups of treatment effects specified in the text (Holm, 1979). This
procedure controls for the family-wise error rate, which is the probability of at least one false
rejection of the null hypothesis.

5. Results
Table 2 presents the effects of our primary treatments—messages indicating poor behavior,
the IEP, poor grades, and good grades and attendance—on response rates. The control mean at
the bottom of Column (1) shows that the baseline message received a response 53% of the time.
We find that schools are less likely to reply to messages signaling that the potential applicant
has had bad grades, poor behavior, and an IEP (Column (1)) than the baseline message. The badgrades treatment reduces response rates by 2.4 percentage points. The IEP message and poor
behavior message reduce response rates by 5.2 percentage points and

7.0

percentage

points,

respectively. The signal of good grades and attendance has no discernable impact on response
rates (the coefficient is 0.0 percentage points). One caveat of the latter result is that parents’
describing their child’s good grades may be considered “cheap talk” by the schools, and not taken
seriously.
In interpreting these results, note that the IEP message signals a student who requires a
restrictive environment. If the signal were for less restrictive or less costly services, the results
may have differed. And while schools do not actively provide more information to higherperforming students, this signal could be viewed by schools as “cheap talk.” Messages signaling
these positive traits may be perceived as less truthful than the messages signaling other student
traits.
Column (1) of Table 2 also shows the results of our other treatments signaling race, gender
and household structure. Only the message that indicates a Hispanic-sounding name results in

16

significantly lower—by 2.0 percentage points—response rates.27 The coefficient on Black-sounding
names is not significant. An F-test for whether we can reject the null hypothesis that these
treatments of demographic characteristics are jointly equal to zero cannot be rejected (p=0.14 for
the specification in column 1). One point of consideration for the Hispanic-sounding names is that
the emails were sent in English; the results could be a lower bound if schools are concerned about
a student’s English-language skills.
In the appendix (Table A4), we show that the results above are extremely similar across
different specifications. The coefficients do not vary significantly with the addition of school-level
covariates, census tract-level covariates, or pair fixed effects.
Heterogeneity across Traditional Public and Charter Schools
Given the growth rate of charter schools and concerns over whether they encourage all
students to apply, we test whether our findings differ between charter schools and traditional
public schools. Columns (2) and (3) of Table 2 show results separately for traditional public
schools and charter schools, respectively. Column (4) shows the difference in response rates
between traditional public schools and charter schools and its statistical significance.
Traditional public schools in areas of school choice generally respond at similar rates as
charter schools for each treatment, with one exception. Traditional public schools are
significantly—5.8 percentage points—more likely to respond to the IEP message than charter
schools. This result remains statistically significant even after adjusting for multiple testing of
these interaction terms (p=0.05, Table A5). A test of whether the interaction effects for the
primary treatments are jointly different from zero is also statistically significant (p=0.03).
Treatment effects for signals of demographic characteristics do not differ significantly between
traditional public schools and charter schools.

27

These results are larger in magnitude for predominantly-white schools (results available upon request), though we do not know how

common it is for these schools to receive inquiries from Black or Hispanic families.

17

Although we observe that traditional public schools respond at higher rates than charters for
children with IEPs, it could be that traditional public schools are systematically less encouraging
in their responses, which might discourage parents from applying. Based on qualitative data
collected by trained coders, Table 3 reports findings on whether school responses are encouraging,
with no response coded as a zero and an encouraging response coded as a one. (See Appendix C
for a description of our qualitative data and reports on inter-rater reliability.) Responses to the
IEP message are 4.2 percentage points more likely to be encouraging from traditional public
schools than from charter schools. However, the evidence suggests otherwise for the bad grades
and poor behavior treatments, where traditional publics provide significantly fewer encouraging
responses.28
Exploratory Analyses
Heterogeneity Across School Performance Levels and “No-Excuses” Charter Schools
Access to high-performing schools is important if school choice is to reduce gaps in
achievement across groups of students with different histories and attributes. We explore
heterogeneity in Table 4. Columns (1) and (2) show treatment effects for all high and all lowvalue-added schools, which we define as above- or below-average value added in the sample. The
IEP, behavior, and grades treatment effects are similar for high- and low-value-added schools.
Differences arise with the race implied by the messages, however: high value-added schools are
less likely to respond to messages from families with Black or Hispanic-sounding names than low
value-added schools. Results for messages from parents with Hispanic-sounding names remain
significant at the 1% level after adjusting for multiple testing (p =0.01 and p =0.15 for Hispanicsounding name and Black-sounding name, respectively). This result is driven by the high-value

28
Table A8 also presents results on the likelihood that a typical person may apply to the school. Column (4) shows differences in impacts
between traditional public schools and charter schools on the likelihood of applying. The findings are broadly supportive of earlier results:
the typical person—or coder—is similarly likely to apply to traditional public and charter schools across the primary treatments, except
for the IEP treatment. Specifically, we find that the typical person is 4.5 percentage points more likely to apply to a traditional public
versus a charter school when sent the IEP message. Given that coders are in fact non-applicant parents, we generally view this evidence
as suggestive.

18

added charter schools in the sample and by schools serving low shares of minority students (results
available upon request). All of these findings are exploratory, however.
Prior research has shown that urban, high-value added charter schools can significantly close
racial gaps in student achievement (Abdulkadiroglu et al., 2011). Column (3) shows the treatment
effects from the primary treatment messages do not qualitatively differ for this subgroup. Many
“No-excuses” charter schools have demonstrated particular success in closing racial-achievement
gaps in school districts. We identified a list of 272 no-excuses schools in our sample. These schools
tend to have much higher test scores, value added, and serve larger shares of minority and lowincome students relative to other charter schools. These schools also, however, serve fewer
students with IEPs and have higher rates of suspensions than other charter schools (see Table
A1). While the sample of “no-excuses” charter schools is small, the coefficient on the IEP
treatment remains large (10 percentage points), negative, and significant. Again, this analysis is
just exploratory.
Moderating Factors: Funding Strategies
States typically provide funding for special education students in three ways. Most common
is formula-based funding. The formula may be a funding multiplier based on student or staff
counts. Schools receive additional funds, but these are not necessarily ear-marked for specialeducation services. Categorical funds or block grants similarly provide funds for schools, but these
are ear-marked for special-education services. Finally, states may provide partial or full
reimbursement to districts for their realized special education expenditures. We generate a
variable categorizing states by how they provide funding and we interact this variable with our
IEP treatment indicator. We restrict the sample to charter schools, since traditional public schools
are no less likely to respond to students with an IEP than to the baseline message.29
Table 5 shows heterogeneous effects by funding strategy. Charter schools in states that

Traditional public schools exhibit a similar pattern of heterogeneity with respect to funding as charter schools, but the estimates are
much less precise.

29

19

reimburse schools for (at least a portion of) their realized expenditures are 7 percentage points
more likely to respond to the IEP treatment message than charter schools in other states. This
result is statistically significant at conventional levels (p=0.05). Charter schools in states with
categorical funding have slightly higher response rates as well, though the difference is not
statistically significant. In states with formula-based funding, there are several weighting schemes;
we do not find any significant heterogeneity with respect to these different schemes (results
available upon request).
This suggestive pattern of results is consistent with an explanation that the costs of educating
certain students are imperfectly compensated in most contexts. The latter could create an
incentive for schools to provide less application encouragement to students with special needs.
One caveat to a cost-reimbursement policy is that it can create incentives to classify students as
requiring an IEP and associated services, irrespective of the student’s actual needs.
Diversifying Risk: Local Education Agency (LEA) Status
Table 5 also shows heterogeneity by LEA status. LEAs, rather than schools, bear the legal
obligation to provide the services specified in students’ IEPs. Whether or not a charter school is
its own LEA varies across and within states. Charter schools that are their own LEA may have
difficulty pooling resources and risk across multiple schools. We assess whether the opportunity
to pool resources in this way is associated with differential response rates to the IEP message. We
restrict the sample to charter schools and interact an indicator for LEA status with our IEP
treatment. While own-LEA charter schools are slightly less likely to respond to the IEP treatment,
this result is not statistically significant.
Household Structure and Demographics
Lastly, we explore whether signals of household structure exacerbate or attenuate differential
response rates across treatments. We randomized whether a message indicated that a husband
and wife are or just one parent is looking for a school and. We also randomized the race and
gender of the child. Results restricting the sample to each of these groups are shown in Table A7.
20

Messages signaling a two-parent household, which could signal socio-economic advantage, tend to
increase the likelihood of response to children with poor behaviors and decrease the likelihood of
response regarding children with good grades. Messages signaling a two-parent household also
significantly increase the likelihood of response for Black families. Messages with a Hispanicsounding name tend to have higher response rates to the good-grades treatment. Finally,
treatment effects for the primary treatment messages do not differ significantly for messages
signaling a male student compared to female students.

6.

Conclusion
While school choice can, in theory, improve access and quality, competitive pressures may

also induce schools to keep costs low and discourage students perceived as costly to educate from
enrolling. These incentives may be present outside of school choice as well, but the application
process presents a distinct opportunity for schools to treat costlier or students perceived as harderto-educate students differently than with a neighborhood-based, school-assignment mechanism.
Using an audit study approach, we provide the first experimental assessment of whether schools
fail to provide application information to families with children who have a severe special need,
behavior problem, or level of academic achievement, as well as a perceived race and gender. Our
study is also much broader in scope than previous research; we sample schools across 29 states
and the District of Columbia and approximately half of the charter schools in the United States.
We find that, on average, schools are significantly less likely to provide information to families
with students who have low grades, behavior problems, or an IEP requiring they be taught in a
separate classroom, than to families of students without these attributes. Charter schools are
significantly less likely to reply to students to the IEP message than to the baseline message,
while traditional public schools are not. There is also some evidence that schools are less likely to
respond to families with Hispanic-sounding names.
One limitation of our analysis is that we chose one particular signal of disability, and one
that may be perceived as particularly costly for schools to provide services. Our findings may not
21

generalize to other disabilities or students requiring other services, such as students with limited
English proficiency. We did not incorporate a wider variety of treatments because of statistical
power concerns. A second limitation is that we cannot estimate the impacts of these behaviors on
the enrollment rates of different types of students into schools. We capture just one way schools
can dissuade certain students from applying, but schools can use other ordeal mechanisms to
screen students as well, such as moving up application deadlines, requiring parents fill out a paper
applications, and attending school open houses.
The implications of our results for optimal policy requires specifying a social welfare function.
Some might favor maximizing a weighted average of student achievement, but many families see
inclusion as essential for improving pro-social outcomes, especially as they pertain to students
with special needs or diversity.30 Our results suggest that funding is a key constraint on schools’
willingness to serve students with disabilities. We show suggestive evidence that costreimbursement funding mitigates charter schools’ differential response rates for students with
IEPs. All schools—including traditional public and charter—are less likely to respond to students
with poor prior behavior and low grades. Conducting systematic audits could help deter this
behavior. Some education agencies have already undertaken audits via phone calls asking
eligibility questions to charter schools.

For instance, see work by Gautam Rao (2019) on how the integration of castes in India impacted social preferences around inequality,
inclusion and altruism.

30

22

References
Abdulkadiroğlu, Atila, Joshua D Angrist, Susan M Dynarski, Thomas J Kane, and Parag A
Pathak, “Accountability and flexibility in public schools: Evidence from Boston’s charters and pilots,” The
Quarterly Journal of Economics, 2011, 126 (2), 699–748.
Abdulkadiroğlu, Atila, Joshua D Angrist, Peter Hull, and Parag A Pathak, “Charters without
Lotteries: Testing Takeovers in New Orleans and Boston,” American Economic Review, 2016, 106 (7), 1878–
1920.
Abdulkadiroğlu, Atila, Parag Pathag, Jonathan Schellenberg and Christopher Walters, “Do
Parents Value School Effectiveness?” NBER Working Paper 23912, 2017
Altonji, Joseph G, Ching-I Huang, and Christopher R Taber, “Estimating the cream skimming
effect of school choice,” Journal of Political Economy, 2015, 123 (2), 266–324.
Angrist, Joshua D, Parag A Pathak, and Christopher R Walters, “Explaining Charter School
Effectiveness,” American Economic Journal: Applied Economics, 2013, 5 (4), 1–27.
Angrist, Joshua D., Sarah R. Cohodes, Susan M. Dynarski, Parag A. Pathak, and
Christopher R. Walters, “Stand and Deliver: Effects of Boston’s Charter High Schools on College
Preparation, Entry, and Choice,” Journal of Labor Economics, April 2016, 34 (2).
Ayres, Ian and Peter Siegelman, “Race and Gender Discrimination in Bargaining for a New Car,”
The American Economic Review, 1995, 85(3): 304-321.
Baker, Rachel, Thomas Dee, Brent Evans, and Juhn John, “Bias in Online Classes: Evidence
from a Field Experiment,” Center for Education Policy Analysis working paper, 2018.
Batdorff, Meagan, Larry Maloney, Jay F May, Sheree T Speakman, Patrick J Wolf,
Albert Cheng, “Charter School Funding: Inequity Expands” School Demonstration Project, University of
Arkansas.

April

2014.

http://www.uaedreform.org/

expands.pdf
23

wp-content/uploads/charter-funding-inequity-

Bertrand, Marianne and Jessica Pan, “The trouble with boys: Social influences and the gender
gap in disruptive behavior,” American Economic Journal: Applied Economics, 2013, 5 (1), 32–64.
and Sendhil Mullainathan, “Are Emily and Greg More Employable Than Lakisha and Jamal?
A Field Experiment on Labor Market Discrimination,” American Economic Review, 2004, 94 (4), 991–1013.
Bergman, Peter, “Parent-Child Information Frictions and Human Capital Investment: Evidence from a
Field Experiment,” CESifo Working Paper 5391, 2015.
Bergman, Peter, “The Risks and Benefits of School Integration for Participating Students: Evidence from
a Randomized Desegregation Program,” No. 11602. Institute for the Study of Labor (IZA), 2018.
Bergman, Peter, Eric Chan, and Adam Kapor, “Housing Search Frictions: Evidence from
Detailed Search Data and a Field Experiment,” 2019.
Bifulco, Robert, Helen F Ladd, and Stephen L Ross, “The effects of public school choice on
those left behind: Evidence from Durham, North Carolina,” Peabody Journal of Education, 2009, 84 (2), 130–
149.
Booker, Kevin, Ron Zimmer, and Richard Buddin, “The effects of charter schools on school peer
composition,” 2005.
Chabrier, Julia, Sarah Cohodes, and Philip Oreopoulos, “What Can We Learn from Charter School
Lotteries?,” The Journal of Economic Perspectives, 2016, 30 (3), 57–84.
Chambers, JG, “Geographic Variations in Public Schools Costs (NCES 98-04),” US Department of
Education. Washington, DC: National Center for Education Statistics Work- ing Paper, 1998.
Chetty, Raj, John N. Friedman, Nathaniel Hilger, Emmanuel Saez, Diane Whitmore
Schanzenbach, and Danny Yagan, “How does your kindergarten classroom affect your earnings?
Evidence from Project STAR.” The Quarterly Journal of Economics, 2011, 126(4), 1593-1660.
Chetty, Raj, John N. Friedman, and Jonah E. Rockoff, “Measuring the impacts of teachers II:
Teacher value-added and student outcomes in adulthood,” American Economic Review, 2014, 104(9), 263324

79.
Clark, Melissa A, Philip M Gleason, Christina C Tuttle, and Marsha K Silverberg, “Do
Charter Schools Improve Student Achievement,” Educational Evaluation and Policy Analysis, 2015, 37 (4),
419–436.
Cohodes, Sarah, “Charter schools and the achievement gap,” The Future of Children, winter, 2018.
Cohodes, Sarah and Susan Dynarski, “Massachusetts charter cap holds back disadvantaged,”
Evidence Speaks Reports, Economics Studies at Brookings, 2016, Vol 2 (1) pp. 1-6.
Collins, LA and PA Zirkel, “To What Extent, If Any, May Cost Be a Factor in Special Education
Cases?,” Education Law Reporter, 1992, pp. 11–25.
Corcoran, Sean P and Jennifer L Jennings, “The Gender Gap in Charter School Enrollment,”
Education Policy, forthcoming.
Cullen, Julie Berry and Steven G Rivkin, “The role of special education in school choice,” in
“The economics of school choice,” University of Chicago Press, 2003, pp. 67– 106.
Darolia, Rajeev, Cory Koedel, Paco Martorell, Katie Wilson, and Francisco Perez-Arce,
“Do Employers Prefer Workers Who Attend For-Profit Colleges? Evidence from a Field Experiment,”
Journal of Policy Analysis and Management, 2015, 34(4): 881-903.
DeArmond, Michael, Ashley Jochim, and Robin Lake, “Making School Choice Work,” Center
on

Reinventing

Public

Education,

July

2014.

http://www.crpe.org/

sites/default/files/CRPE_MakingSchoolChoiceWork_Report.pdf
Deming, David J., Justine S. Hastings, Thomas J. Kane, and Douglas O. Staiger, “School
Choice, School Quality, and Postsecondary Attainment,” American Economic Re- view, 2014, 104 (3), 991–
1013.
Deming, David J. “Better schools, less crime?,” Quarterly Journal of Economics, 2011, 126(4), 2063-

25

2115.
Deming, David J., Noam Yuchtman, Amira Abulafi, Claudia Goldin, and Lawrence F.
Katz, “The Value of Postsecondary Credentials in the Labor Market: An Experimental Study,” American
Economic Review, 106(3): 778-806.
Dobbie, Will and Jr. Fryer Roland G., “Getting beneath the Veil of Effective Schools: Evidence
from New York City,” American Economic Journal: Applied Economics, 2013, 5 (4), 28–60.
and Roland G. Fryer, “The Medium-Term Impacts of High-Achieving Charter Schools,” Journal of
Political Economy, 2015, 123 (5), 985–1037.
Drame, Elizabeth R. “An analysis of the capacity of charter schools to address the needs of students
with disabilities in Wisconsin.” Remedial and Special Education, 2011, 32(1): 55-63.
Edelman, Benjamin, Michael Luca, and Dan Svirsky, “Racial Discrimination in the Sharing
Economy: Evidence from a Field Experiment,” American Economic Journal: Applied Economics, 2017,
9(2), 1-22.
Epple, Dennis, Richard Romano, and Miguel Urquiola, “School Vouchers: A Survey of the
Economics Literature,” Journal of Economic Literature, 2017, 55(2), 441-492.
Epple, Dennis, Richard Romano, and Ronald Zimmer, “Charter Schools: A Survey of Research
on their Characteristics and Effectiveness,” Handbook of Economics of Education, Editors, Eric Hanushek,
Stephen Machin, and Ludger Woessmann, 2016, Vol. 5, 139-208.
Fryer, Roland, “Injecting Charter Schools Best Practices into Traditional Public Schools: Evidence
from Field Experiments,” The Quarterly Journal of Economics, 2014, 29 (3), 1355-1407.
Giuletti, Corrado, Tonin, Mirco, and Michael Vlassopoulos, “Racial Discrimination in Local
Public Services: A Field Experiment in the United States,” Journal of the European Economic Association,
2019, 17(1): 165-204.
26

Griffith, Michael. “State Funding Programs for High-Cost Special Education Students,” Education
Commission of the States: State Notes, May 2008.
Government Accountability Office (GAO), “Charter Schools: Additional Federal Attention
Needed to Help Protect Access for Students with Disabilities. Report to Congressional Requesters. GAO-12543.,” US Government Accountability Office, 2012.
Hanushek, Eric and Steven Rivkin. “Understanding the Twentieth-Century Growth in U.S. School
Spending,” Journal of Human Resources, 1997, 32(1), 35-68.
Hastings, Justine, Thomas Kane, and Douglas O. Staiger, “Parental Preferences and School
Competition: Evidence from a Public School Choice Program,” NBER Working Paper No. 11805, 2006.
Hastings, Justine and Jeffrey Weinstein, “Information, School Choice, and Academic Achievement:
Evidence from Two Experiments,” The Quarterly Journal of Economics, 2008, 123 (4), 1373–1414.
Heubert, Jay P, “Schools without Rules-Charter Schools? Federal Disability Law, and the Paradoxes
of Deregulation,” Harv. CR-CLL Rev., 1997, 32, 301.
Holm, Sture, “A Simple Sequentially Rejective Multiple Test Procedure”, Scandinavian Journal of
Statistics, 1979, 6(2), pp. 65-70.
Hoxby, Caroline Minter, and Jonah E. Rockoff, “The impact of charter schools on student
achievement,” Cambridge, MA: Department of Economics, Harvard University, 2004.
Hoxby, Caroline and Sonali Murarka, “Charter schools in New York City: Who enrolls and how they
affect their students’ achievement,” Technical Report, National Bureau of Economic Research 2009.
Hoxby, Caroline M and Sarah Turner, “Expanding College Opportunities for High-Achieving,
Low Income Students,” 2013. SIEPR Discussion Paper 12-014.
Hsieh, Chang-Tai and Miguel Urquiola, “The effects of generalized school choice on achievement
and stratification: Evidence from Chile’s voucher program,” Journal of public Economics, 2006, 90 (8), 1477–
27

1503.
Kapor, Adam, Christopher Neilson, and Seth D Zimmerman, “Heterogeneous Beliefs and School
Choice,” forthcoming, American Economic Review.
Kline, Patrick and Christopher R. Walters, “Evaluating Public Programs with Close Substitutes:
The Case of Head Start,” Quarterly Journal of Economics, 2016, pp. 1795-1848.
Lacireno-Paquet, Natalie, Thomas T Holyoke, Michele Moser, and Jeffrey R Henig,
“Creaming versus cropping: Charter school enrollment practices in response to market incentives,” Educational
Evaluation and Policy Analysis, 2002, 24 (2), 145–158.
Lazear, Edward P, “Educational Production,” The Quarterly Journal of Economics, 2001, 116 (3), 777–
803.
Loeb, Susanna, Jon Valant, and Matt Kasman, “Increasing Choice in the Market for Schools:
Recent Reforms and their Effects on Student Achievement,” National Tax Journal, 2011, 64 (1), 141-164.
Lubienski, Christopher, “Marketing Schools: Consumer Goods and Competitive Incentives for Consumer
Information,” Education and Urban Society, 2007, 40 (1), 118-141.
Lubienski, Christopher, Charisse Gulosino, and Peter Weitzel, “School Choice and Competitive
Incentives: Mapping the Distribution of Educational Opportunities across Local Educational Markets,” American
Journal of Education 2009, 115.
Millard, Maria and Stephanie Aragon. “State funding for students with disabilities,” Education
Commission of the States: 50-State Review, June 2015.
Moore, Mary, William Strang, Myron Schwartz, and Mark Braddock, “Patterns in Special
Education Service Delivery and Cost.” 1988.
Muralidharan, Karthik and Venkatesh Sundararaman, “The Aggregate Effect of School Choice:
Evidence from a Two-Stage Experiment in India,” The Quarterly Journal of Economics, 2015, 130 (3), 1011–
1066.
28

National Center for Education Statistics (NCES), “Table 216.90: Public elementary and
secondary charter schools and enrollment, by state: Selected years, 1999-2000 through 2013-14,” in “Digest of
Education Statistics,” National Center for Education Statistics, 2015.
National Center for Special Education in Charter Schools (NCSECS), “What is an
Infrastructure? Special Education Support Systems in Charter Schools,” 2017, Issue Brief—Infrastructures,
Nichols-Barrer, Ira, Philip Gleason, Brian Gill, and Christina Clark Tuttle, “Student selection,
attrition, and replacement in KIPP middle schools,” Educational Evaluation and Policy Analysis, 2015, p. 16273.
Oreopoulos, Philip, “Why Do Skilled Immigrants Struggle in the Labor Market? A Field Experiment
with Thirteen Thousand Resumes,” American Economic Journal: Economic Policy, 3(4): 148-171.
Orfield, Myron W., Baris Gumus-Dawes, and Thomas Luce, “The state of public schools in
post-Katrina New Orleans: The challenge of creating equal opportunity,” Educational Delusions?: Why
Choice Can Deepen Inequality and How to Make Schools Fair. University of California Press, 2013. 159184.
Prothero, Arianna, “Mystery Parents Test Charters’ Enrollment of Special Ed., ELL Students,”
Education Week, 2014.
Rao, Gautam, “Familiarity Does Not Breed Contempt: Diversity, Discrimination and Generosity in
Delhi Schools,” 2019, American Economic Review.
Rhim, Lauren M., Paul O’Neill, Amy Ruck, Kathryn Huber, and Sivan Tuchman, “Getting lost
while trying to find the money: Special education finance in charter schools,” 2015 Report.
Scott, George A, “Charter Schools: Additional Federal Attention Needed to Help Protect Access for
Students with Disabilities. Report to Congressional Requesters. GAO-12-543.,” US Government Accountability
Office, 2012.
Setren, Elizabeth, “Special Education and English Language Learner Students in Boston Charter
29

Schools: Impact and Classification,” 2015.
Walters, Christopher, “The Demand for Effective Charter Schools,” Journal of Political Economy,
2018, Forthcoming.
Welner, Kevin G, “The Dirty Dozen: How Charter Schools Influence Student Enrollment” Teachers
College Record 17104.
Winters, Marcus A., Grant Clayton, and Dick M. Carpenter II, “Are low-performing students
more likely to exit charter schools? Evidence from New York City and Denver, Colorado,” Economics of
Education Review 2017, 56 pp. 110-117.
Wong, Mitchell D., Karen M. Coller, Rebecca N. Dudovitz, David P. Kennedy, Richard
Buddin, Martin F. Shapiro, Sheryl H. Kataoka Arleen F. Brown, Chi-Hong Tseng, Peter
Bergman, and Paul J. Chung, “Successful schools and risky behaviors among low-income adolescents,”
Pediatrics, 2014, 134(2).
Zimmer, Ron, Brian Gill, Kevin Booker, Stephane Lavertu, Tim R Sass, and John Witte,
Charter schools in eight states: Effects on achievement, attainment, integration, and competition, Vol. 869, Rand
Corporation, 2009.
Zimmer, Ron and Cassandra Guarino, “Is there empirical evidence that charter schools push out lowperforming students?,” Educational Evaluation and Policy Analysis, 2013, 35 (4), 461–80.

30

Figures

(a) First Experiment

(b)Second Experiment

Figure 1 - Experimental Design for (a) First experiment sample and (b) Second experiment sample

(Note: This figure shows the study design and examples of the treatment phrasings). Race, gender and
other email characteristics were assigned with equal probability across these treatment messages.
31

Tables
Table 1 – Testing for baseline balance across treatment messages
Baseline
TPS
% LEP students
% Chronically absent students
% Students with IEP
% Proficiency
% White students
% Black students
% Hispanic students
% FRPL students
Value added measure
% Urban
% Suburban
% Rural

Bad
Grades

IEP

Bad
Behavior

Good
Grades

0.293

0.008

0.010

-0.004

-0.007

(0.455)

(0.008)

(0.007)

(0.007)

(0.007)

0.123

0.026

0.019

-0.039

-0.013

(0.18)

(0.042)

(0.037)

(0.04)

(0.038)

0.159

-0.031

0.041

-0.012

0.019

(0.187)

(0.038)

(0.033)

(0.039)

(0.034)

0.075

-0.034

-0.049

0.032

0.029

(0.084)

(0.069)

(0.066)

(0.068)

(0.06)

0.463

0.011

0.073*

0.045

0.060

(0.225)

(0.047)

(0.042)

(0.047)

(0.045)

0.288

0.094*

0.024

-0.004

-0.026

(0.308)

(0.057)

(0.051)

(0.054)

(0.051)

0.254

-0.082

0.022

0.022

0.010

(0.321)

(0.054)

(0.05)

(0.051)

(0.05)

0.321

-0.036

0.046

0.060

-0.019

(0.31)

(0.054)

(0.049)

(0.051)

(0.049)

0.59

-0.036

0.001

-0.013

0.019

(0.329)

(0.026)

(0.024)

(0.026)

(0.023)

0.017

-0.002

-0.012

-0.008

-0.010

(0.992)

(0.008)

(0.007)

(0.008)

(0.007)

0.606

-0.043

-0.026

0.014

0.000

(0.489)

(0.032)

(0.027)

(0.033)

(0.027)

0.269

-0.046

-0.015

0.005

0.003

(0.443)

(0.033)

(0.028)

(0.034)

(0.028)

0.058

-0.024

0.010

0.042

0.022

(0.235)

(0.037)

(0.03)

(0.038)

(0.029)

0.391

0.194

0.691

0.862

P-value, joint test

Observations
5,031
2,441
3,522
2,462
1,434
Notes: Column (1) shows means for the baseline message sample and the standard deviations in
parenthesis. Each other column is as follows: For each treatment, the sample is restricted to baseline and
treatment observation indicated in the column header, and the treatment indicator is regressed on the
covariates shown in each row. School-level demographic data including absenteeism, Free-Reduced Priced
Lunch (FRPL), Limited English Proficiency (LEP), and disability data are from the Civil Rights Data
Collection 2013-14 public dataset. % Proficiency measures the rate of students at or above proficiency, as
reported by Great Schools from 2016. See text for the construction of the Value-added measure. Two-way
cluster-robust standard errors (by pair and school) in parentheses. The joint test is a test of whether the
covariates are jointly different from zero.
*** p<0.01, ** p<0.05, * p<0.1

32

Table 2 – Effects of Message Treatments on School Response Rates
Sample

Full Sample

TPS

Charter
-0.015

TPS - Charter diff.

Bad Grades

-0.024**

-0.044**

(0.011)

(0.021)

(0.013)

(0.025)

IEP

-0.052***

-0.002

-0.065***

0.058**

(0.010)

(0.021)

(0.011)

(0.023)

Bad Behavior

-0.070***

-0.053***

(0.011)

(0.021)

(0.013)

(0.024)

0.001

0.013

-0.005

0.015

(0.014)

(0.021)

(0.020)

(0.029)

-0.012

-0.004

-0.015

0.010

Good Grades
Black
Hispanic
Father
Son
Two Parents
TPS

-0.076***

-0.030

0.021

(0.010)

(0.018)

(0.011)

(0.022)

-0.020**

-0.015

-0.023**

0.009

(0.010)

(0.018)

(0.011)

(0.021)

0.003

-0.009

0.008

-0.017

(0.008)

(0.015)

(0.009)

(0.017)

-0.011

-0.007

-0.011

0.004

(0.007)

(0.011)

(0.008)

(0.014)

0.010

0.007

0.014

-0.009

(0.011)

(0.015)

(0.015)

(0.021)

4,296

10,510

14,806

0.008
(0.013)

Observations

14,806

Control Group Mean
0.533
0.496
0.548
0.533
Notes: Table shows the results of a multivariate regression of an indicator for whether or not a
school responded to the message on message-treatment indicators. Columns (1)-(3) show the results for
different samples: (1) full sample, (2) only traditional public schools and (3) only charter schools. Column
(4) interaction between primary and secondary treatments and TPS. TPS is an indicator variable (not
a treatment) for whether a school is a traditional public school. All other variables included in the table
are randomly assigned characteristics of the emails. Regressions include fixed effects by wave and state.
Two-way cluster-robust standard errors (by pair and school) in parentheses.
*** p<0.01, ** p<0.05, * p<0.1

33

Table 3 - Effects of Message Treatments on School Response Perceived as Encouraging
Sample
Bad Grades
IEP
Bad Behavior
Good Grades
Black
Hispanic
Father
Son
Two Parents
TPS

Full Sample

TPS

Charter

TPS - Charter diff

-0.027***

-0.065***

-0.012

-0.052**

(0.010)

(0.016)

(0.012)

(0.020)

-0.035***

0.001

-0.044***

0.042**

(0.009)

(0.017)

(0.010)

(0.020)

-0.061***

-0.083***

-0.051***

-0.033*

(0.010)

(0.015)

(0.012)

(0.019)

-0.003

-0.009

0.001

-0.013

(0.012)

(0.017)

(0.017)

(0.024)

-0.018**

-0.013

-0.019*

0.006

(0.008)

(0.014)

(0.010)

(0.017)

-0.010

0.004

-0.016

0.019

(0.008)

(0.014)

(0.010)

(0.017)

-0.005

-0.008

-0.004

-0.006

(0.007)

(0.011)

(0.008)

(0.014)

-0.005

-0.013

-0.002

-0.012

(0.006)

(0.010)

(0.008)

(0.013)

0.012

0.022*

0.003

0.017

(0.009)

(0.012)

(0.013)

(0.017)

-0.056***
(0.009)

Observations

14,806

4,296

10,510

14,806

Control Group Mean

0.256

0.195

0.281

0.256

Notes: Table shows the results of a multivariate regression of an indicator for whether or not a school responds in an "encouraging"
manner to the message. Non-response is coded as not encouraging (or zero). See Appendix C for a description of our qualitative
data and reports on inter-rater reliability. Columns (1)-(3) show the results for different samples: (1) full sample, (2) only traditional
public schools and (3) only charter schools. Column (4) interaction between primary and secondary treatments and TPS. TPS is
an indicator variable (not a treatment) for whether a school is a traditional public shcool. All other variables included in the table
are randomly assigned characteristics of the emails. Regressions include fixed effects by wave and state. Two-way cluster-robust
standard errors (by pair and school) in parentheses. *** p<0.01, ** p<0.05, * p<0.1

34

Table 4 - Differential Effect of Messages by VAM Performance and Type of Charter
Sample
Bad Grades
IEP
Bad Behavior
Good Grades
Black
Hispanic
Father
Son
Two Parents
TPS

Mean

Observations
Control Group

High VAM

Low VAM

High-VAM,
Urban Charter

No Excuses
Charter

-0.022
(0.018)
-0.049***
(0.016)
-0.067***
(0.018)
0.035
(0.022)
-0.032**
(0.015)
-0.049***
(0.015)
0.012
(0.013)
-0.006
(0.011)
0.012
(0.017)
0.016
(0.020)

-0.025
(0.018)
-0.056***
(0.016)
-0.066***
(0.018)
-0.034
(0.022)
0.007
(0.016)
0.019
(0.016)
-0.006
(0.013)
-0.012
(0.010)
0.007
(0.018)
0.003
(0.020)

0.010
(0.028)
-0.093***
(0.024)
-0.070**
(0.028)
0.054
(0.036)
-0.049**
(0.024)
-0.062**
(0.024)
0.017
(0.020)
-0.007
(0.017)
0.002
(0.029)

0.016
(0.052)
-0.104**
(0.044)
0.009
(0.054)
0.033
(0.063)
0.036
(0.050)
0.029
(0.044)
0.085**
(0.037)
-0.069**
(0.031)
0.046
(0.047)

8,835

2,516

3,457

689

0.549
0.538
0.538
0.559
Notes: Table shows the effect of different treatments on response rate for different samples. TPS is a covariate for
traditional public school. All other variables included in the table are randomly assigned characteristics of the emails.
Sample row shows the population for each regression: (1) Schools with high value-added measure (>0), (2) Schools with
low high value-added measures (<0), (3) Urban charter schools with high VAM, and (4) “No Excuses” charter schools.
Regressions include fixed effects by wave and state. Two-way cluster-robust standard errors (by pair and school) in
parentheses.
*** p<0.01, ** p<0.05, * p<0.1

35

Table 5 - Effect of IEP Message by State Funding Category and LEA status for Charter
Schools
(1)
IEP
IEP x Funding = Categorical

(2)

-0.080***

-0.064***

(0.015)

(0.016)

0.022
(0.025)

IEP x Funding = Reimbursement

0.068**
(0.035)

IEP x Own LEA

-0.008
(0.023)

Observations

10,510

10,087

Control Group Mean

0.548

0.548

Notes: Table shows the effect of different treatments on response
rate for charter schools in the sample depending on state funding and
whether they are their own LEA (Local Education Agency). The
categories for forms of funding are “Formula” (base), “Categorical,” and
“Reimbursement.” All other treatment variables are randomly assigned
characteristics of the emails. Regressions include all treatment indicators
(only IEP is shown), wave and state fixed effects. Nested cluster-robust
standard errors (by pair and school) in parentheses.
*** p<0.01, ** p<0.05, * p<0.1

36

Appendix A.
Figures

(a)

(b)

Figure A1 - Message sent to parents based for (a) baseline and (b) IEP intervention

(Note: This figure shows an example of one phrasing for the Baseline and IEP message)

37

Figure A2 - Map of Sample Frame

(Note: The map shows the sample frame for the experiment)

38

Appendix Tables
Table A1 – Total Population of Traditional Public Schools and Charter Schools vs. Sample
CRDC Population

% White students
% Black students
% Hispanic students
% FRPL
% Limited English Proficiency
Absenteeism Rate
% IEP Students
% of IEP students in regular class <40% of time
% Students Receiving In-School Suspension
% Students Receiving 1 Out-of-school Suspension
% Proficiency
% Urban schools
% Suburban schools
% Rural schools

Sample

TPS

Charter

TPS

Charter
(All)

Charter
(Exp 1)

Charter
(Exp 2)

0.57

0.351

0.237

0.342

0.405

0.251

(.327)

(.325)

(.262)

(.323)

(.332)

(.285)

0.133

0.291

0.279

0.262

0.231

0.306

(.219)

(.338)

(.311)

(.324)

(.304)

(.347)

0.205

0.275

0.399

0.314

0.278

0.367

(.258)

(.285)

(.307)

(.304)

(.285)

(.322)

0.684

0.53

0.684

0.549

0.533

0.572

(.293)

(.329)

(.293)

(.33)

(.321)

(.34)

0.091

0.094

0.186

0.096

0.079

0.124

(0.148)

(0.162)

(0.2)

(0.167)

(0.154)

(0.182)

0.112

0.162

0.188

0.17

0.18

0.152

(0.292)

(0.384)

(0.226)

(0.333)

(0.313)

(0.363)

0.108

0.086

0.117

0.088

0.093

0.08

(0.134)

(0.166)

(0.099)

(0.146)

(0.167)

(0.099)

0.008

0.004

0.017

0.002

0.003

0.002

(0.023)

(0.03)

(0.034)

(0.024)

(0.03)

(0.008)

0.022

0.016

0.028

0.018

0.016

0.021

(0.296)

(0.304)

(0.109)

(0.227)

(0.207)

(0.257)

0.01

0.024

0.031

0.027

0.027

0.026

(0.263)

(0.299)

(0.077)

(0.223)

(0.203)

(0.253)

40.329

44.475

42.177

48.374

48.406

48.327

(15.725)

(20.445)

(22.273)

(22.515)

(21.998)

(23.282)

0.239

0.518

0.682

0.575

0.504

0.682

(0.427)

(0.500)

(0.466)

(0.494)

(0.500)

(0.466)

0.318

0.291

0.285

0.269

0.263

0.278

(0.466)

(0.454)

(0.452)

(0.443)

(0.440)

(0.448)

0.299

0.117

0.017

0.098

0.148

0.023

(0.458)

(0.321)

(0.130)

(0.298)

(0.355)

(0.151)

Observations
89,898
5,813
2,170
4,283
2,635
1,648
Notes: Means are reported for all schools that are available in CRDC 2013-14 dataset. The CRDC does not have data on all
schools in the study sample. TPS schools are traditional public schools. IEP students refers to students that have an
Individualized Education Program. School proficiency scores show the percentage of students scoring at or above proficiency
on state assessments across grades and subject as reported by Great Schools. Charter (All) column represents all the charter
schools in the experimental sample for both experiments. Charter (Exp 1) and Charter (Exp 2) columns show the
characteristics of the charter schools included in the first and second experiment, respectively.

39

Table A2 – The Relationship between Response and School Locations and Demographics
Responded
City

0.008
(0.027)

Suburb

-0.021
(0.027)

Rural

0.015
(0.031)

% FRL

-0.070***
(0.019)

% Female

0.180***
(0.067)

% Black

-0.321***
(0.047)

% White

0.006
(0.051)

% Hispanic

-0.252***
(0.047)

Enrollment

0.000**
(0.000)

% Proficiency

-0.001**
(0.000)

Proficiency (missing)

-0.055***
(0.018)

Constant

0.662***
(0.059)

Observations

14,064

R-squared
0.049
Notes: Table shows regression coefficients and standard errors
for a regression of school covariates on an indicator for response.
School-level demographic data include enrollment, FRPL population,
race and ethnicity breakdowns is retrieved from National Center for
Education Statistics (NCES) Common Core of Data for the 2015-16
school year. School proficiency shows the % of proficient (or higher)
students as reported by GreatSchools in their most recent dataset
from 2016. Nested clustered standard errors in parentheses.
*** p<0.01, ** p<0.05, * p<0.1

40

Table A3 - Effect of Message Treatments on Response Rates from Schools by Experiment
Sample

Bad Grades

1st Experiment

0.004

2nd
Experiment

TPS, 2nd
Experiment

-0.044***

-0.044**

Charter, 2nd
Experiment
-0.043**

Diff. TPS Charter, 2nd
Experiment
-0.005

(0.017)

(0.014)

(0.021)

(0.020)

(0.029)

IEP

-0.055***

-0.042***

-0.002

-0.082***

0.073**

(0.013)

(0.015)

(0.021)

(0.020)

(0.029)

Bad Behavior

-0.064***

-0.072***

-0.053***

-0.091***

0.037

(0.017)

(0.015)

(0.021)

(0.020)

(0.029)

-0.003

0.013

-0.019

0.027

Good Grades
Black
Hispanic
Father
Son

(0.014)

(0.021)

(0.020)

(0.029)

-0.024

-0.003

-0.004

-0.002

-0.003

(0.015)

(0.013)

(0.018)

(0.018)

(0.026)

-0.031**

-0.013

-0.015

-0.011

-0.003

(0.015)

(0.013)

(0.018)

(0.018)

(0.025)

0.012

-0.001

-0.009

0.007

-0.016

(0.012)

(0.011)

(0.015)

(0.015)

(0.021)

-0.020*

-0.004

-0.007

-0.000

-0.007

(0.011)

(0.008)

(0.011)

(0.010)

(0.015)

0.011

0.007

0.014

-0.008

(0.011)

(0.015)

(0.015)

(0.021)

Two Parents
TPS

0.008
(0.013)

Mean

Observations

6,210

8,596

4,296

4,300

8,596

R-squared
Control Group

0.039

0.043

0.064

0.038

0.044

0.575
0.503
0.496
0.510
0.503
Notes: Table shows the effect of different treatments on response rate for the first (old) and second (new) experimental
sample. TPS is a covariate for traditional public school. All other variables included in the table are randomly assigned
characteristics of the emails. Regressions include fixed effects by wave and states. Sample row shows the population for each
regression: (1) Old sample (full), which only included charter schools), (2) New sample (full), (3) only traditional public schools
for new sample, (4) only charter schools for new sample, and (5) interaction between primary and secondary treatments and
TPS for new sample. Nested cluster-robust standard errors (by pair and school for new sample (full and diff TPS-Charter) and
only school for others) in parentheses.
*** p<0.01, ** p<0.05, * p<0.1

41

Table A4 - Effect of Message Treatments on Response Rates from Schools for Different
Specifications
(1)
Bad Grades
IEP
Bad Behavior
Good Grades
Black
Hispanic
Father
Son
Two Parents
TPS

(2)

(3)

(4)

(5)

-0.024**

-0.027**

-0.027**

-0.022*

-0.022*

(0.011)

(0.011)

(0.011)

(0.012)

(0.012)

-0.052***

-0.051***

-0.051***

-0.056***

-0.056***

(0.010)

(0.010)

(0.010)

(0.010)

(0.010)

-0.070***

-0.069***

-0.069***

-0.070***

-0.069***

(0.011)

(0.011)

(0.011)

(0.012)

(0.012)

0.001

0.002

0.002

-0.007

-0.007

(0.014)

(0.014)

(0.014)

(0.015)

(0.015)

-0.012

-0.014

-0.014

-0.019*

-0.019*

(0.010)

(0.010)

(0.010)

(0.011)

(0.011)

-0.020**

-0.021**

-0.021**

-0.018*

-0.017*

(0.010)

(0.009)

(0.009)

(0.010)

(0.010)

0.003

0.000

-0.000

-0.001

-0.000

(0.008)

(0.008)

(0.008)

(0.009)

(0.009)

-0.011

-0.010

-0.010

-0.009

-0.009

(0.007)

(0.007)

(0.007)

(0.007)

(0.007)

0.010

0.009

0.010

0.013

0.013

(0.011)

(0.011)

(0.011)

(0.012)

(0.012)

0.008

0.027**

0.030**

0.033**

0.031*

(0.013)

(0.013)

(0.014)

(0.016)

(0.016)

Observations

14,806

14,806

14,806

14,806

14,806

R-squared

0.039

0.068

0.070

0.267

0.269

Control Group Mean

0.533
X

X

X

X

X

X

X

X

X

School-level Controls 1
School-level Controls 2
Pair Fixed Effects

Tract-level Controls
X
Notes: Table shows the effect of different treatments on response rate. TPS is a covariate for traditional
public school. All other variables included in the table are randomly assigned characteristics of the emails.
School-level controls 1 include fraction of school population that is black, Hispanic, free-and-reduced price lunch
eligible, female, the school's proficiency rating, and city, suburb, and rural status of the school's location. Schoollevel controls 2 include fraction of the school population that has a disability, proportion of students with
disabilities that are taken out of regular class <39%, 40-79%, and 80%+ of the time, fraction of school population
that is chronically absent, and fraction of the population that has received an out-of-school suspension. Tractlevel controls include disability shares, race/ethnicity shares, share of residents with a Bachelor’s Degree, median
earnings, poverty rates, and food stamp recipients for the school’s associated census tract. Nested cluster-robust
standard errors (by pair and school) in parentheses.
*** p<0.01, ** p<0.05, * p<0.1

42

Table A5 – Effects of Message Treatments on School Response Rates Adjusting by Multiple
Hypothesis Testing
Sample
Bad Grades
IEP
Bad Behavior
Good Grades
Black
Hispanic
Father
Son
Two Parents

Observations

Full Sample

TPS

Charter

TPS - Charter diff.

-0.024*

-0.044*

-0.015

-0.030

[0.064]

[0.097]

[0.512]

[0.648]

-0.052***

-0.002

-0.065***

0.058*

[<0.001]

[0.917]

[<0.001]

[0.053]

-0.070***

-0.053**

-0.076***

0.021

[<0.001]

[0.038]

[<0.001]

[0.772]

0.001

0.013

-0.005

0.015

[0.941]

[1.000]

[0.817]

[0.595]

-0.012

-0.004

-0.015

0.010

[0.631]

[0.818]

[0.578]

[1.000]

-0.02

-0.015

-0.023

0.009

[0.166]

[1.000]

[0.201]

[1.000]

0.003

-0.009

0.008

-0.017

[0.724]

[1.000]

[0.396]

[1.000]

-0.011

-0.007

-0.011

0.004

[0.441]

[1.000]

[0.599]

[0.76]

0.010

0.007

0.014

-0.009

[0.719]

[1.000]

[0.708]

[1.000]

14,806

4,296

10,510

14,806

Control Group Mean
0.533
0.496
0.548
0.533
Notes: Table shows the results of regressing message-treatment indicators on an indicator variable
for whether or not a school responded to the message. Columns (1)-(3) show the results for different
samples: (1) full sample, (2) only traditional public schools and (3) only charter schools. Column (4)
interaction between primary and secondary treatments and TPS. TPS is an indicator variable (not a
treatment) for whether a school is a traditional public school. All other variables included in the table
are randomly assigned characteristics of the emails. Regressions include fixed effects by wave and state.
Adjusted p-values by multiple hypothesis testing in squared parentheses. Multiple hypothesis
adjustment was done using Holm’s method separately for primary and secondary treatments. ***
p<0.01, ** p<0.05, * p<0.1

43

Table A6 - Total Population of Charter Schools vs Sample Charter Schools and No Excuses
Charter Schools
CRDC

% Limited English Proficiency
Absenteeism Rate
% IDEA Students
% of IEP students in regular class <40% of time
% of IEP students in regular class 40-79% of time
% of IEP students in regular class >80% of time
% Students Receiving In-School Suspension
% Students Receiving 1 Out-of-school Suspension
% Proficiency
Value Added Measure

Sample

Charter

Charter

No Excuses
Charter

0.094

0.096

0.157

(0.162)

(0.167)

(0.19)

0.162

0.17

0.112

(0.384)

(0.333)

(0.157)

0.086

0.088

0.069

(0.166)

(0.146)

(0.054)

0.004

0.002

0.001

(0.03)

(0.024)

(0.006)

0.005

0.005

0.004

(0.02)

(0.021)

(0.012)

0.067

0.069

0.047

(0.088)

(0.096)

(0.051)

0.016

0.018

0.041

(0.304)

(0.227)

(0.064)

0.024

0.027

0.045

(0.299)

(0.223)

(0.042)

44.475

48.374

62.733

(20.445)

(22.515)

(21.945)

-

0.000

0.775

(1.092)

(0.976)

Observations
5813
4283
272
Notes: Means are reported for all schools that are available in CRDC 2013-14 dataset. The CRDC does not
have data on all schools in the study sample, which was from 2016-17. Charter schools are all schools that are
classified by CRDC as charter schools. IDEA students refer to students in the Individuals with Disabilities
Education Act. IEP students refers to students that have an Individualized Education Program covered by
IDEA. Charter schools may be alternative, magnet, or special education schools. School proficiency scores show
the percentage of students scoring at or above proficiency on state assessments across grades and subject as
reported by Great Schools in their most recent dataset from 2016. Value-added measure (VAM) is constructed
by calculating the normalized difference between the observed proficiency and the predicted proficiency from a
regression including school-level and tract-level controls. VAM measures can only be estimated for the sample.

44

Table A7 - Effect of Message Treatments on Response Rates by Different Randomized
Characteristics
Sample
Bad Grades
IEP

Two parent

One parent

-0.031

-0.023*

Daughter

Black

Hispanic

-0.023

-0.023

-0.011

-0.035*

(0.028)

(0.012)

(0.017)

-0.049*

-0.052***

-0.051***

(0.017)
-0.053***

(0.021)
-0.038**

(0.026)

(0.011)

(0.016)

-0.025

-0.080***

-0.058***

(0.027)

(0.013)

(0.017)

(0.017)

(0.020)

(0.021)

Good Grades

-0.047*

0.029

-0.012

0.015

-0.027

0.058**

(0.027)

(0.025)

Hispanic
Father

-0.084***

(0.026)

(0.018)

(0.022)

(0.021)

0.022

-0.021*

-0.023*

-0.001

(0.018)

(0.018)

-0.091***

-0.061***

0.000

(0.022)

(0.011)

(0.014)

(0.014)

-0.002

-0.025**

-0.024*

-0.018

0.000

(0.022)

(0.011)

(0.014)

(0.014)

(0.000)

0.011

0.001

0.007

-0.001

0.017

0.010

(0.009)

(0.011)

(0.011)

(0.018)
Son

(0.015)

(0.020)
-0.053***

Bad Behavior

Black

0.014
(0.017)

-0.017**
(0.008)

Two Parents

(0.000)

(0.013)

(0.014)

-0.022*

-0.010

(0.013)

(0.014)

0.025

-0.004

0.033*

0.005

(0.016)

(0.016)

(0.020)

(0.019)

0.003

0.010

0.003

0.012

0.006

0.007

(0.019)

(0.015)

(0.015)

(0.015)

(0.020)

(0.019)

Observations

2,916

11,890

7,375

7,431

4,999

4,916

R-squared
Control Group

0.052

0.041

0.039

0.042

0.044

0.044

TPS

Mean

Son

0.522
0.535
0.532
0.551
0.546
0.523
Notes: Table shows the effect of different treatments on response rate for different samples. TPS is a covariate for
traditional public school. All other variables included in the table are randomly assigned characteristics of the emails.
Regressions include fixed effects by wave and states. Sample row shows the population for each regression identified in
the e-mails: (1) two parents, (2) one parent, (3) student is a son, (4) students is a daughter, (5) black-sounding name, and
(6) Hispanic-sounding name. Two-way cluster-robust standard errors (by pair and school) in parentheses.
*** p<0.01, ** p<0.05, * p<0.1

45

Table A8 - Effects of Message Treatments on the Likelihood of Applying to School
Sample

Bad Grades
IEP
Bad Behavior
Good Grades
Black
Hispanic
Father
Son
Two Parents
TPS

Full Sample

TPS

Charter

TPS - Charter diff

-0.045***

-0.065***

-0.038***

-0.027

(0.009)

(0.015)

(0.011)

(0.019)

-0.054***

-0.016

-0.063***

0.045**

(0.008)

(0.017)

(0.010)

(0.019)

-0.071***

-0.081***

-0.067***

-0.016

(0.009)

(0.015)

(0.011)

(0.018)

-0.015

0.006

-0.033**

0.037

(0.011)

(0.017)

(0.016)

(0.023)

-0.013*

-0.027**

-0.007

-0.021

(0.008)

(0.014)

(0.009)

(0.016)

-0.003

-0.002

-0.003

0.001

(0.008)

(0.013)

(0.009)

(0.016)

0.002

0.011

-0.001

0.009

(0.006)

(0.011)

(0.008)

(0.013)

-0.003

-0.012

0.002

-0.014

(0.006)

(0.010)

(0.007)

(0.012)

0.010

0.016

0.004

0.011

(0.008)

(0.011)

(0.012)

(0.016)

4,296

10,510

14,806

-0.020**
(0.009)

Observations

14,806

Control Group Mean
0.211
0.180
0.225
0.211
Notes: Table shows the results of a multivariate regression of a binary indicator for whether a school's response is
likely to elicit an application based on trained coders review of school responses. See Appendix C for a description
of our qualitative data and reports on inter-rater reliability. Non-response by the school is coded as not likely to
apply to the school (or zero). Columns (1)-(3) show the results for different samples: (1) full sample, (2) only
traditional public schools and (3) only charter schools. Column (4) interaction between primary and secondary
treatments and TPS. TPS is an indicator variable (not a treatment) for whether a schoool is a traditional public
shcool. All other variables included in the table are randomly assigned characteristics of the emails. Regressions
include fixed effects by wave and state. Two-way cluster-robust standard errors (by pair and school) in parentheses.
*** p<0.01, ** p<0.05, * p<0.1

46

Table A9 - States Included in Field Experiments

States

1st Experiment

Alaska
Arizona

2nd Experiment

✔
✔

Arkansas

✔
✔

California

✔

✔

Colorado

✔

✔

Connecticut

✔

Delaware

✔

DC

✔

Florida

✔

✔

Georgia

✔

✔

Illinois

✔

Indiana

✔

Louisiana

✔

Maryland

✔

Massachusetts

✔

Michigan

✔

✔

Minnesota

✔

✔

Nevada
New Jersey

✔
✔

New Mexico

✔
✔

New York

✔

✔

North Carolina

✔

✔

Ohio

✔

✔

Oregon

✔

✔

Pennsylvania

✔

✔

South Carolina

✔

Tennessee

✔

Texas

✔

✔

Utah

✔

✔

Wisconsin

✔

✔

Notes: Table reports which states were included in the first and second field experiments, respectively.

47

Appendix B.
Data Collection and Implementation
First field experiment
This subsection describes data collection for the first field experiment. Data collection of school
contact information took place between August and October 2014. Email and web-form contact
information was collected for charter schools from the 17 states with the most charter schools. Once
these states were identified we used the U.S. Department of Education’s Common Core of Data to
determine which public schools identify as charter schools and limited our sample to these schools. The
next step involved collecting contact information. To the extent possible, we visited charter school
websites to obtain contact information. Sometimes websites were not easily located, so we used
databases maintained by state Departments of Education. We emphasized the collection of contact
information from school websites, because this is the likely contact information a parent would use if
emailing a school. For both first and second field experiments, we prioritized the type of contact
information collected. If there is an email address or webform that is used to field general inquiries,
then we used this. Otherwise, we would identify a front office receptionist or office manager. If this was
not available, then we would look for the contact information for one of the school principals.
To conduct the first experiment, we created several internet domains. For each internet domain,
we created email addresses that signal ethnic- and gender-sounding names. In the first and second field
experiments, we used Tor browsers to obscure the IP address of the sending location to make it appear
messages were sent from local parents.
Second field experiment
This describes how we collected data on schools for the second field experiment for the school
choice audit study. Data collection for the field experiment took place between Nov. 2017 and Jan 2018.
We worked with research assistants to identify school districts across states that practice some
type of intra-district school choice and that also have charter schools within their respective catchment
areas. In identifying these school districts, we first focused on whether intra-district choice is practiced.
We used geographic information systems (GIS) data from U.S. Census Bureau to identify local school
district catchment areas. We also used latitude and longitude data on each school from the U.S.
Department of Education’s Common Core of Data to identify whether charter schools are geographically
situated in school districts that practice intra-district choice.
One challenge is that there is a variation in how intra-district choice is practiced across school
districts within states. These practices helped guide the selection of catchment areas to include in our
sample. Open enrollment represents one end of the spectrum. This type of intra-district school choice
is well-studied. See, for example, the research conducted by David Deming on Charlotte-Mecklenburg
48

schools. Bergman (2016) investigates another type of inter-district choice involving transfers in
California schools. Typically, open enrollment and transfer policies are well known and formal
application processes have been established. Less known are intra-district choice policies that require
some form of administrative approval. For example, a superintendent may need to grant approval before
parents can send a child to another school in a district, or, within a specific district, principals from
sending and receiving schools may have to agree upon a child moving from one school to another. These
types of intra-district choice might be viewed as more restrictive than open-enrollment policies. For
school districts that place strong restrictions on intra-district choice, it may not be viewed as
unreasonable that teachers, receptionists, front desk and office managers may be neither aware of the
policy nor its eligibility requirements.
We restricted our attention to states with the most charter schools such as California, Texas,
Florida, Arizona, Ohio, Michigan, New York, Nevada, and the District of Columbia. For each of these
states, we then identified the top 40 school districts in terms of student enrollments. Some states, such
as Arizona, have mandatory uniform intra-district school choice policies. Uniformity in implementation
of intra-district school choice laws simplified the process of selecting school district geographic areas to
include in our sample because we did not have to check choice policies district by district. Other states,
such as Tennessee, allow intra-district choice but it is only practiced among its largest school district
located in Nashville. National Center for Education Statistics Table 4.2 identifies more than 25 different
types of policies across states with open enrollment policies.31
For each state, our research assistants visited school districts’ websites to learn about their intradistrict enrollment policy. Once we determined that a school district within a state practices intradistrict choice, we made the decision to include that school district’s geographic area in our sampling
frame (provided there are charter schools within the catchment area).
The NCES data used are for the 2014-2015 school year. Although these are not the latest files from
the Common Core of Data, at the time we defined the sampling frame, they were the most recent files
that are complete.
For each charter school, research assistants found the nearest traditional public school. We matched
the charter school to the nearest traditional public based on the type of school (i.e., regular, special
education, etc.) and the entry grade level.
We kept charter schools in our sample (and their respective matched pair) if the charter school
enrollment was greater than 200 students. We also limited the number of schools we attempted to
contact in a single school district to 80.
In order to implement the experiment, we created 24 domains and developed distinct websites for
31

See Table 4.2: “Numbers and types of open enrollment policies, by state: 2017.”

49

each domain. For each website, we created email addresses that signal ethnic-sounding names as well
as whether a fictitious parent is male or female. When sending emails to schools, we used Tor browsers
to obscure our IP address and make it appear that the email was being sent from a local parent. For
our study, we did not observe any increase in website visits based on the implementation of the
experiment.

50

Appendix C.
Content Analysis Methodology
Content analyses turn unstructured textual content into quantitative data that can be analyzed
as such. Specifically, features of text are converted into categorical, ordinal, or binary numbers. The
current content analysis quantified the emailed responses from charter schools and traditional public
schools subject to school choice to the inquiring “parents,” as described above.
A data scientist who specializes in text analytics and the intersection of qualitative and
quantitative research methods oversaw the content analysis utilized in this research. Specifically, the
researcher created a codebook based on the research hypotheses and a grounded textual analysis of
sample responses from the schools to the parents that creates quantitative variables from the text
regarding the content of the email responses. The codebook continued to be adapted during preliminary
coding.
The content analysis coded for two variables of interest. For whether the response was considered
encouraging or discouraging, coders rated this on a four-point scale: very discouraging, somewhat
discouraging, somewhat encouraging, and very encouraging. And for whether the response was
considered to evoke the likelihood a coder would apply to the charter school or traditional public school,
which also was coded on a four-point scale: very unlikely, somewhat unlikely, somewhat likely, and very
likely.
The researcher also oversaw the training of four independent coders. Training included coding
several emails as a group, working through any questions or differences of opinion on how to code.
Then, the coders and researcher each independently coded the same 25 emails. The results were
compared, with any discrepancies worked through as a group and resolved. All four coders then recoded
the same 25 responses to ensure consistent coding. The codebook creates quantitative variables from
the text regarding the content of the email. With consistent quality established, the group then
proceeded to code a new set of 100 responses. Inter-coder reliability was calculated in several ways
across each coded variable in order to ensure coders were consistent in their approach to coding.
Inter-coder reliability was calculated in four ways. First, average percent agreement is simply the
percent of cases agreed upon across all coders. Scores above 80% are considered reliable for most studies,
while scores above 70% are considered valid in exploratory studies. In this study, the mean average
percent agreement across all coded variables is 96.8%. Second, Pairwise Cohen’s Kappa is similar to
average percent agreement but also factors in values coded by chance, with scores ranging from -1 to
1. Generally, .41 to .60 is considered moderate reliability, .61 to .80 substantial reliability, and .81 to
1.0 almost perfect reliability. The average across all variables in this study is .92. Fleiss Kapp, which
also ranges from -1 to 1, is if the observed agreement is less than expected, while also penalizing 100%
agreement. Above .75 is considered excellent. In this study, the average Fleiss’ Kappa across coded
51

variables is .92. Finally, Krippendorf’s Alpha measured observed and expected agreement, and is
considered the gold standard of inter-coder reliability. Scores above .80 are considered adequate. The
average across all variables coded in this study is .92.

52

