NBER WORKING PAPER SERIES

THE DOZEN THINGS EXPERIMENTAL ECONOMISTS
SHOULD DO (MORE OF)
Eszter Czibor
David Jimenez-Gomez
John A. List
Working Paper 25451
http://www.nber.org/papers/w25451

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
January 2019

This paper is based on a plenary talk given by John List at the Economics Science Association in
Tucson, Arizona, 2016, titled “The Dozen Things I Wish Experimental Economists Did (More
Of)”. We are grateful for the insightful comments provided by Jonathan Davis, Claire
Mackevicius, Alicia Marguerie, David Novgorodsky, Daniel Tannenbaum, and by participants at
a seminar held at the University of Alicante and at the Barcelona GSE Summer Forum’s
workshop on External Validity. We thank Ariel Listo and Eric Karsten for excellent research
assistance. Eszter Czibor gratefully acknowledges support from the Netherlands Organisation for
Scientific Research (NWO) through the Rubicon research grant. The views expressed herein are
those of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2019 by Eszter Czibor, David Jimenez-Gomez, and John A. List. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.

The Dozen Things Experimental Economists Should Do (More of)
Eszter Czibor, David Jimenez-Gomez, and John A. List
NBER Working Paper No. 25451
January 2019
JEL No. C9,C90,C91,C92,C93,D03
ABSTRACT
What was once broadly viewed as an impossibility – learning from experimental data in
economics – has now become commonplace. Governmental bodies, think tanks, and corporations
around the world employ teams of experimental researchers to answer their most pressing
questions. For their part, in the past two decades academics have begun to more actively partner
with organizations to generate data via field experimentation. While this revolution in evidencebased approaches has served to deepen the economic science, recently a credibility crisis has
caused even the most ardent experimental proponents to pause. This study takes a step back from
the burgeoning experimental literature and introduces 12 actions that might help to alleviate this
credibility crisis and raise experimental economics to an even higher level. In this way, we view
our “12 action wish list” as discussion points to enrich the field.

Eszter Czibor
Department of Economics
University of Chicago
1126 East 59th
Chicago, IL 60637
eczibor@uchicago.edu
David Jimenez-Gomez
Department of Economics
University of Alicante
Spain
davidjimenezgomez@ua.es

John A. List
Department of Economics
University of Chicago
1126 East 59th
Chicago, IL 60637
and NBER
jlist@uchicago.edu

“There is a property common to almost all the moral sciences, and by which they are distinguished
from many of the physical... that it is seldom in our power to make experiments in them”, Mill (1836,
p.124).
“Unfortunately, we can seldom test particular predictions in the social sciences by experiments explicitly designed to eliminate what are judged to be the most important disturbing influences. Generally,
we must rely on evidence cast up by the ‘experiments’ that happen to occur”, Friedman (1953, p.10).
“Economists cannot make use of controlled experiments to settle their differences: they have to appeal
to historical evidence”, Robinson (1977, p.1319).
“The economic world is extremely complicated. There are millions of people and firms, thousands of
prices and industries. One possible way of figuring out economic laws in such a setting is by controlled
experiments... like those done by chemists, physicists, and biologists... Economists have no such luxury
when testing economic laws. They cannot perform the controlled experiments of chemists or biologists
because they cannot easily control other important factors. Like astronomers or meteorologists, they
generally must be content largely to observe”, Samuelson and Nordhaus (1985, p.8)

Introduction
The give and take between theory and data in the natural sciences is so ingrained in modern
thought that an integral part of the scientific method – that theories must be tested against
experimental evidence – is now second nature. This fact, of course, was not lost on the icons
of economics, many of whom felt compelled to express their anguish by comparing empirical
approaches across the social and natural sciences. The common thread in the epigraph musings
is that if economists desire to do experimentation they should choose another practice, and
if they want to engage in empirical economics, they should start looking for available naturally occurring data. This is presumably because the writers believed that it was impossible
to collect/learn from experimental data in economics. These general feelings were shared ubiquitously throughout the 19th and 20th centuries, as extracting knowledge from historical data
and personal introspection represented the primary source, and indeed in most cases the sole
source, of new empirical knowledge in economics.
The economic landscape is changing. In the past several decades constructing new approaches to generate data have opened up several avenues for a fresh approach to understanding the economic relationship of theory and data. Whether by lab or by field, the popularity
of experiments in economics has steadily increased, in large part due to the advantages they
offer in identification, control, statistical inference, and interpretability. Properly constructed
experiments take the analyst beyond measurement, into the “whys” of the causal relationship.
It is often within these “whys” where the deep theoretical understandings or the key policy
takeaways reside (see, e.g., List (2004) on using field experiments to understand the nature of
discrimination observed in markets).

2

While many would consider using randomization in the lab and the field as an unequivocal success in moving economics to a new scientific level, recently critics in the broader social
sciences have called for the movement to proceed more cautiously. As Maniadis et al. (2017)
point out, an active debate has surfaced that claims there is a “credibility crisis” in several
scientific disciplines, including psychology (Nosek et al., 2012), management (Bettis, 2012), and
several branches of the biological and human sciences (e.g., Jennions and Møller (2003); Ioannidis (2005)). While the crises take many forms, one common widespread concern revolves around
reproducibility, with the nature of ‘false positives’ representing a particular concern.
This literature motivated us to step back from the evidence-based movement and ask a
simple question: if we could gather social scientists in a room and had the goal of enhancing
knowledge discovery, what advice would we give to experimental researchers? We group our list
of 12 recommendations into three bins. We begin with the general decision concerning what
data to acquire. We proceed to focus on how to generate and create useful information via experimentation. We conclude with how to interpret, build on, and scale the initial experimental
evidence. This thought experiment yields a wish list of 12 things that we hope experimental
economists will do more of in the future. We summarize these dozen items under three broad
questions.
What data should we acquire? The first item in this bin is to use lab and field experiments
as complements to naturally-occurring data. This is important because the three major sources
of empirical evidence that we consider – lab experiments, field experiments, and naturallyoccurring data – each provide different parameters of interest (see Al-Ubaydli and List (2015)).
Naturally what falls out of this discussion is our second item: a call for researchers to more
carefully consider control and generalizability when making the data acquisition choice. The
first bin closes with our third wish: run more field experiments, especially natural field experiments. This is an outcome not only because natural field experiments are relatively new
compared to the other empirical approaches, and therefore much ground is untrodden, but also
because they provide unique elements – randomization and realism – that the other approaches
have difficulty combining by their very nature.
How should we generate data and interpret information from experiments? This second bin
collects thoughts on optimal steps for the researcher who has decided to conduct an experiment
to generate information. This bin includes a description of how to make use of more than pvalues and measured effect sizes when making inference. Complementing that interpretational
question is a discussion of proper replication. Replication was a leg of Fisher (1935) experimental tripod and represents a signature issue within the credibility revolution and we simply
echo that point here. Other issues that we discuss in this second bin are considering statistical
power in the design, using blocking, adjusting for multiple hypothesis testing (a common reason

3

for false positives) not only in our data analysis but also in our designs, and creating interventions to address heterogeneity using within-subject variation when necessary and appropriate.
Our second bin includes five necessary design elements that critically determine what, and how
much, we can learn from the data generating process.
How can we most usefully impact policy? Our last bin revolves around the area of how
experimentalists can most usefully impact policymaking. Perhaps surprisingly, this discussion
begins with advocating for a deeper use of theory to motivate designs by going beyond typical
A/B tests. This approach importantly allows the analyst to determine the “whys” behind a
result, leading to more effective policies. In addition, it helps to maintain fidelity of the program
when policymakers scale the intervention. Scaling needs science in and of itself, and that science
should be considered early in the experimental design phase. Complementing this discussion is
a plea for experimentalists to go beyond measurement of short-run substitution effects to focus
also on long run effects–these are behavioral effects that policymakers often find of great import
but are often not provided in the literature.
The remainder of our study proceeds as follows. The next section presents preliminaries
and sets the stage for development of our three bins. We then describe our views on what data
to acquire, how to generate data and create useful information via experimentation, and how
to interpret, build on, and scale the initial experimental evidence. This discussion yields our
dozen ideas that we hope experimental economists will do more of in the future. We conclude
with summary thoughts.

Preliminaries: experiments, estimation and randomization
This section offers an overview of the most relevant concepts to be applied throughout the
paper (readers familiar with the inferential basics of economic experiments may wish to skip
this overview, and start directly at Section 1). We begin by defining our subject of interest.
In principle, we consider an experiment broadly: a study that generates primary, or original, data in a controlled environment. This inclusive definition permits the original studies of
early experimentalists to be classified as experiments as well as important studies that exploit
controlled environments but use non-experimental variation as the primary means of identification.1 While we view experiments broadly, for the purposes of this study, we take a narrower
definition of experiment by considering only those studies where researchers generate primary
data by using randomization to identify a causal relationship.2 Accordingly, there is a clear
1

Examples include studies that, instead of comparing outcomes between a treated and a control group, make
comparisons along pre-existing traits of their subjects, such as their gender, age, religion, occupation, etc. Consider, for instance, Koudstaal et al. (2015) who study differences in risk attitudes among entrepreneurs, managers
and employees.
2
Our definition is in the spirit of Shadish et al. (2002), who define an experiment as “a study in which an
intervention is deliberately introduced to observe its effects”, and a randomized experiment, in addition, must
be such that “units are assigned to receive the treatment or an alternative condition by a random process”.

4

difference between these type of data and data obtained from so-called natural experiments,
where subjects are randomly allocated to different treatment groups by a process outside of the
researcher’s control (such as the draft lottery in Angrist, 1990); or from quasi-experiments,
in which subjects are not randomly assigned to treatments (Greenstone and Gayer, 2009). The
interested reader can find more on the different types of experiments in Shadish et al. (2002).
In the following, we present a framework intended to guide our discussion of experiments,
formalizing the most important concepts used in the paper. Individual i has covariates xi .
The experiment has characteristics ω, where ω includes the subject population (university students, CEOs, etc.), context (artificial vs. natural), time horizon (long vs. short), and other
characteristics. The experiment consists of the following stages:
• Let pi be an indicator variable for the participation decision such that pi = 1 if subject
i chooses to participate in the experiment, and pi = 0 otherwise.
• Let zi denote assignment to treatment. For example, zi = 1 if student i is assigned to
a small class size (zi will be random in the experiments we discuss).
• Let di be the treatment status, which is the treatment individual i actually receives
(e.g. di = 1 if student i actually attends a small class). Note that it is possible that zi
and di are different.3
• Let yi1 be the outcome of interest (e.g. the child’s test scores) when treatment status
is di = 1, and yi0 when treatment status is di = 0.4
We follow the potential outcomes framework, in which an individual i has outcome yi1
in the treated group and yi0 in the control group. Ideally, when conducting an experiment,
researchers would like to measure individual treatment effects for each individual i, yi1 − yi0 ,
which is the difference in outcomes for individual i being in the treated versus the control group.
In practice, of course, they cannot observe both of these outcomes; instead they can only observe
individual outcomes in one of the treated states, and the counterfactual outcome in the other
state remains unobserved. Instead of individual treatment effects, researchers therefore usually
consider the Average Treatment Effect (ATE), given by τ ∗ = E[yi1 − yi0 ]. The ATE
measures the average difference in the outcomes for the population. The ATE τ ∗ is not directly
observable either; instead researchers estimate τ , defined as:
τ = E[yi1 |di = 1] − E[yi0 |di = 0].
3

There are two possible cases: (1) The subject is assigned to a treatment z, such as a voucher to enroll in
training, that is of a different nature than the treatment status, which is whether the subject actually enrolled
in treatment. In that case, Z 6= D. (2) Alternatively, the subject is assigned to a treatment z, which is already
one of the potential treatment statuses. For example, the subject is assigned to a training course z = 1 or not
z = 0. Subjects can still opt in (d = 1) or out (d = 0) of the training course, and in this case Z = D.
4
Our framework follows the tradition of Rubin (1974), which can traced back to the work of Jerzy Neyman;
see also Freedman (2006).

5

Estimate τ measures the difference between the average effect of the treatment on those
who were treated and the baseline average outcome of those who were not treated. As it will
become clear below, when: 1) d is randomly assigned, 2) subjects do not opt in or out of their
assigned treatments, and 3) potential outcomes of an individual are unrelated to the treatment
status of any other individual, then τ = τ ∗ .5 Note that the ATE does not allow researchers to
estimate the percentiles of the distribution of treatment effects, or other moments such as the
variance (we discuss estimating heterogeneous treatment effects in Section 9) and, unlike measures based on percentiles such as the median, the ATE is sensitive to outliers, i.e. observations
whose value greatly differs from the rest (Deaton and Cartwright, 2018). Note also that the
“experiment population” is not necessarily a random sample of the entire population and may
be selected according to observables; in that case, we only learn the effect of the treatment on
the particular sub-population from which the sample is drawn (Duflo et al., 2007), an issue we
discuss in detail in Section 1.
In the absence of randomization, researchers estimate
τ = E[yi1 |di = 1] − E[yi0 |di = 0] = E[yi1 − yi0 |di = 1] + E[yi0 |di = 1] − E[yi0 |di = 0]
|
{z
} |
{z
}
ATE on the treated

selection bias

A non-zero selection bias term in the previous equation indicates that those who select
into treatment are different in the untreated state from those who do not sort into treatment.
This happens, for example, if smokers who are more motivated to quit are more likely to enroll
in a smoking cessation treatment than those who are unmotivated: in such a case, we end
up with program participants who are inherently different (more motivated) than those who
did not take up the program, leading to a biased (in our case, overoptimistic) estimate of the
program’s effect on quitting. In order to rule out selection bias, it is necessary to make certain
assumptions, such as the Conditional Independence Assumption (Rosenbaum and Rubin, 1983):
{yi0 , yi1 } ⊥⊥ di |xi ,
which claims that the outcome in each state and the assignment to treatment for a given
individual are independent conditional on the observable covariates. Intuitively, the Conditional
Independence Assumption means that conditional on the observables xi , the assignment to
treatment is as good as random, and it implies that E[yi1 |xi , di = 1] − E[yi0 |xi , di = 0] =
E[yi1 − yi0 |xi ], and therefore that τ = τ ∗ .
Crucially, random assignment to treatment automatically implies the Conditional Independence Assumption and hence solves the issue of selection bias (Duflo et al., 2007). As such, the
most important reason why researchers (not just economists) use randomization is because it
allows causal inference under potentially weaker assumptions than alternative methods. Randomization serves as a novel instrumental variable (IV), balancing unobservables across control
5

The third assumption is the “Stable Unit Treatment Value Assumption” (Angrist et al., 1996; Duflo et al.,
2007), which assumes away any spillover effects.

6

and treatment groups (Al-Ubaydli and List, 2013).6
Studies based on randomization also have the advantage of being easily replicable, in contrast to methods that rely on baseline covariates to assign treatments without randomization.7
Randomization also accomplishes three other crucial things: first, it prevents the experimenter
from allocating subjects to treatment and control in ways that would bias the results (for example, politicians assigning their constituents to a “schooling” treatment, or physicians assigning
patients with higher perceived need to treatment). Second, it provides a credible way to measure treatment effects, because it is a straightforward calculation where researchers have little
leeway. Third, randomization is crucial in instances when fairness and transparency are a concern, because it ensures that there is no favor/discrimination towards particular groups.8 In
sum, while randomization does not solve all problems of causal inference, when proposing alternatives to randomization in experiments researchers should be very precise about the exact
details of the alternative they propose, or else there is a danger in underestimating the value of
experimentation (Senn, 2013).
Throughout our paper, we follow the taxonomy for experiments developed by Harrison and
List (2004), who identify four general categories, as summarized in Table 1. Laboratory
experiments study university students as subjects in an artificial environment i.e. the lab.
For example, Goeree and Holt (2001) have university students play a series of games, showing how the predictive power of the Nash equilibrium is not robust to (supposedly innocuous)
changes in payoffs in those games. Artefactual Field Experiments (AFE), also known as
lab-in-the-field, share most of the characteristics of lab experiments (such as having an artificial
environment), but use the relevant population of interest as subjects. For example, Levitt et al.
(2009) used chess players at two international open tournaments to gather data on strategic
behavior on some well-known games. Framed Field Experiments (FFE), like AFE, use the
relevant population as subjects, but take place in a natural environment, such as the market,
school, hospital, etc. For example, Gosnell et al. (2017) incentivized airline captains to improve
efficiency and save fuel (via performance information, personal targets, and prosocial incentives), and the pilots were aware that an experiment was taking place. Note that all three types
of experiments described above are overt: subjects are aware of being part of an experiment.
In contrast, Natural Field Experiments (NFE) are covert: they study the relevant population in a natural setting and, crucially, subjects are not aware of being part of an experiment,
setting NFE apart from the other types of experiments, as we discuss further below. For exam6

For a discussion of some popular non-experimental methods, and their comparison to experiments, see Duflo
et al. (2007). For a comprehensive discussion of the problems of randomization, see Deaton and Cartwright
(2018).
7
We return to the topics of replicability in Section 5 and optimization-based methods (e.g. Kasy, 2016) in
Section 8.
8
Note that methods other than randomized experiments can achieve this goal too, see Deaton and Cartwright
(2018); Kasy (2016); Banerjee et al. (2017b). We discuss scaling up further in Section 12.

7

Population we study
Environment
Type of awareness
Who do we observe?

Lab
U
A
O
pi = 1

AFE
S
A
O
pi = 1

FFE
S
N
O
pi = 1

NFE
S
N
C
All

Table 1: Summary of the characteristics of each type of experiment. Population can be
U niversity students or the Special population of interest. The environment can be Artificial or
N atural. The experiment can either be Overt or Covert.
ple, Hallsworth et al. (2015) randomized the letters sent to individuals who had debt obligations
with the government in UK (the treatment group had an extra sentence that informed them
that refusal to pay would be considered as an active choice). In this case, subjects belonged to
the relevant population and were in a natural context; moreover they were not aware of being
part of an experiment.9
In sum, we can define the relevant estimates from lab, AFE, FFE and NFE as:
τ lab = E[τ |i ∈ U, e = A, t = O, p = 1],
τ AF E = E[τ |i ∈ S, e = A, t = O, p = 1],
τ F F E = E[τ |i ∈ S, e = N, t = O, p = 1],
τ N F E = E[τ |i ∈ S, e = N, t = C],
where U and S refer to students vs. a special population, the environment e can be artificial
(A) or natural (N ), the type t of experiment can be overt (O) or covert (C), and p indicates
the presence or absence of an active decision to participate in the experiment.
With these preliminaries mind, we turn to the dozen things we hope experimentalists do
more of. While there is no inherent ordering by importance of our 12 ideas, we attempted to
group the topics loosely by what data to generate, how to efficiently generate and interpret the
data, and how to give the most informative advice to evidence-based policymakers.

1

Appropriately consider generalizability and control, across
the lab and the field

When designing an experiment, researchers need to balance two key aspects that determine the
value of their contribution to science and policy. One aspect is correct statistical inference,
including internal validity (the property of being able to identify the parameters of interest in
a given design) and informativeness (how much a result can change the prior of the scientific
community). The second is generalizability (also known as external validity): whether a
causal relationship continues to hold when subjects, context, location, or treatment details are
9

Randomized Controlled Trials (RCT) would fall under either the FFE or the NFE classification, depending
mainly on whether subjects are aware of being part of an experiment or not.

8

modified (Shadish et al., 2002). This section outlines a framework for discussing threats to generalizability, building on the basic ingredients introduced in the Preliminaries. In the following
two sections we use this framework to evaluate the different types of experiments (laboratory,
artefactual field, framed field and natural field experiments) as defined in the Preliminaries. In
what follows, we use the term generalizability instead of external validity, following Harrison
and List (2004).
The question of generalizability has long been studied in the social sciences, but has been
often obfuscated, especially in non-experimental research, by the more pressing problem of
internal validity (Al-Ubaydli and List, 2013; Deaton and Cartwright, 2018). While internal validity is necessary for generalizability, it is not sufficient (Duflo et al., 2007). In economics, the
“Lucas critique” (Lucas, 1976) famously tackled the issue of generalizability, by arguing against
econometric policy evaluations that failed to recognize that agents’ behavior varies systematically with changes in policy.10 More recently, a new literature on “generalizability theory”
has grown within psychology and economics (Briggs and Wilson, 2007; Higgins and Thompson,
2002; Al-Ubaydli and List, 2013).11 In order to improve generalizability of research findings,
it is useful to differentiate the potential threats to generalizability according to their causes.
We have identified four potential threats to generalizability that we discuss below: interaction
between treatment and other characteristics of the experiment, selective noncompliance, nonrandom selection into the experiment, and differences in populations.
Threat I: Characteristics of the experiment. Characteristics inherent to the experiment can inadvertently affect outcomes and incorrect interpretation of results. In all experiments, but especially in overt ones, y will be affected by the elements of ω, such as scrutiny,
stakes, the time horizon in which the intervention takes place, and whether the environment
is artificial or natural (Deaton and Cartwright, 2018). Generalizing our estimates to settings
where those parameters are different threatens the validity of our estimates.12 Overt experiments in which subjects are aware of being part of an experiment such as lab experiments,
artefactual field experiments (AFE), and framed field experiments (FFE) have characteristics
that make them particularly prone to this threat to generalizability. The high level of scrutiny
present in overt experiments potentially induces Hawthorne and John Henry effects13 as well as
10

In particular, the Lucas critique censured using estimates from past data to forecast the effects of a new policy,
because the behavior of the agents will change in response to the implementation of the new policy, invalidating
those estimates (Ljungqvist, 2008). The interested reader will also find Goodhart’s Law and Campbell’s Law as
two social science contemporaries.
11
See Briesch et al. (2014) for an introductory article to generalizability theory. Vivalt (2017) used techniques
from generalizability theory to perform a meta-analysis of 20 types of intervention in economic development,
collected from 635 papers, and found that results are more heterogeneous than in other fields such as medicine.
Within generalizability theory, there is also an intriguing approach that attempts to generalize by establishing
networks of causality (Bareinboim and Pearl, 2013).
12
This can be seen from the fact that definition of the ATE of each type of experiment (τ lab , τ AT E , τ F F E and
NF E
τ
, presented in the Preliminaries) depends on the characteristics in ω.
13
The Hawthorne effect is defined by the Oxford English Dictionary as “an improvement in the performance
of workers resulting from a change in their working conditions, and caused either by their response to innovation
or by the feeling that they are being accorded some attention”; for a review and a re-analysis of the data from

9

“experimenter demand effects”, such that subjects attempt to behave in the way they believe
the experimenter wants them to (de Quidt et al., 2018).14 Several studies have found that
scrutiny is an important determinant of e.g. pro-social behavior (Bandiera et al., 2005; List,
2006b; Benz and Meier, 2008; Alpizar et al., 2008).15
Threat II: Selective noncompliance. We define as noncompliance instances when subjects end up receiving a different treatment than what they were initially assigned to (that is,
zi 6= di ), either by omission or by commission. Noncompliance is especially problematic when
subjects actively change their treatment, e.g. because they derive higher utility from a treatment
other than they were assigned to, causing what is known as a selection problem (Heckman,
2010, see also Footnote 3).16 Let Z be the set of assignments to treatment in the experiment,
and D the set of treatment statuses in the experiment, so that zi ∈ Z and di ∈ D. In the most
general framework, subject i is assigned to treatment zi , and there is a selection function that
determines which treatment status di the subject ends up with. For example, subject i has
di = arg max ˆ
u(xi , ω, dˆi ) − C(xi , zi , dˆi ), where u(xi , ω, di ) is the subject’s utility of being in
di ∈D

treatment status di , and C(xi , zi , di ) is her cost of choosing di conditional on being assigned to
zi . In these cases, the researcher assigns zi , and then the subject chooses di to maximize her
utility net of switching costs.17 As a result, we may observe zi 6= di for some individuals. In the
case of imperfect compliance, we have that τ ∗ = E[yi1 − yi0 ] 6= E[yi1 |di = 1] − E[yi0 |di = 0] = τ .
When researchers cannot obtain the ATE due to noncompliance, they can instead estimate
the “Policy Relevant Treatment Effect” (which, in the case when zi is uncorrelated with yi ,
coincides with the “Intention to Treat Effect” (ITT), Heckman, 2010). The ITT might be the
relevant estimate in some situations, because it provides researchers with a measure of how
much the intervention is “converting” into outcomes, as it considers the difference in outcomes
between those who were initially assigned to treatment and the control group, irrespectively
of whether they complied with their treatment assignment. Researchers can also estimate the
“Local Average Treatment Effect” (LATE), Angrist and Imbens (1994):
(1)

LATEp=1 = E[yi1 − yi0 |ω F F E , di (zi = 1) = 1, di (zi = 0) = 0, pi = 1],
where pi refers to the decision of participating in the experiment (see Preliminaries). The

the original Hawthorne experiment, see Levitt and List (2011). The John Henry effect refers to subjects exerting
greater effort because they treat the experiment like a competitive contest (Horton et al., 2011).
14
See de Quidt et al. (2018) for a methodological approach to bounding the experimenter demand effects.
15
Camerer (2015) argues that scrutiny is not likely to affect subject’s behaviors, based on the fact that subjects
cannot usually guess the purpose of the study (Lambdin and Shaffer, 2009), or that people are usually observed
when making economic decisions (Falk and Heckman, 2009). However, we believe that the scrutiny in overt
experiments is of a much higher degree than what subjects normally experience, and that it is likely to affect
behavior directly, even if subjects cannot correctly guess the purpose of the study.
16
We highly recommend the recent paper by Kowalski (2018) who argues that rather than considering it a
nuisance, researchers could treat this type of selection as a useful source of information that can be combined
with assumptions to learn about the external validity of their experiment.
17
Note that whether subjects solve this maximization problem ex-ante (so that they sort into treatment groups)
or ex-post (they switch treatment groups) can have consequences for estimation (Heckman, 2010).

10

LATE measures the average treatment effect for individuals induced into treatment di = 1 by
a change in zi (Heckman, 2010). Note, however, that the average treatment effect measured by
the LATE is only valid for that particular subpopulation (the compliers), and might differ from
the ATE for the whole population, limiting its generalizability.18
An extreme case of non-compliance would be attrition, in which subjects leave the experiment (and are therefore not observable to the experimenter). While random attrition only
reduces power, attrition that is not random can bias the results (Duflo et al., 2007), for example
when those individuals who are the most motivated leave the experiment if they are not assigned
to a certain treatment. The best approach to solving inference problems related to attrition is
to design the experiment in a way that allows researchers to track subjects even if they leave
the experiment (for more details, see Duflo et al., 2007), or conduct a natural field experiment.
Threat III: Non-random selection into the experiment. Recall that pi refers to the
decision of participating in the experiment. As we have seen above, in lab experiments, AFE,
and FFE, the estimates are only valid for those individuals who select into the experiment
(those with pi = 1). Because of that, the ability of such experiments to identify parameters of
interest depends on assumptions about the individual’s selection into the experiment. There
is a threat in introducing participation bias from the decision to participate in the experiment, which arises when participation into the experiment is non-random, but is the result of
a cost/benefit analysis by the subjects (Al-Ubaydli and List, 2013; Slonim et al., 2013). This
is not just a theoretical concern: Slonim et al. (2013) found that, from a population of roughly
900 university students, those who selected into lab experiments had less income, more leisure
time, more interest in economics and were more pro-social in the dimension of volunteering, all
of which are consistent with participation being the result of a cost/benefit decision.
Recall that the parameter of interest is the ATE for the whole population: τ ∗ = E[y1i − y0i ].
Overt experiments, however, provide the following estimate: E[y1i − y0i |pi = 1]. Note that this
bias can arise even if one is conducting a standard lab experiment and the effect we are looking
for can reliably be found in university students, in an artificial environment, with low stakes
and with scrutiny (so the first threat to generalizability is not a concern), and even if zi = di
for all individuals (so the second threat to generalizability is not a concern either). The ATE
τ ∗ is given by: τ ∗ = P[pi = 1] · E[y1i − y0i |pi = 1] + P[pi = 0] · E[y1i − y0i |pi = 0]. Because
P[pi = 0] = 1 − P[pi = 1], we can compute the participation bias:
(2) E[y1i − y0i |pi = 1] − E[y1i − y0i ] = P[pi = 0] × (E[y1i − y0i |pi = 1] − E[y1i − y0i |pi = 0]).
|
{z
}
|
{z
}
participation bias

treatment specific selection bias

In other words, participation bias is the product of the probability of not being in the
18
For a more detailed account of LATE, and the conditions for its use, see Angrist and Imbens (1994) and
Heckman (2010).

11

experiment P[pi = 0] and the Treatment Specific Selection Bias (which is analogous to the
classical selection bias, except that the selection is with respect to participation in the experiment, Al-Ubaydli and List, 2013). Participation bias is important, because organizations who
agree to collaborate with researchers in an experiment are usually exceptional (Banerjee et al.,
2017a). Consider the example of Behaghel et al. (2015), who analyzed an experiment in which
French firms could opt into an experiment that would randomize whether they received resumes from applicants in an anonymized form or not. They found the counterintuitive result
that anonymizing resumes hurt applicants from minorities at the selection stage. They argue
that this is the result of selection into the experiment: those firms who chose to participate
were firms that already treated applicants from minorities better than those firms who opted
out (for example, by viewing with more leniency unemployment periods in minority resumes
due to the adverse conditions those individuals face in the labor market), and the fact that
resumes were anonymized precluded those firms from positively discriminate towards minority
applicants. This a clear case where Treatment Specific Selection Bias is important.
Because in general P[pi = 0] is very large (usually close to 1), then the bias in the estimate
will be determined mainly by the Treatment Specific Selection Bias.19 Participation bias will
not be an issue in overt experiments when E[y1i − y0i |pi = 1] ≈ E[y1i − y0i |pi = 0]. This
will happen when pi is independent of yi (because selection does not depend on xi , or because
selection depends on some subset of xi which are in turn independent of yi ). The following
condition for overt experiments guarantees that the Treatment Specific Selection Bias will be
zero:20
{yi0 , yi1 } ⊥⊥ pi |xi

(Generalizability Independence Condition).

When the independence condition does not hold (as in Behaghel et al., 2015), researchers
must explicitly consider selection into the experiment, in order to derive general conclusions.
For example, risk averse individuals might be less likely to enroll in an experiment (Al-Ubaydli
and List, 2013; Heckman, 2010). In that case, NFEs are much more useful, because we can
recover E[y1i − y0i ] through them without further assumptions, since there is no selection into
the experiment (Al-Ubaydli and List, 2013). In this sense, contrary to conventional wisdom,
field experiments have the potential for more control, and not less, than lab experiments. We
return to this point in Section 2.21 Note that even when researchers manage to have a pool
of subjects that satisfy the Generalizability Independence Condition (GIC, so that pi is not
correlated with outcomes), they can only generalize to pi = 0 for the subpopulation they draw
subjects from, but not necessarily to other populations (Deaton and Cartwright, 2018). For
example, even if researchers managed to collaborate with an NGO that has access to a large
19

A fact anticipated by Slonim et al. (2013).
This condition is similar in spirit to the Conditional Independence Assumption (Rosenbaum and Rubin,
1983).
21
However, researchers might be interested in the Intent-to-Treat (ITT) or the Average treatment effect on
the Treated (ATT) estimates. If this is the case, then participation bias might not be a problem, in the sense
that researchers might only be interested in estimating effects for those who would choose to participate in the
experiment.
20

12

and representative sample of the population in California (so that the GIC would hold), they
might able to generalize to those with pi = 0 in California, but not necessarily to the population of Massachusetts or France. We discuss this issue of generalizing to other populations next.
Threat IV: Different populations. There are two types of heterogeneity in the population that researchers should consider. The first type is heterogeneity in treatment effects across
subjects in the study (or in the population from which the sample was drawn). We discuss
how to deal with this type of heterogeneity, through blocking (Section 8) and within-subject
design (Section 9). There is a second (and related) type of heterogeneity which is a threat
to generalizability: how a population different from the one in our experiment would react to
the same treatment (Athey and Imbens, 2017). First, note that if the subject population was
a random sample of the “population of interest”,22 then the estimates of the Average Treatment Effect generalize to the entire population. Instead researchers often rely on “convenience
samples” which are easily accessible to the research team, but the estimates they provide do
not necessarily generalize to the entire population (Duflo et al., 2007; Deaton and Cartwright,
2018). This problem has been traditionally exacerbated in lab experiments, where subjects
were almost always from W.E.I.R.D. social groups (Western, Educated, Industrialized, Rich
and Democratic, Henrich et al., 2010a; Henrich and Heine, 2010). Even behavior in a stylized
and simple game such as the Ultimatum Game exhibits substantial heterogeneity across populations, as seen in a series of AFE conducted in small-scale societies across the world (Henrich
et al., 2001). This problem of the representativeness of the population is pervasive and not
confined to economics: subjects in randomized clinical trials for new drugs are not necessarily
a random sample of the population of interest, but healthier individuals than the population
on which the drugs will be used (for example, Travers et al., 2007, found that less than 10% of
asthma patients surveyed qualified for a clinical trial of an asthma medication).23
One especially important dimension of generalizability across populations is gender: either
across men and women, or from one gender to the entire population. Recent years have established a rich and robust literature documenting gender differences in response to a variety
of incentive schemes, most notably along the dimensions of competition and risk (Croson and
Gneezy, 2009), supporting the claim that conclusions drawn from the behavior of members of
one gender are unlikely to generalize to the other.24 The issue of gender becomes even more
complex as we take into account the interaction with other covariates. For example, there is
evidence that women’s preferences over competition change with age such that the gender gap
22
The population of interest is the one for which we want to find out the treatment effect; for example the
potential target of a policy.
23
For an entertaining and interesting discussion of heterogeneity in clinical trials, we recommend listening to
(or reading the transcript of) the episode “Bad Medicine, Part 2” of the Freakonomics podcast.
24
Another very stark example concerns the case of clinical trials in the US. In the late 1950s and early 1960s,
a drug called thalidomide caused birth defects in hundreds of newborns in a number of countries (Lenz, 1988).
Although thalidomide was mostly avoided in the US thanks to Frances Oldham Kelsey at the Food and Drug
Administration (FDA Bren, 2001), more stringent regulations were passed that summarily excluded women from
participation in clinical trials (Food and Drug Administration, 1997, 2017). Partly as a consequence of those
regulations, 8 out of 10 drugs pulled from the market by the FDA in the years 1997-2000 had worse adverse
effects for women (Heinrich, 2001).

13

in competition (men tend to be more competitive), while large among young adults, disappears
in older populations (Flory et al., 2018).
In sum, we urge researchers to consider carefully and understand the generalizability of
their results, and to design their experiments in ways that tackle these four threats to the
greatest extent possible. Nevertheless, we also argue that while generalizability is important to
understand and model, a needless self-destructive overreaction to the generalizability problem
should not hinder scientific pursuits. Taken to the extreme, no empirical exercise is perfectly
generalizable so the perfect should not be the enemy of the good (i.e., journals constantly
rejecting excellent empirical work on the basis of external validity concerns soon devolves to a
reductio ad absurdum). With that in mind, in the next section, we present an application of
our framework to a discussion of natural field experiments, and show how they can mitigate or
eliminate many potential threats to generalizability.

2

Do More Field Experiments, Especially Natural Field Experiments

For this second recommendation, we use the framework developed in Section 1 for the analysis
of generalizability to discuss the advantages and disadvantages of running field experiments.
We first argue that natural field experiments, and to a lesser extent framed field experiments,
are often less subject to the threats to generalizability than other types of experiments. We
then discuss two typically raised objections to conducting field experiments: lack of control and
higher cost, and argue that many times such arguments are confused. In doing so, we provide
approaches to tackle these issues.
The first threat to generalizability we identified in Section 1 is the change in subjects’ behavior by virtue of being in an experiment and feeling scrutinized: Hawthorne, John Henry, and
experimenter demand effects are all commonly used terms describing such potential impacts
(see Section 1 for definitions). In case of overt experiments such as lab, artefactual field (AFE)
and framed field experiments (FFE), it is often impossible to rule out these potential nuisances.
Alternatively, the fact that natural field experiments are covert (i.e. subjects are not aware of
being part of an experiment) ensures that the environment is natural and there is no sense of
scrutiny beyond that which is natural in the market of interest, ruling out such effects by design
(Al-Ubaydli and List, 2013). As a result, there are fewer threats to generalization from direct
correlation between yi and ω when using a NFE to obtain our estimates.25 Note, however, that
the manner in which most economic experiments are conducted might impart certain biases on
the part of the researchers because they are typically not double-blinded : even when subjects
are not aware of being treated, the research team (performing the data collection and statistical
25
FFE can potentially attenuate these effects by collecting data over a longer time period, which we discuss in
Section 11.

14

analysis) knows which subjects are in each group (Deaton and Cartwright, 2018).26
The second threat is selective noncompliance: the case when the probability of ending up in
a treatment group differs depending on the initial assignment to treatment versus control. In
lab experiments, AFEs, and FFEs, noncompliance with one’s treatment assignment is typically
only possible through leaving the experiment entirely (DellaVigna et al., 2012). Similarly, in
NFE subjects are unaware of being assigned to a certain treatment and are thus unlikely to
actively try to change their assignment. Switching to a different condition or opting out is often
impossible by design (think of Lyft consumers who are randomized into a high or low price for
a ride–they receive that price and decide whether to purchase, which is the outcome variable of
interest). By their very nature, selective noncompliance is most likely to present problems in
certain FFEs.27
The third threat to generalizability, that of non-random selection into the experiment, is an
aspect where NFEs gain significant attractiveness. By virtue of bypassing the experimental participation decision of subjects altogether, there is no selection into natural field experiments by
design, something which overt experiments cannot easily avoid (Al-Ubaydli and List, 2013).28
In lab experiments, it might still be possible to avoid non-random selection into the experiment. For example, Borghans et al. (2009) sought volunteers (i.e. those with pi = 1) for their
experiment among high-school students, although the experiment was actually compulsory (and
included the volunteers). This process avoids non-random selection and also allows for measurement of participation bias, since the researchers know whether pi is 0 or 1 for all subjects in the
population. However, in practice it is often inconvenient or impossible to make participation in
a lab experiments compulsory.
In sum, NFEs are less prone to biases stemming from non-random selection into the experiment, including randomization bias (when subjects are averse to the act of randomization
itself), as well as systematic differences in the outcomes or compliance of those who select into
the experiment. However, there is an important caveat: even when subjects are unaware of
the experiment, there can be participation bias if the participation decision is made on their
26

A subtle point is that randomized controlled trials, including FFE, can potentially do a single-blind study
where subjects might be aware of the experiment but not of the particular treatments: this is the case when
subjects in the control group are given a placebo treatment which they cannot distinguish from the treatment
(Senn, 2013).
27
For a mechanism design approach to solving this issue in FFE, see Chassang et al. (2012).
28
See List (2008) about the ethical considerations behind informed consent and bypassing that decision in NFE.
He notes that it revolves around benefits and costs, where in some cases the major benefit is that there are certain
questions where the subjects’ awareness of being part of an experiment undermines the validity of the research.
For example, when we are attempting to measure the nature and extent of gender or race based discrimination
(but see Section 3 for how lab and field experiments can be combined). As List (2008) writes: “This does not
suggest that moral principles should be altogether abandoned in the pursuit of science. Quite the opposite: the
researcher must weigh whether the research will inflict harm, gauge the extent to which the research benefits
others, and determine whether experimental subjects chose the experimental environment of their own volition
and are treated justly in the experiment. Local Research Ethics Committees and Institutional Review Boards
in the United States serve an important role in monitoring such activities”. We support this point of view and
would like to emphasize that research can (and should) make participants better off and benefit society, while
preserving anonymity and not posing a risk to subject’s well-being.

15

behalf by a process that introduces correlation. For example, if firms selecting to participate
in an experiment are such that their employees share a certain characteristic that correlates
with the outcome of interest (as in Behaghel et al., 2015), this would mean that interpretation
of the results should be only over these types of firms. This is because the Generalizability
Independence Condition (GIC) derived in Section 1 is violated. In cases in which the participation decision is made on behalf of the subjects by another agent, the researchers need to
carefully consider whether the GIC holds. If it does not, then statistical intepretation should
be adjusted accordingly. This might be difficult when the researchers need to collaborate with
a number of small self-selected firms, but it can be potentially alleviated when partnering with
administrations or large firms who have access to a representative pool of subjects.
The last threat to generalizability applies when we try to extrapolate the findings of one
study to a different population. Note that, in this regard, all field experiments (AFE, FFE and
NFE) offer an advantage over traditional lab experiments, because they select the population
S of interest by design, which is usually different from traditional “W.E.I.R.D. university students” (Henrich et al., 2010b, see also the discussion of Threat IV in Section 1), such as farmers,
traders, entrepreneurs, CEOs, physicians, etc. Absent participant selection, within field experiments, NFEs do not have an inherent advantage over AFEs and FFEs, in the sense that the
population selected S for an NFE can still be very different from the population of interest S 0 ,
as would happen in AFEs and FFEs. However, NFEs offer a potential advantage because, by
collaborating with large entities, researchers can reach a large and often representative sample
of the population or the direct population of interest. As an example, consider Hallsworth et al.
(2017) who collaborated with a public administration to conduct their tax debtor experiment.
Because the population of interest S 0 over which results should generalize is often the entire
population (say, of a given country or region), running NFEs through these types of collaborations allows researchers to have a representative sample. As a result, generalization is either
unnecessary (because S = S 0 ), or it is feasible either because the subset of interest is part of the
experimental population (S 0 ⊂ S) or because the treatment effect for S 0 can be extrapolated
from a subset of S.
Al-Ubaydli and List (2013) propose a simple framework for generalizability, building on the
“all causes model” of Heckman (2000), of which we include the mathematical details in the
Appendix, and describe here the main intuition.29 There are three potential cases of generalizability: zero, local, and global generalizability. Under zero generalizability, results cannot be
generalized to any setting different from the one in which they were obtained, which is the most
conservative approach. Under local generalizability, results can only be generalized to situations that are very similar to the ones studied in the experiment. Al-Ubaydli and List (2013)
argue that under the conservative conditions of zero or local generalizability, field experiments
(especially NFEs) can actually offer greater generalizability than lab experiments (and AFEs),
29
In our model in the Appendix, we use different definitions as in Al-Ubaydli and List (2013), but the spirit is
the same as in the original framework.

16

because their results can be applied in some natural setting (the one in which the experiment
was originally performed), for populations and in contexts which would be similar to those of
the original experiment. This is especially true if the experiment is implementing a program,
and the researchers are evaluating the effects of the program in a particular population. Under
global generalizability, on the other hand, results can be extrapolated to contexts that are not
necessarily similar to those in which the experiment took place. In this case, neither lab experiments nor field experiments are superior to each other in that they each measure the parameter
in the exact situation studied.
One of the most often heard disadvantages of using field experiments is that the lab provides
more control than the field.30 We agree that lab experiments can have better control over the
task subjects agree to participate in, and can used induced values (which NFEs by definition
have more difficultly doing). However, this alleged disadvantage must be qualified, depending
on how we define what is meant by “field” and “control”.
As noted above, we follow Harrison and List (2004), who view the concept “field” as a
continuum, where FFE and NFE are clearly inside the set of field experiments, lab experiments
are clearly outside the set, and AFE are somewhere in between. By control, we mean the ability
of the researcher to exclude alternative explanations of the outcome, other than the cause of
interest. With this definition, the different types of experiments allow for different types of
control.31
NFE could offer more control than lab experiments, not less, along certain important dimensions, the main one being selection into the experiment (Al-Ubaydli and List, 2015). As
discussed in Section 1, lab experiments, AFE and FFE estimate treatment effects only for those
who decide to participate in the experiment (pi = 1), and not for the individuals who do not
participate (pi = 0), potentially generating an important bias. Therefore, while the lab provides
researchers with more control in the environment which participants opt into, it provides the
researcher with less control than NFE over the participation decision (Al-Ubaydli and List,
2015). Moreover, while lab experiments are well suited to produce qualitative treatment effects or comparative statics (Levitt and List, 2007), under participation bias even qualitative
treatment effects are not robust (Slonim et al., 2013).32 Therefore, when considering the entire
experimental situation – from start to finish – NFE could potentially offer more control than
lab experiments, because by bypassing the participation decision, they are not subject to participation bias (Al-Ubaydli and List, 2013, 2015, see also Section 1).
30

For example, according to Falk and Heckman (2009) “the laboratory allows tight control of decision environments”, while Camerer (2015) claims that “there is little doubt that the quality of control is potentially very high
in lab experiments”. In a similar vein, Deaton and Cartwright (2018) write: “Exactly what randomization does is
frequently lost in the practical literature, and there is often a confusion between perfect control, on the one hand
– as in a laboratory experiment or perfect matching with no unobservable causes – and control in expectation –
which is what RCTs do”.
31
We elaborate on this point further in Section 3, where we discuss the pros and cons of each type of experiment
and the complementarities between them.
32
In the sense that the direction of the estimated effect might be opposite to the direction of the true treatment
effect.

17

Despite the several benefits of running FFE and NFE discussed in the paragraphs above,
there remains a large obstacle to running more field experiments related to cost considerations. As compared to lab experiments, field experiments can be more expensive both in
monetary terms and also with respect to the planning they require and the time they take to
yield results and, ultimately, publications. However, partnering with administrations, NGOs
or firms can substantially reduce the costs of field experiments, and thus result in a win-win
collaboration (Levitt and List, 2009). Indeed, there are cases when NFE are very low cost,
and entail simply the researcher’s time when the organization is searching for partners to help
generate ideas, implement, and conduct the experiment.33 In the limit, we imagine that NFE
will be a negative cost: organizations will soon realize that the opportunity cost of not knowing
the necessary information is too costly and they will actually employ experimenters to conduct
field experiments that can turn into science.
In summary, field experiments, and especially NFE, offer several advantages over other types
of experiments: being covert, they avoid bias stemming from experimenter demand effects, they
allow for a more complex and natural environment in which the researcher does not need to
know a priori all the variables that affect the outcome, subjects belong to the population of
interest instead of being W.E.I.R.D. (Henrich et al., 2010a) and, in the case of NFE, there
is no participation bias because subjects do not self-select into the experiment. All of these
features enhance the generalizability of field experiments. When a researcher decides which
type of experiment to conduct (lab, AFE, FFE, NFE), there is a tradeoff between the benefits
obtained from conducting the experiment (the private benefits to the experimenter, in terms of
publication and advancement of her career, and the societal benefit from advancing knowledge)
and the cost of running the experiment. In the following section we discuss this tradeoff in more
detail in the context of choosing which type of experiment to run.

3

Use lab and field experiments as complementary approaches

After reviewing various issues related to the generalizability of experimental results in Section
1, and discussing what we view as the advantages of field experiments in Section 2, we now
tackle the broader question of choosing the right type of experiment (lab experiments, AFE,
FFE, or NFE; see the Preliminaries for definitions) for a given research question. Ultimately,
we believe that lab and field experiments serve different purposes, and as such they offer complementarities in the production of knowledge (Harrison and List, 2004; Falk and Heckman,
2009). We identify five main issues researchers should consider when choosing between different
types of experiments.
First, researchers need to consider the properties of the different types of experiments from
the point of view of proper statistical inference (more on this in Section 4). Lab experiments,
33

For a practical take on running field experiments, see (List, 2011).

18

AFEs, and FFEs can offer more control on the task that subjects perform, once they agree
to be in the experiment, than natural field experiments (Al-Ubaydli and List, 2013, 2015). This
control comes in two forms: i) a more precise environment to establish causation (as an example, consider studies using induced values to test whether prices and quantities converge to
neoclassical expectations, as in Smith’s (1962) double oral auction lab experiments or List’s
(2004) multi-lateral bargaining framed field experiments. And ii) more precise estimates (i.e.
lower variance), because one can collect a more homogeneous sample and there can be fewer
unobservables affecting behavior in the lab, so it is easier to run well-powered studies (see Section 4).
It is also crucial that researchers consider the properties of replicability. For example, it
has been argued that an advantage of lab experiments is their better replicability (Camerer,
2015). Lab experiments can offer a more portable protocol than field experiments, and experimental conditions might be kept constant with more precision. We direct the reader to Section
5 for an extended discussion on the properties of replication.
Combining different types of experiments allows researchers to tackle the issue of generalizability by exploring how different factors such as context, scrutiny, stakes and population
affect the outcome.34 As a rule of thumb, the lab will be a good place for experiments where the
identity of the population does not matter. Neuroeconomic experiments studying brain areas
that can be extrapolated to the entire population, fit in this category, as well as experiments for
which the outcome of interest has been shown to generalize (Stoop et al., 2012; Cleave et al.,
2013). Gächter (2010) argue that lab experiments using students are excellent as a first step
to test economic theories, precisely because most theories assume generality. AFE, FFE and
NFE offer the possibility of using a population of interest instead of a W.E.I.R.D. population
(Henrich et al., 2010a,b, also Section 1). FFE and NFE offer the additional benefit of having
a natural context, where not only the population but also the environment resemble the object
of interest. As discussed in the previous section, NFE offer the additional advantage (over lab,
AFE and FFE) that they bypass subjects’ decision of participating in the experiment, therefore avoiding participation bias and experimenter demand effects. Avoiding participation bias
can be especially important if researchers want to scale up their proposed program (Section 12).
Researchers also need to consider the costs of running each type of experiment, including
all the monetary and logistical costs (recruiting participants and paying them fees, providing
treatments and incentives, etc.) as well as the opportunity cost of doing other types of research.
As we discussed in Section 2, lab experiments are typically (but not always) cheaper than field
experiments. Consequently, researchers can often begin by exploring questions using lower-cost
lab experiments, and later move into the field to replicate their initial results in a more diverse environment and population. However, this rule of thumb has exceptions. As discussed
34

Falk and Heckman (2009), discussing the generalizability of experiments, argue that the issue is not necessarily
lab vs. field, but “the prevailing conditions such as the details of agent interactions” (see also Section 1).

19

in Section 2, FFE and NFE can be cheaper (sometimes virtually costless in monetary terms
for the researchers) when researchers partner up with governmental agencies, firms and NGOs,
creating win-win partnerships (Levitt and List, 2009). Moreover, the unit cost per subject can
be reduced in field experiments due to economies of scale, and this is compounded with the cost
reduction of running experiments in countries with lower costs.
Finally, there are many questions that researchers might simply not be able to tackle in the
field, due to ethical or cost constraints. To illustrate this point, consider the case of discrimination (Al-Ubaydli and List, 2013), where the two main theories in economics are preference-based
discrimination (Becker, 2010) versus statistical discrimination (Arrow, 1973; Phelps, 1972). Natural field experiments are clearly effective at differentiating between the two potential sources
of discrimination, as they target the population and context of interest, and avoid participation
bias and experimenter demand effects (for a survey, see List (2006a)). However, the lab can offer a complementary approach to exploring this question: for example, Niederle and Vesterlund
(2007) used lab experiments to investigate whether affirmative action policies affect selection
into a tournament, an intervention that would have been difficult to carry out in a natural
setting.
In conclusion, different types of experiments offer complementarities in the level of control,
replicability and generalizability they allow given their cost, and these trade-offs ultimately
determine, for any particular research question, the type of experiment that offers the most
value.

4

For proper inference, go beyond p-values

Throughout the previous sections, we focused on the generalizability of experimental results,
discussing the extent to which we can extrapolate findings from a given study to other contexts.
We now take a step back, and examine how priors should change in light of empirical findings.
What conclusions can we draw upon observing a statistically significant result? More generally,
what should we consider standards of evidence, and what is the framework of proper inference
given our research data? We suggest a framework where the benefits from running experiments
can be measured by their informativeness, i.e. how much they change the priors of the scientific
community.35
In biomedical and social sciences, including experimental economics, researchers typically
obtain their conclusions regarding the existence of an effect or association in their data by conducting (null hypothesis) significance testing (Fisher, 1925). In particular, they formulate a
statistical model complete with a set of assumptions, among them their null hypothesis (H0 ,
often postulating the absence of the effect/association in question), calculate a test statistic sum35

This aspect should be considered even when members of the scientific community have multiple priors (see
the discussion on priors in this section, and also in Section 8).

20

marizing their data, then compare this statistic to the distribution expected under the model
they specified (i.e. assuming that all the model’s assumptions, including the null hypothesis,
are true). The outcome of this comparison is summarized in the p-value: the probability under
the specified model that the test statistic would be equal to or more extreme than its observed
value (Wasserstein and Lazar, 2016). A result is then pronounced statistically significant if
the p-value falls below a pre-specified cut-off (often 0.05, but see the plea from Benjamin et al.
(2017) for 0.005 ). This interpretation, however, is a departure from Fisher’s original framework. In his view, significance testing essentially measures the strength of evidence against a
null hypothesis, and he leaves the interpretation of the p-value to the researcher. Instead of a
strict decision rule, he advocates for examining whether or not the observed p-value is “open
to suspicion” - and if so, to run another experiment (Lehmann, 1993).
A conceptually different approach to statistical inference is hypothesis testing, developed
by Neyman and Pearson (1933) with the aim to reduce the subjectivity inherent to Fisher’s
method. This framework simultaneously addresses the probabilities of two different types of
errors of inference: incorrect rejection of a true null (Type I error) and incorrect acceptance
of a false null (Type II error). The method requires researchers to formulate a precise alternative hypothesis against which the null hypothesis is tested (in practice, this often means
pre-specifying a particular effect size), and to fix in advance the rates of Type I and Type II
errors (typically denoted by α and β, respectively). Central to this approach is the concept of
statistical power: the pre-study probability that the test will correctly reject a false null as a
function of the alternative hypothesis (calculated as 1 − β, i.e. 1 minus the Type II error rate).
Given the a priori specified decision rule (α, β and the alternative hypothesis Ha ), the analysis
results in the acceptance or rejection of the null hypothesis. The framework allows for ex ante
sample size calculations, whereby the researchers assess the number of observations required
to detect an effect of the size as stated in the alternative hypothesis, with the pre-specified
Type I and II error rates. It is important to point out that hypothesis testing is a frequentist
approach: it limits the number of mistakes made over several different experiments, but it does
not attach an interpretation to a p-value resulting from a single study (Sterne and Smith, 2001).
In practice, researchers all too often focus exclusively on the statistical significance of the
results when interpreting their findings. Such narrow focus on p-values is dangerous, as it gives
rise to several misconceptions. It is crucial to understand that the p-value indicates the incompatibility of the data generated in the experiment with the proposed model, but it does not
measure the probability that the null hypothesis is true: recall, the p-value is calculated under
the assumption that the model is true (Greenland et al., 2016). Thus a p-value of 0.05 from a
single study does not ensure that the finding has a mere 5% chance of being a “false positive”
(more on false discovery rates later). Furthermore, low p-values should be interpreted as providing evidence against the proposed model as a whole, not necessarily against the null hypothesis
in particular. Data and model could be incompatible if any of the underlying assumptions are

21

violated, including those related to the quality of measurement, the conduct of the analysis, the
reporting of results, etc. Thus, a p-value of a comparison cannot be interpreted in isolation,
without considering researcher degrees of freedom and the resulting potential bias (Wasserstein
and Lazar, 2016). Finally, p-values do not convey any information about the size or importance
of the effect in question: tiny effects can produce low p-values if the sample size is large or the
precision of the estimate is high enough, and vice versa (Greenland et al., 2016).
Despite repeated calls for moving beyond p-values and examining the statistical power function (McCloskey, 1985), most published studies continue to ignore the issue entirely. Ziliak and
McCloskey (2004) report that among empirical papers published in the American Economic
Review in the 1990s, only 8% considered the power of the tests used. More recently, Zhang
and Ortmann (2013) failed to find a single study discussing optimal sample size in relation to
statistical power among all the articles published in Experimental Economics between 2010 and
2012. Given this lack of attention, it is unsurprising that most published studies have very low
statistical power. Despite the convention of defining adequate power as 80%, studies in most
fields fall dramatically short of this level.36 In a survey of more than 6700 studies in empirical
economics, Ioannidis et al. (2017) find that the median statistical power of the reviewed research
areas is a mere 18%, and nearly 90% of results are under-powered in half of the areas assessed.
Coville and Vivalt (2017), focusing on studies in the field of development economics, estimate a
median power to detect an average predicted effect of 59%. Only a third of the studies included
in their analysis have power greater than 80%. Analyzing time trends, Smaldino and McElreath
(2016) find little reason for optimism: according to their survey of review papers published between 1960 and 2011, mean statistical power was 0.24 in social and behavioral sciences, and
showed no increase over time.37
Inference from low-powered studies is problematic for at least three reasons. First, by definition, adequate power is required to ensure that studies have a high likelihood of detecting a
genuine effect. Low power implies high rates of false negatives whereby the null hypothesis of
“no effect” is not rejected, despite being false. This aspect is highlighted by De Long and Lang
(1992), who review papers published in the 1980s in major economic journals (American Economic Review, Econometrica, Journal of Political Economy, Quarterly Journal of Economics,
and Review of Economics and Statistics) that failed to reject the null hypothesis at the 0.1 level.
The authors estimate that in their sample “failures to reject nulls are [...] almost always due
to lack of power in the test, and not to the truth of the null hypothesis tested” (De Long and
Lang, 1992, p.1261). More recently, Coville and Vivalt (2017) estimate an average false negative
36

To put it differently, the Type II error rate should be no more than four times the usually prescribed Type
I error rate - a convention that is arguably arbitrary and yet routinely followed across different fields of science
(Ioannidis et al., 2017). An alternative approach is to simultaneously determine the optimal pair of Type I and
II errors according to the circumstances and aim of the specific study, as originally suggested by Neyman and
Pearson (1933) and recently reiterated by Ioannidis et al. (2013).
37
The problem of insufficient power is by no means specific to economics: Button et al. (2013) estimate that
the median statistical power in neuroscience is 21%.

22

reporting probability in development economics of approximately 0.53, calculated as the share
of incorrectly accepted null hypotheses over all accepted null hypotheses. Fiedler et al. (2012)
argues that researchers’ relatively high tolerance for false negatives has potentially irreversible
effects on the development of scientific knowledge: since false negative results are less likely to
be followed up than false positives, self-correction is less likely to occur in these cases.
A second channel through which low power threatens the credibility of research findings is
effect inflation: the phenomenon of obtaining “an exaggerated estimate of the magnitude of
the effect when a true effect is discovered” (Button et al., 2013, p. 366). This problem is also
known as the winner’s curse, the Type M error (Gelman and Carlin, 2014) or the statistical significance filter (Loken and Gelman, 2017). Intuitively, effect inflation occurs because in settings
where standard errors are large, only those findings that by chance overestimate the magnitude
of the effect will appear statistically significant and thus pass the threshold for discovery. Effect inflation is therefore more severe in underpowered studies that are based on small samples
in the presence of high measurement error: studies with power below 50% are likely to yield
exaggerated estimates of magnitudes (Gelman and Carlin, 2014). In line with this prediction,
Ioannidis et al. (2017) estimate that over one-third of the average results of economics research
are exaggerated by a factor of more than four, and the majority of reported research is at least
twice too large.
The third, less appreciated aspect of statistical power is its relation to false discoveries.
The connection becomes clear once we abandon the practice of treating a single finding that has
achieved formal statistical significance as conclusive evidence, and instead consider a Bayesian
framework of statistical inference whereby any individual study contributes to scientific
knowledge insofar as it moves our priors regarding the existence of the effect/association in
question. In this framework, studies may be assessed on the basis of their positive predictive
value: the post-study probability that a research finding that has achieved formal statistical
significance is indeed true (Wacholder et al., 2004; Ioannidis, 2005).38 The basic ingredients
of this metric are the Type I and II error rates (α and β, respectively), together with π, the
fraction of true associations among all associations tested in a given field. We treat this fraction
as our prior: the pre-study odds that the association in question is true (we discuss different
ways to obtain priors later in this section). The post-study probability (PSP) is then defined
as the share of true associations which are declared true ((1 − β)π) divided by the share of
all associations which are declared true ((1 − β)π + α(1 − π)). As shown in Equation 3 below
(reproduced from Maniadis et al. (2014)), the PSP depends on the power of a study (1 − β) in
the following way:
(3)

PSP =

(1 − β)π
(1 − β)π + α(1 − π)

38

The positive predictive value can be understood as the complementary probability of the false positive reporting probability, defined by Wacholder et al. (2004) as the probability of no true association given a statistically
significant finding.

23

In particular, since the derivative of Equation 3 with respect to (1 − β) is positive, the
positive predictive value of a study is increasing in its power. As an example, consider a field
where π is 0.1 (i.e. 1 out of 10 examined associations is true) and α is fixed at 0.05. Using
Equation 3, we find that the post-study probability that a statistically significant finding is
genuinely true is 64% in case the level of power is 80%, but falls to a mere 31% for a study with
20% power. Statistically significant results from low-powered studies thus contribute little to
scientific knowledge as they lead to a low post-study probability of the findings being genuinely
true. Ignoring the above-described framework of inference and treating statistically significant
results from underpowered studies as conclusive evidence for the existence of an effect increases
the rate of false discoveries, leading to low reproducibility of published results and undermining the credibility of the research field (Munafò et al., 2017; Button et al., 2013). A Bayesian
framework also shows that non-significant results, especially when they are obtained in large
datasets, can be more informative than significant ones. In particular, rejection of a point null
hypothesis is often less likely to substantially change priors over a large range of values than is
a failure to reject the null (Abadie, 2018).
As a side note, we would like to draw attention to the other crucial, yet often overlooked,
ingredient of Equation 3: π, the prior probability (or pre-study odds) that the effect being
tested exists. The post-study probability that a statistically significant finding is actually true
is an increasing function of this prior. Continuing our example above, if the prior we consider
changes from 0.1 to 0.01, the PSP falls from 64% to 14% even if the error rates remain fixed at
levels conventionally deemed adequate (α = 0.05 and β = 0.2). Consequently, a single “surprise
discovery,” i.e., the first study to find a statistically significant association in a question where
the prior probability was quite low, should only have a limited impact on our post-study belief
that the effect actually exists. Given their importance, it is crucial to improve our understanding
of priors, and to consider the range of pre-study odds for the question in consideration before
running an experiment (Ioannidis, 2005). As a general rule, priors are higher in disciplines
where empirical research has sound theoretical foundations than in fields where exploratory
research is the norm (Maniadis et al., 2017).39 Abadie (2018) provides a numerical example
for constructing a prior distribution for experimental economics studies, using estimates from
a replication project we discuss in Section 5 (Camerer et al., 2016; Andrews and Kasy, 2017).
Obtaining prior probabilities for any particular research question is less than straightforward.
One solution is to calculate the PSP using a range of possible values for priors, as demonstrated
in e.g. Maniadis et al. (2014). Alternatively, estimates for the pre-study odds may be obtained
by consulting experts. As examples, consider Groh et al. (2016), who undertake an “audience
expectation elicitation exercise”, collecting treatment effect estimates from members of their
audience prior to presenting their results, Coville and Vivalt (2017), who survey a panel of researchers to collect anticipated effects in various development economics studies, or DellaVigna
and Pope (2018), who compare expert and non-expert forecasts. Finally, Dreber et al. (2015)
39
Card et al. (2011) estimate that 68% of economic field experiments are purely descriptive in the sense that
they do not contain even a single line of formal mathematical modeling.

24

use prediction markets to obtain estimates for prior probabilities of specific hypotheses being
true.
In the above discussion of the mechanics of statistical inference we have ignored any “researcher degrees of freedom” in the design, analysis and reporting that may lead to identifying effects even in the absence of a true association. Recent studies indicate that both
specification searching (the practice of trying out several specifications and selectively reporting
outcomes that support the researcher’s intended conclusion, see e.g. Simmons et al. (2011);
Brodeur et al. (2016)) and publication bias (the greater tendency of researchers to submit
and editors to publish studies with significant rather than non-significant findings, see e.g.
Doucouliagos and Stanley (2013); Christensen and Miguel (2016); Andrews and Kasy (2017))
are prevalent in empirical economics. As Ioannidis (2005) points out, such bias also reduces the
post-study probability of a positive finding actually being true.40 Repeated independent testing
by different teams of investigators further lowers the PSP: intuitively, the positive predictive
value in this case reflects the fact that only 1 out of n independent studies found a positive
association (in Section 5 we discuss how the PSP changes when r out of n independent studies
find evidence for the existence of an effect).
In sum, an exclusive reliance on formal statistical significance and inadequate attention to
the other ingredients determining a study’s positive predictive value (priors, bias, competition
and, crucially, statistical power) compromise researchers’ ability to draw correct inferences from
data. Besides the adaptation of a Bayesian framework, proposed solutions to the problem include greater attention to effect sizes, and a departure from methods that focus on testing
towards those that emphasize estimation, such as confidence, credibility or prediction intervals
(Wasserstein and Lazar, 2016; Munafò et al., 2017) or even “hacking intervals” (Coker et al.,
2018). On the other hand, using a p-value threshold as an established standard for evaluating research findings might still be helpful for consumers of research. Given that the scientific
community continues to rely on a universally accepted p-value cut-off, a group of scientists
now propose to make this standard more stringent: Benjamin et al. (2017) argue that novel
findings should be labeled as “statistically significant” only if they pass a p-value threshold
of 0.005, and recommend treating evidence with p-values between 0.005 and 0.05 merely as
“suggestive”. Their proposal promises to reduce false positive rates to acceptable levels in
most professions. The proposal sparked an intense debate, with critiques calling for removing
(Amrhein and Greenland, 2018) or abandoning (McShane et al., 2017), rather than redefining,
statistical significance, and suggesting a new approach to reporting results whereby researchers
transparently present and justify their design choices, including their chosen significance level
(Lakens et al., 2018).
40
In the presence of such practices, the positive predictive value may be calculated as follows, where u indicates
(1−β)π+βπu
the extent of bias: PSP = (1−β)π+βπu+[α+(1−α)u](1−π)
. Maniadis et al. (2017) discuss the determinants of u for
a given discipline.

25

This lively debate signals a growing interest among experimental scientists in the issue of
proper inference, a welcome development that we hope will translate into actual changes in
practices and norms in the scientific community. In the following sections we review several
practical recommendations that have the potential to substantially improve the reliability of
scientific results. In Section 5 we begin with what we see as the most pressing issue currently: we
discuss the importance of replications, and present incentive-compatible methods to encourage
them.

5

Replicate early and often

We believe that the best approach to increasing the reliability of results from experimental
economics lies in replication. Recent controversies surrounding topics such as ego depletion in
the psychology literature (Hagger and Chatzisarantis, 2016; Hagger et al., 2010; Carter and
McCullough, 2014) or the impact of de-worming programs in development economics (Croke
et al., 2016) all highlight the importance of replications. In the following we define what we
consider replication, and demonstrate using a Bayesian framework of inference why it is crucial
for the credibility of science. We then discuss what the “natural rate of replication” and the
rate of reproducibility in economics are today. Additionally, we review several proposals to
incentivize replication.41
As Clemens (2015) points out, there is currently no universally accepted standard in economics as to what exactly constitutes a replication. Levitt and List (2009) propose definitions
that are well-suited to experimental studies. In the most narrow interpretation, a replication
means taking the original data generated by an experiment and re-analyzing it to confirm the
original findings. In the terminology of Hamermesh (2007)’, this would constitute a pure replication: examining the same question and model using the underlying original data set. This
approach may help to address issues with the internal validity of a study, for instance through
uncovering coding errors or mistakes in calculations.42 A broader interpretation of replication
in experiments involves running a new experiment closely following the original protocol to
test whether similar results can be generated using a new subject pool. Such a study would
be classified as statistical replication: based on a different sample, but using an identical
model and underlying population (Hamermesh, 2007). This method has the potential to fix
sampling errors or insufficient power. Finally, the third and broadest category entails testing
the hypotheses of the original study using a new research design. This characterization is a
scientific replication according to Hamermesh (2007), as it involves a different sample, a
different population, a different situation, and a perhaps similar but not identical model. These
41

Other valuable methods aimed to serve the goal of “research synthesis” are literature surveys and metaanalyses; for reviews on these methods, refer to e.g. Anderson and Kichkha (2017); Maniadis et al. (2017);
Maniadis and Tufano (2017).
42
Even without an explicit mistake on the researchers’ side, empirical results are not necessarily robust; as an
example, consider McCullough and Vinod (2003) who report that nonlinear maximization methods from different
software packages often produce wildly different estimates.

26

replications help assess the robustness of the original finding, and may inform discussions on
the generalizability of the original result (see Section 1).43
To illustrate why statistical replications are crucial, let us return to the Bayesian framework
of inference introduced in Section 4. Equation 3 presented the post-study probability (PSP) of a
finding actually being true, conditional on a single study providing statistical evidence in favor
of its existence. Following Moonesinghe et al. (2007), we can adapt this formula to calculate the
PSP when at least r out of n independent studies find a significant result for the association in
question. As before, we obtain the PSP as the fraction of true associations declared true over
all associations declared true:
(4)

PSP =

π

Pn

i=r


P
π ni=r ni (1 − β)i β (n−i)

Pn n i
n
(n−i)
i (n−i) + (1 − π)
i=r i α (1 − α)
i (1 − β) β

Using Formula 4, Moonesinghe et al. (2007) and Maniadis et al. (2014) demonstrate how a few
successful replications can increase the positive predictive value of a finding. This increase is
particularly dramatic in cases when prior probabilities are low.44 Within the same framework,
Coffman and Niederle (2015) argue that even the most inaccurate beliefs can be corrected within
three to five replications.45
Despite a general consensus among economists regarding the importance of replication, it
remains largely “an ideal to be professed but not practiced” (Mueller-Langer et al., 2017). Incentives for individual researchers to replicate a project or to have their own work replicated are
low or missing entirely. Replications typically bring little recognition for their authors despite
the substantial work they entail. The process is particularly tedious because data and code
for published articles are often unavailable - even though most leading economics journals have
introduced data sharing requirements and mandatory data archives, such policies are not necessarily enforced (Höffler, 2017). As Duvendack et al. (2017) observe, replications are usually
regarded as unoriginal or “derivative”. Worse, they may ignite animosity among researchers if
authors of the original work treat replication attempts as threats. Moreover, journals may be
reluctant to publish replication studies for fear of not receiving enough citations (Duvendack
et al., 2017). Indeed, according to a survey by Mueller-Langer et al. (2017), from 1974 to 2014
less than 0.1% of publications in the top-50 economics journals were replications. Given the
43

Clemens (2015) suggests an alternative classification, differentiating between replication tests (including
verification and reproduction tests) and robustness tests (including reanalysis and extension tests). See also the
discussion in Duvendack et al. (2017).
44
Consider the following example, based on Maniadis et al. (2014): n = 15 researchers independently run the
same study with 80% power to detect an association that has a 10% prior probability of being true. When a
single study out of the fifteen attempts finds a statistically significant association (i.e. r = 1), then the post-study
probability that this positive finding is actually true is a mere 17% (remember that the corresponding PSP was
well above 50% in the absence of researcher competition (i.e. n = 1), see Section 4). However, the post-study
probability that the association in question really exists increases to over 90% in case of just two successful
replications.
45
For a more nuanced approach that takes into account various forms of researcher bias among the replicators,
see Maniadis et al. (2017).

27

difficulties of publishing a ‘mere replication,’ conducting an extension study where the control
treatment replicates a previous finding is often a more attractive alternative. This, however,
makes replications hard to identify: as Coffman et al. (2017) point out, such ‘implicit replications’ are often reported as part of a paper with a much larger scope, without being labeled as
replications. Other times, successful replications are simply not considered interesting enough
to be published. As a result, it is less than straightforward to assess how often replications
actually occur.
A few recent papers attempt to address this issue and estimate the “natural rate of replication” in economics. First, Berry et al. (2017) focus on all the empirical papers published in the
centenary volume (2010) of the American Economic Review, and manually code all their published citations as either replications, robustness tests, extensions, or none of the above. They
find that less than a third of the 70 papers have been replicated at least once, where a replication
is defined as a project “speaking directly to the veracity of the original paper.” Validating the
assertion that the visibility of replications is low, Berry et al. (2017) find considerable uncertainty among the authors of the original papers over the number of extant replications of their
studies. Second, Sukhtankar (2017) analyzes 1056 empirical papers in development economics
published in the top ten general interest journals between 2000 through 2015, perform a reverse
citation search, then search within the ensuing list for “replication” or alternative cognates. His
results suggest that only 5.4% of the studies in their sample were replicated in a published paper
or a working paper, the rate being higher (12.5%) for studies based on randomized controlled
trials. Third, Hamermesh (2017) collects ten leading papers from labor economics with at least
20 years of citation history, and classifies their citing papers as either i) related to, ii) inspired
by, iii) very similar to but using different data, or iv) a direct replication at least partly using
the same data. He finds that of the more than 3000 citing studies, only 0.6% fall into the last
category. On the other hand, 7 out of the 10 original studies he surveyed were replicated at least
five times, and all of them at least once. Finally, Maniadis et al. (2017) survey experimental
papers published between 1975–2014 in the top 150 journals in economics, and estimate that
the fraction of replication studies among all experimental papers in their sample is 4.2% (taking
into account ‘implicit replications’ as well). Overall, these studies suggest that the natural rate
of replication in empirical economics is low, although heavily cited and influential papers do
tend to get replicated.
The above results concern the rate at which replications are attempted, leaving aside the
question of what share of these replications is positive, i.e. confirm the findings of the original study. Measuring rates of reproducibility in economics dates back to the quest of Dewald
et al. (1986) to replicate findings from articles published in the Journal of Money, Credit and
Banking. The authors famously concluded that inadvertent errors were a “commonplace rather
than a rare occurrence.” Another key insight of the Dewald et al. (1986) study was the alarmingly high share of authors who were unwilling or unable to supply their data and code to

28

the replicators. According to Chang and Li (2017), this problem is still pervasive: in their
attempt to replicate macroeconomic papers published in 13 well-regarded journals, the greatest
obstacle they faced was authors’ failure to provide their data and code files. As a result, they
were only able to qualitatively reproduce the key results of 29 out of the 59 papers they sampled.
Focusing on experimental economics specifically, Deck et al. (2015) review several replication attempts, mostly in the context of public goods provision, with varying outcomes. The
first systematic evidence of replicability of laboratory experiments in economics is provided by
Camerer et al. (2016) who replicate 18 studies published in the American Economic Review
and the Quarterly Journal of Economics between 2011 and 2014, according to pre-analysis
plans posted prior to conducting the replication studies. They find a significant effect in the
same direction as in the original study in 11 out of the 18 studies, corresponding to a reproducibility rate of 61%.46 They also discuss alternative replication indicators, e.g. whether the
95% confidence interval of the replication effect size includes the original effect size, or whether
the replicated effect lies in a 95% prediction interval. These measures suggest higher rates of
replicability (66.7% and 83.3%, respectively). The authors also compare the replicated effect
sizes with the original, and find a mean relative effect size of 65.9%. The finding that the
replicated effect sizes tend to be smaller than the original ones reflects the effect size inflation
phenomenon discussed in Section 4. Overall, Camerer et al. (2016) interpret their findings as
suggesting “relatively good replicability of results.”
Another noteworthy replication initiative is the Social Sciences Replication Project, whose
collaborators aimed to replicate 21 experimental studies in the social sciences published in the
prestigious journals Nature and Science between 2010 and 2015. They find a significant effect
in the same direction as the original study for 13 (62%) studies, and the effect size of the replications is on average about 50% of the original effect size (Camerer et al., 2018). Finally, while
both of the above-mentioned projects focus on results published in top journals, Maniadis et al.
(2017) analyze replication attempts from 150 economic journals, and find a “success rate” of
42.3% among the 85 experimental replication studies in their sample.
The recent surge of interest in reproducibility also ignited an intense discussion about the
most effective ways to incentivize replications. We conclude this section by reviewing a few
suggestions that we find particularly promising. The first set of ideas addresses the current
difficulty of publishing replication studies, suggesting the creation of a specific outlet in the
form of a new journal dedicated to replications (Coffman and Niederle, 2015), or including
46

While lower than desirable, this share is considerably higher than the replicability rates uncovered in the Reproducibility Project: Psychology (RPP) (Open Science Collaboration, 2015), a project that involved replicating
100 studies published in three psychology journals. Their results paint a rather grim picture of the reliability of
psychological research: while 97% of the original studies found significant results, only 36% of the replications
were able to reproduce these significant findings. In the Many Labs 2 project, 15 of the 28 attempted replications
provided evidence in the same direction as the original finding and statistically significant at the 5% level (Klein
et al., 2018).

29

one-page “replication reports” in top journals (Coffman et al., 2017). These suggestions could
be especially effective coupled with a new norm that requires citing replication work alongside
the original, increasing the returns both to the publishing journals and to the authors of the
replications. The recent launch of the Journal of the Economic Science Association, with a
special section devoted to replications, is a promising step in this direction. Second, Maniadis
et al. (2015) emphasize the need to change authors’ incentives to collaborate with replicators.47
In their view, journals should always allow original authors to give their commentaries after a
replication attempt; they also suggest considering the number of replication attempts as a metric for one’s research quality. Third, Butera and List (2017) design a new, incentive-compatible
mechanism whereby the original investigators of a study commit to only publishing their results
as a working paper, and offer co-authorship of a second paper (submitted to a peer-reviewed
journal) to other researchers who are willing to independently replicate their experimental protocol in their own research facilities. This mechanism allows the original authors to signal the
ownership of the research idea, while ensuring the credibility of their results (in case they indeed
replicate). At the same time, scholars on the team of replicators, in return for bearing the cost
of replications, would benefit from coauthoring a novel study. Finally, Dreber et al. (2015)
suggest using prediction markets with experts as quick and low cost ways to obtain information
about reproducibility.48
Combined, these approaches have the potential to make replication more prevalent by increasing its attractiveness to researchers. Such a culture could also have positive consequences
on how research is conducted in the first place: as Duvendack et al. (2017) point out, replication
may have a deterrent effect on questionable or fraudulent research practices by increasing the
likelihood that such practices will be discovered. In sum, replication serves to prevent, expose
and correct wrong inferences, and is thus inevitable for producing empirical results that can
reliably form the basis of both economic theory and policy. However, replication does not eliminate the need for well-powered studies: replication projects with low statistical power contribute
little to the evidence in favor of or against a hypothesis. In the next section, we follow this line
of reasoning by urging researchers to perform ex ante power calculations and to design their
experiments in ways that maximize statistical power.

6

Consider statistical power in the design phase

As discussed in Section 4, insufficient statistical power in experiments poses a major challenge
to proper inference. The most straightforward remedy, of course, is to avoid conducting low47

See also Maniadis et al. (2017) for a systematic review on the problem of information revelation in science.
Dreber et al. (2015) set up prediction markets in conjunction with the Reproducibility Project: Psychology
(described in footnote 46) where participants could bet on the success of the attempted replications. Prediction markets were found to predict the outcomes of the replications well, performing better than a survey of
participants’ individual forecasts. While Camerer et al. (2016) confirm the result that beliefs elicited though
a prediction markets are positively correlated with a successful replications, they do not find that this method
works better than belief elicitation through a survey.
48

30

powered studies in the first place. In case of experiments, this requires taking the question of
statistical power seriously in the design phase. In the following, we describe the basic principles of optimal sample size calculations, and then review sample arrangement practices that
maximize power given the available budget. The section is intended as a overview of the most
important considerations; the interested reader can find more details in List et al. (2011), Duflo
et al. (2007) and Cox and Reid (2000).
Power calculations (i.e. the assessment of the precision of inferences expected to be achieved
with a given sample size), or optimal sample size calculations (i.e. the estimation of the sample
size required to attain a certain precision), are crucial steps prior to conducting an experiment
(Gelman and Hill, 2007).49 Power calculations are most often advocated as tools for preventing
high rates of false negatives. They also increase efficiency by ensuring that scarce resources are
not wasted on studies that are larger than necessary. Moreover, pre-determining sample sizes
can curb bias by reducing experimenters’ temptation to collect more data when initial results
are insignificant but “go in the right direction” – a practice that could lead to high rates of false
positives (Zhang and Ortmann, 2013). Despite these arguments, in practice researchers often
forgo ex ante sample size calculations and rely on shortcuts with little theoretical justification
when designing their experiments.50
As discussed in Section 4, sample size calculations are rooted in the framework of hypothesis
testing. As such, they require researchers to specify (1) a null hypothesis and an alternative
hypothesis, (2) the desired significance level and power of the test and (3) the statistical test to
be used in the subsequent analysis (List et al., 2011). These considerations allow the researcher
to simultaneously control the likelihood of committing either a Type I or a Type II error. In
particular, by considering the hypothetical distributions of the test statistic under the null and
the alternative hypothesis, the researcher obtains critical values for the test statistic corresponding to the pre-specified error rates. The null hypothesis for a test is typically specified as no
effect/association, while the alternative hypothesis typically postulates that the effect size is at
least as large as a specific value. Alternatively, for a given budget and thus fixed sample size,
one can calculate the minimum detectable effect size given the pre-specified acceptable error
rates.
While the researcher has discretion over these three building blocks of power calculations
(hypotheses; acceptable error rates; the test used for comparison), translating critical values to
optimal sample size requires knowledge of the variance of the outcome – a parameter unknown
49

Gelman and Carlin (2014) go a step further and suggest performing what they call a “design analysis,”
complementing power calculations with an assessment of the sign error rates (the probability that the replicated
estimate has the incorrect sign, if it is statistically significantly different from zero) and the exaggeration ratio (the
expectation of the absolute value of the estimate divided by the effect size, if statistically significantly different
from zero).
50
List et al. (2011) mention the practice of assigning 30 subjects to each treatment arm as an example for a
widely used yet theoretically unfounded rule-of-thumb.

31

prior to conducting the experiment. This feature makes ex ante power calculations inherently
hypothetical, as they are based on the researcher’s expectations about the underlying data
generating process (Gelman and Carlin, 2014). However, one should not use the hypothetical
nature of power calculations as an excuse for skipping this step in the design phase. Researchers
can use data from previous experiments or pilot studies to form beliefs about the variance of
outcomes. It is also instructive to calculate the statistical power for a range of different hypothesized values of the variance. When deciding what effect size to target, researchers should
consider what difference is actually practically or economically relevant – an aspect that is still
largely overlooked both at the design and the inference stage.51 A useful practice is to express
minimum detectable effect sizes in terms of standard deviation changes to facilitate comparison
with existing studies in the field (e.g., the researcher may desire to have her experiment detect
a 0.1 standard deviation treatment effect).
We demonstrate the framework of power calculations through a simple example adapted
from Section 3.1 of List et al. (2011). Suppose we are interested in estimating the average
treatment effect from an experiment where participants are randomly assigned to either the
treatment or the control group. For now, assume that we only test a single hypothesis in
our study (more on multiple comparisons later). For simplicity, we assume that our data is
generated by the following model:
Yi = β + τ Di + i ,
where Yi is a continuous outcome variable, Di is a binary treatment indicator, the estimated
treatment effect is homogeneous, and i is an idiosyncratic error term with variance σ2 . Throughout this example, we assume that the unobserved components of outcomes are independently
distributed among our subjects, and relegate the discussion of inference with grouped errors to
later in the section. Errors may be heteroscedastic: we allow the variances of the error term
2 ) and the treatment conditions (σ 2 ) to vary. Assuming normality, we use a
in the control (σC
T

two-sided t-test for comparing the means of the outcome variable between the groups. These
assumptions allow us to derive simple and intuitive closed-form solutions for the optimal sample
size or the minimum detectable effect size.
There are nC and nT subjects in the control and the treatment groups. Due to random
assignment, the estimated average treatment effect is obtained simply as the difference in
means between the treatment and the control group: τ̂ = Y¯T − Y¯C , with variance: V̂ =
Var(Y¯T ) + Var(Y¯C ) − 2Cov(Var(Y¯T ), Var(Y¯C )) = σ 2 /nT + σ 2 /nC .52 Our goal in this exercise
T

C

51
Ziliak and McCloskey (2004) review papers published in the American Economic Review and find that the
share of papers discussing effect sizes rather than merely the significance (and maybe the sign of the estimated
coefficients) is still low.
52
Note that the true variances in the treatment and control groups are typically unknown a priori and are themselves estimated from the data, often by means of the Neyman variance estimator, a conservative randomizationbased approach (Samii and Aronow, 2012).

32

is to determine the smallest true effect size we can detect given our sample size and required
statistical significance and power.

Reject H0

Accept H0

RejectH0

α = P(Type I Error)

0

β = P(Type II Error)

δ
tα/2

tβ

Figure 1: Hypothetical distributions of the estimated treatment effect under H0 and Ha
To begin, let us assume that the null hypothesis is true: the true average treatment effect is
zero. The hypothetical distribution of the estimated treatment effects is then centered around
zero, as shown in the top panel of Figure 1. As aforementioned, in order to control the Type I
error at a rate of α, we reject the null hypothesis only if we observe a t-statistic that is equal to
or more extreme than our critical value. Equation 5 summarizes this condition: the left hand
side of the equation is the t statistic estimated from a test comparing the means of the outcome
variable in the control and the treatment group assuming that the true average treatment effect
is zero, while tα/2 is the critical value corresponding to a false positive rate of α in a two-sided
test.

(5)

Y¯ − Y¯C
q T2
≥ tα/2
2
σC
σT
nC + nT
Now consider the distribution of the treatment effect under the alternative hypothesis, as33

suming a true effect size of δ. The hypothetical distribution of the estimated treatment effects
under this alternative hypothesis is shown in the bottom panel of Figure 1. The power of our
test to identify a true effect size of δ can be thought of as the fraction of the area under this
distribution that falls to the right of the critical value tα/2 : this is the region where we correctly
reject the null hypothesis. Limiting the Type II error rate to β (resulting in a statistical power
of 1−β for our test), we can calculate the minimum detectable effect size of our experiment:
the smallest value for which we can (correctly) reject the null hypothesis of no treatment effect
with probability 1 − β at a significance level of α. This minimum detectable effect size δmin
can be expressed as a function of the sample sizes and variances in the control and treatment
groups as:
s
δmin = (tα/2 + tβ )

(6)

p
2
σC
σ2
+ T = (tα/2 + tβ ) V̂
nC
nT

Equation 6 shows that the lower the variance of the treatment effect estimator, the smaller
the effect size we can detect. This estimated variance, in turn, depends on the sample sizes
and the variances of the error terms in the two groups.53 Equation 6 can be re-arranged to
determine the sample sizes in the treatment and the control group that are required to detect a
treatment effect of the size of δ given the variance of the estimator and the pre-specified Type
I and II error rates.
Analytical power calculations such as the example presented above are useful for simple
comparisons. For non-parametric tests and more complex or more specific design choices,
simulation-based power calculations provide more flexibility. These approaches require the
researcher to specify the underlying model complete with the experimental design and sample sizes, the values of the covariates, the parameter values expressing the distribution of the
outcome variable under the alternative hypothesis, and the variances (Feiveson, 2002). Based
on this model, the researchers generate their synthetic data and run their estimation on these
data a large number of times, obtaining a p-value in each round of the simulations. Power is
then calculated as the proportion of p-values that are lower than the pre-specified cutoff value
α. Several recent papers provide more details along with software packages that implement
simulation-based power calculations (Feiveson, 2002; Luedicke, 2013; Bellemare et al., 2016;
Burlig et al., 2017).
While often not flexible enough for practical purposes, Equation 6 can still provide valuable insights for the derivation of basic heuristics intended to maximize the precision of a
study through design. A straightforward way to increase power is to increase the number of
observations – this, however, is often impossible due to budget constraints or other practical
53
As a numerical example, consider an experiment where a total of 2000 participants are equally divided
between a treatment and a control group (nC = nT = 1000), and we assume equal variances in the two groups
(σC = σT = σ). Assuming α = 0.05 and β = 0.2, using
detect a minimum effect
q a two-sided t-test we can then
p
σ2
σ2
size of 0.125 standard deviation: δmin = (t0.025 + t0.2 ) 1000 + 1000 = (1.96 + 0.84) 1/500σ ≈ 0.125σ

34

considerations. We thus focus on sample arrangement techniques for a given experimental budget that aim to reduce the variance of the estimate through other channels.
The first such rule concerns the assignment of subject to treatment or control groups. While
it is common to assign equal number of subjects to all conditions, studying Equation 6 we find
that this practice is only optimal in case we expect the variances to be the same across groups.
Otherwise, the ideal ratio of the sample sizes assigned to treatment and control is equal to the
ratio of the standard deviation of outcomes in the two groups. For the special case of a binary
outcome variable, the same logic implies that sample sizes in the treatment and control groups
should only be equal in case the null hypothesis predicts equal means – and thus equal variances
– between the two groups.
The optimal design also takes potential heterogeneity in data collection costs into account
to maximize power for a given experimental budget. The unit cost of obtaining an extra subject
might differ between treatment and control groups. Intuitively, providing a treatment is often a
lot more costly than simply surveying someone or relying on administrative data in the control
condition (Duflo et al., 2007). It can be shown that the optimal share of subjects assigned to
treatment versus control is inversely proportional to the square root of the respective sampling
unit costs (List et al., 2011).
So far we have focused on experiments with a binary treatment indicator. In studies where
the experimenter can choose different levels of treatment intensity, precision may be increased
through design that maximizes the variance of the treatment variable. In particular, the number of subjects allocated to the different levels of treatment should reflect our priors of the
functional form of the treatment effect. Identification requires the number of treatment cells
used to be equal to the highest polynomial order plus one (List et al., 2011). For instance,
assuming a quadratic treatment effect, we should allocate subjects to three treatment cells at
the two extremes and at the midpoint of the feasible range, assigning one-fourth of subjects to
each extreme and half to the midpoint for maximum variation in the treatment variable.
Power calculations also need to account for the randomization technique used to assign subjects to specific treatments. Cluster-randomized designs, where the unit of randomization does
not coincide with the unit of analysis, are commonly used in field experiments. For instance,
even if student-level data is available, institutional constraints or fear of spillovers might induce
researchers to randomize at the classroom or school level. In such cases, our assumption of i.i.d.
error terms is often not justified. Clustered designs therefore necessitate power analysis that
accounts for group-level shocks (see Duflo et al. (2007) who derive the expression for the variance of the estimated treatment effect in cluster-randomized designs, and Abadie et al. (2017)
on when to adjust standard errors for clustering). Optimal design in such experiments balances
two opposing forces: for a given number of experimental subjects, increasing the number of

35

clusters we sample from leads to greater gains in power than sampling additional individuals
from already included clusters. However, adding a participant from a new cluster tends to be
more expensive than someone from existing clusters. Additionally, Chandar et al. (2018) argue
that in case of heterogeneous treatment effects at the cluster level, the researcher may want to
include more treated clusters than control clusters.54 We return to the question of treatment
assignment in Sections 8 and 9 where we discuss in detail the practice of blocked randomization
and within subject designs.
Finally, we would like to draw attention to an implicit assumption we made throughout this
section: we assumed that participants comply with their treatment assignment. However, as
we have discussed in Section 1, compliance is often imperfect. In overt field experiments that
randomize access to a particular program or service (common in development economics), takeup of the offered treatment is often low, jeopardizing researchers’ ability to detect the impact
of the program. McKenzie (2011) points out that the sample size required to detect a given
change resulting from a treatment is inversely proportional to the difference in the proportion
of the treatment group that takes up a given intervention relative to the control group.
We end this section by re-iterating an important qualification to the above-described framework: it discusses power calculations in cases of a single comparison, whereas most studies test
multiple hypotheses. In the following section, we discuss different manifestations of the multiple
comparisons problem, and show how the experimental design and the statistical analysis should
be modified to account for multiple hypothesis testing.

7

Adjust for multiple hypothesis testing, in power tests and in
data analysis

In the previous section we derived our results based on the assumption that researchers evaluate
a single hypothesis. In practice, however, most research in applied economics entails more than
one comparison performed within the same study. Multiple hypothesis testing (MHT), or
the multiple comparisons problem, refers to the practice of simultaneously considering multiple statistical inferences (Miller, 1981). Failure to account and correct for multiple hypothesis
testing increases the likelihood of false positives and contributes to the replicability crisis of
the social sciences (List et al., 2016). As an example, consider a study where a researcher
jointly tests N mutually independent hypotheses, all of which are true and therefore should be
accepted. Fixing the Type I error rate for a single comparison at a level α, the probability of
at least one false rejection among all comparisons in this case is 1 − (1 − α)N . Setting α = 0.05,
the probability of observing at least one false positive is over 14% in case of just three hypotheses, and it exceeds 50% when testing 14 or more hypotheses. In the following, we provide an
overview of the prevalence of the problem, and discuss possible solutions.
54
The intuition is that if the researcher’s intervention leads to different effects across different clusters, having
more treated clusters can help average over those differences and recover the mean effect (Chandar et al., 2018).

36

The practice of ignoring multiple hypothesis testing corrections is pervasive in experimental
social sciences. List et al. (2016) differentiate between three main cases of multiple hypothesis testing. The most common occurrence involves analyzing the impact of an intervention on
multiple outcomes. According to an overview by Anderson (2008), 81% of surveyed papers
published from 2004 to 2006 report results on at least five outcomes, and a striking 61% consider ten or more outcomes (the number of unreported comparisons is likely to be even higher).
Yet only 7% of these papers account for multiple hypothesis testing in their inference. The
second widespread form of MHT entails comparisons across multiple subgroups of the study
population. Analyzing heterogeneous response to treatment by gender, ethnicity, age, etc. falls
into this category. Finally, analyzing experiments with multiple treatments (either estimating the effect of each treatment condition versus a control, or performing all possible pairwise
comparisons across multiple treatments and a control) also constitutes a case of MHT, as noted
in Section 6.
There are two main approaches to dealing with the multiple hypothesis problem: reduce
the number of comparisons carried out, and/or adjust the inference to take into account the
family of hypotheses considered in the analysis. The first approach involves choosing a specific
set of outcomes based on a priori notions of importance, and using summary index tests
that pool multiple outcomes into a single measure (Anderson, 2008). The second method (the
focus of this section) accounts for the multitude of tests carried out by adjusting the inference
from the analysis. Multiple testing procedures often control the family-wise error rate: the
probability of rejecting at least one true null hypothesis among a set of hypotheses we jointly
test (Heckman et al., 2010). Alternatively, when the number of hypotheses tested is very large,
researchers often choose instead to control the m-familywise error rate (i.e. the probability of
m or more false rejections), the tail probability of the false discovery proportion (the fraction of
false rejections), or the false discovery rate (the expected value of the false discovery proportion)
- for more details on these approaches, see List et al. (2016).
Different techniques have been developed to adjust the standards of inference for multiple
hypothesis testing. Single-step procedures simultaneously compare all the individual test
statistics from the different comparisons to their critical values. Often (though not always) the
same critical value is used for all comparisons. As an example, consider the most well known
multiple testing procedure developed by Bonferroni (1935), applied to the calculation of confidence intervals by Dunn (1961). This technique consists of computing an individual p-value
for each hypothesis tested, and rejecting a hypothesis only if its p-value does not exceed α/S,
where S is the total number of comparisons performed. Under the assumption that the null
distribution of each p-value is uniform, this method asymptotically controls the family-wise
error rate at level α (Romano and Wolf, 2005).

37

Stepwise methods of multiple testing procedures also start with a single-step method.
However, instead of stopping after the first set of comparisons, these methods allow the researchers to reject further hypotheses in subsequent steps by decreasing the critical values for
the remaining hypotheses, taking into account the hypotheses already rejected in previous steps.
The methods continue until no further hypotheses are rejected (Romano and Wolf, 2010). Stepwise procedures can be further classified into stepdown and stepup methods. Stepdown methods
begin by considering the most significant hypotheses, and then continue to evaluate hypotheses
with smaller test statistics. Romano and Wolf (2010) show that the classical method of Holm
(1979) can be formulated as a stepdown procedure where the criterion for rejection for the most
significant hypothesis is the same as in the Bonferroni-method, but the criteria get less strict
for larger p-values.55
The appeal of the traditional methods of Bonferroni (1935) and Holm (1979) lie in their
simplicity – however, they are often overly conservative. Procedures with more power to reject
false null hypotheses have been designed by taking into account the joint dependence structure
of the individual p-values (e.g. Romano and Wolf, 2005; Heckman et al., 2010).56 Based on
Romano and Wolf (2010), List et al. (2016) developed a bootstrap multiple hypothesis testing
procedure that asymptotically controls the family-wise error rate under fairly weak assumptions
and provides an improvement over classical methods in terms of power through incorporating
information on the dependence structure (and can lead to further gains by exploiting transitivity). Their procedure was designed to simultaneously handle all three scenarios of MHT
in experimental economics discussed above. The method is asymptotically balanced (loosely
speaking, this implies that all tests contribute equally to error control, see Romano and Wolf
(2010)).57
Accounting for multiple hypothesis testing often leads to different conclusions than inference that ignores the multitude of tests carried out at once. For instance, Lee and Shaikh
(2014) demonstrate how the significance of PROGRESA’s estimated effects on school enrollment across sub-populations change once we account for multiple inferences. While developing
their approach to correcting for MHT, List et al. (2016) also show a large reduction in the
number of null hypotheses rejected in Karlan and List (2007) once multiple testing is taken into
account. These examples serve to encourage researchers to identify and properly correct for all
the different comparisons within a study to avoid the false positives that mechanically result
from multiple testing.
Besides ex post corrections, researchers should pre-emptively take into account the problem
of multiple hypothesis testing in the design phase. Intuitively, to control the false positive rate
55

For an example for a stepup procedure, refer to e.g. Benjamini and Hochberg (1995).
For a discussion on the meaning of power in a multiple hypothesis testing context, refer to Romano and Wolf
(2005).
57
List et al. (2016) made their MATLAB and Stata code available to other researchers for easy implementation
of the procedure at https://github.com/seidelj/mht; see Seidel and Xu (2016) for documentation.
56

38

across all comparisons, stricter significance level requirements should be applied for each individual test ex ante. In practice, this means specifying lower levels of α in the power calculation
for each comparison (see Section 6 for details on power calculations for single comparisons).
Acknowledging this imperative already in the design phase reveals a “hidden cost” of adding
another outcome, treatment arm, or subsample analysis to an experiment: every additional
comparison the researcher plans to perform increases in all existing comparisons the number of participants (or the precision of measurement) that is required to maintain control over
the study-level false positive rate. As a simple example, consider a researcher trying to determine the optimal sample size for an experiment that compares a treatment and a control group
along two different outcomes. In order to ensure a study-level false positive rate of 5%, the
researcher can use the above-described method by Bonferroni (1935) and set the significance
level cut-off to 0.05/2 = 0.025 for each individual comparison, and calculate the optimal sample
sizes accordingly.58
Ensuring sufficient statistical power in a study while accounting for multiple comparisons
may substantially increase the number of participants required and thus the cost of an experiment. As discussed in Section 6, appropriate design choices can be helpful in increasing
statistical power without expanding the experimental budget. In the following two sections,
we review in detail two such techniques that have the potential to reduce the variance of the
estimated treatment effect: blocked randomization and within subject experimental designs.

8

Use blocked randomization to increase power and credibility

In Section 6, we have shown that the statistical power of a study is decreasing in the variance
of the estimated treatment effect. We also highlighted approaches to reduce this variance by
optimally choosing the ratio of subjects assigned to the treatment vs. the control group. This
section considers more broadly the process of assigning treatment status to any given participant. In the following, we review the merits of blocked randomization compared to complete
randomization in terms of statistical power. We then approach the topic from a different angle
and show that blocking may reduce bias by serving as a commitment device against specification
searching. Finally, we discuss the choice between randomization and optimization.
In the section on Preliminaries, we outlined the logic for randomly assigning subjects to
treatments: randomization balances the treatment and control groups both in terms of observables and unobservables, allowing an unbiased identification of the treatment effect. However,
in a completely randomized design, the variance of outcomes is potentially very large, and the
sample sizes of treatment and control are randomly generated (List et al., 2011; Deaton and
58
Of course, this heuristic represents an overly conservative approach compared to our preferred MHT correction
method byList et al. (2016) that has more power than traditional approaches to correct for MHT.

39

Cartwright, 2018). As Duflo et al. (2007) point out, pure randomization only achieves balance
in expectation: in practice, especially in the case of smaller samples, randomization may yield
experimental groups that differ from each other along important observable dimensions. A
popular way to address this issue is to include covariates in the estimation ex post. However,
when data on subjects’ relevant observable characteristics are available prior to conducting the
experiment, it is preferable to use this information in the design phase and improve the overall
precision of the study through blocked randomization.59
Blocking (also knows as stratification) refers to the practice of dividing experimental subjects
into blocks (strata) by observable characteristics, such that randomization is performed within,
but not between, these blocks (List et al., 2011). More formally, blocking involves partitioning
the covariate space into a finite set and carrying out a completely randomized experiment within
each of these subsets (Athey and Imbens, 2017). Using Neyman’s repeated sampling approach,
we can estimate the average treatment effect within each block as the difference between the
average outcomes for treated and control subjects, then estimate the overall average effect of the
treatment by averaging the within-block estimates weighted by the share of subjects assigned
to the block. In case the share of treated subjects is the same in each block, this simplifies to
the difference in means between treated and control subjects – the same estimator we use for
completely randomized designs (Athey and Imbens, 2017). Blocked randomization is beneficial
because it increases precision: the estimated variance of the treatment effect is smaller once
we take into account the gains from stratification. Compared to ex post regression adjustment,
blocking in the design phase is preferred because it can ensure that the share of treated subjects
is the same in each stratum, minimizing the variance of the estimate overall.60
Despite popular beliefs to the contrary, blocking does not lower precision ex ante even when
the correlation between the outcome variable and the covariates on which we block is weak.61
Note that the same is not true for ex post adjustments: adding covariates that do not explain
the outcome variable in a regression increases standard errors by reducing the degrees of freedom (Duflo et al., 2007). Consequently, one should stratify on a rich set of covariates whenever
59

An alternative approach to dealing with covariate imbalance is re-randomization (Morgan and Rubin, 2012;
Bruhn and McKenzie, 2009; Banerjee et al., 2017b). Two commonly used forms of re-randomization are the “big
stick” method that requires a new random draw if the imbalance between treatment and control groups in the
resulting allocation exceeds a pre-specified threshold, and the “minimum maximum t-stat” method that suggests
performing multiple (1,000 or 10,000) draws, checking for balance each time, then choosing the draw with the
minimum maximum t-stat. Bruhn and McKenzie (2009) show that for very persistent outcome variables, and in
smaller samples, blocked randomization performs better than re-randomization.
60
Following Athey and Imbens (2017), we obtain the estimated variance of the treatment effect with blocked
randomization as follows (where g indexes blocks):
(7)

V̂ blocked =

G
X
g=1


V̂ (τ̂g )

Ng
N

2
where

τ̂g = ȲT,g − ȲC,g

and

V̂ (τ̂g ) =

2
2
σC,g
σT,g
+
.
nC,g
nT,g

Comparing the expression in (7) with the variance estimate for completely randomized experiments presented in
2
Section 6, σT2 /nT + σC
/nC , in general we find that the latter is more conservative.
61
See Athey and Imbens (2017) for an explanation and two important qualifications to this result.

40

possible, including continuous variables (Moore, 2012).62 The limit of stratified randomization
is a paired design where each block contains only two observations: a treated and a control
subject. While this approach has advantages in terms of precision, it complicates the subsequent estimation of variance (Athey and Imbens, 2017). Such a perfect matched pairs design
also comes at high attrition costs for the matched units, and should thus be applied with caution.
Stratification is also desirable when the researcher expects heterogeneity in response to the
treatment and wants to analyze subsamples separately. In this case, ex ante blocking maximizes
power for estimating the treatment effect for each subsample. Equally importantly, stratifying
on variables that the researcher ex ante deems as relevant increases the credibility of the study:
it demonstrates to the reader that the subsample analysis presented in the research paper was
actually planned in advance and is not merely the result of “data mining” or a “fishing expedition.” In this sense, blocking on variables to be used in subsequent heterogeneity analysis helps
address the problem of researcher bias discussed in Section 4, by limiting analytical flexibility
(Munafò et al., 2017). If a researcher uses blocking primarily for credibility reasons rather than
to increase precision, she should limit herself to blocking only on a few key variables. While hypothesis registries and pre-analysis plans (Christensen and Miguel, 2016; Coffman and Niederle,
2015) provide stronger remedies against specification searching, blocking has the advantage of
also increasing the power of the resulting subsample analyses. Note, however, that blocking
alone does not address all the issues that arise from subgroup analysis: even if the experimenter
“ties herself to the mast” by stratifying on the relevant dimensions, standard errors still need
to be adjusted ex post for multiple hypothesis testing (see Section 7 for more details).
Utilizing baseline information to an even greater degree than in the stratification case,
some researchers have recently suggested relying on optimization instead of randomization
for assigning treatment status to subjects. Bertsimas et al. (2015) propose a method based
on discrete linear optimization, such that assignment is chosen to minimize the discrepancy
between treatment groups in terms of means and variances of covariates. Kasy (2016) considers
the experiment as a statistical decision problem where the goal is to find the unique treatment
assignment that minimizes a Bayesian or minimax risk function (based on the mean squared
error of a point estimator). While these approaches have the potential to increase power, gains
in precision are substantial only when baseline variables strongly predict future outcomes (see
Bruhn and McKenzie (2009) for an illustration of this point), and come at the cost of more complicated inference (a bootstrap method is required to obtain the p-values of the estimates).63
Banerjee et al. (2017b) model the experimenter’s problem in a Bayesian decision theoretical
framework. They argue that for a given prior over treatment effects, there exists a deterministic treatment assignment that maximizes the experimenter’s expected utility, leading to the
62

A fascinating recent paper explores blocking on the predicted treatment effects and on subjects’ willingnessto-pay for the treatments in order to design an “ethical experiment” that takes subjects’ predicted welfare into
consideration (Narita, 2018).
63
For more details, see David McKenzie’s excellent blog post on the topic: https://blogs.worldbank.org/
impactevaluations/optimization-just-re-randomization-redux-thoughts-recent-don-t-randomize-optimize-papers.

41

proposition “Bayesians do not Randomize.” Once multiple decision makers (or a single decision
maker with different priors) are considered, however, they identify randomization as the only
method to yield results whose interpretation cannot be challenged.
Overall, we are of the opinion that optimization on the basis of baseline covariates may be
a useful method for assigning subjects to treatments in pilot studies. Sample sizes in pilots are
typically small, so an increase in power is crucial. Furthermore, it is easily justifiable to design
pilots so that they are most informative for a specific prior (that of the experimenter) rather
than for a wider audience with arbitrary priors. For most experiments, randomization with
“improvements” such as stratification or re-randomization remains more suitable.64

9

Use within subject designs when appropriate

In our overview so far we have focused on experiments consisting of a single period where each
subject is assigned to either the control or the treatment condition. These cases fall into the category of between subject (BS) designs, because the estimated treatment effect is obtained
through a comparison of means between the two groups. This represents the current state of
art when economists generate data. Of course, researchers have the choice to collect data in
multiple periods, allowing for the use of a within subject (WS) design, such that the same
individual experiences different treatment conditions in subsequent periods. In the following,
we discuss the benefits and threats associated with using within subject designs along the dimensions of statistical power, confoundedness, and heterogeneity in response to treatments.
Within subject designs have been advocated for their potential to yield more powerful
tests for the same cost than between subject experiments (Charness et al., 2012). In particular, they allow for estimations controlling for individual-specific effects, reducing the variance
of the treatment effect estimator to the extent that within-subject correlations explain the outcome (Frison and Pocock, 1992; McKenzie, 2012). Bellemare et al. (2016) suggest an approach
based on Monte Carlo simulations to compare the power achieved in BS vs. WS designs. They
provide a numeric example in the context of field experiments on gift exchange, and find that
a BS design requires 4-8 times more subjects than a WS design to reach an acceptable level of
statistical power (as discussed in Section 4, the conventionally required level of power is 80%).
They also emphasize that adding more experimental periods can substantially increase the statistical power of a WS design, but has very little effect in the BS design.65
64

See footnote 59 for details on re-randomization. As another possible design improvement, consider Wilhelm
et al. (2017)’s procedure based on an orthogonal greedy algorithm that uses pre-experimental data to inform,
rather than treatment assignment, the choice of both the sample size and the covariates to be collected.
65
While a within-subject design typically leads to lower variances for the estimated treatment effect, this is
not necessarily the case. See Keren and Lewis (1993) for more details on precision in WS versus BS designs in
the presence of treatment effects that are correlated with the individual-specific error term. For panel data with
a non-i.i.d. error structures, we recommend Burlig et al. (2017)’s power calculation method that accounts for
serial correlation in errors.

42

Note that in the above comparison we ignored cost considerations, assuming that collecting
data from n subjects twice (WS design) is as costly as collecting data from 2n subjects (BS
design). In practice, however, this is often not the case; in laboratory experiments, adding additional periods to an experiment often comes at no additional monetary cost for the researchers
(think of the typical practice of determining subjects’ earnings based on their behavior in one
period randomly selected at the end of the experiment). Field experiments, on the other hand,
often have large per-period fixed costs (e.g. hiring and training surveyors) that make additional
rounds of data collection more expensive on the margin.
Despite the fact that within subject designs have the potential to achieve better precision,
in practice researchers do not seem to take the choice of design into account when setting the
number of experimental subjects: surveying two recent volumes of the journal Experimental
Economics, Bellemare et al. (2014) find that the median number of participants per treatment
was relatively similar (43.5 and 50, respectively) for studies using BS and WS designs. They
also find that the majority of studies in their survey (41 out of 58) are based on BS experimental designs. The relative unpopularity of within subject designs may be due to the strong
assumption they require for inference. When the same subject is exposed to different treatment
conditions, within-subject comparisons only provide a causal estimate if there is independence
of these multiple exposures (Charness et al., 2012). There are different reasons why this
assumption may not hold, such as learning, history and demand effects, and sensitization to
perceived dependencies between treatments (List et al., 2011; Keren and Lewis, 1993).
As a result, findings from WS designs may be confounded and hard to interpret. Crossover
designs (where subjects are exposed to treatments in random order) may ameliorate, but not
eliminate, these fears. The extent to which confoundedness is a threat depends very much on
the particular research question. In some cases, one design is clearly better suited to test a particular theory (consider, for instance, predictions about individual preference reversals). When
treatments are suspected of having persistent psychological effects or when experimenter demand effect is a concern, researchers should be cautious when using WS designs.66 On the other
hand, skill-based experiments, such as the study of Smith et al. (1989) on eyewitness accuracy
and confidence, are less likely to yield a biased result under a WS design (Charness et al., 2012).
This trade-off between power and bias has been a central theme of the within versus between
subject debate. We would like to conclude this section by emphasizing another aspect of the
design choice that often receives less attention: between and within subject designs differ in
the extent to which they are informative of individual differences in response to treatment. As
we mentioned in the Preliminaries, a between subject comparison with random assignment to
treatment only produces an unbiased estimate of the average treatment effect, but does not identify other moments of the distribution. While the average treatment effect conveys important
66

See Bohnet et al. (2016) for a discussion on preference reversals between separate and joint evaluations and
Hsee (1996) for an overview of the literature on evaluability.

43

information regarding the existence of an association or effect, it may mask crucial differences
between participants in their reaction to treatment. For instance, assessing the share of the
population that was helped or harmed by the treatment requires knowledge of the distribution
of the difference between the outcomes of each individual in the presence and absence of the
program. In experiments using between-subject treatment assignment, each participant is only
observed in one of the states, thus welfare analysis requires additional, often strong, assumptions.67 Within person designs facilitate welfare calculations by measuring baseline outcomes
together with changes in response to the treatments for the different baseline values, providing the entire joint distribution rather than marginals. Allcott and Taubinsky (2015) use a
within-subject design in their information nudge experiment to estimate the average change in
valuation induced by their nudge for each level of initial valuation. This strategy allows them
to calculate the market demand curve and the average marginal bias, statistics they show are
sufficient for computing the welfare effects of a policy.
Further, WS designs can help researchers uncover heterogeneous treatment effects. Data
from WS experiments can be used to plot a histogram of the realized “individual linear differences,” calculated for each subject as the difference between their outcome in the treated
vs. the control state. Such histograms may be informative about important dimensions of heterogeneity, and could suggest subgroups that benefit most/least from the treatment.68 As an
example, consider the study of Hoel (2015) on the role of asymmetric information in dictator
games. While she finds that subjects on average give more in games when the choice is public
than when it is secret, a within subject comparison reveals that almost half of the participants
give the same amount in both conditions. Her design allows her to classify participants into
types based on their individual response to the treatment, such that types identified in the
laboratory also behave differently in the field. Finally, WS designs can help distinguish between
behavioral theories by showing heterogeneity in preferences within individuals. Gibson et al.
(2013), for instance, refute the type-based model of lying aversion by documenting differences
within individuals (across situations) in the estimated cost of lying.
In sum, we encourage researchers to carefully weigh the pros and cons of between and within
subject designs along the dimensions discussed above (power, cost, bias, learning about heterogeneity) and pick the one better suited to answering their particular research question. Just as
in the case of the choice between lab or field experiments discussed in Section 3, there is no universally preferred method: the choice to vary treatment conditions within or between subjects
should depend on the characteristics of the experiment and the nature of what information is
sought and the trade-offs the researcher is willing to make. In a nutshell, it depends on the
theory to be tested, the topic we discuss next.
67

Quantile regressions, for example, are only informative of the distribution of individual changes in outcomes
if a rank invariance condition is satisfied. Bedoya Arguelles et al. (2018) provide an excellent summary of different
methods aimed at identifying the distribution of individual specific treatment effects in RCTs.
68
It is important to emphasize that WS designs on their own still do not allow identification of the distribution
of treatment effects. Readers interested in identifying the probability distribution of individual effects in panel
data should consult e.g. Arellano and Bonhomme (2012).

44

10

Go beyond A/B testing by using theoretically-guided designs

In Section 6 and the discussion of optimal experimental design that followed, we explicitly focused on experiments that aim to estimate average treatment effects (ATE), that is, to measure
whether one treatment yields a different mean outcome than another treatment. An example for
such studies is A/B testing, a method common both in research and in business, that consists of
showing subjects one of two versions of the same product or service at random, and comparing
responses. Yet, the experimental method will never reach its true potential unless we go beyond
simply documenting an effect. More specifically, what separates the experimental method from
the other empirical approaches is that experiments can generate data to explore the underlying
mechanisms at work, or the whys behind the data patterns observed. In this manner, we need
to design experiments tightly linked to economic theory to reap the true benefits of the experimental method.
Accordingly, we advocate for experimental economists to use economic theory to inform
their experimental design whenever possible (Banerjee, 2005; Heckman, 2010; List, 2011), and
to incorporate results from experiments into existing economic theory (Deaton and Cartwright,
2018), thereby creating a feedback process that guides the development of theory and the design of future experiments (Duflo et al., 2007). Combining economic theory with experiments
allows researchers to estimate a wider array of parameters (Attanasio and Meghir, 2012), to
perform counterfactual analysis, to test theories directly (Browning and Chiappori, 1998), and
to account for general equilibrium and welfare effects.
Despite its advantages, using theory is still not a commonplace occurrence: out of all the
experiments published in the Top Five journals in Economics (American Economic Review,
Econometrica, Journal of Political Economy, the Quarterly Journal of Economics, Review of
Economic Studies) between 1975 and 2010, 68% were “descriptive”, meaning that they lacked
an economic model; of the remaining articles, only 14% allowed for more than a single model,
either by directly comparing competing models or by estimating one or more structural parameters (Card et al., 2011). In the following, we review in detail the benefits from designing
experiments in connection with economic theory.
First, we can use theory to explicitly model selection into the experiment. When running
lab experiments, AFE, and FFE, researchers should make explicit their assumptions about selection by using theory and, in doing so, address some of the concerns about generalizability
discussed in Section 1. For example, a variant of the Roy model can be used to model selection
into the experiment (Heckman, 2010).
Using economic theory jointly with an experiment allows the researcher to perform structural estimation to estimate the parameters of a theoretical model from data collected in an
45

experiment. This enables researchers to perform counterfactual policy analysis: to predict the
effects of policies or programs that have not yet been implemented and over which no data
are available (Heckman, 2010).69 Therefore, economic theory allows researchers to extrapolate
the results of existing experiments to other populations and settings (Banerjee, 2005; Falk and
Heckman, 2009, see Section 12 for a discussion of scalability). The theory-free nature of RCTs
is a serious disadvantage in attempting to generalize (Deaton and Cartwright, 2018). By combining experiments with theory we can reap the best of both worlds: preserving causal inference
(due to the exogenous variation created by the experiment) and improving the generalizability
of predictions through theory and structural estimation (for a recent example see Della Vigna
et al., 2012).
Economic theory is also useful in considering spillover effects where subjects impose externalities on others (for example, those in treatment can inform their friends in the control),70
and general equilibrium (GE) effects, by which agents react to the intervention in a way
that changes the environment itself (Duflo et al., 2007; Maniadis et al., 2015).71 GE effects
are of great importance in diverse contexts, such as economic development (Banerjee, 2005;
Acemoglu, 2010), the value of a life (Jena et al., 2008), and the effects of Medicare on health
care spending (Finkelstein, 2007). However, most experiments are conducted assuming partial equilibrium, i.e. assuming away spillover and GE effects.72 This is partly due to the fact
that measuring GE effects requires an experimental design that explicitly accounts for them.73
Disregarding the GE effects of experiments might be justified for small interventions that only
affect a few participants or that have a small impact, such as in lab experiments and artefactual
field experiments (AFE). However, field experiments (FFE and NFE) can induce changes in the
local economy that could translate into general equilibrium effects. Neglecting these GE effects
biases the results and leads to misleading conclusions about the true effect of the intervention.
This is especially true for large-scale interventions claiming to have a large impact. Such experiments should attempt to include measuring the general equilibrium effects as part of their
experimental design whenever possible.
Economic theory also allows for an analysis of the welfare effects of experiments: in addition to measuring the traditional outcomes of the experiment, researchers can use theory
to infer the change in the well-being of subjects as a consequence of treatment. The importance of measuring a program’s welfare effects has been widely acknowledged (Heckman, 2010).
69

For a discussion of structural estimation and reduced-form estimation, see Nevo and Whinston (2010).
Note that spillover effects violate the “Stable Unit Treatment Value Assumption” (Angrist et al., 1996; Duflo
et al., 2007) that we discussed in the Preliminaries.
71
For the rest of the section, we will use the term “general equilibrium effects” to also include spillovers.
72
An important exception is Burke et al. (2014), who show (in a microcredit context) how taking GE effects
into account changes the size of the effect of the intervention, what has relevant policy implications.
73
For example, Crépon et al. (2013) included two levels of randomization in their experiment, one of them being
the proportion of individuals assigned to treatment, allowing them to capture the GE effects of their intervention.
Another example is Cunha et al. (2017), who estimate the GE effects of cash versus in-kind transfers in Mexican
villages.
70

46

Recently, welfare analysis has made its way into behavioral economics, allowing a fruitful collaboration between theory and field experiments. An early example of a study that measured
the welfare effects of a natural field experiment using structural estimation is DellaVigna et al.
(2012), who determined that a door-to-door campaign of charity donation decreased the welfare
of the average household due to the social pressure associated with not donating.74 However,
welfare calculations may be sensitive to certain details. First, the particular theory researchers
base their calculations on has important consequences for the conclusions on welfare (JimenezGomez, 2018). This aspect is even more salient in areas where individuals are known to be
subject to behavioral biases.75 Second, GE effects should be included in welfare calculations.
As an illustration, consider Handel (2013)’s study of health insurance markets where substantial
inertia had been observed. He structurally estimated a choice model, in order to compute the
welfare effects of a counterfactual “nudge reminder”, and concluded that although the nudge
would increase the rate at which people selected into plans that better matched their needs
(reducing inertia), the GE effects would exacerbate adverse selection, leading to a price increase
and a loss in average welfare. Finally, researchers should pay attention to potential heterogeneity
(see Sections 1 and 12). Heterogeneity is crucial when computing welfare effects, because different individuals may benefit (or suffer) to varying degrees from a given program, and therefore
the distribution of welfare can be very different for some subpopulations (Jimenez-Gomez, 2017).
Using economic theory is not without caveats. It is always possible that the particular
economic theory we consider is wrong, and this concern has been particularly exacerbated with
the rise in importance of behavioral economics (Banerjee, 2005). Moreover, structural estimates
are sensitive to assumptions about functional forms and distribution of unobservables (Heckman,
2010). The correct design of the experiment can never be undermined due to confidence in the
theory.76 To conclude, we would like to emphasize what we are not advocating. We do not call
for every experiment to be derived from economic theory or to be structurally estimated. There
are occasions when a descriptive study is perfectly appropriate, for example when attempting
to differentiate between several competing theories whose predictions go in opposite directions
(Acemoglu, 2010). We also do not advocate for journals to demand that authors include ad-hoc
economic models after the experiment has been conducted and the data analyzed. Such models
add little value in our opinion and can confuse readers as to the true intent and nature of
the studies. We do believe that there is value in descriptive experiments, but the limitations
74
Related studies test specific aspects of behavioral theory and their welfare consequences through field experiments (Zhe Jin et al., 2010; Bernheim et al., 2011; Allcott and Taubinsky, 2015; Allcott and Kessler, 2019;
DellaVigna et al., 2017, 2016), and develop theories that explicitly consider how behavioral economics affects welfare measurement (Spiegler, 2014; Gabaix and Farhi, 2017; Jimenez-Gomez, 2017). Finkelstein and Notowidigdo
(2018) use a randomized natural field experiment to test two competing explanations – based on neoclassical
and behavioral theory, respectively – for the low take-up of SNAP benefits, and estimate the welfare impact of
different interventions aimed at increasing take-up.
75
When individuals face behavioral biases, welfare calculations using only the demand curve (and therefore
ignoring those biases) can be potentially mistaken by orders of magnitude, and even have the wrong sign (Baicker
et al., 2015).
76
Card et al. (2011) claim this was the case in the negative income tax experiments done in the late 1960s and
early 1970s.

47

of these types of studies should be explicitly acknowledged. We also believe that in order to
make generalizable predictions, using economic theory to design experiments and to guide the
analysis is often the correct choice (Heckman, 2010; Acemoglu, 2010). Theory is portable.
Thus, understanding underlying mechanisms is the key to unlocking the true gains from the
experimental science in economics.

11

Focus on the long run, not just on the short run

In economics, experiments often tend to focus on short-term substitution effects. This is probably due to the fact that designing an experiment that follows up subjects for months or years
is substantially more costly: the logistics required become complex, and there is a need to
incentivize subjects to come back for follow-ups to avoid attrition.77 In addition, there is always an implicit opportunity cost in having a longer experiment, because it will take longer to
complete such a project than a similar study focusing only on the short-term effects, delaying
publication.78 However, understanding the long-term effects of interventions is critical. For
example, demand elasticities can be very different in the short-run vs. the long-run (Huang
et al., 2017). Long-term effects are especially relevant when the interventions are programs
that governments or other organizations intend to roll out in large scale (we discuss scalability
in Section 12). As we emphasized in Section 10, it is fundamental to take into account the
general equilibrium (GE) effects of the implemented policies and programs. However, those GE
effects often need time to manifest, so measuring the long-term effect of interventions is even
more important. Moreover, Hawthorne, John Henry, and experimenter demand effects can in
principle be identified by collecting long-run data (Duflo et al., 2007).
In addition, the return on investment (ROI) per dollar will be much larger if the effects persist in the long-term. For example, consider Levitt et al. (2016), who provided incentives to high
school freshmen for improvements in academic performance: it is crucial to understand whether
the effects of the incentives are long-lived. Students were provided incentives conditional on
meeting an achievement standard (with an expected value of $50 per month) for 8 months, and
were followed up for five years. The incentives had a significant positive effect on academic
achievement during the first year, but not during the second year and onwards. Studies like
this show the importance of following subjects for a period of time after the intervention, and
reporting the long-term effects, even if they are not significantly different from zero. Despite
the widespread focus on the short-run in experimental economics, there are several notable papers which analyze the medium- and long-run effects of experimental interventions.79 Rogers
77

For example, (Charness and Gneezy, 2009) paid 50 US dollars to subjects for each of two follow-ups.
Note, however, that researchers could in principle continue collecting data after the first results have been
published (Banerjee, 2005). Moreover, it is sometimes possible to track subjects over time, without the need to
incur additional costs; for example when the subjects’ data is already being collected for some other reason, such
as administrative purposes, or when partnering with a firm.
79
In the areas of: exercise (Charness and Gneezy, 2009; Milkman et al., 2013; Royer et al., 2015; Acland and
Levy, 2015), smoking cessation (Volpp et al., 2006, 2009; Giné et al., 2010), weight loss (Volpp et al., 2008;
Burke et al., 2012; John et al., 2011; Anderson et al., 2009), charitable giving (Meier, 2007; Shang and Croson,
78

48

and Frey (2016) offer a review of the behavioral science on how interventions work over time.
The evidence on whether interventions work in the long-run is mixed even when restricted to a
single subfield. As an example, consider Brandon et al. (2017) who find that out of ten studies
included in their review (out of the fields of charitable giving, education, exercise, smoking cessation and weight loss), four are consistent with habit formation, whereas the other six are not
(see Figure 2, plotting the proportion of the intervention’s effect that persists after incentives
are removed). Moreover, even when the effects persist, they decay rapidly: only two of the
aforementioned studies found more than 25 percent of the initial effect after a month, and only
one found any persistence after six months.
Figure 1: Persistence in Habit Formation Literature
1.00

A. Charitable Giving

B. Education

0.75
0.50
Meier 2007

Proportion of Effect Persisting

0.25
0.00
1.00

Landry et al. 2010

Levitt et al. 2016

C. Exercise

0.75

D. Smoking Cessation

Charness and Gneezy 2009

Volpp et al. 2009

0.50
Milkman et al. 2014
Acland and Levy 2015
Royer et al. 2015

0.25
0.00
1.00

Volpp et al. 2006

Allcott and Rogers 2014
Site 2

E. Weight Loss

0.75
Allcott and Rogers 2014
Site 1
Allcott and Rogers 2014
Site 3

0.50
0.25

Volpp et al. 2008

0.00
0

50

100
150 0
50
Weeks Since End of Intervention

100

150

Notes: Each point represents the proportion of the initial treatment effect that persists
Reproduced for
witha given
permission
from
Brandon
et al.
“each
point represents
the proportion
of the initial
amount
of time
since the
end(2017):
of a given
intervention.
All observations
are
treatment that
persists
for aestimates
given amount
of time
since
the end of astudies
given intervention
[...] insignificance
at the
based
on point
presented
in the
corresponding
with insignificance
at
five percent level
constituting
persistence
of zero.”
the five
percent level
constituting
persistence of zero.

Figure 2: Persistence of effects across domains

There are two potential and non-excluding reasons why researchers may find small effects
of interventions in the long run, with very different implications for the actual existence of a
treatment effect. The first, and most obvious one, is that the effect of the interventions is truly
2009; Landry et al., 2010), water conservation (Ferraro et al., 2011; Ferraro and Price, 2013), energy conservation
(Allcott and Rogers, 2014; Brandon et al., 2017), voting, (Gerber et al., 2003) labor effort (Gneezy and List,
2006) and education (Jackson, 2010; Jensen, 2010; Walton and Cohen, 2011; Rodriguez-Planas, 2012; Levitt et al.,
2016). Note that some of these papers would be better classified as belonging to the literatures on psychology,
medicine and behavior change, but are included here for completeness.

49

zero in the long run. This could happen if the intervention is not successful in changing behavior
in the first place, either because subjects do not enroll in the relevant treatment, because the
incentives or nudges are “too weak,” or because the treatment changes behavior in a way different than expected. Even if the treatment is successful in changing behavior in the short-term,
subjects could revert to baseline behavior over time even in the presence of the intervention: as
the novelty of the incentives tapers off, or if the intervention crowds out intrinsic motivation,
subjects revert to old habits.
Yet, there is a second explanation for the zero effect observed in the long-run in most experiments, which can be described as “attenuation bias over time.” Consider a field experiment
where subjects are assigned to a treatment zi ∈ Z. The researchers would like to measure the
Average Treatment Effect at time t, τt∗ = E[(yi1t − yi1t0 ) − (yi0t − yi0t0 )].80 Note, however,
that they can only measure τ = E[yit − yit0 |zi0 = 1] − E[yit − yit0 |zi0 = 0].81 Importantly, zi0
refers to the original assignment at time t0 , but subjects could have changed their behavior over
time (effectively changing their treatment, which is a form of non-compliance, see Section 1).
Subjects self-select into other treatments over time: we can think of this change as probabilistic,
happening with a higher probability when the gains in changing treatments are high and the
cost of changing is low. In the Appendix we formally show that as time goes by, attenuation
bias increases and that, in the limit (as time goes to infinity), τt → 0.

82

If selection into the

experiment (pi = 1) is correlated with the utility (positively) or the cost of changing treatments
(negatively), then attenuation bias will be exacerbated in overt experiments compared to natural
field experiments. When researchers find an ATE close to zero due to attenuation bias over time
in a an overt experiment, it does not mean that the treatment did not work in the first place, but
that subjects who selected into the experiment (pi = 1) found a way to improve their outcomes
over time, even when they were initially assigned to control.83 A solution to attenuation bias
over time is to measure the effect of the intervention with respect to a control group that did
not self-select into the experiment (p = 0): τ 0 = E[yit − yit0 |p = 1, z0 = 1] − E[yit − yt0 |p = 0],
where yit = yidit t is the outcome for individual i at time t. Note that τ 0 controls for changes
that happen in the treated group, with respect to a group that is not part of the experiment.84
This also allows researchers to realize when the treatment works and has spillover effects, which
80

Where yidt is the outcome of individual i in treatment d at time t, and t0 is the time at which the intervention
starts.
81
Where yit = yidit t is the outcome of individual i at time t, given the fact that individual i is in treatment
dit .
82
Formally, we assume that opportunities for changing treatment happen following a Poisson distribution, and
that the change of treatment follows a Markov process.
83
This could be either due the fact that those who selected into the experiment were more motivated, and hence
more likely to find ways to select into programs (outside of the experiment) that would help them, or because
the experiment had spillover effects (see Section 10) and those initially assigned to the control group were more
likely to be affected by the treatment because of the fact that the experiment was being run (for example, their
friends in the treatment group informed them about the ways to get enrolled in similar programs, etc).
84
This can be seen because τ 0 gives us the correct measurement in the extreme cases when either the treatment
has no effect but selection into the experiment has an effect (in which case τ 0 = 0), or when selection into the
experiment has no effect but the treatment has an effect (in which case τ 0 = τ ∗ ).

50

could be otherwise mistakenly seen as the treatment having no effect.85
In summary, measuring the long-run effects of interventions is crucial for several reasons,
including general equilibrium (GE) and welfare effects. In general, studying long-run effects
should be a routine practice of experimental economics whenever doing so enhances our scientific
understanding of the theory, mechanisms, or facts around the question.

12

Understand the science of scaling ex ante and ex post

In the previous sections, we have discussed problems related to statistical inference, generalization and reproducibility. These issues become especially salient when researchers attempt to
scale up their interventions, i.e. to extend them to a population which is larger (and usually
more diverse) than the original one. Unfortunately, out of the enormous amount of program
evaluations performed today, it is a common occurrence that such programs are never scaled
and, when they are, the effect size diminishes substantially (a phenomenon called “voltage drop”
Al-Ubaydli et al., 2017a). There are many reasons why the voltage effect might occur, but the
science of using science is at such an early stage there is little known definitively. It is crucial
to understand the reasons why scaled-up programs often do not work as intended. Yet, similar
to Al-Ubaydli et al. (2017c), we believe currently the problem is too narrowly defined in two
important dimensions. First, when a program is scaled, understanding its relative benefits and
costs are both invaluable, not just an accounting of benefits, as described in the implementation
science literature discussing voltage effects. Second, whereas that literature tends to focus on
program fidelity as a major reason for lack of scaling, we see three main areas that organizations face in scaling up, in terms of the potential changes to their costs and benefits. Al-Ubaydli
et al. (2017c) provide a formal model of how these three factors arise in the market for scientific
knowledge. We simply sketch them here to make our points of what experimenters should be
aware when completing their design, analysis, and interpretation.
Statistical inference. In Section 4, we introduced the concept of the post-study probability (PSP, Maniadis et al., 2014): the probability that a declaration of a research finding made
upon statistical significance would actually be true. We discussed how insufficient statistical
power combined with bias resulting from specification searching may lead to low post-study
probabilities. We further explained that as more researchers investigate the same relationship
independently of each other, the probability that a statistically significant effect or association
truly exists becomes smaller. Moreover, studies with “surprising” results (i.e. low prior probabilities of being true) tend to be published in journals more often, in turn resulting in low PSP.
For all these reasons, a program that is selected on the basis of just one successful initial trial
may fail to produce effects when scaled. Furthermore, the phenomenon of effect inflation, i.e.
obtaining an exaggerated estimate of the magnitude of a true effect (discussed in Section 4),
85

See Section 1 for a discussion of selection into experiments and its impact on generalizability, and Section 10
on spillover effects.

51

implies that a program that is scaled after a single study is likely to yield a much smaller effect
than the original study.
To curb these problems, Al-Ubaydli et al. (2017a) advocate to advance results to the policymaking stage only once the PSP passes 95%. Crucially, the PSP can be raised substantially if
the initial positive finding is followed by at least two successful replications (Section 5). Moreover, successful replications in different contexts are valuable in ensuring generalizability (Duflo,
2004; Muralidharan and Niehaus, 2017, see Section 1). Replication may also be used to measure
average within- and across-study variation in outcomes: when these are close, the concern about
across-context generalizability is reduced (Vivalt, 2017).86
Representativeness of the population. Heterogeneity in populations is a problem for
scalability, as discussed in the context of generalizability in Section 1. During scaling, the population that selects into the experiment can often be different than the original population,
raising the concern whether the original results generalize. A reason why scaling might not be
effective is that the estimate of the treatment effect is different in the new population (usually
smaller). Another related source of “scaling bias” is adverse heterogeneity, in which experimental participants’ attributes are correlated with higher outcomes (Al-Ubaydli et al., 2017c), for
example if there is participation bias (those who select into the experiment are different than
the rest of the population, Al-Ubaydli and List, 2013). Moreover, because of publication bias
(Section 5), researchers have incentives to find participants who yield large treatment effects
(Al-Ubaydli et al., 2017c). These effects make the benefits of scaling up smaller than those of
the original intervention. Finally, another concern for scalability related to the population is
attrition, an issue we discussed in Section 1.
Representativeness of the situation. The first and most obvious change when scalingup is infrastructure: scaled-up programs often need a larger and more complex infrastructure to
support them, what can increase their cost. Moreover, programs are often run by particularly
high-quality officials or NGOs in a way that is hardly possible to scale up: as the program is
expanded, its quality may deteriorate (Duflo et al., 2007). When scaling, researchers need to be
aware that more workers must be hired, and they might be of lower quality or have less interest
in the program, simply due to the diseconomies of scale associated with requiring more labor
in a labor market with inelastically-supplied human capital (Al-Ubaydli et al., 2017c; Davis
et al., 2017; Banerjee et al., 2017a). This has the effect to reduce the benefits of the program
(through lower-quality program workers) and/or increase its costs (associated with trying to
keep the quality of workers high). In addition, there can be a change in the cost of recruiting
more subjects, and this cost can either increase or decrease, depending on the particular implementation of the program (for example, a program that automatically enrolls citizens can have
lower marginal costs, once the setup for recruitment is ready).
86
Other issues related to generalizability are experimenter demand effects (Duflo, 2004) and noncompliance;
we discuss these issues at length in Section 1.

52

General equilibrium (GE) effects are crucial when considering scaling up interventions
(Banerjee et al., 2017b; Al-Ubaydli et al., 2017b, also Section 10 for the importance of GE
effects). GE effects can cause researchers to underestimate the effect of their interventions in
at least two ways: if there are spillovers from the treated groups to the control groups, in such
a way that the ATE was biased downwards in the original intervention because the control
group was benefiting from treatment; and if there are complementarities between those treated,
for example in an educational intervention in which students benefit not only from their own
treatment, but also from having treated peers.
Dramatic evidence of the first type of effect is contained in List et al. (2018), who examine
a randomized field experiment among 3-5 year olds in Chicago described in Fryer et al. (2015,
2017). They find that each additional treated child residing within a three kilometer radius
of a control child’s home increases that child’s cognitive score by 0.0033 to 0.0042 standard
deviations. Given that an average child in their sample has 178 treated neighbors residing
within a three-kilometer radius of her home, on average, a child gains between 0.6 to 0.7 in
cognitive test scores and about 1.2 in non-cognitive test scores in spillover effects from her
treated neighbors. These are large spillovers, which serve to highlight that the program at scale
would have much larger effects than the Fryer et al. (2015, 2017) summaries of the research
program predicted, ceteris paribus.
On the other hand, researchers can overestimate the effect of the intervention if they do not
take into account its crowding out effect (Crépon et al., 2013).87 Moreover, the interventions
that work when scaling up might be more complex than those employed while piloting the
program.88 For example, in the case of long-term, chronic medical conditions, the most effective interventions are usually complex combinations of basic interventions (Al-Ubaydli et al.,
2017a).89 Therefore, the benefit of a scaled-up program can be higher or lower than that of the
original intervention, depending on the direction of the GE effects.
In light of the scaling problem, there are several factors researchers should consider when
designing their interventions – issues that are often not taken into account when designing
the original experiment or program.90 Fortunately, researchers can include these caveats in
their initial cost effectiveness calculations, before rolling out the experiment. Our main recommendation is that researchers “backward induct”, having the issue of scaling already in mind
when designing their experiments and programs. First, as we argued in Section 10, researchers
87
A related issue is that of construal, i.e. the subjects’ subjective understanding of the intervention. Paluck and
Shafir (2017) argue that scaling up mandatory arrest of abusive domestic partners (as a result of an experiment)
backfired due to the construal of (what it meant) calling the police in that situation.
88
Some people might suffer from psychological switching costs (Klemperer, 1987; Carroll et al., 2000), or
hyperbolic discounting, which would tend to inhibit the creation of new habits (see also Section 11). Moreover,
people have limited cognitive resources to make complex decisions (Simon, 1972; Shah et al., 2012).
89
For example, combining educational sessions, counseling, and a selection of reminder methods.
90
Interestingly, these issues could be exacerbated “when rolling out revolutionary ideas, as these often challenge
the power and established practices of incumbent organizations” (Al-Ubaydli et al., 2017c). Because of that,
programs with greater community coalition functions, communication to stakeholders, and sustainability are
more likely to still be in place over two or more years beyond their original funding (Cooper et al., 2015).

53

should go beyond A/B testing to the whys of the phenomena they want to study, using existing
evidence to build theories that can help explain the experimental results. This feature provides
a theoretical basis for fidelity in the Al-Ubaydli et al (2017c) model. Researchers should also
consider whether their program is likely to generalize, being especially sensitive to heterogeneity
across populations and contexts (Section 1), and choose the optimal experiment type in light
of their scaling goals (an issue we discussed in Section 3).
An approach that is likely to result in better generalization is to first consider the (large)
population of interest, take a representative (smaller) sample of observations, and then randomize those to treatment or control (Muralidharan and Niehaus, 2017).91 Compliance to the
program must be taken seriously into account, and lessons derived from earlier interventions
can help in this endeavor: Al-Ubaydli et al. (2017a) argue that there is much to be learned from
medical researchers who have been rigorously studying a similar problem for years.92 Because of
that, economists could structurally estimate behavioral models (DellaVigna et al., 2012; Allcott
and Taubinsky, 2015) as they seek to scale results, as we argued in Section 10. Moreover, the
issue of diseconomies of scaling in hiring workers to scale-up an intervention can be tackled by
randomizing the hiring process itself, in such a way that researchers can actually estimate the
loss in worker quality as they move along the supply curve, as part of their experiment (Davis
et al., 2017).
In addition, GE effects can be estimated by using large units of randomization whenever
possible (for example, randomizing at the school level instead of at the student level, Muralidharan and Niehaus, 2017).93 It is important to evaluate the implementation, and document all
steps of the process (Duflo et al., 2007; Banerjee et al., 2017a), including having a pre-analysis
plan, and to ensure that the programs that are evaluated in the first place are modular, in the
sense that what is implemented is described by a simple protocol (Banerjee, 2005). Both clinical
researchers and economists can greatly benefit from following experimental best practices, and
think ahead of time to the issue of scaling-up, including ensuring appropriate sample sizes and
sufficient power (Section 6), sound inference (Sections 4 and 7) and replication (Section 5).

Conclusion
When new areas of inquiry arise in the sciences they are oftentimes greeted with much skepticism, yet if they prove fruitful they grow quickly but many times non-optimally. Such an
oft-observed pattern effectively results in a missed moment to advance knowledge, and back91

For example, Muralidharan and Sundararaman (2015) first sample a “representative universe” of villages with
a private school, and then randomly assign each of them to treatment or control in a school choice experiment.
92
In particular, non-adherence to medication can lead to financial and personal costs – incentives that nonetheless seem too weak to motivate individuals.
93
A prominent example is that of Miguel and Kremer (2004), who realized that there were spillover effects
of deworming programs, by randomizing the programs at the school (rather than the student) level. However,
Banerjee et al. (2017a) warn that sometimes it is difficult to know ex-ante what randomization unit will be large
enough to capture all GE effects.

54

tracking on ill-advised research journeys often is difficult. We now find ourselves in a phase of
rapid growth of the experimental method in economics, especially as applied in the field. To
help ensure that we seize this opportunity to maximize the scientific knowledge created using
experiments, we take this occasion to step back and attempt to set up some guard rails by
crafting a 12 item wish list that we hope scholars can do more of in their own research. By
creating such a list we are not implying that these 12 items are entirely ignored in the extant
literature; indeed, quite the contrary in some lines of work certain items are prominent.
While picking a dozen items for such an exercise is akin to picking one’s favorite research
project or one’s favorite child, we nevertheless attempt in this tome to do just that. Rather
than regurgitate how our twelve items span three bins that are usefully summarized by three
questions, we wish to close with a few items that we find important but just missed our wish list.
Our first addition is a call to define the research questions and analysis plan before observing
the outcomes of an experiment – a process known as preregistration (Nosek et al., 2018). This
approach has proven very effective in a plethora of other fields, most notably medical trials, in
limiting specification searching and bounding Type I errors, and we fully anticipate considerable
scientific gains in economics thanks to this movement (Christensen and Miguel, 2016; Coffman
and Niederle, 2015). The reason we did not include it on our list, though it did permeate certain aspects of our discussion (see Section 8), is that this push has taken place and is relatively
far along already: the Open Science Framework’s database has received 18,000 preregistrations
since its launch in 2012, with the number roughly doubling every year, and more than 120
journals in various fields now offer registered reports. As such, while we strongly recommend
pre-registering experiments, we feel that its inclusion in our wish list would add less value than
other, less-discussed items.
Our second addition emphasizes the need to work on issues of first order import, and disseminate results from economic experiments more broadly. Academic researchers, responding
to career incentives that are almost exclusively tied to publications in peer-reviewed economics
journals, typically spend little time and effort communicating their findings to a wider audience.
While the profession has made progress towards experiments that produce relevant, generalizable and scalable results, researchers are typically not rewarded for getting involved with the
actual larger-scale implementation of their results. As a result, even the most important new
scientific findings with direct practical relevance often take long to reach policymakers or practitioners, and when they do, they are often misrepresented. To change this practice, we urge
researchers to make their results more available by creating informative press briefings, posting
summaries and non-gated versions of their published papers online, exploiting the opportunities offered by social media, and by establishing contact with policymakers and practitioners
interested in applying their findings in practice. Furthermore, we see great potential gains in
engaging with the scientific community more broadly, both by working more closely with re-

55

searchers from other social science disciplines, and by following the methodological discussions
in other experimental sciences such as biostatistics or neuroscience.
We end our wish list with a call to the experimental economics community to continue
engaging in the discussion to improve our field. While Samuelson’s famous quip that doing
methodological research is akin to doing calisthenics remains true today, we hope that our work
provides a useful starting point for those new to this discussion. We encourage researchers who
have been actively shaping this debate to create their own wish lists, or to share with us items
that we have missed in ours since choosing one’s favorite methodological points is often fraught
with err and oversight, and we trust that our list contains both.

56

Appendix
Generalizability Threat III: noncompliance case
We consider here the more general case of Threat III to generalizability (see Section 1), for
the case of noncompliance. The issue of selective noncompliance can be exacerbated by nonrandom selection into the experiments, in which case pi is correlated with (zi , di ).94 In that
case, there is a problem of generalizability when rolling the program to the entire population.95
If the researchers are interested in the effect of the program λ∗ = E[yi1 − yi0 |ω F F E , d(zi = 1) =
1, d(zi = 0) = 0], we have that
λ∗ = P[pi = 1] · LATEp=1 + P[pi = 0] · LATEp=0 ,
where LATEp=1 is defined as in Equation 1, and LATEp=0 is defined analogously for those
with pi = 0.96 We can calculate the bias as we did in Equation 2 for the case of non-compliance,
which is given by P[pi = 0] · (LATEp=1 − LATEp=0 ).97 If the value of LATEp=0 is very different
from LATEp=1 , then the estimate from the FFE is not generalizable because, for most FFE,
P[pi = 0] is much larger than P[pi = 1]. However, the estimate from FFE will be generalizable
when LATEp=1 ≈ LATEp=0 , and this can happen if p is independent of (zi , di ):
pi ⊥⊥ (zi , di )|xi

(Compliance Independence Condition).

Generalizability framework from Al-Ubaydli and List (2013)
We present a very similar version of the framework in Al-Ubaydli and List (2013), which was
based on Heckman (2000)].
We say D has local generalizability if
∀(x, x0 , z) ∈ D, ∃ > 0 : B (x, x0 , z) ⊂ D ∪ ∆(R).
Note that if D is an open set, then D has local generalizability, even if D has zero generalizability.98 We say that D has global generalizability (of size M ) if DM ⊂ D ∪ ∆(R),
where
D = {(x, x0 , z) : ∃(x̄, x̄0 , z̄) ∈ D with (x, x0 , z) ∈ B (x̄, x̄0 , z̄)}.
94
For example, if the stakes involved may affect both the selection decision into the experiment pi , and then
the subsequent behavior in response to the treatment di , or if those who select into the experiment are more
likely to comply with their assigned treatment, or the opposite, those who select into the experiment are more
likely to choose a particular d no matter what zi is.
95
See also Section 12 for issues on scalability.
96
That is, LATEp=0 = E[yi1 − yi0 |ω F F E , di (zi = 1) = 1, di (zi = 0) = 0, pi = 0].
97
Because LATEp=1 − λ∗ = (1 − P[pi = 1]) · LATEp=1 − P[pi = 0] · LATEp=0 .
98
This is because if D is open, then around each point (x, x0 , z) ∈ D we can always find a small enough open
ball around (x, x0 , z) that is contained in D.

57

Note that global generalizability of size M > 0 implies local generalizability. Moreover, if
D is finite, local generalizability implies global generalizability for some M > 0.99

Attenuation bias over time
Let τt = E[yit − yit0 |zi0 = 1] − E[yit − yit0 |zi0 = 0]. Since zi0 is random, then we have that
E[yit0 |zi0 = 1] = E[yit0 |zi0 = 0], and hence
τt = E[yit |zi0 = 1] − E[yit |zi0 = 0].
We assume that each individual changes treatments with a constant probability that is
positive if the new treatment has a higher outcome, and zero otherwise. For each individual,
this generates a Markov Chain, and the stationary distribution πi is such that there is a mass 1
of probability at the treatment with the highest outcome (it is an absorbing state) for individual
i. Therefore, as t → ∞, each individual converges to the treatment with the highest outcome.
Again, because zi0 is random, that means that distribution of stationary distributions πi is
equal on expectation for zi0 = 1 and zi0 = 0, and therefore:
lim τt = lim E[yit |zi0 = 1] − E[yit |zi0 = 0] = E[y · πi |zi0 = 1] − E[y · πi |zi0 = 0] = 0.

t→∞

99

t→∞

In particular, for M = min{ : B (x, x0 , z) ⊂ D ∪ ∆(R)∀(x, x0 , z) ∈ D}.

58

References
Abadie, A. (2018). On Statistical Non-Significance. NBER Working Paper No. 24403.
Abadie, A., Athey, S., Imbens, G., and Wooldridge, J. (2017). When Should You Adjust
Standard Errors for Clustering? NBER Working Paper No. 24003.
Acemoglu, D. (2010). Theory, General Equilibrium, and Political Economy in Development
Economics. The Journal of Economic Perspectives, 23(4):17–32.
Acland, D. and Levy, M. R. (2015). Naiveté, Projection Bias, and Habit Formation in Gym
Attendance. Management Science, 61(1):146–160.
Al-Ubaydli, O. and List, J. A. (2013). On the generalizability of experimental results in economics: with a response to Camerer. NBER Working Paper No. 19666.
Al-Ubaydli, O. and List, J. A. (2015). Do Natural Field Experiments Afford Researchers More
or Less Control Than Laboratory Experiments? American Economic Review, 105(5):462–466.
Al-Ubaydli, O., List, J. A., LoRe, D., and Suskind, D. (2017a). Scaling for Economists: Lessons
from the Non-Adherence Problem in the Medical Literature. Journal of Economic Perspectives, 31(4):125–144.
Al-Ubaydli, O., List, J. A., and Suskind, D. (2017b). The Science of Using Science. In preparation for the International Economic Review.
Al-Ubaydli, O., List, J. A., and Suskind, D. (2017c). What Can We Learn From Experiments?
Understanding the Threats to the Scalability of Experimental Results. American Economic
Review, 107(5):282–86.
Allcott, H. and Kessler, J. B. (2019). The Welfare Effect of Nudges: A Case Study of Energy
Use Social Comparisons. 11(1):236–276.
Allcott, H. and Rogers, T. (2014). The Short-Run and Long-Run Effects of Behavioral Interventions : Experimental Evidence from Energy Conservation. American Economic Review,
104(10):3003–3037.
Allcott, H. and Taubinsky, D. (2015). Evaluating behaviorally-motivated policy: Experimental
evidence from the lightbulb market. American Economic Review, 105(8):2501–2538.
Alpizar, F., Carlsson, F., and Johansson-Stenman, O. (2008). Does context matter more for
hypothetical than for actual contributions? Evidence from a natural field experiment. Experimental Economics, 11(3):299–314.
Amrhein, V. and Greenland, S. (2018). Remove, rather than redefine, statistical significance.
Nature Human Behaviour, 2(1):4–4.
Anderson, L. M., Quinn, T. A., Glanz, K., Ramirez, G., Kahwati, L. C., Johnson, D. B.,
Buchanan, L. R., Archer, W. R., Chattopadhyay, S., Kalra, G. P., Katz, D. L., and Task
Force on Community Preventive Services (2009). The effectiveness of worksite nutrition and
physical activity interventions for controlling employee overweight and obesity: a systematic
review. American Journal of Preventive Medicine, 37(4):340–357.
Anderson, M. L. (2008). Multiple Inference and Gender Differences in the Effects of Early Intervention: A Reevaluation of the Abecedarian, Perry Preschool, and Early Training Projects.
Journal of the American Statistical Association, 103(484):1481–1495.
59

Anderson, R. G. and Kichkha, A. (2017). Replication, meta-analysis, and research synthesis in
economics. American Economic Review, 107(5):56–59.
Andrews, I. and Kasy, M. (2017). Identification of and Correction for Publication Bias. NBER
Working Paper No. 23298.
Angrist, J., Imbens, G., and Rubin, D. (1996). Identification of causal effects using instrumental
variables. Journal of the American Statistical Association, 91(434):444–455.
Angrist, J. D. (1990). Lifetime Earnings and the Vietnam Era Draft Lottery: Evidence from
Social Security Administrative Records. The American Economic Review, 80(3):313–336.
Angrist, J. D. and Imbens, G. W. (1994). Identification and estimation of local average treatment
effects. Econometrica, 62(2):467–475.
Arellano, M. and Bonhomme, S. (2012). Identifying Distributional Characteristics in Random
Coefficients Panel Data Models. The Review of Economic Studies, 79(3):987–1020.
Arrow, K. (1973). The Theory of Discrimination. In Ashenfelter, O. and Rees, A., editors,
Discrimination in Labor Markets, pages 3–33. Princeton University Press.
Athey, S. and Imbens, G. (2017). The Econometrics of Randomized Experiments. In Banerjee,
A. and Duflo, E., editors, Handbook of Economic Field Experiments, volume 1, pages 73–140.
North-Holland.
Attanasio, O. and Meghir, C. (2012). Education choices in Mexico: using a structural model and
a randomized experiment to evaluate Progresa. The Review of Economic Studies, 79(1):37—
-66.
Baicker, K., Mullainathan, S., and Schwartzstein, J. (2015). Behavioral hazard in health insurance. Quarterly Journal of Economics, 130(4):1623–1667.
Bandiera, O., Barankay, I., and Rasul, I. (2005). Social preferences and the response to incentives: Evidence from personnel data. The Quarterly Journal of Economics, 120(3):917–962.
Banerjee, A. (2005). ‘New Development Economics’ and the Challenge to Theory. Economic
and Political Weekly, 40(40):4340–4344.
Banerjee, A., Banerji, R., Berry, J., Duflo, E., Kannan, H., Mukerji, S., Shotland, M., Walton,
M., Baneijee, A., Baneiji, R., Berry, J., Duflo, E., Kannan, H., Mukerji, S., Shotland, M., and
Walton, M. (2017a). From Proof of Concept to Scalable Policies : Challenges and Solutions,
with an Application. Journal of Economic Perspectives, 31(4):73–102.
Banerjee, A., Chassang, S., and Snowberg, E. (2017b). Decision Theoretic Approaches to
Experiment Design and External Validity. Handbook of Economic Field Experiments, 1:141–
174.
Bareinboim, E. and Pearl, J. (2013). A General Algorithm for Deciding Transportability of
Experimental Results. Journal of Causal Inference, 1(1):107–134.
Becker, G. S. (2010). The Economics of Discrimination. University of Chicago Press.
Bedoya Arguelles, G., Bittarello, L., Davis, J. M. V., and Mittag, N. K. (2018). Distributional
impact analysis: Toolkit and illustrations of impacts beyond the average treatment effect.
IZA Discussion Paper No. 11863.
60

Behaghel, L., Crépon, B., and Barbanchon, T. L. (2015). Unintended effects of anonymous
resumes. American Economic Journal: Applied Economics, 7(3):1–27.
Bellemare, C., Bissonnette, L., and Kröger, S. (2014). Statistical Power of Within and BetweenSubjects Designs in Economic Experiments. IZA Discussion Papers No. 8583.
Bellemare, C., Bissonnette, L., and Kröger, S. (2016). Simulating power of economic experiments: the powerBBK package. Journal of the Economic Science Association, 2(2):157–168.
Benjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J., Berk, R.,
Bollen, K. A., Brembs, B., Brown, L., Camerer, C., Cesarini, D., Chambers, C. D., Clyde,
M., Cook, T. D., De Boeck, P., Dienes, Z., Dreber, A., Easwaran, K., Efferson, C., Fehr, E.,
Fidler, F., Field, A. P., Forster, M., George, E. I., Gonzalez, R., Goodman, S., Green, E.,
Green, D. P., Greenwald, A. G., Hadfield, J. D., Hedges, L. V., Held, L., Hua Ho, T., Hoijtink,
H., Hruschka, D. J., Imai, K., Imbens, G., Ioannidis, J. P. A., Jeon, M., Jones, J. H., Kirchler,
M., Laibson, D., List, J., Little, R., Lupia, A., Machery, E., Maxwell, S. E., McCarthy, M.,
Moore, D. A., Morgan, S. L., Munafó, M., Nakagawa, S., Nyhan, B., Parker, T. H., Pericchi,
L., Perugini, M., Rouder, J., Rousseau, J., Savalei, V., Schönbrodt, F. D., Sellke, T., Sinclair,
B., Tingley, D., Van Zandt, T., Vazire, S., Watts, D. J., Winship, C., Wolpert, R. L., Xie, Y.,
Young, C., Zinman, J., and Johnson, V. E. (2017). Redefine statistical significance. Nature
Human Behaviour, 2(1):6–10.
Benjamini, Y. and Hochberg, Y. (1995). Controlling the False Discovery Rate: A Practical and
Powerful Approach to Multiple Testing. Journal of the Royal Statistical Society. Series B
(Methodological), 57(1).
Benz, M. and Meier, S. (2008). Do people behave in experiments as in the field? Evidence from
donations. Experimental Economics, 11(3):268–281.
Bernheim, B. D., Fradkin, A., and Popov, I. (2011). The welfare economics of default options:
A theoretical and empirical analysis of 401 (k) plans. NBER Working Papers No. 17587.
Berry, J., Coffman, L. C., Hanley, D., Gihleb, R., and Wilson, A. J. (2017). Assessing the Rate
of Replication in Economics. American Economic Review, 107(5):27–31.
Bertsimas, D., Johnson, M., and Kallus, N. (2015). The Power of Optimization Over Randomization in Designing Experiments Involving Small Samples. Operations Research, 63(4):868–876.
Bettis, R. A. (2012). The search for asterisks: Compromised statistical tests and flawed theories.
Strategic Management Journal, 33(1):108–113.
Bohnet, I., van Geen, A., and Bazerman, M. (2016). When Performance Trumps Gender Bias:
Joint vs. Separate Evaluation. Management Science, 62(5):1225–1234.
Bonferroni, C. (1935). Il calcolo delle assicurazioni su gruppi di teste. Tipografia del Senato.
Borghans, L., Heckman, J. J., Golsteyn, B. H. H., and Meijers, H. (2009). Gender differences
in risk aversion and ambiguity aversion. Journal of the European Economic Association,
7(2-3):649–658.
Brandon, A., Ferraro, P. J., List, J. A., Metcalfe, R. D., Price, M. K., and Rundhammer, F.
(2017). Do The Effects of Social Nudges Persist? Theory and Evidence from 38 Natural Field
Experiments. NBER Working Paper No. 23277.

61

Bren, L. (2001). Frances Oldham Kelsey: FDA medical reviewer leaves her mark on history.
FDA Consumer, 35(2):24–29.
Briesch, A. M., Swaminathan, H., Welsh, M., and Chafouleas, S. M. (2014). Generalizability
theory: A practical guide to study design, implementation, and interpretation. Journal of
School Psychology, 52(1):13–35.
Briggs, D. C. and Wilson, M. (2007). Generalizability in Item Response Modeling. Journal of
Educational Measurement Summer, 44(2):131–155.
Brodeur, A., Lé, M., Sangnier, M., and Zylberberg, Y. (2016). Star wars: The empirics strike
back. American Economic Journal: Applied Economics, 8(1):1–32.
Browning, M. and Chiappori, P.-A. (1998). Efficient intra-household allocations: A general
characterization and empirical tests. Econometrica, 66(6):1241–1278.
Bruhn, M. and McKenzie, D. (2009). In pursuit of balance: Randomization in practice in
development field experiments. American Economic Journal: Applied Economics, 1(4):200–
232.
Burke, L. E., Styn, M. A., Sereika, S. M., Conroy, M. B., Ye, L., Glanz, K., Sevick, M. A., and
Ewing, L. J. (2012). Using Health technology to enhance self-monitoring for weight loss: a
randomized trial. American Journal of Preventive Medicine, 43(1):20–26.
Burke, M., Bergquist, L. F., and Miguel, E. (2014). Selling low and buying high: An arbitrage
puzzle in Kenyan villages. Working Paper.
Burlig, F., Preonas, L., and Woerman, M. (2017). Panel Data and Experimental Design. Energy
Institute at Haas Working Paper No. 277.
Butera, L. and List, J. A. (2017). An Economic Approach to Alleviate the Crises of Confidence
in Science: With An Application to the Public Goods Game. NBER Working Paper No.
23335.
Button, K. S., Ioannidis, J. P. a., Mokrysz, C., Nosek, B. a., Flint, J., Robinson, E. S. J., and
Munafò, M. R. (2013). Power failure: why small sample size undermines the reliability of
neuroscience. Nature reviews. Neuroscience, 14(5):365–76.
Camerer, C. (2015). The promise and success of lab-field generalizability in experimental economics: A critical reply to Levitt and List. In Fréchette, G. and Schotter, A., editors,
Handbook of Experimental Economic Methodology, pages 249–295. Oxford University Press.
Camerer, C. F., Dreber, A., Forsell, E., Ho, T.-h., Huber, J., Kirchler, M., Almenberg, J.,
Altmejd, A., Chan, T., Holzmeister, F., Imai, T., Isaksson, S., Nave, G., Pfeiffer, T., Razen,
M., and Wu, H. (2016). Evaluating replicability of laboratory experiments in economics.
Science, 351(6280):1433–1436.
Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T. H., Huber, J., Johannesson, M., Kirchler,
M., Nave, G., Nosek, B. A., Pfeiffer, T., Altmejd, A., Buttrick, N., Chan, T., Chen, Y.,
Forsell, E., Gampa, A., Heikensten, E., Hummer, L., Imai, T., Isaksson, S., Manfredi, D.,
Rose, J., Wagenmakers, E. J., and Wu, H. (2018). Evaluating the replicability of social
science experiments in Nature and Science between 2010 and 2015. Nature Human Behaviour,
2(9):637–644.

62

Card, D., DellaVigna, S., and Malmendier, U. (2011). The Role of Theory in Field Experiments.
Journal of Economic Perspectives, 25(3):39–62.
Carroll, C. D., Overland, J., and Weil, D. N. (2000). Saving and growth with habit formation.
American Economic Review, 90(3):341–355.
Carter, E. C. and McCullough, M. E. (2014). Publication bias and the limited strength model of
self-control: has the evidence for ego depletion been overestimated? Frontiers in Psychology,
5:1–11.
Chandar, B. K., Hortacsu, A., List, J. A., Muir, I., and Wooldridge, J. M. (2018). Design and
Analysis of Cluster-Randomized Field Experiments in Panel Data Settings. Working Paper.
Chang, A. C. and Li, P. (2017). A Preanalysis Plan to Replicate Sixty Economics Research
Papers That Worked Half of the Time. American Economic Review, 107(5):60–64.
Charness, G. and Gneezy, U. (2009). Incentives to exercise. Econometrica, 77(3):909–931.
Charness, G., Gneezy, U., and Kuhn, M. A. (2012). Experimental methods: Between-subject
and within-subject design. Journal of Economic Behavior and Organization, 81(1):1–8.
Chassang, S., Miquel, P. I., Snowberg, E., and Others (2012). Selective trials: A principal-agent
approach to randomized controlled experiments. American Economic Review, 102(4):1279–
1309.
Christensen, G. S. and Miguel, E. (2016). Transparency, Reproducibility, and the Credibility
of Economics Research. NBER Working Papers 22989.
Cleave, B., Nikiforakis, N., and Slonim, R. (2013). Is there selection bias in laboratory experiments? The case of social and risk preferences. Experimental Economics, 16(3):372–382.
Clemens, M. A. (2015). the Meaning of Failed Replications: a Review and Proposal. Journal
of Economic Surveys, 31(1):326–342.
Coffman, L. C. and Niederle, M. (2015). Pre-Analysis Plans Have Limited Upside, Especially
Where Replications Are Feasible. Journal of Economic Perspectives, 29(3):81–98.
Coffman, L. C., Niederle, M., and Wilson, A. J. (2017). A Proposal to Organize and Promote
Replications. American Economic Review, 107(5):41–45.
Coker, B., Rudin, C., and King, G. (2018). A Theory of Statistical Inference for Ensuring the
Robustness of Scientific Results. Retrieved from http://arxiv.org/abs/1804.08646. Accessed
on 04-28-2018.
Cooper, B. R., Bumbarger, B. K., and Moore, J. E. (2015). Sustaining evidence-based prevention
programs: Correlates in a large-scale dissemination initiative. Prevention Science, 16(1):145–
157.
Coville, A. and Vivalt, E. (2017). How Often Should We Believe Positive Results. Working
Paper.
Cox, D. R. and Reid, N. (2000). The Theory of the Design of Experiments. Chapman &
Hall/CRC Press.

63

Crépon, B., Duflo, E., Gurgand, M., Rathelot, R., and Zamora, P. (2013). Do labor market
policies have displacement effects? Evidence from a clustered randomized experiment. The
Quarterly Journal of Economics, 128(2):531–580.
Croke, K., Hicks, J. H., Hsu, E., Kremer, M., and Miguel, E. (2016). Do Mass Deworming Affect
Child Nutrition? Meta-analysis, Cost Effectiveness and Statistical Power. NBER Working
Paper No. 22382.
Croson, R. and Gneezy, U. (2009). Gender Differences in Preferences. Journal of Economic
Literature, 47(2):448–474.
Cunha, J. M., Giorgi, D. G., and Jayachandran, S. (2017). The Price Effects of Cash Versus
In-Kind Transfers. Review of Economic Studies, 86(1):240–281.
Davis, J., Guryan, J., Hallberg, K., and Ludwig, J. (2017). The Economics of Scale-Up. NBER
Working Paper No. 23925.
De Long, J. B. and Lang, K. (1992). Are all Economic Hypotheses False? Journal of Political
Economy, 100(6):1257–1272.
de Quidt, J., Haushofer, J., and Roth, C. (2018). Measuring and Bounding Experimenter
Demand. American Economic Review, 108(11):3266–3302.
Deaton, A. and Cartwright, N. (2018). Understanding and misunderstanding randomized controlled trials. Social Science and Medicine, 210:2–21.
Deck, C. A., Fatas, E., and Rosenblat, T., editors (2015). Replication in Experimental Economics. Research in Experimental Economics. Emerald Group Publishing Limited.
DellaVigna, S., List, J. A., and Malmendier, U. (2012). Testing for Altruism and Social Pressure
in Charitable Giving. The Quarterly Journal of Economics, 127(1):1–56.
DellaVigna, S., List, J. A., Malmendier, U., and Rao, G. (2016). Estimating social preferences
and gift exchange at work. NBER Working Paper No. 22043.
DellaVigna, S., List, J. A., Malmendier, U., and Rao, G. (2017). Voting to tell others. The
Review of Economic Studies, 84(1):143–181.
DellaVigna, S. and Pope, D. (2018). Predicting Experimental Results: Who Knows What?
Journal of Political Economy, 126(6):2410–2456.
Dewald, W. G., Thursby, J. G., and Anderson, R. G. (1986). Replication in Empirical Economics: The Journal of Money, Credit and Banking Project. The American Economic Review,
76(4):587–603.
Doucouliagos, C. and Stanley, T. D. (2013). Are all economic facts greatly exaggerated? Theory
competition and selectivity. Journal of Economic Surveys, 27(2):316–339.
Dreber, A., Pfeiffer, T., Almenberg, J., Isaksson, S., Wilson, B., Chen, Y., Nosek, B. A., and
Johannesson, M. (2015). Using prediction markets to estimate the reproducibility of scientific
research. Proceedings of the National Academy of Sciences, 112(50):15343–7.
Duflo, E. (2004). Scaling up and evaluation. In Annual World Bank Conference on Development
Economics 2004, pages 341–369.

64

Duflo, E., Glennerster, R., and Kremer, M. (2007). Using randomization in development economics research: A toolkit. In Schultz, T. and Strauss, J., editors, Handbook of Development
Economics, volume 4, pages 3895–3962.
Dunn, O. J. (1961). Multiple Comparisons Among Means. Journal of the American Statistical
Association, 56(293):52–64.
Duvendack, M., Palmer-Jones, R., and Reed, W. R. (2017). What Is Meant by “Replication” and
Why Does It Encounter Resistance in Economics? American Economic Review, 107(5):46–51.
Falk, A. and Heckman, J. J. (2009). Lab Experiments Are a Major Source of Knowledge in the
Social Sciences. Science, 326(5952):535 –538.
Feiveson, A. H. (2002). Power by simulation. The Stata Journal, 2(2):107–124.
Ferraro, P. J., Miranda, J. J., and Price, M. K. (2011). The persistence of treatment effects
with norm-based policy instruments: evidence from a randomized environmental policy experiment. The American Economic Review, 101(3):318–322.
Ferraro, P. J. and Price, M. K. (2013). Using Nonpecuniary Strategies to Influence Behavior:
Evidence from a Large-Scale Field Experiment. Review of Economics and Statistics, 95(1):64–
73.
Fiedler, K., Kutzner, F., and Krueger, J. I. (2012). The Long Way From α-Error Control
to Validity Proper: Problems With a Short-Sighted False-Positive Debate. Perspectives on
Psychological Science, 7(6):661–669.
Finkelstein, A. (2007). The Aggregate Effects of Health Insurance: Evidence from the Introduction of Medicare. The Quarterly Journal of Economics, pages 1–37.
Finkelstein, A. and Notowidigdo, M. (2018). Take-up and Targeting: Experimental Evidence
from SNAP. NBER Working Paper 24652.
Fisher, R. A. (1925). Statistical Methods for Research Workers. Genesis Publishing Pvt Ltd.
Fisher, R. A. (1935). The Design of Experiments. Oliver and Boyd, London, Edinburgh.
Flory, J. A., Gneezy, U., Leonnard, K. L., and List, J. A. (2018). Gender, Age, and Competition:
a Disappearing Gap? Journal of Economic Behavior and Organization, 150(June):256–276.
Food and Drug Administration (1997). Guidance for Industry. Technical report.
Food and Drug Administration (2017). Women’s Health Research. Regulations, Guidance and
Reports related to Women’s Health.
Freedman, D. (2006). Statistical models for causation: What inferential leverage do they provide? Evaluation Review, 30(6):691–713.
Friedman, M. (1953). The methodology of positive economics. In Essays in Positive Economics.
University of Chicago Press, Chicago.
Frison, L. and Pocock, S. J. (1992). Repeated measures in clinical trials: Analysis using mean
summary statistics and its implications for design. Statistics in Medicine, 11(13):1685–1704.
Gabaix, X. and Farhi, E. (2017). Optimal Taxation with Behavioral Agents. Society for Economic Dynamics Working Paper No. 1634.
65

Gächter, S. (2010). (Dis)advantages of student subjects: What is your research question?
Behavioral and Brain Sciences, 33(2-3):92–93.
Gelman, A. and Carlin, J. (2014). Beyond Power Calculations: Assessing Type S (Sign) and
Type M (Magnitude) Errors. Perspectives on Psychological Science, 9(6):641–651.
Gelman, A. and Hill, J. (2007). Data Analysis Using Regression and Multilevel. Cambridge
University Press, New York, NY.
Gerber, A. S., Green, D. P., and Shachar, R. (2003). Voting may be habit-forming: evidence
from a randomized field experiment. American Journal of Political Science, 47(3):540–550.
Gibson, R., Tanner, C., and Wagner, A. F. (2013). Preferences for Truthfulness: Heterogeneity
among and within Individuals. American Economic Review, 103(1):532–548.
Giné, X., Karlan, D., and Zinman, J. (2010). Put your money where your butt is: a commitment
contract for smoking cessation. American Economic Journal: Applied Economics, 2(4):213–
235.
Gneezy, U. and List, J. A. (2006). Putting behavioral economics to work: Testing for gift
exchange in labor markets using field experiments. Econometrica, 74(5):1365–1384.
Goeree, J. K. and Holt, C. A. (2001). Ten Little Treasures of Game Theory and Ten Intuitive
Contradictions. The American Economic Review, 91(5):1402–1422.
Gosnell, G. K., List, J. A., and Metcalfe, R. (2017). A new approach to an age-old problem: Solving externalities by incenting workers directly. Journal of Public Economics, 148(April):14–31.
Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., and
Altman, D. G. (2016). Statistical tests, P values, confidence intervals, and power: a guide to
misinterpretations. European Journal of Epidemiology, 31(4):337–350.
Greenstone, M. and Gayer, T. (2009). Quasi-experimental and experimental approaches to
environmental economics. Journal of Environmental Economics and Management, 57(1):21–
44.
Groh, M., Krishnan, N., McKenzie, D., and Vishwanath, T. (2016). The impact of soft skills
training on female youth employment: evidence from a randomized experiment in Jordan.
IZA Journal of Labor and Development, 5(1):9.
Hagger, M. S. and Chatzisarantis, N. L. D. (2016). A Multilab Preregistered Replication of the
Ego-Depletion Effect. Perspectives on Psychological Science, 11(4):546–573.
Hagger, M. S., Wood, C., Stiff, C., and Chatzisarantis, N. L. D. (2010). Ego depletion and the
strength model of self-control: a meta-analysis. Psychological bulletin, 136(4):495–525.
Hallsworth, M., List, J. A., Metcalfe, R., and Vlaev, I. (2017). The behavioralist as tax collector:
using natural field experiments to enhance tax compliance. Journal of Public Economics,
148:14–31.
Hallsworth, M., List, J. A., Metcalfe, R. D., and Vlaev, I. (2015). The Making of Homo
Honoratus: From Omission To Commission. NBER Working Paper No. 21210.
Hamermesh, D. S. (2007). Viewpoint: Replication in economics. Canadian Journal of Economics, 40(3):715–733.
66

Hamermesh, D. S. (2017). Replication in labor economics: Evidence from data and what it
suggests. American Economic Review, 107(5):37–40.
Handel, B. R. (2013). Adverse Selection and Inertia in Health Insurance Markets: When
Nudging Hurts. American Economic Review, 103(7):2643–2682.
Harrison, G. W. and List, J. A. (2004). Field Experiments. Journal of Economic Literature,
42(4):1009–1055.
Heckman, J. (2000). Causal parameters and policy analysis in economics: A twentieth century
retrospective. The Quarterly Journal of Economics, 115(1):45–97.
Heckman, J., Moon, S. H., Pinto, R., Savelyev, P., and Yavitz, A. (2010). Analyzing social
experiments as implemented: A reexamination of the evidence from the HighScope Perry
Preschool Program. Quantitative Economics, 1(1):1–46.
Heckman, J. J. (2010). Building Bridges Between Structural and Program Evaluation Approaches to Evaluating Policy. Journal of Economic Literature, 48:356–398.
Heinrich, J. (2001). General Accounting Office Report: GAO-01-286R Drugs Withdrawn From
Market. Technical report.
Henrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H., and McElreath, R. (2001). In
Search of Homo Economicus: Behavioral Experiments in 15 Small-Scale Societies. American
Economic Review, Papers & Proceedings, 91(2):73–78.
Henrich, J. and Heine, S. (2010). Beyond WEIRD: Towards a broad-based behavioral science.
Behavioral and Brain, 33(2-3):111–135.
Henrich, J., Heine, S., and Norenzayan, A. (2010a). Most people are not WEIRD. Nature,
466:29–29.
Henrich, J., Heine, S. J., and Norenzayan, A. (2010b). The weirdest people in the world?
Behavioral and Brain Sciences, 33(2010):61–135.
Higgins, J. and Thompson, S. (2002). Quantifying heterogeneity in a meta-analysis. Statistics
in medicine, 21(11):1539–1558.
Hoel, J. B. (2015). Heterogeneous households: A within-subject test of asymmetric information
between spouses in Kenya. Journal of Economic Behavior & Organization, 118:123–135.
Höffler, J. H. (2017). Replication and Economics Journal Policies. American Economic Review,
107(5):52–55.
Holm, S. (1979). A Simple Sequentially Rejective Multiple Test Procedure. Scandinavian
Journal of Statistics, 6(2):65–70.
Horton, J., Rand, D., and Zeckhauser, R. (2011). The online laboratory: conducting experiments in a real labor market. Experimental Economics, 14(3):399–425.
Hsee, C. K. (1996). The evaluability hypothesis: An explanation for preference reversals between
joint and separate evaluations of alternatives. Organizational Behavior and Human Decision
Processes, 67(3):247–257.
Huang, J., Reiley, D. H., and Riabov, N. M. (2017). Measuring Consumer Sensitivity to Audio
Advertising: A Field Experiment on Pandora Internet Radio. Working Paper.
67

Ioannidis, J. P., Hozo, I., and Djulbegovic, B. (2013). Optimal type I and type II error pairs
when the available sample size is fixed. Journal of Clinical Epidemiology, 66(8):903–910.e2.
Ioannidis, J. P., Stanley, T. D., and Doucouliagos, H. (2017). The Power of Bias in Economics
Research. Economic Journal, 127(605):F236–F265.
Ioannidis, J. P. A. (2005). Why most published research findings are false. PLoS Medicine,
2(8):0696–0701.
Jackson, C. K. (2010). A little now for a lot later a look at a texas advanced placement incentive
program. Journal of Human Resources, 45(3):591–639.
Jena, A., Mulligan, C., Philipson, T. J., and Sun, E. (2008). The Value of Life in General
Equilibrium. NBER Working Paper No. 14157.
Jennions, M. D. and Møller, A. P. (2003). A survey of the statistical power of research in
behavioral ecology and animal behavior. Behavioral Ecology, 14(3):438–445.
Jensen, R. (2010). The (perceived) returns to education and the demand for schooling. The
Quarterly Journal of Economics, 125(2):515–548.
Jimenez-Gomez, D. (2017). Nudging and Phishing: A Theory of Behavioral Welfare Economics.
Working Paper.
Jimenez-Gomez, D. (2018). Hyperbolic Discounting Is Not Lack of Self-Control. Working Paper.
John, L. K., Loewenstein, G., Troxel, A. B., Norton, L., Fassbender, J. E., and Volpp, K. G.
(2011). Financial incentives for extended weight loss: a randomized, controlled trial. Journal
of General Internal Medicine, 26(6):621–626.
Karlan, D. and List, J. A. (2007). Does price matter in charitable giving? Evidence from a
large-scale natural field experiment. American Economic Review, 97(5):1774–1793.
Kasy, M. (2016). Why experimenters might not always want to randomize, and what they could
do instead. Political Analysis, 24(3):324–338.
Keren, G. and Lewis, C. e. (1993). A Handbook for Data Analysis in the Behavioral Sciences:
Volume 1: Methodological Issues. Lawrence Erlbaum Associates, Inc.
Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Reginald B. Adams, J., Alper, S.,
Aveyard, M., Axt, J., Babaloia, M., Bahnı́k, Š., Berkics, M., Bernstein, M. J., Berry, D. R.,
Bialobrzeska, O., Bocian, K., Brandt, M., Busching, R., Cai, H., Cambier, F., Cantarero,
K., Carmichael, C. L., Cemalcilar, Z., Chandler, J. J., Chang, J.-H., Chatard, A., CHEN,
E., Cheong, W., Cicero, D. C., Coen, S., Coleman, J. A., Collisson, B., Conway, M., Corker,
K. S., Curran, P. G., Cushman, F., Dalgar, I., Davis, W. E., de Bruijn, M., de Vries, M.,
Devos, T., Doğulu, C., Dozo, N., Dukes, K., Dunham, Y., Durrheim, K., Easterbrook, M.,
Ebersole, C. R., Edlund, J., English, A. S., Eller, A., Finck, C., Freyre, M.-Á., Friedman,
M., Frankowska, N., Galliani, E. M., Ghoshal, T., Giessner, S. R., Gill, T., Gnambs, T.,
Gomez, A., Gonzalez, R., Graham, J., Grahe, J., Grahek, I., Green, E., Hai, K., Haigh, M.,
Haines, E. L., Hall, M. P., Heffernan, M. E., Hicks, J. A., Houdek, P., van der Hulst, M.,
Huntsinger, J. R., Huynh, H. P., IJzerman, H., Inbar, Y., Innes-Ker, Å., Jimenez-Leal, W.,
John, M.-S., Joy-Gaba, J., Kamiloglu, R., Kappes, A., Kappes, H., Karabati, S., Karick, H.,
Keller, V. N., Kende, A., Kervyn, N., Knezevic, G., Kovacs, C., Krueger, L. E., Kurapov,
G., Kurtz, J., Lakens, D., Lazarevic, L., Levitan, C., Neil Lewis, J., Lins, S., Maassen, E.,
68

Maitner, A., Malingumu, W., Mallett, R., Marotta, S., McIntyre, J., Medjedovic, J., Milfont,
T. L., Morris, W., Myachykov, A., Murphy, S., Neijenhuijs, K. I., Nelson, A. J., Neto, F.,
Nichols, A. L., O’Donnell, S. L., Oikawa, M., Orosz, G., Osowiecka, M., Packard, G., Pérez,
R., Petrovic, B., Pilati, R., Pinter, B., Podesta, L., Pollmann, M., Rosa, A. D., Rutchick,
A. M., M., P. S., Sacco, A., Saeri, A. K., Salomon, E., Schmidt, K., Schönbrodt, F., Sekerdej,
M., Sirlopu, D. R., Skorinko, J., Smith, M. A., Smith-Castro, V., Sobkow, A., Sowden, W. J.,
Spachtholz, P., Steiner, T. G., Stouten, J., Street, C. N., Sundfelt, O., Szumowska, E., Tang,
A., Tanzer, N. K., Tear, M., Theriault, J., Thomae, M., Torres, D., Traczyk, J., Tybur, J. M.,
Ujhelyi, A., van Assen, M. A., Veer, A. v. t., Echeverrı́a, A. V., Vaughn, L. A., Vázquez,
A., Vega, D., Verniers, C., Verschoor, M., Voermans, I., Vranka, M., Welch, C., Wichman,
A., Williams, L. A., Woodzicka, J. A., Wronska, M. K., Young, L., Zelenski, J. M., and
Nosek, B. A. (2018). Many Labs 2: Investigating Variation in Replicability Across Sample
and Setting. Retrieved from https://psyarxiv.com/9654g/. Accessed on 12-20-2018.
Klemperer, P. (1987). Markets with consumer switching costs. The Quarterly Journal of
Economics, 102(2):375–394.
Koudstaal, M., Sloof, R., and Van Praag, M. (2015). Risk, uncertainty, and entrepreneurship:
Evidence from a lab-in-the-field experiment. Management Science, 62(10):2897–2915.
Kowalski, A. (2018). How to Examine External Validity Within an Experiment. NBER Working
Paper No. 24834.
Lakens, D., Adolfi, F. G., Albers, C., Anvari, F., Apps, M. A., Argamon, S. E., van Assen,
M. A., Baguley, T., Becker, R., Benning, S. D., Bradford, D. E., Buchanan, E. M., Caldwell,
A., van Calster, B., Carlsson, R., Chen, S.-C., Chung, B., Colling, L., Collins, G., Crook, Z.,
Cross, E. S., Daniels, S., Danielsson, H., DeBruine, L., Dunleavy, D., Earp, B. D., Ferrell,
J. D., Field, J. G., Fox, N., Friesen, A., Gomes, C., Grange, J. A., Grieve, A., Guggenberger,
R., Harmelen, A.-L. V., Hasselman, F., Hochard, K. D., Hoffarth, M. R., Holmes, N. P.,
Ingre, M., Isager, P., Isotalus, H., Johansson, C., Juszczyk, K., Kenny, D., Khalil, A. A.,
Konat, B., Lao, J., Larsen, E. G., Lodder, G. M., Lukavsky, J., Madan, C., Manheim, D.,
Gonzalez-Marquez, M., Martin, S. R., Martin, A. E., Mayo, D., McCarthy, R. J., McConway,
K., McFarland, C., Nilsonne, G., Nio, A. Q., de Oliveira, C. L., Parsons, S., Pfuhl, G.,
Quinn, K., Sakon, J., Saribay, S. A., Schneider, I., Selvaraju, M., Sjoerds, Z., Smith, S.,
Smits, T., Spies, J. R., Sreekumar, V., Steltenpohl, C., Stenhouse, N., Świa̧tkowski, W.,
Vadillo, M. A., Williams, M., Williams, D., de Xivry, J.-J. O., Yarkoni, T., Ziano, I., and
Zwaan, R. (2018). Justify Your Alpha: A Response to ”Redefine Statistical Significance”.
Nature Human Behaviour, 2(3):168–171.
Lambdin, C. and Shaffer, V. A. (2009). Are within-subjects designs transparent? Judgment
and Decision Making, 4(7):554–566.
Landry, C. E., Lange, A., List, J. A., Price, M. K., and Rupp, N. G. (2010). Is a donor in
hand better than two in the bush? Evidence from a natural field experiment. The American
Economic Review, 100(3):958–983.
Lee, S. and Shaikh, A. M. (2014). Multiple Testing and Heterogeneous Treatment Effects: ReEvaluating the Effect of PROGRESA on School Enrollment. Journal of Applied Econometrics,
29(4):612–626.
Lehmann, E. L. (1993). The Fisher, Neyman-Pearson Theories of Testing Hypotheses: One
Theory or Two? Journal of the American Statistical Association, 88(424):1242–1249.
69

Lenz, W. (1988). A short history of thalidomide embryopathy. Teratology, 38(3):203–215.
Levitt, S. D. and List, J. A. (2007). What Do Laboratory Experiments Measuring Social
Preferences Reveal about the Real World? The Journal of Economic Perspectives, 21(2):153–
174.
Levitt, S. D. and List, J. A. (2009). Field Experiments in Economics: the Past, the Present,
and the Future. European Economic Review, 53(1):1–18.
Levitt, S. D. and List, J. A. (2011). Was there really a Hawthorne effect at the Hawthorne
plant? An analysis of the original illumination experiments. American Economic Journal:
Applied Economics, 3(1):224–238.
Levitt, S. D., List, J. A., and Sadoff, S. (2016). The effect of performance-based incentives on
educational achievement: Evidence from a randomized experiment. NBER Working Paper
No. 22107.
Levitt, S. D., List, J. A., and Sadoff, S. E. (2009). Checkmate: exploring backward induction
among chess players. American Economic Review.
List, J. A. (2004). The nature and extent of discrimination in the marketplace: Evidence from
the field. The Quarterly Journal of Economics, 119(1):49–89.
List, J. A. (2006a). Field Experiments: A Bridge between Lab and Naturally Occurring Data.
The BE Journal of Economic Analysis & Policy, 5(2).
List, J. A. (2006b). The behavioralist meets the market: Measuring social preferences and
reputation effects in actual transactions. Journal of Political Economy, 114(1):1–37.
List, J. A. (2008). Informed consent in social science. Science, 322(5902):672.
List, J. A. (2011). Why Economists Should Conduct Field Experiments and 14 Tips for Pulling
One Off. Journal of Economic Perspectives, 25(3):3–16.
List, J. A., Sadoff, S., and Wagner, M. (2011). So you want to run an experiment, now
what? Some simple rules of thumb for optimal experimental design. Experimental Economics,
14(4):439–457.
List, J. A., Shaikh, A. M., and Xu, Y. (2016). Multiple Hypothesis Testing in Experimental
Economics. NBER Working Paper No. 21875.
Ljungqvist, L. (2008). Lucas critique. In Durlauf, S. N. and Blume, L. E., editors, The New
Palgrave Dictionary of Economics. Palgrave Macmillan, Basingstoke.
Loken, E. and Gelman, A. (2017). Measurement error and the replication crisis. Science,
355(6325):584–585.
Lucas, R. E. (1976). Econometric Policy Evaluations: A Critique. In Carnegie-Rochester
Conference Series on Public Policy, volume 1, pages 19–46.
Luedicke, J. (2013). POWERSIM: Stata module for simulation-based power analysis for linear
and generalized linear models. Statistical Software Components.
Maniadis, Z. and Tufano, F. (2017). The Research Reproducibility Crisis and Economics of
Science. The Economic Journal, 127(605):F200–F208.
70

Maniadis, Z., Tufano, F., and List, J. A. (2014). One swallow doesn’t make a summer: New
evidence on anchoring effects. The American Economic Review, 104(1):277–290.
Maniadis, Z., Tufano, F., and List, J. A. (2015). How to make experimental economics research
more reproducible: Lessons from other disciplines and a new proposal. In Replication in
experimental economics, pages 215–230. Emerald Group Publishing Limited.
Maniadis, Z., Tufano, F., and List, J. A. (2017). To Replicate or Not To Replicate? Exploring
Reproducibility in Economics through the Lens of a Model and a Pilot Study. Economic
Journal, 127(605):F209–F235.
McCloskey, D. N. (1985). The loss function has been mislaid: The rhetoric of significance tests.
The American Economic Review, 75(2):201–205.
McCullough, B. D. and Vinod, H. D. (2003). Verifying the Solution from a Nonlinear Solver:
A Case Study. American Economic Review, 93(3):873–892.
McKenzie, D. (2011). Power Calculations 101: Dealing with Incomplete Take-up. The World
Bank. Development Impact Blog. Accessed on 01-02-2019.
McKenzie, D. (2012). Beyond baseline and follow-up: The case for more T in experiments.
Journal of Development Economics, 99(2):210–221.
McShane, B. B., Gal, D., Gelman, A., Robert, C., and Tackett, J. L. (2017). Abandon Statistical
Significance. Retreieved from http://arxiv.org/abs/1709.07588. Accessed on 02-20-2018.
Meier, S. (2007). Do subsidies increase charitable giving in the long run? Matching donations
in a field experiment. Journal of the European Economic Association, 5(6):1203–1222.
Miguel, E. and Kremer, M. (2004). Worms: identifying impacts on education and health in the
presence of treatment externalities. Econometrica, 72(1):159–217.
Milkman, K. L., Minson, J. A., and Volpp, K. G. M. (2013). Holding the Hunger Games hostage
at the gym: An evaluation of temptation bundling. Management Science, 60(2):283–299.
Mill, J. S. (1836). On the definition of political economy and the method of investigation proper
to it. In Collected Works of John Stuart Mill, pages 120–164. University of Toronto Press,
Toronto.
Miller, R. G. (1981). Simultaneous Statistical Inference. Springer Series in Statistics. Springer
New York.
Moonesinghe, R., Khoury, M. J., and Janssens, A. C. J. W. (2007). Most published research
findings are false - But a little replication goes a long way. PLoS Medicine, 4(2):0218–0221.
Moore, R. T. (2012). Multivariate continuous blocking to improve political science experiments.
Political Analysis, 20(4):460–479.
Morgan, K. L. and Rubin, D. B. (2012). Rerandomization to Improve Covariate Balance in
Experiments. The Annals of Statistics, 40(2):1263–1282.
Mueller-Langer, F., Fecher, B., Harhoff, D., and Wagner, G. G. (2017). The Economics of
Replication. IZA Discussion Papers No. 10533.

71

Munafò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. S., Chambers, C. D., Percie, N.,
Simonsohn, U., and Wagenmakers, E.-J. (2017). A manifesto for reproducible science. Nature
Publishing Group, 1:1–9.
Muralidharan, K. and Niehaus, P. (2017). Experimentation at Scale. Journal of Economic
Perspectives, 31(4):103–124.
Muralidharan, K. and Sundararaman, V. (2015). The aggregate effect of school choice: Evidence
from a two-stage experiment in India. The Quarterly Journal of Economics, 130(3):1011–1066.
Narita, Y. (2018). Toward an Ethical Experiment.
https://ssrn.com/abstract=3094905.

Working paper. Available at SSRN:

Nevo, A. and Whinston, M. D. (2010). Taking the Dogma out of Econometrics: Structural
Modeling and Credible Inference. Journal of Economic Perspectives, 24(2):69–82.
Neyman, J. and Pearson, E. S. (1933). On the Problem of the Most Efficient Tests of Statistical
Hypotheses. Philosophical Transactions of the Royal Society A: Mathematical, Physical and
Engineering Sciences, 231(694-706):289–337.
Niederle, M. and Vesterlund, L. (2007). Do women shy away from competition? Do men
compete too much? The Quarterly Journal of Economics, 122(3):1067–1101.
Nosek, B. A., Ebersole, C. R., DeHaven, A. C., and Mellor, D. T. (2018). The preregistration
revolution. Proceedings of the National Academy of Sciences, 115(11):201708274.
Nosek, B. A., Spies, J. R., and Motyl, M. (2012). Scientific Utopia: II. Restructuring Incentives
and Practices to Promote Truth Over Publishability. Perspectives on Psychological Science.
Open Science Collaboration (2015). Estimating the reproducibility of psychological science.
Science, 349(6251):aac4716–aac4716.
Paluck, E. and Shafir, E. (2017). The Psychology of Construal in the Design of Field Experiments. In Handbook of Economic Field Experiments, volume 1, pages 245–268.
Phelps, E. S. (1972). The statistical theory of racism and sexism. The American Economic
Review, 62(4):659–661.
Robinson, J. (1977). What are the questions? Journal of Economic Literature, 15:1318–1339.
Rodriguez-Planas, N. (2012). Longer-term impacts of mentoring, educational services, and
learning incentives: Evidence from a randomized trial in the United States. American Economic Journal: Applied Economics, 4(4):121–139.
Rogers, T. and Frey, E. L. (2016). Changing behavior beyond the here and now. In Blackwell
Handbook of Judgment and Decision Making, pages 726–748. 1 edition.
Romano, J. and Wolf, M. (2010). Balanced control of generalized error rates. Annals of Statistics, 38(1):598–633.
Romano, J. P. and Wolf, M. (2005). Stepwise multiple testing as formalized data snooping.
Econometrica, 73(4):1237–1282.
Rosenbaum, P. R. and Rubin, D. B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):41–55.
72

Royer, H., Stehr, M., and Sydnor, J. (2015). Incentives, Commitments and Habit Formation
in Exercise: Evidence from a Field Experiment with Workers at a Fortune-500 Company.
American Economic Journal: Applied Economics, 7(3):51–84.
Rubin, D. (1974). Estimating causal effects of treatments in randomized and nonrandomized
studies. Journal of Educational Psychology, 66(5):688–701.
Samii, C. and Aronow, P. M. (2012). On equivalencies between design-based and regressionbased variance estimators for randomized experiments. Statistics & Probability Letters,
82(2):365–370.
Samuelson, P. A. and Nordhaus, W. D. (1985). Economics. McGraw Hill, New York, 12 edition.
Seidel, J. and Xu, Y. (2016). MHTEXP: Stata module to perform multiple hypothesis testing
correction procedure. Statistical Software Components.
Senn, S. (2013). Seven myths of randomisation in clinical trials. Statistics in medicine.
Shadish, W. R., Cook, T. D., and Campbell, D. T. (2002). Experimental and quasi-experimental
designs for generalized causal inference. Wadsworth Cengage learning.
Shah, A. K., Mullainathan, S., and Shafir, E. (2012). Some consequences of having too little.
Science, 338(6107):682–685.
Shang, J. and Croson, R. (2009). A field experiment in charitable contribution: The impact
of social information on the voluntary provision of public goods. The Economic Journal,
119(540):1422–1439.
Simmons, J. P., Nelson, L. D., and Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.
Psychological Science, 22(11):1359–1366.
Simon, H. A. (1972). Theories of bounded rationality. Decision and organization, 1(1):161–176.
Slonim, R., Wang, C., Garbarino, E., and Merrett, D. (2013). Opting-in: Participation bias in
economic experiments. Journal of Economic Behavior and Organization, 90(June):43–70.
Smaldino, P. E. and McElreath, R. (2016). The natural selection of bad science. Royal Society
Open Science, 3(9):160384.
Smith, V. L., Kassin, S. M., and Ellsworth, P. C. (1989). Eyewitness accuracy and confidence:
Within- versus between-subjects correlations. Journal of Applied Psychology, 74(2):356–359.
Spiegler, R. (2014). On the Equilibrium Effects of Nudging. Journal of Legal Studies, 44(2):389–
416.
Sterne, J. A. and Smith, G. D. (2001). Sifting the evidence—what’s wrong with significance
tests? BMJ, 322(7280):226.
Stoop, J., Noussair, C. N., and Van Soest, D. (2012). From the lab to the field: Cooperation
among fishermen. Journal of Political Economy, 120(6):1027–1056.
Sukhtankar, S. (2017). Replications in Development Economics. American Economic Review,
107(5):32–36.

73

Travers, J., Marsh, S., Williams, M., Weatherall, M., Caldwell, B., Shirtcliffe, P., Aldington, S.,
and Beasley, R. (2007). External validity of randomised controlled trials in asthma: to whom
do the results of the trials apply? Thorax, 62(3):219–223.
Vivalt, E. (2017). How Much Can We Generalize from Impact Evaluations? Working Paper.
Volpp, K., Troxel, A. B., Pauly, M. V., Glick, H. A., Puig, A., Asch, D. A., Galvin, R., Zhu,
J., Wan, F., DeGuzman, J., Corbett, E., Weiner, J., and Audrain-McGovern, J. (2009).
A Randomized, Controlled Trial of Financial Incentives for Smoking Cessation. The New
England Journal of Medicine, 360(7):699–709.
Volpp, K. G., Gurmankin Levy, A., Asch, D. A., Berlin, J. a., Murphy, J. J., Gomez, A., Sox,
H., Zhu, J., and Lerman, C. (2006). A randomized controlled trial of financial incentives for
smoking cessation. Cancer Epidemiology, Biomarkers & Prevention, 15(1):12–18.
Volpp, K. G., John, L. K., Troxel, A. B., Norton, L., Fassbender, J., and Loewenstein, G. (2008).
Financial Incentive – Based Approaches for Weight Los: A Randomized Trial. Journal of the
American Medical Association, 300(22):2631–2637.
Wacholder, S., Chanock, S., El, L., and Rothman, N. (2004). Assessing the Probability That a
Positive Report is. Cancer Research, 96(6):434–442.
Walton, G. M. and Cohen, G. L. (2011). A brief social-belonging intervention improves academic
and health outcomes of minority students. Science, 331(6023):1447–1451.
Wasserstein, R. L. and Lazar, N. A. (2016). The ASA’s Statement on p -Values: Context,
Process, and Purpose. The American Statistician, 70(2):129–133.
Wilhelm, D., Lee, S., and Carneiro, P. (2017). Optimal data collection for randomized control
trials. Cemmap Working Paper No. CWP45/17.
Zhang, L. and Ortmann, A. (2013). Exploring the Meaning of Significance in Experimental
Economics. UNSW Australian School of Business Discussion Paper.
Zhe Jin, G., Kato, A., and List, J. A. (2010). That’s news to me! information revelation in
professional certification markets. Economic Inquiry, 48(1):104–122.
Ziliak, S. T. and McCloskey, D. N. (2004). Size matters: the standard error of regressions in
the American Economic Review. The Journal of Socio-Economics, 33(5):527–546.

74

