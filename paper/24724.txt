NBER WORKING PAPER SERIES

PUBLIC CONTRACTING FOR PRIVATE INNOVATION:
GOVERNMENT EXPERTISE, DECISION RIGHTS, AND PERFORMANCE OUTCOMES
Joshua R. Bruce
John M. de Figueiredo
Brian S. Silverman
Working Paper 24724
http://www.nber.org/papers/w24724

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
June 2018

We are grateful to Jean-Etienne de Bettignies, Robert Gibbons, Ricard Gil, Mitch Hoffman, Chris
McKenna, Arti Rai, Pablo Spiller, Jesper Sørensen, Giorgio Zanarone; seminar participants at
CUNEF, George Mason University, Queens University, Stanford University, and the University
of California, Berkeley; and attendees at the Society for Institutional and Organizational
Economics and Mack Institute/Wharton Technology & Innovation Conference, for comments on
previous drafts of this paper. This study is based upon work supported by the National Science
Foundation under Grants Numbers 1061600 and 1443014. The views expressed herein are those
of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2018 by Joshua R. Bruce, John M. de Figueiredo, and Brian S. Silverman. All rights reserved.
Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.

Public Contracting for Private Innovation: ¸˛Government Expertise, Decision Rights, and
Performance Outcomes
Joshua R. Bruce, John M. de Figueiredo, and Brian S. Silverman
NBER Working Paper No. 24724
June 2018
JEL No. H11,H57,L14,L24,L33,O32
ABSTRACT
We examine how the U.S. Federal Government governs R&D contracts with private-sector firms.
The government chooses between two contractual forms: grants and cooperative agreements. The
latter provides the government substantially greater discretion over, and monitoring of, project
progress. Using novel data on R&D contracts and on the geo-location and technical expertise of
each government scientist over a 12-year period, we test implications from the organizational
economics and contracting literatures. We find that cooperative agreements are more likely to be
used for early-stage projects and those for which local government scientific personnel have
relevant technical expertise; in turn, cooperative agreements yield greater innovative output as
measured by patents, controlling for endogeneity of contract form. The results are consistent with
multi-task agency and transaction-cost approaches that emphasize decision rights and monitoring.

Joshua R. Bruce
Duke University
Department of Sociology
417 Chapel Drive
Box 90088
Durham, NC 27708-0088
joshua.bruce@duke.edu
John M. de Figueiredo
The Law School and Fuqua School
Duke University
210 Science Drive, Box 90360
Durham, NC 27708
and NBER
jdefig@duke.edu

Brian S. Silverman
Rotman School of Management
University of Toronto
105 St. George Street
Toronto, Ontario M5S 3E6
CANADA
silverman@rotman.utoronto.ca

INTRODUCTION
There is a vast literature documenting the solutions to contractual opportunism in agreements
between private firms. In principle, public-private contracting should be amenable to many of the
same solutions. However, in practice, contracting that involves the public sector is characterized
by strong rigidity (Moszoro, Spiller, and Stolorz, 2016), due both to political pressure from third
parties (Spiller and Moszoro, 2014) and more general bureaucratic structures (Moe, 1990).
Consequently, although many aspects of contract theory can inform public-private contracting,
the distinct characteristics of the public sector – which often preclude the use of several creative
contracting mechanisms implicated in such theories – suggest a more directed theoretical and
empirical approach is warranted to address the peculiarities of contracting in the governmental
setting.
This paper examines public-private contracting in the specific setting of public
contracting for private innovation. Government entities frequently encourage the development of
specific types of innovations that are deemed necessary to achieve public aims. For example, in
2016 the U.S. federal government funded $148 billion in R&D (Hourihan and Parkes, 2015), at
least $30 billion of which was devoted to contracts for private innovation regarding specific
goals such as preclinical trials for new pharmaceutical drugs, studies of chemical toxicity in
water, and the development of systems on the Mars rover missions.1
Contracting for innovation faces a central problem: when a client organization pays a
research firm to pursue a specific project, there is a risk that the research firm will divert the
payment to pursue its own interests. If the research project fails, it is difficult for the client
organization to distinguish between research-firm malfeasance and bad luck. This problem is

1

Similarly, in 2012 the United Kingdom allocated £1.6B (18%) of its £8.7B government R&D budget to private
firms (National Audit Office, 2013).

2

exacerbated as the uncertainty surrounding the project increases. Organizational economists offer
several prescriptions to overcome this problem. However, virtually all of the proposed solutions
depend on substantial flexibility in contract design: judicious allocation of property rights
(Aghion and Tirole, 1994), menus of fixed fees and royalty payments (Hegde, 2014), appropriate
investments in equity (Oxley, 1997), or sophisticated contractual provisions to deal with a range
of contingencies that might arise (Reuer and Ariño, 2007). These solutions, however, are often
precluded by the rigidities of public-sector contracting, such as the need for standardized
contracts, the desire to prevent officeholders from self-dealing, the requirement for procedural
adherence and transparency by agencies and civil servants, the presence of public sector unions,
and the attempt to insulate the bureaucracy from the political pressures of third parties (Moszoro
et al., 2016). Given these constraints, how can public-private contracts for innovation overcome
contractual hazards in order to create value?
We invoke and extend a solution that stems from recent analyses of privatization:
government retention of specific decision rights even while the private actor retains residual
control rights. Specifically, Hart et al.’s (1997) influential model of privatization predicts that
privatized services, for which the private provider owns residual control rights including decision
rights, will have lower costs but also lower quality than publicly-provided services. Yet
Williamson (1999) proposes that overlapping decision-making authority between the private
entity and public bureau may ameliorate quality degradation. Empirically, Cabral et al. (2010,
2013) find that privatized prisons in Brazil exhibit quality equivalent to that of publicly run
counterparts, attributing this to the presence at each private prison of a government supervisor, or
‘warden,’ whose job is to review and occasionally overrule decisions about those aspects of
operations that could adversely affect quality. This preserves quality as long as the government

3

warden is motivated to make good decisions.
We extend this logic to make three predictions about public contracting for private
innovation. As contracting hazards increase due to project uncertainty, the government agency is
more likely to include an information exchange and decision-rights mechanism in the contract.
However, this mechanism will only be implemented if the government agency has employees
with the requisite subject-matter expertise to make effective and appropriate project decisions.
Finally, conditional on project uncertainty and government expertise, cooperative agreements
will be more successful at generating patented innovations (which are the explicit goal of such
projects) because of more effective monitoring of private-sector R&D efforts by the government.
We test these predictions using a sample of more than 4,000 R&D contracts between U.S.
federal agencies and private firms. Similar to many countries, U.S. law generally restricts these
contracts to take one of two forms: a ‘grant,’ which affords the government no in-process
decision rights, and a ‘cooperative agreement,’ in which government employees have substantial
in-process decision rights. Using novel data on the technical expertise of government agency
personnel located in geographic proximity to the private firm’s R&D site, we find support for our
theoretical predictions. Notably, 1) earlier-stage projects (which are likely to entail greater
uncertainty) are more likely to be governed by cooperative agreements than by grants, 2)
agencies rely on cooperative agreements more readily when they have relevant technical
capabilities near the R&D site, 3) cooperative agreements perform better than grants in terms of
patents generated and the citations to these patents, and 4) although cooperative agreements
perform better than grants overall, those projects that were governed by grants would not have
been as productive as current cooperative agreements had they been organized as cooperative
agreements. We then consider a number of alternative theoretical explanations, econometric

4

specifications, and data measurements, and find the results are robust to these approaches.
This study makes three contributions to the theoretical literature. First, whereas most
analyses of privatized services have focused on the in-house vs. privatization decision, we extend
the logic to consider variations in privatized governance based on different characteristics of
projects. Second, whereas recent literature on hybrid governance highlights the condition that the
government monitor must be willing to monitor effectively, we propose that, for projects that
rely on highly idiosyncratic knowledge, the monitor must be both willing and able – i.e., must
have the requisite skills to monitor and make good decisions. Third, this focus on requisite skills
extends the literature on government capabilities: whereas prior research on value creation in
public-private collaboration has tended to emphasize a government entity’s capabilities in
contracting (e.g., Klein et al., 2013), we extend this to consider how a government entity’s
technological capabilities and expertise influence the organization of privatized services.2
Through these contributions, we further flesh out the implications of Moszoro et al.’s (2016)
insights about rigidity in public-private contracting.
The paper also makes an empirical contribution. We develop measures of government
innovative capabilities, based upon the specific technical expertise of government scientists,
engineers, doctors, and researchers as measured by 9.5 million person-year observations of
government personnel data. We then geo-locate those capabilities throughout the United States,
essentially creating a map of government capabilities along 59 expertise dimensions. To the best
of our knowledge, this is the first time that government capabilities have been measured in such
a microanalytic way.
The paper proceeds as follows. In section 2, we analyze the public-sector challenge of

2

Relatedly, Decarolis et al. (2018) examine public-private procurement contracting but focus on the competencies
of private firms rather than the public sector.

5

contracting effectively for R&D, ultimately generating predictions regarding the use of
government decision rights in contracts for innovation. Section 3 provides institutional detail on
the U.S. empirical setting. Section 4 introduces our data, model, and empirical strategy. Section 5
presents empirical results, and section 6 offers a brief discussion and conclusion.

CONTRACTING FOR RESEARCH – THE PRIVATE-PRIVATE VS. PUBLICPRIVATE CONTEXT
The private-private context
The market for technology suffers from several well-documented defects (Arora and
Gambardella, 2010). The R&D process is commonly characterized by several features that create
contractual hazards, including uncertainty, noncontractible effort, tacit knowledge, and
appropriability concerns. Organizational economics theories generally agree that contracting
difficulties rise monotonically with these characteristics. Given such difficulties, scholars have
devoted substantial attention to explicating contractual mechanisms that private entities can use
to efficiently govern R&D transactions.
Consider an example in which a pharmaceutical firm seeks to contract with a
biotechnology firm for R&D into a new drug. The client firm pays the research firm to conduct a
set of specified research tasks. But it is nearly impossible for the client firm to observe the effort
that the research firm’s employees actually devote to the tasks. R&D is an uncertain endeavor, so
if the research firm does not generate the desired innovation, it is difficult to tell whether this was
the result of insufficient effort or bad luck. Even when the innovation is developed, if transfer to
the client firm requires the provision of attendant tacit knowledge, then it is difficult to monitor
whether the researchers are making a good-faith effort to provide this knowledge (Hegde, 2014),

6

especially if either firm has latent concerns that proprietary knowledge outside the scope of the
contract will ‘leak’ to the other party during the course of the endeavor (Oxley and Wada, 2009).
Given these challenges, how might the firms successfully govern their exchange? One
prescription is to judiciously assign property rights so as to elicit noncontractible effort as
effectively as is feasible (Aghion and Tirole, 1994; Grossman and Hart, 1986). Lerner and
Merges (1998) test this empirically by exploring the pattern of property-rights assignment in
R&D contracts between pharmaceutical companies and biotechnology firms. They find modest
evidence that these contracts do indeed assign more property rights to the biotech firm when
projects are earlier-stage (and hence are more uncertain and require more non-contractible effort
from the biotech firm). Lerner and Malmendier (2010) consider termination options that
distribute rights to R&D results when projects characterized by unobservable effort also generate
observable milestones, finding that such termination options also appear more frequently in
contracts for earlier-stage projects than in contracts for later-stage projects.3
An alternative prescription is to implement a combination of fixed fees and royalty
payments to align the firms’ incentives. Since royalties depend on successful commercialization
of an innovation, they can provide a strong incentive to the research firm to both conduct the
requisite R&D and devote effort to transferring the results to the client firm (Xiao and Xu, 2012).
Although reliance on royalties shifts risk to the research firm, which in many models is more
risk-averse than the client firm, the benefits of incentive alignment outweigh the attendant costs
for sufficiently high levels of uncertainty and noncontractible effort. In a study of biomedical
invention, Hegde (2014) finds systematic patterns of complex royalty payments between

3

A unilateral termination option for the client firm encourages the research firm to devote appropriate effort to the
project, while a termination fee set at an appropriate level discourages the client firm from strategically terminating
the project. This option can align incentives between client firm and research firm.

7

commercializing firms and inventors that are consistent with theoretical predictions.
A third prescription is to judiciously use equity investments to align incentives, direct
effort, and protect knowledge (Pisano, 1990; Teece, 1986). While non-equity arrangements such
as licensing contracts will suffice for high-appropriability or low-tacit-knowledge research in the
presence of low uncertainty, equity joint ventures will be used to govern research agreements
with higher levels of contractual hazards (Oxley and Wada, 2009). Shared ownership of the
collaborative venture implies shared ownership of the attendant profits, thus aligning the firms’
incentives regarding success of the venture. Equity arrangements also provide formal monitoring
and, in particular, decision-making authority over the research effort (Reuer, Ariño, and
Mellewigt, 2006). These predictions have been borne out in specific industry settings (Sampson,
2004a) and multi-industry studies (Oxley, 1997). Going beyond the governance-choice decision,
Sampson (2004b) also finds that R&D alliances that are organized according to transaction-cost
precepts generate more patented innovations than those that are organized inappropriately.
Finally, complex contractual provisions may be employed to coordinate and control effort
in the face of uncertainty. Contractual features such as contingency payments can elicit effort
and align incentives in contractual relationships, while provisions that specify responses to
potential contingencies can restrict opportunistic behavior (Argyres, Bercovitz, and Mayer, 2007;
Reuer and Ariño, 2007). Contractual clauses that thus effectively address hazards can
dramatically increase contractual effectiveness (Anderson and Dekker, 2005) and facilitate
resolution of disagreements (Lumineau and Malhotra, 2011). Alternatively, judicious assignment
of decision rights and monitoring provisions can dramatically influence the effectiveness of
incentives and the performance of the project (Arruñada, Garicano, and Vázquez, 2001; Athey
and Roberts, 2001; Reuer and Devarakonda, 2016).

8

In general, then, contracting between private firms is frequently facilitated by a range of
governance mechanisms including judicious allocation of property rights, complex royalty
schemes, equity holdings, and/or sophisticated contingent contracts with attendant decision
rights. These mechanisms support a substantial market for technology both within and across
nations (Arora and Gambardella, 2010).

The public-private context
At first glance, one might expect that the above prescriptions are straightforwardly applicable to
public-private contracting. However, in many countries, strict rules and processes constrain the
form of public contracts for innovation. In the U.S., as in several other OECD countries,
government entities are prohibited from owning property rights in the resulting innovations,
paying royalties to the contracted firms, or taking equity in these firms. Strict contracting policies
also hinder attempts to craft project-specific contractual provisions.4 Thus, the most common
levers available to private-private contracts for research are unavailable in public-private
research contracts. These constraints reflect the stylized fact that public-sector contracts tend to
be far more rigid than their private-sector counterparts (Moszoro et al., 2016). This enduring
feature of public bureaucracy (Boyne, 2002) is often attributed to a desire to restrict public
agents’ ability to engage in self-dealing (Lan and Rainey, 1992); or, alternatively, to concerns
about political pressure from third parties (Spiller and Moszoro, 2014). This then leads to
processes in government which are procedurally onerous and substantively transparent, often
leading to inefficiency in the government by design (Moe, 1989). Because of these substantial
procedural requirements and limited resources for government contracting, the government tends

4

For example, similar to the U.S., Canada’s federal contracts for R&D take the form of either grants or
‘contributions,’ which are largely analogous to U.S. cooperative agreements.

9

to favor standardized, rather than customized, contracts for many purposes, limiting the ability of
the government to employ specialized terms (Miller, 1955).
This rigidity is manifest in Hart et al.’s (1997) incomplete-contract model of
privatization. In this model, a government actor chooses between delivering a service through inhouse provision or through a contract with a private provider. The service requires investment in
an asset and then operation using that asset. Of particular relevance, property rights over the
asset cannot be divided, but rest entirely with either the government or the firm, and payment to
the firm is limited to a fixed fee that can be renegotiated upward if the quality of the service is
increased. Given these blunt levers, the private firm has a strong incentive to lower the cost of
provision, even at the expense of quality, while an in-house provider has little incentive to
improve either cost or quality.5 Consequently, the authors predict that privatized services will
have lower costs but also lower quality than their publicly-provided counterparts. Levin and
Tadelis (2010) find that municipalities are less likely to outsource services for which quality is
important yet noncontractible, concluding that this is consistent with the Hart et al. (1997)
model. This provides a pessimistic assessment of the feasibility of public-private contracting for
innovation, given its reliance on noncontractible effort.
Yet Cabral et al. (2010, 2013) find that privatized prisons in two Brazilian states exhibit
quality that is equal to or better than that of their publicly run counterparts, even while enjoying
lower costs. They propose that the key quality-protection mechanism is the appointment to each
private prison of a government ‘warden’ whose job is to monitor prison operation and ensure that
it adheres to specified minimum quality standards. As long as the warden remains committed to

5

The private firm reaps the entire benefit from cost reduction, but only incurs a fraction of the benefit to quality
improvement because it must bargain with the government ex post for fee increases associated with improved
quality. The private firm thus ‘overinvests’ in cost reduction, yielding a socially suboptimal level of quality.

10

her task – i.e., she is not bribed by the private firm – then this ‘hybrid’ form of private operation
and public supervision appears to solve the problem of quality deterioration.6
To the extent that public contracting for innovation is characterized by the constraints
embodied in the Hart et al. (1997) model, perhaps the sole available lever is the prospect of
government supervision of the research project, analogous to the government prison warden. Yet
one difference stands out in the innovation setting. Cabral et al. (2010, 2013) implicitly assume
that the government warden understands the causal mechanisms linking cost-reduction and
quality shading – in essence, she knows which actions by the private agent are good and which
are bad. However, evidence indicates that an organization’s possession of relevant technological
capability helps it appraise the value of external research (Cassiman and Veugelers, 2006): ‘[t]he
ability to evaluate…outside knowledge is largely a function of the level of prior related
knowledge’ (Cohen and Levinthal, 1990: 128).
Indeed, precisely for this reason, an organization’s technological capability in a particular
sphere can influence its competence at contracting in that sphere. Mayer and Salomon (2006)
study an IT firm’s decisions to complete client projects with in-house or outsourced teams,
finding evidence that strong technological capability in, for example, mainframe technology
allows the firm to outsource on mainframe-related projects in the face of contractual hazards.
They conclude that the firm is better able to manage an external contract when it has
technological capabilities that enable it to anticipate problems and monitor outcomes. Building
on this idea, Argyres and Mayer (2007) propose that the technological (i.e., engineering)

6

Although the Brazilian prison setting only allows comparison of public to private-hybrid prisons, it should be the
case that a purely private prison would have lower costs than the private-hybrid prison. Some of this would be due to
quality-shading efforts that are socially destructive. But some should be due to lower effort to invest in costreduction by the private-hybrid, given that the warden may sometimes erroneously negate a valid cost-reduction
scheme.

11

expertise of a firm’s employees is particularly relevant to establishing effective interfirm
communication flows in research contracts. Thus, in the context of public contracting for
innovation, the government supervisor must have the requisite technological expertise to know
which actions are good and bad – in other words, if the supervisor is willing but not able to make
good decisions, then public supervision is a hindrance.
In sum, extending the predictions concerning decision rights and monitoring above, we
expect to find two patterns in contract choice: research projects that are more uncertain in
outcome are more likely to be governed by contracts that afford greater public supervision (i.e.,
cooperative agreements), as are research projects for which the available public personnel have
relevant expertise. We further expect research projects governed according to the above precepts
will outperform those that are not in terms of innovative output.

GOVERNMENT CONTRACTING FOR RESEARCH: INSTITUTIONAL DETAILS
The U.S. federal government is composed of 381 agencies, which in turn are composed of 874
bureaus. Although formally overseen by the Executive Branch of the government, agencies
pursue their own research and development agendas, each determined by a variety of different
considerations. To meet their required objectives, bureaus often determine that specific research
endeavors would require expertise beyond that available within the Federal government.7 In such
cases, the bureau contracts with outside entities for the requisite research effort.
The process begins when a bureau’s program office issues a call for research proposals,
or CFP (see ‘Appendix A: The Grant-Making Process’ for a detailed description of the CFP
process). The CFP outlines the motivation for the research project, the statutory authority for the

7

McKenna (2006: 103-105) describes the government’s strategic decision, at the beginning of the U.S. space
program, to rely on external expertise rather than try to employ all necessary experts within NASA.

12

agency to conduct the research, a list of requirements, milestones, expectations, and objectives of
the project, a list of eligibility requirements for the private contractor, and a description of how
the project will be managed. The CFP may specify that the research will be conducted through a
grant, a cooperative agreement, or either. As Appendix A describes, the CFP process is virtually
identical across the two types of contracts. In both cases, all property rights resulting from the
contracted research are owned by the contracted entity while the U.S. government receives a
royalty-free license. This precludes the judicious allocation of property rights and the use of
royalty schemes to elicit effort.8 However, for the purposes of this study, there are three key
differences between the governance forms.
The first difference is the degree of cooperative effort between the government agency
and private firm. As stipulated in the Federal Grant and Cooperative Agreement Act of 1977
(FGCAA) and the Code for Federal Regulation (CFR), grants do not provide for ‘substantial
involvement’ between government employees and the firm, whereas cooperative agreements do.9
This is reinforced by each agency’s own guidance documents. For example, Section 3 of the
NASA Grant and Cooperative Agreement Manual (2016: 3) notes that unlike a grant, a cooperative agreement should be used if ‘substantial involvement is expected between the executive
agency and the…other recipient when carrying out the activity contemplated in the agreement.’10
The second difference relates to the disparate pattern of decision rights assigned to

8

Some Federal procurement agreements also involve R&D effort by the private vendor. In these agreements, called
‘contracts,’ the Federal government funds the vendor’s R&D as ‘work for hire’ and receives ownership of any
resulting patents. We exclude these from this study for two reasons: they are not designed to support significant
R&D; and their different (although still rigid) allocation of property rights would conflate the incentives affecting
contract performance.
9
See ‘Implementation of the Federal Grants and Cooperative Agreements Act of 1977, Office of Management
Budget, August 18, 1978, Federal Register 43(161): 36860-36865. ‘Substantial involvement’ does not have a formal
regulatory definition, but it is described variously as entailing direction and redirection of the technical aspects of
the project as a whole; sharing responsibility with the firm for the management, control, direction, and performance
of the project.
10
NASA Grant and Cooperative Agreement Manual, Revised September 16, 2016.

13

private firm and government. Grants typically allow the recipient firm’s principal investigator to
make virtually all key decisions during the research project, subject to compliance with Federal
regulations. In cooperative agreements, decision rights are more evenly distributed between
government and firm personnel. Daily decisions are often jointly determined by both parties. For
example, in a cooperative agreement between the National Cancer Institute (NCI) and
GlobeImmune, Inc. to develop yeast-based tarmogens for cancer immuno-therapy, the NCI and
GlobeImmune each had its own Principal Investigator. This role, as specified in the agreement,
was to be ‘person(s) designated by the Parties who will be responsible for the scientific conduct
of the Research Plan.’11
Cooperative agreements also often provide the government with the right to terminate a
project before its official completion should the government’s principal decision-maker on the
project determine that its progress is not satisfactory. Indeed, the Department of Energy’s Model
Cooperative Agreement in Energy Efficiency and Renewable Energy contains not only regular
review meetings for the government, but also contains a section that grants the government
‘Go/No Go Decisions’ and decision-making authority at key milestones in the project.12 In
contrast, although the government can in principle decide to withhold subsequent funding
payments from an in-process grant, this tactic is cumbersome to implement and rarely employed.
The third substantive difference between grants and cooperative agreements stems from
the decision-rights difference, and relates to the degree of information that passes between the
private firm and the government. Although a grant is awarded to a recipient firm through a

11

‘Preclinical and Clinical Development of GlobeImmune, Inc’s Proprietary Yeast-Based Tarmogens Expressing
Tumor-Associated Antigens for Cancer Immunotherapy,’ between the National Cancer Institute, NIH, and
GlobeImmune, Inc., signed 05/08/2008.
12
‘Model Cooperative Agreement,’ Contractual Term 7D. U.S. Department of Energy, Energy Efficiency and
Renewable Energy Program, 02/19/2013.

14

rigorous review process, during the project the recipient is only required to provide the agency
with periodic (often annual) reports of progress made on the grant’s objectives. After the project
is completed, the recipient has a finite amount of time (usually 90 days) to file a final report of
accomplishments.
In cooperative agreements, the private firm is expected to provide information to the
government on a much more frequent basis. Given that decisions regarding project tasks are
sometimes made as frequently as daily, information must flow almost continuously to support
informed government decision-making. In those cases where government and private firm
scientists work closely together, this can occur informally through the collaborative effort. In
cases where this collaboration does not occur consistently, agreements stipulate formal
obligations to provide for communication. Thus, in contrast to research grants where researchers
provide information to the government at specified, infrequent intervals, cooperative agreements
stipulate more rapid communication and flow of information.
For example, a cooperative agreement between the Department of Energy and Mascoma
Corporation, for a project to demonstrate feasibility of biorefining technology using plant
biomass, specified that ‘in order to adequately monitor project progress and provide technical
direction to the Recipient, DOE must [attend Mascoma Corporation] meetings, reviews and
tests.’ Presumably to protect against malfeasance, the cooperative agreement further noted,
‘[Mascoma Corporation] shall notify the DOE Project Officer of meetings, reviews, and tests in
sufficient time to permit DOE participation and provide all appropriate documentation for DOE
review.’13
Overall, then, research grants and cooperative agreements represent discrete structural

13

‘Demonstration of Biorefinery Application,’ between the Mascoma Corporation and the Department of Energy,
signed 09/30/2008.

15

alternatives for R&D contracting between the U.S. Federal government and private firms. Grants
largely reflect canonical arms-length contracting, with little interaction during the research
project except for intermittent progress reports and with the research firm retaining almost
complete discretion over its allocation of effort. Cooperative agreements reflect contracting of
the type prescribed above to effectively manage contractual hazards, with the government
holding substantial monitoring authority and discretion over effort allocation and with the
requisite information flow and interaction between the parties.

DATA, MEASURES, AND EMPIRICAL STRATEGY
Data sources
To empirically test the predictions in this study, we employ data on the characteristics of U.S.
Federal Government research grants and cooperative agreements, characteristics of the
government bureau soliciting the project (notably the degree of relevant expertise in the local
bureau offices), characteristics of the firm performing the project, and measures of innovative
outcomes. We obtain this data from three sources.
The first dataset contains information on the characteristics of agreements from
USASpending.gov. We downloaded all government grants and cooperative agreements (termed
‘assistance’ in the USASpending.gov nomenclature) executed between fiscal years 2000 and
2011. Each record contains information on the governance mechanism (grant or cooperative
agreement), the firm that received the funding, the principal location in which the organization
would perform the research (e.g., Cincinnati, Ohio), the agency or bureau of the government that
made the award (e.g., National Institute of Standards and Technology), the title and short

16

description of the project, and other details.14 Of particular relevance, project descriptions list a
set of activities necessary for the research project. We use records for only those organizations
categorized as businesses in the government agreement records.15 We also remove cases where
the funding agency was part of the Department of Defense or military due to data limitations.
We employ a second dataset of granted U.S. patents, provided by PatentsView.org, to
measure patent generation. We download all U.S. patents with a ‘government interest’ indicated
in the patent application. Per U.S. regulations, patents that have any affiliation with a
government unit – including any funding from that unit – must include a government-interest
statement that acknowledges this affiliation. Government-interest statements refer to affiliated
grants and cooperative agreements by their unique ‘funding identification numbers.’ We then
conduct an exact match against the data in the USASpending database using the funding
identification numbers included in patent applications, thus identifying all patents that stipulated
an affiliation with any of the contracts in the sample. We ultimately identified 1,544 patents. Of
the 4,074 contracts in our sample, 508 (12.47%) led to at least one patent; 56 of these agreements
supported five patents or more. Separately, to create control variables as discussed below, we use
the PatentsView data to construct counts of aggregate patenting per year by each of the 383
private firms involved in any of the sample contracts.
To identify the level of relevant expertise available in specific government bureau
offices, we rely on a third and relatively novel database from the U.S. Office of Personnel
Management’s (OPM) Central Personnel Data Files (CPDF). These records contain annual,

14

A codebook for the federal assistance dataset is available at https://goo.gl/TW7QHY (last accessed February 17,
2017). To fill in some missing project descriptions, we searched the Federal Procurement Data System (FPDS) and
National Institutes of Health RePORTER system for federal award IDs matched to government-supported patents.
15
To avoid any university affiliates misclassified as businesses, we also exclude records where the word ‘university’
appears in the organization name.

17

individual-level information on nearly all U.S. civil servants during the sample period (using
anonymous identifier codes), including information on work location, job title and occupation,
and research-related job functions.16 The CPDF allows us to measure the precise number of
government employees in each office of each Federal bureau who perform specific jobs. The
CPDF categorizes over 800 occupations into 59 occupational groups/families, as detailed in
OPM’s Handbook of Occupational Groups and Families.17 For example, the ‘Medical, Hospital,
Dental, and Pubic Health Group’ includes physician assistants, nurses, nurse assistants, and
doctors of dentistry, medicine, and osteopathy, among other related job titles. From these data,
we compute the number of personnel in each of the 59 occupational categories at every known
Federal work location in the US, geocoding each employee’s latitude and longitude. This
occupation-location data is further disaggregated to the bureau level (e.g., the NIH is a bureau of
the Department of Health and Human Services). Thus, we are able to identify how many of the
NIH’s employees in a particular location are in the Medical, Hospital, Dental, and Public Health
Group occupational category.
Further, for each government employee who is involved in research-related activities,
broadly defined, the CPDF also includes a ‘functional research’ category, where the set of
categories includes research, development, testing and evaluation, construction, production,
installation, data collection, project management, and teaching. These classifications are created
by the National Science Foundation for OPM to describe the work that comprises the majority of
each research employee’s time. The ‘research’ function, for example, emphasizes early-stage
research – ‘systematic, critical, intensive investigation directed toward the development of new

16

This dataset excludes the military, U.S. Post Office, and ‘sensitive’ agencies (colloquially known as ‘three-letter
agencies’) and occupations (such as U.S. Marshals). More than 70 percent of U.S. Federal Government employees
work outside the greater-DC metro area.
17
Available at: https://www.opm.gov/fedclass/GShbkocc.pdf (last accessed February 14, 2017)

18

or fuller scientific knowledge of the subject studied’ – whereas the ‘testing and evaluation’
function emphasizes later stage ‘testing of equipment, materials, devices, components, systems
and methodologies under controlled conditions and the systematic evaluation of test data to
determine the degree of compliance of the test item with predetermined criteria and
requirements.18 We are thus able to identify how many of the NIH’s Medical, Hospital, Dental,
and Public Health Group personnel in any location are dedicated to early-stage research, how
many dedicated to development, and so on.
With data from these three sources – USASpending, PatentsView, and the CPDF
database – we construct our variables.

Variables
Our first two predictions relate to the choice of governance for a research contract. The
dependent variable for testing these predictions is Coop Agreementj, which is a binary indicator
set equal to one if contract j was a cooperative agreement and zero if a grant. Our third prediction
relates to the innovative performance of research contracts. We employ five dependent variables
to test this prediction. Generates Patentj, is a binary indicator set equal to one if contract j
generated at least one patent and zero otherwise. NumPatentsj is a count of patents generated by
contract j. Citation-Weighted Patentsj, is the sum of the patents generated by contract j and the
subsequent citations to those patents. Citations/Patentj is constructed as Citation-Weighted
Patents/NumPatents. Citations/Patent/Yearj is constructed as Citation-Weighted
Patents/NumPatents divided by the number of years since contract j was signed.
Governance. The main independent variables of interest predicting the choice of

18

Office of Personnel Management (November 14, 2014). The Guide to Data Standards, Update 16, A159-A167.

19

governance are Early-Stage Personnelj, which proxies for high-uncertainty projects, and
Personnel Expertise Ratioj, which measures the ability of government personnel to provide
effective oversight on a project. For ease of explication, we discuss these in reverse order.
Personnel Expertise Ratioj is defined as the proportion of occupational categories
required to conduct contract j that are available among geographically proximate client bureau
personnel. We measure this using a three-step procedure. First, for each of the 59 occupational
categories identified in the CPDF handbook, we create a list of distinct terms in the constituent
job titles.19 Next, we search for these terms in contract j’s project description. If a term from an
occupational category is found in the project description, then contract j is coded as requiring the
skills of that category. Thus, each contract is characterized as drawing on a subset of the 59
occupational skill sets, with the median contract requiring skills from eight occupational
categories, the mean contract requiring skills from 12.8 occupational categories, and the standard
deviation across all contracts at 15.8. Finally, for each contract j, we calculate the proportion of
requisite categories for which the sponsoring government bureau had at least one employee
within a 100-mile radius of the principal research location during the year that the contract was
signed.20 For example, if a project description signed in 2005 contained terms that occurred in
occupational categories x, y, and z, and the sponsoring government bureau had at least one
employee working in each of categories x and y that year within 100 miles of the research
location, then the Personnel Expertise Ratio for that contract would be 0.67. We predicted that
contracts are more likely to include monitoring/decision-rights provisions when the government

19

For example, the term list for the ‘Medical, Hospital, Dental and Public Health Group’ includes terms such as
health, scienc*, medic*, physician, autopsy*, dietitian, nutritionist, diagnost*, radiolog*.
20
We use a 100-mile radius for our core estimations because this roughly corresponds to the maximum distance that
one can drive twice in one workday (outbound and return) and still have time for a half-day meeting. We replicate
all estimations with alternative radii of 200, 300, 400, and 500 miles.

20

has personnel who are sufficiently expert to fulfill these duties effectively; consequently, we
expect the coefficient on Personnel Expertise Ratio to be positive.
As noted above, we expect cooperative agreements will be favored for more uncertain
projects. We test this by using the functional research categories in the CPDF data to proxy for
the early-stage nature of a contract. Imagine that contract j’s project description contains words
that occur only in occupational category x and contract k’s project description contains words
that occur only in occupational category y. If the government employees in occupational
category x are clustered in the research function, while employees in occupational category y are
clustered in the development function, then contract j is likely to cover a more early-stage
research project than contract k.
Given this, Early-Stage Personnelj is defined as the proportion of a bureau’s
geographically proximate employees in contract j’s requisite occupational categories who are
assigned to a research function. We measure this in a three-step process. We start with the list of
relevant occupational categories for contract j and identify client bureau personnel in those
categories within 100 miles of the location of work. We then calculate the percent of these
relevant employees who are also categorized in the research function. We predicted that
contracts are more likely to include monitoring/decision-rights provisions when the project
entails early-stage effort; consequently, we expect the coefficient on Early-Stage Personnel to be
positive.
Performance. The main independent variable of interest in the innovative performance of
research contracts is the contractual form: Coop Agreement, defined above. We predicted that,
conditional on government personnel assigning contracts according to project uncertainty and
presence of relevant skills, cooperative agreements should outperform grants. Therefore, we

21

expect the coefficient on Coop Agreement to be positive.
Control variables. We include several additional variables to control for various project,
firm, bureau, and time-based characteristics. It is possible that projects with larger budgets or
more firm co-funding are more likely to fall under a particular governance form or generate
patents. We therefore include Federal Fundingj, defined as the dollar amount contributed by the
government to support contract j, as well as Firm/Total Fundingj, defined as the amount
contributed by the firm divided by total funding for contract j. Larger or higher-patenting firms
might be more likely to generate a patent from the contracted research and/or be differentially
likely to operate under a particular contractual form. To address this, we include Large Firmj,
which equals one if the firm is categorized by the government as a ‘large for-profit enterprise’ in
the research contract document and zero if it is coded as a ‘small business enterprise.’ We also
include Prior Patentsj, defined as the number of patent applications filed by the focal firm in the
year preceding the signing of contract j. Given the skew in Federal Funding, Firm/Total
Funding, and Prior Patents, we standardize each variable and use the z-scores rather than using
the raw values.
A bureau’s choice of contractual form, and the performance of its contract, might be
affected by the degree to which its local office is managing several concurrent contracts. For a
cooperative agreement in particular, this could affect performance if government researchers
with relevant expertise are unable to devote as much attention and effort to contract j’s research
project as would be optimal. We include Coops Within 100 Milesj and Coops/Personnel Ratioj,
defined respectively as the number of in-process cooperative agreements within a 100-mile
radius of contract j’s principal research location and the ratio of these agreements to research
personnel in the local bureau. Note that the Coops/Personnel Ratio variable obliquely proxies for

22

the feasibility of coordination between government researchers and the focal firm; if the
government researchers are stretched too thin, then they will not be able to effectively monitor or
make decisions regarding contract j’s research project. Contracts undertaken in different years
might have different forms and outcomes due to temporal pressures on personnel; to address this
we include fiscal-year fixed effects. Finally, for roughly 31 percent of contracts, the project
description does not yield a link to any occupational categories, which precludes identifying
employees with relevant skills. In these instances, we set Personnel Expertise Ratio equal to 0.
To separate these from the qualitatively different instances in which contract j is linked to
occupational categories and the bureau has no relevant local personnel, we also include No
Expertisej, a binary variable set to one if contract j’s project description is not linked to any
occupational categories and to zero otherwise.21
Table 1 provides summary statistics for our sample. As noted above, 12.5 percent of our
sample contracts generate at least one patent. Almost one-quarter of the contracts are cooperative
agreements. Slightly more than one-quarter of the contracts involve a large firm. The average
number of concurrent cooperative agreements managed by the local relevant bureau is nearly six,
which equates to nearly 0.5 agreements per local research employee. The average cooperative
agreement is five times as likely to generate a patent as the average grant, and is more likely to
entail early-stage effort. The average cooperative agreement is also more likely to be performed
by a large, high-patenting organization in conjunction with a bureau that has relevant local
expertise. Correlation matrices for the sample are provided in Table B1 of Appendix B.

[TABLE 1 ABOUT HERE]

21

We further address the issue of missing values in the Data section of Appendix B.

23

Empirical strategy
To appropriately estimate the models we employ a two-stage econometric technique that first
estimates the probability of selecting into a grant or cooperative agreement governance mode and
then estimates the effect of this cooperative agreement ‘treatment’ on patenting, using
information from the selection model to correct for the non-random nature of the treatment
model. The first stage provides a test for our governance predictions while the second stage
provides a test for our performance predictions.
This estimation approach requires an instrumental variable for contract form in the firststage selection model. Our instrument is Personnel Expertise Ratio. As described above, when
there are fewer government research personnel with relevant expertise in the geographic area of a
research location, the government bureau is less able to accurately evaluate progress in the
research project. Thus, allocating decision rights and monitoring rights to the government will
provide little governance benefit and will merely impose bureaucracy costs on the project;
therefore, cooperative agreements will be negatively correlated with the number of
geographically proximate government personnel with relevant technical expertise. At the same
time, the presence of local government research personnel with expertise per se is unlikely to be
correlated with patenting, to the extent that firm and government scientists are effectively
substitutes in production. We address the robustness of this assumption in the Robustness Checks
section of Appendix B.
One complication in our setting is that both selection and treatment models have
dichotomous dependent variables. Consequently, several conventional two-stage approaches are
inappropriate because they are only robust in linear settings (Chesher, 2010; Wooldridge, 2010).

24

Our preferred method is the inverse probability weighting regression adjustment method
(IPWRA) (Angrist, 1998; Angrist and Pischke, 2009). Appendix B provides a more detailed
explanation of the IPWRA method, along with an assessment of its strengths and weaknesses. In
short, this method is appropriate when a researcher wants to estimate treatment effects from
observational data combining regression adjustment with inverse probability weighting. It is also
appropriate when the choice of treatment is endogenous (e.g., whether to use a cooperative
agreement or grant agreement) and there is a dichotomous variable in the second stage outcome
equation (e.g., whether an agreement yields a patent). Finally, it is attractive because it allows
both the treatment and control groups to have their own set of second-stage coefficients,
recognizing that those in each group may be differentially affected by the covariates.
We believe that the IPWRA approach is the most relevant and most accurate statistical
approach for this research question and this data set. Nevertheless, we re-estimate the models
using alternative approaches, including the single stage (‘naïve’) probit, the two-stage least
squares linear probability model, the bivariate probit, the instrumental variables probit, and the
full-information-maximum-likelihood (FIML) approach with joint normality in the error terms. 22
The results of these robustness checks are reported in Appendix B. Virtually all results using
these methods are qualitatively identical to the results discussed in the next section.

EMPIRICAL RESULTS
The central results of this paper are reported in Table 2. Model 1 presents the first stage IPWRA
results that emanate from the governance predictions in the theory. Models 2A and 2B present

22

All of these estimation approaches are appropriate conditional on the government funding a project. If one instead
assumes that the government decides among grant, cooperative agreement, and not funding the project at all, then a
multi-level treatment model would be appropriate. However, we cannot observe non-funded CFPs.

25

the second stage IPWRA results that arise from the contract performance predictions of the
theory. The first stage of this model includes all exogenous predictors as well as the instrument,
Personnel Expertise Ratio, which predicts selection into a cooperative agreement rather than a
grant. The second stage incorporates the inverse of the predicted probability of selection from the
first stage as a weight in the estimation as well as regression adjustment.

[TABLE 2 ABOUT HERE]

We begin with the first-stage results in Model 1, for which the dependent variable is
Coop Agreement. Of particular importance, the coefficient on Personnel Expertise Ratio is
positive, statistically significant (p = 0.000), and of substantial magnitude. Research contracts are
more likely to be organized as cooperative agreements when the sponsoring bureau has relevant
skills in its geographically proximate offices. The marginal effect of a bureau having personnel
in all relevant areas for a project leads to an 11 percent increase in the likelihood of a contract
being organized as a cooperative agreement rather than as a grant. This is also consistent with the
theoretical prediction that public-private contracts will be more likely to entail ongoing public
supervision when the public client possesses sufficient technical expertise to effectively exert its
supervisory responsibilities.
Consistent with our prediction, the coefficient on Early-Stage Personnel in the first stage
is positive and significant (p = 0.000), indicating that earlier-stage projects are more likely to be
governed by cooperative agreements. If all the personnel with expertise in areas relevant to a
project’s required expertise work in early-stage research positions, an agreement is 16 percent
more likely to be organized cooperatively than if none of them does. Early-stage projects, which

26

are typically considered to be more uncertain and hence entail more unobservable effort, are thus
associated with high-monitoring, high-client-decision-rights governance.
As for the control variables, contracts involving larger firms are more likely to be
organized as cooperative agreements. The amount of federal funding is not associated with
governance form, but the proportion of firm funding to total funding is positively associated with
cooperative agreements. Finally, projects that do not identify any areas of expertise were roughly
11 percent less likely to be organized as cooperative agreements, suggesting the government may
prefer arms-length financial support for narrowly-defined projects.
We now turn to the second-stage results, for which the dependent variable is Generates
Patent. Model 2A presents the estimated coefficients for contracts that were governed as
cooperative agreements, while Model 2B presents these for contracts that were governed as
grants. Both second-stage models rely on the results from the first stage. We have converted the
coefficient on the treatment variable, Coop Agreement, to a marginal effect. Thus, the coefficient
on Coop Agreement in Model 2A reflects the marginal effect of being a cooperative agreement
vs. being governed by a grant, for those contracts that were actually governed by cooperative
agreement, while its counterpart in Model 2B reflects the effect of being a cooperative agreement
vs. a grant, for those contracts that were actually governed by grant. The coefficient in Model 2A
is 0.278, indicating that the average cooperative agreement in our sample was nearly 28 percent
more likely to generate a patent than it would have been if it were organized as a grant, holding
all other variables at the mean. In contrast, the coefficient in Model 2B is 0.083, indicating that
the average grant in our sample would have been eight percent more likely to generate a patent
had it been organized as a cooperative agreement. Put differently, consistent with our prediction,
cooperative agreements are associated with higher innovative output than are grants, controlling

27

for the other independent variables; this effect is more pronounced for contracts that actually
were organized as cooperative agreements than for contracts that actually were organized as
grants.
In model 2A, several other variables influence the likelihood that a cooperative
agreement generates a patent. Of particular note, Coops/Personnel Ratio is negatively related to
Generates Patent; this is consistent with the notion that as a bureau’s scientific personnel gets
stretched thinly, they are less able to engage in smooth coordination of effort with firm
personnel, thus lowering research productivity. No coefficients on the control variables are
significant at conventional thresholds except for Prior Year Patents.
Turning to Model 2B, which focuses on contracts that were organized as grants, the
coefficients on Large Firm and Firm/Total Funding are both positive and significant (p = 0.033
and p = 0.046, respectively). This indicates that larger organizations are more likely to generate
patents and, consistent with incentive theory, firms that have ‘skin in the game’ are also more
likely to generate patents.
Taken together, the above results suggest that the presence or absence of relevant
expertise influences the governance of research contracts, such that cooperative agreements are
substantially more likely when the sponsoring government bureau has relevant skills in
geographically proximate offices. In addition, early-stage projects are more likely to be governed
as cooperative agreements. In turn, cooperative agreements are more likely than grants to
generate patents. Had the average cooperative agreement been governed as a grant, it would have
had a 28 percent lower probability of generating a patent. That said, those projects organized as
grants would not have enjoyed a comparable increase in probability of patent generation since
they are qualitatively different than the projects organized as cooperative agreements. Had the

28

average grant been governed as a cooperative agreement, it would have had an eight percent
higher probability (from a lower initial baseline) of generating a patent. 23

Extensions: The magnitude of innovative performance
The above estimation focuses on a binary measure of innovative performance—whether or not a
research contract generates at least one patent. It is possible that other measures of innovative
output will indicate different impacts of contract structure. As noted above, we constructed
alternative measures of innovative output, notably Num Patents, Citation-Weighted Patents,
Citations/Patent, and Citations/Patent/Year. Table 3 presents results for IPWRA estimation of
models with these four dependent variables. The Table only shows the second-stage results
because the first-stage results are identical to those of the IPWRA estimation in Table 2’s Model
1, by definition.

[TABLE 3 ABOUT HERE]

For each measure of innovative output, the coefficient on Coop Agreement is uniformly
positive and statistically significant (p = 0.000 in all models). In all four cases, this coefficient is
substantially higher for contracts that actually were governed by cooperative agreements than for
contracts that were governed by grants; the coefficient ranges from roughly five times larger to
as much as 15 times larger. (The largest differences occur because cooperative agreements
23

This raises a question: If the average grant would enjoy a positive (albeit small) increase in probability of patent
generation if it were organized as a cooperative agreement, then why isn’t it governed by a cooperative agreement?
The negative coefficient on Coops/Personnel ratio implies that each cooperative agreement imposes a negative
externality on other geographically proximate cooperative agreements due to a congestion effect. Hence, for a wide
range of values, the modest bump in research productivity from converting a focal grant to cooperative agreement
will be offset by the declining productivity of nearby cooperative agreements. See Appendix B for a further
discussion of this point.

29

simultaneously generate more patents and more citations/patent, affecting Models 3–8.) This
consistent pattern of coefficient sign and magnitude matches the core results above. Because of
the extreme skewness of the dependent variables used in Table 3, we include in Appendix B the
coefficients on the second-stage Coop Agreement variable for identical models using logged
dependent variables (addressed as item seven in the robustness checks below and Table B8 in
Appendix B). The results for all eight models retain their sign and significance (p = 0.000 in all
models).

Robustness checks
There are several concerns that may arise from the specification and estimation strategies we
employ. They include: 1) sensitivity of results to the IPWRA approach or to the chosen
geographic radius for Personnel Expertise Ratio; 2) omitted variable bias related to temporal
variance, e.g., changes to the Federal budget thanks to the American Recovery and Reinvestment
Act of 2009 (ARRA, a.k.a. the Federal ‘stimulus package’); 3) unobserved heterogeneity at the
bureau level and/or firm level; 4) the empirical appropriateness of the instrumental variable; 5)
skewness of patenting and citation rates; 6) the possibility that the performance results reflect
more effective coordination between government and private sector scientists when they work on
cooperative agreements; and 7) unobserved heterogeneity in project quality assigned to grants
and cooperative agreements. We address in detail each of these concerns in appendices to the
paper; the first six concerns are addressed in the Robustness Checks section of Appendix B, and
the final concern is addressed in Appendix A. To summarize briefly: Across numerous
estimation methods and a wide range of geographic radii, the results illustrated in Table 2 remain
materially unchanged. Results also remain largely the same when a dummy variable to identify

30

ARRA contracts is included, when bureau and firm effects are included, and when logged
patenting and citations are used as dependent variables. Finally, after controlling for endogeneity
in contract choice to the best of our ability, the results remain qualitatively unchanged.

DISCUSSION AND CONCLUSION
Governments throughout the world spend tens of billions of dollars annually on contractually
sourced research. Yet the challenge of public contracting for private innovation, although of
substantial importance, is not well understood. In this paper, we shed light on this topic.
Conventional prescriptions from the ‘contracting for innovation’ literature do not apply
straightforwardly to government contracting because of restrictions that preclude the judicious
use of property rights, equity investment, royalty payments, or complex contractual provisions to
align parties’ incentives in the face of unobservable effort. Put differently, public contracting for
private innovation is an excellent setting in which to examine one particular mechanism to
induce effort: allocation of decision rights.
We predict that public retention of decision rights is more likely to be used in the face of
high project uncertainty and when the available government personnel have project-relevant
technical expertise. We also predict that the use of monitoring and decision rights will positively
influence the likelihood that a project results in a patented innovation. We then test these
predictions with data on U.S. Federal government contracts for innovation by private firms,
which are generally constrained to take one of two forms: grants, in which the government
retains virtually no decision rights, and cooperative agreements, in which the government retains
ongoing decision rights and attendant monitoring rights.
We find empirical support for the above predictions: cooperative agreements are more

31

likely to be used for early-stage projects than for later-stage projects, and cooperative agreements
are more likely to be used when local government personnel have relevant technical expertise.
Similarly, after accounting for endogeneity in governance choice due to project uncertainty and
personnel expertise, we find cooperative agreements are indeed associated with higher
innovative output than are grants. We interpret these results as evidence consistent with the idea
that a principal chooses to govern more-uncertain projects, where the problem of noncontractible
effort is higher, by retaining more decision rights and enforcing greater monitoring over the
project. However, when the principal lacks the relevant know-how to properly evaluate project
progress, it is preferable to leave decisions in the hands of the agent.
These results contribute to our understanding of value creation involving public
organizations. Specifically, as public actors strive to generate valuable innovations to serve
government needs or more broadly enhance social welfare, this study’s insights may help these
actors overcome the constraints of contractual rigidities. More generally, these results contribute
to the literature on public contracting (Moszoro et al., 2016) and to the debate over management
of noncontractible quality for a privatized service (Hart et al., 1997; Williamson, 1999), notably
for ‘hybrid’ public management (Ménard, 2004; Rangan, Samii, and Van Wassenhove, 2006).
Whereas prior literature has identified the importance of incentives for the government overseer
of a hybrid (Cabral et al., 2010, 2013), this study highlights the importance of that overseer’s
ability to evaluate the effort of the private provider.
At the broadest level, recent research on contractual governance has assessed the distinct
roles of coordination mechanisms vs. control mechanisms (Malhotra and Lumineau, 2011; Oxley
and Wada, 2009; Ryall and Sampson, 2009). For example, Lumineau and Malhotra (2011)
distinguish between contractual clauses that emphasize control and those that emphasize

32

coordination, and find that coordination-related clauses are associated with smoother functioning
of contracts in the face of interfirm friction. In a review of this literature, Lumineau (2017: 1561)
concludes that ‘a strong controlling focus may raise a constant policing…of the partner’s
performance…. Such a ‘carrot-and-stick’ approach with a strict oversight may create rigidity and
over-monitoring.’ Our study indicates that the relationship between rigidity and reliance on
control mechanisms may also flow in the opposite direction: in institutional contexts that impose
contractual rigidity, control mechanisms may be more feasible than coordination mechanisms.
This study also makes an additional empirical contribution in developing a measure of
government personnel skills at a far more microanalytic level than has been done in the past. We
can thus measure the precise level of skills or capabilities, across 59 occupational categories and
19 functional areas, possessed by the personnel of a given U.S. government bureau at a precise
geographic level such as an office location, town, or any geographic radius. To the best of our
knowledge, no other measure of government technical expertise exists at such a level of
granularity. More generally, although the capabilities literature focuses theoretically on
capabilities with specific uses, data constraints have tended to restrict empirical measurement of
capabilities to features such as patenting productivity (e.g., Tortoriello, 2015) or prior experience
in a particular industry (e.g., Klepper and Simons, 2000).
There are limitations to this study. First, we have excluded from our analysis pure
outsourcing arrangements, in which the government rather than the firm owns the property rights
to the innovation. It would be interesting to explore whether and how our theories and empirical
methods might apply to these very common arrangements. Second, our paper measures
innovative output as patents. One could imagine a situation where the government might value
other outputs, such as jobs, regional economic development, or a variety of political criteria.

33

Those aspects of a potential government utility function are outside the scope of this paper.
Third, we take contractual form as endogenously determined by the government, but without the
influence of firms who did not win. To the extent that non-winning applicants for innovation
contracts with the government influence contract form, our analysis will not capture that
influence. Finally, while our theoretical framework aspires to be universal, the empirical work is
particular to the institutional details and structure of government contracting for innovation in the
United States. We believe that exploring applications of the theory to other countries would be a
fruitful avenue for research.
There exist a number of further unexplored questions in this vein. Does government
funding enable a firm to deepen its current expertise, or to broaden its technological portfolio and
capabilities? How do the human-capital capabilities of the government affect the effectiveness of
private sector research beyond patents? These questions are part of a vibrant avenue for future
research on innovation and appropriation at the nexus of the government and private-firm R&D.

34

REFERENCES
Aghion P, Tirole J. 1994. The management of innovation. Quarterly Journal of Economics
109(4): 1185–1209.
Anderson SW, Dekker HC. 2005. Management control for market transactions: The relation
between transaction characteristics, incomplete contract design, and subsequent
performance. Management Science 51(12): 1734–1752.
Angrist JD. 1998. Estimating the labor market impact of voluntary military service using social
security data on military applicants. Econometrica 66(2): 249–288.
Angrist JD, Pischke J-S. 2009. Mostly Harmless Econometrics: An Empiricist’s Companion.
Princeton University Press: Princeton.
Argyres N, Mayer KJ. 2007. Contract design as a firm capability: An integration of learning and
transaction cost perspectives. Academy of Management Review 32(4): 1060–1077.
Argyres NS, Bercovitz J, Mayer KJ. 2007. Complementarity and evolution of contractual
provisions: An empirical study of IT services contracts. Organization Science 18(1): 3–19.
Arora A, Gambardella A. 2010. Ideas for rent: An overview of markets for technology.
Industrial and Corporate Change 19(3): 775–803.
Arruñada B, Garicano L, Vázquez L. 2001. Contractual allocation of decision rights and
incentives: The case of automobile distribution. Journal of Law, Economics, & Organization
17(1): 257–284.
Athey S, Roberts J. 2001. Organizational design: Decision rights and incentive contracts.
American Economic Review 91(2): 200–205.
Boyne GA. 2002. Public and private management: What’s the difference? Journal of
Management Studies 39(1): 97–122.
Cabral S, Lazzarini SG, de Azevedo PF. 2010. Private operation with public supervision:
Evidence of hybrid modes of governance in prisons. Public Choice 145(1/2): 281–293.
Cabral S, Lazzarini SG, de Azevedo PF. 2013. Private entrepreneurs in public services: A
longitudinal examination of outsourcing and statization of prisons. Strategic
Entrepreneurship Journal 7(1): 6–25.
Cassiman B, Veugelers R. 2006. In search of complementarity in innovation strategy: Internal
R&D and external knowledge acquisition. Management Science 52(1): 68–82.
Chesher A. 2010. Instrumental variable models for discrete outcomes. Econometrica 78(2): 575–
601.
Cohen WM, Levinthal DA. 1990. Absorptive capacity: A new perspective on learning and
innovation. Administrative Science Quarterly 35(1): 128–152.
Decarolis F, Giuffrida LM, Iossa E, Mollisi V, Spagnolo G. 2018. Bureaucratic Competence and
Procurement Outcomes. Working Paper, National Bureau of Economic Research. Available
at: http://www.nber.org/papers/w24201.
Grossman SJ, Hart OD. 1986. The costs and benefits of ownership: A theory of vertical and
lateral integration. Journal of Political Economy 94(4): 691–719.
Hart O, Shleifer A, Vishny RW. 1997. The proper scope of government: Theory and an
application to prisons. The Quarterly Journal of Economics 112(4): 1127–1161.
Hegde D. 2014. Tacit knowledge and the structure of license contracts: Evidence from the
biomedical industry. Journal of Economics & Management Strategy 23(3): 568–600.

35

Hourihan M, Parkes D. 2015, February 24. Federal R&D in the FY 2016 budget: An overview.
American Association for the Advancement of Science. Available at:
https://www.aaas.org/fy16budget/federal-rd-fy-2016-budget-overview [26 September 2017].
Klein PG, Mahoney JT, McGahan AM, Pitelis CN. 2013. Capabilities and strategic
entrepreneurship in public organizations. Strategic Entrepreneurship Journal 7(1): 70–91.
Klepper S, Simons KL. 2000. The Making of an oligopoly: Firm survival and technological
change in the evolution of the U.S. tire industry. Journal of Political Economy 108(4): 728–
760.
Lan Z, Rainey HG. 1992. Goals, rules, and effectiveness in public, private, and hybrid
organizations: More evidence on frequent assertions about differences. Journal of Public
Administration Research and Theory: J-PART 2(1): 5–28.
Lerner J, Malmendier U. 2010. Contractibility and the design of research agreements. American
Economic Review 100(1): 214–246.
Lerner J, Merges RP. 1998. The control of technology alliances: An empirical analysis of the
biotechnology industry. The Journal of Industrial Economics 46(2): 125–156.
Levin J, Tadelis S. 2010. Contracting for government services: Theory and evidence from U.S.
cities. The Journal of Industrial Economics 58(3): 507–541.
Lumineau F. 2017. How contracts influence trust and distrust. Journal of Management 43(5):
1553–1577.
Lumineau F, Malhotra D. 2011. Shadow of the contract: How contract structure shapes interfirm
dispute resolution. Strategic Management Journal 32(5): 532–555.
Malhotra D, Lumineau F. 2011. Trust and collaboration in the aftermath of conflict: The effects
of contract structure. Academy of Management Journal 54(5): 981–998.
Mayer KJ, Salomon RM. 2006. Capabilities, contractual hazards, and governance: Integrating
resource-based and transaction cost perspectives. Academy of Management Journal 49(5):
942–959.
McKenna, C.D. 2006. The World's Newest Profession: Management Consulting in the Twentieth
Century. Cambridge: Cambridge University Press.
Ménard C. 2004. The economics of hybrid organizations. Journal of Institutional and
Theoretical Economics JITE 160(3): 345–376.
Miller AS. 1955. Government contracts and social control: A preliminary inquiry. Virginia Law
Review 41(1): 27–58.
Moe TM. 1989. The politics of bureaucratic structure. In Can the Government Govern?
Brookings Institution: Washington, D.C.: 267–329.
Moe TM. 1990. Political institutions: The neglected side of the story. Journal of Law,
Economics, & Organization 6: 213–253.
Moszoro M, Spiller PT, Stolorz S. 2016. Rigidity of public contracts. Journal of Empirical Legal
Studies 13(3): 396–427.
National Audit Office. 2013. Research and Development funding for science and technology in
the UK. Memorandum for the House of Commons Science and Technology Committee,
London.
Oxley JE. 1997. Appropriability hazards and governance in strategic alliances: A transaction cost
approach. Journal of Law, Economics, & Organization 13(2): 387–409.
Oxley JE, Wada T. 2009. Alliance structure and the scope of knowledge transfer: Evidence from
U.S.-Japan agreements. Management Science 55(4): 635–649.

36

Pisano GP. 1990. The R&D Boundaries of the firm: An empirical analysis. Administrative
Science Quarterly 35(1): 153–176.
Rangan S, Samii R, Van Wassenhove LN. 2006. Constructive partnerships: When alliances
between private firms and public actors can enable creative strategies. Academy of
Management Review 31(3): 738–751.
Reuer JJ, Ariño A. 2007. Strategic alliance contracts: Dimensions and determinants of
contractual complexity. Strategic Management Journal 28(3): 313–330.
Reuer JJ, Ariño A, Mellewigt T. 2006. Entrepreneurial alliances as contractual forms. Journal of
Business Venturing 21(3): 306–325.
Reuer JJ, Devarakonda SV. 2016. Mechanisms of hybrid governance: Administrative
committees in non-equity alliances. Academy of Management Journal 59(2): 510–533.
Ryall MD, Sampson RC. 2009. Formal contracts in the presence of relational enforcement
mechanisms: Evidence from technology development projects. Management Science 55(6):
906–925.
Sampson RC. 2004a. Organizational choice in R&D alliances: Knowledge-based and transaction
cost perspectives. Managerial and Decision Economics 25(6/7): 421–436.
Sampson RC. 2004b. The cost of misaligned governance in R&D alliances. Journal of Law,
Economics, & Organization 20(2): 484–526.
Spiller PT, Moszoro M. 2014. Third-party opportunism and the theory of public contracts:
Operationalization and applications. In The Manufacturing of Markets: Legal, Political and
Economic Dynamics, Brousseau E, Glachant J-M (eds). Cambridge University Press: New
York: 229–252.
Teece DJ. 1986. Profiting from technological innovation: Implications for integration,
collaboration, licensing and public policy. Research Policy 15(6): 285–305.
Tortoriello M. 2015. The social underpinnings of absorptive capacity: The moderating effects of
structural holes on innovation generation based on external knowledge. Strategic
Management Journal 36(4): 586–597.
Williamson OE. 1999. Public and private bureaucracies: A transaction cost economics
perspectives. Journal of Law, Economics, and Organization 15(1): 306–342.
Wooldridge JM. 2010. Econometric Analysis of Cross Section and Panel Data, 2nd ed. MIT
Press: Cambridge, MA.
Xiao W, Xu Y. 2012. The impact of royalty contract revision in a multistage strategic R&D
alliance. Management Science 58(12): 2251–2271.

37

Table 1. Descriptive statistics for variables used in primary analysis

Variable

Mean

All contracts
(N=4,074)
S.D.
Min.

Generates Patent
0.125
0.330
0
Num. Patents Generated
0.322
1.963
0
Citation-Weighted Patents
1.815 23.513
0
Citations/Patent
0.496
3.607
0
Citations/Patent/Year
0.045
0.252
0
Coop Agreement
0.225
0.418
0
Personnel Expertise Ratio
0.059
0.159
0
Early Stage Personnel
0.009
0.070
0
Federal Fundinga
0
1
-2.331
Firm/Total Fundinga
0
1
-0.354
Large Firm
0.268
0.443
0
Prior Year Patentsa
0
1
-0.316
Coops/Personnel Ratio
0.459
1.839
0
Coops within 100 Miles
5.703 10.553
0
No Expertise
0.312
0.463
0
Note. All variables calibrated to 100-mile distance.
a
z-score standardized

Max.
1
59
1082
100.333
6.271
1
1
1
31.400
4.101
1
16.323
25
68
1

Cooperative agreements only
(N=916)
Mean
S.D.
Min.
Max.

Mean

Grants only
(N=3,158)
S.D.
Min.

0.326
1.096
7.480
1.793
0.146
1
0.117
0.013
0.214
0.870
0.750
0.483
1.093
12.503
0.084

0.066
0.097
0.172
0.120
0.015
0
0.042
0.008
-0.062
-0.252
0.128
-0.140
0.275
3.730
0.378

0.249
0.439
1.817
1.323
0.112
0
0.134
0.068
0.871
0.561
0.334
0.801
1.322
7.945
0.485

38

0.469
3.964
49.073
7.050
0.475
0
0.217
0.078
1.342
1.546
0.433
1.392
2.917
14.771
0.278

0
0
0
0
0
1
0
0
-0.609
-0.354
0
-0.316
0
0
0

1
59
1082
100.333
6.271
1
1
1
21.336
4.101
1
16.323
24
65
1

0
0
0
0
0
0
0
0
-2.331
-0.354
0
-0.316
0
0
0

Max.
1
6
61
61
3.813
0
1
1
31.400
4.101
1
16.323
25
68
1

Table 2. Two-stage IPWRA probit estimation of patent generation
First-stage
model
Model 1
Personnel Expertise Ratio (IV)

Second-stage models
Model 2A
(subsample:
Coops)

Model 2B
(subsample:
Grants)

0.651**
(0.165)

Coop Agreement

0.278**
0.083**
(0.032)
(0.006)
Early Stage Personnel
1.007**
-3.660
0.700
(0.275)
(2.155)
(0.385)
Federal Funding
0.038
0.046
0.061
(0.035)
(0.046)
(0.035)
Firm/Total Funding
0.286**
0.077
0.148*
(0.031)
(0.046)
(0.069)
Prior Year Patents
0.005
-0.171**
-0.025
(0.024)
(0.058)
(0.066)
Coops/Personnel Ratio
-0.003
-0.090*
-0.079
(0.014)
(0.046)
(0.048)
Coops within 100 Miles
0.018**
0.005
0.005
(0.003)
(0.007)
(0.006)
Large Firm
1.270**
0.239
0.304*
(0.068)
(0.154)
(0.152)
No Expertise
-0.663**
0.057
-0.152
(0.090)
(0.284)
(0.152)
Fiscal Year Fixed Effects
YES
YES
YES
Note. N = 4,074 in Model 1, 916 in Model 2A, and 3,158 in Model 2B.
Heteroskedasticity-robust standard errors are reported in parentheses. Models
specified using 100-mile distance variables. Coefficients on Coop Agreement in
Models 2A & 2B are interpreted as marginal effects; i.e., conditional on being a
cooperative agreement or grant, respectively.
* p < .05, ** p < .01

39

Table 3. Second-stage IPWRA results of patent generation and quality outcomes
Citations/
patent/year
Variable
Model 1
Model 2
Model 3
Model 4
Model 5
Model 6
Model 7
Model 8
Coops
Grants
Coops
Grants
Coops
Grants
Coops
Grants
Coop Agreement
0.666**
0.145**
4.516**
0.273**
1.506**
0.169**
0.121**
0.022**
(0.090)
(0.016)
(0.819)
(0.063)
(0.230)
(0.031)
(0.018)
(0.003)
Early Stage Personnel
-1.285
0.072
-6.281*
0.098
-1.699*
0.035
-0.160*
0.003
(0.663)
(0.121)
(2.976)
(0.268)
(0.803)
(0.098)
(0.077)
(0.014)
Federal Funding
0.208*
0.101*
0.781*
0.031
0.099
-0.006
0.011
-0.001
(0.098)
(0.040)
(0.394)
(0.035)
(0.059)
(0.010)
(0.008)
(0.002)
Firm/Total Funding
0.060
0.107*
1.184
0.148
-0.062
0.042
0.001
0.006
(0.064)
(0.046)
(0.632)
(0.173)
(0.112)
(0.060)
(0.010)
(0.009)
Prior Year Patents
0.174
-0.038
1.658
-0.167
0.029
-0.063
-0.004
-0.009*
(0.143)
(0.037)
(0.958)
(0.091)
(0.124)
(0.033)
(0.009)
(0.005)
Coops/Personnel Ratio
0.042
-0.054*
0.034
-0.050
-0.039
-0.014
-0.005
-0.002
(0.107)
(0.021)
(0.158)
(0.041)
(0.021)
(0.016)
(0.002)
(0.002)
Coops within 100 Miles
-0.009
0.003
-0.079
-0.008
-0.008
-0.003
-0.001
-0.000
(0.021)
(0.004)
(0.054)
(0.006)
(0.007)
(0.002)
(0.001)
(0.000)
Large Firm
0.609
0.136*
-0.244
0.512*
0.291
0.204
0.033
0.032*
(0.320)
(0.059)
(1.133)
(0.257)
(0.210)
(0.115)
(0.021)
(0.015)
No Expertise
0.481
0.031
-1.329
-0.099
-0.106
-0.069
0.012
-0.001
(0.381)
(0.036)
(1.247)
(0.135)
(0.245)
(0.085)
(0.031)
(0.006)
Year Fixed Effects
YES
YES
YES
YES
YES
YES
YES
YES
Note. N = 916 in odd-numbered models and 3,158 in even-numbered models. Heteroskedasticity-robust standard errors are reported in
parentheses. Models specified using 100-mile distance variables.
* p < .05, ** p < .01
Num. patents generated

Citation-weighted patents

40

Citations/patent

APPENDIX A: THE PUBLIC CONTRACTING PROCESS FOR INNOVATION

The public contracting process for innovation operates similarly for grants and for
cooperative agreements. The process unfolds as each government agency strives to accomplish
its research agenda, which itself is driven by the agency’s strategic plans, input from scientific
review boards, legislative mandates, and/or current exigencies. An agency is allocated funds
from Congress. Some of these funds are earmarked for specific research areas; the bulk are to be
allocated at the agency’s discretion. Given its agenda, the agency identifies a particular research
project of interest and posts a Request or Call for Proposals (CFP) to the public. The CFP usually
specifies the nature of the research, the desired deliverables, the general process for oversight,
and the anticipated maximum amount of the grant/cooperative agreement. Firms (and other
organizations) then submit proposals; coincidentally, the funding amount specified in almost all
submitted proposals is exactly the anticipated maximum amount in the CFP. The proposals are
then evaluated by agency personnel according to the criteria specified in the CFP, which includes
metrics on the ability of the proposed researchers to successfully meet the objectives of the CFP.
Agency personnel can choose to award a single project (to a single applicant), or multiple
projects (to multiple applicants) that work along different paths towards the same goals. In both
grants and cooperative agreements, the proposal must specify the researchers, equipment, and
facilities to be used. Although there can be some modest “revise and resubmit” interaction
around these proposals, they normally do not entail extended negotiation/lobbying between firm
and agency.
Although there is little room for negotiation between firm and agency once a CFP is
released, it is possible that firms lobby the agency to encourage CFPs in certain broad areas of

41

research. (That said, the firm would still need to be awarded the proposal in a competitive
process.) Also, although these projects are intended to support an agency’s overall research
agenda, it is possible that agency researchers favor CFPs in certain fields because this allows
them to pursue their pet research. For the purposes of our study, the main question is: would such
distortions affect the governance or performance of research projects in a way that conflates our
results?
For example, if innovative firms are influential and also prefer cooperative agreements,
then we might find that cooperative agreements yield more patenting than grants simply because
“better” firms are lobbying the agency for projects that will be governed as cooperative
agreements. We offer partial assurance here. Theoretically, if a firm is influential enough to
affect the subject matter of a CFP, one might expect that it is also influential enough to affect the
governance choice. Which governance form would a firm prefer – the grant, in which it has great
freedom to operate, or a cooperative agreement, in which it operates under the eye of
government personnel? Most theoretical lenses suggest that the firm would prefer the lowermonitoring grant form. This would bias against the results that we find. Empirically, in a
robustness check that includes firm random effects, we find that cooperative agreements still
outperform grants at a level comparable to that of the main results. Although neither of these is
dispositive, it suggests that firm influence/preferences are not driving our results.
Alternatively, if agency personnel prefer to govern research contracts as cooperative
agreements when the projects involve high-status firms or high-upside projects, then again we
might find that cooperative agreement generate more patents because “better” projects are set up
as cooperative agreements. As noted above, our robustness test with firm random effects
suggests that our main results are not driven by better firms. As for better projects, two questions

42

arise. Are better projects routed to cooperative agreements? Why doesn’t the agency permit all
research to be governed by cooperative agreements and credit claim over all innovation if then
funds?
There are a number of reasons why this is an unlikely outcome, and that grants and
cooperative agreements are sorted appropriately based on government research contribution.
First, legislation specifies that cooperative agreements must have a “substantial” contribution by
the government agency. The contribution of the government in research must be specified in
writing in the proposal. This, in turn, requires researchers who are qualified to conduct the
research, which the government may not have. Thus, there are ex ante gates, before the research
begins, to ensure that substantial government cooperation is featured in the research. Second,
although there is no clear criteria in the legislation or regulations specifying how “substantial”
involvement is measured, government officials who merely claim collaboration when such
collaboration is lacking, risk running afoul of the law and becoming subject to severe penalties.
Auditing of researcher time certification, whistleblowing, and inspector general investigations
are all mechanisms by which such illegal behavior would be discovered. Third, in a career
concerns model, government officials are generally risk averse and extremely concerned about
downside outcomes. If a research project unravels and receives substantial negative press, the
researchers in the agency who are supposed to be (but are not really) engaged in the purported
collaborative agreement, will be found equally culpable of research failures as the researchers
who were engaged in the project, subjecting them to lower probabilities of promotion. The latter
two critiques might be remedied if the researchers had time to engage in each research project in
a significant way. However, researchers encounter time constraints to their involvement, as noted
in the paper. In fact, in Table 2, we control for the time effects by including a measure of the

43

number of collaborative agreements to the number of personnel. Overall, it is unlikely that
government researchers would have the incentive to classify projects as cooperative agreements
without their substantial contribution to the direction and content of the work.

Cited:
De Figueiredo JM, Silverman BS. 2007. How Does the Government (Want to) Fund Science?
Politics, Lobbying and Academic Earmarks. In Science and the University, Stephan P,
Ehrenberg RG (eds). University of Wisconsin Press: 36–51.
Hegde D, Mowery DC. 2008. Research Funding: Politics and Funding in the U.S. Public
Biomedical R&D System. Science 322(5909): 1797–1798.

44

APPENDIX B: DATA, METHODS, AND ROBUSTNESS CHECKS

This appendix provides additional details about the data and methods used in this study.
It then provides additional details about a series of robustness checks designed to further test our
predictions and to test alternative explanations. It is divided into three sections: Data, Methods,
and Robustness Checks.

Data
Correlation matrices
The correlation matrices for our sample appear in Table B1.

Missing Project Descriptions
As discussed in the text, 31.2% of funding agreements (i.e., 1,271 of 4,074) lack a match
with any of the 59 knowledge areas identified using OPM job titles. There are two reasons this
may be the case: (1) either there was text in the description field of a project’s record that did not
include any of the keywords used to identify subject matter expertise (24% of missing cases), or
(2) a description was entirely missing (76% of missing cases). When we initially downloaded the
agreement data from USASpending.gov, a considerable number of project descriptions were
missing. To address this, we searched the Federal Procurement Data System (FPDS;
www.fpds.gov) ATOM feed, a searchable Application Programming Interface (API) for
government spending records, for project descriptions based on the federal award ID number
included in the agreement records. We also searched the NIH RePORTER system

45

(https://projectreporter.nih.gov/), a similar system for several other agencies. While this
improved our data coverage, roughly 900 contracts continue to omit descriptions.
In the analyses presented in the paper, we control for the lack of match to any of the 59
areas of expertise by including the dummy variable No Expertise (coded 1 for no match found),
but this does not discriminate between the two sources of non-matching discussed above. To
ensure that the type of non-matching does not materially affect the outcomes reported, we reestimated all models under two alternative specification: (1) including a second dummy variable
to indicate whether or not any text was included in the project description field from USA
spending; and (2) after dropping all cases without text in the description field. Under both
specifications, substantive results remain consistent with those reported in the main paper: for
example, the coefficient on Personnel Expertise Ratio in the first-stage = 0.618 and 0.617 in the
respective models (p < 0.001), and the marginal effect of Cooperative Agreement in the second
stage for contracts that are organized as cooperative agreements = 0.189 and 0.184, respectively
(p < 0.001).

Methods
Explanation of inverse probability weighted regression with adjustment (IPWRA)
Here we outline the general approach to IPWRA.24 Consider each agreement that is
chosen for treatment, ! ∈ {0,1}. The potential outcome of the treatment is denoted as yt. As
researchers, we are interested in three parameters: the mean potential outcome, () = +(-) ); the
average treatment effect, /) = +(-) − -1 ); and the average treatment effect on the treated 2) =

24

A more detailed discussion of the conceptual and mathematical underpinnings can be found in Greene (2012),
Cameron and Trivedi (2009), Imbens and Wooldridge (2009), and Angrist and Krueger (2001)..

46

+(-) − -1 |! = 1).25 To derive these values, we need to implement estimating equations for the
treatment equation and outcome equation. Estimating equations solve systems of equations to
computer the estimates of these parameter values, based on the functional forms for the probit. In
particular, if 4(5, !, 6) ) is the conditional mean for the outcome y conditional of covariates x and
treat level t, then + (7|5, !) = 4(5, !, 6) ) where 6) are the paremeters of the condition mean
model given the treatment model given the treatment t=1. For both of these equations, we use
probit models where the functional form for outcome model (see Stata 2014 Reference Manual).
The estimators are derived through the estimating equations for the treatment model and
outcome model using quasi-maximum likelihood approaches. There are two general approaches
to solving these models. The first is regression adjustment methods. Regression adjustment
estimators estimate the effect parameters using the means of the observation-level predictions of
the condition means on the outcome. The second method, using inverse probability weighting,
develops estimators to determine the effect parameters using the means of the observed
outcomes weighted by their inverse probability of being treated. We incorporate both methods
using the inverse probability weighting with regression adjustment used by Cattaneo (2010) and
Cattaneo et al. (2013).
These types of treatment models have additional attractive properties for our purposes.
First, they allow for different models predicting the treatment and outcome. Second, they are
econometrically identifiable from both functional form and instruments. In our case, we have an
instrument which is predictive of the choice of agreement form, but uncorrelated (except through
agreement form) with the probability of obtaining a patent. Third, these models are “double
robust.” This means that even if one of the treatment or outcome models is not fully specified,

25

The no-treatment level is zero.

47

the estimates are still consistent. Fourth, they allow for different estimates of the variables of
interest in the treated and non-treated group. This would seem to be important as projects which
are selected into cooperative agreements may have different characteristics than those that are
chosen for grant agreements. The effect of each of the variables in the treatment equation may be
different in each circumstance.

Robustness Checks
Alternatives to the IPWRA method
We argued above that the IPWRA method possesses numerous qualities that make it the
most appropriate statistical method for our research question and data. Nevertheless, to
demonstrate that the results are not an artifact of this statistical approach, we present results
using alternative econometric methods. Each model we discuss is successively closer to the type
of estimation procedure our data require. However, each entails tradeoffs that make them
second-best alternatives.
We begin by considering single-stage, “naïve” probit models, both with and without year
fixed-effects, present in Table B2. In both models, Cooperative Agreement is positive as
expected (p = 0.000). These models are straightforward, but risk being misidentified due to
endogeneity between the treatment (agreement form) and outcome (patenting) variables. To
alleviate concerns over endogeneity, we next turn to alternative two-stage estimation methods,
more analogous to the IPWRA method we employ in the paper. These results are presented in
Table B3.
The first two-stage candidate is two-stage least squares (2SLS) with instruments. This
estimation procedure linearizes the probability function in both the first stage and second stage.

48

However, when applied to binary outcome data, this method produces increasingly incorrect
parameter estimates as the probability mass moves away from the center of the probability
distribution (Wooldridge 2002). In our setting, the mean for the patent-creation probability
distribution is 0.13, indicating that the probability mass is beginning to get into the tails of the
probability distribution. Nevertheless, in Table B3, Model 1, we provide estimates for the both
stages of a two stage least squares (2SLS) linear probability model (LPM) estimation. The
coefficient on Coop Agreement becomes indistinguishable from zero. In the first-stage, the
coefficient on Personnel Expertise Ratio remains positive and significant (p = 0.001). We note
that the mean for cooperative agreement is 0.23, substantially closer to the center of the
probability distribution.
A second approach is to use a two-stage probit model with instrumental variables. This
method cannot derive unbiased or consistent point estimation of coefficients except under a set
of very restrictive assumptions (Chesher, 2010). One solution to this problem is to linearize the
first stage of the regression and use instrumental-variables probit for the second stage.26 Of
course, linearizing the first stage is problematic, for the reasons noted above. Nevertheless,
Model 2 in Table B3 presents the results using this estimation procedure, following the threestep approach outlined in Wooldridge (2010) and Adams et al. (2009). The instrument Personnel
Expertise Ratio, employed in a single-stage probit, is positive and significant (p = 0.000) in
predicting cooperative agreement adoption. The effect of Coop Agreement on patent generation
is positive and significant (p = 0.002).
A third statistical approach is to use bivariate probit estimation. The bivariate probit

26

The converse is infeasible; if one uses a probit estimation in the first stage and then OLS in second stage, then the
coefficients in the second stage will be incorrect. This is popularly known as the “forbidden regression” (Angrist and
Pischke, 2009: 109).

49

allows for correlation between the first- and second-stage error terms (Greene, 2012). Moreover,
similar to the IPWRA model, instruments in the first stage lead to a more precisely estimated
coefficient in the second stage. Although the bivariate probit also has several attractive features,
not least of which is that its statistical properties are well understood, it does have some
limitations. The relevant concern in our setting is that it assumes (i.e., forces) the treatment effect
to be equal across both the treated and untreated groups, providing only one set of second stage
coefficients (Lokshin and Sajaia, 2011). Model 3 presents results from a bivariate probit
estimation. Again, Personnel Expertise Ratio is positively related to cooperative agreement
selection, and again Coop Agreement is positively related to patent generation.
A final emerging estimation technique for binary choice models with endogenous binary
regressors with instruments is to use FIML (Lokshin and Sajaia, 2011). This technique relies on
joint normality of the error terms in the treatment and outcome equations. Lokshin and Sajaia
(2011) show that, with good instruments in the first stage, this method produces estimated
coefficients that are very close to the true coefficients in Monte Carlo simulations. This method
also allows for the treatment effects to differ across the treated group and untreated group. Model
4 presents results of a FIML model, known as a “switch probit” in Stata parlance. Again,
Personnel Expertise Ratio is positively related to cooperative agreement selection, and again
Coop Agreement is positively related to patent generation.
Overall, then, three out of four alternative two-stage empirical methods generate results
that are qualitatively similar to those of the IPWRA method. Given that the method that did not
replicate the results is also the least suitable for our data, we conclude that the core results of the
paper are generally robust to alternative estimation procedures.

50

Alternative cutoffs for geographic proximity
Another potential concern regarding our analysis is that our results may depend on the
geographic range we consider when identifying relevant, local government personnel. Four of
our independent variables and our instrumental variable are all based on geographic proximity,
which necessitates an arbitrary decision regarding what distance is “proximate” – a day’s
roundtrip by a government scientist (4 hours of driving). We rely above on a radius of 100 miles
from the focal location of research work as the default distance – that is, the personnel counted as
potentially relevant to the agreement being carried out must work for the sponsoring agency
within 100 miles of the principal worksite indicated in the agreement. Perhaps 100 miles is an
overly stringent or optimistic threshold for collaborative work on a contract-research project. We
therefore re-estimate our primary model, the two-stage IPWRA probit reported in Models 2A
and 2B of Table 2 (main paper), using variables based on thresholds of 200, 300, 400, and 500
miles. The coefficients on Coop Agreement are presented in Table B4, along with the first-stage
coefficients on our instrumental variable in the selection model.
In every model, the coefficient on Coop Agreement remains positive and significant (p =
0.000 in all models). The magnitudes of the coefficients remain relatively consistent, and close to
the increased probabilities of 27% for cooperative agreements and 8% for grants reported above.
The coefficient on the instrument, Personnel Expertise Ratio, retains similar magnitude
throughout all models. In unreported models, we also re-estimated the non-instrumented singlestage probit model reported in Model 2 for 200-, 300-, 400-, and 500-mile specifications. Our
core result is robust against geographic manipulation in multiple model specification methods.

51

Appropriateness of instrumental variable
To be a valid instrument, Personnel Expertise Ratio must be correlated with the
endogenous regressor, Coop Agreement, and orthogonal to the error term in the main equation.
Table B5, Model 1 reports a single-stage probit model predicting patent generation with
coefficients converted to marginal effects, which is identical to Model 2 in Table B2 except that
it includes the instrument as an independent variable. The coefficients on all of the variables
common to the two models are little. Of particular importance, though, is that Personnel
Expertise Ratio exhibits no direct relationship with Generates Patent. Indeed, the BIC for Table
B5, Model 1 is slightly larger than the BIC for Table B2, Model 2, showing that including the
personnel expertise variable worsens model fit rather than improving it. In every two-stage
model specification reported in this paper, Personnel Expertise Ratio continues to be a positive
and significant predictor of Coop Agreement (p < 0.003 in all models).
As discussed in the main paper, we elected to employ inverse-propensity weighted
regression adjustment (IPWRA) in our primary analysis to account for covariate imbalance.
IPWRA is a “doubly robust” estimation method, in that it gives “the analyst two chances to ‘get
it right’” (Morgan and Winship, 2015: 234). Furthermore, on the chance that agreement type
(cooperative versus grant) is endogenous to our main outcome variable (patent generation), we
include an instrument for selection into the cooperative agreement format – a continuous
measure of locally available government personnel in the relevant bureau with the relevant
expertise. Nevertheless, one might wonder what if any of our first-stage independent variables
are correlated with this instrument. Based on the correlations presented in Table B1, three righthand side variables are correlated above 0.3 with our instrument: No Expertise, Coops-toPersonnel Ratio, and Early Stage Personnel.

52

As long as the instrument (Personnel Expertise Ratio) is not correlated with the errors of
our final outcome of concern (patent generation), it remains a valid instrument, known as a
“conditional instrumental variable” (Morgan and Winship, 2015: 298–299) in the presence of
correlation with other first-stage variables. The minimal correlation between the variables and
the lack relationship between Personnel Expertise Ratio and Generates Patent in Table B5
indicates this assumption is valid.
However, we can model the data under the assumption of endogenous regressors for the
instrument in addition to a potentially endogenous treatment variable using an extended
regression models (ERMs). ERMs allow for multiple, simultaneous equations to be estimated
accounting for both endogenous treatment assignment and endogenous predictors of an
instrumental variable (Wooldridge, 2010). We fit a three-equation model using the same
functional setup as Table 2 in the main paper, but include an additional equation specifying that
Personnel Expertise Ratio be regressed on No Expertise, Coops-to-Personnel Ratio, and Early
Stage Personnel under the assumption of endogeneity; heteroscedasticity-robust standard errors
are included as an additional precaution. Results are reported in Table B6.
Functionally, the results do not change. First, the instrument remains positive in
predicting treatment assignment (p = 0.000). Second, the treatment (cooperative agreement form)
remains a positive predictor of patent generation (p = 0.000). Furthermore, using Stata’s postestimation commands, we can estimate that the average treatment effect on the treated (ATET) is
a 28.2% increase in the likelihood of patent generation (p < .001), which is slightly larger than
the effect reported in Table 2 (main paper). Finally, the ERM method also evaluates error
correlations between dependent variables; important for our concerns is that the errors for

53

personnel expertise are uncorrelated with patent generation, further supporting our use of this
variable as an instrument.

Temporal Variance
Although there are multiple paths to analyzing the data, we opt for a strategy that
minimizes variation between observations (via inverse-probability weighting in the first stage of
our two-stage models) in order to achieve covariate balance between cooperative agreements and
grants. An alternative is to examine within-firm and/or time-period specific changes. We do this
in part in our current analyses by including fiscal year fixed effects in all primary analyses.
These year fixed effects capture variance due to government-wide or economy-wide events due
to the time period, such as the economic recession and government response in 2008-2009. In the
first-stage Model 1 (Table 2, main paper), the yearly coefficients are indeed negative during FYs
2008-2010, suggesting the government was more likely to issue grants over cooperative
agreements (which was the case throughout, shown by the higher proportion of agreements that
were grants).
However, a major initiative of the federal government during this time was to stimulate
spending via the American Recovery and Reinvestment Act (ARRA). This stimulus package was
distributed over multiple fiscal years, and so might have effects not captured by year dummy
variables, in particular in the choice of issuing grants rather than cooperative agreements (i.e., the
ARRA could be an omitted variable in our first-stage, treatment-assignment model). While this
is not a concern as long as our covariate balance estimates are valid or as long as our
instrumental variable is valid, it is possible to determine if the ARRA had an effect in our data,
as agreement records indicate whether or not they were funding via the ARRA specifically. In

54

our sample, 291 agreements (7.14%) were flagged as stemming from the ARRA. Of those, 210
(72.16%) were cooperative agreements, rather than grants. Including a dummy variable (coded 1
if an agreement was supported by the ARRA) in the first and second stage models does not
materially affect any of our results, and is not statistically significant at a = 0.05 in predicting
agreement form (b = -0.239).

Bureau-Level Differences
Another concern that may arise is that governmental units (“bureaus” in our discussion,
which correspond to the level of government just below agencies; e.g., the Centers for Disease
Control and Prevention is a bureau of the Health and Human Services agency) may influence
results in a manner not adequately captured without bureau fixed-effects. We are unable to use
bureau fixed-effects in our analysis because it introduces too much missing data. In the treatment
effects setup, models are fit separately (but relatedly) to the treatment and control conditions, so
if a bureau does not have sufficient coverage on all variables in both conditions, we do not
observe a sufficient number of both treatment and control cases in for each variable in our model
to accurately estimate a treatment effect under this specification. This is, fundamentally, the
problem of causal inference caused by missing data in non-experimental studies (Rosenbaum and
Rubin, 1983).
One method for addressing causal concerns in the presence of missing and unbalanced
data is entropy balancing (Hainmueller, 2012; Hainmueller and Xu, 2013). Entropy balancing
overcomes observational differences in a manner similar to propensity score weighting, but with
increased flexibility and greater use of information. Important for our concerns, “Since the
entropy balancing weights vary smoothly across units, they also commonly retain more

55

information in the preprocessed data than other approaches” (Hainmueller and Xu, 2013: 2),
which is indeed the case in our analysis.
We estimate entropy-balancing weights based on the first-stage covariates reported in
(main paper) Table 3, with the addition of indicators for each agency. This accounts for not only
the first-stage variables’ differences across grants and cooperative agreements, but also the
different likelihood of any bureau to use one form of support over the other. We lose 133 cases
for which entropy balancing was not possible (N = 3,941). Rather than estimating a two-stage
model, we then estimate a single-stage probit including these weights as probability weights.
Table D6 reports the results of this estimation, including the Personnel Expertise Ratio
instrument.
There are several takeaways from Table B7. First, as in the paper, the coefficient for
cooperative agreements is positive and significant (p = 0.000), indicating that cooperative
agreement structure does enhance the likelihood of patent generation for those that were
cooperative agreements. The difference in predicted outcomes by support structure is a 21.5%
increase in the marginal likelihood that an agreement generates a patent, holding all other
variables at their mean value (this effect is equivalent to the average treatment effect, computed
using Stata’s ‘margins’ command). Furthermore, our instrument is not distinguishable from zero
in this model, which is equivalent to a second-stage regression in the two-stage least-squares
framework. This lends further support that it does not have a relationship to the final outcome, a
key concern in using it as an instrument.

Skew in patent and citation measures

56

It is well known that patenting and citation rates often are highly skewed. Our dependent
variable Generates Patent addresses this because it is a binary measure of at-least-one-patent.
Nevertheless, to explore whether our results for the other dependent variables, which are based
on patent and citation counts, are driven by skew, in Table B8 we re-estimate the basic models
using the natural logs of these dependent variables. The results are materially unchanged.

Heterogeneity in Scientist Coordination Across Grants and Cooperative Agreements
As noted above, one alternative explanation for our predicted pattern of results is that
contract choice and innovative performance are both driven by government scientists’ ability to
collaborate with firm scientists. Specifically, when government personnel with relevant skills are
locally available, project tasks can entail greater collaboration; since federal policies stipulate
that high-collaboration endeavors be managed under cooperative-agreement contracts, then if
collaboration yields greater innovative output, cooperative agreements will be correlated with
innovative output as an artifact of collaborative activity. Although we cannot identify the specific
degree of collaboration that occurs in each research project, Coops-to-Personnel Ratio proxies
for the feasibility of collaboration by measuring the other demands on government researchers’
attention. The positive relationship between Coop Agreement and Generates Patent is unaffected
by inclusion of this proxy, thus indicating a salutary effect of cooperative-agreement governance
on innovative output beyond mere coordination in a project.

Firm Differences
A final concern is that unobserved heterogeneity among firms may correlate with
performance of the contracts. We are unable to obtain convergence using firm fixed effects with

57

either IPWRA or entropy balancing due to sparseness of the data. We thus turn to random-effects
estimation, as this relaxes the model assumptions and makes fuller use of the data. A single-stage
random-effects model predicting patent generation is presented in Table B9. As in our primary
analysis, cooperative agreement increases the likelihood of patenting.
An alternative form of time-varying heterogeneity across firms relates to firm experience
in contracting with the government. In Table B10 we re-estimate the probability that a contract is
governed as a cooperative agreement, controlling for the number of prior contracts the firm had
with the specific agency funding the focal contract (Model 1) or any agency (Model 2). We
further disaggregate this into prior grants and prior cooperative agreements in Models 3 and 4.
Models 1 and 2 indicate that contracts are more likely to be organized as grants the greater
number of prior contracts that a firm has had with the government. Models 3 and 4 show that a
firm’s current contract mode is likely to be similar to the mode of its prior contracts, suggesting
perhaps that there is a class of firm that is more likely to be awarded grants and another that is
more likely to be awarded cooperative agreements, or perhaps that agencies favor a modal form
that firm has experienced before. Of particular note, the inclusion of these variables does not
qualitatively change the magnitude of Personnel Expertise Ratio, and reduces the magnitude of
Early Stage Project by up to one-half in some models.

58

References in Appendix B
Adams R, Almeida H, Ferreira D. 2009. Understanding the relationship between founder–CEOs
and firm performance. Journal of Empirical Finance 16(1): 136–150.
Angrist JD, Krueger AB. 2001. Instrumental Variables and the Search for Identification: From
Supply and Demand to Natural Experiments. The Journal of Economic Perspectives
15(4): 69–85.
Angrist JD, Pischke J-S. 2009. Mostly harmless econometrics: an empiricist’s companion.
Princeton University Press: Princeton.
Cameron AC, Trivedi PK. 2009. Microeconometrics using Stata. Stata Press: College Station,
Texas.
Cattaneo MD. 2010. Efficient Semiparametric Estimation of Multi-valued Treatment Effects
Under Ignorability. Journal of Econometrics 155(2): 138–154.
Cattaneo MD, Drukker DM, Holland AD. 2013. Estimation of multivalued treatment effects
under conditional independence. Stata Journal 13(3): 407–450.
Chesher A. 2010. Instrumental Varaible Models for Discrete Outcomes. Econometrica 78(2):
575–601.
Greene WH. 2012. Econometric analysis, 7th ed. Prentice Hall: Boston.
Hainmueller J. 2012. Entropy Balancing for Causal Effects: A Multivariate Reweighting Method
to Produce Balanced Samples in Observational Studies. Political Analysis 20(1): 25–46.
Hainmueller J, Xu Y. 2013. Ebalance: A Stata Package for Entropy Balancing. Journal of
Statistical Software 54(7): 1–18.
Imbens GW, Wooldridge JM. 2009. Recent Developments in the Econometrics of Program
Evaluation. Journal of Economic Literature 47(1): 5–86.
Lokshin M, Sajaia Z. 2011. Impact of interventions on discrete outcomes: Maximum likelihood
estimation of the binary choice models with binary endogenous regressors. Stata Journal
11(3): 368–385.
Morgan SL, Winship C. 2015. Counterfactuals and Causal Inference: Methods and Principles
for Social Research, 2nd ed. Cambridge University Press: New York.
Rosenbaum PR, Rubin DB. 1983. The Central Role of the Propensity Score in Observational
Studies for Causal Effects. Biometrika 70(1): 41–55.
Wooldridge JM. 2010. Econometric analysis of cross section and panel data, 2nd ed. MIT Press:
Cambridge, MA.

59

Table B1: Correlation Matrices
Panel A: Spearman Correlation Matrix – All Agreements
Variable
(1)
(2)
(3)
1. Generates Patent
1.000
2. Coop Agreement
0.329
1.000
3. Personnel Expertise Ratio
0.043
0.180
1.000
a
4. Federal Funding
0.179
0.189
-0.035
a
5. Firm/Total Funding
0.216
0.495
0.085
a
6. Prior Year Patents
0.102
0.390
0.121
7. Large Firm
0.277
0.587
0.123
8. No Expertise
-0.171
-0.265
-0.358
9. Coops/Personnel Ratio
0.074
0.298
0.556
10. Early Stage Personnel
0.055
0.164
0.502
11. Coops within 100 Miles
0.151
0.416
0.261

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

1.000
0.157
0.046
0.131
-0.013
-0.053
0.007
-0.021

1.000
0.300
0.462
-0.137
0.147
0.083
0.271

1.000
0.551
-0.244
0.184
0.083
0.289

1.000
-0.248
0.271
0.106
0.346

1.000
-0.242
-0.235
-0.386

1.000
0.284
0.606

1.000
0.192

1.000

(6)

(7)

(8)

(9)

(10)

(11)

1.000
0.320
0.021
0.039
-0.010
0.310

1.000
-0.061
0.048
-0.201
0.111

1.000
0.039
-0.153
0.018

1.000
0.043
0.357

1.000
0.110

1.000

Panel B: Spearman Correlation Matrix – Cooperative Agreements Only
Variable
(1)
(2)
(3)
(4)
(5)
1. Generates Patent
1.000
2. Coop Agreement
.
.
3. Personnel Expertise Ratio
-0.102
.
1.000
a
4. Federal Funding
0.229
.
-0.132
1.000
a
5. Firm/Total Funding
0.035
.
-0.028
0.054
1.000
a
6. Prior Year Patents
-0.148
.
-0.022
-0.139
0.100
7. Large Firm
0.133
.
-0.082
0.032
0.196
8. No Expertise
-0.010
.
-0.278
-0.090
-0.046
9. Coops/Personnel Ratio
-0.153
.
0.229
-0.143
-0.077
10. Early Stage Personnel
-0.118
.
0.537
-0.051
-0.049
11. Coops within 100 Miles
-0.096
.
0.031
-0.105
0.042

60

Table B1: Correlation Matrices (continued)
Spearman Correlation Matrix – Grants Only
Variable
(1)
(2)
1. Generates Patent
1.000
2. Coop Agreement
.
.
3. Personnel Expertise Ratio
0.087
.
4. Federal Fundinga
0.065
.
5. Firm/Total Fundinga
0.063
.
6. Prior Year Patentsa
0.033
.
7. Large Firm
0.096
.
8. No Expertise
-0.134
.
9. Coops/Personnel Ratio
0.088
.
10. Early Stage Personnel
0.091
.
11. Coops within 100 Miles
0.130
.

a

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

1.000
-0.040
0.052
0.091
0.109
-0.545
0.469
0.495
0.300

1.000
0.013
0.021
0.001
0.072
-0.095
-0.002
-0.114

1.000
0.137
0.301
0.019
0.100
0.063
0.137

1.000
0.426
-0.189
0.153
0.031
0.182

1.000
-0.138
0.145
0.129
0.174

1.000
-0.357
-0.230
-0.413

1.000
0.185
0.791

1.000
0.190

1.000

z-score standardized

61

Table B2: Single-Stage Probit Estimation
Variable

Model 1

Cooperative Agreement

Model 2

0.705**
0.763**
(0.075)
(0.075)
Federal Funding
0.069**
0.061*
(0.026)
(0.026)
Firm/Total Funding
0.009
0.033
(0.026)
(0.027)
Prior Year Patents
-0.075*
-0.064*
(0.031)
(0.028)
Coops/Personnel Ratio
-0.051**
-0.054**
(0.019)
(0.019)
Coops within 100 Miles
0.005
-0.002
(0.003)
(0.003)
Early Stage Personnel
0.242
0.375
(0.377)
(0.378)
Large Firm
0.445**
0.462**
(0.075)
(0.077)
No Expertise
-0.472**
-0.254**
(0.075)
(0.095)
Fiscal Year Fixed Effects
NO
YES
Observations
4074
4074
BIC
2662.019
2602.842
Pseudo R2
0.159
0.210
Note. Heteroskedasticity-robust standard errors are reported in
parentheses.
* p < .05, ** p < .01

62

Table B3. Methodological Robustness Checks: Comparison of Two-Stage Estimation Procedures

Variable
Personnel Expertise Ratio
(Instrument)
Federal Funding
Firm/Total Funding
Prior Year Patents
Coops/Personnel Ratio
Coops within 100 Miles
Early Stage Personnel
Large Firm
No Expertise
Fiscal Year Fixed Effects

Coop Agreement
Federal Funding
Firm/Total Funding
Prior Year Patents
Coops/Personnel Ratio
Coops within 100 Miles

Model 1
2SLS
0.160**
(0.047)
0.016
(0.012)
0.089**
(0.008)
-0.002
(0.008)
-0.001
(0.004)
0.006**
(0.001)
0.186**
(0.067)
0.362**
(0.021)
-0.128**
(0.017)
YES

Model 2
2S/3S IV
0.113**
(0.029)
0.007
(0.006)
0.050**
(0.005)
0.001
(0.004)
-0.001
(0.002)
0.003**
(0.001)
0.175**
(0.048)
0.324**
(0.021)
-0.111**
(0.014)
YES

Model 1
2SLS

Model 2
2S/3S IV

-0.118
(0.258)
0.026**
(0.010)
0.035
(0.025)
-0.017**
(0.006)
-0.014**
(0.004)
0.001
(0.002)

1.726**
(0.545)
0.043
(0.029)
-0.057
(0.060)
-0.062*
(0.028)
-0.051*
(0.020)
-0.008
(0.005)
63

First-Stage Models
Model 3
Bi-probit
0.673**
(0.163)
0.041
(0.035)
0.277**
(0.030)
0.008
(0.025)
-0.002
(0.014)
0.017**
(0.003)
1.033**
(0.266)
1.272**
(0.068)
-0.663**
(0.088)
YES

Model 4
Switch Probit
0.656**
(0.175)
0.034
(0.034)
0.289**
(0.032)
-0.001
(0.025)
-0.007
(0.014)
0.020**
(0.003)
0.792**
(0.256)
1.280**
(0.066)
-0.634**
(0.083)
NO

Second-Stage Models
Model 4A
Model 4B
Model 3
Switch Probit Switch Probit
Bi-probit
Coops
Grants
0.240**
0.281**
0.081**
a
b
(0.142)
(0.079)
(0.074)b
0.069*
0.070
0.059
(0.028)
(0.038)
(0.037)
0.095**
-0.035
0.057
(0.026)
(0.074)
(0.075)
-0.060*
-0.055
-0.090
(0.027)
(0.036)
(0.055)
-0.053**
-0.067**
-0.014
(0.018)
(0.029)
(0.026)
0.002
0.001
0.005
(0.003)
(0.006)
(0.005)

Early Stage Personnel

0.079
0.145
0.541
-2.039
0.772*
(0.099)
(0.405)
(0.364)
(1.101)
(0.373)
Large Firm
0.213*
0.067
0.745**
0.371
0.201
(0.096)
(0.255)
(0.071)
(0.439)
(0.191)
No Expertise
-0.053
-0.077
-0.391**
0.003
-0.260*
(0.041)
(0.149)
(0.093)
(0.268)
(0.128)
Year Fixed Effects
YES
YES
YES
NO
NO
Observations
4,074
4,074
4,074
4,074
Note. Heteroskedasticity-robust standard errors are reported in parentheses.
a
t-statistic of difference in predicted probability of patent generation by agreement type
b
t-statistic of difference in estimated treatment effect by agreement type
* p < .05, ** p < .01

64

Table B4. Two-Stage IPWRA Probit Results, Sensitivity to Different Thresholds for Geographic Proximity
(dependent variable: Generates Patent)
100 Miles

Coop Agreement

Co-ops
0.278**
(0.032)

Grants
0.083**
(0.006)

200 Miles
Co-ops
0.269**
(0.031)

300 Miles

Grants
0.085**
(0.006)

Co-ops
0.272**
(0.029)

Grants
0.083**
(0.006)

400 Miles
Co-ops
0.273**
(0.029)

Grants
0.084**
(0.006)

500 Miles
Co-ops
0.264**
(0.028)

Grants
0.085**
(0.007)

First-stage Personnel
0.651**
0.471**
0.346**
0.572**
0.639**
Expertise Instrument
(0.165)
(0.137)
(0.123)
(0.109)
(0.109)
Note. Heteroskedasticity-robust standard errors are reported in parentheses. All independent variables from Table 2 are included in
estimation, but coefficients are not reported.
** p < .01

65

Table B5. Single-Stage Probit Estimation of Patent
Generation, With Instrument as Independent Variable
Variable

Model 1

Coop Agreement

0.125**
(0.012)
Personnel Expertise Ratio
-0.030
(0.028)
Federal Funding
0.010*
(0.004)
Firm/Total Funding
0.005
(0.004)
Prior Year Patents
-0.010*
(0.005)
Coops/Personnel Ratio
-0.009**
(0.003)
Coops within 100 Miles
-0.000
(0.001)
Early Stage Personnel
0.065
(0.061)
Large Firm
0.083**
(0.015)
No Expertise
-0.041**
(0.013)
Year Fixed Effects
Yes
N
4,074
2
Pseudo R
0.211
BIC
2,610
Note. Heteroskedasticity-robust standard errors are
reported in parentheses.
* p < .05, ** p < .01

66

Table B6: Three-Equation Endogenous Treatment Extended Regression Model

Treatment Assignment Model
(Predicting Cooperative
Agreement = 1)

Final-Stage Model
(Predicting Patent
Generation = 1)

Model

Variable
Federal Funding
Firm/Total Funding
Prior Year Patents
Coops/Personnel Ratio
Coops within 100 Miles
Early Stage Personnel
Large Firm
No Expertise
Coop Agreement
Personnel Expertise Ratio
Federal Funding
Firm/Total Funding
Prior Year Patents
Coops/Personnel Ratio
Coops within 100 Miles
Early Stage Personnel
Large Firm
No Expertise

Predicting
Personnel
Ratio IV

Early Stage Personnel
Coops/Personnel Ratio
No Expertise
Constant
Note. N = 4,074

Coefficient
0.050
-0.018
-0.061
-0.054
-0.005
0.253
0.241
-0.154
1.330

Robust SE
0.026
0.035
0.028
0.019
0.004
0.374
0.120
0.103
0.253

z-statistic
1.93
-0.51
-2.19
-2.79
-1.54
0.68
2.01
-1.50
5.26

p-value
0.054
0.610
0.029
0.005
0.123
0.498
0.045
0.134
0.000

0.735
0.036
0.291
0.002
-0.004
0.019
0.940
1.267
-0.650

0.233
0.035
0.033
0.024
0.014
0.003
0.280
0.068
0.093

3.15
1.04
8.96
0.08
-0.28
5.74
3.36
18.66
-7.03

0.002
0.300
0.000
0.937
0.777
0.000
0.001
0.000
0.000

0.147
0.003
-0.082
0.082

0.049
0.001
0.004
0.004

2.98
2.84
-22.68
22.34

0.003
0.005
0.000
0.000

67

Table B7: Entropy-Balanced Probit Estimation Predicting Patent Generation
Variable
Coefficient Robust SE z-statistic
p-value
Coop Agreement
0.215
0.029
7.39
0.000
Personnel Expertise Ratio
0.029
0.071
0.41
0.684
Federal Funding
0.019
0.005
4.11
0.000
Firm/Total Funding
0.031
0.012
2.63
0.008
Prior Year Patents
-0.023
0.015
-1.52
0.129
Coops/Personnel Ratio
-0.015
0.008
-1.92
0.055
Coops within 100 Miles
0.002
0.001
1.86
0.063
Early Stage Personnel
0.177
0.139
1.28
0.202
Large Firm
0.004
0.045
0.09
0.930
No Expertise
-0.048
0.038
-1.24
0.213
Note. Year fixed-effects included. N = 3,941

68

Table B8. Second-Stage IPWRA Results of Logged Patent Generation and Quality Outcomes
Num. Patents
Generated

Citation-Weighted
Patents

Variable
Cooperative Agreement

Citations/Patent

Citations//Year

Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7 Model 8
0.293** 0.079** 0.459** 0.086** 0.352** 0.070** 0.082** 0.017**
(0.032)
(0.007)
(0.044)
(0.011)
(0.037)
(0.008)
(0.010)
(0.002)
Note. Heteroskedasticity-robust standard errors are reported in parentheses. Odd-number models report point estimates
for cooperative agreement analyses; even-number models report point estimates for grant analyses.
** p < .01

69

Table B9:
Single-Stage Probit Predicting Patent Generation with Firm Random-Effects
Variable
Coefficient Robust SE z-statistic
p-value
Coop Agreement
0.192
0.020
9.46
0.000
Fed. Funding
0.016
0.006
2.56
0.011
Firm-to-Total Funding
0.001
0.007
0.19
0.851
Prior Year Patents
-0.027
0.012
-2.21
0.027
Coops-to-Personnel Ratio
-0.004
0.005
-0.77
0.440
Coops within 100 Miles
0.000
0.001
0.16
0.873
Early Stage Personnel
0.075
0.123
0.61
0.539
No Expertise
-0.067
0.025
-2.67
0.008
Note. Year fixed-effects included. 4,074 observations/383 groups. Coefficients are
predicted marginal effects.

70

Table B10:
Probit Estimation of Cooperative Agreement = 1 with Prior Agreement Counts

Personnel Expertise Ratio
Fed. Funding
Firm-to-Total Funding
Prior Year Patents
Research Personnel Prop
Coops-to-Personnel Ratio
Coops within 100 Miles
Early Stage Personnel
Major Organization
No Expertise
Prior 1-Year Agency Contracts

Model 1

Model 2

Model 3

Model 4

0.642**
(0.158)
0.037
(0.020)
0.255**
(0.027)
0.006
(0.025)
-0.133
(0.255)
0.005
(0.013)
0.018**
(0.003)
0.870*
(0.341)
1.082**
(0.073)
-0.457**
(0.089)
-0.013**
(0.002)

0.636**
(0.159)
0.037*
(0.020)
0.254**
(0.027)
0.006
(0.025)
-0.137
(0.255)
0.006
(0.013)
0.018**
(0.003)
0.853*
(0.341)
1.075**
(0.073)
-0.457**
(0.089)

0.496**
(0.162)
0.040*
(0.020)
0.237**
(0.027)
-0.026
(0.025)
-0.091
(0.256)
-0.021
(0.013)
0.009**
(0.003)
0.574
(0.366)
1.081**
(0.074)
-0.463**
(0.091)

0.501**
(0.162)
0.039*
(0.020)
0.236**
(0.027)
-0.026
(0.025)
-0.092
(0.256)
-0.021
(0.013)
0.010**
(0.003)
0.562
(0.365)
1.064**
(0.074)
-0.461**
(0.091)

Prior 1-Year Contracts (Any Agency)

-0.013**
(0.003)

Prior 1-Year Agency Grants

-0.026**
(0.004)
0.103**
(0.013)

Prior 1-Year Agency Coops
Prior 1-Year Grants (Any Agency)

-0.026**
(0.004)
Prior 1-Year Coops (Any Agency)
0.100**
(0.013)
Year Fixed Effects
Yes
Yes
Yes
Yes
N = 3,724; only cases from 2001 onward used. Heteroskedasticity-robust standard errors are
reported in parentheses. Results are substantively unchanged by using a longer time window.
* p < .05, ** p < .01

71

