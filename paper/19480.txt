NBER WORKING PAPER SERIES

SMALL DIFFERENCES THAT MATTER:
MISTAKES IN APPLYING TO COLLEGE
Amanda Pallais
Working Paper 19480
http://www.nber.org/papers/w19480

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2013

I would like to thank Josh Angrist, David Autor, Esther Duflo, Sue Dynarski, Amy Finkelstein, Maria
Fitzpatrick, Michael Greenstone, Jonathan Goldberg, Jerry Hausman, Lisa Kahn, Lawrence Katz, Alan
Manning, Whitney Newey, Jesse Rothstein, Chris Smith, Christopher Taber, Sarah Turner, and participants
at MIT's labor lunch and NBER's Higher Education Working Group meeting for their many helpful
comments and suggestions. I am also grateful to Jesse Rothstein, Princeton University, James Maxey,
Julie Noble, and the ACT Corporation for allowing me access to the ACT database, and Caroline Hoxby
for help in accessing the American College Survey data. Financial support from the National Science
Foundation and the George and Obie Shultz Fund is gratefully acknowledged. The views expressed
herein are those of the author and do not necessarily reflect the views of the National Bureau of Economic
Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2013 by Amanda Pallais. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.

Small Differences that Matter: Mistakes in Applying to College
Amanda Pallais
NBER Working Paper No. 19480
September 2013
JEL No. I21,I23,I24,J24
ABSTRACT
This paper estimates the sensitivity of students' college application decisions to a small change in the
cost of sending standardized test scores to colleges. Using confidential ACT micro data, I find that
when the ACT increased from three to four the number of free score reports that ACT-takers could
send, the fraction of test-takers sending four reports rose substantially while the fraction sending three
fell by an offsetting amount. Students simultaneously sent their scores to a wider range of colleges.
Using micro data from the American Freshman Survey, two identification strategies show that ACT-takers
sent more college applications and low-income ACT-takers attended more selective colleges after
the cost change. The first strategy compares ACT-takers before and after the cost change, controlling
for time trends and covariates, and the second estimates difference-in-difference regressions using
SAT-takers as a control group. Back-of-the-envelope calculations suggest that by inducing low-income
students to attend more selective colleges, the policy change significantly increased their expected
earnings. Because the cost of sending an additional (non-free) ACT score was merely $6 throughout,
this sizable behavioral change is surprising and suggests that students may use simple heuristics in
making their application decisions. In such a setting, small policy perturbations can have large effects
on welfare.

Amanda Pallais
Department of Economics
Harvard University
Littauer Center
Cambridge, MA 02138
and NBER
apallais@fas.harvard.edu

1

Introduction

Where a student applies to college greatly a¤ects both whether she attends college and the
type of college she attends. Yet, little is known about how students decide where to apply.
An expanding literature suggests that students’application decisions, particularly those
of low-income students, may be suboptimal. Low-income students are less likely to attend
college than are their higher-income peers, conditional on high school achievement (e.g.,
Ellwood and Kane, 2000). Conditional on high school achievement, they are also less likely
to attend selective colleges (e.g., Hill et al., 2005; Winston and Hill, 2005). This is troubling
as Card (1995) …nds that the return to a year of college is particularly large for disadvantaged
students and the literature suggests that low-income students have larger returns to attending
selective colleges.1
Much of this underrepresentation may result from low-income students’application choices.
Low-income students are less likely to apply to selective colleges than are their higher-income
peers, but, conditional on applying, are no less likely to be admitted or matriculate (Bowen
et al., 2005; Pallais and Turner, 2006; Spies, 2001; and Hoxby and Avery, 2012). Additionally, a number of recent papers …nd that providing students with information about
colleges or assistance with the college application process change students’ college matriculation outcomes, particularly those of low-income students. Bettinger et al. (2012) …nd
that …lling out …nancial aid forms for students increased their matriculation rates. Carrell
and Sacerdote (2013) …nd that giving high school students college counseling guidance and
application fee waivers increased college matriculation, particularly for students attending
disadvantaged high schools. Hoxby and Turner (2013) …nd that sending high-achieving,
low-income students application fee waivers and information about colleges and optimal
application strategies induced them to attend more-selective colleges.
This paper shows that students are particularly responsive to a $6 decrease in the cost
of sending standardized test scores to colleges. Before the fall of 1997, students taking the
ACT, a popular college entrance exam, could send their test scores to three colleges for free
while each additional score report cost $6. Afterwards, students could send four score reports
for free with the same $6 cost for each report beyond four. I …nd that, in response to this $6
1

Many studies have found a large return to college quality for students of all income levels (e.g., Hoxby,
1998; Zhang 2005; Brewer et al., 1999; and Black and Smith, 2006). There is no consensus in this literature,
however, because Dale and Krueger (2002) …nd there is no return to college selectivity for most students
when they compare the earnings of students who were admitted to the same colleges, but chose to attend
di¤erent ones. Yet, Dale and Krueger do …nd large returns to college selectivity for low-income students.
Many other studies (e.g., Saavedra, 2008; Monks, 2000; Behrman et al., 1996; and Loury and Garman,
1995) …nd that low-income students and minorities receive particularly high returns from attending selective
colleges.

2

cost change, both high- and low-income students sent substantially more score reports and
applications and low-income students attended more selective colleges.
Figure 1 uses data from the ACT and SAT (a competing college entrance exam) to show
the fraction of di¤erent high school classes that sent exactly three and exactly four score
reports.2 ACT-takers graduating high school before 1998 were eligible for three free score
reports, those graduating after 1998 were eligible for four, and those in the class of 1998
received three if they took the ACT in their junior year and four if they took the test as
seniors. SAT-takers received four free score reports throughout the period. The …gure shows
that between the classes of 1996 and 2000, the fraction of ACT-takers sending exactly four
score reports jumped from 3% to 74%, while the fraction sending exactly three fell from 82%
to 10%. In contrast, SAT-takers experienced relatively small changes in their score-sending
patterns. Micro data from the ACT con…rm that this large increase in score-sending was
not driven by changes in the pool of test-takers.
Sending additional score reports bene…tted students only if they also sent additional
applications. I use the American Freshman Survey (AFS), a survey of college freshmen, and
two identi…cation strategies to directly evaluate the e¤ect of the cost change on the number of
applications students sent. First, I look only at students who took the ACT before and after
the cost change. Then, I use a di¤erence-in-di¤erence methodology, comparing ACT-takers
to SAT-takers. Both identi…cation strategies show that ACT-takers sent more applications
after the cost change, though the increase in applications was much smaller than the increase
in score-sending.
When students gained access to the fourth free report, they widened the range of colleges
to which they sent scores. Some students sent scores to colleges that were more selective than
any they would have sent scores to otherwise, giving the students an additional opportunity
to attend a more-selective college. Other students sent scores to less-selective colleges with
higher admission rates, giving them another chance to be admitted to any college. Using the
AFS data (and both identi…cation strategies), I …nd that, after the cost change, low-income
students attended more-selective colleges, while higher-income students did not.
I do not observe how the cost change a¤ected low-income students’ earnings. However, a back-of-the-envelope calculation suggests that by increasing the probability that a
low-income student attended a more-selective college, sending an additional score report increased her expected future earnings by over $10,000. A similar calculation estimates that
2

The ACT data used here come from a database compiled by Jesse Rothstein for other projects. The
dataset covers about half the years from 1991 to 2004. I did not choose these years; they were chosen for
another project and the …gure displays data from all the years to which I have access. The SAT data come
from a similar dataset. Jesse Rothstein provided the tabulations for Figure 1b as I do not have access to the
SAT micro data. Both graphs are limited to students sending at least one score report.

3

even if only one out of every 29,000 low-income ACT-takers who sent an additional score
report was induced to attend two years of college, the bene…ts of sending an additional report
for the average low-income student through this channel would exceed $6.
In the paper’s conclusion I consider explanations for students’large reaction to the cost
change. It seems unlikely that it could be optimal for so many students to change their
behavior as a result of so small a change. However, deciding on the optimal portfolio of
colleges to apply to is a di¢ cult problem, which depends on many parameters students
may not know. Instead, may students use rules of thumb to determine which colleges to
apply to. They may interpret the ACT’s providing three (or four) free score reports as
an indication that sending that many reports is recommended. When the cost structure
changed, so did their rule of thumb. In this way, this paper’s …ndings are complementary to
the …ndings in Madrian and Shea (2001), Choi et al. (2002), and Thaler and Sunstein (2008)
that individuals are strongly a¤ected by default choices when choosing among savings and
health insurance plans. If this is the case, providing students with information on optimal
application strategies instead of having them deduce rules of thumb from external sources
could induce low-income students to attend more-selective colleges, potentially facilitating
better student-college matches.
The paper proceeds as follows: Section 2 discusses the policy change and the datasets
used. Section 3 uses ACT micro data to determine the e¤ect of the cost change on the
number and selectivity of colleges to which students sent scores. Section 4 uses the AFS data
to analyze the e¤ect of the cost change on students’application behavior and the selectivity
of the colleges they attended. Section 5 benchmarks the bene…ts low-income students might
receive from sending an additional score report, while Section 6 concludes and discusses why
students’behavior changed so much in response to a small cost change.

2

Background Information

2.1

Setting and Policy Change

The ACT is a nationwide college entrance exam that is particularly popular in the Midwest.
At the time of the policy change, just under one million students took the ACT each year
(ACT Corporation, 1999). During the period considered in the paper, the test consisted of
English, math, reading, and science sections. Students’ scores on these four sections were
averaged to create an overall ACT score, an integer ranging from one to 36.3
Throughout the period analyzed in this paper, when students registered for the ACT,
3

The ACT introduced an optional writing section in 2005, after the time period analyzed in this paper.

4

they provided their demographics, information about their high school experience, and up
to six colleges they wanted their ACT scores sent to. Students could send additional score
reports after they took the test, but this was relatively uncommon: only 8% of students did
so in 2004 (the only year for which this information is available). Free score reports could
be sent only at test registration.
Before the fall of 1997, the ACT allowed students to send three free score reports. Starting
in the fall of 1997, it provided four free reports. The marginal cost of an additional score
report was constant at $6 from the fall of 1995 to the fall of 2001. Before then (in the years
analyzed in the paper) it ranged from $4 to $5.50, while afterwards, each additional score
report cost $7. While the monetary cost of sending four score reports decreased in the fall
of 1997, the non-monetary cost did not. Because students chose the colleges they sent scores
to when they registered for the test, they had to provide payment at that time regardless
of the cost of sending score reports.4 Both before and after the cost change, students were
given six lines to record the colleges to which they wanted their scores sent.
Students who had not taken the ACT under the old cost structure may not have been
aware that the ACT was changing its score-sending policy. It was not generally publicized
nor even mentioned in the ACT’s annual newsletter to guidance counselors. The ACT
registration documents described the then-current cost structure, but never mentioned that
there had been a change.
In comparison, the SAT provided students with four free score reports throughout the
entire period analyzed in the paper. The marginal cost of sending an additional SAT score
report was $6 before the fall of 1994 and $6.50 afterwards.

2.2

Data

This paper uses three datasets: a large database from the ACT Corporation, the American
College Survey (ACS), and the AFS.
2.2.1

ACT Database

I use the ACT database, which contains administrative data from the ACT Corporation
on test-takers, to analyze the change in ACT-takers’ score-sending patterns after the cost
change. The database includes information on students planning to graduate from high
school in 1991, 1992, 1994, 1996, 1998, 2000, and 2004. In particular, it provides information
on one out of every four Caucasians, one out of every two minorities, and every test-taker who
4

This is not necessarily true for low-income students who could waive only the testing, not the scoresending fee.

5

did not provide a race in these classes. This provides a large sample: 2,486,159 observations
with over 287,000 in each year. I observe each student’s ACT score, high school GPA,
race, gender, family income, high school, courses taken, and extracurricular activities. I
also observe up to six colleges to which each student sent her ACT scores at the time of
registration.
In the analysis, I exclude students who sent no score reports at test registration. These
students likely either took the ACT for reasons other than college admissions, sent score
reports after viewing their scores, or sent SAT score reports instead. However, I show the
e¤ect of the cost change on score-sending using the entire sample in an appendix table.
2.2.2

American College Survey

The ACS is a yearly survey of colleges and universities in which over 3,000 colleges provide
data ranging from their courses of study and admissions statistics to their sports teams. I
link the ACS to the ACT database to determine the selectivities of colleges the students
sent scores to. My measure of college selectivity is based on the ACT scores of each college’s
entering freshman class. The ACS provides the 25th and 75th percentile ACT scores of
the entering class.5 I discuss only the results using only colleges’ 25th percentile ACT
scores because the results using the 75th percentiles are so similar.6 I use test scores from
freshmen matriculating in a base year, 1993, so that the analysis is not confounded by
colleges becoming more competitive over time. Using test scores of matriculated students
as a measure of selectivity is common in the literature (e.g., Hoxby and Turner, 2013, Dale
and Krueger, 2002, Loury and Garman, 1995) and the only measure available in the AFS.
However, I show that other selectivity measures provide the same results in the ACT data.
2.2.3

American Freshman Survey

The AFS is a yearly survey of …rst-time full-time (FTFT) freshmen at four-year colleges
and universities. I use the survey to analyze the e¤ect of the cost change on the number of
applications students sent and the selectivities of the colleges they attended.
This paper uses data on the entering college cohorts of 1992 through 1999, the last
cohort with publicly available data. It includes 1,886,245 total observations covering over
350 colleges and over 200,000 students in each year. Colleges come in and out of the survey
such that only 18% of colleges (comprising 36% of student observations) are present in all
5

Many colleges do not provide both SAT and ACT scores of matriculating freshmen. For schools that
only provide their freshman classes’25th and 75th percentile SAT scores, I impute the corresponding ACT
scores using a concordance produced by the College Board.
6
The results for the 75th percentile are readily available upon request.

6

eight cohorts I use. However, survey weights are provided to make the sample representative
of the national population of FTFT freshmen.7
In addition to background characteristics, the data provide information on whether students took the SAT or the ACT, the number of college applications they sent, and the
selectivity of the colleges they enrolled in. The AFS asks students to provide their ACT and
SAT scores. I de…ne students as taking the ACT if they provided ACT scores and taking
the SAT if they provided SAT scores. I limit the sample to students who took only one test.
This eliminates students who took the ACT but were coded as taking neither test because
they didn’t want to provide their test scores and students who took both tests who could
have sent either ACT or SAT scores.8
The AFS directly provides the "median" SAT score of incoming freshmen at each student’s college.9 I convert this SAT score to an ACT score using a concordance produced
by the College Board. So that the analysis is not confounded by colleges changing selectivity over time, I average each college’s yearly ACT scores to create one constant selectivity
measure for each college.
2.2.4

Summary Statistics

Appendix Table 1 displays descriptive statistics from the ACT and AFS databases. Appendix Table 2 shows that low-income students send fewer score reports and applications than
students with higher family incomes even controlling for demographics, high school performance, and high school activities. Appendix Table 3 shows that low-income students send
scores to and attend much less-selective colleges than their higher-income peers. These gaps
decrease, but remain large and signi…cant when control variables are included.
The ACT and AFS data have di¤erent income categories. In the ACT data, I de…ne lowincome students as those with family incomes below $36,000, while in the AFS data I de…ne
them as students with family incomes below $40,000. These are relatively high de…nitions
of low-income: approximately 40% of ACT-takers in the data have family incomes below
$36,000. However, in 1998-1999, 9% of dependent Pell Grant recipients had family incomes
above $40,000, while 25% had family incomes above $30,000 (Department of Education,
1999). In footnotes, I report results for "very low-income students:" students with family
7

The number of colleges included in the survey increases over my sample period. However, the characteristics of the average college are more stable and neither the average selectivity nor the average number of
students at these colleges has a clear trend.
8
The results using the entire sample are qualitatively similar, but slightly attenuated (as expected). These
results are readily available upon request.
9
In fact, this median is the average of the 25th and 75th percentile. While the data has a unique identi…er
for each college, this identi…er purposely cannot be linked to other datasets, so no other college selectivity
measures can be constructed.

7

incomes below $18,000 in the ACT data and below $20,000 in the AFS.

3
3.1

Changes in Score-Sending
Number of Score Reports

When the ACT allowed students to send a fourth free score report, ACT-takers sent substantially more score reports. In particular, there was a dramatic increase in the fraction
of ACT-takers sending exactly four score reports and corresponding decrease in the fraction
sending exactly three. Figure 1 shows that in each class graduating before 1997 (in which
all ACT-takers received only three free score reports) over 80% of ACT-takers sent exactly
three score reports. Less than 5% sent exactly four. On the other hand, in the class of 2000
when test-takers received four free score reports, the fraction sending three score reports
plummeted to 10% while the fraction sending four increased to just under 75%. The class of
1998, in which only some ACT-takers were eligible for four free score reports, represents an
intermediate case where the fraction of ACT-takers sending three score reports had dropped
to just under 40% and the fraction sending four had increased to just over 45%.10
While there were large changes in the fraction of ACT-takers sending exactly three and
exactly four score reports, there were very small changes in the fraction of students sending
other numbers of scores. Aside from the fraction of students sending one score report in 2004,
over the 13 years spanned by these data, the fraction of students sending one, two, …ve, and
six score reports each varied by fewer than one percentage point, remaining almost unchanged
after 1997. The …gure also shows that there was no similar increase in score-sending among
SAT-takers. In fact, after the cost change, there was actually a small decrease in the fraction
of SAT-takers sending four score reports and no change in the fraction of students sending
three. This suggests that it was the change in the ACT’s score-sending cost structure and
not some general secular change that caused the dramatic increase in ACT score-sending.
Table 1 displays regression estimates of the e¤ect of the cost change on the number of
score reports ACT-takers sent. It presents estimates of the regression
yi =

+

1 class1998i

+

2 post1998i

+

3t

+ Xi

4

+ "i :

(1)

Here, the dependent variable, yi ; is the number of score reports student i sent. The variable
class1998i is an indicator for being in the high school class of 1998 and post1998i is an
10

Appendix Figure 1 replicates Figure 1 including students who sent no score reports. The results are very
similar although slightly attenuated due to the fact that over this period there was a 10 percentage point
increase in the fraction of students who sent no score reports.

8

indicator for graduating after 1998 (being in the class of 2000 or 2004). I include separate
indicators for the class of 1998 and classes after 1998 because I expect the policy to have larger
e¤ects in years when all test-takers received four free score reports. The vector Xi includes
controls for student demographics and high school performance.11 Adding these controls
ensures that any change in score-sending behavior is not a result of changing demographics
of ACT-takers. The variable t represents a linear time trend. The …rst column does not
contain the time trend or any controls. The second column adds the time trend, the third
adds the demographic controls, the fourth adds the high school performance controls, and
the …fth adds high school …xed e¤ects. Throughout the paper, standard errors calculated
from the ACT data are clustered at the state level. Standard errors calculated from the AFS
data are robust Huber-White errors (state is not included in the AFS data).
The regressions in Panel A include only middle- and high-income students (students
with family incomes above $36,000 per year). Before the time trend is added, the estimates
indicate that students sent an additional 0.39 score reports in the class of 1998 and an
additional 0.61 score reports in later classes. Including the time trends increases these
coe¢ cients as, on average, students sent 0.02 fewer score reports each year between 1991
and 2004. However, the other covariates and high school …xed e¤ects have very little e¤ect
on the estimates. After these controls are included, the estimates show that on average,
middle- and high-income students in the classes of 2000 and 2004 sent 0.78 more score
reports than those in classes in which students only received three free score reports.
Panel B estimates the same regression on the sample of low-income students. The results
are similar to the results for middle- and high-income students. Low-income students also
substantially increased their score-sending when the fourth score report became free: sending
on average an additional 0.80 score reports.12 Appendix Table 4 replicates this table, including students who did not send any score reports. It shows a large increase in score-sending,
11
I use the same controls in regressions throughout the paper. The demographic controls are the same in
the ACT and AFS data. They are race dummies, an indicator for being a U.S. citizen, an English language
indicator, and gender. In the ACT data, the English language indicator is whether English is the primary
language spoken in the home, while in the AFS data, the indicator is whether English is the student’s native
language. High school performance controls are as follows. In both datasets, I control for high school GPA,
whether the student had college credit, and dummies for each ACT score. (For students in the AFS data who
took only the SAT, I convert their SAT scores to ACT scores using the concordance produced by the College
Board.) In the ACT data, I also control for the number of years of English and math classes the student
took as well as indicators for taking honors English and math, attending a private high school, and being on
a college preparatory track. I add indicators for ever having been elected to a student o¢ ce, working on the
sta¤ of a school paper or yearbook, earning a varsity letter for sports participation, and holding a regular
part-time job. In the AFS data, I do not have these additional controls, but do include controls for whether
the student drank beer, smoked cigarettes, performed volunteer work, spent at least one hour per week on
student clubs or groups, and spent more than …ve hours a week on homework in the last year.
12
Very-low income students (students with family incomes below $18,000 per year) also sent 0.80 additional
score reports on average.

9

but one that is attenuated due to the increase in the number of students sending zero score
reports over this period.

3.2

Selectivity of Score Reports

When students sent more score reports, they sent scores to a wider range of colleges: that
is, those that were both more- and less-selective than any they would have sent scores
to otherwise. Figure 2 shows the average selectivity of students’ most- and least-selective
colleges within each high school class. The …gure shows that the range of colleges students
sent scores to was relatively constant before the cost change. But, it widened for the class
of 1998 and continued to widen for the class of 2000.
Tables 2 and 3 analyze these changes through regressions. They present results from
estimating Equation (1) on the ACT database. Table 2 is limited to middle- and highincome students while Table 3 is limited to low-income students. In Panel A of both tables,
the dependent variable is a student’s range of colleges: the di¤erence between the selectivities
of the most- and least-selective colleges to which a student sent scores. In Panels B and
C, the dependent variables are the selectivities of these most- and least-selective colleges,
respectively. The controls are the same as in Table 1.
The tables show that both middle- and high-income students and low-income students
increased the range of colleges they sent scores to. Low-income students experienced a
slightly larger increase (0.93 points) than did higher-income students (0.88 points) o¤ of
a slightly lower base. (Low-income students had an average range of 2.82 points in 1996,
relative to 3.11 for higher-income students.) For both higher- and low-income students,
about 60% of this increase resulted from students applying to more-selective colleges than
they otherwise would have. Overall, after the cost change, students of all income groups sent
their scores to more-selective colleges on average.13
However, some students sent scores to less-selective schools as a result of the cost change.
Low-income students experienced approximately the same change in the selectivity of their
least-selective colleges as did higher-income students (0.36 and 0.37 ACT points, respec13

The fact that students sent scores to more-selective colleges does not depend on the selectivity metric.
The fraction of students sending scores to a college in one of the top three Barron’s selectivity categories
(most competitive, highly competitive plus, and highly competitive) increased by 7.3 percentage points for
middle- and high-income students and 6.6 percentage points for low-income students. The number of colleges
in the top three Barron’s categories students applied to increased by 0.18 and 0.13 for higher- and low-income
students, respectively. The admissions rate of the most-selective college students applied to decreased by
3.0 and 2.5 percentage points for higher- and low-income students respectively. (So that my results are not
confounded by colleges becoming more competitive over time, I use consistent measures of colleges’Barron’s
ratings and admissions standards for each college across the di¤erent cohorts.)

10

tively).14 When students sent scores to less-selective colleges, they sent scores to colleges
with higher admissions rates. The highest admissions rate of the colleges students sent scores
to increased by 1.5 percentage points for all students and by 1.7 percentage points for lowincome students. This could increase college matriculation by increasing the probability that
a student was admitted to any college.

4
4.1

Changes in Applications and College Selectivity
Number of Applications

The AFS data show that the increase in score reports translated into a substantial increase
in applications, but one that was much smaller than the increase in score-sending. I use two
identi…cation strategies to determine the change in applications. First, I estimate regressions similar to the ones in the previous section using only ACT-takers. Second, I estimate
di¤erence-in-di¤erence regressions, utilizing students who took only the SAT as controls.
The …rst four columns of results in Table 4 present the results of estimating Equation (1)
on students who took only the ACT. Panel A considers middle- and high-income students
while Panel B considers low-income students. The dependent variable is the number of
applications sent. In the AFS data, post1998i indicates that the student was in the high
school class of 1999. As with the score-sending results, the estimates increase when time
trends are added as ACT-takers are estimated to send 0.01 fewer applications every year, but
are robust to the addition of controls. When the time trend and all the controls are added,
the estimates indicate that higher-income students in the class of 1999 sent an additional
0.19 applications and students in the class of 1998 sent an additional 0.11 applications than
students in previous cohorts.
Panel B shows that low-income students also sent more applications when the fourth
score report became free. As with the increase in score reports, their response was similar in
magnitude to that of their higher-income peers. Conditional on all the controls, I estimate
that low-income students sent an additional 0.14 applications in the class of 1999 and 0.11
additional applications in the class of 1998, close to 20% of the increase in score-sending.
The …nal four columns of Table 4 present estimates from the second identi…cation strategy.
14

Students with very low family incomes experienced a slightly larger changes in the range of colleges they
sent scores to (1.00 points) and the selectivity of the most-selective college they sent scores to (0.63 points)
than did low-income students. They experienced about the same decrease in selectivity of their least-selective
college (0.37 points).

11

Speci…cally, it displays results from estimating the equation
yi =

+
+

1 (class1998i

5 post1998i

+

ACTi ) +
6 ACTi

+

7t

2 (post1998i

+

8 (t

ACTi ) +

ACTi ) + Xi

9

3 class1998i

+ "i

(2)

where yi is the number of applications sent and ACTi is an indicator for taking the ACT.
The estimates also suggest that ACT-takers sent signi…cantly more applications as a result
of the cost change. Once the time trends and all the controls are added, the increase in
applications measured for the average higher-income student in the class of 1998 (0.13) is
the same as in the other identi…cation strategy. The e¤ect for students graduating after 1998
(0.07) is about 40% the size of the e¤ect measured with the other identi…cation strategy.
The results suggest that low-income students increased the number of applications they sent
by 0.08 in both the class of 1998 and later classes, approximately 70% and 55%, respectively,
of the estimates using only ACT-takers.15

4.2

Selectivity of Attended College

I use the same two identi…cation strategies to examine the change in the selectivity of the
colleges attended by ACT-takers after the cost change. Table 5 replicates Table 4 where the
dependent variable is the selectivity of the college attended instead of number of applications
sent. I consider Panel B, which shows the e¤ect of the cost change on the college selectivity
of low-income students, …rst. The …rst identi…cation strategy (using only ACT-takers) …nds
that the average low-income ACT-taker in the classes of 1998 and 1999 attended moreselective colleges (colleges that where 0.26 ACT points and 0.24 ACT points more selective,
respectively). These changes are approximately half the di¤erence in the selectivities of
colleges attended by observationally equivalent low- and high-income students and 40% of
the increase in the average selectivity of the most-selective colleges low-income students sent
scores to.16 The second identi…cation strategy also …nds that low-income students attended
more-selective colleges as a result of the cost change. The estimated magnitudes of the e¤ect
for low-income students are similar to the estimates using the other identi…cation strategy
(0.14 and 0.31 ACT points for the classes of 1998 and 1999, respectively). However, these
15

Students with very low family incomes experienced relatively similar increases the number of applications
they sent as low-income students in general: 0.13 in the class of 1999 using only ACT-takers and 0.10 using
the di¤erence-in-di¤erence strategy.
16
This does not necessarily imply that score reports sent to more-selective colleges translated into applications at a higher-than-average rate. Consider a student who would have applied to three colleges before
the cost change: a very selective college from which she was rejected and two unselective colleges. The cost
change induced her to apply to a moderately selective college to which she could gain admission. Thus, it
could induce her to attend a more selective college without a¤ecting the range of colleges she applied to.

12

estimates do not appear to be as robust as the ones using the …rst identi…cation strategy.
There are two potential threats to the validity of these results. The …rst is that the
supply of college slots may not be perfectly elastic. If low-income ACT-takers displaced
SAT-takers from selective colleges, the di¤erence-in-di¤erence results in the …nal columns of
Table 5 would overestimate the e¤ect of the cost change. The estimates in the …rst columns
of the table would not su¤er from this bias. The second potential threat is that I observe only
matriculated students in the AFS. Sending an additional application may have induced some
ACT-takers to attend college. If these new matriculants were unobservably di¤erent from
other matriculated ACT-takers, then the results from both identi…cation strategies would be
biased. However, this e¤ect would have to be implausibly large (and in a counterintuitive
direction) to drive these results.17 Moreover, if the results were being driven by increased
matriculation, this would suggest that some low-income students bene…tted from the cost
change (see the next section).
Panel A shows the results for middle- and high-income students. Despite the fact that
higher-income students changed their score-sending and application behavior similarly to
low-income students, the estimates using ACT-takers only suggest that they did not attend
more-selective colleges as a result of the cost change. In fact, when all the controls are added,
middle- and high-income students are estimated to attend less-selective colleges as a result
of the cost change. This estimate is relatively small (about one fourth of the e¤ect for lowincome students), but suggests that some higher-income students may have been crowded
out of selective colleges by lower-income students. The second identi…cation strategy also
indicates that higher-income students attended less-selective colleges after the cost change,
though this is not robust to the exclusion of controls.18

5

Assessing Bene…ts to Students

Sending an additional application may bene…t a student by increasing the probability she
is admitted to any college, allowing her to attend a more-selective college, or allowing her
to attend a college with a better …nancial aid package. But, it also has costs. It costs
17

For selection to drive the results for low-income students in Table 5, the new matriculants would have
to have attended more-selective colleges than the students who did not need an additional free score report
to induce them to attend college. Even if, conditional on observables, the new matriculants would have
attended colleges that were one standard deviation more selective than the existing matriculants conditional
on observables, the number of matriculants would have had to increase by approximately 9% to cause the
0.24 ACT point change in college selectivity. This seems implausibly large relative to the 14% of test-takers
who sent an additional application.
18
Despite having relatively similar changes in score-sending and application behavior, very low-income students experienced increases in the selectivity of the colleges they attended almost twice as large as low-income
students: 0.48 points using only ACT-takers and 0.53 points using the di¤erence-in-di¤erence strategy.

13

the student time to complete the application and the admissions o¢ cer time to read it.
Higher-income students often have to pay application fees. Moreover, students induced to
attend college or more-selective colleges may have crowded out other students. Bound and
Turner (2007) …nd that college slots are partially elastic, suggesting full crowdout is unlikely.
However, even if there were perfect crowdout, given that low-income students are estimated
to have particularly large returns from attending college and selective colleges, increasing
the number of low-income students may increase e¢ ciency. The fact that many selective
colleges have recently attempted to attract more low-income students suggests they also
value economically diverse student bodies.
In this section, I benchmark the bene…ts low-income ACT-takers received from sending
another score report through (1) attending more-selective colleges and (2) increased college
matriculation. I do not calculate the costs to potentially displaced higher-income students,19
the direct costs students or colleges incurred from submitting or receiving additional applications, or bene…ts students received from any other channels (e.g., obtaining a better …nancial
aid package). However, the bene…ts to sending an additional score report appear so large
that they are likely to have greatly outweighed the time and monetary costs of sending an
additional application.
First, I consider the bene…t low-income students receive from attending more selective
colleges. I use Dale and Krueger’s (2002) estimate that low-income students receive a 4%
wage premium for attending a college whose students score 100 points higher on the SAT.
I use this estimate because it is directly comparable to the college selectivity measures
in the AFS, it examines low-income students separately, and because Dale and Krueger’s
methodology generally …nds smaller returns to college quality than do other approaches,
making my estimates more conservative. A concern with using this estimate is that the
low-income students have relatively low graduation rates and inducing them to attend more
selective colleges may decrease graduation rates. Dale and Krueger (2002) do not condition
on graduating from college, so their estimate of the return to college quality takes potential
falling graduation rates into account. However, they consider students attending highlyselective colleges among whom dropout is a smaller issue. To the extent that inducing
students to attend more-selective colleges decreases graduation rates, this may overestimate
the e¤ect of attending a more-selective college. However, recent papers …nd that graduation
rates decrease when students are induced to attend less-selective colleges (Cohodes and
Goodman, 2013) and that students induced to attend college or attend more-selective colleges
after receiving assistance with or information about the application process have high college
persistence rates (Hoxby and Turner, 2013; Bettinger et al., 2012; Carrell and Sacerdote,
19

Dale and Krueger (2002) suggest displaced higher-income students would not su¤er earnings losses.

14

2013).
Based on the concordance produced by the College Board, one ACT point is equivalent
to 44 SAT points. Day and Newburger (2002) estimate that the average college graduate
will earn $2.1 million in 1999 dollars over her lifetime. Under these assumptions, the bene…t
a low-income student receives from attending a college with average ACT scores one point
higher is $2; 100; 000 4% 0:44 = $36; 960:
I …nd that 80.1% of low-income students sent an additional score report after the cost
change (Table 1). Using only ACT-takers, I estimate that the average low-income student
attended a college with ACT scores 0.236 points higher after the cost change. (I use this
estimate instead of the di¤erence-in-di¤erence estimate because it is more conservative and
not potentially biased upward by the displacement of SAT-takers.) Thus, if the only lowincome ACT-takers a¤ected by the policy were those who sent an additional score report,
then the expected bene…t these students received from sending the additional report was
$36; 960 0:236
0:801

$10; 900:

(3)

If, in fact, low-income ACT-takers who did not send additional score reports were displaced
from selective colleges by those who did, this calculation understates the bene…ts obtained by
students who sent additional score reports. In either case, the bene…ts low-income students
received through this channel far outweighed the $6 cost of sending additional score report.
I next consider the bene…ts low-income students could have obtained through increased
college matriculation. I estimate the fraction of students that would have had to have been
induced to attend college for the average bene…t of sending an additional score report to
exceed $6. I assume students induced to attend college by sending an additional score report
attend two years of college. This is somewhat arbitrary, but is intended to account for
dropout. To calculate the earnings gain from attending two years of college, I use Card’s
(1995) estimate that students’earnings increase by 10% for each additional year of college.20
I use Day and Newburger’s (2002) estimate that high school graduates from these cohorts
will have lifetime earnings of $1.2 million in 1999 dollars. Thus, the bene…t of attending two
years of college is $1:2 million 20% = $240; 000:
This bene…t is o¤set by tuition costs and foregone earnings. The Digest of Education
Statistics reports that the average tuition, room, and board at four-year colleges and universities was $12,352 in 1999-2000. This likely overestimates low-income students’costs because
they receive …nancial aid and may be more likely to attend less-expensive colleges. To cal20

Card (1995) estimates the gains to an additional year of education to be between 10% to 14%. I use his
lower-bound estimate here to be conservative.

15

culate forgone earnings, I use Day and Newburger’s (2002) estimate that recent high school
graduates earn $20,975 per year in the labor market. This estimate is for ages 25 to 29
and thus may overstate earnings at ages 18 and 19, but I use it to be consistent with the
lifetime earnings measures (Day and Newburger do not show average earnings for younger
ages). Under these assumptions, the bene…t of attending two years of college is approxi$6
= 3:5 10 5 or one out of approximately every 29,000
mately $173; 350: Thus, only $173;350
students who sent an additional score report would have had to attend two years of college
for the bene…ts of sending an additional score report through this channel to exceed $6.

6

Conclusion

The colleges a student applies to greatly a¤ect whether she attends college, the type of college
she attends, and her future earnings. Yet, little is known about how students decide where
to apply. This paper analyzes the e¤ect of the ACT’s increasing the number of free score
reports it provided from three to four, a $6 decrease in students’ cost of sending a fourth
score report. It …nds that when the fourth score report became free, students sent many
more score reports and applications. They sent their scores to a wider range of colleges and
low-income students also attended more-selective colleges as a result.
I estimate that the expected bene…t a low-income ACT-taker received from sending an
additional score report likely far exceeded $6. Thus, it seems unlikely that it could be optimal
for so many low-income students to change their application behavior as a result of a $6 cost
change. It is not necessarily surprising that students may not be applying to the optimal
set of colleges, given the di¢ culty in identifying this set. Students must choose one of over
22;400 combinations of colleges to apply to. The value of applying to even one combination
depends on many parameters students may not know: the probability of admission to each
set of colleges in the combination, the utility from attending each college, and the cost
of applying. The utility a student would get from attending a given college depends on
her …nancial aid package (which is often not revealed until after admissions decisions), her
earnings after attending the college, her earnings if she did not attend a four-year college,
and the utility derived from experiences she would have at the school. Avery and Kane
(2004) show that students have di¢ culty estimating even part of this utility.
It may not be the $6 cost that was important. If it were, then higher-income students
should respond similarly to a $6 decrease in application fees and score-sending costs.21 Yet,
they do not appear to do so. There is no relationship in the ACS data between changes
21

Low-income students may be more responsive to changes in score-sending costs than application fees as
they can often waive application fees, but not score-sending costs.

16

in colleges’application fees and changes in the number of applications they received during
the period from 1993 to 2002. This lack of relationship could result from the endogeneity of
application fees. However, many colleges go to great lengths to encourage applications. If
they believed application fees substantially increased their applicant pools, they would likely
greatly reduce these fees.
An alternative explanation is that the cost of the fourth score report fell to $0. Several
studies (e.g., Kremer and Miguel, 2007 and Ariely, 2008) have found that demand is discontinuous at a price of zero. However, even though the fourth score report cost $0, sending an
additional application was still costly for higher-income students because of application fees.
A …nal alternative is that students may interpret the ACT providing three (or four) free
score reports as a signal that sending three (or four) applications is recommended and use
that signal as a rule of thumb about how many colleges to apply to. This explanation is
consistent with the Madrian and Shea (2001), Choi et al. (2002), and Thaler and Sunstein
(2008) results that default 401(k) plans and Medicare Part D plans have large e¤ects on plan
choices. College application guides show that many students are looking for an authority to
provide a rule of thumb on how many colleges they should apply to. “How many applications
are enough?”is the …rst frequently asked question on the College Board’s website for college
counselors22 and is prominently featured in many other college guides. The College Board
suggests sending …ve to eight applications, many more than students send on average. If
students are responding to the rule of thumb, providing them with rules of thumb based on
data as opposed to the pricing structure of the ACT could lead to large changes in application
behavior, facilitating higher college attendance and better student-college matches.

References
ACT Corporation (1999): “Selections from the 1999 National Score Report:
Academic Abilities and Nonacademic Characteristics of ACT-Tested Graduates,”
http://www.act.org/newsroom/data/1999/99data.html.
Ariely, D. (2008): Predictably Irrational: The Hidden Forces that Shape Our Decisions.
HarperCollins, New York, NY.
Avery, C., and T. J. Kane (2004): “Student Perceptions of College Opportunities: The
Boston COACH Program,”in College Choices: The Economics of Where to Go, When to
Go, and How to Pay for It, ed. by C. M. Hoxby, pp. 355–393. University of Chicago Press,
Chicago.
22

See http://professionals.collegeboard.com/guidance/applications/how-many (accessed September 17,
2013).

17

Behrman, J. R., M. R. Rosenzweig, and P. Taubman (1996): “College Choice and
Wages: Estimates Using Data on Female Twins,” Review of Economics and Statistics,
78(4), 672–685.
Bettinger, E. P., B. T. Long, P. Oreopolous, and L. Sanbonmatsu (2012): “The
Roles of Application Assistance and Information in College Decisions: Results from the
H&R Block Fafsa Experiment,”Quarterly Journal of Economics, 127(3), 1205–1242.
Black, D., and J. Smith (2006): “Estimating the Returns to College Quality with Multiple
Proxies for Quality,”Journal of Labor Economics, 24(3), 701–728.
Bound, J., and S. Turner (2007): “Cohort crowding: How resources a¤ect collegiate
attainment,”Journal of Public Economics, 91(5), 877–899.
Bowen, W. G., M. A. Kurzweil, and E. M. Tobin (2005): Equity and Excellence in
American Higher Education. University of Virginia Press, Charlottesville.
Brewer, D. J., E. R. Eide, and R. G. Ehrenberg (1999): “Does it Pay to Attend an
Elite Private College? Cross-cohort Evidence on the E¤ects of College Type on Earnings,”
Journal of Human Resources, 34(1), 104–123.
Card, D. (1995): “Using Geographic Variation in College Proximity to Estimate the Return to Schooling,” in Aspects of Labour Market Behavior: Essays in Honour of John
Vanderkamp, ed. by L. N. Christo…des, E. K. Grant, and R. Swidinsky, pp. 201–222.
University of Toronto Press, Toronto.
Carrell, S. E., and B. Sacerdote (2013): “Late Interventions Matter Too: The Case
of College Coaching in New Hampshire,”NBER Working Paper, 19031.
Choi, J. J., D. Laibson, B. Madrian, and A. Metrick (2002): “De…ned Contribution
Pensions: Plan Rules, Participant Decisions, and the Path of Least Resistance,” in Tax
Policy and the Economy, ed. by J. Poterba, pp. 67–113. MIT Press, Cambridge, MA.
Cohodes, S., and J. Goodman (2013): “Merit Aid, College Quality and College Completion: Massachusetts Adams’Scholarship as an In-Kind Subsidy,” HKS Faculty Research
Working Paper Series, RWP13-005.
Dale, S. B., and A. B. Krueger (2002): “Estimating the Payo¤ to Attending a More Selective College: an Application of Selection on Observables and Unobservables,”Quarterly
Journal of Economics, 117(4), 1491–1527.
Day, J. C., and E. C. Newburger (2002): “The Big Payo¤: Educational Attainment and
Synthetic Estimates of Work-life Earnings,”Current Population Reports, pp. P23–210.
Department of Education (1999): Title IV/Federal Pell Grant Program 1998-1999 End
of Year Report.

18

Ellwood, D. T., and T. J. Kane (2000): “Who is Getting a College Education: Family
Background and the Growing Gaps in Enrollment,” in Securing the Future: Investing in
Children from Birth to College, ed. by S. Danziger, and J. Waldfogel, pp. 283–324. Russell
Sage Foundation, New York, NY.
Hill, C. B., G. C. Winston, and S. A. Boyd (2005): “A¤ordability: Family Incomes
and Net Prices at Highly Selective Private Colleges and Universities,” Journal of Human
Resources, 40(4), 769–790.
Hoxby, C. M. (1998): “The Return to Attending a More Selective College: 1960 to the
Present,”http://www.nyu.edu/classes/jepsen/hoxby-selective.pdf.
Hoxby, C. M., and C. Avery (2012): “The Missing "One-O¤s": The Hidden Supply of
High-Achieving, Low Income Students,”NBER Working Paper, 18586.
Hoxby, C. M., and S. Turner (2013): “Expanding College Opportunities for HighAchieving, Low Income Students,”SIEPR Discussion Paper, 12-014.
Kremer, M., and E. Miguel (2007): “The Illusion of Sustainability,” Quarterly Journal
of Economics, 112(3), 1007–1065.
Loury, L. D., and D. Garman (1995): “College Selectivity and Earnings,” Journal of
Labor Economics, 13(2), 289–308.
Madrian, B. C., and D. F. Shea (2001): “The Power of Suggestion: Inertia in 401(k)
Participation and Savings Behavior,”Quarterly Journal of Economics, 116(4), 1149–1188.
Monks, J. (2000): “The Return to Individual and College Characteristics: Evidence from
the National Longitudinal Survey of Youth,”Economics of Education Review, 19(3), 279–
289.
Pallais, A., and S. Turner (2006): “Opportunities for Low Income Students at Top
Colleges and Universities: Policy Initiatives and the Distribution of Students,” National
Tax Journal, 59(2), 357–386.
Saavedra, J. E. (2008): “The Returns to College Quality: A Regression Discontinuity
Analysis,”Harvard University.
Snyder, T. D., and C. M. Hoffman (2002): Digest of Education Statistics 2001. National
Center for Education Statistics, Washington D.C.
Spies, R. R. (2001): “The E¤ect of Rising Costs on College Choice: The Fourth and Final
in a Series of Studies on This Subject,”Princeton University Research Report Series, 117.
Thaler, R. H., and C. R. Sunstein (2008): Nudge: Improving Decisions About Health,
Wealth, and Happiness. Yale University Press, New Haven.
Winston, G. C., and C. B. Hill (2005): “Access to the Most Selective Private Colleges
by High Ability, Low-Income Students: Are They Out There?,” Williams Project on the
Economics of Higher Education, Discussion Paper 69.

19

Zhang, L. (2005): “Do Measures of College Quality Matter? The E¤ect of College Quality
on Graduates’Earnings,”The Review of Higher Education, 28(4), 571–596.

20

Figure 1. Number of Scores Sent by High School Graduation Year
Figure 1a: Students Who Took the ACT

0.9

Fraction of ACT-Takers

0.8
0.7
0.6
0.5
Three Score Reports
Four Score Reports

0.4
0.3
0.2
0.1
0
1991

1992

1994

1996

1998

2000

2004

High School Graduation Year

Figure 1b: Students Who Took the SAT

0.9

Fraction of SAT-Takers

0.8
0.7
0.6
Three Score Reports

0.5

Four Score Reports

0.4
0.3
0.2
0.1
0
1994

1995

1996

1997

1998

1999

2000

2001

High School Graduation Year

Notes: The bars indicate the fraction of each high school class that sent either exactly three or exactly four
score reports. The analysis is limited to students who sent at least one score report. Data in Panel A come
from the ACT database and data in Panel B come from a database of SAT-takers produced by the College
Board.

21

Figure 2. Selectivity of Score Reports by High School Graduation Year

22

College ACT Score

21
Most‐Selective, Higher‐Income Students
20
Least‐Selective, Higher‐Income Students
19

Most‐Selective, Low‐Income Students
Least‐selective, Low‐Income Students

18
17
1990

1992

1994

1996

1998

2000

2002

2004

High School Graduation Year
Notes: The y-axis measures the 25th percentile ACT scores of incoming freshmen at the most- and least-selective
colleges students sent scores to. The data points marked with diamonds show the average selectivity of the mostselective college each student sent scores to. The data points marked with circles show the average selectivity of the
least-selective college each student sent scores to. The blue data points include data from middle- and high-income
students (students with family incomes at least $36,000 per year), while the red data points include data from only
low-income students (students with family incomes below $36,000 per year). The data come from the ACT database
and the American College Survey.

22

Table 1. Change in the Number of Scores Sent by Family Income
Dependent Variable: Number of Score Reports Sent
ACT Data
A. Middle- and High-Income Students
0.488**
0.489**
0.456**
(0.025)
(0.025)
(0.024)

Class of 1998

0.388**
(0.028)

0.455**
(0.024)

Post-1998

0.610**
(0.026)

0.798**
(0.027)

0.799**
(0.027)

0.779**
(0.012)

0.779**
(0.013)

Constant

3.036**
(0.015)

3.091**
(0.026)

3.030**
(0.040)

2.455**
(0.223)

2.423**
(0.250)

Observations
R-squared

938,257
0.093

938,257
0.095

938,257
0.097

938,257
0.121

938,257
0.160

Class of 1998

0.508**
(0.023)

B. Low-Income Students
0.593**
0.593**
0.561**
(0.027)
(0.026)
(0.018)

0.561**
(0.019)

Post-1998

0.656**
(0.037)

0.810**
(0.019)

0.809**
(0.019)

0.803**
(0.012)

0.801**
(0.012)

Constant

2.924**
(0.011)

2.962**
(0.021)

2.899**
(0.032)

2.519**
(0.149)

2.542**
(0.145)

Observations
R-squared

819,576
0.130

819,576
0.131

819,576
0.135

819,576
0.163

819,576
0.210

Time Trend
No
Yes
Yes
Yes
Yes
Demographics
No
No
Yes
Yes
Yes
High School Performance
No
No
No
Yes
Yes
High School Fixed Effects
No
No
No
No
Yes
Notes: Each panel displays the results of estimating Equation (1) where the dependent variable
is the number of score reports a student sent. Data come from the ACT database. Panel A
includes only middle- and high-income students (students with family incomes at least $36,000
per year) while Panel B includes only low-income students (students with family incomes below
$36,000 per year). All regressions include only students who sent at least one score report.
Standard errors (in parentheses) are clustered at the state level. The first column of each panel
adds no controls, the second column adds a linear time trend, the third column adds controls for
demographics (see footnote 11), the fourth column adds controls for high school performance
(see footnote 11), and the fifth column adds high school fixed effects. When high school fixed
effects are added, the control for attending a private high school is dropped. Two asterisks
indicate the coefficient is significant at the 5% level.

23

Table 2. Changes in the Selectivity of Score Reports Sent
ACT Data: Middle- and High-Income Students
A. Dependent Variable: Selectivity Range
0.516**
0.518**
0.505**
(0.033)
(0.033)
(0.034)

Class of 1998

0.376**
(0.027)

Post-1998

0.581**
(0.036)

0.842**
(0.037)

0.847**
(0.037)

0.873**
(0.035)

0.882**
(0.035)

Constant

3.165**
(0.091)

3.242**
(0.088)

3.650**
(0.130)

1.316**
(0.444)

1.349**
(0.475)

Observations
R-squared

881,709
0.009

881,709
0.009

881,709
0.014

881,709
0.062

881,709
0.150

Class of 1998

0.507**
(0.033)

B. Dependent Variable: Selectivity of Students' Most-Selective College
0.086**
0.287**
0.293**
0.290**
0.273**
(0.022)
(0.023)
(0.023)
(0.030)
(0.021)

Post-1998

0.075**
(0.029)

0.453**
(0.029)

0.463**
(0.029)

0.524**
(0.031)

0.509**
(0.029)

Constant

21.902**
(0.146)

22.013**
(0.144)

23.156**
(0.191)

19.453**
(0.872)

18.730**
(0.696)

Observations
R-squared

881,709
0.000

881,709
0.001

881,709
0.019

881,709
0.244

881,709
0.349

Class of 1998

C. Dependent Variable: Selectivity of Students' Least-Selective College
-0.290**
-0.229**
-0.225**
-0.215**
-0.235**
(0.029)
(0.023)
(0.022)
(0.025)
(0.020)

Post-1998

-0.506**
(0.037)

-0.390**
(0.026)

-0.385**
(0.026)

-0.348**
(0.029)

-0.373**
(0.026)

Constant

18.736**
(0.164)

18.771**
(0.164)

19.505**
(0.193)

18.137**
(0.764)

17.381**
(0.490)

Observations
R-squared

881,709
0.007

881,709
0.007

881,709
0.024

881,709
0.116

881,709
0.288

Time Trend
No
Yes
Yes
Yes
Yes
Demographics
No
No
Yes
Yes
Yes
High School Performance
No
Yes
No
Yes
Yes
High School Fixed Effects
No
Yes
No
No
Yes
Notes: Each panel displays the results of estimating Equation (1). The dependent variable is the
difference between the 25th percentile ACT scores of incoming freshmen at the most- and leastselective colleges a student sent scores to (Panel A), the 25th percentile ACT score of incoming
freshmen at the most-selective college she sent scores to (Panel B) and the 25th percentile ACT score
of incoming freshmen at the least-selective college she sent scores to. Data come from the ACT
database and American College Survey. The regressions include all middle- and high-income students
(students with family incomes at least $36,000 per year) who sent a score report to a college for which
the ACS has selectivity information. Standard errors (in parentheses) are clustered at the state level.
The first column of each panel adds no controls, the second column adds a linear time trend, the third

24

column adds controls for demographics (see footnote 11), the fourth column adds controls for high
school performance (see footnote 11), and the fifth column adds high school fixed effects. When high
school fixed effects are added, the control for attending a private high school is dropped. Two asterisks
indicate the coefficient is significant at the 5% level.

25

Table 3. Changes in the Selectivity of Score Reports Sent
ACT Data: Low-Income Students
A. Dependent Variable: Selectivity Range
0.639**
0.640**
0.635**
(0.040)
(0.040)
(0.039)

Class of 1998

0.555**
(0.036)

Post-1998

0.695**
(0.041)

0.847**
(0.045)

0.844**
(0.044)

0.890**
(0.040)

0.925**
(0.041)

Constant

2.833**
(0.107)

2.870**
(0.109)

3.216**
(0.153)

1.875**
(0.550)

1.699**
(0.579)

Observations
R-squared

737,135
0.011

737,135
0.012

737,135
0.017

737,135
0.057

737,135
0.161

Class of 1998

0.652**
(0.040)

B. Dependent Variable: Selectivity of Students' Most-Selective College
0.238**
0.370**
0.380**
0.386**
0.383**
(0.031)
(0.026)
(0.027)
(0.034)
(0.030)

Post-1998

0.225**
(0.042)

0.462**
(0.036)

0.484**
(0.039)

0.561**
(0.034)

0.570**
(0.034)

Constant

20.890**
(0.192)

20.948**
(0.187)

21.911**
(0.239)

19.318**
(0.518)

18.784**
(0.468)

Observations
R-squared

737,135
0.001

737,135
0.002

737,135
0.036

737,135
0.173

737,135
0.302

Class of 1998

C. Dependent Variable: Selectivity of Students' Least-Selective College
-0.317**
-0.270**
-0.259**
-0.249**
-0.269**
(0.042)
(0.022)
(0.021)
(0.023)
(0.020)

Post-1998

-0.470**
(0.063)

-0.385**
(0.033)

-0.360**
(0.031)

-0.329**
(0.030)

-0.355**
(0.030)

Constant

18.058**
(0.211)

18.078**
(0.212)

18.695**
(0.295)

17.442**
(0.703)

17.085**
(0.612)

Observations
R-squared

737,135
0.005

737,135
0.005

737,135
0.053

737,135
0.100

737,135
0.306

Time Trend
No
Yes
Yes
Yes
Yes
Demographics
No
No
Yes
Yes
Yes
High School Performance
No
Yes
No
Yes
Yes
High School Fixed Effects
No
Yes
No
No
Yes
Notes: Each panel displays the results of estimating Equation (1). The dependent variable is the
difference between the 25th percentile ACT scores of incoming freshmen at the most- and leastselective colleges a student sent scores to (Panel A), the 25th percentile ACT score of incoming
freshmen at the most-selective college she sent scores to (Panel B) and the 25th percentile ACT
score of incoming freshmen at the least-selective college she sent scores to (Panel C). Data come
from the ACT database and American College Survey. The regressions include all low-income
students (students with family incomes below $36,000 per year) who sent a score report to a college
for which the ACS has selectivity information. Standard errors (in parentheses) are clustered at the
state level. The first column of each panel adds no controls, the second column adds a linear time

26

trend, the third column adds controls for demographics (see footnote 11), the fourth column adds
controls for high school performance (see footnote 11), and the fifth column adds high school fixed
effects. When high school fixed effects are added, the control for attending a private high school is
dropped. Two asterisks indicate the coefficient is significant at the 5% level.

27

Table 4. Change in the Number of Applications Sent
Dependent Variable: Number of Applications Sent
AFS Data
A. Middle- and High-Income Students
ACT-Takers Only

Class of 1998 × ACT

ACT-Takers and SAT-Takers
0.100**
0.196**
0.176**
0.128**
(0.015)
(0.019)
(0.019)
(0.018)

Post-1998 × ACT

0.007
(0.014)

0.132**
(0.021)

0.130**
(0.021)

0.072**
(0.021)

ACT

-1.235**
(0.006)

-1.132**
(0.014)

-1.112**
(0.014)

-1.108**
(0.014)

Class of 1998

0.032**
(0.011)

0.105**
(0.014)

0.113**
(0.014)

0.105**
(0.014)

-0.068**
(0.010)

-0.092**
(0.013)

-0.068**
(0.013)

-0.028**
(0.012)

Post-1998

0.080**
(0.010)

0.174**
(0.015)

0.190**
(0.015)

0.186**
(0.015)

0.073**
(0.010)

0.043**
(0.015)

0.054**
(0.015)

0.116**
(0.014)

Constant

2.777**
(0.004)

2.856**
(0.010)

3.110**
(0.045)

2.853**
(0.234)

4.012**
(0.004)

3.988**
(0.009)

4.335**
(0.023)

4.223**
(0.063)

Observations
R-squared

328,271
0.000

328,271
0.001

328,271
0.017

328,271
0.039

879,176
0.096

879,176
0.096

879,176
0.108

879,176
0.142

B. Low-Income Students
ACT-Takers Only

Class of 1998 × ACT

ACT-Takers and SAT-Takers
0.011
0.164**
0.108**
0.076**
(0.025)
(0.032)
(0.031)
(0.030)

Post-1998 × ACT

-0.078**
(0.025)

0.116**
(0.036)

0.118**
(0.035)

0.077**
(0.034)

ACT

-0.977**
(0.009)

-0.836**
(0.019)

-0.811**
(0.019)

-0.832**
(0.019)

Class of 1998

0.105**
(0.017)

0.109**
(0.021)

0.120**
(0.020)

0.108**
(0.020)

0.094**
(0.018)

-0.055**
(0.024)

0.005**
(0.023)

0.022**
(0.023)

Post-1998

0.112**
(0.016)

0.116**
(0.024)

0.154**
(0.022)

0.141**
(0.022)

0.190**
(0.019)

0.000
(0.027)

0.033
(0.026)

0.060**
(0.026)

Constant

2.571**
(0.006)

2.575**
(0.013)

2.747**
(0.044)

2.721**
(0.370)

3.549**
(0.007)

3.411**
(0.015)

3.763**
(0.026)

3.449**
(0.081)

Observations
R-squared

156,477
0.001

156,477
0.001

156,477
0.057

156,477
0.075

337,705
0.074

337,705
0.074

337,705
0.117

337,705
0.141

Time Trends
No
Yes
Yes
Yes
No
Yes
Yes
Yes
Demographics
No
No
Yes
Yes
No
No
Yes
Yes
High School Performance
No
No
No
Yes
No
No
No
Yes
High School Fixed Effects
No
No
No
No
No
No
No
No
Notes: The first four columns of each panel display the results of estimating Equation (1), while the last four columns display
the result of estimating Equation (2). The dependent variable is the number of applications a student sent and data come
from the AFS. Panel A includes middle- and high-income students (students with family incomes at least $40,000 per year)

28

while Panel B is limited to low-income students (students with family incomes below $40,000). The first four columns of
results include students who only took the ACT, while the last four include students who took the ACT or the SAT, but not
both. Huber-White standard errors are in parentheses. The first column of each panel adds no controls, the second column
adds a linear time trend, and the third column adds controls for demographics (see footnote 11), and the fourth column
adds controls for high school performance (see footnote 11). Two asterisks indicate the coefficient is significant at the 5%
level.

29

Table 5. Change in the Selectivity of College Attended
Dependent Variable: Selectivity of College Attended
AFS Data
A. Middle- and High-Income Students
ACT-Takers Only

Class of 1998 × ACT

ACT-Takers and SAT-Takers
0.108**
0.272**
0.212**
-0.081**
(0.023)
(0.032)
(0.029)
(0.026)

Post-1998 × ACT

0.204**
(0.022)

0.415**
(0.035)

0.353**
(0.032)

-0.055*
(0.029)

ACT

-1.910**
(0.010)

-1.743**
(0.023)

-1.649**
(0.021)

-1.830**
(0.019)

Class of 1998

0.047**
(0.015)

0.076**
(0.022)

0.018
(0.020)

-0.017
(0.019)

-0.062**
(0.018)

-0.196**
(0.023)

-0.193**
(0.021)

0.037**
(0.017)

Post-1998

-0.039**
(0.013)

0.000
(0.024)

-0.061**
(0.021)

-0.080**
(0.020)

-0.243**
(0.017)

-0.415**
(0.026)

-0.416**
(0.024)

-0.042**
(0.020)

Constant

19.408**
(0.007)

19.441**
(0.016)

20.182**
(0.062)

19.006***
(0.302)

21.319**
(0.007)

21.184**
(0.016)

22.000**
(0.039)

19.664**
(0.089)

Observations
R-squared

325,801
0.000

325,801
0.000

325,801
0.079

325,801
0.192

871,857
0.091

871,857
0.091

871,857
0.146

871,857
0.380

B. Low-Income Students
ACT-Takers Only

Class of 1998 × ACT

ACT-Takers and SAT-Takers
0.411**
0.506**
0.326**
0.136**
(0.043)
(0.058)
(0.049)
(0.046)

Post-1998 × ACT

0.661**
(0.040)

0.783**
(0.063)

0.608**
(0.054)

0.311**
(0.051)

ACT

-1.391**
(0.016)

-1.301**
(0.032)

-1.171**
(0.029)

-1.571**
(0.028)

Class of 1998

0.269**
(0.025)

0.391**
(0.036)

0.318**
(0.031)

0.255**
(0.030)

-0.142**
(0.035)

-0.115**
(0.045)

-0.013
(0.038)

0.091**
(0.034)

Post-1998

0.251**
(0.023)

0.406**
(0.040)

0.300**
(0.034)

0.236**
(0.032)

-0.410**
(0.033)

-0.377**
(0.049)

-0.311**
(0.043)

-0.104**
(0.038)

Constant

18.542**
(0.010)

18.656**
(0.021)

20.136**
(0.068)

19.391**
(0.464)

19.933**
(0.013)

19.957**
(0.024)

21.249**
(0.043)

19.465**
(0.121)

Observations
R-squared

154,603
0.002

154,603
0.002

154,603
0.171

154,603
0.250

332,940
0.047

332,940
0.047

332,940
0.182

332,940
0.326

Time Trends
No
Yes
Yes
Yes
No
Yes
Yes
Yes
Demographics
No
No
Yes
Yes
No
No
Yes
Yes
High School Performance
No
No
No
Yes
No
No
No
Yes
High School Fixed Effects
No
No
No
No
No
No
No
No
Notes: The first four columns of each panel display the results of estimating Equation (1), while the last four columns
display the result of estimating Equation (2). The dependent variable is the median ACT score of incoming freshmen at the
college a student attended. Data come from the AFS. Panel A includes middle- and high-income students (students with

30

family incomes at least $40,000 per year) while Panel B is limited to low-income students (students with family incomes
below $40,000). The first four columns of results include students who only took the ACT, while the last four include
students who took the ACT or the SAT, but not both. Huber-White standard errors are in parentheses. The first column of
each panel adds no controls, the second column adds a linear time trend, and the third column adds controls for
demographics (see footnote 11), and the fourth column adds controls for high school performance (see footnote 11). Two
asterisks indicate the coefficient is significant at the 5% level.

31

Appendix Figure 1. Number of Scores Sent by High School Graduation Year
Including Students Who Sent Zero Score Reports
1a: Students Who Took the ACT
0.8

Fraction of ACT-Takers

0.7
0.6
0.5
Three Score Reports

0.4

Four Score Reports

0.3
0.2
0.1
0
1991

1992

1994

1996

1998

2000

2004

High School Graduation Year

1b: Students Who Took the SAT
0.8

Fraction of SAT-Takers

0.7
0.6
0.5
Three Score Reports
Four Score Reports

0.4
0.3
0.2
0.1
0
1994

1995

1996

1997

1998

1999

2000

2001

High School Graduation Year

Notes: The bars indicate the fraction of each high school class that sent either exactly three or exactly four
score reports. The analysis is not limited to students who sent at least one score report. Data in Panel A
come from the ACT database and data in Panel B come from a database of SAT-takers produced by the
College Board.

32

Appendix Table 1. Descriptive Statistics from the High School and Entering College Classes of 1996
ACT and AFS Data
A. ACT Data
Sent Score Reports
All Students
2.95
2.63

ACT Only

B. AFS Data
SAT Only
All ACT-Takers

Non-ACT-Takers
Score Reports
Applications
2.67
3.93
3.04
3.74
ACT Score Equivalent
20.9
20.9
22.5
24.4
23.3
24.4
Income < $36,000
42%
41%
Income ≥ $80,000
7%
7%
Income < $40,000
34%
29%
31%
31%
Income ≥ $75,000
25%
36%
29%
34%
H.S. GPA
3.08
3.07
3.27
3.32
3.33
3.26
U.S. Citizen
97%
97%
99%
94%
99%
94%
Female
56%
56%
57%
50%
56%
53%
Caucasian
72%
71%
80%
72%
79%
70%
Black
10%
9%
10%
9%
10%
10%
Asian
3%
3%
2%
8%
2%
7%
Hispanic
5%
5%
2%
4%
2%
5%
Other Race
3%
4%
5%
6%
6%
6%
No Race Given
7%
8%
1%
2%
1%
2%
Notes: Each cell presents the mean of the indicated characteristic for the indicated sample. Panel A uses data from the ACT database on
students graduating high school in 1996 while Panel B uses data from the AFS on freshmen starting college in 1996. "ACT Score Equivalent" is
the ACT score of students who took the ACT. For students who only took the SAT, I convert their SAT scores into an ACT score equivalent using
a concordance produced by the College Board.

33

Appendix Table 2. Score-Sending and Application Patterns by Family Income in the Class of 1996
ACT and AFS Data
Middle-Income

A. Dependent Variable: Number of Scores Sent
0.084**
0.092**
0.052**
0.028**
(0.004)
(0.004)
(0.003)
(0.004)

High-Income

0.194**
(0.014)

0.201**
(0.014)

0.153**
(0.013)

0.112**
(0.013)

Constant

2.904**
(0.011)

2.864**
(0.023)

2.108**
(0.370)

2.245**
(0.419)

Observations
R-squared

252,120
0.006

252,120
0.008

252,120
0.028

252,120
0.132

Middle-Income

B. Dependent Variable: Number of Applications Sent
0.094**
0.219**
0.192**
(0.020)
(0.019)
(0.019)

High-Income

0.372**
(0.023)

0.504**
(0.022)

0.464**
(0.022)

Constant

2.538**
(0.015)

2.567**
(0.084)

2.473**
(0.354)

Observations
R-squared

58,968
0.008

58,968
0.045

58,968
0.065

Demographics
No
Yes
Yes
Yes
High School Performance
No
No
Yes
Yes
High School Fixed Effects
No
No
No
Yes
Notes: Panels A and B display the results of regressing the number of score reports sent (Panel A) and
the number of applications sent (Panel B) on indicators for coming from middle- and high-income
families. Regressions in Panel A use data from the ACT and are limited to students in the class of 1996
who sent at least one score report. In this panel, middle-income indicates an annual family income
between $36,000 and $80,000, high-income indicates a family income of at least $80,000, and standard
errors (in parentheses) are clustered at the state level. Regressions in Panel B use data from the AFS
database and are limited to students who took only the ACT and entered college in 1996. In this panel,
middle-income indicates students with family incomes between $40,000 and $75,000, high-income
indicates family incomes of at least $75,000, and robust Huber-White standard errors are in
parentheses. The first column of each panel adds no controls, the second column adds controls for
demographics (see footnote 11), the third column adds controls for high school performance (see
footnote 11), and the fourth column (Panel A only) adds high school fixed effects. When high school
fixed effects are added, the control for attending a private high school is dropped. Two asterisks indicate
that the coefficient is significant at the 5% level.

34

Appendix Table 3. College Selectivity by Family Income in the Class of 1996
ACT and ACS Data
Middle-Income

A. Dependent Variable: Selectivity of Students' Most-Selective College
0.888**
0.809**
0.341**
0.076**
(0.074)
(0.040)
0.0250
(0.020)

High-Income

1.365**
(0.098)

1.267**
(0.069)

0.740**
(0.053)

0.350**
(0.033)

Constant

20.878**
(0.204)

21.878**
(0.250)

16.577**
(0.939)

16.840**
(1.168)

Observations
R-squared

231,480
0.024

231,480
0.051

231,480
0.232

231,480
0.392

Middle-Income

B. Dependent Variable: Selectivity of Students' Least-Selective College
0.616**
0.496**
0.235**
0.036**
(0.086)
(0.054)
(0.042)
(0.014)

High-Income

0.910**
(0.098)

0.775**
(0.085)

0.485**
(0.067)

0.206**
(0.030)

Constant

18.059**
(0.211)

18.774**
(0.288)

17.733**
(0.961)

17.371**
(0.901)

Observations
R-squared

231,480
0.014

231,480
0.042

231,480
0.119

231,480
0.342

Middle-Income

C. Dependent Variable: Selectivity of Attended College
0.698**
0.334**
0.195**
(0.038)
(0.027)
(0.026)

High-Income

1.082**
(0.041)

0.684**
(0.030)

0.527**
(0.029)

Constant

18.394**
(0.031)

19.402**
(0.102)

19.467**
(0.519)

59,390
0.031

59,390
0.188

59,390
0.272

Observations
R-squared

Demographics
No
Yes
Yes
Yes
High School Performance
No
No
Yes
Yes
High School Fixed Effects
No
No
No
Yes
Notes: Panels A, B, and C display the results of regressing the 25th percentile ACT score of incoming
freshmen at the most-selective college a student sent scores to (Panel A), the 25th percentile score of
incoming freshmen at the least-selective college a student sent scores to (Panel B), and the median ACT
score of incoming freshmen at the college the student attended (Panel C) on indicators for coming from
middle- and high-income families. Regressions in Panels A and B use data from the ACT database and
American College Survey and are limited to students in the class of 1996 who sent a score report to a
college for which the ACS has selectivity information. In this panel, middle-income indicates an annual
family income between $36,000 and $80,000, high-income indicates a family income of at least $80,000,
and standard errors (in parentheses) are clustered at the state level. Regressions in Panel C use data from
the AFS and are limited to students who took only the ACT and entered college in 1996. In this panel,

35

middle-income indicates students with family incomes between $40,000 and $75,000, high-income
indicates family incomes of at least $75,000, and robust Huber-White standard errors are in parentheses.
The first column of each panel adds no controls, the second column adds controls for demographics (listed
in footnote 11), the third column adds controls for high school performance (listed in footnote 12), and the
fourth column (Panels A and B only) adds high school fixed effects. When high school fixed effects are
added, the control for attending a private high school is dropped. Two asterisks indicate the coefficient is
significant at the 5% level.

36

Appendix Table 4. Change in the Number of Scores Sent by Family Income
Dependent Variable: Number of Score Reports Sent
Including Students Who Sent No Score Reports
(1)

(2)

(3)

(4)

A. Middle- and High-Income Students
0.461**
0.464**
0.412**
(0.021)
(0.021)
(0.023)

(5)

Class of 1998

0.295**
(0.030)

0.423**
(0.024)

Post-1998

0.370**
(0.025)

0.685**
(0.021)

0.689**
(0.021)

0.653**
(0.018)

0.662**
(0.018)

Constant

2.750**
(0.024)

2.843**
(0.029)

2.613**
(0.054)

1.655**
(0.304)

1.774**
(0.336)

Observations
R-squared

1,067,363
0.016

1,067,363
0.019

Class of 1998

0.411**
(0.024)

B. Low-Income Students
0.551**
0.553**
0.516**
(0.021)
(0.021)
(0.017)

0.527**
(0.018)

Post-1998

0.475**
(0.036)

0.732**
(0.015)

0.733**
(0.015)

0.724**
(0.011)

0.724**
(0.011)

Constant

2.736**
(0.017)

2.799**
(0.022)

2.648**
(0.042)

1.910**
(0.209)

1.967**
(0.198)

Observations
R-squared

891,731
0.037

891,731
0.039

891,731
0.042

891,731
0.063

891,731
0.123

1,067,363 1,067,363 1,067,363
0.021
0.036
0.099

Time Trends
No
Yes
Yes
Yes
Yes
Demographics
No
No
Yes
Yes
Yes
High School Performance
No
No
No
Yes
Yes
High School Fixed Effects
No
No
No
No
Yes
Notes: Each panel displays the results of estimating Equation (1) where the dependent variable
is the number of score reports a student sent. Data come from the ACT database. Panel A
includes middle- and high-income students (students with family incomes at least $36,000 per
year) while Panel B includes only low-income students (students with family incomes below
$36,000 per year). Students who did not send score reports are included. Standard errors (in
parentheses) are clustered at the state level. The first column of each panel adds no controls,
the second column adds a linear time trend, the third column adds controls for demographics
(see footnote 11), the fourth column adds controls for high school performance (see footnote
11), and the fifth column adds high school fixed effects. When high school fixed effects are
added, the control for attending a private high school is dropped. Two asterisks indicate the
coefficient is significant at the 5% level.

37

