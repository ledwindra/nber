NBER WORKING PAPER SERIES

WHEN EDUCATORS ARE THE LEARNERS:
PRIVATE CONTRACTING BY PUBLIC SCHOOLS
Silke J. Forbes
Nora E. Gordon
Working Paper 18185
http://www.nber.org/papers/w18185

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
June 2012

Gordon thanks the National Academy of Education/Spencer Foundation Postdoctoral Fellowship program
for support of this work. The authors thank Israel Malkin for outstanding research assistance. We are
grateful to Julie Cullen, and to seminar participants at the Brookings Institute, Brown University, George
Washington University, Georgetown University, Michigan State University, Stanford University, UC
Irvine, UC San Diego, and the University of Kentucky for helpful suggestions. All opinions expressed
and any errors are our own and do not represent those of the National Academy of Education or the
Spencer Foundation. The views expressed herein are those of the authors and do not necessarily reflect
the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2012 by Silke J. Forbes and Nora E. Gordon. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.

When Educators Are the Learners: Private Contracting by Public Schools
Silke J. Forbes and Nora E. Gordon
NBER Working Paper No. 18185
June 2012
JEL No. H52,I2,L14
ABSTRACT
We investigate decision-making and the potential for social learning among school administrators
in the market for school reform consulting services. Specifically, we estimate whether public schools
are more likely to choose given Comprehensive School Reform service providers if their “peer” schools—defined
by common governance or geography—have performed unusually well with those providers in the
past. We find strong evidence that schools tend to contract with providers used by other schools in
their own districts in the past, regardless of past performance. In addition, our point estimates are consistent
with school administrators using information from peers to choose the plans they perceive to have
performed best in the past. Despite choosing a market with an unusually comprehensive data source
on contracts between public schools and private firms, our statistical power is sufficiently weak that
we cannot reject the absence of social learning.

Silke J. Forbes
Weatherhead School of Management
Case Western Reserve University
10900 Euclid Avenue
Cleveland, Ohio 44106-7235
silke.forbes@gmail.com
Nora E. Gordon
Georgetown Public Policy Institute
306 Old North
37th and O Streets NW
Washington, DC 20057
and NBER
neg24@georgetown.edu

I. INTRODUCTION

In recent years, federal elementary and secondary education policy has become increasingly
activist, utilizing conditional formula-driven (e.g., No Child Left Behind) and competitive (e.g.,
Race to the Top) grants as levers to prompt states, districts and schools to adopt specific policies
that many historically—and vehemently—resisted. Burch (2006) argues persuasively that
accountability policies stimulated demand for goods and services sold by private vendors to
public educational agencies, documenting major increases in revenues of for-profit educational
services providers in the years immediately following the passage of No Child Left Behind.1 At
the same time that federal policymakers have been more willing to dictate the specifics of how
state and local educational agencies should operate, however, they have chosen to regulate the
private markets stimulated by these policies relatively little, relying on competition to lead to
high-quality, fairly-priced inputs into public education. One of the key assumptions underlying
this

logic

is

that

educational

administrators

have

access

to

comprehensive—and

comprehensible—information about the relative quality of the products they consume, so that
low-quality or overpriced vendors eventually are forced to exit the market. Whether such
information exists, how it flows, and if it is used in practice, however, are open questions.
Empirical evidence on how school administrators choose among private vendors is scarce
because data on such choices are rarely available. In the current study, we aim to close this gap
by examining a setting for which such data are available: the Comprehensive School Reform
(CSR) program in Texas public schools from 2000 to 2005. In theory, school administrators
could rely upon analyses generated by professional researchers via the What Works
Clearinghouse or other “report card” types of sources, or they could gather and analyze data,
including objective or subjective measures, directly from their peers or from publicly-available
sources, such as raw data provided online by local and state education agencies. In practice, little
“approved” empirical research is available to guide the highly specific types of decisions
administrators face (see, for example, the What Works Clearinghouse)2, and the limited case
1

Using annual reports, she finds annual revenue increases of 77 percent for firms specializing in test development
and preparation, 46 percent for data management and analysis, 300 percent for remedial services, and 150 percent
for content area specific programming over the period from 2001 to 2004.
2
The What Works Clearinghouse, http://ies.ed.gov/ncee/wwc/, is sponsored by the Department of Education. A
September 9, 2011 search of all its indexed interventions related to academic achievement, the broadest possible
category, returned 71 studies, of which 51 were characterized as based on “small extent of evidence” due to limited

1

study evidence that exists suggests networks of administrators are relatively weak (Daly and
Finnigan, 2010). In this paper we examine the role of social learning by assessing the extent to
which schools chose CSR vendors with whom their peers previously had particularly good
experiences.
Now defunct, CSR was a federal program which awarded one-time grants to public
schools to purchase consulting services from private firms or to develop their own local reform
plans. These products were bundles of relatively complex services, such as teacher training and
curriculum development, whose quality would have been difficult to assess prior to purchase.
Schools chose specific CSR plans, and each plan was provided by its sole firm (the contractor).3
Our empirical analysis relies on a comprehensive database of the CSR providers chosen by
schools which purchased the service with federal grants. To generate measures of perceived plan
quality, we combine these data with information on school-level pass rates from the Texas
Assessment of Academic Skills (TAAS) test, restricting our attention to data already generated
and publicly available at the time of vendor selection for each particular grant.
The problem of choice among experience goods—products with quality that is difficult to
observe ex ante—is not unique to the education context. Research in disparate settings points to a
role for social learning in helping agents to make these decisions, ranging from farmers in
developing countries basing decisions on production technologies on the success (or failure)
peers have had with them (Munshi, 2004; Conley and Udry, 2010) to individuals in the U.S.
using information from their peers in choosing among employer-provided health insurance plans
(Sorenson, 2006). This paper is the first to our knowledge to empirically examine social learning
among public administrators. We expect that administrators may gather and use information
quite differently from households and firms, but the direction of the difference is ambiguous.
Given the professional nature of their appointments, administrators may have skills which reduce
the cost of gathering data from public sources; at the same time, agents may have lesser
incentives to invest in data for optimization than do principals.

numbers of studies and/or sample sizes. Of the remaining 20 studies categorized as based on “medium to large
extent of evidence,” only five were categorized as having positive effects (the remainder had potentially positive
effects, no discernible effects, or mixed effects).
3
Our language throughout refers to choice of a product, firm, or vendor, but the same logic would apply to the
choice of a particular technology to be applied internally. Much of the social learning literature focuses on
technology diffusion rather than choice of firm to supply an experience good.

2

Empirical studies of social learning grapple with Manski’s (1993) “reflection problem:”
agents may make similar choices as their peers not because they are learning from them, but
because they have common characteristics or preferences unobservable to the researcher. Two
institutional features of our empirical setting allow us to minimize such incorrect inference. First,
all public schools in Texas participated in statewide standardized testing with publicly- available
school-level results throughout the time period in our sample, and second, the CSR grants were
awarded in multiple waves. We therefore are able to identify the impact of a plan’s perceived
past performance on its subsequent adoption by peer schools, controlling for its adoption by the
initial school and any correlated unobservables. Our identifying assumption is that common
unobserved characteristics and preferences affect the choice of a plan, but not its correlation with
test score changes once adopted (controlling for school characteristics). We therefore interpret
our estimates as Manski’s endogenous effects—the extent to which an individual’s behavior
varies with the behavior of the group.
Several caveats apply. First, we do not observe actual patterns of communications across
schools or districts. Our setting suggests several plausible characteristics along which relevant
peer groups may form: belonging to the same school district, being geographically proximate to
one another (beyond school district boundaries), or belonging to the same regional professional
development group (Education Service Center, or ESC). A second caveat is that, though we rely
predominately on our use of the peer’s perceived past experience to circumvent the reflection
problem, there is still potential for reflection if some unobserved shared characteristic between
two peer schools gives them a strong match with a particular plan—it would make one school
fare well with it and the other, regardless of its knowledge of the first school’s experience,
choose and thrive with that plan as well. We control for school-level demographics to minimize
this possibility. Our third and most major caveat is that, despite choosing the largest state with
appropriate data available, the large number of CSR vendors relative to the total number of
grants limits our statistical power.
We find strong evidence that schools are much more likely to choose plans that were
used within the same school district in the past, regardless of past performance. This effect could
be due to a number of reasons, besides learning about the existence and the quality of the plan.
For example, there might be fixed costs for the school district or the provider (or both) of
establishing a relationship with each other. We do not find that the prior presence of a plan in

3

either a geographically-neighboring district or within the same regional administrative unit has a
statistically significant effect on a school’s choice of plan. We also find no statistically
significant effect of the perceived quality of the prior experience with a plan in any of the peer
groups we consider. However, the magnitudes of our point estimates would be interpreted as
revealing an economically meaningful role for social learning were they more precisely
predicted.
The remainder of the paper is organized as follows. Section II provides institutional
background on the Comprehensive School Reform Program. Section III describes the empirical
strategy and Section IV the data sources and descriptive statistics. Section V describes our results
and a final section concludes.

II. BACKGROUND ON EDUCATIONAL CONTRACTING AND COMPREHENSIVE SCHOOL REFORM

School and district administrators face many contracting decisions. Using 1997 data, Rowan
(2002) finds over 80 percent of school districts in the U.S. contracted with some outside source
for “professional and technical services in a given year.” Our question of interest—do decisionmakers gather relevant information about the quality of private contractors from their peers, and
subsequently apply it?—requires data on purchases of private contracting services by public
schools, as well as information of the quality of the services that were received. In this paper, we
focus on a specific market for which proxies for both of these types of data are available, the
market for Comprehensive School Reform (CSR) services. CSR is a bundle of school
improvement products emphasizing professional development.4 In practice, a CSR plan could
contain any or all of the following elements: curriculum, school organization or management
strategies, assessments, teaching aids such as software or workbooks, and teacher training to go
with the entire system. Overall the characteristics of the CSR market are similar to Rowan’s
(2002) description of the broader school improvement industry, making it a relevant case despite
the discontinued status of the federal CSR program: there are many firms offering highly
heterogeneous products, low concentration, little regulation and almost no objective information
on firm quality.
4

Formally, the Department of Education (2007) states that “[t]he Comprehensive School Reform program is
designed to foster coherent schoolwide improvements that cover virtually all aspects of a school's operations, rather
than piecemeal, fragmented approaches to reform.”

4

In this section, we delineate the most basic features of the CSR program, with particular
attention to aspects that are relevant to our analysis.5 Schools had to specify their CSR vendor on
their grant application. Each school was eligible at most once for a CSR grant, and the grants
were funded over multiple years. We model the school’s choice of vendor as a function of the
information potentially available to it at the time of the application and actually available to us,
the researchers, now. While schools (via districts) purchased CSR from a variety of funding
sources, including but not limited to the federal grants program, there is no publicly-available
comprehensive accounting of which schools use particular CSR plans and public information
requests are difficult for contracts older than one or two years.6 There is, however, a complete
database of all schools purchasing CSR plans with federal funds from 1997 to 2005, so we
restrict our attention to these schools. That is, we observe how schools with federal CSR grants
choose CSR plans based on their perceptions of the performance of other federally-funded CSR
plans in peer schools.
It is important for our analysis that grants (1) were chosen at the school rather than
district level, and (2) were not awarded systematically based on the provider specified in the
application. Research from case studies (Datnow, 2000), and our own interviews with
administrators suggest that the choice of CSR plan – regardless of funding source – in most
districts was ultimately made at the school rather than district level, with varying levels of
guidance or pressure from the district.7 Data on unsuccessful applications are not available but
by all accounts, it appears that grants were not systematically awarded in any way based on the
provider specified in the application and the formal scoring of grant applications scarcely
recognized empirical research in support of specific plans.8 As Table 1 shows, the schools
5

For CSR program details, see Borman et al. (2003); Berends et al. (2001) discuss the New American Schools
program which led up to it.
6
Datnow (2000) reports over 6,500 schools in 45 states implementing CSR plans (with any funding source) at the
time of her study, so the non-federally funded CSR sector is a significant one.
7
Because we study schools purchasing CSR with federal grants rather than local funds, districts should have less
leverage over school choices in our data than in the period studied by Datnow. Our data reveal significant variation
in plan choice within many districts (see Appendix Table 1).
8
The state education agency staff member in charge of the program reports that no particular models of school
reform were given preference in the selection process (based on scientific evidence, perceived efficacy within the
state, or other factors). While federal requirements post-No Child Left Behind mandated that funds be used for
models with positive evidence from “scientifically-based research” the dearth of such research on these plans makes
the requirement irrelevant in practice. For example, in the 2004 request for applications for the CSR – Texas High
Schools Initiative Program, the application specifies that two out of 100 points are to be awarded based on the extent
to which, “The design of the proposed project reflects up-to-date knowledge from scientifically-based research and
effective practice.” In practice it is difficult to know how even these two points could be awarded given the lack of

5

ultimately awarded CSR grants were disproportionately disadvantaged and low achieving, but
still were a highly varied group.
The CSR awards database provides information on award amounts, revealing significant
heaping around round numbers (most strongly at $50,000, the mandated minimum) and a much
wider range in per pupil grant amounts than in total grant amounts. Because we lack data on
what specific goods and services are included within each contract between a school and a
provider—and the ideal measure, product “quality”—we cannot meaningfully interpret the data
on award amount and limit our analysis to the choice of provider.9 Nationwide, 755 different
plans provided federally-financed CSR services to schools over the period from 1998-2005, over
half of which served only a single school. Nationally, the largest CSR plan was Success For All
with a market share of 6.9 percent, the second-largest plan was Lightspan with a market share of
4.5 percent, and the third largest was Accelerated Schools with a market share of 3.8 percent;
these market shares vary considerably by state, and we restrict our study to Texas based on its
scale and publicly-available school-level test scores.
Grants in Texas were awarded in three distinct waves from 2000 through 2005 to a total
of 124 different plans. As in the national sample, over half of all plans provided CSR to only a
single Texas school. In Table 2, we present the market shares in Texas for the 16 largest plans.
Most of these plans (save the largest two, AVID and Accelerated Schools) have relatively small
market shares in Texas, but nearly all operate at a significant scale nationally. The larger plans
are used in similar numbers across the three grant waves (not shown), whereas some of the
smaller plans are skewed towards earlier or later waves.10 Some of the plans specialize in certain
grade levels; we account for this in our empirical analysis by only including plans in a school’s
choice set that are used at least three times nationally at the school’s grade level (in any wave).
We interpret the pairing of a school and a provider as reflecting the school’s choice,
rather than the provider’s willingness to sell to that particular school over some other school. For
the large plans on which we focus, the data support this interpretation: while many plans did
have particular states which used them more heavily, they each were used at least a few times in
relevant research. Overall 15 of 100 points that year were awarded based on the larger category of “quality of
project services,” further supporting anecdotal evidence that applications were chosen primarily based on the
school’s level of need and capacity to implement CSR plans from any given provider, rather than due to the
particular provider selected.
9
Mean award amounts per school and per pupil by CSR provider do not align with the potential quality proxies we
discuss in Table 2.
10
Most plans are used across all waves at the national level.

6

many states across geographic regions. While we were not able to contact representatives of all
plans, personal communications with plan staff support this interpretation for AVID, where staff
report that new business was not turned away (Ellis, 2011).
In practice, federal dollars have funded quite heterogeneous CSR programs, ranging from
highly specific interventions to those in which it is difficult to define the CSR treatment.11 Such
great variety in CSR program design naturally leads to the question of the heterogeneity in
efficacy of these programs. This is of primary interest to us, because we investigate the extent to
which demand for CSR depends on perceived plan quality. There is little existing large-scale
quantitative evidence on this question, and less still using experimental or quasi-experimental
variation (see Borman et al., 2003 for a review and meta-analysis of CSR research; see Mason,
2005 for a detailed study of five programs in Los Angeles Unified School district).12
Correspondingly, and relevant to this study, there has been little technical guidance offered to
schools facing the choice of CSR provider.13 Both Borman et al. (2003) and Mason (2005)
emphasize difficulties in faithful implementation of CSR plans; we view this as a critical
example of how the match between the school and provider matters beyond some constant
measure of provider quality. The setting we study in this paper is one in which schools actively
chose their CSR providers, posing two empirical difficulties. To the extent that school-plan
match matters, we can only identify the effect of the plan in schools which revealed their
expectations of having a good match with it by buying it; to the extent that plan quality has a
uniform component across schools, we cannot identify the effect of the provider on achievement
as distinct from the effect of having an administrator savvy enough to choose it. Throughout the
paper, our emphasis is therefore on CSR plan quality as perceived by school administrators, as
opposed to a true causal measure of such quality.

11

At one end of the CSR spectrum are programs like the Coalition for Essential Schools: on its website, it describes
“school reform as an inescapably local phenomenon” and acknowledges that “no two Essential schools are alike.” It
instead emphasizes the shared commitment to a set of principles that are both relatively uncontroversial and vague,
such as personalized instruction and an atmosphere of trust and high expectations. In contrast, Success For All
(SFA) is an example of a tightly-scripted program, with teachers adhering to a prescribed curriculum and methods,
rhyming mnemonics included. In order to create this uniformity, SFA must provide many more curricular,
assessment, and training materials than would a plan emphasizing locally-specific needs.
12
Mason finds that, of the five programs in his study, none led to uniform improvements in achievement. He and
others (Vernez et al., 2006) also found that very few schools fully implemented their CSR plans; this paper, like the
bulk of the literature on CSR, studies CSR as implemented in practice rather than the ideal plans as initially
developed by New American Schools and related research efforts.
13
See Borman et al. (p. 130) for discussion of several “practitioner-oriented reviews, or ‘catalogs.’”

7

The last two columns of Table 2 present two measures of potentially perceived plan
quality. One of these measures is a rating produced by the American Institutes of Research
(1999) meant to help administrators in precisely this situation. The AIR study considered the
largest plans nationally; as Table 2 shows, many of the largest plans in Texas were not included.
The AIR study reviewed the evidence plan by plan; in most cases it concluded not that plans
were effective or not, but rather that they lacked strong research bases. The other measure is our
own estimate of the correlation between the plan—as endogenously adopted—and achievement
gains (essentially a flawed value-added measure).14 Based on both these potential proxies for
quality, the “best-performing” plans clearly do not have the largest market share. This is not
necessarily indicative of an information failure in the market since it is likely that neither of these
measures captures the plans’ true quality, but it does suggest that other measures—like peer
experiences—could matter.15
In summary, administrators choosing CSR plans had relatively little “scientific” research
evidence to guide them. Existing research in other institutional settings suggests that even when
such evidence is available, the transaction costs of obtaining and processing it—“comparison
frictions”—serve as a significant deterrent, even when such costs are low (Kling et al., 2011).
Social networks could thus provide a rare source of information to administrators faced with
contracting decisions. The limited research available on such networks of education
administrators, however, suggests relatively little communications across schools, other than
those facilitated by district staff (Daly and Finnigan, 2010); no such research directly investigates
the extent of networks across districts, but given their limited nature within districts, we expect
that the additional costs of building and maintaining networks across district lines would result in
quite limited communication across districts unless facilitated by state or regional administrators.

III. EMPIRICAL APPROACH

Our empirical approach is built on the assumption that school administrators have the objective
to improve their students’ achievement. This measure is consistent with an objective function
formed by altruism as well as with one influenced by incentives imposed by accountability
14

We discuss in detail how we arrive at this estimate in section III below.
Given the variation in observed achievement gains, one might wonder if the decision to adopt any CSR plan at all
also depends on the peer’s prior experiences. We have investigated this but found no evidence for such effects.

15

8

regimes such as the one relevant in our sample. We assume that a given CSR plan has an average
(however noisy) plan-specific effect on achievement, as well as a school-specific match
component to its efficacy. We expect schools to form expectations of CSR plan quality from
communications with their peers and, following the literature on social learning (e.g. Ellison and
Fudenberg, 1993), anticipate that schools may rely on “rules of thumb” to limit the quantity of
information to be processed because the expected benefit of the information may exceed the cost
of obtaining it. School-level achievement data were publicly available and highly salient over
this time period; while schools’ plan choices were publicly available, they were available from
the Southwest Educational Development Laboratory, not the state, and were targeted towards
researchers. We expect the transaction costs of learning of and obtaining relevant data from these
two sources and merging them to be sufficiently high as to rule out this possibility for nearly all
administrators.
Our empirical model, the conditional logit, allows plan choice to be determined by both
school and plan attributes. The model controls for school-level fixed effects. We also allow key
school characteristics—the fraction of its students who were white, and its average pass rate
across math and reading, both measured at baseline—to affect the likelihood of choosing each
CSR plan differentially. Our independent variables of interest are measures of school-level
potential exposure to information about individual plans’ observed correlations with achievement
changes in other “peer” schools.16

Depending on our specification, we allow exposure to

information to come from various sets of peers, starting with other schools in the same district,
then schools in geographically-neighboring districts and in common administrative regions, but
in all cases limiting it to observations with publicly-available achievement and CSR provider
data by the time the new grantee was choosing a provider.
We estimate school plan choice using the conditional logit specification in equation [1]
below. We denote schools with subscript i and plans with j, where the sample of schools is
restricted to those schools with CSR grants choosing a provider for the first year of their grant.
All schools receiving grants are thus in the dataset only once and schools which never receive
grants are not included in this specification.
′

[1]

∑∀

′

16

We treat the prior experience with a plan as a plan attribute that varies across schools in the conditional logit
model.

9

The school-level independent variables of interest described above are included in xij in equation
[1]. Because we have a large number of plans that are chosen by very few schools, we do not
include all of them as separate choices in the conditional logit model that we estimate. Instead,
we define a category that we call “other” for plans that are infrequently chosen. The choice set of
plans for a given school thus consists of a number of individual large plans and “other.” In
constructing our definition of a large plan, our goal is to identify plans which (1) are not supply
constrained and therefore likely constitute a viable option for a school with a new grant,17 and (2)
have had the opportunity to generate results which new grantees may observe and use to inform
their plan selection. For our main specifications, we define large plans as plans that have been
used at least six times in Texas, in at least two different counties and in at least two of the three
waves. Table 2 shows that 41 percent of all schools in our sample chose plans in the “other”
category.
Our use of the conditional logit model relies on the “Independence of Irrelevant
Alternatives” (IIA) assumption. This could be a particular concern because we group the smaller
plans together in a single category. We have investigated the robustness of our empirical results
by varying the cutoff for the minimum number of times that a “large” plan must have been used
between six and ten and find that our results are robust to this variation.
Approximating plan quality as perceived by administrators poses a significant challenge.
The ideal measure of true plan quality would capture its causal effect on school achievement,
again assuming that administrators care about pass rates on the state test during a strong
accountability regime; no such measure is available because very few plans have been subject to
high quality evaluations and the correlation between plan choice and achievement reflects the
endogenous choice of plan by the school.18 Given these limitations, we pursue two separate
strategies to estimate perceived plan quality. Both strategies assume that administrators base
inference on the correlation between (endogenous) plan choice and student outcomes.19
17

All of these “large” providers operate in multiple states, and each of them operates across multiple non-contiguous
Texas counties.
18
Research attempting to link plan attributes to outcomes over a sample of large plans nationally concludes that a
plan is not the sum of its parts (Borman, 2003). We do not attempt such analysis due to this finding, combined with
the vague descriptions of CSR plans and subsequent difficulty in even determining what those hedonic values
should be.
19
It is possible that some administrators are aware, intuitively if not in a formal statistical sense, that this method
would not identify plan efficacy if the most effective administrators are likely to have selected the most effective
plans. Lang (2010) offers persuasive anecdotal evidence suggesting that such intuitive understanding is in short
supply in this sector; we have not seen a comprehensive investigation of this issue in the literature.

10

In our first approach, we calculate the raw change in achievement (pass rates) before and
after the CSR intervention in the peer school(s). We view this as a plausible input into a rule of
thumb that administrators could use.20 For example, when we consider other schools in the same
district to be the relevant peer group, we calculate the mean pass rate change over all schools in
the district which have used the plan in the past. The plan-specific attribute used in the
conditional logit estimation is then the interaction of this mean pass rate change with an indicator
variable for whether the plan has been used in peer schools in the past. This measure would be
appropriate if the school administrator either learned directly from her peers about achievement
levels before and after the plan was implemented, or if she looked up pass rates for the relevant
peer school(s). Because the mapping of school to CSR plan was more difficult to access, we
assume these data come from social interactions.
In the second approach, we compute an estimate of perceived CSR plan quality based on
more observations (all schools who have used the plan statewide rather than only among peer
schools) and adjusted for school characteristics. Specifically, we estimate equation [2] below:
[2]
The pass rate at school i using CSR plan (equivalent to CSR model or firm) j in period t is
predicted by time-varying school characteristics Xit as well as school-, plan- and year-specific
fixed effects. The coefficients for these plan fixed effects,

, are reported in Appendix Table 2

for several specifications varying in the control variables included at the school level. We
describe the data used to generate these perceived value-added measures in greater detail in the
following section. Notably, most plans (including the two largest plans) have statistically
significant and positive coefficients; however, as Table 2 shows, the ranking of plans by these
coefficients does not align closely with the market share of the plans. We are not particularly
surprised by this lack of correlation in the aggregate, given the documented importance of match
quality in the effectiveness of CSR plans. We choose to include this strategy alongside the one
using raw achievement changes in peer schools because these estimated plan fixed effects are
based on more observations, potentially mitigating the noise in the other approach. Again, in
constructing the plan-specific attributes for the conditional logit estimation, we interact these
plan fixed effects with prior use of the plan by a peer, to approximate a setting in which
20

We have explored other potential measures, such as the levels of pass rates, and standardized changes in pass
rates, with qualitatively similar results.

11

administrators only learn of these estimated effects if the plan has been used by a peer in the
past. This approach would identify social learning if (1) the plan fixed effects from our statewide
regression correspond to plan quality as estimated from administrators who have used the plan in
their schools—a question the literature to date does not allow us to answer—and (2)
administrators share their perceptions of plan quality with their peers, as defined by our various
measures.

IV. DATA SOURCES AND DESCRIPTIVE STATISTICS

We focus our analysis on a single state, Texas, for several reasons. Most importantly, because
plans align themselves to some extent with state curricular standards, there are significant crossstate differences in plan popularity. Furthermore, because Texas began testing students and
collecting achievement data relatively early, before the start of the federal CSR program, we can
establish a school’s initial achievement level before the intervention. More than 60 percent of the
Texas schools ever awarded federal grants could potentially observe at least one year of test
score data from other schools implementing CSR with earlier grants.21 Finally, Texas schools
were under a high-stakes accountability regime at the time of the federal CSR grants.
Administrators should be most willing to bear the costs of establishing or using social networks,
or of collecting data via other mechanisms, under such a regime.
We obtain data on federal CSR grants from the Comprehensive School Reform Awards
Database. This database was collected and maintained by the Southwest Educational
Development Laboratory (SEDL), a largely federally- funded not-for-profit research firm. SEDL
collected these data from state CSR administrators and maintained an online database.22 The
federal Department of Education later took over responsibility for this database, and then (after
the program ended and no schools were receiving new grants) removed it from the Internet.23
21

The first year of implementation (for first-wave schools) was the 1999-2000 school year, and accountability
ratings for that year were released on August 17, 2000. The deadline for applications for the second wave of grants
was December 15, 2000, and the corresponding first year of implementation was 2001-2002. Accountability ratings
from the first year of implementation for second-wave schools were released on August 1, 2002, and the
applications for the final wave of grants were due March 15, 2004 and October 7, 2004 (the later deadline was for
high schools).
22
For a small minority of the schools in this database, the name of the CSR provider was missing. We obtained this
information through a Public Information Request from the Texas Education Agency.
23
See http://csrprogram.ed.gov/ for details on accessing the offline data. All data in the paper are public and
available from the authors upon request.

12

The CSR Awards Database is organized at the school level of observation; the grants were three
years in duration, and no school received more than one grant during the seven year span of the
program, so this is essentially a school-grant level of observation. For each school, we use data
on the “models used” (equivalent to the CSR firm with whom the school contracted, and in most
cases, only one model) and the first year of the grant. Each school is identified with its unique
twelve-digit National Center for Education Statistics (NCES) identification number, allowing us
to link these data with the racial and ethnic composition of each school from the NCES Common
Core of Data (Table 1).
The Texas Education Agency’s Academic Excellence Indicator System contains annual
information on each school in Texas, including the percentage of participating students at each
school who passed the statewide achievement test in that year (“pass rate”) 24 and the percentage
of students at the school who are exempt from the test.25 Pass rates are reported separately for
math and reading. We use the average of those two pass rates averaged over all schools meeting
a particular peer group definition as our “naïve” measure of school performance; we also use this
averaged pass rate as the dependent variable in computing a plan’s perceived value-added. These
are also reported in Table 1.
To more fully explore the mechanism for dissemination of information, we have gathered
data on the composition of Education Service Centers (ESCs) within Texas. These provide a
regional layer between the state and local education agencies, and primarily provide professional
development and technical assistance. There are 20 centers statewide, with each center serving
multiple (contiguous) counties. They therefore constitute networks of administrators that are both
plausible (teachers and administrators may attend professional development programs physically
provided at the ESC) and observable.
In constructing our regression sample, we start with all schools in Texas which ever
received a federal CSR grant. We drop any schools categorized in the Common Core of Data as
special education, vocational, or alternative. We also drop schools which are missing in one of
our three data sets. Next, we drop all schools in four districts in El Paso because El Paso had a
local CSR initiative which schools in these districts were very likely to choose. This yields a
24

Pass rates describe only one part of the distribution, and administrators may well care about other points in the
distribution more (for example, if the parents of higher-achieving students are more politically vocal). The pass rate
is the most consistently reported measure in the data, however.
25
Cullen and Reback (2006) provide evidence suggesting that schools use exemptions strategically to improve their
accountability ratings.

13

total of 433 Texas schools that receive CSR grants. Some of these schools use their CSR grants
for more than one provider, and in these cases we include one observation for each school-grant
combination. The total number of school-grant combinations in our sample is 497. Of these, 136
are in the first wave (1999-2000 academic year), 201 in the second (2001-2002 academic year)
and 160 in the last (2004-2005 academic year). We include schools in the first wave in the
sample to help identify the effect of school demographics on plan choice.
The values of our plan-specific explanatory variables vary by school, conditional on plan,
and relate to the different concepts of peer or neighboring school. We include these variables to
reflect both the transmission of information (if “peer” administrators share access to qualitative
or quantitative data) and the issue of “fit” (even if administrators have complete information,
they may weight it more heavily if generated in a context similar to their own) but do not have
any way of disentangling these two effects. In this vein, we have indicator variables for whether
the plan was used in the past by another school in the same district (this was the case for 4
percent of plans chosen after the first wave), by a school in a geographically-neighboring district,
defined as one with its centroid within a 50-mile radius of the centroid of the school’s own
district (true for 30 percent of plans chosen after the first wave), or by a school in a common
ESC (true for 23 percent of plans chosen after the first wave).26
Table 3 above compares measures of perceived quality available to schools with new
grants at the time they chose their providers, depending upon their peer groups. Among plans
previously used by schools within a choosing school’s district or ESC, those plans not chosen
were associated with slightly higher achievement gains on average, without adjusting for any
covariates. For plans used by schools in geographically-neighboring districts, the difference in
means takes the opposite sign. However, the standard deviations on these variables are quite
large and the differences are not statistically significant.

V. RESULTS

We estimate a school’s choice of plan with the conditional logit model described in equation [1];
the coefficients of interest are the plan-specific attributes which depend on the past use of the

26

Our results are robust to defining “geographically-neighboring districts” instead as districts within a 100-mile
radius.

14

specific plan among schools in the relevant peer group. All specifications also include controls
for the school’s initial pass rate and its percentage of white students, interacted with dummies for
the plans in the choice set, but we do not report these coefficients due to space constraints. Table
4 presents the coefficient estimates and standard errors from our regressions. Panel A shows
results using the change in raw pass rates as the measure of perceived quality; Panel B presents
the same specifications using the estimated value-added from equation [2] as a measure of
perceived quality instead.27 We find nearly identical results in both panels.
Plans that were used within the same school district in the past are significantly more
likely to be chosen by other schools in the same district. Prior experience within the own district
could affect future plan choice for a number of reasons, besides learning about the existence and
the quality of the plan. CSR providers may face fixed costs of establishing a presence within a
given district and therefore concentrate their marketing efforts to schools within a subset of
districts. District administrators may face fixed costs in dealing with each individual CSR
supplier and pressure school administrators to choose correspondingly. Large districts may be
able to exert market power in negotiating with providers if several schools within the district
choose the same plan; in this case, the CSR provider may offer enhanced services to schools in
the district such that schools would be more likely to choose that provider even without pressure
from the district. Finally, districts could choose plans initially and repeatedly due to high districtplan match quality that is both unobservable to the researcher and not necessarily reflected in
short-term achievement gains. We are not able to test these explanations, but any of them would
favor multiple schools within the same district contracting with the same plan. The welfare
implications of this inertia depend on the optimality of the initial choice, not just for the school
making it but for schools using that information to inform their own choices. Unfortunately, due
to schools’ own selection of plans, we cannot infer the optimality of the initial plan choice at its
own school; even if we could, the likely importance of the match between provider and school
would prevent us from interpreting how good this choice turned out to be for peer schools
choosing in later waves.
To get a sense for the magnitude of the effect of past plan use within the district, we
compute the predicted choice probabilities for the largest two plans, AVID and Accelerated
Schools, for the case where the district had no prior experience with the plan and compare it to
27

The change in raw pass rates and the value-added measures enter the regression model demeaned.

15

the case where the district had prior experience with the plan and the quality of the experience
was equal to the mean, and find quite large changes in predicted probabilities. We find for
specification (B1), for example, that the choice probability for AVID would increase from 10.7
percent to 25.2 percent and the choice probability for Accelerated Schools would increase from
6.5 percent to 16.4 percent. While we expect that some past experiences are positive, we also
expect others to be negative, so we do not interpret the correlation between past use of a plan
within a district and increased likelihood of future adoption as necessarily indicative of evidencebased decision making.28
When we look at the effect of the perceived quality of prior experience with a plan in the
same district, we find that the estimated coefficients on either of the perceived quality measures
we use are statistically insignificant across all specifications. Ai and Norton (2003) explain how
the sign and statistical significance of the marginal effect of such interaction terms vary with the
values of covariates and cannot be inferred from the model coefficients. We therefore simulate
the change in choice probabilities that our model would predict for a one percentage point
increase in the perceived quality of a plan (e.g., change in pass rate), conditional on the school’s
relevant peer group having any prior experience with the plan. Table 5 shows the results of these
simulations, which generate large confidence intervals that include zero. We therefore cannot
rule out that there is no effect of improving the perceived pass rate or value added on the
likelihood that a school will choose a given plan. We find that a typical increase in choice
probability would be 0.43 percentage points in the case where we use the change in pass rates as
the measure of perceived quality and 0.68 percentage points when we use our estimated value
added measure. These effects would be interpreted as economically meaningful were they more
precisely predicted, given the range of market shares across plans, of changes in pass rates (see
Table 3) and of value added (see Table 2).
In specifications (A2) and (B2), we investigate the effect of the past presence of a plan
and the perceived information about the plan in a geographically-neighboring district, while still
controlling for past experience in the school’s own district. We find a positive point estimate on
the dummy which captures prior presence of the plan in a neighboring district and on the
perceived quality of the plan, but the coefficients are very imprecisely estimated and we cannot

28

This is a direct result of each school receiving only one opportunity to choose a vendor with the grant, as opposed
to a consumer-producer match observed in equilibrium, such as the employee-health plan match in Sorenson (2006).

16

rule out the lack of any effect. This is confirmed by our simulations in Table 5, which predict
that choice probabilities would increase when the perceived effectiveness of a plan increases, but
none of the increases in choice probabilities are statistically different from zero.
We find very similar results in specifications (A3) and (B3), where we consider the own
district and the Education Service Centers as relevant peer groups, and in specifications (A4) and
(B4), where we control for information that might come either from the school’s own district, its
geographic neighbors, or the Education Service Centers. In all of these specifications, the only
statistically significant effect comes from the dummy for past presence of the plan in the school’s
own district.
In results available upon request, we have performed a number of robustness checks to
confirm that our results are not sensitive to the way we treat small plans in the “other” category.
We have varied the cutoff for the minimum number of times that a plan must have been chosen
within Texas to be considered “large” from 6 to 10 (using 6 in our preferred specifications
presented). We also have varied the measures of perceived plan quality for “other” plans. In the
regressions we report, the plan quality for any plan in the “other” category is equal to the peer’s
past experience with plans in the “other” category in the past. Alternatively, we have assigned
the means of past experiences across all plans to any plan in the “other” category, and we find
that our results remain qualitatively the same. We have also investigated whether schools might
learn from other schools that share similar demographic characteristics (such as having a high
percentage of students with limited English proficiency, having a low initial pass rate, or being in
a particularly large school district) and have not found any evidence of social learning from these
alternative potential peer groups.

VI. DISCUSSION

The question of how school administrators use data to inform decisions is more salient now than
ever due to the magnitude of federal funds currently available to education contractors via the
Title I, Investing in Innovation (i3), and Race to the Top programs, among others. Rudy Crew,
the former New York City schools chancellor and now a contractor paid from Title I funds,
characterized the supply response to Title I School Improvement Grants (SIGs), funded at over
half a billion dollars per year, as “like the aftermath of the Civil War, with all the carpetbaggers

17

and charlatans,” (Dillon 2010). Unsurprisingly, some firms previously receiving CSR grants are
now also marketing themselves as specialists in school turnaround and transformation (specific
options available to schools with SIGs).29 Funds can flow directly from the federal government
to educational service providers through i3, which granted Success for All (SFA) nearly $50
million to scale up its programs in its 2011 competition. SFA, in turn, is now offering $50,000
(which was the minimum award per school under the federal CSR grants over ten years ago) to
each of over 150 elementary schools to cover first-year implementation costs.
More broadly, our question fits into the literature about the extent to which policymakers
can help agents—whether embedded in households, firms, or public agencies—optimize their
decisions through dissemination of information versus more direct limitations on choice,
including regulation of choice sets and setting defaults. Recent studies have shown that consumer
demand does respond to highly publicized and easily digestible information—basically, front
page news (see Freedman, Kearney and Lederman, 2012, on toy recalls and Simonsohn, 2011,
on Consumer Reports warnings on child safety seats). The health literature has identified
consumer responses to perceived provider quality in a variety of settings, even when perceived
quality may be falsely informed by considering patient outcomes in the absence of appropriate
adjustments for patient characteristics (for one example, see Howard, 2005, on patient choice of
kidney transplant facility). Recent experimental work examining the use of information that
consumers must actively gather themselves, either from online sources or calling public
agencies, concludes that such costs pose significant deterrents (Kling et al., 2011; Hastings and
Weinstein, 2008).
How government can most help agents make sound decisions, whether about
technologies to be implemented from within or goods or services to be purchased from private
vendors, depends on how administrators access and use information. Obtaining access to data on
such specific purchasing or policy choices at the school level is quite challenging, and
unfortunately we lack sufficient statistical power here to identify clear patterns. We thus
conclude by briefly discussing research strategies to help inform policy.
In order to understand how information flows, one must first identify the information
itself, in this case by better understanding the educational production function. Once researchers

29

See, for example, Success for All <http://www.successforall.net/Turnaround/turnaround.html>.

18

have identified actual best practices, one could follow the approaches taken by Hastings and
Weinstein (2008) or Kling et al. (2011) and randomly assign some school (administrators) to a
treatment group in which they receive highly-accessible, low-cost information related to an
upcoming policy choice, while a control group receives guidance on pursuing a higher-cost route
to information. If the group with more processed information responds to it by making more
evidence-based choices, this could suggest a cost-effective policy. If school administrators
appear unresponsive to such data, policymakers might consider more active policies such as
setting defaults or regulating choice sets.

19

REFERENCES
Ai, Chunrong and Edward C. Norton. 2003. “Interaction Terms in Logit and
Probit Models.” Economics Letters 80, 123‐129.
American Institutes for Research. 1999. An Educators’ Guide to Schoolwide
Reform. Washington, DC: Educational Research Service.
Berends, Mark, Sheila N. Kirby, Scott Naftel, and Christopher McKelvey. 2001.
Implementation and Performance in New American Schools: Three Years
into Scale-up. Washington, DC: RAND.
Borman, Geoffrey D., Gina M. Hewes, Laura T. Overman, and Shelly Brown.
2003. “Comprehensive School Reform and Achievement: A Metaanalysis.” Review of Educational Research 73(2), 125-230.
Burch, Patricia E. 2006. “The New Educational Privatization: Educational
Contracting and High Stakes Accountability.” Teachers College Record
108(12), 2582–2610.
Conley, Timothy G. and Christopher R. Udry. 2010. “Learning about a New
Technology: Pineapple in Ghana.” American Economic Review 100(1),
35-69.
Cullen, Julie Berry and Randall Reback. 2006. “Tinkering toward Accolades:
School Gaming under a Performance Accountability System.” In T.
Gronberg and D. Jansen, eds. Improving School Accountability: CheckUps or Choice, Advances in Applied Microeconomics 14 (Amsterdam:
Elsevier Science), 1-34.
Daly, Alan and Kara Finnigan. 2010. “Understanding Network Structure to
Understand Change Strategy.” Journal of Educational Change 111, 111138.
Datnow, Amanda. 2000. “Power and Politics in the Adoption of School Reform
Models.” Educational Evaluation and Policy Analysis 22(4), 357-374.
Department of Education. 2007. “About CSR.”
http://www.ed.gov/programs/compreform/2pager.html, accessed
November 29, 2007.

20

Dillon, Sam. 2010. “Inexperienced Companies Chase School Funds.” New York
Times, August 10, 2010, A11.
Dillon, Sam. 2010. “U.S. Awards Millions for School Innovations.” New York
Times, August 6, 2010.
Ellis, Rosemary. 2011. Personal communication (e-mail) with the authors.
Ellison, Glenn and Drew Fudenberg. 1993. “Rules of Thumb for Social
Learning.” Journal of Political Economy 101(4), 612-643.
Freedman, Seth, Melissa Kearney, and Mara Lederman. 2012. “Product Recalls,
Imperfect Information, and Spillover Effects: Lessons from the Consumer
Response to the 2007 Toy Recalls.” Review of Economics and Statistics
94(2), 499-516.
Hastings, Justine S. and Jeffrey M. Weinstein. 2008. “Information, School
Choice, and Academic Achievement: Evidence from Two Experiments.”
Quarterly Journal of Economics 123(4), 1373-1414.
Howard, David H. 2005. “Quality and Consumer Choice in Healthcare: Evidence
from Kidney Transplantation.” B. E. Journal of Economic Analysis and
Policy 5(1), Article 24.
Kling, Jeffrey R., Sendhil Mullainathan, Eldar Shafir, Lee Vermeulen, and Marian
V. Wrobel. 2012. “Comparison Friction: Experimental Evidence from
Medicare Drug Plans.” Quarterly Journal of Economics 127(1), 199-235.
Lang, Kevin. 2010. “Measurement Matters: Perspectives on Education Policy
from an Economist and School Board Member.” Journal of Economic
Perspectives 24, 167-182.
Manski, Charles F. 2000. “Economics of Social Interactions.” Journal of
Economic Perspectives 14(3), 115-136.
Mason, Bryce. 2005. “Achievement Effects of Five Comprehensive School
Reform Designs Implemented in Los Angeles Unified School District.”
Pardee RAND Graduate School dissertation.
Munshi, Kaivan. 2004. “Social Learning in a Heterogeneous Population.” Journal
of Development Economics 73, 185-213.

21

Rowan, Brian. 2002. “The Ecology of School Improvement: Notes on the School
Improvement Industry in the United States.” Journal of Educational
Change 3, 283-314.
Simonsohn, Uri. 2011. “Lessons from an Oops at Consumer Reports: Consumer
Follow Experts; Ignore Invalid Information.” Journal of Marketing
Research 48(1), 1-12.
Sorenson, Alan T. 2006. “Social Learning and Health Plan Choice.” RAND
Journal of Economics 37(4), 929-945.
Vernez, Georges, Rita Karam, Louis T. Mariano, and Christine DeMartini. 2006.
“Evaluating Comprehensive School Reform models at Scale: Focus on
Implementation.” RAND Corporation Monograph.

22

Table 1: Comparing Ever- and Never-CSR Elementary Schools in Texas,
Spring 1998
Variable
Demographics
Percent white
Percent Hispanic
Percent black
Percent Asian
Percent limited English proficiency
Percent special education
School enrollment (in thousands)
Number of schools in the district
Distribution of pass rates
Percentile of pass rate:
5
25
50
75
95
Mean pass rate
Observations

Ever-CSR Schools
Mean
s.d.
29.4
(28.4)
48.8
(34.0)
20.0
(24.4)
1.6
(3.0)
18.5
(20.0)
12.5
(4.7)
0.867
(0.633)
51.2
(72.5)

61.2
71.8
79.6
86.3
94.0
78.5
372

Never-CSR Schools
Mean
s.d.
51.4
(31.4)
33.3
(30.5)
13.2
(19.3)
1.8
(3.7)
11.3
(16.8)
12.9
(4.9)
0.599
(0.439)
35.5
(60.7)

68.3
81.8
88.5
93.3
97.6
86.4
5565

Sources: CSR Awards Database, NCES Common Core, Texas Education Agency AEIS.

23

Table 2: CSR Plans in Texas and Nationally
CSR Plan
Accelerated Schools
AVID
Co-nect
Success for All
High Schools That Work
Lightspan
HOSTS
Coalition of Essential Schools
Direct Instruction
Modern Red Schoolhouse
Creating Independent Student-owned Strategies
Literacy Collaborative -- Ohio State University
Breaking Ranks
Effective Schools
Renaissance Learning
Success-in-the-Making
other
Total

Total
Grants in
Texas

Market
Share in
Texas

85
67
17
16
14
14
13
11
10
8
7
7
6
6
6
6
204
497

17.1%
13.5%
3.4%
3.2%
2.8%
2.8%
2.6%
2.2%
2.0%
1.6%
1.4%
1.4%
1.2%
1.2%
1.2%
1.2%
41.0%
100.0%

AIR rating of
Total
Estimated Value
Grants elementary CSR
2
Added
1
Nationally
plans
272
moderate
3.170**
(0.654)
69
5.075**
(0.702)
177
limited
9.260**
(1.409)
443
moderately strong 4.789**
(1.362)
159
6.715*
(2.697)
285
4.406**
(1.326)
60
3.462+
(1.854)
162
zero
2.573
(1.569)
151
moderately strong 8.274**
(1.326)
101
limited
6.743**
(2.608)
17
0.252
(1.449)
101
limited
1.031
(1.673)
12
5.324
(4.363)
151
6.680**
(1.768)
160
14.811** (2.363)
29
-0.855
(2.740)
-4.446**
(0.480)
Mean: 4.560
6476

Sources: CSR Awards Database, AIR.
1
Zero rating means no evidence "was of sufficient quality to be counted as reliable evidence."
2
Value-added regressions are shown in Appendix Table 2. The estimates presented here come from column 6 of that table.
Standard errors are shown in parentheses.
** p<0.01, * p<0.05, + p<0.1

24

Table 3: Mean Pass Rate Changes, Conditional on Having Prior Experience
Source of prior experience

Mean pass rate change Mean pass rate change
for plans that were for plans that were not
chosen
chosen

Own District

5.68
(3.21)
(N=79)

6.69
(4.09)
(N=249)

Geographic Neighbor (50 mile radius)

4.39
(3.21)
(N=227)

4.28
(3.73)
(N=2306)

Education Service Center (ESC)

3.80
(3.99)
(N=222)

4.02
(4.33)
(N=1709)

Notes: T able presents mean values of prior experience within peer group, conditional on having had any experience
with the plan within the peer group. T he level of observation is the school-plan combination. Schools in
geographically neighboring districts are defined as those in other districts with centroids within a 50-mile radius of
own district’s centroid. Standard deviations in parentheses. Number of total observations is given by N; there are
21 plans.
Sources: CSR Awards Database, Common Core of Data and T exas Education Agency Academic Excellence Indicator
System.

25

Table 4: Results from the Conditional Logit Model
Panel A: Prior Experiences Measured by Changes in Raw
Pass Rate

Variable
means 1

(A1)

(A2)

(A3)

(A4)

0.039

1.026**

1.020**

1.013**

1.022**

(0.193)

(0.380)

(0.383)

(0.388)

(0.386)

Past presence of plan in own district

0.250

0.020

0.022

0.019

0.020

(0.811)

(0.075)

(0.076)

(0.073)

(0.075)

Past presence X change in own district pass rates

0.300

0.195

0.184

(0.458)

(0.278)

(0.306)

1.284

0.003

0.017

(2.027)

(0.029)

Past presence of plan in geog. neighbor district
Past presence X change in geog. neighbor pass rates

0.080

0.026

(0.420)

(0.272)

(0.305)

0.912

-0.013

-0.020

(2.061)
Variable
means 1

(0.021)

(0.029)

(B1)

(B2)

(B3)

(B4)

Past presence X change in ESC pass rates
Panel B: Prior Experiences Measured by Plan's Value
Added

(0.040)

0.229

Past presence of plan in ESC

0.039

1.030**

1.035**

0.996*

1.022**

(0.193)

(0.391)

(0.392)

(0.403)

(0.395)

0.176

0.031

0.020

0.014

0.040

(0.989)

(0.145)

(0.146)

(0.145)

(0.141)

Past presence of plan in own district
Past presence in own district X plan's value added

0.300

0.235

0.080

(0.458)

(0.268)

(0.289)

Past presence of plan in geog. neighbor district

1.549

0.095

-0.033

(2.952)

(0.059)

(0.079)

Past presence in geog. neighbor X plan's value added

0.229

0.105

0.190

(0.420)

(0.268)

(0.291)

Past presence of plan in ESC

1.039

0.023

0.109

(2.224)

(0.064)

(0.068)

7,488

7,488

Past presence in ESC X plan's value added
Observations

7,488

7,488

** p<0.01, * p<0.05, + p<0.1
Notes: Results from conditional logit regressions. Standard errors are clustered at district level. Each specification includes plan dummies and
the school's initial pass rate and its percentage of white students interacted with plan dummies. All "change in pass rates" variables are
calculated only over schools in the relevant peer definition which used the particular CSR plan in the past. Changes in pass rates and value
added enter the regression model demeaned.
Sources: CSR Awards Database, Common Core of Data and T exas Education Agency Academic Excellence Indicator System.
1

Standard deviations in parentheses.

26

Table 5: The Effect of Information on Choice Probabilities
Specification
(A1/B1)
(A2/B2)
(A3/B3)
Prior Experiences Measured by Changes in Raw Pass Rate
Mean
0.430
0.225
0.087
5th percentile
0.265
0.001
-0.151
Median
0.434
0.025
0.075
95th percentile
0.494
0.596
0.455
Prior Experiences Measured by Plan's Value Added
Mean
0.680
0.995
0.401
5th percentile
0.425
0.047
0.020
Median
0.689
0.533
0.319
95th percentile
0.783
2.751
0.942
Notes: Table reports percentage point change in choice probability as a result of
increasing change in pass rate/value added by one percentage point estimated by
authors' simulations, based on results from Table 4. No predicted changes in choice
probability are statistically different from zero.

27

APPENDIX TABLES
Appendix Table 1: CSR Plan Adoption within Large Districts over Time
District and spring Number of CSR
of school year plans schools, by year
first implemented
and level
Austin ISD
1998
3
4
2004
1
1
1
3
Cypress-Fairbanks
1998
3
2
1
1
2001
1
1
1
Dallas ISD
2001
2
2004
3
3
El Paso ISD
1998
3
2
1
1
1
2001
1
2004
2
Fort Worth ISD
1998
8
1
2
1
2001
1
2
2004
3
3

School level

Plan adopted

high
middle
elementary
elementary
middle
high

AVID
AVID
Accelerated Schools
Cognitively Guided Instruction
AVID
AVID

elementary
middle
high
high
elementary
middle
high

Literacy Collaborative--Ohio State U.
AVID
AVID
locally-developed plan
locally-developed plan
AVID
AVID

elementary
elementary
middle

Success for All
3-Tier Intervention Model
AVID

elementary
elementary
middle
middle
high
elementary
high

Success for All
El Paso Collaborative for Academic Excellence
El Paso Collaborative for Academic Excellence
Cooperative Integrated Reading and Comprehension
Community for Learning
El Paso Collaborative for Academic Excellence
missing

middle
middle
high
high
elementary
middle
middle
high

Direct Instruction
AVID
AVID
Direct Instruction
Direct Instruction
AVID
AVID
AVID

Sources: CSR Awards Database; authors' Public Information Request (available upon request).

28

Appendix Table 1 (continued): CSR Plan Adoption within Large Districts over Time
District and spring Number of CSR
of school year plans schools, by year
first implemented
and level
School level
Plan adopted
Houston ISD
Accelerated Schools
1998
2
elementary
Core Knowledge
2
elementary
Different Ways of Knowing
1
elementary
Accelerated Schools
1
middle
Coalition of Essential Schools
1
middle
Co-nect
1
middle
Learner-Centered Framework
1
middle
Co-nect
1
high
Consistent Mangement and Cooperative Discipline
2001
1
elementary
Core Knowledge
1
elementary
Houston Annenberg Challenge
1
elementary
Co-nect
2
middle
First Things First
1
middle
Co-nect
2
high
Houston Annenberg Challenge
1
high
Coalition of Essential Schools
1
high
Co-nect
2004
4
elementary
Accelerated Schools
2
middle
Success for All
1
middle
School Development Program
1
middle
Project CLEAR
1
high
Houston Annenberg Challenge
1
high
Accelerated Schools
1
high
High Schools That Work
2
high
Co-nect
1
high
San Antonio ISD
Roots and Wings
1998
6
elementary
Success for All
2
elementary
Co-nect
1
elementary
locally developed
1
elementary
Modern Red Schoolhouse
1
middle
Capacity Building Model
2
middle
locally developed
1
middle
HOSTS
2001
1
elementary
Quantum Learning
1
elementary
Project Reach
2004
1
elementary
Capturing Kids Hearts
2
high
AVID
1
high
missing
1
high
Sources: CSR Awards Database; authors' Public Information Request (available upon request).

29

Appendix Table 2: Perceived Value-added Regressions
Independent variable: change in raw pass rates, averaged over math and reading for all grades reported, during CSR treatment
Dependent variables

(4)

(5)

(6)

Accelerated Schools

15.296**

14.221**

5.075**

AVID

(1.156)
13.263**

(1.130)
14.002**

(0.702)
3.170**

Breaking Ranks

(1.076)
14.516*

(1.052)
14.189*

(0.654)
5.324

Co-nect

(7.140)
19.900**

(6.980)
19.999**

(4.363)
9.260**

Coalition of Essential Schools

(2.324)
12.158**

(2.272)
10.209**

(1.409)
2.573

Creating Independent Student-owned Strategies

(2.590)
8.821**

(2.532)
7.544**

(1.569)
0.252

Direct Instruction

(2.390)
19.427**

(2.336)
17.073**

(1.449)
8.274**

Effective Schools

(2.187)
17.819**

(2.139)
15.559**

(1.326)
6.680**

HOSTS

(2.918)
12.986**

(2.853)
11.781**

(1.768)
3.462+

High Schools That Work

(3.058)
17.548**

(2.990)
16.380**

(1.854)
6.715*

Lightspan

(4.452)
15.124**

(4.351)
14.841**

(2.697)
4.406**

Literacy Collaborative -- Ohio State University

(2.186)
8.908**

(2.137)
7.825**

(1.326)
1.031

M odern Red Schoolhouse

(2.760)
17.700**

(2.698)
16.157**

(1.673)
6.743**

Renaissance Learning

(4.306)
24.799**

(4.209)
23.985**

(2.608)
14.811**

Success for All

(3.901)
15.317**

(3.813)
14.521**

(2.363)
4.789**

(2.248)
9.562*

(2.197)
9.929*

(1.362)
-0.855

(4.523)
14.439**

(4.421)
13.123**

(2.740)
4.446**

(0.787)

(0.770)
43.318**
(0.919)

(0.480)
5.923**
(0.626)

Any CSR

(1)

(2)

(3)

14.667**
(0.460)

13.834**
(0.450)

4.560**
(0.284)

Success-in-the-M aking
other
Exempt rate

43.289**
(0.919)

6.001**
(0.626)

Percent white

0.262**
(0.007)

0.261**
(0.007)

Enrollment

-6.529**
(0.556)

-6.489**
(0.558)

Enrollment squared

0.784**
(0.166)

0.781**
(0.167)

Observations
R-squared

54,512
0.538

54,512
0.559

54,512
0.831

54,512
0.538

54,512
0.559

Notes: Year fixed effects included in all specifications. Standard errors in parentheses. ** p<0.01, * p<0.05, + p<0.1
Sources: CSR Awards Database, Texas Education Agency Academic Excellence Indicator System.

30

54,512
0.831

