NBER WORKING PAPER SERIES

MIS-CLASSIFIED, BINARY, ENDOGENOUS REGRESSORS:
IDENTIFICATION AND INFERENCE
Francis J. DiTraglia
Camilo García-Jimeno
Working Paper 23814
http://www.nber.org/papers/w23814

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2017

We thank Daron Acemoglu, Manuel Arellano, Kristy Buzard, Xu Cheng, Bernardo da Silveira,
Bo Honoré, Arthur Lewbel, Chuck Manski, Sophocles Mavroeidis, Francesca Molinari, Yuya
Takahashi, and seminar participants at Cambridge, CEMFI, Chicago Booth, Manchester,
Northwestern, Oxford, Penn State, Princeton, UCL, the 2016 Greater New York Area
Econometrics Colloquium, Camp Econometrics IX, and the 2017 North American Summer
Meeting of the Econometric Society for valuable comments and suggestions. This document
supersedes an earlier version entitled “On Mis-measured Binary Regressors: New Results and
Some Comments on the Literature.” The views expressed herein are those of the authors and do
not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2017 by Francis J. DiTraglia and Camilo García-Jimeno. All rights reserved. Short sections of
text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.

Mis-classified, Binary, Endogenous Regressors: Identification and Inference
Francis J. DiTraglia and Camilo García-Jimeno
NBER Working Paper No. 23814
September 2017
JEL No. C10,C18,C25,C26
ABSTRACT
This paper studies identification and inference for the effect of a mis-classified, binary,
endogenous regressor when a discrete-valued instrumental variable is available. We begin by
showing that the only existing point identification result for this model is incorrect. We go on to
derive the sharp identified set under mean independence assumptions for the instrument and
measurement error, and that these fail to point identify the effect of interest. This motivates us to
consider alternative and slightly stronger assumptions: we show that adding second and third
moment independence assumptions suffices to identify the model. We then turn our attention to
inference. We show that both our model, and related models from the literature that assume
regressor exogeneity, suffer from weak identification when the effect of interest is small. To
address this difficulty, we exploit the inequality restrictions that emerge from our derivation of
the sharp identified set under mean independence only. These restrictions remain informative
irrespective of the strength of identification. Combining these with the moment equalities that
emerge from our identification result, we propose a robust inference procedure using tools from
the moment inequality literature. Our method performs well in simulations.

Francis J. DiTraglia
Department of Economics
University of Pennsylvania
3718 Locust Walk
Philadelphia
Pennsylvania
19104
fditra@sas.upenn.edu
Camilo García-Jimeno
Department of Economics
University of Pennsylvania
3718 Locust Walk
Philadelphia, PA 19104
and NBER
gcamilo@sas.upenn.edu

1

Introduction

Measurement error and endogeneity are pervasive features of economic data. Conveniently,
a valid instrumental variable corrects for both problems when the measurement error is
classical, i.e. uncorrelated with the true value of the regressor. Many regressors of interest in
applied work, however, are binary and thus cannot be subject to classical measurement error.1
When faced with non-classical measurement error, the instrumental variables estimator can
be severely biased. In this paper, we study an additively separable model of the form
y = c(x) + β(x)T ∗ + ε

(1)

where ε is a mean-zero error term, T ∗ is a binary, potentially endogenous regressor of interest, and x is a vector of exogenous controls.2 Our question is whether, and if so under
what conditions, a discrete instrumental variable z suffices to non-parametrically identify
the causal effect β(x) of T ∗ , when we observe not T ∗ but a mis-classified binary surrogate
T.
We proceed under the assumption of non-differential measurement error. This condition
has been widely used in the existing literature and imposes that T provides no additional
information beyond that contained in (T ∗ , x). Even in this fairly standard setting, identification remains an open question: we begin by showing that the only existing identification
result for this model is incorrect. We then go on to derive the sharp identified set under the
standard first-moment assumptions from the related literature. We show that regardless of
the number of values that z takes on, the model is not point identified. This motivates us to
consider alternative, and slightly stronger assumptions. We show that, given a binary instrument, the addition of a second moment independence assumption suffices to identify a model
with one-sided mis-classification. Adding a second moment restriction on the measurement
error along with a third moment independence assumption for the instrument suffices to
identify the model in general. This result likewise requires only a binary z.
We then turn our attention to inference, showing that both our model and related models from the literature suffer from a weak identification problem. In essence, binary misclassification creates a mixture model and to correct the bias in the instrumental variables
estimator, we must estimate the mixing probabilities. But when β(x) is small the “mixture
modes” are nearly indistinguishable, making it impossible to reliably estimate these proba1

The only way to mis-classify a true one is downwards, as a zero, while the only way to mis-classify a
true zero is upwards, as a one. This creates negative dependence between the true value of the regressor and
the error.
2
Because T ∗ is binary, there is no loss of generality from writing the model in this form rather than the
more familiar y = h(T ∗ , x) + ε. Simply define β(x) = h(1, x) − h(0, x) and c(x) = h(0, x).

2

bilities. To address this difficulty, we exploit the inequality moment restrictions that emerge
from our derivation of the sharp identified set. These restrictions remain informative even
when β(x) is small or zero. Combining them with the moment equalities that emerge from
our identification result, we propose an identification robust procedure for uniformly valid
inference using tools from the moment inequality literature. Our procedure is computationally attractive and performs well in simulations. Moreover, it can be used both in our model
and related models from the literature that assume an exogenous T ∗ .
Our work relates to a large literature studying departures from the textbook linear, classical measurement error setting. One strand of this literature considers relaxing the assumption of linearity while maintaining that of classical measurement error. Schennach (2004),
for example, uses repeated measures of each mis-measured regressor to obtain identification,
while Schennach (2007) uses an instrumental variable. More recently, Song et al. (2015) rely
on a repeated measure of the mis-measured regressor and the existence of a set of additional
regressors, conditional upon which the regressor of interest is unrelated to the unobservables,
to obtain identification. For comprehensive reviews of the challenges of addressing measurement error in non-linear models, see Chen et al. (2011) and Schennach (2013). Another
strand of the literature considers relaxing the assumption of classical measurement error, by
allowing the measurement error to be related to the true value of the unobserved regressor.
Chen et al. (2005) obtain identification in a general class of moment condition models with
mis-measured data by relying on the existence of an auxiliary dataset from which they can
estimate the measurement error process. In contrast, Hu and Shennach (2008) and Song
(2015) rely on an instrumental variable and an additional conditional location assumption
on the measurement error distribution. More recently, Hu et al. (2015) use a continuous
instrument to identify the ratio of partial effects of two continuous regressors, one measured
with error, in a linear single index model. Unfortunately, these approaches cannot be applied
to the case of a mis-measured binary regressor.
A number of papers have studied models with an exogenous binary regressor subject to
non-differential measurement error. One group of papers asks what can be learned without
recourse to an instrumental variable. An early contribution by Aigner (1973) characterizes
the asymptotic bias of OLS in this setting, and proposes a correction using outside information on the mis-classification process. Related work by Bollinger (1996) provides partial
identification bounds. More recently, Chen et al. (2008a) use higher moment assumptions
to obtain identification in a linear model, and Chen et al. (2008b) extend these results to
the non-parametric setting. van Hasselt and Bollinger (2012) and Bollinger and van Hasselt
(2015) provide additional partial identification results. For results on the partial identification of discrete probability distributions under mis-classification, see Molinari (2008).
3

Continuing under the assumption of exogeneity and non-differential measurement error,
another group of papers relies on the availability of either an instrumental variable or a
second measure of T ∗ . Black et al. (2000) and Kane et al. (1999) consider a linear model and
show that when two alternative measures T1 and T2 of T ∗ are available, a non-linear GMM
estimator can be used to recover the effect of interest. Subsequently, Frazis and Loewenstein
(2003) note that an instrumental variable can take the place of one of the measures. Mahajan
(2006) extends the results of Black et al. (2000) and Kane et al. (1999) to a more general
setting using a binary instrument in place of one of the treatment measures, establishing nonparametric identification of the conditional mean function. When T ∗ is in fact exogenous,
this coincides with the causal effect. Hu (2008) derives related results when the mis-classified
discrete regressor may take on more than two values. Lewbel (2007) provides an identification
result for the same model as Mahajan (2006) under different assumptions. In particular, his
“instrument-like variable” need not satisfy the usual exclusion restriction so long as it does
not interact with T ∗ and takes on three or more values.
Much less is known about the case in which a binary, or discrete, regressor is not only
mis-classified but endogenous. The first paper to provide a formal result for this case is Mahajan (2006). He extends his main result to the case of an endogenous treatment, providing
an explicit proof of identification under the usual IV assumption in a model with additively
separable errors. As we show below, however, this result is false.3 Several more recent papers also consider the case of a mis-classified, endogenous, binary regressor. Kreider et al.
(2012), partially identify the effects of food stamps on health outcomes of children under
weak measurement error assumptions by relying on auxiliary data. Similarly, Battistin et al.
(2014) study the returns to schooling in a setting with multiple mis-reported measures of
educational qualifications. Unlike these two papers, our approach does not depend on the
availability of auxiliary data. In a different vein, Shiu (2015) uses an exclusion restriction
for the participation equation and an additional valid instrument to identify the effect of a
discrete, mis-classified endogenous regressor in a semi-parametric selection model. Similarly,
Nguimkeu et al. (2016) use exclusion restrictions for both the participation equation and
measurement error equation to identify a parametric model with endogenous participation
and one-sided endogenous mis-reporting. Unlike those of the preceding two papers, our results rely neither on parametric assumptions nor additional exclusion restrictions. Other
than Mahajan (2006), the paper most closely related to our own is that of Ura (2015), who
derives partial identification results for a local average treatment effect without assuming
non-differential measurement error. Unlike Ura (2015) we study an additively separable
model under non-differential measurement error and derive both partial and point identifi3

Appendix B provides a detailed explanation of the error in Mahajan’s proof.

4

cation results.
Our work also relates to a large literature on inference using inequality moment conditions. In particular, we adopt the generalized moment selection (GMS) approach of Andrews
and Soares (2010) to construct a procedure for identification-robust inference that combines
the moment equalities from our point identification results with inequalities from our partial
identification results. Although the equalities alone globally identify our model, the inequalities turn out to be extremely valuable in settings where β(x) may be small. Although our
specific approach differs from theirs, the idea of including moment inequalities in a model
that is already point identified by a collection of moment equalities relates to work by Moon
and Schorfheide (2009). While the weak identification problem that we point out and address here also emerges in several closely related models, e.g. (Mahajan, 2006) and Frazis and
Loewenstein (2003), we are unaware of any other work from the literature that acknowledges
or addresses it. As shown in Appendix C, our inference procedure can be applied to the case
of an exogenous regressor with only minor modifications.
The remainder of the paper is organized as follows. Section 2.1 describes our model and
assumptions, Section 2.2 relates our results to existing work, and Sections 2.3–2.4 present our
identification results. Section 3.1 points out the special inferential difficulties that arise in
models with mis-classification while Section 3.2 gives a high-level overview of our proposed
inference procedure. Full details of the procedure follow in Sections 3.3–3.5. Section 4
presents simulation results, and Section 5 concludes. Proofs appear in Appendix A, and we
give a detailed explanation of the error in Mahajan (2006) in Appendix B.

2
2.1

Identification Results
Baseline Assumptions

As defined in the preceding section, our model is y = c(x) + β(x)T ∗ + ε, where ε is a meanzero error term, and the parameter of interest is β(x) – the effect of an unobserved, binary,
endogenous regressor T ∗ . Suppose we observe a valid and relevant binary instrument z. In
the discussion following Corollary 2.3 below, we explain how these results generalize to the
case of an arbitrary discrete-valued instrument. We assume that the model and instrument
satisfy the following conditions:
Assumption 2.1.
(i) y = c(x) + β(x)T ∗ + ε where T ∗ ∈ {0, 1} and E[ε] = 0;
(ii) z ∈ {0, 1}, where 0 < P(z = 1|x) < 1, and P(T ∗ = 1|x, z = 1) =
6 P(T ∗ = 1|x, z = 0);
5

(iii) E[ε|x, z] = 0.
Assumptions 2.1(ii) and (iii) are the standard instrument relevance and mean independence assumptions.4 If T ∗ were observed, Assumption 2.1 would suffice to identify β(x).
Unfortunately we observe not T ∗ but a mis-classified binary surrogate T . Define the following mis-classification probabilities:
α0 (x, z) = P (T = 1|T ∗ = 0, x, z) ,

α1 (x, z) = P (T = 0|T ∗ = 1, x, z) .

(2)

Following the existing literature for the case of an exogenous regressor (Black et al., 2000;
Frazis and Loewenstein, 2003; Kane et al., 1999; Lewbel, 2007; Mahajan, 2006), we impose
the following conditions on the mis-classification process.
Assumption 2.2.
(i) α0 (x, z) = α0 (x), α1 (x, z) = α1 (x)
(ii) α0 (x) + α1 (x) < 1
(iii) E[ε|x, z, T ∗ , T ] = E[ε|x, z, T ∗ ]
Assumption 2.2 (i) states that the mis-classification probabilities do not depend on z. As
we maintain this assumption throughout, we drop the dependence of α0 and α1 on z and
write α0 (x) and α1 (x). Assumption 2.2 (ii) restricts the extent of mis-classification and is
equivalent to requiring that T and T ∗ be positively correlated. Assumption 2.2 (iii) is often
referred to as “non-differential measurement error.” Intuitively, it maintains that T provides
no additional information about ε, and hence y, given knowledge of (T ∗ , z, x).

2.2

Point Identification Results from the Literature

Existing results from the literature – see for example Frazis and Loewenstein (2003) and Mahajan (2006) – establish that β(x) is point identified if Assumptions 2.1–2.2 are augmented
to include the following condition:
Assumption 2.3 (Joint Exogeneity). E[ε|x, z, T ∗ ] = 0.
Assumption 2.3 strengthens the mean independence condition from Assumption 2.1 (iii)
to hold jointly for T ∗ and z. By iterated expectations, this implies that T ∗ is exogenous,
4
Assumption 2.1 (ii) states that z is a relevant instrument for the unobserved regressor T ∗ . Under
Assumption 2.2, however, this is equivalent to assuming that z is a relevant instrument for the observed
regressor T by Lemma 2.1 below.

6

i.e. E[ε|x, T ∗ ] = 0. If T ∗ is endogenous, Assumption 2.3 clearly fails. Mahajan (2006)
argues, however, that the following restriction, along with our Assumptions 2.1–2.2, suffices
to identify β(x) when T ∗ may be endogenous:
Assumption 2.4 (Mahajan (2006) Equation 11). E[ε|x, z, T ∗ , T ] = E[ε|x, T ∗ ].
Assumption 2.4 does not require E[ε|x, T ∗ ] to be zero, but maintains that it does not
vary with z. We show in Appendix B, however, that under Assumptions 2.1–2.2, Assumption
2.4 can only hold if T ∗ is exogenous. If z is a valid instrument and T ∗ is endogenous, then
Assumption 2.4 implies that there is no first-stage relationship between z and T ∗ . As such,
identification in the case where T ∗ is endogenous is an open question.

2.3

Partial Identification

In this section we derive the sharp identified set under Assumptions 2.1–2.2 and show that
β(x) is not point identified. To simplify the notation, define the following shorthand for the
unobserved and observed first stage probabilities
p∗k (x) = P(T ∗ = 1|x, z = k)

(3)

pk (x) = P(T = 1|x, z = k).

(4)

We first state two lemmas that have appeared in various guises throughout the literature.
These will be used repeatedly below.
Lemma 2.1. Under Assumption 2.2 (i),
[1 − α0 (x) − α1 (x)] p∗k (x) = pk (x) − α0 (x)
[1 − α0 (x) − α1 (x)] [1 − p∗k (x)] = 1 − pk (x) − α1 (x)
where the first-stage probabilities p∗k (x) and pk (x) are as defined in Equations 3–4.
Lemma 2.2. Under Assumptions 2.1 and 2.2 (i)–(ii),
β(x)Cov(z, T |x) = [1 − α0 (x) − α1 (x)] Cov(y, z|x)
Lemma 2.1 relates the observed first-stage probabilities pk (x) to their unobserved counterparts p∗k (x) in terms of the mis-classification probabilities α0 (x) and α1 (x). By Assumption
2.2 (ii), 1 − α0 (x) − α1 (x) > 0 so that Lemma 2.1 provides non-trivial bounds for α0 (x) and
α1 (x) in terms of the observed first-stage probabilities. Lemma 2.2 relates the instrumental variables (IV) estimand, Cov(y, z|x)/Cov(z, T |x), to the mis-classification probabilities.
7

Since 1 − α0 (x) − α1 (x) > 0, IV is biased upwards in the presence of mis-classification. Combining the two lemmas yields a well-known bound, namely that β(x) lies between the reduced
form and IV estimators. Our first result shows that without Assumption 2.2 (non-differential
measurement error) these bounds are sharp.
Theorem 2.1. Under Assumptions 2.1 and 2.2 (i)–(ii), the sharp identified set is characterized by


pk (x) − α0 (x)
(5)
E[y|x, z = k] = c(x) + β(x)
1 − α0 (x) − α1 (x)
and α0 (x) ≤ pk (x) ≤ 1 − α1 (x) for k = 0, 1 where pk (x) is defined in Equation 4.
Corollary 2.1. Under the conditions of Theorem 2.1, the sharp identified set for β(x) is the
closed interval between the reduced form estimand Cov(y, z|x)/Var(z|x) and the IV estimand
Cov(y, z|x)/Cov(z, T |x).
Corollary 2.1 follows by taking differences of the expression for E[y|x, z = k] across k = 1
and k = 0, and substituting the maximum and minimum value for α0 (x) + α1 (x) consistent
with the observed first-stage probabilities. When the mis-classification probabilities are
known a priori to satisfy additional restrictions, these bounds can be tightened.5 The
following corollary collects results for two common cases: one-sided misclassification (either
α0 (x) or α1 (x) equals zero), and symmetric mis-classification (α0 (x) = α1 (x)).
Corollary 2.2. Under the conditions of Theorem 2.1, the following restrictions on the misclassification probabilities α0 (x), α1 (x) shrink the sharp identified set for β(x) to the closed
interval between ∆ × [Cov(y, z|x)/Cov(z, T |x)] and Cov(y, z|x)/Cov(z, T |x).
(i) If α0 (x) = 0 then ∆ = maxk pk (x).
(ii) If α1 (x) = 0 then ∆ = 1 − mink pk (x).
(iii) If α0 (x) = α1 (x) then ∆ = 1 − 2 min {mink pk (x), 1 − maxk pk (x)}.
Theorem 2.1 and Corollaries 2.1–2.2 do not impose Assumption 2.2 (iii) – non-differential
measurement error. We now show that this assumption yields further restrictions on the misclassification probabilities α0 (x) and α1 (x). While these restrictions are more complicated to
describe than those from Theorem 2.1, they are straightforward to implement in practice and
can be extremely informative, as we will show in our simulation exercises below. To the best
of our knowledge, the sharp bounds that we derive by adding Assumption 2.2 (iii) are new to
5

Frazis and Loewenstein (2003) consider a model in which α0 and α1 do not depend on the exogenous
covariates x. In this case α0 ≤ P(T = 1|x, z) ≤ 1 − α1 and they suggest minimizing the bounds over x.

8

the literature. Our result uses two additional conditions to simplify the proof of sharpness.
First, we assume that y is continuously distributed. This is natural in an additively separable
model and holds in our simulation examples below. Without this assumption, the bounds
that we derive are still valid, but may not be sharp. Nevertheless, the reasoning from our
proof can be generalized to cases in which y does not have a continuous support set. We
also impose E[y|x, T = 0, z = k] =
6 E[y|x, T = 1, z = k] for any k. This holds generically and
is not essential to the proof: it merely simplifies the description of the identified set.
Theorem 2.2. Suppose that the conditional distribution of y given (x, T, z) is continuous
for any values of the conditioning variables and E [y|x, T = 0, z = k] =
6 E [y|x, T = 1, z = k]
for all k. Then, under Assumptions 2.1 and 2.2, the sharp identified set is characterized by
Equation 5 from Theorem 2.1 along with α0 (x) < pk (x) < 1 − α1 (x) for k = 0, 1 and







µtk q tk α0 (x), α1 (x), x , x ≤ µk α0 (x), x ≤ µtk q tk α0 (x), α1 (x), x , x
for all pairs (t, k) where

µtk q, x = E [y | y ≤ q, x, T = t, z = k] ,


µtk q, x = E [y | y > q, x, T = t, z = k]

 pk (x)E[y|x, z = k, T = 1] − α0 (x)E[y|x, z = k]
µk α0 (x), x =
pk (x) − α0 (x)
and we define


Ftk−1



Ftk−1

q tk α0 (x), α1 (x), x =
q tk α0 (x), α1 (x), x =


rtk α0 (x), α1 (x), x




x


1 − rtk α0 (x), α1 (x), x




x

where Ftk−1 (·|x) is the conditional quantile function of y given (x, T = t, z = k),
r0k
r1k



α1 (x)
pk (x) − α0 (x)
α0 (x), α1 (x), x =
1 − pk (x) 1 − α0 (x) − α1 (x)


 1 − α1 (x)
pk (x) − α0 (x)
α0 (x), α1 (x), x =
pk (x)
1 − α0 (x) − α1 (x)


and pk (x) is defined in Equation 4.
The intuition for Theorem 2.2 is as follows. For simplicity, suppress dependence on x.
Now, fix (T = t, z = k) and (α0 , α1 ). The observed distribution of y given (T = t, z = k),
9

call it Ftk , is a mixture of two unobserved distributions: the distribution of y given (T =
1, z = k, T ∗ = 1), call it Ftk1 , and the distribution of y given (T = t, z = k, T ∗ = 0), call it Ftk0 .
The mixing probabilities are rtk and 1 − rtk from the statement of Theorem 2.2 and are fully
determined by (α0 , α1 ) and pk . Assumptions 2.1 (i) and 2.2 (ii) imply that the unobserved
means E[y|T ∗ , T, z] are fully determined by (α0 , α1 ) given the observed means E[y|T, z]. The
question is whether it is possible, given the observed distribution Ftk , to construct Ftk1 and
Ftk0 with the required values for E[y|T ∗ , T, z] such that Ftk = rtk Ftk1 + (1 − rtk )Ftk0 for all
combinations (t, k). If not, then (α0 , α1 ) does not belong to the identified set. Our proof
provides necessary and sufficient conditions for such a mixture to exist at a given point
(α0 , α1 ). We can then appeal to the reasoning from Theorem 2.1 to complete the argument.
By ruling out values for α0 and α1 , Theorem 2.2 restricts β via Lemma 2.2. While these
restrictions can be very informative in practice, they do not yield point identification.
Corollary 2.3. Under Assumptions 2.1 and 2.2 the identified set for β(x) contains both the
IV estimand Cov(y, z|x)/Cov(z, T |x) and the true coefficient β(x).
Corollary 2.3 follows by Lemma 2.2 because α0 (x) = α1 (x) = 0 always belongs to the
sharp identified set from Theorem 2.2. Non-differential measurement error cannot exclude
the possibility that there is no mis-classification because in this case it is trivial to construct
the required mixtures.
Although we focus throughout this paper on the case of a binary instrument, one might
wonder whether point identification can be achieved by increasing the support of z, perhaps
along the lines of Lewbel (2007). The answer turns out to be no. Suppose that we were
to modify Assumptions 2.1 and 2.2 to hold for all values of z in some discrete support set.
By Lemma 2.2, a binary instrument identifies β(x) up to knowledge of the mis-classification
probabilities α0 (x) and α1 (x). It follows that any pair of values (k, `) in the support set
of z identifies the same object. Accordingly, to identify β(x) it is necessary and sufficient
to identify the mis-classification probabilities. A binary instrument fails to identify these
probabilities because we can never exclude the possibility of zero mis-classification. The
same is true of a discrete K-valued instrument. Increasing the support of z does, however,
shrink the identified set by increasing the number of restrictions available. If z takes on more
than two values, our results in Theorems 2.1–2.2 continue to apply if “k = 0, 1” is replaced
by “for all k.”

2.4

Point Identification

The results of the preceding section establish that β(x) is not point identified under Assumptions 2.1 and 2.2. In light of this, there are two possible ways to proceed: either one
10

can report partial identification bounds based on our characterization of the sharp identified
set from Theorem 2.2, or one can attempt to impose stronger assumptions to obtain point
identification. In this section we consider the second possibility. We begin by defining the
following functions of the model parameters:
θ1 (x) = β(x) [1 − α0 (x) − α1 (x)]−1

(6)

θ2 (x) = [θ1 (x)]2 [1 + α0 (x) − α1 (x)]


θ3 (x) = [θ1 (x)]3 {1 − α0 (x) − α1 (x)}2 + 6α0 (x) {1 − α1 (x)}

(7)
(8)

Now consider the following additional assumption:
Assumption 2.5. E[ε2 |x, z] = E[ε2 |x]
Assumption 2.5 is a second moment version of the standard mean exclusion restriction
for the instrument z – Assumption 2.1 (iii). It requires that the conditional variance of the
error term given the covariates x does not depend on z. Notice that this assumption does
not require homoskedasticity with respect to x, T ∗ or T . Assumption 2.5 allows us to derive
the following lemma:
Lemma 2.3. Under Assumptions 2.1, 2.2 and 2.5,
Cov(y 2 , z|x) = 2Cov(yT, z|x)θ1 (x) − Cov(T, z|x)θ2 (x)
where θ1 (x) and θ2 (x) are defined in Equations 6–7.
Lemma 2.2 identifies θ1 (x). Since Cov(z, T |x) =
6 0 by Assumption 2.1 (ii), we can solve
for θ2 (x) in terms of observables only, using Lemma 2.3. Given knowledge of θ1 (x), we can
solve Equation 7 for the difference of mis-classification rates so long as β(x) =
6 0.
Corollary 2.4. Under Assumptions 2.1–2.2 and 2.5, α1 (x) − α0 (x) is identified so long as
β(x) =
6 0.
Corollary 2.4 identifies the difference of mis-classification error rates. Hence, under onesided mis-classification, α0 (x) = 0 or α1 (x) = 0, augmenting our baseline Assumptions
2.1–2.2 with Assumption 2.5 suffices to identify β(x). Notice that β(x) = 0 if and only if
θ1 (x) = 0. Thus, β(x) is still identified in the case where Corollary 2.4 fails to apply.
Assumption 2.5 does not suffice to identify β(x) without a priori restrictions on the
mis-classification error rates. To achieve identification in the general case, we impose the
following additional conditions:
11

Assumption 2.6.
(i) E[ε2 |x, z, T ∗ , T ] = E[ε2 |x, z, T ∗ ]
(ii) E[ε3 |x, z] = E[ε3 |x]
Assumption 2.6 (i) is a second moment version of the non-differential measurement error
assumption, Assumption 2.2 (iii). It requires that, given knowledge of (x, T ∗ , z), T provides
no additional information about the variance of the error term. Note that Assumption 2.6
(i) does not require homoskedasticity of ε with respect to x or T ∗ . Assumption 2.6 (ii) is a
third moment version of Assumption 2.5. It requires that the conditional third moment of
the error term given x does not depend on z. This condition neither requires nor excludes
skewness in the error term conditional on covariates: it merely states that the skewness is
unaffected by the instrument.
While Assumptions 2.5 and 2.6 may appear unfamiliar, we consider them to be fairly
natural in the context of an additively separable model in which one has already assumed
that E[ε|z] = 0 and E[ε|x, z, T ∗ , T ] = E[x, z, T ∗ ] – Assumptions 2.1 (iii) and 2.2 (iii) from
above.6 For example, if an applied researcher reports results both for an outcome in logs
and levels, she has implicitly assumed independence rather than first moment exclusion.
Assumptions 2.1 (iii), 2.5 and 2.6 (ii) are of course implied by ε ⊥ z|x while Assumptions
2.2 (iii) and 2.6 (i) are implied by ε ⊥ T |(x, T ∗ , z). Achieving identification via Assumptions
2.5–2.6 involves using information beyond first moments and as such does places higher
demands on the data. Assumption 2.6 allows us to derive the following Lemma which,
combined with Lemma 2.3, leads to point identification:
Lemma 2.4. Under Assumptions 2.1–2.2 and 2.5–2.6,
Cov(y 3 , z|x) = 3Cov(y 2 T, z|x)θ1 (x) − 3Cov(yT, z|x)θ2 (x) + Cov(T, z|x)θ3 (x)
where θ1 (x), θ2 (x) and θ3 (x) are defined in Equations 6–7.
Theorem 2.3. Under Assumptions 2.1–2.2 and 2.5–2.6, β(x) is identified. If β(x) =
6 0,
then α0 (x) and α1 (x) are likewise identified.
Lemmas 2.2–2.4 yield a linear system of three equations in θ1 (x), θ2 (x) and θ3 (x). Under
Assumption 2.1 (ii), the system has a unique solution so θ1 (x), θ2 (x) and θ3 (x) are identified.
The proof of Theorem 2.3 shows that, so long as β(x) =
6 0, Equations 6–8 can be solved for
β(x), α0 (x) and α1 (x). If we relax Assumption 2.2 (ii) and assume α0 (x) + α1 (x) =
6 1 only,
β(x) is only identified up to sign.
6

If one wishes to weaken our Assumption 2.1 (i) to allow for some form of unobserved heterogeneity, our
higher moment assumptions may impose additional restrictions.

12

3

Identification-Robust Inference

We now turn our attention to inference based on the identification results from above. We
begin by expressing Lemmas 2.2, 2.3 and 2.4 as unconditional equality moment conditions,
and describing the resulting just-identified GMM estimator. As we explain in Section 3.1,
inference under binary mis-classification is complicated by problems of weak identification
and parameters on the boundary. Section 3.2 provides an overview of our inference procedure.
Full details appear in Sections 3.3–3.5. For simplicity we fix the exogenous covariates at
some specified level and suppress dependence on x in the notation. This is appropriate if the
covariates have a discrete support. We discuss how to incorporate covariates more generally
in Section 3.6.

3.1

The Non-standard Inference Problem

Lemmas 2.2–2.4 yield the following system of linear moment equalities in the reduced form
parameters θ = (θ1 , θ2 , θ3 ) from Equations 6–8:
Cov(y, z) − Cov(T, z)θ1 = 0
Cov(y 2 , z) − 2Cov(yT, z)θ1 + Cov(T, z)θ2 = 0
Cov(y 3 , z) − 3Cov(y 2 T, z)θ1 + 3Cov(yT, z)θ2 − Cov(T, z)θ3 = 0
Non-linearity arises solely through the relationship between the reduced from parameters
θ and the structural parameters (α0 , α1 , β). To convert the preceding moment equations
into unconditional moment equalities, we define the additional reduced form parameters
κ = (κ1 , κ2 , κ3 ) as follows:
κ1 = c − α0 θ1
κ2 = c2 + σεε + α0 (θ2 − 2cθ1 )


κ3 = c3 + 3 (c − θ1 α0 ) σεε + E[ε3 ] − α0 θ3 − 3cα0 θ1 (c + β) − 2θ12 (1 − α1 )
Building on this notation, let
ψ 01 = (−θ1 , 1, 0, 0, 0, 0),

ψ 02 = (θ2 , 0, −2θ1 , 1, 0, 0),

13

ψ 03 = (−θ3 , 0, 3θ2 , 0, −3θ1 , 1)

(9)

h
i
and collect these in the matrix Ψ = ψ 1 ψ2 ψ3 . Defining the observed data vector
wi0 = (Ti , yi , yi Ti , yi2 , yi2 Ti , yi3 ) for observation i, we can re-write the moment equations as:
"
E


Ψ0 (θ)wi − κ ⊗

1
zi

!#
= 0.

(10)

Equation 10 is a just-identified, linear system of moment equalities in the reduced form
b From Theorem 2.3, knowledge
parameters (θ, κ) and yields explicit GMM estimators (b
κ, θ).
of θ suffices to identify β. From the definitions of κ above and θ in Equations 6–8, however,
the moment equalities from Equation 10 do not depend on (α0 , α1 ) if β equals zero. By
continuity, they are nearly uninformative about the mis-classification probabilities if β is
small. But unless β = 0, knowledge of (α0 , α1 ) is necessary to recover β, via Lemma 2.2.
Thus, we face a weak identification problem.7 Indeed, the GMM estimator of βb based on
Equation 10 may even fail to exist. Using arguments from the proof of Theorem 2.3, this
estimator is given by
r 
2



b
b
b
b
b
b
β = sign θ1
3 θ2 /θ1 − 2 θ3 /θ1
Under our assumptions, 3(θ2 /θ1 )2 > 2(θ3 /θ1 ) provided that β =
6 0, but this may not be true
b
of the sample analogue. Indeed, because θ1 appears in the denominator, the terms within
the square root will be highly variable if β is small. Even if the GMM estimator exists, it
may violate the partial identification bounds for (α0 , α1 ) from Theorem 2.2, or imply that
(α0 , α1 ) are not valid probabilities. Importantly, the partial identification bounds remain
informative even if β is small or zero: so long as Assumption 2.1 (ii) holds, the first-stage
probabilities bound α0 and α1 from above.
Exactly the same inferential difficulties arise in the case where T ∗ and z are assumed to
be jointly exogenous, as in Black et al. (2000); Frazis and Loewenstein (2003); Kane et al.
(1999); Lewbel (2007); Mahajan (2006).8 This issue, however, has received little attention
in the literature. Kane et al. (1999) ensure that (α0 , α1 ) are valid probabilities by employing
a logit specification. Frazis and Loewenstein employ a pseudo-Bayesian approach to ensure
that α0 and α1 are valid probabilities, and to impose partial identification bounds related
to those from our Theorem 2.1, i.e. without using the non-differential measurement error
restrictions. Because they provide neither simulation evidence nor a theoretical justification
for their procedure, however, it is unclear whether this method will yield valid Frequentist
coverage. We are unaware of any papers in the related literature that discuss the weak
7

This is essentially equivalent to the problem of estimating mixture probabilities when the means of the
component distributions are very similar to each other.
8
We provide details for Frazis and Loewenstein (2003) and Mahajan (2006) in Appendix C.

14

identification problem arising when β is small.

3.2

Overview of the Inference Procedure

In the following sections we develop a procedure for uniformly valid inference in models with
a mis-classified binary regressor. Our purpose is to construct a confidence interval for β that
is robust to possible weak identification, respects the restricted parameter space for (α0 , α1 ),
and incorporates both the information in the equality moment conditions from Equation 10
along with the partial identification bounds from Theorem 2.2.9 As argued in the preceding
section, our partial identification bounds remain informative even when the equality moment
conditions contain essentially no information about (α0 , α1 ).
To carry out identification-robust inference combining equality and inequality moment
conditions, we adopt the generalized moment selection (GMS) approach of Andrews and
Soares (2010). This procedure provides a uniformly valid test of a joint null hypothesis for
the full parameter vector. In our model, this includes the parameter of interest β along with
various nuisance parameters: the mis-classification probabilities α0 and α1 , the reduced form
parameters κ, defined in Section 3.1, and a vector q of parameters that enter the moment
inequalities.10 Under a given joint null hypothesis for (β, α0 , α1 ), however, κ and q are
strongly identified and lie on the interior their respective parameter spaces. Accordingly, in
Section 3.4 we explain how to concentrate these parameters out of the GMS procedure, by
deriving an appropriate correction to the asymptotic variance matrix for the test.11
This leaves us with a uniformly valid test of any joint null hypothesis for (β, α0 , α1 ). To
construct a marginal confidence interval for β we proceed as follows. Suppose that z is a
strong instrument. Then the usual IV estimator provides a valid confidence interval for the
reduced from parameter θ1 . By Lemma 2.2, knowledge of (1 − α0 − α1 ) suffices to determine
β from θ1 . Thus, a valid confidence interval for (1 − α0 − α1 ) can be combined with the IV
interval for θ1 to yield a corresponding interval for β, via a Bonferroni-type correction. To
construct the required interval for (1 − α0 − α1 ), we note from Equations 6–8 that β only
enters the moment equality conditions in Equation 10 through θ1 . But, again, inference for
θ1 is standard provided that z is a strong instrument. We can thus pre-estimate θ1 along
with κ and q, yielding a uniformly valid GMS test of any joint null hypothesis for (α0 , α1 ).
By inverting this test, we construct a joint confidence set for (α0 , α1 ) which we then project
9

Note that β = 0 if and only if θ1 = 0. Thus, if one is merely interested in testing H0 : β = 0, one can
ignore the mis-classification error problem and test H0 : θ1 = 0 using the standard IV estimator and standard
error, provided that z is a strong instrument.
10
These are defined below in Section 3.3.
11
Note that we cannot take the same approach to concentrate out α0 and α1 because the mis-classification
probabilities may be weakly identified or lie on the boundary of their parameter space.

15

to obtain a confidence interval for (1 − α0 − α1 ). Because the parameter space for (α0 , α1 )
is bounded and two-dimensional, the projection step is computationally trivial.12 If desired,
one could also carry out a valid test of the null hypothesis that there is no mis-classification,
α0 = α1 = 0, using the joint test for (α0 , α1 ). In the following sections we provide full details
of our Bonferroni-based confidence interval procedure for β.

3.3

Moment Inequalities

As noted above, the partial identification bounds from Theorems 2.1 and 2.2 remain informative about (α0 , α1 ) even when β is small. To incorporate them in our inference procedure,
we first express them as unconditional moment inequalities. The bounds from Theorem 2.1
are given by
pk − α0 ≥ 0,
1 − pk − α1 ≥ 0, for all k
where the first-stage probabilities pk are defined in Equation 4. We write
as

(1 − zi )(Ti − α0 )

 (1 − zi )(1 − Ti − α1 )


E mI1 (wi , ϑ) ≥ 0, mI1 (wi , ϑ) ≡ 
 z (T − α )
0
 i
zi (1 − Ti − α1 )

these inequalities







(11)

The bounds derived in Theorem 2.2 by imposing assumption 2.2 (iii) are

µk (α0 ) − µtk q tk (α0 , α1 ) ≥ 0,


µtk q tk (α0 , α1 ) − µk (α0 ) ≥ 0,

for all t, k

12
We considered two alternatives to the Bonferroni-based inference procedure described here. The first
constructs a marginal confidence interval for β by projecting a joint confidence set for (β, α1 , α0 ), i.e. without
preliminary estimation of θ1 . This method is more computationally demanding than our two-dimensional
projection and involves a parameter space that is unbounded along the β-dimension. From a practical perspective, the relevant question is whether the reduction in conservatism from projecting a lower dimensional
set is outweighed by the additional conservatism induced by the Bonferroni correction. In our experiments,
the full three-dimensional projection and Bonferroni procedure produced broadly similar results: neither
reliably dominated in terms of confidence interval width. Given its substantially lower computational burden, we prefer the Bonferroni procedure. We also experimented with two recently proposed methods for
sub-vector inference: Kaido et al. (2016) and Bugni et al. (2017). In both cases we obtained significant size
distortions, suggesting that our model may not satisfy the regularity conditions required by these papers.

16

where µk , µtk , µtk , q tk and q tk are defined in the statement of the Theorem. Expressing these
as unconditional moment inequalities, we have

E[mI2 (wi , ϑ, q)]

≥ 0,

mI2 (wi , ϑ, q)



≡



mI2,00 (wi , ϑ, q)
mI2,10 (wi , ϑ, q)
mI2,01 (wi , ϑ, q)
mI2,11 (wi , ϑ, q)








(12)

where q ≡ (q 00 , q 00 , q 10 , q 10 , q 01 , q 01 , q 11 , q 11 ) and we define


mI2,0k

n

o 
yi 1 (zi = k) (Ti − α0 ) − 1(yi ≤ q 0k )(1 − Ti ) 1−αα01−α1
o 
n

wi , ϑ, q) ≡ 
1−α0 −α1
−yi 1(zi = k) (Ti − α0 ) − 1(yi > q 0k )(1 − Ti )
α1

n

o 
1−α0 −α1
yi 1 (zi = k) (Ti − α0 ) − 1(yi ≤ q 1k )Ti
 1−α1 o  .
n
mI2,1k (wi , ϑ, q) ≡ 
0 −α1
−yi 1(zi = k) (Ti − α0 ) − 1(yi > q 1k )Ti 1−α
1−α1

(13)



0

(14)

0

Finally we define mI = (mI1 , mI2 )0 . Notice that the second set of inequalities, mI2 , depends
on the unknown parameter q which is in turn a function of (α0 , α1 ). In the next section we
discuss how q can be estimated under a given null hypothesis for (α0 , α1 ).

3.4

Accounting for Preliminary Estimation

Let ϑ = (α0 , α1 ) and γ = (κ, θ1 ) where θ1 is defined in Equation 6 and κ in Section 3.1.
Our moment conditions take the form
E[mI (wi , ϑ0 , q0 )] ≥ 0,
0

E[mE (wi , ϑ0 , γ 0 )] = 0

(15)

0

where mI = (mI1 , mI2 )0 , defined in Section 3.3, and
"
mE (wi , ϑ0 , γ 0 ) =

{ψ 02 (θ1 , α0 , α1 )wi − κ2 } zi
{ψ 03 (θ1 , α0 , α1 )wi − κ3 } zi

#
.

(16)

Notice that we now write ψ 2 and ψ 3 , defined in Equation 9, as explicit functions of (θ1 , α0 , α1 ),
using the definitions of (θ2 , θ3 ) from Equations 7–8. To construct a GMS test of the null
b (ϑ0 ) and
hypothesis H0 : ϑ = ϑ0 based on Equation 15, we require preliminary estimators γ
b(ϑ0 ) that are consistent and asymptotically normal under the null. We now provide full
q
details of the construction and derive the associated adjustment to the asymptotic variance
17

matrix.
Consider first the equality moment conditions mE . For these we require preliminary
estimators of θ1 , κ2 , and κ3 . Recall that θ1 is simply the IV estimand: it can be consistently estimated directly from observations of (y, T, z) without knowledge of α0 or α1 . Note,
moreover, from Equation 10 that κ is simply a vector of intercepts. These can be directly
estimated from observations of w because Ψ(θ1 , α0 , α1 ) is consistently estimable under the
null H0 : ϑ = ϑ0 : the hypothesis specifies α0 and α1 and IV provides a consistent estimator
of θ1 . Accordingly, define
"
E

h (wi , ϑ, γ) =

Ψ0 (θ1 , α0 , α1 )wi − κ
{ψ 01 (θ1 )wi − κ1 } zi

#
.

(17)

Under H0 : ϑ = ϑ0 , the just-identified GMM-estimator based on E[hE (wi , ϑ0 , γ 0 )] = 0 yields
a consistent and asymptotically normal estimator of γ 0 under the usual regularity conditions.
Now consider the inequality moment conditions mI . From Section 3.3 we see that mI2
depends on q, the vector of conditional quantiles q tk and q tk defined in Theorem 2.2. Under
the assumption that y follows a continuous distribution, as maintained in Theorem 2.2, these
can be expressed as conditional moment equalities as follows:
i
h
E 1(y ≤ q tk )|T = t, z = k − rtk (α0 , α1 ) = 0

E [1(y ≤ q tk )|T = t, z = k] − 1 − rtk (α0 , α1 ) = 0

(18)
(19)

where rtk is defined in Theorem 2.2 and t, k = 0, 1. Under H0 : ϑ = ϑ0 , a consistent estimator
rbtk of rtk can be obtained directly from pbk , the sample analogue of pk based on iid observations
of wi . In turn, the (b
rtk )th and (1 − rbtk )th sample conditional quantiles of y provide consistent
b(ϑ0 ). Now, define
estimates of q tk and q tk .13 Collecting these for all (t, k) gives q
"
hI (wi , ϑ, q) =
13

hI0 (w, ϑ, q)
hI1 (w, ϑ, q)

#
(20)

Consistency of the sample quantiles requires 0 < rtk < 1. If rtk = 0 or 1 for some (t, k), however, then
the associated moment inequality is trivially satisfied and we no longer require estimates of q tk , q tk .

18

where


1(yi ≤ q 0k )1(zi = k)(1 − Ti ) −



α1
1−α0 −α1





1(zi = k)(Ti − α0 )





1−α0
1(y
≤
q
)1(z
=
k)(1
−
T
)
−
1(zi = k)(1 − Ti − α1 )

i
i
i
0k
1−α0 −α1


hIk (wi , ϑ, q) = 

1−α1
1(zi = k)(Ti − α0 )
 1(yi ≤ q 1k )1(zi = k)Ti − 1−α
 0 −α1 

1(yi ≤ q 1k )1(zi = k)Ti − 1−αα00−α1 1(zi = k)(1 − Ti − α1 )




 . (21)




b(ϑ0 )
Equation 21 gives the unconditional version of Equations 18–19. Now, under the null q
converges in probability to q0 , which satisfies the just-identified collection of moment equalities E[hI (wi , ϑ0 , q0 )] = 0. Although hI is a discontinuous function of q, it is bounded
for any fixed (α0 , α1 ). Moreover, since y|(T = t, z = k) is a continuous random variable,
b is asymptotically
E[hI (wi , ϑ, q)] is continuously differentiable with respect to q. Hence, q
normal under mild regularity conditions.14 To account for the effect of preliminary estimation of q and γ on the asymptotic variance matrix used in the GMS test, we rely on the
following Lemma:
P
I
E
Lemma 3.1. Let m̄I1,n (ϑ) = n−1 ni=1 mI1,n (wi , ϑ) and define m̄I2,n , m̄E
n , h̄n , h̄n analogously.
I
b (ϑ0 ) = arg minγ kh̄E
b(ϑ0 ))k ≤ inf q khIn (ϑ0 , q)k + op (1).
Further let γ
n (ϑ0 , γ)k and khn (ϑ0 , q
Then, under standard regularity conditions






I 0 0
0
0
m̄I1,n (ϑ0 )

√
√  I
 


n  m̄2,n ϑ0 , q
b(ϑ0 )  →p  0 I 0 B I (ϑ0 , q 0 )
0
 n



E
b
0
0
I
0
B
(ϑ
,
γ
)
m̄E
ϑ
,
γ
(ϑ
)

0
0
0
0
n


m̄I1,n (ϑ0 )
m̄I2,n (ϑ0 , q0 )
m̄E
n (ϑ0 , γ 0 )
I
h̄n (ϑ0 , q0 )
h̄E
n (ϑ0 , γ 0 )










where we define B I (ϑ, q) = (1 − α0 − α1 ) [diag(a)]−1 q and B E (ϑ, γ) = −M E (H E )−1 with
a0 = (α1 , α1 , 1 − α1 , 1 − α1 , α1 , α1 , 1 − α1 , 1 − α1 ), and



∂ψ 2

0
−E[z
0
E[w
z
]
i]
i
i
 , HE = 
 ∂θ1 0
ME = 

∂ψ 3

0
0
−E[zi ]
E[wi zi ]

∂θ1





0



−1

0

0



∂ψ 1
∂θ1

0



E[wi ] 

∂ψ 2

E[w
]
0
−1 0
i

 ∂θ1 0
.
∂ψ 3

0
0 −1
E[w
]
i

∂θ1

0

∂ψ 1
−E[zi ] 0
0
E[w
z
]
i i
∂θ1


0

Lemma 3.1 relates the sample analogues m̄I2,n and m̄E
n evaluated at the preliminary esti14

For details, see Andrews (1994) and Newey and McFadden (1994) Section 7.

19

b(ϑ0 ) and γ
b (ϑ0 ) to their counterparts evaluated at the true parameter values q0 and
mators q
b (ϑ0 ) exactly solves hE
γ 0 . The estimator γ
b(ϑ0 ), constructed as described
n (ϑ0 , γ) = 0 while q
immediately before the statement of the Lemma, approximately solves h̄In (ϑ0 , q) = 0. A few
lines of matrix algebra show that the determinant of H E equals Cov(z, T ). Hence, B E is
well-defined if z is a relevant instrument. The matrix B I is likewise well-defined provided
that α1 =
6 0 and the elements of q0 are computed for probabilities strictly between zero and
one. If either of these conditions fails, however, some of the moment inequalities in mI2 are
trivially satisfied and can be dropped (see Footnote 13). After removing the corresponding elements of q0 and a, B I becomes well-defined. The regularity conditions required for
Lemma 3.1 are mild. The result relies on a number of mean-value expansions: h̄E
n (ϑ0 , γ 0 ) and
E
I
b (ϑ0 ) while E[h (wi , ϑ0 , q0 )] and E[mI2 (wi , ϑ0 , q0 )]
m̄n (ϑ0 , γ 0 ) are expanded around γ = γ
b(ϑ0 ). These expansions, in turn, rely on the fact that q and γ are
are expanded around q = q
interior to their respective parameter spaces and the relevant functions are all continuously
differentiable in our example.
We now have all the ingredients required to construct an asymptotic variance matrix for
0
0
0
the GMS test that accounts for preliminary estimation of γ and q. Let m0 = (mI1 , mI2 , mE ),

0
0
b 0 (ϑ0 ), q
b0 (ϑ0 ) . Given a
h0 = (hI , hE ), and define the shorthand τ 00 = (γ 00 , q00 ) and τb 00 = γ
collection of iid observations (w1 , . . . , wn ), we have
√

"
n

m̄n (ϑ0 , τ 0 )
h̄n (ϑ0 , τ 0 )

#

"


→d N 0, V(ϑ0 , τ 0 ) ,

V(ϑ0 , τ 0 ) = Var

m(wi , ϑ0 , τ 0 )
h(wi , ϑ0 , τ 0 )

#
(22)

under H0 : ϑ = ϑ0 , by an appropriate central limit theorem. What we require for the test,
√
however, is the asymptotic variance matrix of n m̄n (ϑ0 , τb 0 ). Combining Equation 22 with
Lemma 3.1, we obtain
Avar

√


n m̄n (ϑ0 , τb 0 ) = Ξ(ϑ0 , τ 0 ) V(ϑ0 , τ 0 ) Ξ0 (ϑ0 , τ 0 )

(23)

with

0
0


B(ϑ, τ ) =  B I (ϑ, q)
0

E
0
B (ϑ, γ)


Ξ(ϑ, τ ) =

h

I B(ϑ, τ )

i

,

(24)

where B I (·, ·) and B E (·, ·) are defined in Lemma 3.1. Finally, we construct a consistent
b n (ϑ0 ) of the asymptotic variance matrix of √n m̄n (ϑ0 , τb 0 ) under the null:
estimator Σ
b n (ϑ0 ) ≡ Ξ(ϑ0 , τb 0 ) V
bn (ϑ0 , τb ) Ξ0 (ϑ0 , τb 0 )
Σ

20

(25)

where
n

X
bn (ϑ, τ ) ≡ 1
V
n i=1

"

m(wi , ϑ, τ ) − m̄n (ϑ, τ )
h(wi , ϑ, τ ) − h̄n (ϑ, τ )

#"

m(wi , ϑ, τ ) − m̄n (ϑ, τ )
h(wi , ϑ, τ ) − h̄n (ϑ, τ )

#0
.

(26)

In the following section we provide a step-by-step description of our inference procedure.

3.5

Details of the Inference Procedure

In this section we provide full details of our Bonferroni-based inference procedure. We begin
by defining some notation. Let J denote the total number of inequality moment conditions,
K denote the total number of equality moment conditions, and define
"
m̄n (ϑ, τ ) =

m̄In (ϑ, q)
m̄E
n (ϑ, γ)

#



 I
(w
m
m̄In,1 (ϑ)
i , ϑ)
n
1

 1 X I

=  m̄In,2 (ϑ, q)  =
 m2 (wi , ϑ, q) 
n i=1
mE (wi , ϑ, γ)
m̄E
n (ϑ, γ)


(27)

with mI1 as defined in Equation 11, mI2 in Equations 12–14 and mE in Equation 16.15 Now
let S be the function
X

S(x, y) =
min 0, x2j + y0 y
(28)
j

where x, y are two finite-dimensional real vectors and xj denotes the j th element of x. This
function will be used to calculate the modified method of moments (MMM) test statistic as
part of the GMS test below. The argument x stands in for the moment inequalities, which
only contribute to the test statistic when they are violated, i.e. take on a negative value.
Using this notation, we now detail the first step of our inference procedure: a GMS test for
ϑ = (α0 , α1 ) with preliminary estimation of q and γ under the null.
Algorithm 3.1 (GMS Test for α0 and α1 ).
Inputs: hypothesis ϑ0 ; iid dataset {wi }ni=1 ; simulations {ζ (r) }R
r=1 ∼ iid NJ+K (0, I).
b n (ϑ0 ).
1. Calculate the variance matrix estimator Σ
b 00 )0 where γ
b0 = γ
b (ϑ0 ) and q
b0 = q
b(ϑ0 ) from Section 3.4.
(i) Calculate τb 0 = (b
q00 , γ
(ii) Calculate Ξ(ϑ0 , τb 0 ) using Equation 24.
bn (ϑ0 , τb 0 ) using Equation 26.
(iii) Calculate V
b n (ϑ0 ) = Ξ(ϑ0 , τb 0 ) V
bn (ϑ0 , τb ) Ξ0 (ϑ0 , τb 0 ).
(iv) Set Σ
15

In our problem K = 2 and J is at most 12. Under certain nulls for (α0 , α1 ), however, we drop components
of mI2 as they are trivially satisfied. See footnote 13 and Section 3.4 for further discussion.

21

2. Calculate the test statistic Tn (ϑ0 ).
√
n m̄n (ϑ0 , τb 0 ) using Equation 27.
h
n
oi−1/2 √
b n (ϑ0 )
(ii) Set ν n (ϑ0 ) = diag Σ
[ n m̄n (ϑ0 , τb 0 )].
(i) Calculate

(iii) Let ν In (ϑ0 ) denote the first J elements of ν n and ν E
n (ϑ0 ) the last K elements.

(iv) Set Tn (ϑ0 ) = S ν In (ϑ0 ), ν E
n (ϑ0 ) using Equation 28.
3. Construct the moment selection matrix Φ.
 I
√
P
(ϑ0 ) ≤ log n and let Je = Jj=1 ϕIj .
(i) For j = 1, . . . , J set ϕIj = 1 νn,j
(ii) For j = 1, . . . , K set ϕE
j = 1.
E 0
(iii) Set ϕ = (ϕI1 , . . . , ϕIJ , ϕE
1 , . . . , ϕK ) .

(iv) Let Φ be the (Je + K) × (J + K) of zeros and ones that selects those elements xj
of an arbitrary vector x that correspond to ϕj = 1.
4. Simulate the sampling distribution of Tn (ϑ0 ) under the null.
b be the correlation matrix that corresponds to Σ
b n (ϑ0 ).
(i) Let Ω
h
i1/2
b Φ0
(ii) For each r = 1, . . . , R set ξ (r) = Φ Ω
Φζ (r) .
(r)
(r)
(iii) Let ξ I denote the first Je and ξ E the last K elements of ξ (r) .
(r)
(r)
(r) 
(iv) For each r = 1, . . . , R set Tn (ϑ0 ) = S ξ I , ξ E using Equation 28.
R

1 X  (r)
5. Calculate the p-value of the test: pb(ϑ0 ) =
1 Tn (ϑ0 ) > Tn (ϑ0 ) .
R r=1
Algorithm 3.1 corresponds to the asymptotic version of the GMS test from Andrews
and Soares (2010), based on the MMM test statistic – S1 in Andrews and Soares (2010)
√
– and the “BIC choice” κn = log n for the sequence of constants κn used for moment
selection. The procedure is as follows. In Step 1, we compute a consistent estimator of the
asymptotic variance matrix of the full set of moment conditions, under the null, accounting
for preliminary estimation of q and γ as explained in Section 3.4. In step 2, we calculate
the observed value of the MMM test statistic. Note that this test statistic uses only the
b n (ϑ0 ). Moreover, the moment inequalities only contribute to Tn if
diagonal elements of Σ
they are violated, i.e. if they take on a negative value. In step 3 we determine which moment
√
inequalities are “far from binding,” defined as having a t-ratio greater than log n. These
moment inequalities will be excluded when approximating the large-sample distribution of
22

the test statistic. The matrix Φ encodes the results of the moment selection step. Pre–
e whose last K elements
multiplying a (J + K)–vector x by Φ results in a (Je × K)–vector x
match the last K elements of x but whose first Je elements contain the subset of (x1 , . . . , xJ )
whose indices match those of the moment inequalities with t-ratios less than or equal to
√
log n, i.e. those that are not far from binding.16 Step 4 uses a collection of iid normal draws,
{ζ (r) }R
r=1 , to approximate the large-sample distribution of Tn under the null. The appropriate
multiplications by Φ ensure that this approximation includes all moment equalities, but
excludes any moment inequality judged to be far from binding in step 3. Finally, step 5
computes the p-value of the test by comparing the actual test statistic Tn (ϑ0 ) to the collection
(r)
of simulated test statistics {Tn (ϑ0 )}R
r=1 from step 4. We now detail our Bonferroni-based
17
confidence interval for β.
Algorithm 3.2 (Bonferroni-based Confidence Interval for β).
Inputs: significance levels (δ1 , δ2 ); iid dataset {wi }ni=1 ; simulations {ζ (r) }R
r=1 ∼ iid NJ+K (0, I).
1. Construct a (1 − δ1 ) × 100% joint confidence set C(δ1 ) for ϑ = (α0 , α1 )0 .

(i) Let ΛN = 0, N1 , N2 , . . . , NN−2 , NN−1

where N > 1 is a natural number.

(ii) Set ∆N = {(α0 , α1 ) ∈ (ΛN × ΛN ) : α0 + α1 < 1}.
(iii) For each ϑ ∈ ∆N calculate pb(ϑ) using Algorithm 3.1, holding {ζ (r) }R
r=1 fixed.
(iv) Set C(δ1 ) = {ϑ ∈ ∆N : pb(ϑ) ≥ δ1 }.
2. Construct a (1 − δ1 ) × 100% confidence interval [s(δ1 ), s(δ1 )] for s ≡ (1 − α0 − α1 ).
(i) Set s(δ1 ) = min {(1 − α0 − α1 ) : (α0 , α1 ) ∈ C(δ1 )}.
(ii) Set s(δ1 ) = max {(1 − α0 − α1 ) : (α0 , α1 ) ∈ C(δ1 )}.


3. Construct a (1 − δ2 ) × 100% confidence interval θ1 (δ2 ), θ1 (δ2 ) for θ1 .
(i) Use the standard IV interval from a regression of y on T with instrument z.


4. Construct the (1 − δ) × 100% Bonferroni-based confidence interval β(δ), β(δ) for β.
16
Although this does not affect the results of the procedure, notice that Algorithm 3.1 carries out moment
selection in a slightly different way from the steps given by Andrews and Soares (2010). In particular, before
b and normal vectors ζ (r) to remove
carrying out any further calculations, we subset the correlation matrix Ω
elements corresponding to moment inequalities deemed far from binding. In contrast, Andrews and Soares
(2010) carry along the full set of inequalities throughout, but add +∞ to the appropriate elements when
(r)
computing Tn to ensure that only the moment inequalities that are not far from binding affect the results.
Although it requires more notation to describe, sub-setting is substantially faster, as it avoids carrying out
computations for inequalities that cannot affect the result.
17
Code implementing this procedure is available at https://github.com/fditraglia/mbereg.

23

(i) Let δ = δ1 + δ2 .
(ii) Set β(δ) = min {s(δ1 ) × θ1 (δ2 ), s(δ1 ) × θ1 (δ2 )}.

(iii) Set β(δ) = max s(δ1 ) × θ1 (δ2 ), s(δ1 ) × θ1 (δ2 ) .
Step 1 of Algorithm 3.2 constructs a (1 − δ1 ) × 100% joint confidence set C(δ1 ) for
ϑ = (α0 , α1 ) by inverting the GMS test from Algorithm 3.1 over a discretized parameter
space ∆N . Because the parameter space for (α0 , α1 ) is bounded, this is computationally
straightforward. Note that the same normal draws {ζ (r) }R
r=1 are used to test each null
hypothesis contained in ∆N . Step 2 projects C(δ1 ) to yield a (1 − δ1 ) × 100% confidence
interval for s ≡ (1 − α0 − α1 ), simply taking the maximum and minimum values of s in the
discrete set C(δ1 ). Step 3 constructs the usual IV confidence interval for the reduced form
parameter θ1 , and step 4 combines the results of steps 2–3 with Bonferroni’s inequality to
yield a (1 − δ1 − δ2 ) × 100% confidence interval for β. For some discussion of alternatives
to Algorithm 3.2, see Footnote 12. Notice that, by construction, the Bonferroni interval for
β excludes zero if and only if the confidence interval for θ1 from step 3 of Algorithm 3.2
excludes zero. Under mild regularity conditions, the confidence interval from Algorithm 3.2
is uniformly asymptotically valid.
Theorem 3.1. Let w1 , . . . , wn be an iid collection of observations satisfying the conditions
of Theorems 2.2 and 2.3, and let z be a strong instrument. Then, under standard regularity conditions, the confidence interval for β from Algorithm 3.2 has asymptotic coverage
probability no less than 1 − (δ1 + δ2 ) as R, N, n → ∞ uniformly over the parameter space.
Theorem 3.1 is effectively a corollary of Theorem 1 from Andrews and Soares (2010),
which establishes the uniform asymptotic validity of the GMS test, and Lemma 3.1, which
accounts for preliminary estimation of γ and q. Given iid observations wi , the only substan√
tive condition required for Theorem 3.1 is the joint asymptotic normality of n m̄n (ϑ0 , τ 0 )
√
and nh̄n (ϑ0 , τ 0 ), where h̄n denotes the sample analogues for the full set of auxiliary moment conditions (hI , hE ) defined in Section 3.4. For further discussion of the regularity
conditions required for the GMS procedure, see Appendix A3 of Andrews and Soares (2010).
For some discussion of the regularity conditions required for Lemma 3.1, see Section 3.4.
Theorem 3.1 invokes the higher-moment assumptions (Assumptions 2.5–2.6) under which
we establish global identification of β in Theorem 2.3, and Algorithm 3.1 likewise incorporates
the higher-moment equality conditions that arise from this result. To proceed without these
conditions, simply remove mE from the set of moment conditions used in the algorithm
and leave the steps unchanged. In this case β is no longer point identified but the inference
procedure provides valid inference for the points in the sharp identified set from Theorem 2.2.
24

Algorithm 3.2 can likewise be used in the case of an exogenous T ∗ , as in Mahajan (2006) and
Frazis and Loewenstein (2003). As mentioned above in Section 3.1, the exogenous regressor
case is subject to the same inferential difficulties as the endogenous case on which we focus
in this paper. To accommodate an exogenous regressor, simply replace mE with the moment
equalities described in Appendix C.

3.6

Further Details Regarding Covariates

The inference procedure described in the preceding sections holds x fixed, and is thus appropriate for examples with discrete covariates. To accommodate covariates more generally,
there are several possible approaches. At one extreme, suppose one were willing to assume
that (α0 , α1 ) did not vary with x and that y = c+βT ∗ +x0 φ+ε, as in Frazis and Loewenstein
(2003). In this case, the standard IV estimator identifies φ and one could simply augment
the moment equalities mE from Equation 16 above to provide a preliminary estimator of
φ in Algorithm 3.1. At the other extreme, if one wished to remain fully non-parametric,
one could adopt the approach of Andrews and Shi (2014), based on kernel averaging near
a fixed covariate value x = x0 . As a compromise between these two extremes, one could
alternatively specify a semi-parametric model, perhaps along the lines of Section 4 of Lewbel
(2007), and follow the approach of Andrews and Shi (2013). Both of these latter possibilities
could be an interesting extension of the method described above.

4

Simulation Study

In this section we present results from a simulation study using the inference procedure
described in Section 3.5 above. Unless otherwise specified, all calculations are based on 2000
simulation replications with n = 1000 using Algorithm 3.2 with R = 5000 simulation draws.
Supplementary simulation results appear in Appendix D.

4.1

Simulation DGP

Our simulation design generates n iid draws of the observables (yi , Ti , zi ) as follows:
1. Generate the instrumental variable z.
(i) For each 1 ≤ i ≤ n/2 set zi = 0.
(ii) For each n/2 < i ≤ n, set zi = 1.

25

2. Generate the error terms:
"

ηi
εi

#

"
∼ iid N

0
0

# "
,

1 ρ
ρ 1

#!
.

3. Generate the unobserved regressor: Ti∗ = 1 {d0 + d1 zi + ηi > 0}.
4. Generate the outcome: yi = c + βTi∗ + εi .
5. Generate the observed, mis-classified regressor T .
(i) For all i with Ti∗ = 0 draw Ti ∼ iid Bernoulli(α0 ).
(ii) For all i with Ti∗ = 1 draw Ti ∼ iid Bernoulli(1 − α1 ).
This DGP generates random variables that satisfy the conditions of Theorems 2.2 and 2.3.
Thus β is point identified, and all moment equalities and inequalities from Section 3 hold at
the true parameter values of the DGP. Note from step 1 that we condition on the instrument
z, holding it fixed in repeated samples. Our simulation varies the parameters (α0 , α1 , β, n)
over a grid. Because ε has unit variance, values for β are measured in standard deviations of
the error. For simplicity we present results for c = 0, d0 = Φ−1 (0.15), and d1 = Φ−1 (0.85) −
Φ−1 (0.15), where Φ−1 (·) denotes the quantile function of a standard normal random variable.
Using these values for (d0 , d1 ) holds the unobserved first stage probabilities fixed: p∗0 = 0.15
and p∗1 = 0.85. In contrast the observed first-stage probabilities p0 and p1 vary with (α0 , α1 )
according to Lemma 2.1.

4.2

Simulation Results

As explained in Section 3.1 above, the just-identified, unconstrained GMM estimator based
on Equation 10 suffers from weak identification and boundary value problems. Moreover,
the estimator may not even exist in finite samples. Even when the GMM estimator exists,
its asymptotic variance matrix could be numerically singular, so that the standard GMM
confidence interval is undefined. Table 1 reports the percentage of simulation draws for
which the standard GMM confidence interval is undefined, while Table 2 reports the coverage
probability of a nominal 95% GMM confidence interval, conditional on its existence.
We see from Table 1 that when β is small compared to the error variance, the GMM
confidence interval fails to exist with high probability. When β = 0.5, for example, the
interval is undefined approximately 30% of the time. As β increases, however, it becomes less
likely that the GMM interval is undefined. All else equal, larger amounts of mis-classification,
i.e. higher values for (α0 , α1 ), increase the probability that the GMM interval fails to exist.
26

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
27
27
26
26
26
26
27
25
26
26
26
26
26
24
26
26

0.25
33
32
33
34
32
36
35
35
33
33
35
35
32
35
32
35

0.5
30
29
32
30
31
32
31
32
30
30
33
33
32
33
35
35

β
0.75
14
13
15
17
14
16
18
21
15
19
22
26
16
21
27
28

1
1
2
4
5
2
4
8
11
3
6
12
15
6
11
15
21

1.5
0
0
0
0
0
0
0
1
0
0
1
3
0
1
4
7

2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2

3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Table 1: Percentage of replications for which the standard GMM confidence interval based on
Equation 10 fails to exist, either because the point estimate is NaN or the asymptotic covariance
matrix is numerically singular. Calculations are based on 2000 replications of the DGP from 4.1
with n = 1000.

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
72
72
73
73
73
73
73
74
74
73
73
73
74
75
74
73

0.25
62
62
61
59
63
58
59
59
62
60
58
58
62
59
61
58

0.5
62
63
61
62
60
59
61
58
60
61
57
56
60
58
56
55

β
0.75
80
79
77
76
78
77
75
71
78
74
70
66
76
71
65
64

1
92
92
90
88
91
90
86
82
91
87
81
78
89
82
78
71

1.5
95
95
96
95
95
95
95
94
95
95
93
92
95
93
90
88

2
94
96
96
96
96
95
95
96
96
96
95
95
96
96
96
93

3
95
95
96
95
96
94
94
96
96
94
95
96
96
95
96
96

Table 2: Coverage (%) of the standard nominal 95% GMM confidence interval for β based on
Equation 10. Coverage is calculated only for those simulation draws for which the interval exists.
(See Table 1.) Calculations are based on 2000 replications of the DGP from 4.1 with n = 1000.

27

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
19.07
17.52
17.41
18.23
17.13
17.88
17.37
18.07
17.79
18.98
18.25
19.03
18.27
19.4
18.22
17.56

0.25
3.44
3.47
3.51
3.34
3.51
3.33
3.36
3.33
3.39
3.43
3.26
3.31
3.48
3.41
3.56
3.55

0.5
1.86
1.92
1.9
1.92
1.86
1.85
1.95
1.98
1.92
1.96
1.92
2.02
1.87
1.96
1.96
2.13

β
0.75
1.32
1.41
1.45
1.48
1.38
1.45
1.54
1.63
1.45
1.54
1.64
1.75
1.5
1.63
1.74
1.96

1
0.87
1
1.1
1.24
0.97
1.13
1.24
1.41
1.11
1.26
1.45
1.66
1.25
1.43
1.67
1.86

1.5
0.47
0.61
0.76
0.91
0.61
0.78
0.97
1.17
0.75
0.97
1.2
1.49
0.9
1.18
1.49
1.86

2
0.37
0.51
0.65
0.79
0.51
0.67
0.85
1.04
0.65
0.84
1.06
1.33
0.79
1.04
1.35
1.74

3
0.35
0.46
0.58
0.7
0.46
0.6
0.75
0.92
0.58
0.75
0.95
1.19
0.7
0.92
1.19
1.55

Table 3: Median width of the standard nominal 95% GMM confidence interval for β based on
Equation 10. Coverage is calculated only for those simulation draws for which the interval exists.
Calculations are based on 2000 replications of the DGP from 4.1 with n = 1000.

Turning our attention to the simulation draws for which it is well-defined, we see from
Tables 2 and 3 that the GMM confidence interval performs extremely poorly when β is
small. Substantial size distortions persist until β is 1.5 or larger. All else equal, the size
distortions are more severe the larger the amount of mis-classification error. For sufficiently
large β, however, standard GMM inference performs well. As β grows, the weak identification
problem vanishes. For large enough β the inference problem in effect becomes standard.
We now examine the performance of the Bonferroni-based confidence interval from Algorithm 3.2, beginning with its first step: a joint GMS confidence set for (α0 , α1 ). Table
4 presents coverage probabilities for a nominal 97.5% GMS confidence set for (α0 , α1 ). Because these results are extremely fast to compute, Table 4 is based on 10,000 simulation
replications. Aside from some slight under-coverage at intermediate values of (α0 , α1 ) when
β = 3, the GMS interval makes good on its promise of uniformly valid inference. As shown
in Appendix D, the under-coverage problem appears to be a finite-sample artifact: if we
increase n to 2000, the maximum size distortion becomes negligible. The GMS test tends,
however, to be fairly conservative, particularly for larger values of (α0 , α1 ). When there is no
mis-classification error, the GMS confidence sets are very nearly exact. Results for nominal
95% and 90% intervals are qualitatively similar: see Appendix D.
We now present results for the Bonferroni interval from Algorithm 3.2, setting δ1 = δ2 =

28

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
97.7
98.0
98.4
98.5
98.1
98.6
99.0
99.4
98.6
99.0
99.5
99.7
98.7
99.4
99.8
100.0

0.25
97.7
98.7
98.5
98.8
98.5
99.1
99.3
99.7
98.5
99.5
99.7
99.8
98.7
99.6
99.8
99.9

0.5
97.6
98.8
98.9
98.8
98.3
99.5
99.7
99.8
98.6
99.7
99.8
99.8
98.8
99.6
99.7
99.9

β
0.75
97.7
99.1
98.9
99.0
98.8
99.6
99.8
99.8
98.9
99.7
99.7
99.8
98.7
99.7
99.8
99.8

1
98.0
98.8
98.8
98.7
98.8
99.6
99.7
99.6
98.7
99.4
99.4
99.5
98.7
99.4
99.5
99.6

1.5
98.0
98.4
98.6
98.4
98.4
98.8
98.9
99.0
98.2
99.0
99.0
99.0
98.2
98.9
99.1
99.5

2
97.4
97.1
98.0
97.8
96.8
97.7
97.5
98.2
97.7
98.1
97.8
98.7
98.1
98.3
98.5
99.1

3
97.9
96.4
97.0
97.5
95.7
95.2
95.7
96.7
97.0
96.5
96.8
97.7
97.6
96.8
97.8
98.8

Table 4: Coverage probability (1 - size) in percentage points of a 97.5% GMS joint test for α0 and
α1 using Algorithm 3.1 with n = 1000. Calculations are based on 10,000 replications of the DGP
from Section 4.1.

0.025 to yield an interval with asymptotic coverage no less that 95%.18 Table 5 presents
coverage probabilities in percentage points and Table 5 presents median widths.
The Bonferroni interval achieves its stated minimum coverage uniformly over the parameter space. When there is no mis-classification, α0 = α1 , its actual coverage is close or equal
to 95%. In the presence of mis-classification, however, the interval can be quite conservative,
particularly for larger values of β. For smaller but nonzero values of β, this conservatism
reflects the fact that the model is effectively partially identified: although Theorem 2.3 shows
that (α0 , α1 ) are point identified for any β 6= 0, the amount of data required to distinguish
one pair of alphas from another when β is small would be astronomical.
In spite of its conservatism, the Bonferroni interval is informative, as we see from the
median widths in Table 6. Because median widths provide only a limited picture of the
behavior of a confidence interval, Figures 1–3 present further evidence in the form of coverage
functions (1 - power) for β = 0.5, 1, 3. Coverage curves for additional values of β and n appear
in Appendix D. Each figure holds the true value of β fixed and varies (α0 , α1 ) over the grid
{0, 0.1, 0.2} × {0, 0.2, 0.2}. The plots within each Figure give coverage in percentage points
as a function of the specified alternative for β. Solid curves are computed using the full
18

In principle, one could optimize the choice of δ1 subject to the constraint δ1 + δ2 = 0.95 to reduce the
width of the resulting interval. In our experiments, there was no choice of δ1 that uniformly dominated for
all values of (α0 , α1 , β) so we report only results for δ1 = δ2 here.

29

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
96
97
98
97
97
98
98
97
97
98
98
98
97
97
98
98

0.25
97
99
99
100
99
100
100
100
99
100
100
100
99
100
100
100

0.5
97
99
99
100
99
100
100
100
99
100
100
100
100
100
100
100

0.75
96
99
100
100
99
100
100
100
100
100
100
100
100
100
100
100

1
97
99
100
100
100
100
100
100
100
100
100
100
100
100
100
100

1.5
97
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

2
95
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

3
96
99
100
100
98
100
100
100
100
100
100
100
100
100
100
100

Table 5: Coverage probability in percentage points of a nominal > 95% Bonferroni confidence
interval for β using Algorithm 3.2 with n = 1000, R = 5000 and δ1 = δ2 = 0.025. Calculations are
based on 2000 replications of the DGP from Section 4.1.

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
0.4
0.45
0.51
0.58
0.45
0.51
0.58
0.67
0.51
0.58
0.67
0.81
0.58
0.68
0.81
1.01

0.25
0.41
0.47
0.54
0.62
0.47
0.54
0.63
0.75
0.54
0.63
0.75
0.91
0.62
0.74
0.91
1.16

0.5
0.43
0.54
0.65
0.79
0.54
0.66
0.8
1
0.65
0.81
1.01
1.3
0.8
1.01
1.3
1.74

0.75
0.43
0.59
0.76
0.95
0.59
0.77
0.98
1.25
0.76
0.99
1.29
1.7
0.95
1.26
1.7
2.35

1
0.43
0.63
0.85
1.07
0.63
0.86
1.12
1.46
0.86
1.14
1.54
2.09
1.09
1.49
2.11
2.93

1.5
0.42
0.7
0.95
1.17
0.7
1.03
1.38
1.74
0.96
1.42
1.97
2.73
1.18
1.84
2.8
4.17

2
0.41
0.75
1.01
1.24
0.76
1.18
1.55
1.94
1.02
1.64
2.33
3.13
1.25
2.13
3.4
5.2

3
0.41
0.86
1.17
1.48
0.88
1.46
1.88
2.4
1.19
2.08
2.9
3.9
1.5
2.78
4.48
6.85

Table 6: Median width of a nominal > 95% Bonferroni confidence interval for β using Algorithm
3.2 with n = 1000, R = 5000 and δ1 = δ2 = 0.025. Calculations are based on 2000 replications of
the DGP from Section 4.1.

30

set of inequality moment conditions from Section 3.3, while dashed curves use only mI1 , i.e.
they do not impose the restrictions implied by non-differential measurement error. In each
figure, the dashed horizontal line gives the nominal coverage probability, 95%, while the
dashed vertical lines are the reduced from and instrumental variables estimands: for β ≥ 0
the reduced form is always smaller than the IV.
As seen from Figures 1–3, and their counterparts in Appendix D, the Bonferroni procedure has power against the alternative β = 0, even when the true value of β is small. As
described in Section 3.5, the Bonferroni interval excludes zero if and only if the confidence
interval for θ1 from which it is constructed also excludes zero. These figures also indicate the
gains from including mI2 , the moment inequalities that emerge from assuming non-differential
measurement error: substantial increases in power against alternatives between the true parameter value and zero, particularly for larger values of β. Note moreover that the excellent
performance of Bonferroni in the zero mis-classification case (α0 , α1 ) depends crucially on
imposing the assumption of non-differential measurement error. As the true value of β increases, the Bonferroni interval begins to have power against both the reduced form and IV
estimands.
A drawback of the identification-robust inference procedure from Algorithm 3.2 becomes
apparent when both β and the mis-classification probabilities are large. In this case the
confidence interval for β is excessively wide, as we see from Table 6 and Figure 3.19 Note
from Tables 1 and 2, that this is a region of the parameter space in which the plain-vanilla
GMM confidence interval yields valid inference. Moreover, we see from Table 3 that the
median width of the GMM interval is far more reasonable when β is large, even in the
presence of large amounts of mis-classification. It is important to stress that the source of
this excess width is not the Bonferroni correction: the same behavior emerges if one projects
a joint GMS confidence set for (α0 , α1 , β) to yield marginal inference for β. Rather, it is
the inevitable cost of applying a robust inference procedure in a region of the parameter
space where standard inference performs well. While a detailed theoretical investigation of
this problem is beyond the scope of the present paper, we now explore the performance of
a “hybrid” confidence interval that uses a simple heuristic to transition between robust and
standard inference.20 The procedure for constructing the hybrid interval is as follows. First
compute the robust confidence interval based on Algorithm 3.2. Next, determine whether
the GMM interval is well-defined: if so, determine whether it is contained within the robust
interval. If the GMM interval exists and lies within the robust interval, report GMM;
otherwise report the robust interval. Table 7 presents coverage probabilities (in percentage
19
20

As expected, median widths decrease with sample size: see the results for n = 2000 in Appendix D.
This idea is related to Andrews (2016), although somewhat different in its details.

31

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
96
97
98
97
97
98
98
97
97
98
98
98
97
97
98
98

0.25
97
99
99
100
99
100
100
100
99
100
100
100
99
100
100
100

β
0.75
96
99
100
100
99
100
100
100
100
100
100
98
100
100
98
96

0.5
97
99
99
100
99
100
100
100
99
100
100
100
100
100
100
99

1
97
99
100
99
100
100
99
97
100
99
96
95
100
97
94
92

1.5
97
98
97
96
98
96
96
95
96
96
95
95
95
94
94
94

2
95
96
96
96
97
96
96
96
96
96
95
95
96
96
96
95

3
93
95
96
96
95
96
95
96
96
96
96
96
97
96
96
96

Table 7: Coverage probabilities (%) of a hybrid confidence interval constructed from the nominal
95% standard GMM interval and the > 95% Bonferroni confidence interval for β using Algorithm
3.2 with n = 1000, R = 5000 and δ1 = δ2 = 0.025. The hybrid interval reports Bonferroni unless
the GMM interval exists and is contained within the Bonferroni interval. Calculations are based
on 2000 replications of the DGP from Section 4.1.

points) and Table 8 median widths for the resulting hybrid confidence interval. Coverage
plots for β = 1, 2, 3 appear in Figures 4–6. Plots for additional values of β and n appear in
Appendix D. The conventions of these figures are identical to those of Figures 1–3 with one
exception: in Figures 4–6 the dashed curves correspond to the hybrid confidence interval.
The hybrid interval performs extremely well: with the exception of a slight size distortion
at (α0 = α1 = 0.3, β = 1) and (α0 = α1 = 0, β = 3), it is effectively a free lunch.21 Note in
particular that the coverage curves for the hybrid interval from Figures 4–6 (dashed curves)
lie uniformly below those of the Bonferroni interval (solid curves) while still maintaining
correct coverage at the true value of β. It could be interesting to investigate this idea further
in future work.

21

The distortion at (α0 = α1 = 0.3, β = 1) disappears when n increases to 2000: see Appendix D.

32

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
0.4
0.45
0.51
0.58
0.45
0.51
0.58
0.67
0.51
0.58
0.67
0.81
0.58
0.68
0.81
1.01

0.25
0.41
0.47
0.54
0.62
0.47
0.54
0.63
0.75
0.54
0.63
0.75
0.91
0.62
0.74
0.91
1.16

0.5
0.43
0.54
0.65
0.79
0.54
0.66
0.8
1
0.65
0.81
1.01
1.3
0.8
1.01
1.3
1.73

0.75
0.43
0.59
0.76
0.95
0.59
0.77
0.97
1.25
0.76
0.99
1.29
1.67
0.95
1.26
1.66
2.24

1
0.43
0.63
0.84
1.05
0.63
0.86
1.11
1.4
0.85
1.12
1.48
1.95
1.07
1.43
1.98
2.71

1.5
0.42
0.67
0.82
0.96
0.67
0.92
1.17
1.4
0.83
1.18
1.56
1.77
0.95
1.48
1.94
2.33

2
0.4
0.52
0.65
0.79
0.51
0.69
0.87
1.06
0.65
0.86
1.08
1.35
0.8
1.06
1.37
1.78

3
0.35
0.46
0.58
0.7
0.46
0.61
0.75
0.92
0.58
0.75
0.95
1.2
0.7
0.93
1.19
1.55

Table 8: Median width of a hybrid confidence interval constructed from the nominal 95% standard
GMM interval and the > 95% Bonferroni confidence interval for β using Algorithm 3.2 with n =
1000, R = 5000 and δ1 = δ2 = 0.025. The hybrid interval reports Bonferroni unless the GMM
interval exists and is contained within the Bonferroni interval. Calculations are based on 2000
replications of the DGP from Section 4.1.

33

0.0

0.5

1.0

1.5

80
0

20

40

60

80
60
40
20
0

0

20

40

60

80

100

α0 = 0.2, α1 = 0

100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

0.0

1.0

1.5

1.0

1.5

100
60
40
20
0.5

1.0

1.5

1.0

1.5

100
0

20

40

60

80

100
60
40
20
0
1.5

0.5

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

1.0

0.0

α0 = 0.1, α1 = 0.2

0

0.5

1.5

0
0.0

α0 = 0, α1 = 0.2

0.0

1.0

80

100
80
40
20
0
0.5

0.5

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0
0.0

0.0

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

0.5

0.0

0.5

1.0

1.5

0.0

0.5

1.0

1.5

Figure 1: Coverage curves (1 - power) for β when the truth is β = 0.5, from a nominal > 95%
Bonferroni confidence interval using Algorithm 3.2, with n = 1000 and R = 5000. The solid curve
uses all moment inequalities from Section 3.3 in the GMS step, while the dashed curve excludes
mI2 , those implied by non-differential measurement error. The dashed horizontal line gives the
nominal coverage (95%), while dashed vertical lines are the reduced form estimand (left) and the
IV estimand (right). Calculations are based on 2000 replications of the DGP from Section 4.1.

34

0.5

1.0

1.5

2.0

2.5

80
0

20

40

60

80
60
40
20
0

0

20

40

60

80

100

α0 = 0.2, α1 = 0

100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

0.5

1.5

2.0

2.5

1.5

2.0

2.5

100
60
40
1.5

2.0

2.5

2.5

1.0

1.5

2.0

2.5

100
0

20

40

60

80

100
60
40
20
0
2.0

0.5

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

1.5

2.5

20
1.0

α0 = 0.1, α1 = 0.2

0

1.0

2.0

0
0.5

α0 = 0, α1 = 0.2

0.5

1.5

80

100
80
40
20
0
1.0

1.0

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0

0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

1.0

0.5

1.0

1.5

2.0

2.5

0.5

1.0

1.5

2.0

2.5

Figure 2: Coverage curves (1 - power) for β when the truth is β = 1, from a nominal > 95%
Bonferroni confidence interval using Algorithm 3.2, with n = 1000 and R = 5000. The solid curve
uses all moment inequalities from Section 3.3 in the GMS step, while the dashed curve excludes
mI2 , those implied by non-differential measurement error. The dashed horizontal line gives the
nominal coverage (95%), while dashed vertical lines are the reduced form estimand (left) and the
IV estimand (right). Calculations are based on 2000 replications of the DGP from Section 4.1.

35

α0 = 0.2, α1 = 0

2

3

4

5

6

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

100

α0 = 0, α1 = 0

2

4

5

6

4

5

6

100
60
40
4

5

6

6

3

4

5

6

100
0

20

40

60

80

100
60
40
20
0
5

2

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

4

6

20
3

α0 = 0.1, α1 = 0.2

0

3

5

0
2

α0 = 0, α1 = 0.2

2

4

80

100
80
40
20
0
3

3

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0

2

2

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

3

2

3

4

5

6

2

3

4

5

6

Figure 3: Coverage curves (1 - power) for β when the truth is β = 3, from a nominal > 95%
Bonferroni confidence interval using Algorithm 3.2, with n = 1000 and R = 5000. The solid curve
uses all moment inequalities from Section 3.3 in the GMS step, while the dashed curve excludes
mI2 , those implied by non-differential measurement error. The dashed horizontal line gives the
nominal coverage (95%), while dashed vertical lines are the reduced form estimand (left) and the
IV estimand (right). Calculations are based on 2000 replications of the DGP from Section 4.1.

36

α0 = 0.2, α1 = 0

0.5

1.0

1.5

2.0

2.5

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

2.5

1.5

2.0

2.5

100
60
40
1.5

2.0

2.5

2.5

1.0

1.5

2.0

2.5

100
0

20

40

60

80

100
60
40
20
0
2.0

0.5

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

1.5

2.5

20
1.0

α0 = 0.1, α1 = 0.2

0

1.0

2.0

0
0.5

α0 = 0, α1 = 0.2

0.5

1.5

80

100
60
40
20
0
1.0

1.0

α0 = 0.2, α1 = 0.1

80

80
60
40
20
0
0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

0.5

1.0

1.5

2.0

2.5

0.5

1.0

1.5

2.0

2.5

Figure 4: Comparison of Coverage curves (1 - power) for β when the truth is β = 1: the solid curve
corresponds the Bonferroni nominal > 95% interval from Algorithm 3.2 and the dashed curve to the
hybrid interval from Tables 7–8. The dashed horizontal line gives the nominal coverage (95%), while
dashed vertical lines are the reduced form estimand (left) and the IV estimand (right). Results are
based on 2000 simulation replications from the DGP in Section 4.1 with n = 1000.

37

80
20
0

0

20

40

60
40

60

80

80
60
40
20
0

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0
100

α0 = 0, α1 = 0

α0 = 0.2, α1 = 0.1

80
0

20

40

60

80
60
40
20
0

0

20

40

60

80

100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0.1, α1 = 0.1
100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0, α1 = 0.1
100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0.2, α1 = 0.2

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

80
0

20

40

60

80
60
40
20
0

0

20

40

60

80

100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0.1, α1 = 0.2
100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0, α1 = 0.2
100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

Figure 5: Comparison of Coverage curves (1 - power) for β when the truth is β = 2: the solid curve
corresponds the Bonferroni nominal > 95% interval from Algorithm 3.2 and the dashed curve to the
hybrid interval from Tables 7–8. The dashed horizontal line gives the nominal coverage (95%), while
dashed vertical lines are the reduced form estimand (left) and the IV estimand (right). Results are
based on 2000 simulation replications from the DGP in Section 4.1 with n = 1000.

38

α0 = 0.1, α1 = 0
100

α0 = 0.2, α1 = 0

2

3

4

5

6

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0, α1 = 0

2

3

4

5

6

4

5

6

100
60
40
4

5

6

6

3

4

5

6

100
0

20

40

60

80

100
60
40
20
0
5

2

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

4

6

20
3

α0 = 0.1, α1 = 0.2

0

3

5

0
2

α0 = 0, α1 = 0.2

2

4

80

100
60
40
20
0
3

3

α0 = 0.2, α1 = 0.1

80

80
60
40
20
0
2

2

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

2

3

4

5

6

2

3

4

5

6

Figure 6: Comparison of Coverage curves (1 - power) for β when the truth is β = 3: the solid curve
corresponds the Bonferroni nominal > 95% interval from Algorithm 3.2 and the dashed curve to the
hybrid interval from Tables 7–8. The dashed horizontal line gives the nominal coverage (95%), while
dashed vertical lines are the reduced form estimand (left) and the IV estimand (right). Results are
based on 2000 simulation replications from the DGP in Section 4.1 with n = 1000.

39

5

Conclusion

This paper has studied identification and inference for a mis-classified, binary, endogenous
regressor in an additively separable model using a discrete instrumental variable. We have
shown that the only existing identification result for this model is incorrect, and gone on to
derive the sharp identified set under standard first-moment assumptions from the literature.
Strengthening these assumptions to hold for second and third moments, we have established
point identification for the effect of interest. Inference in models with mis-classification error is complicated by problems of weak identification and parameters on the boundary. To
address these challenges, we have proposed a Bonferroni-based procedure for identification
robust inference, using both the moment equalities from our identification results and moment inequalities from our partial identification results. This procedure is computationally
attractive and performs well in simulations. An interesting extension of the results presented
here would be to explore the more general case of a discrete endogenous regressor subject
to mis-classification error, possibly by combining our approach with the matrix factorization
techniques from Hu (2008). Another interesting extension, inspired by our hybrid confidence
interval heuristic from Section 4, would be to study the transition between robust and standard inference in moment condition models. It may be possible, for example, to adapt the
techniques of Andrews (2016) in this direction to provide similar theoretical guarantees.

A

Proofs

Throughout the following arguments, we suppress dependence on x for simplicity.

A.1

Partial Identification Results

Proof of Lemma 2.1. Follows from a simple calculation using the law of total probability.
Proof of Lemma 2.2. Immediate since Cov(z, T ) = (1 − α0 − α1 )Cov(z, T ∗ ) by Lemma 2.1.
Proof of Theorem 2.1. We first show that so long as α0 ≤ pk ≤ 1 − α1 then we can construct
a valid joint probability distribution for (T ∗ , T, z) that satisfies our assumptions. First decompose
the joint probability mass function as
p(T ∗ , T, z) = p(T |T ∗ , z)p(T ∗ |z)p(z).
By Assumption 2.2 (ii), p(T |T ∗ , z) = p(T |T ∗ ) and thus α0 and α1 fully determine p(T |T ∗ , z). Under
the proposed bounds, α0 and α1 are clearly valid probabilities. Since p(z) is observed, it thus suffices
to ensure that p(T ∗ |z) is a valid probability mass function. By Lemma 2.1, p∗k = (pk −α0 )/(1−α0 −
α1 ) and hence 0 ≤ p∗k ≤ 1 if and only if α0 ≤ pk ≤ 1 − α1 . Since (pk − p` ) = (p∗k − p∗` )(1 − α0 − α1 ),
we have p∗k =
6 p∗` provided that pk − p` 6= 0

40

We now show how to construct a valid conditional distribution for y given (T ∗ , T, z) that satisfies
our assumptions if β(pk − α0 ) = (1 − α0 − α1 )[E(y|z = k) − c] for all k. Define
rtk ≡ P(T ∗ = 1|T = t, z = k)

Ft (τ ) ≡ P(y ≤ τ |z = k)
∗

Ftk (τ ) ≡ P(y ≤ τ |T = t, z = k)

t
Ftk
(τ ) ≡ P(y ≤ τ |T ∗ = t∗ , T = t, z = k)

Gk (τ ) ≡ P(ε ≤ τ |z = k)

Gttk (τ ) ≡ P(ε ≤ τ |T ∗ = t∗ , T = t, z = k).

∗

∗

∗

t for each t∗ , namely
Assumption 2.1 (i) implies a relationship between Gttk and Ftk
0
G0tk (τ ) = Ftk
(τ + c),

1
G1tk (τ ) = Ftk
(τ + c + β)

(A.1)

and thus we see that
1
1
Gk (τ ) = r1k pk F1k
(τ + c + β) + r0k (1 − pk )F0k
(τ + c + β)
0
0
+ (1 − r1k )pk F1k
(τ + c) + (1 − r0k )(1 − pk )F0k
(τ + c)

(A.2)

applying the law of total probability and Bayes’ rule. Moreover, again applying the law of total
probability,
0
1
(τ )
(A.3)
(τ ) + (1 − rtk )Ftk
Ftk (τ ) = rtk Ftk
for all t, k ∈ {0, 1}, and by Bayes’ rule,
r1k =

(1 − α1 )p∗k
,
pk

r0k =

α1 p∗k
.
1 − pk

(A.4)

There are four cases, corresponding to different possibilities for the rtk .

Case I: r1k = 0, r0k =
6 0 By Equation A.4, this requires α1 = 1 which is ruled out by Assumption
2.2 (ii).

Case II: r0k = r1k = 0 By Equation A.4, this requires p∗k = 0 which in turn requires pk = α0 .
0 = F , while F 1 is undefined. Substituting into Equation
Moreover, by Equation A.3 we have Ftk
tk
tk
A.2,
Gk (τ ) = pk F1k (τ + c) + (1 − pk )F0k (τ + c) = Fk (τ + c)

Now, since Fk (τ + c) is the conditional CDF of y − c given that z = k, and Gk is the conditional
CDF of ε given z = k, we see that Assumption 2.1 (i) is satisfied if and only if E(y|z = k) = c. But
since pk = α0 in this case, c = c + β(pk − α0 )/(1 − α0 − α1 ).

Case III: r1k =
6 0, r0k = 0 By Equation A.4 this requires α1 = 0 and p∗k 6= 0. By Equation A.3

0 =F
we have F0k
6 1, we can solve to obtain
0k and since r1k =
1
F1k
(τ ) =


1 
0
F1k (τ ) − (1 − r1k )F1k
(τ )
r1k

Substituting into Equation A.2, we obtain
Gk (τ ) = [(1 − pk )F0k (τ + c) + pk F1k (τ + c + β)]
 0

0
+ pk (1 − r1k ) F1k
(τ + c) − F1k
(τ + c + β)

41

Now, F0k (τ + c) is the conditional CDF of (y − c) given (T = 0, z = k) while F1k (τ + c + β) is
0 (τ + c) is the conditional
the conditional CDF of (y − c − β) given (T = 1, z = k). Similarly, F1k
∗
0
CDF of ε given (T = 0, T = 1, z = k) while F1k (τ + c + β) is the conditional CDF of (ε − β)
given (T ∗ = 0, T = 1, z = k). Since Gk (τ ) is the conditional CDF of ε given z = k, we see that
Assumption 2.1 (iii) is satisfied if and only if
0 = (1 − pk )E(y − c|T = 0, z = k) + pk E(y − c − β|T = 1, z = k)
+ pk (1 − r1k ) [E(ε|T ∗ = 0, T = 1, z = k) − E(ε − β|T ∗ = 0, T = 1, z = k)]
Rearranging, this is equivalent to

E(y|z = k) = c + (1 − α1 )β

pk − α0
1 − α0 − α1




=c+β

pk − α0
1 − α0 − α1



0 =F
1
since α1 = 0 in this case. As explained above, F0k
0k in the present case while F0k is undefined.
0
1
We are free to choose any distributions for F1k and F1k that satisfy Equation A.3, for example
0 = F1 = F .
F1k
1k
1k

Case IV: r1k 6= 0, r0k 6= 0 In this case, we can solve Equation A.3 to obtain
1
Ftk
(τ ) =


1 
0
Ftk (τ ) − (1 − rtk )Ftk
(τ )
rtk

Substituting this into Equation A.2, we have

 0
0
(τ + c + β)
(τ + c) − F1k
Gk (τ ) = Fk (τ + c + β) + pk (1 − r1k ) F1k

 0
0
(τ + c + β)
(τ + c) − F0k
+ (1 − pk )(1 − r0k ) F0k
using the fact that Fk (τ ) = pk F1k (τ ) + (1 − pk )F0k (τ ). Now, Fk (τ + c + β) is the conditional CDF
0 (τ + c) is the conditional CDF of ε given (T = t, z = k) and
of (y − c − β) given z = k, while Ftk
0 (τ + c + β) is the conditional CDF of (ε − β) given (T = t, z = k). Since G (τ ) is the conditional
Ftk
k
CDF of ε given z = k, we see that Assumption 2.1 (iii) is satisfied if and only if
0 = E[y − c − β|z = k] + pk (1 − r1k ) [E(ε|T ∗ = 0, T = 1, z = k) − E(ε − β|T ∗ = 0, T = 1, z = k)]
+ (1 − pk )(1 − r0k ) [E(ε|T ∗ = 0, T = 0, z = k) − E(ε − β|T ∗ = 0, T = 0, z = k)]
0 = E[y − c − β|z = k] + β [pk (1 − r1k ) + (1 − pk )(1 − r0k )]
But since [pk (1 − r1k ) + (1 − pk )(1 − r0k )] = (1−p∗k ) and p∗k = (pk −α0 )/(1−α0 −α1 ), this becomes
E[y|z = k] = c + β [(pk − α0 )(1 − α0 − α1 )] .
0 and F 1 that satisfy Equation A.3.
Thus, in this case we are free to choose any distributions for Ftk
tk
0
1
For example we could take Ftk = Ftk = Ftk .

Proof of Corollary 2.1. Follows by plugging in the largest and smallest possible values for α0 +α1
and taking the difference of the expressions for E[y|z = k]
Proof of Theorem 2.2. Under Assumption 2.1 (i) and Assumption 2.2 (iii), we obtain E(y|T ∗ , T, z) =

42

E(y|T ∗ , z). Hence, by iterated expectations
E(y|T = 0, z = k) = (1 − r0k )E(y|T ∗ = 0, z = k) + r0k E(y|T ∗ = 1, z = k)
E(y|T = 1, z = k) = (1 − r1k )E(y|T ∗ = 0, z = k) + r1k E(y|T ∗ = 1, z = k)
where rtk is defined as in the proof of Theorem 2.1. This is system of two linear equations in two
unknowns: E(y|T ∗ = 0, z = k) and E(y|T ∗ = 1, z = k). After some algebra, we find that the
determinant is



1 − pk − α1
p k − α0
r1k − r0k =
1 − α0 − α1
pk (1 − pk )
and thus a unique solution exists provided that α0 6= pk and α1 6= 1 − pk . By our assumption that
E[y|T = 0, z = k] 6= E[y|T = 1, z = k], the system has no solution when the determinant condition
fails. Thus, Assumption 2.2 (iii) rules out α0 = pk and α1 = 1 − pk . Solving,


1
0
∗
µk ≡ E(y|T = 0, z = k) =
[(1 − pk )E(y|T = 0, z = k) − α1 E(y|z = k)]
1 − pk − α1


1
µ1k ≡ E(y|T ∗ = 1, z = k) =
[pk E(y|T = 1, z = k) − α0 E(y|z = k)]
p k − α0
Given (α0 , α1 ), we see that rtk , µ0k , and µ1k are fixed. The question is whether, for a given pair
0 , F 1 such that
(α0 , α1 ) and observed CDFs Ftk , we can construct valid CDFs Ftk
tk
Z
Z
0
1
1
0
(τ )
(τ ) + (1 − rtk )Ftk
(dτ ) = µ1k , Ftk (τ ) = rtk Ftk
τ Ftk
(dτ ) = µ0k ,
τ Ftk
R

R
∗

t are as defined in the proof of Theorem 2.2. For a given pair (t, k), there are two
where Ftk and Ftk
cases: 0 < rtk < 1 and rtk ∈ {0, 1}.

Case I: rtk ∈ {0, 1} Suppose that rtk = 1. Then, µ1k = E[y|T = t, z = k] so we can simply set
1 = F . In this case F 0 is undefined. If instead r = 0, then µ0 = E[y|T = t, z = k] so we can
Ftk
tk
tk
k
tk
0 = F . In this case F 1 is undefined.
simply set Ftk
tk
tk

Case II: 0 < rtk < 1 Define
µtk (ξ) = E[y|y ∈ Itk (ξ), T = t, z = k]
 −1

−1
Itk (ξ) = Ftk
(1 − ξ − rtk ), Ftk
(1 − ξ)
−1
for t, k = 0, 1 where 0 ≤ ξ ≤ 1 − rtk and Ftk
is the quantile function of y given (T = t, z = k).
We see that µtk is a decreasing function of ξ that attains its maximum at ξ = 0 and minimum at
ξ = 1 − rtk . Define these extrema as µtk = µtk (1 − rtk ) and µtk = µtk (0).
Suppose first that µ1k does not lie in the interval [µtk , µtk ]. We show that it is impossible to
0 and F 1 that satisfy F (τ ) = r F 1 (τ ) + (1 − r )F 0 (τ ) where F
construct valid CDFs Ftk
tk
tk tk
tk
tk and
tk
tk
∗
t
Ftk are as defined in the
proof
of
Theorem
2.2.
Since
r
6
=
1,
we
can
solve
the
expression
for
tk


0 (τ ) = F (τ ) − r F 1 (τ ) /(1 − r ). Hence, since r
Ftk to yield Ftk
6
=
0,
the
requirement
that
tk
tk tk
tk
tk
0 (τ ) ≤ 1 implies
0 ≤ Ftk
Ftk (τ ) − (1 − rtk )
Ftk (τ )
1
≤ Ftk
(τ ) ≤
(A.5)
rtk
rtk

43

Now define
F 1tk (τ ) = min {1, Ftk (τ )/rtk }
1

F tk (τ ) = max {0, Ftk (τ )/rtk − (1 − rtk )/rtk }
1 (τ ) ≤ 1, we see that
Combining Equation A.5 with the requirement that 0 ≤ Ftk
1

1
F tk (τ ) ≤ Ftk
(τ ) ≤ F 1tk (τ )
1

1 which in turn first-order stochastically dominates
Hence F tk first-order stochastically dominates Ftk
1
F tk . It follows that
Z
Z
Z

τ F 1tk (dτ ) ≤

1
τ Ftk
(dτ ) ≤

1

τ F tk (dτ )

But notice that
Z
µtk =

τ F 1tk (dτ ),

Z

µ1k

=

1
(dτ ),
τ Ftk

Z
µtk =

1

τ F tk (dτ )

/ [µtk , µtk ].
so we have µtk ≤ µ1k ≤ µtk which contradicts µ1k ∈
h
i
Now suppose that µ1k ∈ µtk , µtk . Since y is assumed to follow a continuous distribution
h
i
conditional on (T, z), µtk is continuous on its domain and takes on all values in µtk , µtk by the
intermediate value theorem. Thus, there exists a ξ ∗ such that µtk (ξ ∗ ) = µ1k . Now let ftk (τ ) =
dFtk (τ )/dτ which is non-negative by the assumption that y is continuously distributed. Define the
densities
ftk (τ ) × 1 {τ ∈ Itk (ξ ∗ )}
ftk (τ ) × 1 {τ ∈ Itk (ξ ∗ )}
1
0
(τ ) =
ftk
(τ ) =
, ftk
.
rtk
1 − rtk
1 ≥ 0 and f 0 ≥ 0. Integrating,
Clearly ftk
tk
Z
Z
1
1
(τ ) dτ =
ftk
ftk (τ ) dτ = 1
rtk Itk (ξ∗ )
R
Z
Z
1
0
ftk (τ ) dτ =
ftk (τ ) dτ = 1
C (ξ ∗ )
1 − rtk Itk
R
C is the complement of I . And, by construction
where Itk
tk
Z
Z
Z
1
0
rtk
ftk (τ ) dτ + (1 − rtk )
ftk (τ ) dτ =
ftk (τ ) dτ
A

A

A

for any set A. Finally,
Z
R

1
τ ftk
(τ )

1
dτ =
rtk

Z
Itk (ξ ∗ )

τ ftk (τ ) dτ = µtk (ξ ∗ ) = µ1k .

The result now follows by appealing to the proof of Theorem 2.1.

44

A.2

Point Identification Results

In the proofs of Lemma 2.3, Lemma 2.4, and Theorem 2.3, we use the shorthand
π ≡ Cov(T, z),

ηj ≡ Cov(y j , z),

τj ≡ Cov(T y j , z)

for j = 1, 2, 3. Using this notation, Lemma 2.2 becomes η1 = πθ1 , while Lemma 2.3 becomes
η2 = 2τ1 θ1 − πθ2 , and Lemma 2.4 becomes η3 = 3τ2 θ1 − 3τ1 θ2 + πθ3 .
Proof of Lemma 2.3. By Assumption 2.1 (i) and the basic properties of covariance,
η2 = β 2 Cov(T ∗ , z) + 2β [c Cov(T ∗ , z) + Cov(T ∗ ε, z)] + 2c Cov(ε, z) + Cov(ε2 , z)
τ1 = cπ + Cov(T ε, z) + βCov(T T ∗ , z)
using the fact that T ∗ is binary. Now, by Assumptions 2.1 (iii) and 2.5 we have Cov(ε, z) =
Cov(ε2 , z) = 0. And, using Assumptions 2.2 (i) and (ii), one can show that Cov(T T ∗ , z) = (1 −
α1 )Cov(T ∗ , z) and Cov(T ∗ , z) = π/(1 − α0 − α1 ). Hence,
η2 = θ1 (β + 2c) π + 2βCov(T ∗ ε, z)


2τ1 θ1 − πθ2 = 2θ1 c + 2θ12 (1 − α1 ) − θ2 π + 2θ1 Cov(T ε, z)
but since θ2 = θ12 [(1 − α1 ) + α0 ], we see that [2θ12 (1 − α1 ) − θ2 ] = θ1 β. Thus, it suffices to show
that βCov(T ∗ ε, z) = θ1 Cov(T ε, z). This equality is trivially satisfied when β = 0, so suppose
that β 6= 0. In this case it suffices to show that (1 − α0 − α1 )Cov(T ∗ ε, z) = Cov(T ε, z). Define
m∗tk = E [ε|T ∗ = t, z = k] and p∗k = P(T ∗ = 1|z = k). Then, by iterated expectations, Bayes’ rule,
and Assumption 2.2 (iii)
Cov(T ∗ ε, z) = q(1 − q) (p∗1 m∗11 − p∗0 m∗10 )
Cov(T ε, z) = q(1 − q) {(1 − α1 ) [p∗1 m∗11 − p∗0 m∗10 ] + α0 [(1 − p∗1 )m∗01 − (1 − p∗0 )m∗00 ]}
But by Assumption 2.1 (iii), E[ε|z = k] = m∗1k p∗k +m∗0k (1−p∗k ) = 0 and thus we obtain m∗0k (1−p∗k ) =
−m∗1k p∗k . Therefore (1 − α0 − α1 )Cov(T ∗ ε, z) = Cov(T ε, z) as required.
Proof of Lemma 2.4. Since T ∗ is binary, if follows from the basic properties of covariance that,


η3 = Cov (c + ε)3 , z + 3βCov[(c + ε)2 T ∗ , z] + 3β 2 Cov[(c + ε)T ∗ , z] + β 3 Cov(T ∗ , z)


τ2 = Cov (c + ε)2 T, z + 2βCov [(c + ε)T T ∗ , z] + β 2 Cov(T T ∗ , z)


By Assumptions 2.1 (iii), 2.5, and 2.6 (ii) , Cov (c + ε)3 , z = 0. Expanding,


η3 = 3βCov(T ∗ ε2 , z) + 3β 2 + 6cβ Cov(T ∗ ε, z) + β 3 + 3cβ 2 + 3c2 β Cov(T ∗ , z)
τ2 = c2 Cov(T, z) + β(β + 2c)Cov(T T ∗ , z) + Cov(T ε2 , z) + 2cCov(T ε, z) + 2β Cov(T T ∗ ε, z)
Now, define s∗tk = E[ε2 |T ∗ = t, z = k] and p∗k = P(T ∗ = 1|z = k). By iterated expectations, Bayes’
rule, and Assumption 2.6 (i),
Cov(T ∗ ε2 , z) = q(1 − q)(p∗1 s∗11 − p∗0 s∗10 )
Cov(T ε2 , z) = q(1 − q) {(1 − α1 ) [p∗1 s∗11 − p∗0 s∗10 ] + α0 [(1 − p∗1 )s∗01 − (1 − p∗0 )s∗00 ]}

45

By Assumption 2.5, E[ε2 |z = 1] = E[ε2 |z = 0] and thus, by iterated expectations we have p∗1 s∗11 −
p∗0 s∗10 = − [(1 − p∗1 )s∗01 − (1 − p∗0 )s∗00 ] which implies
Cov(T ε2 , z) = (1 − α0 − α1 )Cov(T ∗ ε2 , z).

(A.6)

Similarly by iterated expectations and Assumptions 2.2 (i)–(ii)
Cov(T T ∗ ε, z) = q(1 − q)(1 − α1 )(p∗1 m∗1k − p∗0 m∗10 ) = (1 − α1 )Cov(T ∗ ε, z)

(A.7)

where m∗tk is defined as in the proof of Lemma 2.3. As shown in the proof of Lemma 2.3,
Cov(T T ∗ , z) = (1 − α1 )Cov(T ∗ , z)
Cov(T ∗ , z) = π/(1 − α0 − α1 )
Cov(T ∗ ε, z) = Cov(T ε, z)/(1 − α0 − α1 )
and combining these equalities with Equations A.6 and A.7, it follows that


τ2 = 2 [(1 − α1 )(c + β) − cα0 ] Cov(T ∗ ε, z) + (1 − α1 )(c + β)2 − c2 α0 Cov(T ∗ , z)
+ (1 − α0 − α1 )Cov(T ∗ ε2 , z)
τ1 = (1 − α0 − α1 )Cov(T ∗ ε, z) + [(1 − α1 )(c + β) − cα0 ] Cov(T ∗ , z)
using τ1 = cπ + Cov(T ε, z) + βCov(T T ∗ , z) as shown in the proof of Lemma 2.3. Thus,
3τ2 θ1 − 3τ1 θ2 + πθ3 = K1 Cov(T ∗ ε2 , z) + K2 Cov(T ∗ ε, z) + K3 Cov(T ∗ , z)
where K1 ≡ 3θ1 (1 − α0 − α1 ) = 3β and
K2 ≡ 6θ1 [(1 − α1 )(c + β) − cα0 ] − 3θ2 (1 − α0 − α1 )


K3 ≡ 3θ1 (1 − α1 )(c + β)2 − c2 α0 − 3θ2 [(1 − α1 )(c + β) − cα0 ] + θ3 (1 − α0 − α1 )
Substituting the definitions of θ1 , θ2 , and θ3 from Equations 6–8, tedious but straightforward algebra
shows that K2 = 3β 2 + 6cβ and K3 = β 3 + 3cβ 2 + 3c2 β. Therefore the coefficients of η3 equal those
of 3τ2 − 3τ1 θ2 + πθ3 and the result follows.
Proof of Theorem 2.3. Collecting the results of Lemmas 2.2–2.4, we have
η1 = πθ1 ,

η2 = 2τ1 θ1 − πθ2 ,

η3 = 3τ2 θ1 − 3τ1 θ2 + πθ3

which is a linear system in θ1 , θ2 , θ3 with determinant −π 3 . Since π 6= 0 by assumption 2.1 (ii),
θ1 , θ2 and θ3 are identified. Now, so long as β =
6 0, we can rearrange Equations 7 and 8 to obtain
A = θ2 /θ12 = 1 + (α0 − α1 )
B=

θ3 /θ13

2

= (1 − α0 − α1 ) + 6α0 (1 − α1 )

(A.8)
(A.9)

Equation A.8 gives (1 − α1 ) = A − α0 . Hence (1 − α0 − α1 ) = A − 2α0 and α0 (1 − α1 ) = α0 (A − α0 ).
Substituting into Equation A.9 and simplifying, (A2 − B) + 2Aα0 − 2α02 = 0. Substituting for α0
analogously yields a quadratic in (1 − α1 ) with identical coefficients. It follows that one root of

46

(A2 − B) + 2Ar − 2r2 = 0 is α0 and the other is 1 − α1 . Solving,


q
A p 2
1 θ2
2
r = ± 3A − 2B = 2
± 3θ2 − 2θ1 θ3 .
2
θ1 2

(A.10)

By Equations 7 and 8,

2
 

3θ22 − 2θ1 θ3 = 3 θ12 (1 + α0 − α1 ) − 2θ1 θ13 (1 − α0 − α1 )2 + 6α0 (1 − α1 )



= θ14 3(1 + α0 − α1 )2 − 2 (1 − α0 − α1 )2 + 6α0 (1 − α1 ) .
Expanding the first term we find that


3(1 + α0 − α1 )2 = 3 1 + 2(α0 − α1 ) + (α0 − α1 )2
= 3 + 6α0 − 6α1 + 3α02 + 3α12 − 6α0 α1
and expanding the second




2 (1 − α0 − α1 )2 + 6α0 (1 − α1 ) = 2 1 − 2(α0 + α1 ) + (α0 + α1 )2 + 6α0 − 6α0 α1
= 2 + 8α0 − 4α1 + 2α02 + 2α12 − 8α0 α1 .
Therefore

3θ22 − 2θ1 θ3 = θ14 1 − 2α0 − 2α1 + α02 − α12 + 2α0 α1


= θ14 (1 − α0 − α1 )2
which is strictly greater than zero since θ1 =
6 0 and α0 + α1 6= 0. It follows that both roots of the
quadratic are real. Moreover, 3θ22 /θ14 − 2θ3 /θ13 identifies (1 − α0 − α1 )2 . Substituting into Equation
6, it follows that β is identified up to sign. If α0 + α1 < 1 then sign(β) = sign(θ1 ) so that both the
sign and magnitude of β are identified. If α0 + α1 < 1 then 1 − α1 > α0 so (1 − α1 ) is the larger
root of (A2 − B) + 2Ar − 2r2 = 0 and α0 is the smaller root.

B

Comment on Mahajan (2006) A.2

Expanding on our discussion from Section 2.2 above, we now show that Mahajan’s identification argument for an endogenous regressor in an additively separable model (A.2) is
incorrect. Unless otherwise indicated, all notation used below is as defined in Section 2.
The first step of Mahajan (2006) A.2 argues (correctly) that under Assumptions 2.1 and
2.2 (i)–(ii), knowledge of α0 (x) and α1 (x) is sufficient to identify β(x). This step is equivalent
to our Lemma 2.2 above. The second step appeals to Mahajan (2006) Theorem 1 to argue
that α0 (x) and α1 (x) are indeed point identified. To understand the logic of this second
step, we first re-state Mahajan (2006) Theorem 1 in our notation. As in Section 2 above,
T ∗ denotes an unobserved binary random variable, z is a instrument, T an observed binary
surrogate for T ∗ , y an outcome of interest, and x a vector covariates.
Assumption B.1 (Mahajan (2006) Theorem 1). Define g(T ∗ , x) ≡ E[y|x, T ∗ ] and v ≡
y − g(T ∗ , x). Suppose that knowledge of (y, T ∗ , x) is sufficient to identify g and that:
(i) P(T ∗ = 1|x, z = 0) =
6 P(T ∗ = 1|x, z = 1).
47

(ii) T is conditionally independent of z given (x, T ∗ ).
(iii) α0 (x) + α1 (x) < 1
(iv) E[v|x, z, T ∗ , T ] = 0
(v) g(1, x) 6= g(0, x)
Theorem B.1 (Mahajan (2006) Theorem 1). Under Assumption B.1, α0 (x) and α1 (x) are
point identified, as is g(T ∗ , x).
Assumption B.1 (i) is equivalent to our Assumption 2.1 (ii), while Assumptions B.1
(ii)–(iii) are equivalent to our Assumptions 2.2 (i)–(ii). Assumption B.1 (v) serves the same
purpose as β(x) =
6 0 in our Theorem 2.3: unless T ∗ affects y, we cannot identify the misclassification probabilities. The key difference between Theorem B.1 and the setting we
consider in Section 2 comes from Assumption B.1 (iv). This is essentially a stronger version
of our Assumptions 2.1 (iii) and 2.2 (iii) but applies to the projection error v, defined in Assumption B.1 rather than the structural error ε, defined in Assumption 2.1 (i). Accordingly,
Theorem B.1 identifies the conditional mean function g rather than the causal effect β(x).
Although the meaning of the error term changes when we move from a structural to a
reduced form model, the meaning of the mis-classification error rates does not: α0 (x) and
α1 (x) are simply conditional probabilities for T given (T ∗ , x). Step 2 of Mahajan (2006) A.2
relies on this insight. The idea is to find a way to satisfy Assumption B.1 (iv) simultaneously
with Assumptions 2.1 (iii) and 2.2 (iii), while allowing T ∗ to be endogenous. If this can be
achieved, α0 (x), α1 (x) will be identified via Theorem B.1, and identification of β(x) will
follow from step 1 of A.2 (our Lemma 2.2). To this end, Mahajan (2006) invokes the
condition
E(y|x, z, T ∗ , T ) = E(y|x, T ∗ ).
(B.1)
Because Mahajan (2006) A.2 assumes an additively separable model – our Assumption 2.1
(i) – we see that
E(y|x, z, T ∗ , T ) = c(x) + β(x)T ∗ + E(ε|x, z, T ∗ , T )
so Equation B.1 is equivalent to E(ε|x, z, T ∗ , T ) = E(ε|x, T ∗ ). Note that this allows T ∗ to
be endogenous, as it does not require E(ε|x, T ∗ ) = 0. Now, applying Equation B.1 to the
definition of v from Assumption B.1, we have
E(v|x, z, T ∗ , T ) = E [y − E(y|x, T ∗ ) | x, z, T ∗ , T ] = 0
which satisfies Assumption B.1 (iv) as required. Based on this reasoning, Mahajan (2006)
claims that Equation B.1 along with Assumptions B.1 (iv), 2.1, and 2.2 (i)–(ii) suffice to
identify the effect β(x) of an endogenous T ∗ , so long as g(1, x) =
6 g(0, x). As we now show,
∗
however, these Assumptions are contradictory unless T is exogenous.
By Equation B.1 and Assumption 2.1 (i), E(ε|x, z, T ∗ , T ) = E(ε|x, T ∗ ) and thus by
iterated expectations, we obtain
E(ε|x, T ∗ , z) = ET |x,T ∗ ,z [E(ε|x, T ∗ , T, z)] = ET |x,T ∗ ,z [E(ε|x, T ∗ )] = E(ε|x, T ∗ ).
48

(B.2)

Now, let m∗tk (x) = E(ε|x, T ∗ = t, z = k). Using this notation, Equation B.2 is equivalent to
m∗t0 (x) = m∗t1 (x) for t = 0, 1. Combining iterated expectations with Assumption 2.1 (iii),
E(ε|x, z = k) = [1 − p∗k (x)]m∗0k (x) + p∗k (x)m∗1k (x) = 0

(B.3)

for k = 0, 1 where p∗k (x) ≡ P(T ∗ = 1|x, z = k). But substituting m∗t0 (x) = m∗t1 (x) into
Equation B.3 for k = 0, 1, we obtain
[1 − p∗0 (x)]m∗00 (x) + p∗0 (x)m∗10 (x) = 0
[1 − p∗1 (x)]m∗00 (x) + p∗1 (x)m∗10 (x) = 0
The preceding two equalities are convex combinations of m∗00 and m∗10 . The only way that
both can equal zero simultaneously is if either p∗0 (x) = p∗1 (x), contradicting Assumption 2.1
(ii), or if m∗tk (x) = 0 for all (t, k), which implies that T ∗ is exogenous. Hence Mahajan (2006)
A.2 fails: given the assumption that z is a valid instrument for ε, Equation B.1 implies that
either there is no first-stage relationship between z and T ∗ or that T ∗ is exogenous.
The root of the problem with A.2 is the attempt to use one instrument to satisfy both the
assumptions of Theorem B.1 and Lemma 2.2. If one had access to a second instrument w,
or equivalently a second mis-measured surrogate for T ∗ , that satisfied Assumptions B.1, one
could use w to recover α0 (x) and α1 (x) via Theorem B.1 and z to recover the IV estimand
β(x)/[1 − α0 (x) − α1 (x)] via Lemma 2.2. This is effectively the approach used by Battistin
et al. (2014) to evaluate the returns to schooling in a setting with multiple misreported
measures of educational qualifications.

C

Moment Equalities Under Joint Exogeneity

In this Section we discuss the moment equalities that replace Equation 10 under joint exogeneity: Assumption 2.3. Because the moment inequalities from Section 3.3 are unchanged
under this assumption, we do not discuss them further here. Define θ1 as in Equation 6, κ1 as
in Section 3.1, and let ρ = −θ1 α0 (1 − α1 ) and η = θ1 (1 + α0 − α1 ). Now, under Assumptions
2.1, 2.2, and 2.3:

  
y − κ1 − θ1 T
1
E
⊗
= 0.
(C.1)
(y − κ1 )T − ρ − ηT
z
where the equalities involving ρ and η follow from an argument similar to one of the steps
from the proof of Lemma 2.3 – see, e.g., Frazis and Loewenstein (2003) and Mahajan (2006).
The moment equalities from C.1 point identify the reduced form parameters (θ1 , κ1 , ρ, η) and
lead to a just-identified method of moments estimator of the same. To see why knowledge
of (θ1 , κ1 , ρ, η) suffices to identify (β, α0 , α1 ), define
A ≡ η/θ1 = 1 + α0 − α1 ,

B ≡ −ρ/θ1 = α0 (1 − α1 )

Eliminating (1 − α1 ) and α0 , respectively, we obtain:
α02 − Aα0 + B = 0,

(1 − α1 )2 − A(1 − α1 ) + B = 0

49

These are exactly the same quadratic, namely x2 − Ax + B = 0. Hence one root is α0 while
the other is (1 − α1 ). The discriminant is
A2 − 4B = [(1 − α1 ) + α0 ]2 − 4 [α0 (1 − α1 )] = (1 − α0 − α1 )2
so that both roots are real as long as α0 +α1 =
6 0. To solve
√ for α0and α1 we need to calculate
1
2
the roots of x − Ax + B = 0, namely x = 2 A ± A2 − 4B . One of these roots is α0
and the other is 1 − α1 . By assumption, however, α0 + α1 < 1 and thus α0 < 1 − α1 . It
follows that the smaller of the two roots is α0 and the larger is 1 − α1 . Given that (α0 , α1 )
are identified, identification of β follows by Lemma 2.2.
Inference based on the moment equalities from Equation C.1 suffers from the same difficulties as that based on Equation 10 above. First, note that, while A2 > 4B in population
since α0 + α1 < 1 by assumption, the same may not hold in sample. In this case the GMM
estimator of β will fail to exist. Second, notice that the moment equalities from Equation
C.1 only depend on β through θ1 and are completely uninformative about (α0 , α1 ) if β = 0.
Substituting Equation C.1 for Equation 10 in Algorithm 3.1 requires some small changes.
First, mE and hE from Equations 16–17 are replaced by




y − κ1 − θ1 T
(y − κ1 )T − ρ − ηT
E
E
h =
, m =
(y − κ1 − θ1 T )z
{(y − κ1 )T − ρ − ηT } z
where in this case we require preliminary estimators of κ1 and θ1 . Accordingly, H E and M E
from Lemma 3.1 become




−1
−E(T )
−E[T ] 0
E
E
H =
, M =
−E(z) −E(T z)
−E[T z] 0
and thus
E

E

E −1

B = −M (H )

1
=
Cov(T, z)



−E(T )E(T z)
E(T )2
2
−E(T z)
E(T z)E(T )



which is well-defined as long as T is correlated with z.

References
Aigner, D. J., 1973. Regression with a binary independent variable subject to errors of observation.
Journal of Econometrics 1, 49–60.
Andrews, D. W., 1994. Empirical process methods in econometrics. Handbook of econometrics 4,
2247–2294.
Andrews, D. W., Shi, X., 2013. Inference based on conditional moment inequalities. Econometrica
81 (2), 609–666.
Andrews, D. W., Shi, X., 2014. Nonparametric inference based on conditional moment inequalities.
Journal of Econometrics 179 (1), 31–45.
Andrews, D. W., Soares, G., 2010. Inference for parameters defined by moment inequalities using
generalized moment selection. Econometrica 78 (1), 119–157.

50

Andrews, I., 2016. Valid two-step identification-robust confidence sets for GMM. Review of Economics and Statistics (Forthcoming).
Battistin, E., Nadai, M. D., Sianesi, B., 2014. Misreported schooling, multiple measures and returns
to educational qualifications. Journal of Econometrics 181 (2), 136–150.
Black, D. A., Berger, M. C., Scott, F. A., 2000. Bounding parameter estimates with nonclassical
measurement error. Journal of the American Statistical Association 95 (451), 739–748.
Bollinger, C. R., 1996. Bounding mean regressions when a binary regressor is mismeasured. Journal
of Econometrics 73, 387–399.
Bollinger, C. R., van Hasselt, M., 2015. Bayesian moment-based inference in a regression models
with misclassification error, working Paper.
Bugni, F. A., Canay, I. A., Shi, X., 2017. Inference for subvectors and other functions of partially
identified parameters in moment inequality models. Quantitative Economics 8 (1), 1–38.
Chen, X., Hong, H., Nekipelov, D., 2011. Nonlinear models of measurement errors. Journal of
Economic Literature 49 (4), 901–937.
Chen, X., Hong, H., Tamer, E., 2005. Measurement error models with auxiliary data. The Review
of Economic Studies 72 (2), 343–366.
Chen, X., Hu, Y., Lewbel, A., 2008a. Nonparametric identification of regression models containing
a misclassified dichotomous regressor with instruments. Economics Letters 100, 381–384.
Chen, X., Hu, Y., Lewbel, A., 2008b. A note on the closed-form identification of regression models
with a mismeasured binary regressor. Statistics & Probability Letters 78 (12), 1473–1479.
Frazis, H., Loewenstein, M. A., 2003. Estimating linear regressions with mismeasured, possibly
endogenous, binary explanatory variables. Journal of Econometrics 117, 151–178.
Hu, Y., 2008. Identification and estimation of nonlinear models with misclassification error using
instrumental variables: A general solution. Journal of Econometrics 144 (1), 27–61.
Hu, Y., Shennach, S. M., January 2008. Instrumental variable treatment of nonclassical measurement error models. Econometrica 76 (1), 195–216.
Hu, Y., Shiu, J.-L., Woutersen, T., 2015. Identification and estimation of single-index models with
measurement error and endogeneity. The Econometrics Journal 18 (3), 347–362.
Kaido, H., Molinari, F., Stoye, J., 2016. Confidence intervals for projections of partially identified
parameters. arXiv preprint arXiv:1601.00934.
Kane, T. J., Rouse, C. E., Staiger, D., July 1999. Estimating returns to schooling when schooling
is misreported. Tech. rep., National Bureau of Economic Research, NBER Working Paper 7235.
Kreider, B., Pepper, J. V., Gundersen, C., Jolliffe, D., 2012. Identifying the effects of SNAP (food
stamps) on child health outcomes when participation is endogenous and misreported. Journal of
the American Statistical Association 107 (499), 958–975.

51

Lewbel, A., March 2007. Estimation of average treatment effects with misclassification. Econometrica 75 (2), 537–551.
Mahajan, A., 2006. Identification and estimation of regression models with misclassification. Econometrica 74 (3), 631–665.
Molinari, F., 2008. Partial identification of probability distributions with misclassified data. Journal
of Econometrics 144 (1), 81–117.
Moon, H. R., Schorfheide, F., 2009. Estimation with overidentifying inequality moment conditions.
Journal of Econometrics 153 (2), 136–154.
Newey, W. K., McFadden, D., 1994. Large sample estimation and hypothesis testing. Handbook of
econometrics 4, 2111–2245.
Nguimkeu, P., Denteh, A., Tchernis, R., 2016. On the estimation of treatment effects with endogenous misreporting. Working Paper.
Schennach, S. M., 2004. Estimation of nonlinear models with measurement error. Econometrica
72 (1), 33–75.
Schennach, S. M., 2007. Instrumental variable estimation of nonlinear errors-in-variables models.
Econometrica 75 (1), 201–239.
Schennach, S. M., 2013. Measurement error in nonlinear models – a review. In: Acemoglu, D.,
Arellano, M., Dekel, E. (Eds.), Advances in Economics and Econometrics. Vol. 3. Cambridge
University Press, pp. 296–337.
Shiu, J.-L., 2015. Identification and estimation of endogenous selection models in the presence of
misclassification errors. Economic Modelling (Forthcoming).
Song, S., 2015. Semiparametric estimation of models with conditional moment restrictions in the
presence of nonclassical measurement errors. Journal of Econometrics 185 (1), 95–109.
Song, S., Schennach, S. M., White, H., 2015. Estimating nonseparable models with mismeasured
endogenous variables. Quantitative Economics 6 (3), 749–794.
Ura, T., November 2015. Heterogeneous treatment effects with mismeasured endogenous treatment.
Tech. rep., Duke University Department of Economics.
van Hasselt, M., Bollinger, C. R., 2012. Binary misclassification and identification in regression
models. Economics Letters 115, 81–84.

52

D

Supplementary Simulation Results: Online Only

In this section we provide additional simulation results to supplement those from Section 4
above. For details of the simulation DGP, etc. see the discussion above.

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
90
91
92
93
92
93
95
96
93
95
97
98
93
97
98
99

0.25
90
93
93
93
93
95
96
98
93
96
97
98
94
97
98
99

0.5
90
94
94
94
93
96
97
98
93
98
98
98
94
98
98
99

β
0.75
91
94
94
94
94
97
98
98
93
98
98
98
94
98
98
98

1
90
94
94
94
94
97
97
98
93
97
97
97
94
97
97
98

1.5
91
94
94
93
93
96
96
95
93
95
95
95
93
95
94
96

2
90
90
92
92
90
92
92
92
92
93
92
93
92
93
93
95

3
90
89
90
91
87
87
87
88
89
89
89
91
91
89
91
94

Table D.1: Coverage (1 - size) of 90% GMS joint test for α0 and α1 : n = 1000.

D-1

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
90
91
91
92
90
92
92
94
91
92
94
96
92
93
96
98

0.25
91
92
92
93
92
93
94
95
93
95
96
97
92
96
97
98

0.5
91
92
93
93
93
95
96
97
93
96
97
98
93
97
97
98

β
0.75
90
93
93
93
94
96
97
98
93
97
97
98
93
97
97
98

1
90
94
93
94
93
97
97
98
93
97
97
97
93
97
96
97

1.5
90
94
94
93
94
97
96
96
94
96
95
95
93
96
95
95

2
90
92
93
93
92
94
95
94
92
94
93
93
92
93
93
94

3
90
90
91
91
89
90
89
90
90
90
90
90
91
90
90
92

Table D.2: Coverage (1 - size) of 90% GMS joint test for α0 and α1 : n = 2000

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
95
96
96
97
96
97
98
99
97
98
99
99
97
99
99
100

0.25
95
97
97
97
97
98
99
99
97
99
99
100
97
99
99
100

0.5
95
97
98
97
97
99
99
99
97
99
99
100
97
99
99
100

β
0.75
96
97
98
98
97
99
99
99
97
99
99
99
97
99
99
99

1
96
97
97
97
97
99
99
99
97
99
99
99
97
99
99
99

1.5
96
97
97
97
97
98
98
98
96
98
98
98
96
98
98
98

2
95
95
96
96
95
96
96
96
96
96
96
97
96
97
97
98

3
95
94
95
95
93
92
93
94
94
94
94
95
95
94
96
97

Table D.3: Coverage (1 - size) of 95% GMS joint test for α0 and α1 : n = 1000

D-2

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
95
96
96
96
95
96
96
97
96
96
97
98
96
97
98
99

0.25
95
96
96
97
96
97
98
98
96
98
98
99
96
98
99
99

0.5
96
96
97
97
97
98
98
99
97
98
99
99
97
99
99
99

β
0.75
95
97
97
97
97
98
99
99
97
99
99
99
97
99
99
99

1
95
97
97
97
97
99
99
99
97
99
99
99
97
99
98
99

1.5
95
97
97
97
97
99
98
98
97
98
98
98
97
98
98
98

2
95
96
96
97
96
97
97
97
96
97
97
97
96
96
96
97

3
95
95
95
95
94
94
94
95
95
94
95
94
95
94
95
96

Table D.4: Coverage (1 - size) of 95% GMS joint test for α0 and α1 : n = 2000

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
97.7
98.0
98.4
98.5
98.1
98.6
99.0
99.4
98.6
99.0
99.5
99.7
98.7
99.4
99.8
100.0

0.25
97.7
98.7
98.5
98.8
98.5
99.1
99.3
99.7
98.5
99.5
99.7
99.8
98.7
99.6
99.8
99.9

0.5
97.6
98.8
98.9
98.8
98.3
99.5
99.7
99.8
98.6
99.7
99.8
99.8
98.8
99.6
99.7
99.9

β
0.75
97.7
99.1
98.9
99.0
98.8
99.6
99.8
99.8
98.9
99.7
99.7
99.8
98.7
99.7
99.8
99.8

1
98.0
98.8
98.8
98.7
98.8
99.6
99.7
99.6
98.7
99.4
99.4
99.5
98.7
99.4
99.5
99.6

1.5
98.0
98.4
98.6
98.4
98.4
98.8
98.9
99.0
98.2
99.0
99.0
99.0
98.2
98.9
99.1
99.5

2
97.4
97.1
98.0
97.8
96.8
97.7
97.5
98.2
97.7
98.1
97.8
98.7
98.1
98.3
98.5
99.1

3
97.9
96.4
97.0
97.5
95.7
95.2
95.7
96.7
97.0
96.5
96.8
97.7
97.6
96.8
97.8
98.8

Table D.5: Coverage (1 - size) of 97.5% GMS joint test for α0 and α1 : n = 1000

D-3

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
97.7
98.0
98.1
98.2
97.4
98.0
98.2
98.6
97.8
98.3
98.7
99.1
98.2
98.6
99.2
99.6

0.25
97.7
98.1
98.2
98.5
98.1
98.6
98.9
99.1
98.1
98.9
99.4
99.6
98.3
99.3
99.7
99.8

0.5
97.6
98.4
98.8
98.6
98.3
99.1
99.4
99.6
98.5
99.2
99.7
99.7
98.7
99.4
99.7
99.8

0.75
97.6
98.3
98.6
98.6
98.8
99.4
99.6
99.8
98.6
99.6
99.6
99.7
98.5
99.6
99.6
99.7

1
97.6
98.8
98.9
98.8
98.5
99.5
99.7
99.6
98.5
99.5
99.5
99.5
98.6
99.5
99.4
99.4

1.5
97.5
98.6
98.6
98.4
98.5
99.3
99.3
99.2
98.4
99.1
99.0
99.0
98.5
99.2
98.8
99.1

2
97.4
97.8
98.3
98.2
97.9
98.4
98.8
98.4
98.0
98.6
98.4
98.2
98.0
98.1
98.4
98.8

3
97.5
97.0
97.3
97.4
96.9
96.8
96.8
97.0
97.6
97.0
96.9
97.0
97.7
97.0
97.4
98.2

Table D.6: Coverage (1 - size) of 97.5% GMS joint test for α0 and α1 : n = 2000

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
27
27
26
26
26
26
27
25
26
26
26
26
26
24
26
26

0.25
33
32
33
34
32
36
35
35
33
33
35
35
32
35
32
35

0.5
30
29
32
30
31
32
31
32
30
30
33
33
32
33
35
35

β
0.75
14
13
15
17
14
16
18
21
15
19
22
26
16
21
27
28

1
1
2
4
5
2
4
8
11
3
6
12
15
6
11
15
21

1.5
0
0
0
0
0
0
0
1
0
0
1
3
0
1
4
7

2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2

3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Table D.7: Percentage of simulation replications for which the standard GMM confidence interval
fails to exist, either becuase the point estimate is NaN or the asymptotic covariance matrix is
numerically singular (n = 1000)

D-4

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
25
28
28
27
27
26
28
24
26
25
27
25
26
25
27
27

0.25
36
36
37
36
36
36
38
36
36
37
38
39
37
38
38
36

β
0.75
7
7
10
12
10
9
13
15
9
12
17
20
10
16
19
23

0.5
29
29
28
28
27
29
29
31
30
29
32
34
30
31
34
36

1
0
0
1
2
0
1
2
5
1
2
4
9
2
4
9
13

1.5
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
2

2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Table D.8: Percentage of simulation replications for which the standard GMM confidence interval
fails to exist, either becuase the point estimate is NaN or the asymptotic covariance matrix is
numerically singular (n = 2000)

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
72
72
73
73
73
73
73
74
74
73
73
73
74
75
74
73

0.25
62
62
61
59
63
58
59
59
62
60
58
58
62
59
61
58

0.5
62
63
61
62
60
59
61
58
60
61
57
56
60
58
56
55

β
0.75
80
79
77
76
78
77
75
71
78
74
70
66
76
71
65
64

1
92
92
90
88
91
90
86
82
91
87
81
78
89
82
78
71

1.5
95
95
96
95
95
95
95
94
95
95
93
92
95
93
90
88

2
94
96
96
96
96
95
95
96
96
96
95
95
96
96
96
93

3
95
95
96
95
96
94
94
96
96
94
95
96
96
95
96
96

Table D.9: Coverage of nominal 95% GMM Intervals with n = 1000

D-5

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
19.07
17.52
17.41
18.23
17.13
17.88
17.37
18.07
17.79
18.98
18.25
19.03
18.27
19.4
18.22
17.56

0.25
3.44
3.47
3.51
3.34
3.51
3.33
3.36
3.33
3.39
3.43
3.26
3.31
3.48
3.41
3.56
3.55

β
0.75
1.32
1.41
1.45
1.48
1.38
1.45
1.54
1.63
1.45
1.54
1.64
1.75
1.5
1.63
1.74
1.96

0.5
1.86
1.92
1.9
1.92
1.86
1.85
1.95
1.98
1.92
1.96
1.92
2.02
1.87
1.96
1.96
2.13

1
0.87
1
1.1
1.24
0.97
1.13
1.24
1.41
1.11
1.26
1.45
1.66
1.25
1.43
1.67
1.86

1.5
0.47
0.61
0.76
0.91
0.61
0.78
0.97
1.17
0.75
0.97
1.2
1.49
0.9
1.18
1.49
1.86

2
0.37
0.51
0.65
0.79
0.51
0.67
0.85
1.04
0.65
0.84
1.06
1.33
0.79
1.04
1.35
1.74

3
0.35
0.46
0.58
0.7
0.46
0.6
0.75
0.92
0.58
0.75
0.95
1.19
0.7
0.92
1.19
1.55

Table D.10: Median Width of nominal 95% GMM Intervals with n = 1000

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
74
72
72
73
73
74
72
75
74
74
73
74
74
75
73
73

0.25
54
54
53
54
54
55
52
53
54
54
52
50
53
52
52
53

0.5
63
62
64
64
65
64
63
59
61
63
60
57
61
60
57
54

β
0.75
87
86
85
81
83
84
80
77
84
81
75
72
83
78
73
69

1
95
94
94
94
94
93
93
90
93
92
90
86
92
90
85
80

1.5
94
95
95
95
95
95
96
95
96
96
96
95
97
95
95
93

2
96
95
95
95
94
95
95
95
95
95
96
96
95
96
96
96

3
95
96
94
94
96
95
95
95
94
96
95
96
95
96
96
96

Table D.11: Coverage of nominal 95% GMM Intervals with n = 2000

D-6

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
17.4
16.56
16.33
17.06
17.2
17.48
16.32
18.37
17.64
18.25
17.02
18.05
17.72
18.8
18.24
17.43

0.25
2.42
2.51
2.4
2.52
2.5
2.5
2.45
2.43
2.5
2.47
2.4
2.39
2.43
2.46
2.45
2.55

0.5
1.47
1.49
1.53
1.57
1.53
1.53
1.57
1.51
1.49
1.58
1.57
1.61
1.53
1.55
1.61
1.67

β
0.75
1
1.06
1.13
1.19
1.05
1.15
1.2
1.3
1.13
1.22
1.31
1.43
1.19
1.32
1.45
1.62

1
0.62
0.7
0.81
0.91
0.71
0.83
0.97
1.1
0.8
0.96
1.13
1.33
0.91
1.11
1.3
1.57

1.5
0.33
0.43
0.53
0.65
0.43
0.56
0.69
0.84
0.54
0.69
0.86
1.09
0.65
0.84
1.08
1.4

2
0.27
0.36
0.46
0.56
0.36
0.48
0.6
0.73
0.46
0.6
0.76
0.95
0.56
0.74
0.96
1.24

3
0.24
0.33
0.41
0.5
0.33
0.43
0.53
0.65
0.41
0.54
0.67
0.85
0.5
0.65
0.85
1.1

Table D.12: Median Width of nominal 95% GMM Intervals with n = 2000

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
96
97
98
97
97
98
98
97
97
98
98
98
97
97
98
98

0.25
97
99
99
100
99
100
100
100
99
100
100
100
99
100
100
100

0.5
97
99
99
100
99
100
100
100
99
100
100
100
100
100
100
100

0.75
96
99
100
100
99
100
100
100
100
100
100
100
100
100
100
100

1
97
99
100
100
100
100
100
100
100
100
100
100
100
100
100
100

1.5
97
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

2
95
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

3
96
99
100
100
98
100
100
100
100
100
100
100
100
100
100
100

Table D.13: Coverage of nominal > 95% Bonferroni Intervals with n = 1000

D-7

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
96
97
97
97
97
98
98
98
97
98
98
98
97
97
97
97

0.25
97
98
99
99
99
100
100
100
99
100
100
100
100
100
100
100

0.5
96
99
99
100
99
100
100
100
99
100
100
100
100
100
100
100

0.75
97
100
100
100
99
100
100
100
100
100
100
100
100
100
100
100

1
96
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

1.5
96
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

2
95
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100

3
95
99
100
100
99
100
100
100
99
100
100
100
100
100
100
100

Table D.14: Coverage of nominal > 95% Bonferroni Intervals with n = 2000

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
0.4
0.45
0.51
0.58
0.45
0.51
0.58
0.67
0.51
0.58
0.67
0.81
0.58
0.68
0.81
1.01

0.25
0.41
0.47
0.54
0.62
0.47
0.54
0.63
0.75
0.54
0.63
0.75
0.91
0.62
0.74
0.91
1.16

0.5
0.43
0.54
0.65
0.79
0.54
0.66
0.8
1
0.65
0.81
1.01
1.3
0.8
1.01
1.3
1.74

0.75
0.43
0.59
0.76
0.95
0.59
0.77
0.98
1.25
0.76
0.99
1.29
1.7
0.95
1.26
1.7
2.35

1
0.43
0.63
0.85
1.07
0.63
0.86
1.12
1.46
0.86
1.14
1.54
2.09
1.09
1.49
2.11
2.93

1.5
0.42
0.7
0.95
1.17
0.7
1.03
1.38
1.74
0.96
1.42
1.97
2.73
1.18
1.84
2.8
4.17

2
0.41
0.75
1.01
1.24
0.76
1.18
1.55
1.94
1.02
1.64
2.33
3.13
1.25
2.13
3.4
5.2

3
0.41
0.86
1.17
1.48
0.88
1.46
1.88
2.4
1.19
2.08
2.9
3.9
1.5
2.78
4.48
6.85

Table D.15: Median Width of nominal > 95% Bonferroni Intervals with n = 1000

D-8

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
0.29
0.32
0.36
0.41
0.32
0.36
0.41
0.48
0.36
0.41
0.48
0.57
0.41
0.48
0.57
0.72

0.25
0.3
0.35
0.41
0.48
0.35
0.41
0.48
0.59
0.41
0.48
0.59
0.73
0.48
0.59
0.73
0.95

0.5
0.31
0.4
0.51
0.64
0.4
0.51
0.64
0.82
0.51
0.65
0.83
1.09
0.64
0.83
1.1
1.5

0.75
0.31
0.44
0.59
0.76
0.44
0.6
0.79
1.02
0.59
0.79
1.05
1.43
0.77
1.03
1.43
2.03

1
0.31
0.48
0.65
0.81
0.48
0.69
0.91
1.16
0.65
0.92
1.24
1.69
0.82
1.18
1.71
2.53

1.5
0.3
0.53
0.67
0.8
0.53
0.82
1.04
1.25
0.67
1.09
1.49
1.9
0.78
1.36
2.11
3.15

2
0.29
0.55
0.69
0.85
0.56
0.88
1.08
1.33
0.7
1.21
1.61
2.08
0.84
1.57
2.45
3.56

3
0.29
0.61
0.81
1.01
0.62
1.02
1.27
1.61
0.82
1.52
1.96
2.6
1.02
2.06
3.18
4.56

Table D.16: Median Width of nominal > 95% Bonferroni Intervals with n = 2000

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
96
97
98
97
97
98
98
97
97
98
98
98
97
97
98
98

0.25
97
99
99
100
99
100
100
100
99
100
100
100
99
100
100
100

0.5
97
99
99
100
99
100
100
100
99
100
100
100
100
100
100
99

β
0.75
96
99
100
100
99
100
100
100
100
100
100
98
100
100
98
96

1
97
99
100
99
100
100
99
97
100
99
96
95
100
97
94
92

1.5
97
98
97
96
98
96
96
95
96
96
95
95
95
94
94
94

2
95
96
96
96
97
96
96
96
96
96
95
95
96
96
96
95

3
93
95
96
96
95
96
95
96
96
96
96
96
97
96
96
96

Table D.17: Coverage of hybrid CI constructed from nominal 95% GMM and nominal > 95%
Bonferroni intervals: n = 1000

D-9

α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
96
97
97
97
97
98
98
98
97
98
98
98
97
97
97
97

0.25
97
98
99
99
99
100
100
100
99
100
100
100
100
100
100
100

0.5
96
99
99
100
99
100
100
100
99
100
100
100
100
100
100
100

β
0.75
97
100
100
100
99
100
100
99
100
100
100
97
100
100
97
94

1
96
100
100
99
100
100
99
97
100
98
96
95
99
96
94
94

1.5
96
98
97
96
98
96
96
95
97
96
96
95
98
95
96
95

2
95
97
96
96
96
96
96
96
96
96
96
96
97
96
96
96

3
93
96
95
96
95
97
97
96
95
97
96
96
96
97
97
96

Table D.18: Coverage of hybrig CI constructed from nominal 95% GMM and nominal > 95%
Bonferroni intervals: n = 2000

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
0.4
0.45
0.51
0.58
0.45
0.51
0.58
0.67
0.51
0.58
0.67
0.81
0.58
0.68
0.81
1.01

0.25
0.41
0.47
0.54
0.62
0.47
0.54
0.63
0.75
0.54
0.63
0.75
0.91
0.62
0.74
0.91
1.16

0.5
0.43
0.54
0.65
0.79
0.54
0.66
0.8
1
0.65
0.81
1.01
1.3
0.8
1.01
1.3
1.73

0.75
0.43
0.59
0.76
0.95
0.59
0.77
0.97
1.25
0.76
0.99
1.29
1.67
0.95
1.26
1.66
2.24

1
0.43
0.63
0.84
1.05
0.63
0.86
1.11
1.4
0.85
1.12
1.48
1.95
1.07
1.43
1.98
2.71

1.5
0.42
0.67
0.82
0.96
0.67
0.92
1.17
1.4
0.83
1.18
1.56
1.77
0.95
1.48
1.94
2.33

2
0.4
0.52
0.65
0.79
0.51
0.69
0.87
1.06
0.65
0.86
1.08
1.35
0.8
1.06
1.37
1.78

3
0.35
0.46
0.58
0.7
0.46
0.61
0.75
0.92
0.58
0.75
0.95
1.2
0.7
0.93
1.19
1.55

Table D.19: Median width of hybrid CI constructed from nominal 95% GMM and nominal > 95%
Bonferroni intervals: n = 1000

D-10

100
60
40
20
0

0.2

0.4

0.6

α0 = 0.1, α1 = 0.1

0.2

0.4

0.6

0.6

0.6

-0.6 -0.4 -0.2 0.0

0.2

0.4

0.6

α0 = 0.2, α1 = 0.2

80

80

60
40

40

20

20
0.4

0.6

60
0.4

0

0
0.2

0.4

20
0.2

α0 = 0.1, α1 = 0.2

60

80
60
40
20
0
-0.6 -0.4 -0.2 0.0

0.2

α0 = 0.2, α1 = 0.1

0
-0.6 -0.4 -0.2 0.0

100

α0 = 0, α1 = 0.2

100

-0.6 -0.4 -0.2 0.0

-0.6 -0.4 -0.2 0.0

40

60
40
20
0

0

20

40

60

80

100

-0.6 -0.4 -0.2 0.0

100

0.6

80

0.4

α0 = 0.2, α1 = 0

80

100
80
60
40
20

0.2

α0 = 0, α1 = 0.1

80

100

-0.6 -0.4 -0.2 0.0

100

α0 = 0.1, α1 = 0

0

0

20

40

60

80

100

α0 = 0, α1 = 0

-0.6 -0.4 -0.2 0.0

0.2

0.4

0.6

-0.6 -0.4 -0.2 0.0

0.2

0.4

0.6

Figure D.1: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
0, n = 1000

D-11

100

100
60
40
20
0

-0.6 -0.4 -0.2 0.0

0.2

0.4

0.6

α0 = 0.1, α1 = 0.1

40
0
0.2

0.4

0.6

α0 = 0.1, α1 = 0.2

-0.6 -0.4 -0.2 0.0

0.2

0.4

0.6

-0.6 -0.4 -0.2 0.0

0.2

0.4

0.6

α0 = 0.2, α1 = 0.2

0

20

40

60

80
60
40
20
0

0

20

40

60

80

100

-0.6 -0.4 -0.2 0.0

100

0.6

0.6

80

0.4

0.4

20

40
20
0
0.2

α0 = 0, α1 = 0.2

0.2

α0 = 0.2, α1 = 0.1

60

80
60

60
40
20
0
-0.6 -0.4 -0.2 0.0

-0.6 -0.4 -0.2 0.0
100

0.6

80

0.4

α0 = 0.2, α1 = 0

80

100
80
60
40
20

0.2

α0 = 0, α1 = 0.1

80

100

-0.6 -0.4 -0.2 0.0

100

α0 = 0.1, α1 = 0

0

0

20

40

60

80

100

α0 = 0, α1 = 0

-0.6 -0.4 -0.2 0.0

0.2

0.4

0.6

-0.6 -0.4 -0.2 0.0

0.2

0.4

0.6

Figure D.2: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
0, n = 2000

D-12

80

80

60

60

40

40
-0.2 0.0 0.2 0.4 0.6 0.8 1.0

60
-0.2 0.0 0.2 0.4 0.6 0.8 1.0

α0 = 0.2, α1 = 0.2

20

20

40

40

60

60

80

80

100

100

α0 = 0.1, α1 = 0.2

0

0

20

40

60

80

100

0

20

40

60
40
20
0
-0.2 0.0 0.2 0.4 0.6 0.8 1.0

α0 = 0, α1 = 0.2

0

α0 = 0.2, α1 = 0.1

80

100
80

80
60
40
20
0
-0.2 0.0 0.2 0.4 0.6 0.8 1.0

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

α0 = 0.1, α1 = 0.1
100

α0 = 0, α1 = 0.1
100

0

20

20
-0.2 0.0 0.2 0.4 0.6 0.8 1.0

α0 = 0.2, α1 = 0
100

100

α0 = 0.1, α1 = 0

0

0

20

40

60

80

100

α0 = 0, α1 = 0

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

Figure D.3: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
0.25, n = 1000

D-13

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

100
80
60
40
20
-0.2 0.0 0.2 0.4 0.6 0.8 1.0
100
60
40
20
0

20

40
0

20

40

60

80

100

α0 = 0.2, α1 = 0.2

60

80

80
60
40

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

α0 = 0.1, α1 = 0.2
100

100

0
-0.2 0.0 0.2 0.4 0.6 0.8 1.0

α0 = 0, α1 = 0.2

20

α0 = 0.2, α1 = 0.1

80

100
80
60
40
20
-0.2 0.0 0.2 0.4 0.6 0.8 1.0

0

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

α0 = 0.1, α1 = 0.1

0

0

20

40

60

80

100

α0 = 0, α1 = 0.1

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

α0 = 0.2, α1 = 0

0

20

40

60

80

100

α0 = 0.1, α1 = 0

0

0

20

40

60

80

100

α0 = 0, α1 = 0

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

-0.2 0.0 0.2 0.4 0.6 0.8 1.0

Figure D.4: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
0.25, n = 2000

D-14

0.0

0.5

1.0

1.5

80
0

20

40

60

80
60
40
20
0

0

20

40

60

80

100

α0 = 0.2, α1 = 0

100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

0.0

0.5

1.0

1.5

1.0

1.5

100
60
40
20
0.5

1.0

1.5

1.0

1.5

100
0

20

40

60

80

100
60
40
20
0
1.5

0.5

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

1.0

0.0

α0 = 0.1, α1 = 0.2

0

0.5

1.5

0
0.0

α0 = 0, α1 = 0.2

0.0

1.0

80

100
80
40
20
0
0.5

0.5

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0
0.0

0.0

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

0.0

0.5

1.0

1.5

0.0

0.5

1.0

1.5

Figure D.5: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
0.5, n = 1000

D-15

0.0

0.5

1.0

1.5

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

0.0

0.5

1.0

1.5

1.0

1.5

100
60
40
20
0.5

1.0

1.5

1.0

1.5

100
0

20

40

60

80

100
60
40
20
0
1.5

0.5

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

1.0

0.0

α0 = 0.1, α1 = 0.2

0

0.5

1.5

0
0.0

α0 = 0, α1 = 0.2

0.0

1.0

80

100
60
40
20
0
0.5

0.5

α0 = 0.2, α1 = 0.1

80

80
60
40
20
0
0.0

0.0

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

0.0

0.5

1.0

1.5

0.0

0.5

1.0

1.5

Figure D.6: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
0.5, n = 2000

D-16

1.0

1.5

2.0

80
60
40
20
0.5

1.0

1.5

2.0

2.0

60
40
20
0
0.5

1.0

1.5

2.0

2.0

1.5

2.0

0

20

40

60

80

100
60
40
20
0
1.5

1.0

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

1.0

0.5

α0 = 0.1, α1 = 0.2

0

0.5

2.0

100

1.5

1.5

80

100
80
40
20
0
1.0

α0 = 0, α1 = 0.2

1.0

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0

0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

100

0.5

0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

0.5

1.0

1.5

2.0

Figure D.7: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
0.75, n = 1000

D-17

1.5

80
20
0.5

1.0

1.5

1.5

60
40
20
0
0.5

1.0

1.5

1.5

1.0

1.5

α0 = 0.2, α1 = 0.2

0

20

40

60

80

100
60
40
20
0
1.0

0.5

α0 = 0.1, α1 = 0.2

80

100
80
60
40
20
0

0.5

1.5

100

1.0

α0 = 0, α1 = 0.2

1.0

α0 = 0.2, α1 = 0.1

80

100
80
60
40
20
0.5

0.5

α0 = 0.1, α1 = 0.1

0

0

20

40

60

80

100

α0 = 0, α1 = 0.1

100

1.0

0

0

20

40

60
40

60

80

80
60
40
20
0

0.5

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0
100

100

α0 = 0, α1 = 0

0.5

1.0

1.5

0.5

1.0

1.5

Figure D.8: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
0.75, n = 2000

D-18

1.0

1.5

2.0

60
0

20

40
0

20

40

60

80

100
80

80
60
40
20
0
0.5

0.5

1.0

1.5

2.0

1.5

2.0

100
60
40
1.5

2.0

2.0

0.5

1.0

1.5

2.0

100

α0 = 0.2, α1 = 0.2

0

20

40

60

80

100
60
40
20
0
1.5

2.0

20
1.0

α0 = 0.1, α1 = 0.2

80

100
80
60
40
20
0

1.0

1.5

0
0.5

α0 = 0, α1 = 0.2

0.5

1.0

α0 = 0.2, α1 = 0.1

80

100
80
60
40
20
1.0

0.5

α0 = 0.1, α1 = 0.1

0

0

20

40

60

80

100

α0 = 0, α1 = 0.1

0.5

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

0.5

1.0

1.5

2.0

Figure D.9: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
1, n = 1000

D-19

1.0

1.5

2.0

60
0

20

40
0

20

40

60

80

100
80

80
60
40
20
0
0.5

0.5

1.0

1.5

2.0

1.5

2.0

100
60
40
1.5

2.0

2.0

0.5

1.0

1.5

2.0

100

α0 = 0.2, α1 = 0.2

0

20

40

60

80

100
60
40
20
0
1.5

2.0

20
1.0

α0 = 0.1, α1 = 0.2

80

100
80
60
40
20
0

1.0

1.5

0
0.5

α0 = 0, α1 = 0.2

0.5

1.0

α0 = 0.2, α1 = 0.1

80

100
80
60
40
20
1.0

0.5

α0 = 0.1, α1 = 0.1

0

0

20

40

60

80

100

α0 = 0, α1 = 0.1

0.5

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

0.5

1.0

1.5

2.0

Figure D.10: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
1, n = 2000

D-20

0.5

1.0

1.5

2.0

2.5

3.0

3.5

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

1.5

2.0

2.5

3.0

3.5

100
2.5

3.0

3.5

3.5

1.0

1.5

2.0

2.5

3.0

3.5

100
0

20

40

60

80

100
60
40
3.0

0.5

α0 = 0.2, α1 = 0.2

20
2.5

3.5

60
2.0

0
2.0

3.0

40
1.5

80

100
80
60
40
20

1.5

2.5

20
1.0

α0 = 0.1, α1 = 0.2

0

1.0

2.0

0
0.5

α0 = 0, α1 = 0.2

0.5

1.5

80

100
80
40
20
0
1.0

1.0

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0
0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

0.5

1.0

1.5

2.0

2.5

3.0

3.5

0.5

1.0

1.5

2.0

2.5

3.0

3.5

Figure D.11: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
1.5, n = 1000

D-21

0.5

1.0

1.5

2.0

2.5

3.0

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

2.5

3.0

1.5

2.0

2.5

3.0

100
60
2.0

2.5

3.0

3.0

1.0

1.5

2.0

2.5

3.0

100
0

20

40

60

80

100
60
40
20
2.5

0.5

α0 = 0.2, α1 = 0.2

0
2.0

3.0

40
1.5

80

100
80
60
40
20

1.5

2.5

20
1.0

α0 = 0.1, α1 = 0.2

0

1.0

2.0

0
0.5

α0 = 0, α1 = 0.2

0.5

1.5

80

100
80
40
20
0
1.0

1.0

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0
0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

0.5

1.0

1.5

2.0

2.5

3.0

0.5

1.0

1.5

2.0

2.5

3.0

Figure D.12: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
1.5, n = 2000

D-22

1

2

3

4

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0

100

α0 = 0, α1 = 0

1

2

3

4

3

4

100
60
40
20
2

3

4

3

4

100
0

20

40

60

80

100
60
40
20
0
4

2

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

3

1

α0 = 0.1, α1 = 0.2

0

2

4

0
1

α0 = 0, α1 = 0.2

1

3

80

100
80
40
20
0
2

2

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0
1

1

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

1

2

3

4

1

2

3

4

Figure D.13: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
2, n = 1000

D-23

α0 = 0.2, α1 = 0

1.0

1.5

2.0

2.5

3.0

3.5

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

100

α0 = 0, α1 = 0

1.0

1.5

2.0

2.5

3.0

3.5

2.0

2.5

3.0

3.5

100
60
2.5

3.0

3.5

3.5

1.5

2.0

2.5

3.0

3.5

100
0

20

40

60

80

100
60
40
20
3.0

1.0

α0 = 0.2, α1 = 0.2

0
2.5

3.5

40
2.0

80

100
80
60
40
20

2.0

3.0

20
1.5

α0 = 0.1, α1 = 0.2

0

1.5

2.5

0
1.0

α0 = 0, α1 = 0.2

1.0

2.0

80

100
80
40
20
0
1.5

1.5

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0
1.0

1.0

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

1.0

1.5

2.0

2.5

3.0

3.5

1.0

1.5

2.0

2.5

3.0

3.5

Figure D.14: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
2, n = 2000

D-24

α0 = 0.2, α1 = 0

2

3

4

5

6

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

100

α0 = 0, α1 = 0

2

3

4

5

6

4

5

6

100
60
40
4

5

6

6

3

4

5

6

100
0

20

40

60

80

100
60
40
20
0
5

2

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

4

6

20
3

α0 = 0.1, α1 = 0.2

0

3

5

0
2

α0 = 0, α1 = 0.2

2

4

80

100
80
40
20
0
3

3

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0

2

2

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

2

3

4

5

6

2

3

4

5

6

Figure D.15: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
3, n = 1000

D-25

100
60
0

20

40
0

20

40

60

60
40
20
0

α0 = 0.2, α1 = 0

80

100

α0 = 0.1, α1 = 0

80

80

α0 = 0.2, α1 = 0.1

80
0

20

40

60

80
60
40
20
0

0

20

40

60

80

100

1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

α0 = 0.1, α1 = 0.1
100

1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

α0 = 0, α1 = 0.1
100

1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

α0 = 0.2, α1 = 0.2

80
20

40

60

80
60
40
20
0

0

20

40

60

80

100

1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

α0 = 0.1, α1 = 0.2
100

1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

α0 = 0, α1 = 0.2
100

1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

0

100

α0 = 0, α1 = 0

1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

Figure D.16: Coverage Curves for Bonferroni with and without Non-differential Bounds: β =
3, n = 2000

D-26

α0 = 0.1, α1 = 0

α0 = 0.2, α1 = 0

1.0

1.5

2.0

100
80
60
40
20
0.5

1.0

1.5

2.0

2.0

60
40
20
0
0.5

1.0

1.5

2.0

2.0

1.5

2.0

0

20

40

60

80

100
60
40
20
0
1.5

1.0

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

1.0

0.5

α0 = 0.1, α1 = 0.2

0

0.5

2.0

100

1.5

1.5

80

100
60
40
20
0
1.0

α0 = 0, α1 = 0.2

1.0

α0 = 0.2, α1 = 0.1

80

80
60
40
20
0

0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

100

0.5

0

0

0

20

20

40

40

60

60

80

80

100

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

0.5

1.0

1.5

Figure D.17: Coverage Curves for Bonferroni versus Hybrid CIs: β = 0.75, n = 1000

D-27

2.0

α0 = 0.1, α1 = 0

α0 = 0.2, α1 = 0

1.0

1.5

2.0

100
80
60
40
20
0.5

1.0

1.5

2.0

2.0

60
40
20
0
0.5

1.0

1.5

2.0

2.0

1.5

2.0

0

20

40

60

80

100
60
40
20
0
1.5

1.0

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

1.0

0.5

α0 = 0.1, α1 = 0.2

0

0.5

2.0

100

1.5

1.5

80

100
80
40
20
0
1.0

α0 = 0, α1 = 0.2

1.0

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0

0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

100

0.5

0

0

0

20

20

40

40

60

60

80

80

100

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

0.5

1.0

1.5

Figure D.18: Coverage Curves for Bonferroni versus Hybrid CIs: β = 0.75, n = 2000

D-28

2.0

α0 = 0.2, α1 = 0

0.5

1.0

1.5

2.0

2.5

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

2.5

1.5

2.0

2.5

100
60
40
1.5

2.0

2.5

2.5

1.0

1.5

2.0

2.5

100
0

20

40

60

80

100
60
40
20
0
2.0

0.5

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

1.5

2.5

20
1.0

α0 = 0.1, α1 = 0.2

0

1.0

2.0

0
0.5

α0 = 0, α1 = 0.2

0.5

1.5

80

100
60
40
20
0
1.0

1.0

α0 = 0.2, α1 = 0.1

80

80
60
40
20
0
0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

0.5

1.0

1.5

2.0

2.5

0.5

1.0

1.5

2.0

Figure D.19: Coverage Curves for Bonferroni versus Hybrid CIs: β = 1, n = 1000

D-29

2.5

α0 = 0.2, α1 = 0

0.5

1.0

1.5

2.0

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

1.0

1.5

2.0

100
60
40
1.5

2.0

0.5

2.0

1.0

1.5

2.0

100

α0 = 0.2, α1 = 0.2

0

20

40

60

80

100
0

20

40

60

80

100
80
60
40
20

1.5

2.0

20
1.0

α0 = 0.1, α1 = 0.2

0

1.0

1.5

0
0.5

α0 = 0, α1 = 0.2

0.5

1.0

α0 = 0.2, α1 = 0.1

80

100
80
0

20

40

60

80
60
40
20
0
0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

0.5

1.0

1.5

2.0

0.5

1.0

1.5

2.0

Figure D.20: Coverage Curves for Bonferroni versus Hybrid CIs: β = 1, n = 2000

D-30

α0 = 0.2, α1 = 0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

100

α0 = 0, α1 = 0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

1.5

2.0

2.5

3.0

3.5

100
2.5

3.0

3.5

3.5

1.0

1.5

2.0

2.5

3.0

3.5

100
0

20

40

60

80

100
60
40
3.0

0.5

α0 = 0.2, α1 = 0.2

20
2.5

3.5

60
2.0

0
2.0

3.0

40
1.5

80

100
80
60
40
20

1.5

2.5

20
1.0

α0 = 0.1, α1 = 0.2

0

1.0

2.0

0
0.5

α0 = 0, α1 = 0.2

0.5

1.5

80

100
80
40
20
0
1.0

1.0

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0
0.5

0.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

0.5

1.0

1.5

2.0

2.5

3.0

3.5

0.5

1.0

1.5

2.0

2.5

3.0

Figure D.21: Coverage Curves for Bonferroni versus Hybrid CIs: β = 1.5, n = 1000

D-31

3.5

α0 = 0.2, α1 = 0

1.0

1.5

2.0

2.5

3.0

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

α0 = 0, α1 = 0

1.0

1.5

2.0

2.5

3.0

2.0

2.5

3.0

100
60
40
2.0

2.5

3.0

3.0

1.5

2.0

2.5

3.0

100
0

20

40

60

80

100
60
40
20
0
2.5

1.0

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

2.0

3.0

20
1.5

α0 = 0.1, α1 = 0.2

0

1.5

2.5

0
1.0

α0 = 0, α1 = 0.2

1.0

2.0

80

100
80
40
20
0
1.5

1.5

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0
1.0

1.0

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

1.0

1.5

2.0

2.5

3.0

1.0

1.5

2.0

2.5

Figure D.22: Coverage Curves for Bonferroni versus Hybrid CIs: β = 1.5, n = 2000

D-32

3.0

80
20
0

0

20

40

60
40

60

80

80
60
40
20
0

α0 = 0.2, α1 = 0
100

α0 = 0.1, α1 = 0
100

α0 = 0, α1 = 0

α0 = 0.2, α1 = 0.1

80
0

20

40

60

80
60
40
20
0

0

20

40

60

80

100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0.1, α1 = 0.1
100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0, α1 = 0.1
100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0.2, α1 = 0.2

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

80
0

20

40

60

80
60
40
20
0

0

20

40

60

80

100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0.1, α1 = 0.2
100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

α0 = 0, α1 = 0.2
100

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

Figure D.23: Coverage Curves for Bonferroni versus Hybrid CIs: β = 2, n = 1000

D-33

α0 = 0.2, α1 = 0

1.5

2.0

2.5

3.0

3.5

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

α0 = 0, α1 = 0

1.5

2.0

2.5

3.0

3.5

2.5

3.0

3.5

100
60
40
2.5

3.0

3.5

3.5

2.0

2.5

3.0

3.5

100
0

20

40

60

80

100
60
40
20
0
3.0

1.5

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

2.5

3.5

20
2.0

α0 = 0.1, α1 = 0.2

0

2.0

3.0

0
1.5

α0 = 0, α1 = 0.2

1.5

2.5

80

100
80
40
20
0
2.0

2.0

α0 = 0.2, α1 = 0.1

60

80
60
40
20
0

1.5

1.5

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

1.5

2.0

2.5

3.0

3.5

1.5

2.0

2.5

3.0

Figure D.24: Coverage Curves for Bonferroni versus Hybrid CIs: β = 2, n = 2000

D-34

3.5

α0 = 0.1, α1 = 0
100

α0 = 0.2, α1 = 0

2

3

4

5

6

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0, α1 = 0

2

3

4

5

6

4

5

6

100
60
40
4

5

6

6

3

4

5

6

100
0

20

40

60

80

100
60
40
20
0
5

2

α0 = 0.2, α1 = 0.2

80

100
80
60
40
20

4

6

20
3

α0 = 0.1, α1 = 0.2

0

3

5

0
2

α0 = 0, α1 = 0.2

2

4

80

100
60
40
20
0
3

3

α0 = 0.2, α1 = 0.1

80

80
60
40
20
0
2

2

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

2

3

4

5

6

2

3

4

5

Figure D.25: Coverage Curves for Bonferroni versus Hybrid CIs: β = 3, n = 1000

D-35

6

α0 = 0.2, α1 = 0

2.0

2.5

3.0

3.5

4.0

4.5

5.0

80
60
40
20
0

0

0

20

20

40

40

60

60

80

80

100

α0 = 0.1, α1 = 0
100

α0 = 0, α1 = 0

2.0

2.5

3.0

3.5

4.0

4.5

5.0

3.0

3.5

4.0

4.5

5.0

100
4.0

4.5

5.0

2.0

5.0

3.0

3.5

4.0

4.5

5.0

100
0

20

40

60

80

100
60
40
4.5

2.5

α0 = 0.2, α1 = 0.2

20
4.0

5.0

60
3.5

0
3.5

4.5

40
3.0

80

100
80
60
40
20

3.0

4.0

20
2.5

α0 = 0.1, α1 = 0.2

0

2.5

3.5

0
2.0

α0 = 0, α1 = 0.2

2.0

3.0

80

100
60
40
20
0
2.5

2.5

α0 = 0.2, α1 = 0.1

80

80
60
40
20
0
2.0

2.0

α0 = 0.1, α1 = 0.1

100

α0 = 0, α1 = 0.1

2.0

2.5

3.0

3.5

4.0

4.5

5.0

2.0

2.5

3.0

3.5

4.0

4.5

Figure D.26: Coverage Curves for Bonferroni versus Hybrid CIs: β = 3, n = 2000

D-36

5.0

β
α0
0.0

0.1

0.2

0.3

α1
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3
0.0
0.1
0.2
0.3

0
0.29
0.32
0.36
0.41
0.32
0.36
0.41
0.48
0.36
0.41
0.48
0.57
0.41
0.48
0.57
0.72

0.25
0.3
0.35
0.41
0.48
0.35
0.41
0.48
0.59
0.41
0.48
0.59
0.73
0.48
0.59
0.73
0.95

0.5
0.31
0.4
0.51
0.64
0.4
0.51
0.64
0.82
0.51
0.65
0.83
1.09
0.64
0.83
1.1
1.49

0.75
0.31
0.44
0.59
0.76
0.44
0.6
0.78
1.02
0.59
0.79
1.05
1.4
0.77
1.02
1.4
1.93

1
0.31
0.48
0.65
0.79
0.48
0.68
0.89
1.09
0.65
0.9
1.2
1.58
0.8
1.13
1.62
2.36

1.5
0.3
0.48
0.57
0.68
0.48
0.65
0.83
0.98
0.58
0.89
1.22
1.53
0.69
1.19
1.79
1.58

2
0.29
0.36
0.46
0.56
0.37
0.48
0.61
0.75
0.46
0.61
0.77
0.97
0.56
0.75
0.97
1.25

3
0.25
0.33
0.41
0.5
0.33
0.43
0.54
0.65
0.41
0.54
0.67
0.85
0.5
0.65
0.85
1.1

Table D.20: Median width of hybrid CI constructed from nominal 95% GMM and nominal > 95%
Bonferroni intervals: n = 2000

D-37

