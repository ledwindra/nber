{
    "id": 31100,
    "citation_title": "No Ground Truth? No Problem: Improving Administrative Data Linking Using Active Learning and a Little Bit of Guile",
    "citation_author": [
        "Sarah Tahamont",
        "Zubin Jelveh",
        "Melissa McNeill",
        "Shi Yan",
        "Aaron Chalfin",
        "Benjamin Hansen"
    ],
    "citation_publication_date": "2023-04-10",
    "issue_date": "2023-04-06",
    "revision_date": "None",
    "topics": [
        "\n",
        "Econometrics",
        "\n",
        "Estimation Methods",
        "\n",
        "Data Collection",
        "\n"
    ],
    "program": [
        "\n",
        "Law and Economics",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nWhile linking records across large administrative datasets [\u201cbig data\u201d] has the potential to revolutionize empirical social science research, many administrative data files do not have common identifiers and are thus not designed to be linked to others. To address this problem, researchers have developed probabilistic record linkage algorithms which use statistical patterns in identifying characteristics to perform linking tasks. Naturally, the accuracy of a candidate linking algorithm can be substantially improved when an algorithm has access to \u201cground-truth\u201d examples \u2014 matches which can be validated using institutional knowledge or auxiliary data. Unfortunately, the cost of obtaining these examples is typically high, often requiring a researcher to manually review pairs of records in order to make an informed judgement about whether they are a match. When a pool of ground-truth information is unavailable, researchers can use \u201cactive learning\u201d algorithms for linking, which ask the user to provide ground-truth information for select candidate pairs. In this paper, we investigate the value of providing ground-truth examples via active learning for linking performance. We confirm popular intuition that data linking can be dramatically improved with the availability of ground truth examples. But critically, in many real-world applications, only a relatively small number of tactically-selected ground-truth examples are needed to obtain most of the achievable gains. With a modest investment in ground truth, researchers can approximate the performance of a supervised learning algorithm that has access to a large database of ground truth examples using a readily available off-the-shelf tool.\n\n",
    "acknowledgement": "\nThe authors would like to acknowledge the entities who supported the development of the open-source linking algorithms we use in our analysis: University of Chicago Crime Lab, New York and Arnold Ventures (Name Match) and dedupe.io (dedupe). The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}