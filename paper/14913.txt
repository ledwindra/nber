NBER WORKING PAPER SERIES

U.S. STOCK MARKET CRASH RISK, 1926-2006
David S. Bates
Working Paper 14913
http://www.nber.org/papers/w14913

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
April 2009

I am grateful for comments on earlier versions of the paper from seminar participants at Iowa, Turin,
Birkbeck, McGill’s 2008 Risk Management Conference, the 18th Annual Derivatives Securities and
Risk Management Conference in Arlington, VA, and Princeton’s 2008 Conference on Implied Volatility
Models. I also thank Ken French for providing the stock turnover data from French (2008). The views
expressed herein are those of the author(s) and do not necessarily reflect the views of the National
Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2009 by David S. Bates. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.

U.S. Stock Market Crash Risk, 1926-2006
David S. Bates
NBER Working Paper No. 14913
April 2009
JEL No. C22,C46,G1,G13
ABSTRACT
This paper applies the Bates (RFS, 2006) methodology to the problem of estimating and filtering timechanged Lévy processes, using daily data on U.S. stock market excess returns over 1926-2006. In
contrast to density-based filtration approaches, the methodology recursively updates the associated
conditional characteristic functions of the latent variables. The paper examines how well time-changed
Lévy specifications capture stochastic volatility, the “leverage” effect, and the substantial outliers
occasionally observed in stock market returns. The paper also finds that the autocorrelation of stock
market excess returns varies substantially over time, necessitating an additional latent variable when
analyzing historical data on stock market returns. The paper explores option pricing implications,
and compares the results with observed prices of options on S&P 500 futures.

David S. Bates
Henry B. Tippie College of Business
Department of Finance
University of Iowa
Iowa City, IA 52242-1000
and NBER
david-bates@uiowa.edu

What is the risk of stock market crashes? Answering this question is complicated by two features
of stock market returns: the fact that conditional volatility evolves over time, and the fat-tailed
nature of daily stock market returns. Each issue affects the other. What we identify as outliers
depends upon that day’s assessment of conditional volatility. Conversely, our estimates of current
volatility from past returns can be disproportionately affected by outliers such as the 1987 crash.
In standard GARCH specifications, for instance, a 10% daily change in the stock market has 100
times the impact on conditional variance revisions of a more typical 1% move.

This paper explores whether recently proposed continuous-time specifications of timechanged Lévy processes are a useful way to capture the twin properties of stochastic volatility and
fat tails. The use of Lévy processes to capture outliers dates back at least to Mandelbrot’s (1963)
use of the stable Paretian distribution, and there have been many specifications proposed; e.g.,
Merton’s (1976) jump-diffusion, Madan and Seneta’s (1990) variance gamma; Eberlein, Keller and
Prause’s (1998) hyperbolic Lévy; and Carr, Madan, Geman and Yor’s (2002) CGMY process. As
all of these distributions assume identical and independently distributed returns, however, they are
unable to capture stochastic volatility.

More recently, Carr, Geman, Madan and Yor (2003) and Carr and Wu (2004) have proposed
combining Lévy processes with a subordinated time process. The idea of randomizing time dates
back to at least to Clark (1973). Its appeal in conjunction with Lévy processes reflects the increasing
focus in finance – especially in option pricing – on representing probability distributions by their
associated characteristic functions. Lévy processes have log characteristic functions that are linear
in time. If the time randomization depends on underlying variables that have an analytic conditional
characteristic function, the resulting conditional characteristic function of time-changed Lévy
processes is also analytic. Conditional probability densities, distributions, and option prices can then
be numerically computed by Fourier inversion of simple functional transforms of this characteristic
function.

Thus far, empirical research on the relevance of time-changed Lévy processes for stock
market returns has largely been limited to the special cases of time-changed versions of Brownian

2
motion and Merton’s (1976) jump-diffusion. Furthermore, there has been virtually no estimation
of newly proposed time-changed Lévy processes solely from time series data.1 Papers such as Carr
et al (2003) and Carr and Wu (2004) have relied on option pricing evidence to provide empirical
support for their approach, rather than providing direct time series evidence. The reliance on options
data is understandable. Since the state variables driving the time randomization are not directly
observable, time-changed Lévy processes are hidden Markov models – a challenging problem in
time series econometrics. Using option prices potentially identifies realizations of those latent state
variables, converting the estimation problem into the substantially more tractable problem of
estimating state space models with observable state variables.

While options-influenced parameter and state variable estimates should be informative under
the hypothesis of correct model specification, the objective of the paper is to provide estimates of
crash risk based solely upon time series analysis. Such estimates are of interest in their own right,
and can exploit a longer history of extreme stock market movements than can studies constrained
by the availability of options data only since the 1980’s. For instance, the

stock market crash

of October 19, 1987 was the only daily stock market movement over 1945-2006 to exceed 10% in
magnitude, whereas there were seven such movements over 1929-32. Furthermore, time-series
based estimates can be relevant even for testing option pricing hypotheses. For instance, it has been
asserted that deep OTM index put options appear overpriced, based on their surprisingly large
negative returns since the ‘87 crash. But all such tests require reliable estimates of downside risk;
and it can be difficult to establish whether puts are indeed overpriced based only on the limited
amount of data since the 1987 crash.2 Time series estimates can exploit a longer history of downside
risk, and can be used to generate estimates of option prices that can be compared with observed
option prices.

1

Li, Wells and Yu (2008) use MCMC methods to estimate some models in which Lévy
shocks are added to various stochastic volatility models. However, the additional Lévy shocks are
i.i.d., rather than time-changed.
2

See Broadie, Chernov, and Johannes (2006) for a Monte Carlo study of unhedged 1-month
excess returns for puts on S&P 500 futures over August 1987 to June 2005. They find excess return
estimates often lack statistical significance, especially when volatility is stochastic.

3
This paper provides direct time series estimates of some proposed time-changed Lévy
processes, using the Bates (2006) approximate maximum likelihood (AML) methodology. AML is
a filtration methodology that recursively updates conditional characteristic functions of latent
variables over time given observed data. Filtered estimates of the latent variables are directly
provided as a by-product, given the close link between moments and characteristic functions. The
paper primarily focuses on the time-changed CGMY process, which nests various other processes
as special cases. The approach will also be compared to the time-changed jump-diffusions
previously estimated in Bates (2006).

A concern with any extended data set is the possibility that the data generating process may
not be stable over time. Indeed, this paper identifies a major instability in the autocorrelation of
daily stock market returns. Autocorrelation estimates appear to be nonstationary, and peaked at the
extraordinarily high level of 35% in 1971, before trending downwards to the near-zero values
observed since the 1980’s. The instability is addressed directly, by treating the autocorrelation as
another latent state variable to be estimated from observed stock market returns. The paper also
finds apparent instabilities or specification issues in the 1-factor volatility process used, and explores
the implications for volatility filtration and option pricing.

Overall, the time-changed CGMY process is found to be a slightly more parsimonious
alternative to the Bates (2006) approach of using finite-activity stochastic-intensity jumps drawn
from a mixture of normals, although the fits of the two approaches are not dramatically different.
Interestingly, one cannot reject the hypothesis that stock market crash risk is adequately captured
by a time-changed version of the Carr-Wu (2003) log-stable process. That model’s implications for
upside risk, however, are strongly rejected, with the model severely underpredicting the frequency
of large positive outliers.

Section I of the paper progressively builds up the time series model used in estimation.
Section I.1 discusses basic Lévy processes and describes the processes considered in this paper.
Section I.2 discusses time changes and the equivalence with stochastic volatility. Section I.3
contains further modifications of the model to capture leverage effects, time-varying
autocorrelations, and day-of-the-week effects. Section I.4 describes how the model is estimated,

4
using the Bates (2006) AML estimation methodology for hidden Markov models.

Section II describes the data on excess stock market returns over 1926-2006, and presents
parameter estimates, diagnostics, and and filtered estimates of latent autocorrelation and volatility.
Section III examines option pricing implications, while Section IV concludes.

I. Time-changed Lévy processes
I.1 Lévy processes
A Lévy process

is an infinitely divisible stochastic process; i.e., one that has independent and

identically distributed increments over non-overlapping time intervals of equal length. The Lévy
processes most commonly used in finance have been Brownian motion and the jump-diffusion
process of Merton (1976), but there are many others. All Lévy processes without a Brownian
motion component are pure jump processes. Such processes are characterized by their Lévy density
, which gives the intensity (or frequency) of jumps of size x. Alternatively and equivalently,
Lévy processes can be described by their generalized Fourier transform
(1)
where u is a complex-valued element of the set
is the characteristic function of

, while

for which (1) is well-defined. If

is real,

is the cumulant generating function of

. Its linearity in time follows from the fact that Lévy processes have i.i.d. increments.
Following Wu (2006), the function

will be called the cumulant exponent of

The Lévy-Khintchine formula gives the mapping between jump intensities
cumulant exponent for arbitrary

.3

and the

. Lévy processes in finance are typically specified for the

log asset price, and then exponentiated:

. For such specifications, it is convenient

to write the Lévy-Khintchine formula in the form
(2)
where

is the continuously-compounded expected return on the asset:
3

Carr et al (2003) call
the “unit time log characteristic function.” Bertoin (1996) uses
the characteristic exponent, which takes the form
.

5
(3)
Pure-jump Lévy processes can be thought of as a drift term plus an infinite sum
independent point processes, each drift-adjusted to make

of

a martingale:
(4)

where

is an integer-valued Poisson counter with intensity

that counts the occurrence of

jumps of fixed size x. The log characteristic function of a sum of independent point processes is the
sum of the log characteristic functions of the point processes, yielding equation (2). Exponential
martingale processes of the form

for

Lévy processes, as will also diffusions of the form

defined in (4) will be termed compensated
.

As discussed in Carr et al (2002), Lévy processes are finite-activity if

, and

infinite-activity otherwise. Finite-activity jumps imply there is a non-zero probability that no jumps
will be observed within a given time interval.

Lévy processes are finite-variation if

, and infinite-variation otherwise. An infinite-variation process has sample paths
of infinite length – a property also of Brownian motion. All Lévy processes must have finite
, in order to be well-behaved, but need not have finite variance

–

the stable distribution being an counterexample. A priori, all financial prices must be finite-activity
processes, since price changes reflect a finite (but large) number of market transactions. However,
finite-activity processes can be well approximated by infinite-activity processes, and vice versa; e.g.,
the Cox, Ross and Rubinstein (1979) finite-activity binomial approximation to Brownian motion.
Activity and variation will therefore be treated as empirical specification issues concerned with
identifying which functional form

for jump intensities best fits daily stock market excess

returns.

I will consider two particular underlying Lévy processes for log asset prices. The first is
Merton (1976)’s combination of a Brownian motion plus finite-activity normally distributed jumps:
(5)
where

is a Wiener process,

6
is a Poisson counter with intensity

,

is the normally distributed jump conditional upon a jump occurring, and
is the expected percentage jump size conditional upon a jump.
The associated intensity of jumps of size x is

(6)

while the cumulant exponent takes the form

The approach can be generalized to allow alternate distributions for

– in particular, a mixture of

normals:
(7)

Second, I will consider the generalized CGMY process of Carr, Madan, Geman and Yor
(2003), which has a Lévy density of the form

(8)

where

and

. The associated cumulant exponent is

(9)

where

is a mean-normalizing constant determined by

;

V is the variance per unit time, and
is the fraction of variance attributable to the downward-jump component.
The corresponding intensity parameters

in (8) are

7

(10)

where

is the gamma function.

As discussed in Carr et al (2002), the

parameters are key in controlling jump activity near

0, in addition to their influence over tail events. The process has finite activity for
variation for

, but infinite activity or variation if

, finite

is greater or equal to 0 or

1, respectively. The model conveniently nests many models considered elsewhere. For instance,
is the finite-activity double exponential jump model of Kou (2002), while
includes the variance gamma model of Madan and Seneta (1990). As

and

approach 2, the CGMY process converges to a diffusion, and the cumulant exponent converges to
the corresponding quadratic form
(11)

As G and M approach 0 (for arbitrary

,

and fixed

), the Lévy density (8)

approaches the infinite-variance log stable process advocated by Mandelbrot (1963), with a “power
law” property for asymptotic tail probabilities. The log-stable special case proposed by Carr and
Wu (2003) is the limiting case with only negative jumps (

). While infinite-variance for log

returns, percentage returns have finite mean and variance under the log-stable specification. For
daily stock market returns of less than 25% in magnitude, the log-stable process is well
approximated by a finite-variance CGMY process with minimal exponential dampening; e.g.,
.

The cumulant exponent of any finite-variance Lévy process can written in the form
(12)
where

is variance per unit time and

is a standardized cumulant exponent with

unitary variance. One can also combine Lévy processes, to nest alternative specifications within a
broader specification. Any linear combination

of Lévy densities for nonnegative

8
weights that sum to one is also a valid Lévy density, and generates an associated standardized
weighted cumulant exponent of the form
cumulant exponent associated with

for

, where
1,2. The various

is the standardized
specifications that will

be considered in this paper are listed in Table 1.

I.2 Time-changed Lévy processes and stochastic volatility
Time-changed Lévy processes generate stochastic volatility by randomizing time in equation (1).
Since the log transform (1) can be written as

, randomizing time is funda-

mentally equivalent to randomizing variance. As the connection between time changes and
stochastic volatility becomes less transparent once “leverage” effects are added, I will use an explicit
stochastic volatility (or stochastic intensity) representation of stochastic processes.

The leverage effect, or correlation between asset returns and conditional variance
innovations, is captured by directly specifying shocks common to both. I will initially assume that
the log asset price

follows a process of the form

(13)
The log increment

consists of the continuously-compounded return, plus increments to two

exponential martingales.

is a Wiener increment, while

Lévy process, with finite instantaneous variance

is the increment to a compensated
. Further refinements will be added

below, to match properties of stock market returns more closely.

This specification has various features or implicit assumptions. First, the approach allows
considerable flexibility regarding the distribution of the instantaneous shock

to asset returns,

which can be Wiener, compound Poisson, or any other fat-tailed distribution. Three underlying
Lévy processes are considered:
1) a second diffusion with variance

that is independent of

(Heston, 1993);

2) finite-activity jumps drawn from a normal distribution or a mixture of normals; and
3) the generalized CGMY (2003) Lévy process from (8) above.
Combinations of these processes will also be considered, to nest the alternatives.

9
Second, the specification assumes a single underlying variance state variable

that follows

an affine diffusion, and which directly determines the variance of diffusion and jump components.
This approach generalizes the stochastic jump intensity model of Bates (2000, 2006) to arbitrary
Lévy processes.

Two alternate specifications are not considered, for different reasons. First, I do not consider
the approach of Li, Wells and Yu (2008), who model log-differenced asset prices as the sum of a
Heston (1993) stochastic volatility process and a constant-intensity fat-tailed Lévy process that
captures outliers. Bates (2006, Table 7) found the stochastic-intensity jump model fits S&P returns
better than the constant-intensity specification, when jumps are drawn from a finite-activity normal
distribution or mixture of normals. Second, the diffusion assumption for

rules out volatility-jump

models, such as the exponential-jump model proposed by Duffie, Pan and Singleton (2000) and
estimated by Eraker, Johannes and Polson (2003). Estimation on simulated data indicates that the
AML filtration methodology described below has difficulty identifying whether there are jumps in
an underlying conditional variance state variable that is not directly observed.

Define

as the discrete-time return observed over horizon
as the cumulant exponent of

construction,

is a standardized cumulant exponent, with

, and define

conditional upon knowing

. By

and variance

.

A key property of affine models is the ability to compute the conditional generalized Fourier
transform of

. This can be done by iterated expectations, conditioning initially on the

future variance path:

(14)

for
the future spot variance

. This is the generalized Fourier transform of
and the average future variance

. This is a well-known

10
problem (see, e.g., Bakshi and Madan, 2000), with an analytic solution if
process. For the affine diffusion above,

follows an affine

solves the Feynman-Kac partial differential

equation
(15)
subject to the boundary condition

. The solution is
(16)

where

(17)

(18)

(19)

(20)

(21)

The specifications of

considered in this paper are listed above in Table 1.

I.3 Autocorrelations and other refinements
That stock indexes do not follow a random walk was recognized explicitly by Lo and MacKinlay
(1988), and implicitly by earlier practices in variance and covariance estimation designed to cope

11
with autocorrelated returns; e.g., Dimson (1979)’s lead/lag approach to beta estimation. The
positive autocorrelations typically estimated for stock index returns are commonly attributed to stale
prices in the stocks underlying the index. A standard practice in time series analysis is to pre-filter
the data by fitting an ARMA specification; see, e.g., Jukivuolle (1995). Andersen, Benzoni and
Lund (2002), for instance, use a simple MA(1) specification to remove autocorrelations in S&P 500
returns over 1953-96; a data set subsequently used by Bates (2006).

Prefiltering the data was considered unappealing in this study, for several reasons. First, the
1926-2006 interval used here is long, with considerable variation over time in market trading
activity and transactions costs, and structural shifts in the data generating process are probable.
Indeed, Andersen et al (2002, Table 1) find autocorrelation estimates from their full 1953-96 sample
diverge from estimates for a 1980-96 subsample. Second, ARMA packages use a mean squared
error criterion that is not robust to the fat tails observed in stock market returns. Finally, explicit
consideration of autocorrelation is needed when assessing the variance of relevance to option
pricing.

Consequently, autocorrelations were treated as an additional latent variable, to be estimated
as part of the overall time series model. I will explore below two alternate models for daily logdifferenced stock index excess returns

:
(22)

or
(23)
where

(24)

is the effective length of a business day,

determines is the daily autocorrelation,

instantaneous intradaily underlying shock to log asset prices, and
instantaneous conditional variance of

. The intradaily shocks

is the
is the

are given by (13) above.

12
Both models add an autocorrelation state variable

that captures the fact that auto-

correlations of stock market returns are not constant over time.4 Following the literature on timevarying coefficient models, the autocorrelation is modeled as a simple random walk, to avoid
constraining estimates of

. Estimation of the autocorrelation volatility parameter

endogenously

determines the appropriate degree of smoothing to use when filtering the current autocorrelation
value

from past data.

The two models differ in ease of use, in their implications for the interaction between
volatility and autocorrelation, and in the pricing of risks. Model 1 assumes the stock market excess
return residual

is stationary (i.e., with a stationary conditional variance process),

and that the current value of

affects only the conditional mean of

. Autocorrelation filtration

in the model is consequently closer to standard autocorrelation estimation, and becomes identical
when

is i.i.d. Gaussian and the autocorrelation is constant (

). Model 1 is also somewhat

more convenient for estimation, in that it has a “semi-affine” structure that can be directly estimated
using the methodology of Bates (2006).

In Model 2,

is the permanent impact of daily shocks to stock index excess returns, and

is again assumed stationary. The model assumes that infrequent trading in the component stocks
(proxied by

) slows the incorporation of such shocks into the observed stock index, but that the

index ultimately responds fully once all stocks have traded.5 Unlike Model 1, Model 2 is consistent
with LeBaron’s (1992) observation that annual estimates of daily stock market volatility and
autocorrelation appear inversely related. Higher autocorrelations smooth shocks across periods,
reducing observed market volatility. Furthermore, the model is more suitable for pricing risks; i.e.,
identifying the equity premium, or the (affine) risk-neutral process underlying option prices. The
current value of

affects both the conditional mean and higher moments of

, resulting in a

4

See, e.g., Andersen, Benzoni and Lund (2002, Table I), who estimate different
autocorrelations for 1953-96 and 1980-96.
5

Jukivuolle (1995) distinguishes between the “observed” and “true” stock index when trading
is infrequent, and proposes using a standard Beveridge-Nelson decomposition to identify the latter.
This paper differs in assuming that the parameters of the ARIMA process for the observed stock
index are not constant.

13
significantly different filtration procedure for estimating

from past excess returns. The time

series model is not semi-affine, but I develop below a transformation of variables that makes
filtration and estimation as tractable as for Model 1.

Both models build upon previous time series and market microstructure research into stock
market returns. For instance, the effective length

of a business day is allowed to vary based upon

various periodic effects. In particular, day-of-the-week effects, weekends, and holidays are
accommodated by estimated time dummies that allow day-specific variation in

. In addition, time

dummies were estimated for the Saturday morning trading available over 1926-52, and for the
Wednesday exchange holidays in the second half of 1968 that are the focus of French and Roll
(1986).6 Finally, the stock market closings during the “Bank Holiday” of March 3-15, 1933 and
following the September 11, 2001 attacks were treated as

- and

-year returns, respectively.

Treating the 1933 Bank Holiday as a 12-day interval substantially reduces the influence of its
+15.5% return on parameter estimation. September 17, 2001 saw a smaller movement, of -4.7%.

For Model 1, the cumulant generating function of future returns and state variable
realizations conditional upon current values is analytic, and of the semi-affine form

(25)
where

, and

and

are given in (17) and

(18) above. For model 2, the conditional cumulant generating function is of the non-affine form
(26)
given the shocks to

are scaled by

.7

6

Gallant, Rossi and Tauchen (1992) use a similar approach, and also estimate monthly
seasonals.
7

Dilip Madan informs me that practitioners distinguish between time-scaled and space-scaled
models of time-varying volatility. GARCH models are typically space-scaled, whereas Model 1 is
a time-scaled model of stochastic volatility. Model 2 contains both (stationary) time scaling via
and the time dummies, and (non-stationary) space scaling via
.

14
I.4 Filtration and maximum likelihood estimation
If the state variables

were observed along with returns, it would in principle be possible to

evaluate the joint transition densities of the data and the state variable evolution by Fourier inversion
of the joint conditional characteristic function

, and to use this in a maximum

likelihood procedure to estimate the parameters of the stochastic process. However, since
are latent rather than directly observed, this is a hidden Markov model that must be estimated by
other means.

For Model 1, the assumption that the cumulant generating function (25) is affine in the latent
state variables

implies that the hidden Markov model can be filtered and estimated using

the approximate maximum likelihood (AML) methodology of Bates (2006). The AML procedure
is a filtration methodology that recursively updates the conditional characteristic functions of the
latent variables and future data conditional upon the latest datum. Define

as

the data observed up through period t, and define
(27)
as the joint conditional characteristic function that summarizes what is known at time t about
. The density of the observation

conditional upon

can be computed by Fourier

inversion of its conditional characteristic function:
(28)

Conversely, the joint conditional characteristic function
observation can be updated given

needed for the next

by the characteristic-function equivalent of Bayes’ rule:
(29)

The algorithm begins with an initial joint characteristic function
recursively through the entire data set, generating the log likelihood function

and proceeds
used

in maximum likelihood estimation. Filtered estimates of the latent variables can be computed from
derivatives of the joint conditional moment generating function, as can higher conditional moments:

15

(30)

The above procedure, if implementable, would permit exact maximum likelihood function
estimation of parameters. However, the procedure would require storing and updating the entire
function

based on point-by-point univariate numerical integrations. As such a procedure

would be slow, the AML methodology instead approximates

at each point in time by a

moment-matching joint characteristic function, and updates the approximation based upon updated
estimates of the moments of the latent variables. Given an approximate prior
, (30) is used to compute the posterior moments of
an approximate

and a datum

, which are then used to create

. The overall procedure is analogous to the Kalman filtration procedure

of updating conditional means and variances of latent variables based upon observed data, under the
assumption that those variables and the data have a conditional normal distribution. However, the
equations (29) and (30) identify the optimal nonlinear moment updating rules for a given prior
, whereas standard Kalman filtration uses linear rules. It will be shown below that this
modification in filtration rules is important when estimating latent autocorrelations and variances
under fat-tailed Lévy processes. Furthermore, Bates (2006) proves that the iterative AML filtration
is numerically stable, and shows that it performs well in estimating parameters and latent variable
realizations.

Autocorrelations can be negative or positive, while conditional variance must be positive.
Consequently, different two-parameter distributions were used to summarize conditional
distributions of the two latent variables: Gaussian for autocorrelations, gamma for variances.
Furthermore, since volatility estimates mean-revert within months whereas autocorrelation estimates
evolve over years, realizations of the two latent variables were assumed conditionally independent.
These assumptions resulted in an approximate conditional characteristic function of the form
(31)
The following summarizes key features of joint conditional distributions of the latent variables.

16
Autocorrelation

spot variance

Distribution
conditional
cumulant
generating function
initial CGF

assumed independent for all t.

Initial variance was assumed drawn from its unconditional gamma distribution, with the
parameters

given above.

Since autocorrelations were assumed nonstationary, no

unconditional distribution exists. Consequently, the AML algorithm for Model 1 was initiated using
a relatively diffuse conditional distribution for the initial autocorrelation – one much wider than the
plausible (-1, +1) range.

The parameters

– or, equivalently the moments

– summarize what is known about the latent variables. These were updated daily using the latest
observation

and equations (29) - (30). For each day, 5 univariate integrations were required:

1 for the density evaluation in (29), and 4 for the mean and variance evaluations in (30). An upper
was computed for each integral which upper truncation error would be less than
The integrands were then integrated over

in magnitude.

to a relative accuracy of

, using

IMSL’s adaptive Gauss-Legendre quadrature routine DQDAG and exploiting the fact that the
integrands for negative

are the complex conjugates of the integrands evaluated at positive

.

On average between 234 and 448 evaluations of the integrand were required for each integration.8

The non-affine specification
restrictions upon the distribution of latent

in Model 2 necessitates additional
. In particular, it is desirable that the scaling factor

be nonnegative, so that the lower tail properties of

8

originating in the underlying Lévy

The FFT approach used in Carr et al (2002) uses 16,384 functional evaluations.

17
specifications do not influence the upper tail properties of
latent

. Consequently, the distribution of

for Model 2 is modeled as inverse Gaussian – a 2-parameter unimodal distribution with

conditional mean

and variance

. Appendix A derives the resultant filtration procedure

for this model, exploiting a useful change of variables procedure. The filtration is initiated at
, and it is again assumed that

and

are conditionally independent for all t.

II. Properties of U.S. stock market returns, 1926 - 2006
II.1 Data
There are two readily available value-weighted measures of the U.S. stock market: the CRSP valueweighted index, and the S&P Composite Index. This paper will primarily focus upon the former for
time series analysis, but will also consider the latter when assessing options on S&P 500 futures. The
CRSP data consist of 21,519 daily cum-dividend returns over January 2, 1926 through December
29, 2006. CRSP daily returns for each month were converted to daily log excess returns using
Ibbotson and Associates’ data on monthly Treasury bill returns, and the formula
(32)

where

is the daily CRSP cum-dividend return;
is that month’s return on Treasury bills of at least 1 month to maturity;
N is the number of calendar days spanned by the monthly Treasury bill return; and
is the number of calendar days spanned by the “daily” return

.

The monthly interest rate data were downloaded from Ken French’s Web site, and extended
backwards through 1926 using data in Ibbotson and Associates’ SBBI Yearbook.

The Schwert (1990) data set of daily U.S. stock market returns provides cum-dividend
returns on the S&P Composite Index from January 4, 1928 onwards.9 The S&P index was based
upon 90 stocks until March 4, 1957, and 500 stocks thereafter. I updated Schwert’s data through
2006 using Schwert’s data methodology: ex-dividend daily S&P 500 returns from CRSP were
augmented by an average daily dividend yield computed from monthly S&P 500 dividend yields
9

Schwert also has daily data extending back to 1885, based on the (price-weighted) Dow
Jones Industrial Average.

18
from Bloomberg. Cum-dividend returns were then converted into log excess returns using (32).
Furthermore, CRSP value-weighted returns were used instead of the S&P 90 returns prior to March
5, 1957, for two reasons. First, that delivers data over 1926 and 1927, which is important for
volatility assessment prior to the 1929 stock market crash. Second, the S&P Composite Index is
only reported to two decimal places, which creates significant rounding error issues for the low S&P
index values (around 5) observed in the 1930’s.

II.2 Parameter estimates
Table 2 describes and provides estimates of the time dummies from the most general time-changed
CGMY model,10 with Wednesday returns (Tuesday close to Wednesday close) arbitrarily selected
as the benchmark day. Daily variance tended to be highest at the beginning of the week and decline
thereafter, but day-of-the-week effects do not appear to be especially pronounced. The major
exception is the Saturday morning (10 AM to noon) trading generally available over 1926-52.11
Saturdays were effectively 43% as long as the typical Wednesday. Total weekend variance (Friday
close to Monday close) was (.43 + 1.05) / 1.10 - 1 = 34.5% higher when Saturday trading was
available (over 1926-52) than when it was not (over 1945-2006).12 This is qualitatively similar to
but less pronounced than the doubling of weekend variance found by Barclay, Litzenberger and
Warner (1990) in Japanese markets when Saturday half-day trading was feasible. Barclay et al
lucidly discuss market microstructure explanations for the increase in variance.

Holidays generally did not have a strong impact on the effective length of a business day –
with the exception of holiday weekends spanning 4 calendar days. Consistent with French and Roll
(1986), 2-day returns spanning the Wednesday exchange holidays in 1968 (Tuesday close to
Thursday close) had a variance not statistically different from a typical 1-day Wednesday return, but

10

Estimates from other specifications were virtually identical, with estimates typically within
±0.01 of the YY model’s estimates.
11

Saturday trading was standard before 1945. Over 1945-51, it was increasingly eliminated
in summer months, and was permanently eliminated on June 1, 1952.
12

As the time dummy estimates are estimated jointly with the volatility and autocorrelation
filtrations, the estimates of weekend variances with versus without Saturday trading control for
divergences in volatility and autocorrelation levels in the two samples.

19
substantially less than the 1 + .94 = 1.94 two-day variance observed for returns from Tuesday close
to Thursday close in other years. Overall, the common practice of ignoring day-of-the-week effects,
weekends, and holidays when analyzing the time series properties of daily stock market returns
appears to be a reasonable approximation, provided the data exclude Saturday trading.

Tables 3A and 3B report estimates for various specifications listed in Table 1, while Figure
1 presents associated normal probability plots for model 2. (The plots for Model 1 were similar.)
As noted above, all specifications capture the leverage effect by a correlation

with the diffusion

shock to conditional variance. The specifications diverge in their modeling of the Lévy shocks
orthogonal to the variance innovation. SV is the Heston model, while SVJ1 and SVJ2 have a
diffusion for small asset return shocks, plus finite-activity normally-distributed jumps to capture
outliers. The other models examine the generalized time-changed CGMY model, along with
specific parameter restrictions or relaxations.

Most specifications using either Model 1 or Model 2 have similar estimates for the
parameters determining the conditional mean and stochastic variance evolution. The evidence for
a variance-sensitive equity premium (

) is stronger for Model 2 specifications, but

is not

typically significantly different from zero for either model. Latent permanent variance in Model 2
mean-reverts towards an estimated average level around

, with a half-life about 1.6 months.

The SV and LS models are the outliers, with different estimates of the equity premia and variance
process from other specifications. As discussed below in section II.6, this reflects these two
specifications’ substantially different approach to variance filtration, given different assessments of
tail risk.

The various specifications primarily diverge in how they capture tail risk. The Merton-based
SVJ1 and SVJ2 results in Table 3B largely replicate the jump risk results in Bates (2006). The SVJ1
model has symmetric normally-distributed jumps with standard deviation 3 - 3.4% and time-varying
jump intensities that occur on average

= 3.3 - 3.7 jumps per year. As shown in Figure 1,

this jump risk assessment fails to capture the substantial 1987 crash. By contrast, the SVJ2 model
adds a second jump component that directly captures the 1987 outlier. The resulting increase in log
likelihood from is statistically significant under a likelihood ratio test, with a marginal significance

20
level around 3% for Models 1 and 2.

The various CGMY models primarily diverge across the specification of the
parameters – whether they are set to specific levels, and whether they diverge for the intensities of
positive versus negative jumps. The DEXP model with

is conceptually similar to the

jump-diffusion model SVJ1, but uses instead a finite-activity double exponential distribution for
jumps. Despite the fatter-tailed specification, Figure 1 indicates the DEXP model has difficulties
comparable to SVJ1 in capturing the 1987 crash. The VG model replaces the finite-activity double
exponential distribution with the infinite-activity variance process (

), and does slightly

better in fit. Both models include a diffusion component, which captures 73-74% of the variance
of the orthogonal Lévy shock

.

Specifications Y, YY, and LS involve pure-jump processes for the orthogonal Lévy process
, without a diffusion component. Overall, higher values of Y fit the data better – especially the
1987 crash, which ceases to be an outlier under these specifications. Relaxing the restriction
leads to some improvement in fit, with the increase in log likelihood (YY versus Y) having
P-values of 1.8% and 0.8% for Models 1 and 2, respectively. Point estimates of the jump parameters
governing downward jump intensities diverge sharply from the parameters
governing upward jump intensities when the
although standard errors are large. The dampening coefficient

restriction is relaxed,
is not significantly different from

zero, implying one cannot reject the hypothesis that the downward-jump intensity is from a
stochastic-intensity version of the Carr-Wu (2003) log-stable process. By contrast, the upward
intensity is estimated as a finite-activity jump process – which, however, still overestimates the
frequency of big positive outliers (Figure 1, sixth panel).

Motivated by option pricing issues, Carr and Wu (2003) advocate using a log-stable
distribution with purely downward jumps. An approximation to this model generated by setting
and

fits stock market returns very badly. The basic problem is that while the LS

model does allow positive asset returns, it severely underestimates the frequency of large positive
returns. This leads to a bad fit for the upper tail (Figure 1, last panel). However, the YY estimates
indicate that the Carr-Wu specification can be a useful component of a model, provided the upward

21
jump intensity function is modeled separately.

Unrestricted CGMY models generate at least one Y parameter in the infinite-activity,
infinite-variation range [1, 2], and typically near the diffusion value of 2. This suggests that the
models may be trying to capture considerable near-zero activity. However, adding an additional
diffusion component to the time-changed YY Lévy specification to capture that activity separately
(specification YY_D) led to no significant improvement in fit.

Overall, Figure 1 suggests the differences across the alternate fat-tailed specifications are
relatively minor, and fit the data similarly over most of the data range (

). The models SV,

SVJ1, DEXP, VG, and LS appear less desirable, given their failure to capture the largest outliers.
The SVJ2, Y, and YY specifications appear to fit about the same. All models appear to have a small
amount of specification error (deviations from linearity) in the
tail (

range and in the upper

).

II.3 Unconditional distributions
A further diagnostic of model specification is the models’ ability or inability to match the
unconditional distribution of returns – in particular, the tail properties of unconditional distributions.
Mandelbrot (1963) and Mandelbrot and Hudson (2004) argue that empirical tails satisfy a “power
law:” tail probabilities plotted against absolute returns approach a straight line when plotted on a
log-log graph. This empirical regularity underlies Mandelbrot’s advocacy of the stable Paretian
distribution, which possesses this property and is nested within the CGMY model for

.

Mandelbrot’s argument is premised upon i.i.d. returns, but the argument can be extended to
time-changed Lévy processes. Conditional Lévy densities time-average; if the conditional intensity
of moves of size x is

, the unconditional frequency of moves of size x is

. Since unconditional probability density functions asymptotically approach the
unconditional Lévy densities for large

, while unconditional tail probabilities approach the

corresponding integrals of the unconditional Lévy densities, examining unconditional distributions
may still be useful.

22
Figure 2a provides estimates of unconditional probability density functions of stock market
excess return residuals

for various specifications under Model 1, as well as data-

based estimates from a histogram of filtered residuals

. Given the day-of-the-

week effects reported in Table 2, the unconditional density functions are a horizon-dependent
mixture of densities, with mixing weights set equal to the empirical frequencies. (The two shocks
spanning the market closings in 1933 and 2001 were omitted.) The substantial impact of the 1987
crash outlier upon parameter estimates is apparent. The SVJ2 estimates treat that observation as a
unique outlier, while the CGMY class of models progressively fatten the lower tail as greater
flexibility is permitted for the lower tail parameter

. As noted above, the lower tail approaches

the Carr-Wu (2003) log-stable (LS) estimate. However, the LS model is unable to capture the
frequency of large positive outliers, and behaves similarly to the SV model in the upper tail. All
models closely match the empirical unconditional density function in the ±3% range where most
observations occur; and all models underestimate the unconditional frequency of moves of 3% - 7%
in magnitude.

Figure 2b provides similar estimates for unconditional lower and upper tail probabilities.
In addition, 1000 sample paths of stock market excess return residuals over 1926-2006 were
simulated via a Monte Carlo procedure using YY parameter estimates, in order to provide
confidence intervals on tail probability estimates.13 Unsurprisingly, the confidence intervals on
extreme tail events are quite wide. The underestimation of moves of 3% - 7% in magnitude is again
apparent, and is statistically significant. This rejection of the YY model does not appear attributable
to misspecification of the Lévy density

, which in Figure 1 captures conditional densities quite

well. Rather, the poor unconditional fit in Figures 2a and 2b appears due to misspecification of
volatility dynamics. Half of the 3-7% moves occurred over 1929 - 1935 – a prolonged highvolatility period that simulated volatility realizations from the 1-factor variance process of equation
(13) generally do not match.

13

Conditional variance sample paths were simulated using the approach of Bates (2006,
Appendix A.6), while Lévy shocks
conditional upon intradaily average variance and data-based
daily time horizons were generated via an inverse CDF methodology, with CDF’s computed by
Fourier inversion. The two shocks spanning the market closings in 1933 and 2001 were omitted.

23
Figure 3 plots model-specific tail probability estimates for the YY model on the log-log
scales advocated by Mandelbrot, along with data-specific quantiles for 20,004 stock market residuals
that have roughly a 1-day estimated time horizon (±11%). The lower tail probability does indeed
converge to the unconditional tail intensity
(36)

where

and

Furthermore, given G estimates near 0,

is the incomplete gamma function.
is roughly a power function in y,

implying near linearity when plotted on a log-log scale.

However, the graph indicates that the convergence of tail probabilities to the tail intensity
occurs only for observations in excess of 5% in magnitude – roughly 5 standard deviations. As this
is outside the range of almost all data, it does not appear that log-log scales provide a useful
diagnostic of model specification and tail properties for daily data. This is partly due to stochastic
volatility, which significantly slows the asymptotic convergence of unconditional tail probabilities
to

for large

. Absent stochastic volatility (

Lévy process converge to

), the tail probabilities of an i.i.d. YY

for observations roughly in excess of 3% in magnitude (3 standard

deviations).

No power law properties are observed for upper tail probabilities, given substantial estimated
exponential dampening. The failure of both lower and upper unconditional tail probabilities to
capture the frequency of moves of 3-7% in magnitude is again apparent, and statistically significant.

II.4 Subsample estimates
Table 4 provides estimates for data subsamples, as a test of the stability of the time series process.
The mean, stochastic volatility and jump parameters were allowed to differ before and after March
5, 1957.14 The time dummies (similar to those in Table 2) that capture day-of-the-week effects were

14

The data split was chosen so that the second subsample’s estimates could be compared with
estimates from S&P 500 returns, as well as with other studies that use data starting in the 1950’s
(Andersen et al (2002), Bates (2006), Chernov et al (2003)).

24
kept common across subsamples; but some of those dummies also capture subsample-specific
phenomena (Saturday trading before 1953; exchange holidays in 1968). The estimation and
filtration over the two subsamples nest the full-sample estimates of Table 3, so that standard
likelihood ratio tests can be used to test whether the divergence in subsample parameter estimates
are statistically significant.

Parameter estimates diverge strongly across subsamples, with P-values less than

, but

in different fashions for the SVJ1 and YY models. For the SVJ1 model, the major divergence was
clearly in the estimated volatility process. The 1926-57 period includes the highly volatile 1930’s,
yielding an overall average variance of

over 1926-57, as opposed to

over 1957-2006.

The volatility dynamics also diverge, with volatility more volatile and with faster mean reversion
over 1926-57 than over 1957-2006. Jump risk estimates also diverge, with more frequent but
smaller jumps in the first half than in the second half. Progressively relaxing full-sample constraints
on parameter categories (mean;

; stochastic volatility parameters; jump parameters) indicates that

between 71% and 86% of the subsample improvement in log likelihood comes from using
subsample stochastic volatility parameters. Between 8% and 22% of the change in log likelihood
comes from using subsample jump parameters, depending on whether stochastic volatility or jump
parameters are relaxed first.

The 1957-2006 subsample estimates for the YY model are more heavily affected by the 1987
crash than are the full-sample estimates. The parameter G approaches its lower bound of zero,
implying that the lower tail density is approaching the time-changed version of the infinite-variance
log-stable distribution. Correspondingly, the subsample estimate of unconditional variance
=

becomes substantially meaningless, and cannot be compared with estimates from other

models or other periods. By contrast, the estimates over 1926-57 are strictly finite-variance. Given
strong interactions between stochastic volatility and jump parameters, it is not clear which is more
responsible for the strong rejections of parameter stability across subsamples.

II.5 Autocorrelation filtration
Given that the prior distribution

is assumed

, it can be shown that the

autocorrelation filtration algorithm (30) for Model 1 updates conditional moments via the robust

25
Kalman filtration approach of Masreliez (1975):
(33)

(34)

If

were conditionally normal, the log density would be quadratic in

, and (33) would

be the linear updating of standard Kalman filtration. More generally, the conditionally fat-tailed
properties of

are explicitly recognized in the filtration.15 The partial derivatives of log densities

can be computed numerically by Fourier inversion.

Figure 4 illustrates the autocorrelation filtrations estimated under various models. For model
1, the autocorrelation revision is fairly similar to standard Kalman filtration for observations within
a ±2% range – which captures most observations, given a unconditional daily standard deviation
around 1%. However, the optimal filtration for fat-tailed distributions is to downweight the
information from returns larger than 2% in magnitude. The exceptions are the stochastic volatility
(SV) and Carr-Wu log-stable (LS) specifications.

Those specifications do not particularly

downweight outliers occurring in non-fat tails: in both tails for SV, in the upper tail for LS.

The autocorrelation filtration under Model 2 is different. Since
that model, large observations of
of

are attributable either to large values of

), or to large values of the Lévy shocks captured by

in
(small values

. The resulting filtration illustrated in

the lower panels of Figure 2 is consequently sensitive to medium-size movements in a fashion
substantially different from the Model 1 specifications.

Figure 5 presents filtered estimates of the daily autocorrelation from the YY model. The
most striking result is the extraordinarily pronounced increase in autocorrelation estimates from
1941 - 1971, with a peak of 35% reached in June 1971. Estimates from other models give

15

See Schick and Mitter (1994) for a literature review of robust Kalman filtration.

26
comparable results, as do crude sample autocorrelation estimates using a 1- or 2-year moving
window.16 After 1971, autocorrelation estimates fell steadily, and became insignificantly different
from zero after 2002. This broad pattern is observed both for Models 1 and 2, although the precise
estimates diverge given the different filtration methodologies. Filtered autocorrelation estimates
appear inversely related to measures of annual stock turnover computed by French (2008), attaining
values closer to zero in the high-turnover periods before 1933 and after 1982. This is consistent with
the standard stale-price explanation of autocorrelation in stock index returns.

Figure 5 also indicates that the estimates of daily autocorrelation are virtually nonstationary,
indicating that fitting ARMA processes with time-invariant parameters to stock market excess
returns is fundamentally pointless. The conditional standard deviation asymptotes at about 4½%,
implying a 95% confidence interval of ±9% for the autocorrelation estimates.

II.6 Volatility filtration
When returns follow an autocorrelated process with i.i.d. shocks of the form
(35)
there are various ways of measuring variance:
Conditional (or residual) variance:

Unconditional variance of returns:

Conditional permanent variance:

where L is the lag operator. These measure of variance are also approximately relevant in the above
models with stochastic conditional volatility and slow-moving autocorrelation. The

values

in Table 3A are estimates of the average level of residual variance for model 1, but estimates of
average permanent variance for model 2; hence the higher estimates for the latter. Furthermore, the

16

See LeBaron (1992, Figure 1) for annual estimates of the daily autocorrelation of S&P
composite index returns over 1928-1990.

27
ratio of return variance to permanent variance is
than 1 for

, which is less

and is monotonically decreasing in

. If permanent conditional variance is

stationary and autocorrelation evolves independently of permanent variance, as is assumed in model
2, periods of high autocorrelation will generate periods of low observed variance of returns – a
property consistent with the inverse relationship between annual estimates of daily autocorrelation
and volatility over 1928-1990 reported in LeBaron (1992).

The left panel of Figure 6 illustrates how the estimated conditional volatility

is

updated for the various specifications under model 1. The conditional volatility revisions use
median parameter values
a conditional mean

for the prior gamma distribution of
that is close to the

, implying

median value observed for

estimates from the YY model.17 For comparability with GARCH analyses such as Hentschel (1995),
Figure 4 shows the “news impact curve,” or revision in conditional volatility estimates upon
observing a given excess return, using the methodology of Bates (2006, pp.931-2).

All news impact curves are tilted, with negative returns having a larger impact on volatility
assessments than positive returns. This reflects the leverage effect, or estimated negative correlation
between asset returns and volatility shocks. All specifications process the information in smallmagnitude asset returns similarly. Furthermore, almost all specifications truncate the information
from returns larger than 3 standard deviations. This was also found in Bates (2006, Figure 1) for
the SVJ1 model, indicating such truncation appears to be generally optimal for arbitrary fat-tailed
Lévy processes. The SV and LS exceptions support this rule. The LS model has a fat lower tail but
not an especially fat upper tail, and truncates the volatility impact of large negative returns but not
of large positive returns. The fact that volatility revisions are not monotonic in the magnitude of
asset returns is perhaps the greatest divergence of these models from GARCH models, which almost
invariably specify a monotonic relationship.18 However, since moves in excess of ±3 standard

17

As
estimates have substantial positive skewness, the median is substantially below the
mean estimate of
reported in Table 3A.
18

An exception is Maheu and McCurdy (2004), who put a jump filter sensitive to outliers into
a GARCH model. They find that the sensitivity of variance updating to the latest squared return
should be reduced for outliers, for both stock and stock index returns.

28
deviations are rare, all specifications will generate similar volatility estimates most of the time. The
volatility filtrations for model 2 shown in the right panel of Figure 6 for median parameters
= (.00385, 6.01) are qualitatively similar to those for model 1.

Figure 7 presents the filtered estimates of conditional annualized permanent volatility over
1926-2006 from the YY model 2, as well as the associated conditional standard deviation.19
Volatility estimates from other models (except SV and LS) are similar – as, indeed, is to be expected
from the similar volatility updating rules in Figure 4. The conditional standard deviation is about
2.8%, indicating a 95% confidence interval of roughly ±5½% in the annualized volatility estimates.
Because of the 81-year time scale, the graph actually shows the longer-term volatility dynamics not
captured by the model, as opposed to the intra-year volatility mean reversion with 2-month half-life
that is captured by the model. Most striking is, of course, the turbulent market conditions of the
1930's, unmatched by any comparable volatility in the post-1945 era. The graph indicates the 1factor stochastic variance model is too simple, and suggests that multifactor specifications of
variance evolution are worth exploring.20

The inset to Figure 5 compares adjusted filtered volatility estimates

over

1987-89 with realized volatility estimates computed daily from intradaily 15-minute log-differenced
S&P 500 futures prices. (Open-to-close futures returns were 86.55% as volatile as close-to-close
futures returns over 1982-2001.) The inset shows that the AML filtration methodology using daily
data generally tracks realized intradaily volatility quite closely. The filtered estimates do not capture
major realized volatility spikes – especially over October 19-28, 1987. The models estimated in this
paper interpret such spikes and the accompanying large daily stock market movements as stock
market jumps. However, the clustering of high intradaily volatility values is important time series

19

“Annualized” volatility refers to the choice of units. Since time is measured in years,
is variance per year, while the daily volatility estimate for a Wednesday return with an estimated
length of
years (from Table 2) is approximately
. Since variance mean-reverts
with an estimated half-life of roughly 2 months, it is not appropriate to interpret Figure 5 as showing
the volatility estimate for a 1-year investment horizon.
20

The inadequacies of AR(1) representations of conditional variance are already well-known
in volatility research, and have motivated research into long-memory processes.

29
evidence against the models’ diffusive-volatility assumption, and supports an alternate volatilityjump specification.

Filtered volatility estimates do appear sensitive to the data interval used in estimation, via
the underlying parameter estimates. For instance, the subsample SVJ1 estimates in Table 4 yield
filtered annualized

’s that are 1.86% higher on average over 1926-57 than the full-sample

estimates, and 1.29% lower over 1957-2006. A significant underlying factor is the estimate of
unconditional volatility

in Table 4, which is higher in the first than in the second subsample,

and which significantly influences volatility filtration. A similar influence of unconditional variance
estimates upon conditional volatility estimates is observed in GARCH models.21

III. Option pricing implications
Do these alternative models imply different option prices? Exploring this issue requires identifying
the appropriate pricing of equity, jump, and stochastic volatility risks. Furthermore, the presence
of substantial and stochastic autocorrelation raises issues not previously considered when pricing
options. In particular, the observed stock index level underlying option prices can be stale, while
the relevant volatility measure over the option’s lifetime is also affected. The variance of the sum
of future stock market returns is not the sum of the variances when returns are autocorrelated.

To examine these issues, I will focus upon Model 2, with its interpretation in equations (23) (24) of

as the permanent shock to the log stock market level. Furthermore, I will use the myopic

power utility pricing kernel of Bates (2006) to price the various risks:
(36)

This pricing kernel constrains both the equity premium estimated under the objective time
series model, and the transformations of those estimates into the risk-neutral process appropriate for

21

See, e.g., Andersen et al (2005, p.7), who note that GARCH(1,1) models diverge from the
RiskMetrics approach in taking into account mean reversion of conditional variance towards the
unconditional variance. This implies that GARCH conditional volatility estimates are affected by
sample-specific estimates of the unconditional variance.

30
pricing options. In particular, the instantaneous equity premium is
(37)
which implies

(38)

where

is the fraction of variance attributable to an orthogonal diffusion term. The

approximation follows from first-order Taylor expansions, and from the fact that jumps account for
a fraction

of overall variance

. The equity premium (37) is well-defined for the

SVJ1 and SVJ2 models. For the CGMY models, the restriction

is required for a finite equity

premium; the intensity of downward jumps must fall off faster than an investor’s risk aversion to
.22

such jumps. The log-stable process is inconsistent with a finite equity premium for

The change of measure from objective to risk-neutral jump intensities takes the form
(39)
under a myopic power utility pricing kernel. This has assorted implications for parameter
transformations that depend upon the precise specification of the Lévy density

. For the SVJ

models, as discussed in Bates (2006), this modified jump intensity shifts the mean jump size
an amount

, while leaving the jump standard deviation

unchanged. For the CGMY model,

the risk adjustment replaces the downward and upward exponential dampening parameters
M by

and

by

and

, respectively, while leaving the C and Y parameters unchanged.23 These

risk adjustments alter the

and

functions in equation (14). Table 5 summarizes the

parameter transformations under the various models.

22

Carr and Wu (2003) specify a log-stable process for the risk-neutral process underlying
option prices. This can be generated from a CGMY process for the actual process with only
downward jumps, and with
.
23

Wu (2006) discusses this transformation.

31
The key risk aversion parameter R used for change of probability measure was estimated by
imposing the equity premium restrictions (37), and re-estimating all times series models. The
additional parameter restriction

was also imposed upon all CGMY models, and was binding

for the YY model.24 Parameter estimates reported in Table 6 changed little relative to those in Table
3B, while risk aversion was estimated at roughly 2.5 for all models. Furthermore, the restriction of
a purely variance-sensitive equity premium (

) was not rejected for any model.

I address the potential impact of autocorrelations upon option prices by examining prices of
options on S&P 500 futures. I assume that stock index futures prices respond instantaneously and
fully to the arrival of news, whereas lack of trading in the underlying stocks delays the incorporation
of that news into the reported S&P 500 stock index levels. Furthermore, I assume that index
arbitrageurs effectively eliminate any stale prices in the component stocks on days when futures
contracts expire, so that stale stock prices do not affect the cash settlement of stock index futures.
MacKinlay and Ramaswamy (1988) provide evidence supportive of both assumptions.

These assumptions have the following implications under Model 2 (equations 23 and 24):
1. the observed futures price

underlying options on S&P 500 futures is not stale;

2. log futures price innovations equal the intradaily innovations

of equation (13):
(40)

Consequently, European options on stock index futures can be priced directly using a risk-neutral
version of (40) – which is affine, simplifying option evaluation considerably. Furthermore, option
prices do not depend upon

, except indirectly through the impact of autocorrelation filtration upon

the filtration of latent permanent variance

. Following Bates (2006), European call prices on an

S&P 500 futures contract can be priced as

(41)

24

Wu (2006) proposes an alternate pricing kernel with negative risk aversion for downside
risk, thereby automatically imposing
.

32
where

and

are risk-neutral variants25 of those in equations (17)-(21);

is the effective maturity of the option, given individual days’ length from Table 2;
is the maturity associated with the continuously compounded Treasury bill yield
, given

calendar days until option maturity; and
is the filtered cumulant generating function of

what is known about

given past data

that summarizes

.

The associated implicit standard deviations (ISD’s) for standardized maturity

can then be

computed using the Black (1976) formula for European options on futures.

The ISD’s from the various models are graphed in Figure 8, and are compared with observed
ISD’s computed from settlement prices for American options on S&P 500 futures with non-zero
trading volume on December 29, 2006. Figure 8 also shows 95% confidence intervals, computed
as in Bates (2006) based on parameter uncertainty alone, and on parameter and state uncertainty
associated with

estimation. All models, including the SV model, generate virtually identical

option prices at end-2006 over a range of ±2 standard deviations – a range that contains the most
actively traded options. The estimated level of the ATM ISD was roughly the same across all
specifications, reflecting an absence of recent major outliers that would induce divergences in
estimated volatility from different specifications . The tilt of the volatility smirk for near-the-money
options appears to be driven primarily by the “leverage effect,” or correlation between shocks to
variance and stock market returns. Only for deep out-of-the-money put options do the divergences
in estimated tail properties generate substantially different ISD patterns across models.26
Furthermore, those divergences across models generally decrease at longer maturities, as the impact
of jumps falls in importance relative to the projected dynamics of stochastic volatility. The maturity
profile of the YY model is the exception, reflecting the fact that the risk-neutral distribution is
almost infinite-variance.

25

The parameters
are replaced by
, while the function
is replaced by a risk-adjusted function
given in Table 5 for specific models that captures the
risk-adjusted jump intensities from (39).
26

These results differ from Bates (2000, Table 3), who reports substantial divergences
between the SV and SVJ models based substantially upon implicit parameter estimation. It would
appear that implicit parameter estimates are strongly affected by the prices of deep OTM options.

33
Figure 9 chronicles estimated and observed at-the-money ISD’s over 1983-2006 for the
short-term options with maturities of 14 days or more. The overall evolution is broadly comparable
to the estimates in Bates (2006, Figure 7). However, the ISD estimates are substantially closer to
observed ISD’s, with average values over 1988-2006 of 15.8% and 16.8%, respectively. The results
differ from the larger divergences in Bates (2006), for two reasons. First, the earlier study used
Anderson, Benzoni and Lund’s (2002) data, who prefiltered S&P returns over 1953-1996 by
estimating an MA(1), and then rescaled estimated residuals to match the mean and variance of the
original data. Prefiltration removes the autocorrelation structure of the data, and consequently
underestimates the average level

of permanent variance relevant for pricing options. Re-

estimating SVJ2 Model 2 on the raw S&P returns underlying the Anderson et al data over 1953-96
raises

estimates from

to

.

Second, as shown above in Table 6,
the volatile 1930's are included:
over 1957-2006. A higher

estimates are substantially higher when data from

over 1926-2006 for the SVJ2 model, as opposed to
estimate affects ISD estimates at all maturities, through its impact

on the filtration algorithm for estimating spot variance

as well as through its impact on forecasts

of future variance. For instance, some resulting filtered measures of risk-neutral volatility over
1988-2006 using the two sets of SVJ2 parameter estimates are:
Average values over 1988-2006
1-month

1-month

difference
in ISD’s

16.8%
16.8%

1.0%
2.4%

Estimation
1926-2006
1957-2006

18.7%
16.3%

15.9%
14.4%

15.8%
14.4%

difference

2.4%

1.5%

1.4%

Nevertheless, the broad assessment of previous studies appears unchanged. Observed ISD’s
from options on index futures do appear higher on average over the post-1987 period than is justified
by risk-adjusted valuations based upon time series analysis, even when volatility assessments
include data from the 1930's.

34
IV. Summary and Conclusions
This paper provides time series estimates of the time-changed CGMY (2003) Lévy process, and
compares them to the time-changed finite-activity jump-diffusions previously considered by Bates
(2006). Overall, while it is important to use adequately fat-tailed distributions when filtering
volatility and other latent variables, it does not seem especially important which fat-tailed
distribution one uses.

Estimates of the volatility process and realizations are substantially

unchanged across most specifications, while the option pricing implications are virtually identical
for all but the deepest out-of-the-money options. The exceptions are Heston’s (1993) stochastic
volatility model, which underestimates upper and lower tail risk, and the log-stable model of Carr
and Wu (2003), which underestimates upper tail risk. This underestimation of tail risk makes
volatility estimates excessively sensitive to outliers, and also affects estimates of the volatility
process. Conditional upon similar volatility estimates, however, even the Heston model fits option
prices similarly to the fat-tailed distributions for all but deep OTM options. For these stochastic
volatility/stochastic intensity models, the tilt of the volatility smirk for near-the-money options ( 2
standard deviations) appears primarily driven by the “leverage” effect.

The paper also documents some structural shifts over time in the data generating process.
Most striking is the apparently nonstationary evolution of the first-order autocorrelation of daily
stock market returns, which rose from near-zero in the 1930's to 35% in 1971, before drifting down
again to near-zero values after 1987. Autocorrelation estimates are inversely related to stock
turnover, and are of considerable importance when assessing stock market volatility. The paper
develops methods of dealing with time-varying autocorrelation, by treating it as an additional latent
state variable to be filtered from observed data. Longer-term trends in volatility are also apparent
in the filtered estimates, suggesting a need for multifactor models of conditional variance.

35
References
Andersen, Torben G., Luca Benzoni, and Jesper Lund (2002). “An Empirical Investigation of
Continuous-Time Equity Return Models.” Journal of Finance 57, 1239-1284.
Andersen, Torben G., Tim Bollerslev, Peter F. Christoffersen, and Francis X. Diebold (2005).
“Practical Volatility and Correlation Modeling for Financial Market Risk Management.” NBER
working paper 11069, January.
Bakshi, Gurdip and Dilip B. Madan (2000). “Spanning and Derivative-Security Valuation.” Journal
of Financial Economics 55, 205-238.
Barclay, Michael J., Robert H. Litzenberger, and Jerold B. Warner (1990). “Private Information,
Trading Volume, and Stock-Return Variances.” Review of Financial Studies 3, 233-254.
Bates, David S. (2000). “Post-‘87 Crash Fears in the S&P 500 Futures Option Market.” Journal
of Econometrics 94, 181-238.
Bates, David S. (2006). “Maximum Likelihood Estimation of Latent Affine Processes.” Review of
Financial Studies 19, 909-965.
Bertoin, Jean (1996). Lévy Processes, Cambridge: Cambridge University Press.
Black, Fischer (1976). “The Pricing of Commodity Contracts.” Journal of Financial Economics
3, 167-179.
Broadie, Mark N., Mikhail Chernov, and Michael Johannes (2006). “Understanding Index Option
Returns,” Columbia University working paper.
Carr, Peter, Hélyette Geman, Dilip B. Madan, and Marc Yor (2002). “The Fine Structure of Asset
Returns: An Empirical Investigation.” Journal of Business 75, 305-332.
Carr, Peter, Hélyette Geman, Dilip B. Madan, and Marc Yor (2003). “Stochastic Volatility for Lévy
Processes.” Mathematical Finance 13, 345-382.
Carr, Peter and Liuren Wu (2003). “The Finite Moment Log Stable Process and Option Pricing.”
Journal of Finance 58, 753-777.
Carr, Peter and Liuren Wu (2004). “Time-changed Lévy Processes and Option Pricing.” Journal
of Financial Economics 71, 113-141.
Chernov, Mikhail, A. Ronald Gallant, Eric Ghysels, and George Tauchen (2003). “Alternative
Models for Stock Price Dynamics. Journal of Econometrics 116, 225-257.
Clark, Peter K. (1973). “A Subordinated Stochastic Process Model with Finite Variance for
Speculative Prices.” Econometrica 41, 135-155.

36
Cox, John C., Stephen A. Ross, and Mark Rubinstein (1979). “Option Pricing: A Simplified
Approach.” Journal of Financial Economics 7, 229-263.
Dimson, Elroy (1979). “Risk Measurement When Shares are Subject to Infrequent Trading.”
Journal of Financial Economics 7, 197-226.
Duffie, Darrell, Jun Pan, and Kenneth J. Singleton (2000). “Transform Analysis and Asset Pricing
for Affine Jump-Diffusions.” Econometrica 68, 1343-1376.
Eberlein, Ernst, Ulrich Keller, and Karsten Prause (1998). “New Insights into Smile, Mispricing,
and Value at Risk: The Hyperbolic Model.” Journal of Business 71, 371-405.
Eraker, Bjorn, Michael Johannes, and Nicholas G. Polson (2003). “The Impact of Jumps in
Volatility and Returns.” Journal of Finance 58, 1269-1300.
French, Kenneth R. (2008). “The Cost of Active Investing.” Journal of Finance 63, 1537-1573.
French, Kenneth R. and Richard Roll (1986). “Stock Return Variances: The Arrival of Information
and the Reaction of Traders.” Journal of Financial Economics 17, 5-26.
Gallant, A. Ronald, Peter E. Rossi, and George Tauchen (1992). “Stock Prices and Volume.”
Review of Financial Studies 5, 199-242.
Hentschel, Ludger (1995). “All in the Family: Nesting Symmetric and Asymmetric GARCH
Models.” Journal of Financial Economics 39, 71-104.
Heston, Steve L. (1993). “A Closed-Form Solution for Options with Stochastic Volatility with
Applications to Bond and Currency Options.” Review of Financial Studies 6, 327-344.
Jukivuolle, Esa (1995). “Measuring True Stock Index Value in the Presence of Infrequent Trading.”
Journal of Financial and Quantitative Analysis 30, 455-464.
Kou, Steve (2002). “A Jump Diffusion Model for Option Pricing.” Management Science 48,
1086-1101.
LeBaron, Blake D. (1992). “Some Relations between Volatility and Serial Correlations in Stock
Returns.” Journal of Business 65, 199-219.
Li, Haitao, Martin T. Wells, and Cindy L. Yu (2008). “A Bayesian Analysis of Return Dynamics
with Stochastic Volatility and Lévy Jumps.” Review of Financial Studies 21, 2345-2378.
Lo, Andrew W. and A. Craig MacKinlay (1988). “Stock Market Prices Do Not Follow Random
Walks: Evidence from a New Specification Test.” Review of Financial Studies 1, 41-66.
MacKinlay, A. Craig and Krishna Ramaswamy (1988). “Index-Futures Arbitrage and the Behavior
of Stock Index Futures Prices.” Review of Financial Studies 1, 137-158.

37
Madan, Dilip B. and Eugene Seneta (1990). “The Variance Gamma (V.G.) Model for Share Market
Returns.” Journal of Business 63, 511-525.
Maheu, John M. And Thomas H. McCurdy (2004). “News Arrival, Jump Dynamics and Volatility
Components for Individual Stock Returns.” Journal of Finance 59, 755-793.
Mandelbrot, Benoit B. (1963). “The Variation of Certain Speculative Prices.” Journal of Business
36, 394-419.
Mandelbrot, Benoit B. and Richard L. Hudson (2004). The (mis)Behavior of Markets: A Fractal
View of Risk, Ruin, and Reward. New York: Basic Books.
Masreliez, C. J. (1975). “Approximate Non-Gaussian Filtering with Linear State and Observation
Relations.” IEEE Transactions on Automatic Control 20, 107-110.
Merton, Robert C. (1976). “Option Pricing When Underlying Stock Returns are Discontinuous.”
Journal of Financial Economics 3, 125-144.
SBBI Yearbook, 2006. Chicago: R. G. Ibbotson Associates.
Schick, Irvin C. And Sanjoy K. Mitter (1994). “Robust Recursive Estimation in the Presence of
Heavy-tailed Observation Noise.” The Annals of Statistics 22, 1045-1080.
Schwert, G. William (1990). “Indexes of U.S. Stock Prices from 1802 to 1987.” Journal of Business
63, 399-426.
Wu, Liuren (2006). “Dampened Power Law: Reconciling the Tail Behavior of Financial Security
Returns.” Journal of Business 79, 1445-1473.

38
Appendix A. Filtration under Model 2
From equation (26), the cumulant generating function (CGF) for future
conditional upon knowing

is
(A.1)

The filtered CGF conditional upon only observing past data

can be computed by integrating over

the independent conditional distributions of the latent variables

:

(A.2)

where

is the gamma conditional CGF for latent

of variables

. Under the change

, and under the assumption that the scaling term
, the Fourier inversion used in evaluating

from (A.2) becomes

(A.3)

where

denotes the real component of complex-valued c, and the 1/x term in the integrand

reflects the Jacobean from the change of variables.

It is convenient to use the two-parameter inverse Gaussian distribution to approximate
:
(A.4)

where

and

are t-dependent parameters that summarize what is

known about x (and about

) at time t. Under this specification, the inner integration inside (A.3)

can be replaced by the analytic function

39

(A.5)

for

.

Consequently, evaluating (A.3) involves only univariate

numerical integration.1

Similar univariate integrations are used for filtering
The noncentral posterior moments of

and

conditional upon observing

.

are given by

(A.6)
where the derivatives with respect to
specifications for

and

inside the integrand can be easily evaluated from the

in equations (17) - (18) . The posterior moments of

can be

computed by taking partials of (A.2) with respect to , and then again using change of variables to
reduce the Fourier inversion to a univariate integration. The resulting posterior mean and variance
of

are

(A.7)

(A.8)

where

1

A more “natural” choice would be to represent x by a beta distribution over the range [0, 2].
That would constrain
, and results in an
term that involves the confluent
hypergeometric U-function. However, I could not find a method for evaluating that function that
was fast, accurate, and robust to all parameter values.

40

(A.9)
and

(A.10)
Finally, the conditional distribution function

that is used in QQ plots

takes the form

(A.11)

Table 1
Standardized cumulant exponents (with unitary variance) for various compensated Lévy specifications
diffusion:
Normally distributed jumps:

CGMY jump process:
with

such that

General specification:

Weights
Parameter
restrictions

Model
SV

1

SVJ1

SVJ2

DEXP
VG
Y

1

YY

1

LS

1

YY_D
For SVJ1 and SVJ2, the fraction

of variance attributable to jumps is

and

, respectively.

Table 2: Effective length of a business day, relative to 1-day Wednesday returns: 1926-2006. Estimates from YY model. Estimates from
other models are almost identical.
Model 1
#days

Description

NOBS

Model 2

estimate

std. error

estimate

std. error
(.03)

1
1
1
1
1

Monday close 6 Tuesday close
Tuesday close 6 Wednesday close
Wednesday 6 Thursday
Thursday 6 Friday
Friday 6 Saturday (1926-52)

3831
4037
3998
3924
1141

1.02
1
.94
.93
.43

(.04)
(.03)
(.03)
(.02)

1.03
1
.94
.92
.44

2
2
2

Saturday close 6 Monday close (1926-52)
Weekday holiday
Wednesday exchange holiday in 1968

1120
341
22

1.05
1.25
.73

(.05)
(.11)
(.33)

1.07
1.26
.81

(.05)
(.10)
(.35)

3
4
5

Weekend and/or holidaya
Holiday weekend
Holiday weekend

2755
343
6

1.10
1.58
1.31

(.04)
(.14)
(1.00)

1.10
1.56
1.25

(.04)
(.13)
(.93)

(5.6)

260.3

(.03)
(.03)
(.02)

21518
Annualization factor: Wednesday 6
yearly
a

Includes one weekday holiday (August 14 - 17, 1945)

259.8

(5.5)

Table 3A: Estimates of parameters affecting the conditional means and volatilities. Data: daily CRSP value-weighted excess returns, 19262006. See equations (6) - (10), (13), and (25) for definitions of parameters. Models with
combine Lévy jump processes with an additional
independent diffusion, with variance proportions
, respectively. Standard errors are in parentheses.
Model 1:
Conditional mean

Stochastic volatility

Model
HL
(mths)

ln L
SV
SVJ1
SVJ2
DEXP
VG
Y
YY
YY_D
LS

74,940.85
75,043.90
75,048.49
75,047.33
75,049.09
75,049.63
75,052.56
75,052.81
75,007.86

.013 (.015)
.042 (.015)
.042 (.003)
.043 (.015)
.043 (.015)
.042 (.015)
.041 (.015)
.042 (.015)
.018 (.015)

2.16 (.90)
.91 (.91)
.87 (.76)
.87 (.90)
.92 (.91)
.90 (.92)
.87 (.92)
.93 (.91)
1.50 (.73)

.029 (.007)
.030 (.006)
.030 (.007)
.031 (.007)
.030 (.006)
.030 (.006)
.030 (.006)
.030 (.006)
.031 (.007)

.153 (.004)
.155 (.005)
.155 (.007)
.155 (.005)
.155 (.005)
.156 (.009)
.158 (.009)
.156 (.006)
.171 (.006)

5.83 (.44)
4.39 (.40)
4.34 (.37)
4.23 (.38)
4.22 (.39)
3.89 (.38)
4.00 (.38)
3.99 (.38)
4.60 (.40)

.452 (.010)
.374 (.011)
.371 (.015)
.368 (.012)
.366 (.012)
.351 (.020)
.360 (.019)
.355 (.013)
.431 (.015)

.625 (.018)
-.641 (.020)
-.642 (.018)
-.587 (.020)
-.586 (.020)
-.576 (.032)
-.571 (.031)
-.579 (.021)
-.541 (.020)

1.4 (.1)
1.9 (.2)
1.9 (.2)
2.0 (.2)
2.0 (.2)
2.1 (.2)
2.1 (.2)
2.1 (.2)
1.8 (.2)

Model 2:
SV
SVJ1
SVJ2
DEXP
VG
Y
YY
YY_D
LS

74,999.87
75,092.10
75,096.68
75,094.20
75,094.70
75,093.68
75,097.20
75,097.49
75,045.48

-.014 (.020)
.033 (.020)
.037 (.020)
.034 (.020)
.034 (.020)
.036 (.021)
.033 (.020)
.035 (.020)
.053 (.019)

3.04 (.90)
1.69 (1.04)
1.25 (.89)
1.44 (.90)
1.42 (.90)
1.35 (.90)
1.44 (.90)
1.36 (.90)
1.50 (.76)

.043 (.005)
.036 (.005)
.036 (.005)
.036 (.005)
.037 (.005)
.036 (.005)
.036 (.005)
.036 (.005)
.031 (.003)

.170 (.004)
.171 (.004)
.172 (.004)
.171 (.004)
.171 (.004)
.172 (.007)
.172 (.006)
.172 (.005)
.174 (.005)

8.01 (.57)
5.80 (.49)
5.71 (.49)
5.67 (.49)
5.56 (.48)
5.18 (.46)
5.23 (.47)
5.23 (.47)
4.68 (.41)

.562 (.015)
.457 (.015)
.456 (.015)
.452 (.015)
.447 (.016)
.432 (.021)
.437 (.018)
.436 (.016)
.436 (.015)

-.658 (.017)
-.674 (.018)
-.673 (.018)
-.625 (.018)
-.623 (.018)
-.613 (.027)
-.613 (.022)
-.616 (.020)
-.576 (.019)

1.0 (.1)
1.4 (.1)
1.4 (.1)
1.5 (.1)
1.6 (.1)
1.6 (.1)
1.6 (.1)
1.6 (.1)
1.8 (.2)

Table 3B: Estimates of jump parameters. Standard errors in parentheses.
Model 1:
CGMY parameters
Model

G

SVJ1
SVJ2

.150 (.017)
.156 (.054)

DEXP
VG
Y
YY
YY_D
LS

.253 (.027)
.272 (.030)
1
1
.436
1

Merton parameters

M
142.7 (22.7)
162.9 (30.9)
0.5 (1.6)

.49 (.01)
.52 (.07)
.59 (.06)
.88 (.03)
.72 (.15)
1

66.1 (6.0)
41.1 (5.4)
7.0 (4.6)
1.6 (4.5)
6.9 (9.0)
.001

45.4 ( 8.4)
31.6 (9.1)
2.3 (7.3)
40.1 (31.3)
49.2 (34.9)

.000 (.002)
.000 (.002)
-.189 (.094)

.032 (.002)
.029 (.002)
.005 (.189)

-1
0
1.87 (.03)
1.94 (.01)
-.24 (1.36)
1.71 (.35)
-.72 (1.57)
1.96 (.01)

Model 2:
SVJ1
SVJ2

.133 (.015)
.140 (.015)

DEXP
VG
Y
YY
YY_D
LS

.236 (.026)
.257 (.030)
1
1
.380 (.158)
1

114.1 (19.2)
126.4 (23.7)
0.41 (.04)
.53 (.06)
.54 (.07)
.59 (.05)
.89 (.03)
.90 (.30)
1

55.4 (5.4)
41.1 (4.9)
6.8 (4.2)
2.6 (4.1)
8.1 (8.7)
.001

50.0 (4.8)
31.6 (10.7)
3.2 (8.2)
71.1 (57.8)
51.8 (51.7)

-1
0
1.87 (.03)
1.935 (.014) -1.96 (2.62)
1.619 (.403) -1.08 (2.36)
1.965 (.006)

-.001 (.003)
.000 (.002)
-.198 (.022)

.034 (.002)
.031 (.002)
.010 (.046)

Table 4: Subsample estimates for Model 2. Estimates “w/o Oct ’87" exclude daily data observed in October 1987, but include the full month’s return
of -20.6%. Split-sample estimates involve different parameter values before/after March 5, 1957, apart from time dummies. Standard errors are in
parentheses.
Conditional mean
Model

Stochastic volatility

Period
HL
(mths)

ln L

SVJ1

full

75,092.10

.033 (.020)

1.69 (1.04)

.036 (.005)

.171 (.004)

5.80 (.49)

.457 (.015)

-.674 (.018)

1.4 (.1)

SVJ1

1926 - 1957
1957 - 2006

75,183.99

.051 (.034)
.003 (.027)

1.35 (1.38)
2.90 (1.61)

.050 (.009)
.027 (.005)

.202 (.007)
.149 (.005)

8.82 (1.03)
4.93 (0.60)

.678 (.035)
.314 (.015)

-.661 (.027)
-.725 (.023)

0.9 (.1)
1.7 (.2)

YY

full

75,097.20

.033 (.020)

1.44 (.90)

.036 (.005)

.172 (.006)

5.23 (.47)

.437 (.018)

-.613 (.022)

1.6 (.1)

YY

w/o Oct ’87

75,065.01

.036 (.020)

1.37 (.91)

.035 (.005)

.170 (.005)

5.12 (.46)

.427 (.016)

-.620 (.019)

1.6 (.1)

YY

1926 - 1957
1957 - 2006

75,196.14

.056 (.034)
.012 (.027)

1.03 (1.15)
.35 (.67)

.051 (.009)
.025 (.005)

.201 (.008)
.365 (.320)

6.81 (.88)
4.89 (.56)

.657 (.033)
.404 (.026)

-.585 (.026)
-.281 (.247)

1.2 (.2)
1.7 (.2)

CGMY parameters
Model

G

Merton parameters

M

SVJ1

full

.133 (.015)

114.1 (19.2)

SVJ1

1926 - 1956 .167 (.015)
1957 - 2006 .093 (.020)

YY

full

1

.89 (.03)

2.6 (4.1)

71.1 (57.8) 1.94 (.01)

-1.96 (2.6)

YY

w/o Oct ’87

1

.89 (.03)

5.6 (5.3)

72.0 (60.8) 1.93 (.02)

-1.76 (2.6)

YY

1926 - 1957
1957 - 2006

1
1

.86 (.04)
.92 (.15)

20.7 (7.9) 97.8
1.82 (.05)
0.0 (0.0) (107.2)
1.54 (.41)
6.0 (13.4)

-3.1 (4.5)
1.94 (.03)

-.001 (.003) .034 (.002)

216.8 (54.9) .000 (.003) .028 (.003)
49.5 (12.0) -.003 (.007) .043 (.004)

Table 5: Change of parameters (objective versus risk-neutral) under a myopic power utility
pricing kernel
.
Objective

Risk-neutral

Equity premium
0
General jump intensity
Variance process
mean reversion
UC mean
Merton jump process
mean jump size
jump SD
jump intensity

CGMY jump process
G
M

(must be $0)

Table 6: Parameter estimates over 1926-2006 on spliced CRSP/S&P 500 data with constrained equity premium:
LR test of

Conditional mean

,

.

Stochastic volatility

Model

SV
SVJ1
SVJ2
DEXP
VG
Y
YYa

ln L

(p-value)

74,028.53
74,119.26
74,125.33
75,121.73
74,122.51
74,122.19
74,124.33

.383
.265
.507
.247
.206
.185
.212

HL
(mths)
2.49 (.62)
2.44 (.61)
2.43 (.57)
2.44 (.61)
2.50 (.61)
2.42 (.61)
2.38

2.49 (.62)
2.44 (.61)
2.44 (.58)
2.44 (.61)
2.50 (.61)
2.42 (.61)
2.42

.043 (.005)
.038 (.005)
.037 (.005)
.037 (.005)
.037 (.005)
.037 (.005)
.037

.172 (.004)
.173 (.004)
.174 (.004)
.174 (.004)
.174 (.004)
.174 (.006)
.175

7.18 (.48)
5.76 (.43)
5.76 (.42)
5.68 (.43)
5.62 (.43)
5.29 (.41)
5.26

.534 (.014)
.448 (.015)
.449 (.015)
.444 (.015)
.441 (.015)
.427 (.018)
.429

-.649 (.016)
-.678 (.017)
-.679 (.017)
-.632 (.017)
-.631 (.017)
-.623 (.022)
-.621

2.98 (.88)

.026 (.006)

.149 (.006)

3.97 (.44)

.289 (.014)

-.721 .024)

1.2 (.1)
1.4 (.1)
1.4 (.1)
1.5 (.1)
1.5 (.1)
1.6 (.1)
1.6

Estimates on S&P 500 data over 1957-2006
SVJ2

43,707.25

2.97 (.88)

CGMY parameters
Model

G

SVJ1
SVJ2

.126 (.015)
.138 (.021)

DEXP
VG
Y
YYa

.228 (.026)
.247 (.028)
1
1

.55 (.06)
.53 (.07)
.58 (.05)
.90

51.6 (5.0)
35.9 (4.6)
5.4 (4.0)
2.4

2.1 (.2)

Merton parameters

M

53.9 (12.7)
34.4 (11.1)
4.5 (8.7)
64.3

108.7 (18.2)
122.7 (23.1)
0.43 (.38)

-.001 (.003)
.000 (.002)
-.219 (.027)

.034 (.002)
.031 (.002)
.003 (.150)

81.0 (30.1)
.55 (1.32)

.002 (.004)
-.213 (.035)

.030 (.005)
.003 (.020)

-1
0
1.87 (.03)
1.94
-1.29

Estimates on S&P 500 data over 1957-2006
SVJ2

a

.096 (.050)

Parameter constraint

was binding for the YY model; standard errors could not be computed.

Normal Probability Plot SVJ2
Normal Probability Plot SVJ1

0.999
0.997

0.999
0.997

0.99
0.98

0.99
0.98

0.95

0.95

0.90

0.90
0.75
Probability

Probability

0.75
0.50

0.50
0.25

0.25
0.10
0.10

0.05

0.05

0.02
0.01

0.02
0.01

0.003
0.001

0.003
0.001

-6
-6

-5

-4

-3

-2

-1
Data

0

1

2

3

-5

-4

-3

-2

1

2

3

4

1

2

3

4

0.999
0.997

0.999
0.997

0.99
0.98

0.99
0.98

0.95

0.95

0.90

0.90

0.75
Probability

0.75
Probability

0

Normal Probability Plot VG

Normal Probability Plot DEXP

0.50

0.50
0.25

0.25

0.10

0.10

0.05

0.05

0.02
0.01

0.02
0.01

0.003
0.001

0.003
0.001

-6

-5

-4

-3

-2

-1
Data

0

1

2

3

-6

4

-5

-4

-3

-2

-1
Data

0

Normal Probability Plot YY

Normal Probability Plot Y

0.999
0.997

0.999
0.997

0.99
0.98

0.99
0.98

0.95

0.95

0.90

0.90

0.75
Probability

0.75
Probability

-1
Data

4

0.50

0.50
0.25

0.25

0.10

0.10

0.05

0.05

0.02
0.01

0.02
0.01

0.003
0.001

0.003
0.001

-6

-5

-4

-3

-2

-1
Data

0

1

2

3

-6

4

-5

-4

-3

-2

-1
Data

0

1

2

3

4

Normal Probability Plot LS

Figure 1. Normal probability plots for the
normalized returns

0.999
0.997
0.99
0.98

,

0.95
0.90
0.75
Probability

for different specifications under Model 2.
Diagonal line: theoretical quantiles conditional
upon correct specification
+: Empirical quantiles

0.50
0.25
0.10
0.05
0.02
0.01
0.003
0.001

-6

-5

-4

-3

-2

-1
Data

0

1

2

3

4

Figure 2a. Unconditional probability density functions from Model 1 specifications. Databased estimates are from a histogram of residuals (.25% cell width).

1.E+02
1.E+01

SVJ1
SVJ2

1.E+00

DEXP
1.E-01

VG

871019

Y

1.E-02

YY

SVJ2

1.E-03

LS

YY

data

1.E-04
1.E-05
-20%

SVJ1

-15%

LS

-10%

-5%

0%

5%

10%

15%

Figure 2b. Unconditional tail probability estimates. The dotted lines give 95% confidence
intervals, based upon 1000 simulations of the 1926-2006 data set under YY parameter estimates.

1

0.1

SVJ1
SVJ2

0.01

DEXP
VG
Y
YY
LS

0.001

data
2.5%

0.0001

97.5%
0.00001
-20%

-15%

-10%

-5%

0%

5%

10%

15%

Figure 3. Unconditional tail probabilities and tail intensity functions versus
. Log scales on both axes. Data-based estimates from
excess returns’ residuals for 20,004 business days with estimated time horizons of approximately 1 day (±11%). Dotted lines give 95%
confidence intervals, based upon 1000 simulated sample paths under YY parameter estimates.

y<0

y>0

1
0.1

1
0.1

0.01 0.01
data

0.001
0.001

0.0001
0.0001

100.0%

10.0%

1.0%

0.00001
0.00001
0.1%
0.1%

1.0%

10.0%

100.0%

Figure 4: Autocorrelation revision

conditional on observing

, and conditional

on
Model 1

0.8%

0.8%

Kalman

SV

LS

SV
SV
SVJ1
SVJ2
DEXP

0.0%

0.0%

VG
Y
YY
LS
Kalman

-0.8%
-5%

SV
LS
-3%

-1%

1%

3%

SV
-0.8%
-5%

5%

Kalman
-3%

-1%

1%

3%

5%

Model 2

SV

0.0%

0.0%

SVJ1
SVJ2
DEXP
VG
Y
YY
LS

SV
-1.5%
-8%

LS

-4%

0%

SV

4%

8%

SV
-1.5%
-8%
-4%

LS

0%

4%

SV

8%

Figure 5: Autocorrelation estimates and standard errors from YY model, and stocks’ annual
turnover from French (2008).

40%

(Model 2)

30%

250%

200%

20%

150%

(Model 1)
10%

100%

SE (Model 1)
0%

SE (Model 2)

50%

Annual turnover
(right scale)
-10%

0%
26 31 36 41 46 51 56 61 66 71 76 81 86 91 96 01 06

Figure 6: News impact curves for various models
Model 1

Model 2

6%

SV

6%

LS

SV

LS

SV

4%

SV
4%
SV
SVJ1
SVJ2
DEXP

2%

2%

VG
Y
YY
LS

0%

0%
-8

-4

0

4

8

-8

-4

0

4

8

-2%

-2%

Asset return, in SD units

Asset return, in SD units

The graph show the revision in estimated annualized standard deviation
conditional upon observing a standardized return of magnitude

.

100.0%

10.0%
Qvol
filtered

50%
1.0%

0.1%
1/87

40%

5/87

9/87

1/88

5/88

9/88

1/89

5/89

9/89

30%
20%
10%
0%
26

31

36

41

46

51

56

61

66

71

76

81

86

91

96

01

06

Figure 7: Estimates of annualized permanent volatility (YY model 2) and standard errors.
Inset compares daily (open to close) filtered volatility estimates over 1987-89 with the realized
volatilities computed daily from 15-minute log-differenced S&P 500 futures prices, on a log scale.

Figure 8: Estimated and observed ISD’s for options on S&P 500 futures on December 29, 2006. Moneyness is measured in standard deviation units,
given the maturity-specific at-the-money ISD from options. 95% confidence intervals from the SVJ2 model are shown for parameter uncertainty (dark
grey), and parameter and state uncertainty (light grey).

1 SD = 2.3%

YY

1 SD = 5.3%

ln(X/F ), in SD units

1 SD = 3.5%

YY

1 SD = 8.6%

ln(X/F ), in SD units

(left scale)

(right scale)

Figure 9: Estimated and observed at-the-money ISD’s, 1983-2006. Observed ISD’s are from short-term options on S&P 500 futures
with at least 14 days to maturity, while estimated ISD’s are based on the SVJ2 specification’s parameter and volatility estimates. The grey
area is the 95% confidence interval for the difference, given both parameter and state uncertainty.

