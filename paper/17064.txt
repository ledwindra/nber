NBER WORKING PAPER SERIES

RECURSIVE CONTRACTS, LOTTERIES AND WEAKLY CONCAVE PARETO
SETS
Harold L. Cole
Felix Kubler
Working Paper 17064
http://www.nber.org/papers/w17064

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
May 2011

The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2011 by Harold L. Cole and Felix Kubler. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice,
is given to the source.

Recursive Contracts, Lotteries and Weakly Concave Pareto Sets
Harold L. Cole and Felix Kubler
NBER Working Paper No. 17064
May 2011
JEL No. C61,D82
ABSTRACT
Marcet and Marimon (1994, revised 1998, revised 2011) developed a recursive saddle point method
which can be used to solve dynamic contracting problems that include participation, enforcement and
incentive constraints. Their method uses a recursive multiplier to capture implicit prior promises to
the agent(s) that were made in order to satisfy earlier instances of these constraints. As a result, their
method relies on the invertibility of the derivative of the Pareto frontier and cannot be applied to problems
for which this frontier is not strictly concave. In this paper we show how one can extend their method
to a weakly concave Pareto frontier by expanding the state space to include the realizations of an end
of period lottery over the extreme points of a flat region of the Pareto frontier. With this expansion
the basic insight of Marcet and Marimon goes through – one can make the problem recursive in the
Lagrangian multiplier which yields significant computational advantages over the conventional approach
of using utility as the state variable. The case of a weakly concave Pareto frontier arises naturally in
applications where the principal's choice set is not convex but where randomization is possible.

Harold L. Cole
Economics Department
University of Pennsylvania
3718 Locust Walk
160 McNeil Building
Philadelphia, PA 19104
and NBER
colehl@sas.upenn.edu
Felix Kubler
University of Zurich
Plattenstrasse 32
CH-8032 Zurich
Switzerland
and Swiss Financial Institute
fkubler@gmail.com

1

Introduction

Marcet and Marimon (1994, revised 1998, revised 2011), in their still unpublished research
memo, developed a recursive saddle point method which can be used to solve dynamic contracting problems that include participation, enforcement and incentive constraints. Their
method uses a recursive multiplier to capture implicit prior promises to the agent(s) that
were made in order to satisfy earlier (in time) instances of these constraints. Prior to their
work, the standard method to recursively solve dynamic contracting problems treated the
promised utility of the agent(s) as a state variable, and used this state variable to capture
the implicit prior promises. The main advantage of the recursive multiplier is that it allows
the conditional date t problem to be solved without reference to the ex ante date t payo¤ to
the agent(s). In contrast, when one uses ex ante utility as a state variable, there is an overall
ex ante condition as to the allocation of utility across states in t that has to hold, and as a
result the date t actions and date t + 1 continuation utilities must be simultaneously solved
as a block.1
This computational advantage has lead to the recursive multiplier approach being widely
used; to cite a few examples Marcet and Marimon (1992), Kahn, King and Wolman (2003),
Cooley, Marimon and Quadrini (2004) Attanasio and Rios-Rull (2000), Kehoe and Perri
(2002), Aiyagari, Marcet, Sargent and Seppala (2002), Atkeson and Cole (2005), Chien,
Cole and Lustig (2009). Despite these applications, there remain fundamental issues with
respect to the applicability of these methods. Messner and Pavoni (2004) use a simple
example to show that if the Pareto frontier is not strictly concave, then these methods can
yield policy functions which are not only suboptimal but infeasible. This is because Marcet
and Marimon’s method relies on the invertibility of the derivative of the Pareto frontier in
order to map the recursive multiplier into the promised utility level.2 The method must fail
in the presence of ‡at spots on the Pareto frontier. Public randomization, which is commonly
used in environments with nonconvex constraint sets, naturally generates these ‡at spots on
the Pareto frontier.3
In this paper we show how one can deal with a weakly concave Pareto frontier by expanding the state space to include the realizations of an end of period lottery over the extreme
points on the Pareto frontier that share a common slope. The basic idea is as follows. It
can be shown that the value of the recursive programming problem in Marcet-Marimon is
the same as the optimal value of the contracting problem even if there are ‡at spots on the
frontier. The problem is that if for a given value of the multiplier the frontier is ‡at, the
recursive multiplier approach yields a continuum of current actions which solve the Bellman
1

Messner, Pavoni and Sleet (2011) provide an extensive discussion of the various ways in which one can
construct the dual problems to recursive incentive problems and the conditions under which these methods
are valid.
2
Marcet and Marimon’s (2011) revised version of their paper restricts attention to cases in which the
Pareto frontier is strictly concave and hence this criticism does not apply. Subsequent to issuing the …rst
working paper version of this paper, Marimon shared with us a very preliminary draft of an alternative
method of resolving the convexity issue.
3
Lotteries naturally arise in environments with incentive constraints, see for example Prescott and
Townsend (1984 A and B), or discrete choices, see for example Rogerson (1988), and Cole and Prescott
(1997).

2

equation. It is then impossible to pick the correct action that is consistent with previous
periods’incentive constraints. Our approach identi…es those actions in these sets that yield
extreme payo¤s. In the case of one agent and one principal these are the action that yields
the highest payo¤ to the agent and the action that yields the highest payo¤ to the principal,
i.e. the two extreme points of the ‡at spot of the Pareto-frontier. We assume that the
principal has access to a public randomization device and can pick a lottery over these two
extreme points that satis…es last period’s incentive constraints. If in the previous period
the multiplier identi…ed a strictly concave region of the frontier, this resulting policy clearly
yields the optimal value and is feasible. If in the previous period, the multiplier also pointed
to a ‡at spot on the frontier, one again has to identify the extreme points and continue with
this until one is at t = 0, or at a strictly concave region of the frontier.
We discuss the fundamental issue that arises with a Pareto frontier that is not strictly
concave, and the nature of our solution to this problem in terms of a simple example which
we will solve in detail later in the paper. Consider a simple partnership model in which there
is a principal and an agent, and the principal must have the participation of the agent in
order to run a project which produces a pie of size 1 every period that can be split between
the two of them. Each period the principal gets to choose the amount of the pie he eats, a;
and this implies the amount that the agent eats, 1 a: The within period reward functions
for the principal is log(a) and for the agent is log(1 a): Both are expected utility maximizers
and both discount the future at rate :
We will assume that a is bounded between " and 1 "; where " is a small number that
serves to bound the payo¤s of the agent and the principal. We will consider two cases: (i)
the set of actions a is convex and equal to ["; 1 "]; and (ii) the set of actions is not convex.
In the nonconvex case, we will allow for public randomization in order to convexify the set
of payo¤s.
The agent has an initial outside opportunity, and then each period draws an outside
opportunity. These opportunities put an initial ex ante lower bound on his payo¤, which we
call the participation constraint, and a conditional lower bound, which we can an incentive
constraint. The ex ante outside opportunity which goes into the participation constraint is
g2 = 1 1 log(1=5). Assume that in every period there are two states of the world s 2 fl; hg
which are i.i.d. and equi-probable. The conditional opportunity of the agent depends on the
shock, g1 (l) = 1 1 log( ) and g1 (h) = 1 1 log(2=3), where > 0 is a lower bound for a.
The Pareto frontier, V (G); for this environment are the solutions to the optimal contracting problem for each feasible level of ex ante utility for the agent , G, where V is the
principal’s payo¤, and by incentive feasible we mean that the contract satis…es the ex post
incentive constraint in every date and state. Because of possibility of public randomization,
we let (a; s) denote a probability distribution over the possible actions, and implicitly de…ne the Pareto frontier in the following functional equation, which we refer to as the Pareto
problem,
Z
V (G) =

max

(a;s);G(a;s)

E

[log(a) + V (G(a; s))] d (a; s)

a

subject to the ex post incentive constraint (IC)
log(1

a) + G(a; s)

g1 (s) for all a in the support of
3

and each s;

(1)

and the ex ante participation constraint (PC)
Z
E
[log(1 a) + G(a; s)] d (a; s)

G:

a

The solution to the original contracting problem is generated when we set G = g2 .
If the set of possible actions a by the principal is convex, then randomization is degenerate
and one can show that the Pareto frontier is strictly concave. However if the action set is not
convex, then public randomization may be necessary and as a result the Pareto frontier is
weakly concave but not strictly so. It is now easy to explain that in this case the multiplier
approach in Marcet and Marimon need not yield the optimal policy.
We will make the following assumptions about the solution to this problem at G = g2
(i.e. when we impose the participation constraint). Assume that the IC binds in state s = h:
Assume that the PC binds and that its shadow value is : Clearly the IC cannot bind in
state s = l since the bound is the lowest feasible transfer to the agent. Denote the multiplier
on the IC constraint by (s; a):
If the action set is convex there will be a unique action in the support of ; a(s); and
V (G) is strictly concave. Consequently, there is a unique continuation payo¤ G(s): Moreover,
as we show later, at the solution
1
1
=
;
a(l)
1 a(l)
V 0 (G(l)) = ;
1
= ( + (h))
a(h)
1

1
;
a(h)

and
V 0 (G(h)) =

( + (h)):

And, these conditions uniquely determined the solution given the multipliers and (h)
(remember that (l) = 0).
Now consider the Social Planning problem associated with this Pareto problem. Recall
that the Pareto-frontier is described by V (G) and that is the welfare weight for the principal
that is implied by the participation constraint. The social planning problem is then given
by
Z
Z
max Es
[log(a) + V (G(a; s))] d (a; s) +
[log(1 a) + G(a; s)] d (a; s)
(a;s);G(a;s)

a

a

subject to the incentive constraint. In the case in which the action set is convex, and hence
the Pareto frontier is strictly concave, the solution to this problem is unique and will satisfy
the same optimality conditions as the Pareto problem with G = g2 : Hence, the solution to
the social planning problem will correspond to the solution to the planning problem, and
the payo¤ to the agent is uniquely determined by his weight in the social planning problem.
In the case in which the action set is not convex, and hence the Pareto frontier is not
necessarily strictly concave, this correspondence need not hold precisely because the payo¤
to the agent is not uniquely determined by his social planning weight. For example, assume
4

that in the solution to the original problem G(l) lay on a linear portion of the Pareto frontier.
It follows then that the value of the social planning problem doesn’t change if we vary G(l)
along its linear portion since the V and G trade-o¤ is exactly
; which is the weight in the
social planning problem. Or, for another example, assume that in the original problem the
randomization in the low state was not degenerate,and we were randomizing over a1 and a2 :
Then, it must be the case that
log(a1 ) + log(1

a1 ) = log(a2 ) + log(1

a2 );

and any randomization over these two points can be part of a solution to the social planning
problem. Hence, the payo¤ to the agent is not uniquely determined by his weight in the social
planning problem. These examples illustrate that when the Pareto frontier is not strictly
concave, there can be solutions to the social planning problem in which the PC constraint is
violated or is slack. Of course the solution set will also include the solution to the planning
problem.
If we consider the Lagrangian associated with our Social Planning problem,
R
R
[log(1 a) + G(a; s)] d (a; s)
[log(a)
+
V
(G(a;
s))]
d
(a;
s)
+
a
a
R
max E
(a;
s)
[log(1
a)
+
G(a;
s) g1 (s)] d (a; s)
(a;s);G(a;s)
a

these same issues will arise. The Lagrangian for the social planning problem will pick out
the correct [ (a; h); G(a; h)] because of the requirement that the IC hold with equality if
(a; h) > 0. However the PC may not hold with equality in the solution to the either the
Social Planning problem or its associated Lagrangian problem.
A recursive saddlepoint methodology uses the sum of the past multipliers on the PC and
IC constraints in a dynamic optimization problem as the state variable to determine the
relative treatment of the principal and the agent. Now the problem becomes even worse. In
the solution to the social planning problem V 0 (G(a; s)) = [ + (a; s)] : Since both G(a; l)
and G(a; h) could fall on on linear portions of the Pareto frontier, this condition is not
su¢ cient to uniquely determine either of these continuation payo¤s for the agent. Hence it
cannot ensure that either the PC or the IC conditions hold as equalities which they must
in the true solution given our assumptions. Of course the set of possible solutions to this
problem will contain the true solution. However, since the value V (G) + [ + (a; s)] G is the
same for all G such that V 0 (G) = [ + (a; s)] ; the lack of strict concavity of the Pareto
Problem is a problem for the policy functions and not the value of the solution.4
One simple way around this problem is to appropriately randomize over extreme points
in the solution set to the social planning problem. To illustrate how this works, let A(s)
denote the set of fa(s); G(s)g pairs that are solutions to
max log(a(s)) + V (G(s)) + ( + (s)) [log(1

a(s);G(s)
4

a) + G(s)] :

For values of G at which V (G) is not di¤erentiable, we can use the subdi¤erential (the collection of
all subgradients of V at G) and associate G with any value of [ + (a; s)] which is an element in the
subdi¤erential. (Note that the subdi¤erential is convex and this association will be unique since V is weakly
concave).

5

Let
GH (s) =
GL (s) =

max

log(1

a) + G

min

log(1

a) + G;

fa;Gg2A(s)
fa;Gg2A(s)

Let (s) denote the conditional probability of the agent getting the lowest optimal payo¤
and 1
(s) of getting the highest payo¤. In this case conditional expected the payo¤ of
the agent is
(s)GL (s) + (1
(s))GH (s);
and the principal is
(s)V (GL (s)) + (1

(s))V (GH (s)):

By choosing (l) and (h) appropriately, we can ensure that these conditional payo¤s are
such that both the PC and the IC hold as equalities. Under this resolution the state space
tomorrow becomes (s0 ; + (s); (s)); where s0 is tomorrow state, and this is su¢ cient to
generate a correct policy choice.
In the dynamic problem that we will consider below this randomization is with respect to
the selection of the continuation payo¤ and hence can be done at the end of the period. As
a result the state space that we will use will be even simpler since it will be (s0 ; + ; i = L
or H); that is, we will include an indicator of which element of the extreme points in the
solution set will be used.
The rest of the paper is organized as follows. In Section 2 we describe the general
contracting problem. In Section 3 we explain the recursive multiplier approach from Marcet
and Marimon and give a simple example to illustrate that without strict concavity this
approach cannot yield the correct policy. In Section 4 we describe our solution to the
problem and link it to the promised utility approach. Section 5 revisits the example from
the introduction. In Section 6 we argue that more complicated problems can easily be solved
numerically with our method.

2

The general problem

Assume the exogenous shock (st ) follows a …nite Markov chain with transition and support
S = f1; :::; Sg. Each period the principal randomizes over a compact action set A Rn –
to simplify the notation and in anticipation of our results below we assume without loss of
generality that he chooses a simple probability distribution (i.e. a distribution with …nite
support) t : A ! R+ , t 2 . The choice can depend on the history of realized shocks and
realized actions, i.e. t = (ht 1 ; st ) where ht 1 = (s0 ; a0 ; : : : st 1 ; at 1 ). In a slight abuse of
notation we write 1 to denote the space of all (history-dependent) sequences. We denote
the support of t by supp( t ) = fa 2 A : t (a) > 0g. Note that the space of all simple
probability distribution forms a vector space and convexity is well de…ned.
It is useful to denote by t (ht+n ) the conditional probability of history ht+n , given history
t
h and a choice ( t ) 2 1 . We assume that there is a …xed initial probability distribution
6

over the shock in period 0, s0 and E 1 denotes the expectation under this distribution. We
write the expectation of a function f (:) that depends on the realized at+n as
X
t+n
Et (f (at+n )) =
)f (at+n (h)):
t (h
ht+n 2supp(

t

)

We use Et 1;st for the expectation conditional on a history ht 1 and a realized shock st –Es0
denotes the expectation conditional on a realized …rst period shock, s0 .
The physical state, x, realizes in a compact subset of Euclidean space X
Rm and
depends on last periods’realized action, last periods’s state and last period’s realized shock
but not on the current shock (for our approach, it is trivial to also let the state depend on
the current shock, but this adds little to the economics of the model), i.e. the law of motion
for xt is given by xt+1 = (xt ; at ; st ).
There are I agents, i = 1; :::; I. The principal can randomize over the actions, but for
each agent i = 1; :::; I the incentive constraint (ICi ) is assumed to hold conditional on each
realized action. The participation constraint (P Ci ), on the other hand, only has to hold ex
ante, before either shock or actions are realized.
Our contracting problem then can be written in sequence form as

(

max1 E

1

t )2

1
X

t

r(xt ; at ; st ) subject to

xt+1 = (xt ; at ; st )
p(xt ; at ; st ) 0; 8at 2 supp( t )
1
X
n i
g i (xt ; at ; st ) + Et
g (xt+n ; at+n ; st+n )
n=1

E

1

1
X

(2)

t=0

t i

g (xt ; at ; st )

t=0

g2i ; 8i = 1; :::; I

x0 given :

(3)
(4)
g1i ; 8i = 1; :::; I; 8at 2 supp( t )

(5)
(6)
(7)

For consistency, we assume that for all (x; a; s) 2 X A S, (x; a; s) 2 X whenever
p(x; a; s)
0. It is useful to combine constraints (2) and (3) – for a …xed x0 , we write
xt = x(ht 1 ) to denote the state at t resulting from history ht 1 as well as A(ht 1 ; st ) to be
the set of all actions satisfying (3) given x(ht 1 ) and the current shock st . Since the space of
exogenous shocks is assumed to be …nite, we frequently write rs (x; a) instead of r(x; a; s) and
gsi (x; a) instead of g i (x; a; s). To simplify notation we sometimes use g 0 to denote the reward
function of the principal, i.e. g 0 (x; a; s)
r(x; a; s): In general we could let the outside
option in the (IC) constraint, g1i depend on the shock s. It will be easy to see that this does
not change our analysis, we just drop the dependence to simplify notation.
We make the following assumptions on the fundamentals.
Assumption 1
1. The reward functions rs : X A ! R and the constraint functions gsi : X A ! R
are continuous and bounded (both from below and above) for all shocks s 2 S.
7

2. There is discounting, i.e.

2 (0; 1).

3. There is an > 0 such that for each initial condition (x0 ) 2 X there exists an action
( t ) such that for all agents i = 1; :::; I,
#
"
1
X
n i
inf
g i (xt ; a; st ) + Et
g (xt+n ; at+n ; st+n ) g1i >
t 1
h

;st ;at 2supp(

t)

n=1

and
E

1

1
X

t i

g (xt ; at ; st )

g2i > :

t=0

Assumption 1.1 is perhaps stronger than needed. Assumption 1.2 and the Slater condition
1.3 are standard. Condition 1.3 can of course be rewritten in terms of non-randomized
actions.

3

The social planning problem

In this section, we disregard the participation constraint (6) and focus on a social planner’s
problem for given welfare weights. Without the participation constraint the planner’s problem can be divided into S subproblems, one for each initial shock. To solve the original
problem the welfare weight needs to be chosen to ensure that this constraint holds, i.e.
becomes the initial multiplier on the participation constraint.

3.1

Non-recursive formulation

For a given choice of ( 1 ; : : : ; I ) 2 = [0; 1)I , for a given s0 and for an admissible state
x0 2 X , de…ne the social planner’s problem (in sequence form) as
"1
!#
I
X
X
t
i
max1 Es0
r(xt ; at ; st ) +
subject to
i g (xt ; at ; st )
(

t )2

t=0

i=1

A(ht 1 ; st ) for all ht 1 ; st
1
X
n i
i
g (xt ; at ; st ) + Et
g (xt+n ; at+n ; st+n )
supp( t )

(8)

g1i

n=1

3.1.1

8at 2 supp( t ); 8t; 8i

The Lagrangian

It is useful to introduce the Lagrangian for this problem and to write
L ( it ); ( t ); ( i ); x0 ; s0 = E0

1
X

t

(9)

t=0

r(xt ; at ; st ) +

I
X
i=1

1
X
i
i
g
(x
;
a
;
s
)
+
(
t t t
i
t;at
n=0

8

n

g(xt+n ; at+n ; st+n )

!!

g1i )

:

Note that for given ( t ) this function is concave in
and the constraint set f( t ) :
t 1
t 1
supp( t )
A(h ; st ) for all h ; st g is a convex set. Therefore, it is standard to show
(e.g. Luenberger (1969, Theorem 2, page 221)) that if there exist ( t ) with supp( t )
A(ht 1 ; st ) for all ht 1 ; st and a ( t )
0 that satisfy
L (( t ); ( t ) )

L (( t ) ; ( t ) )

(10)

L (( t ) ; ( t ))

for all ( t ) 0 and all ( t ) with supp( t ) A(ht 1 ; st ) for all ht 1 ; st then ( t ) is a solution
to the original problem (8)). The results in Rustichini (1998) and Dechert (1982) imply that
under our assumptions (1)-(3) there exists such a saddle point with the sequences ( t ) being
bounded.

3.2

Recursive multiplier formulation

We now de…ne a functional equation similar to the one in Marcet and Marimon (1994) and
prove that the value-function of the problem gives the value of the social planner’s problem.
The following functional equation de…nes the social planning problem in recursive multiplier
form
X
F ( ; x; s) = sup inf
(a)
"

0

2

r(x; a; s) +

I
X

a2supp( )

i i

g (x; a; s) +

i
i
a (g (x; a; s)

g1i ) + Es F ( +

#

0 0
a; x ; s )

i=1

subject to
x = (x; a; s)
p(x; a; s) 0;

(11)

0

8a 2 supp( ):

The following theorem guarantees the existence of a unique solution.
Theorem 1 The functional equation (11) has a unique solution in the space of bounded
functions Fs : [0; 1) X ! R, s 2 S.
A similar theorem is already proven in Marcet and Marimon (1994). We give a proof
here to make our paper self-contained.
The main problem in proving the theorem is to bound the multipliers . For this we
need to …rst consider a slightly di¤erent problem: For a given and for bounded functions
f : ([0; 1) X )S ! R de…ne the operator
X
T (f ) = max min
(a)
"

2

r(x; a; s) +

2[0; ]I

X

subject to
0
x = (x; a; s)
p(x; a; s) 0;

a2supp( )

i i

g (x; a; s) +

i
i
a (g (x; a; s)

i

8a 2 supp( );
9

g1i ) + Es f ( +

#

0
a; x )

P
where Es f = s0 (s; s0 )fs0 . By Assumption 1.1. and for a given , clearly T maps bounded
functions into bounded functions.
Therefore it is easy to proof the following lemma.
Lemma 2 The operator T has a unique …xed point in the (Banach) space of bounded functions.
Proof. We apply the contraction mapping theorem and verify Blackwell’s su¢ cient conditions monotonicity (M) and discounting (D) (see e.g. Stokey and Lucas (1989), Theorem
3.3).
(M) Given bounded f 1 and f 2 with f 1 ( ; x)
f 2 ( ; x) for all ( ; x), let ( 1 ; 1 ) be the
max-minimizer of T f 1 ( ; x) Clearly,
X
1
min
T f 1 ( ; x)
(a)
2[0; ]I

"

r(x; a; s) +

X

a2supp(

i i

1

)

g (x; a; s) +

i
i
a (g (x; a; s)

g1i ) + Es f 2 ( +

i

2

#

0
a; x )

T f ( ; x):

(D) Just substituting in yields T (f + a) = T (f ( ; x) + a) = T f + a:
We can now prove the theorem by letting become su¢ ciently large and showing that
the constraint never binds.
Proof of Theorem 1 Since the problem in the functional Equation (11) is convex, it suf…ces to show that for su¢ ciently large the constraint a (ht )
can never be binding.
1
Let be as in Assumption 1.3. De…ne d = 1
supx;a;s r(x; a; s) inf x;a;s r(x; a; s) and let
d
= 2 . Suppose at some state (x; s) 2 X S and some
0 the constraint is binding.
By Assumption 1.3 there exists a policy which yields a total reward of at least 2d. But for
= 0, the value of total reward is smaller and hence cannot be the minimizing choice of
.
Having established the existence of a solution of the functional equation (11) raises the
question how this solution relates to the solution of the original problem. This turns out to
be somewhat intricate.

3.3

Policy- and value-functions

Unfortunately, while the value function F is well de…ned and continuous, the set of arguments
that maximize F might contain in…nitely many solutions. While it is true that if a solution
to original saddle point problem exists that this also solves (9), the converse is false, in the
sense that there can be many solutions to (9) that do not solve the saddle point problem.
This was …rst observed by Messner and Pavoni (2004). The problem has nothing to do
with in…nite horizon or uncertainty but is simply caused by the fact that one cannot recover
policies from correspondences in a min-max problem. One can illustrate the problem with a
trivial two period example.
10

3.3.1

Simple example

Suppose we want to solve the following problem
V =

max

a1 + a2

a2 subject to

a1

(a1 ;a2 )2[0;1]2

1

The value of the corresponding Lagrangian, can be obtained recursively (or by backward
induction). We can write
V2 ( ) =

max ( a2 + a2 )

a2 2[0;1]

V1 =

max min ( a1 + (a1
0

a1 2[0;1]

1) + V2 ( ))

and get the correct value of the saddle point
V1 =

max

(a1 ;a2 )2[0;1]2

min( a1

a2 ) + (a1 + a2

0

1):

However, one cannot recover the correct policies from this if one takes the Lagrange multiplier
as a state variable: The correspondence solving the problem in the second period is given by
8
<1
< 0
[0; 1]
=1
a2 ( ) = arg max ( a2 + a2 ) =
:
a2 2[0;1]
1
> 1:

So clearly, one element of the recursive problem’s argmax is a1 = 1=2, = 1 and a2 = 0.
But this is not a feasible point. Another element of the recursive problem’s solution is
a1 = 1=2; = 1 and a2 = 1. This is a feasible point, but obviously suboptimal.
So clearly, the recursive formulation has solutions which are infeasible and has solutions
that are feasible but suboptimal. However note that in the above problem the solution
to the optimization problem is in the correspondence of possible solutions to the recursive
formulations.
A somewhat unrelated notational issue concerns how to properly de…ne the arguments
of a max-min problem. If we consider the original problem
max

(a1 ;a2 )2[0;1]2

then in the solution

min( a1
0

a2 ) + (a1 + a2

1);

is implicitly a function of the action (a1 ; a2 ); where this function is
8
< 1 if a1 + a2 < 1
1 if a1 + a2 = 1 ;
(a) =
:
0 if a1 + a2 > 1

and this function serves to enforce the constraint. In this case, a1 2 [0; 1] and a2 = 1
a solution to this problem and the value of the multiplier at the solution is of course
However, if the we consider the min-max version of this problem,
min

max

0 (a1 ;a2 )2[0;1]2

( a1

a2 ) + (a1 + a2
11

1);

a1 is
= 1:

then = 1, a1 2 [0; 1] and a2 = 1 a1 is a solution to this problem but it is not the
only one. In particular, with = 1 any choices of a1 and a2 are solutions to the problem,
including a1 = a2 = 1 and a1 = a2 = 0: Hence, one cannot enforce the constraint in a
min-max version of the problem. With convexity, we can apply the max-min theorem and
the value of the two problems is identical, however the solution to the …rst problem is the
‘correct’solution.5 It will therefore be useful to make the following de…nition. We say that
(x; y) 2 arg maxx miny f (x; y) if x 2 arg maxx [miny f (x; y)], if y 2 arg miny f (x; y) and if
y 2 arg miny [maxx f (x; y)].
Both of these issues arise because of the lack of strict concavity our problem. If the
solution is unique, then neither is a problem.

3.4

Principle of optimality

We are given a solution F to the functional equation (11). In order to construct a policy
which solves the saddle point problem (and therefore the original maximization problem),
using the above convention about the arg max min, we can de…ne the correspondence
X
C( ; x; s) = arg max min
(a)
"

a2supp( )

r(x; a; s) +

I
X

i i

g (x; a; s) +

a (g

i

(x; a; s)

g1i ) + Es F ( +

i=1

subject to
0
x = (x; a; s)
p(x; a; s) 0;

#

0 0
a; x ; s )

8a 2 supp( ):

In this subsection, we want to prove the following theorem.
Theorem 3 Given solutions to the recursive problem, (F ; C), there exists a sequence ( (ht );
with ( (ht ); (ht )) 2 C( ( 0 ; :::; t 1 ); x(ht ); st ) for all ht that solves the saddle point problem (10). Moreover, the value of the saddle point problem at ( ; x; s) is given by F ( ; x; s).
5

To understand the need for convexity of the constraint set in order to interchange the max and the min
operators, consider the simple matching pennies game. In the deterministic version of this game x and y are
elements of f0; 1) and the objective is f (x; y) = (x y)2 : In this case
max min f (x; y) < min max f (x; y):
x

y

y

x

When we allow players to randomize, then the constraint set is convex. Let x and y denote the probabilities
of playing 0 for each of the two players, x and y are elements of [0; 1]; and the objective function becomes
f (x; y) =

(1

x)y

x(1

y):

In this case
max min f (x; y) = min max f (x; y):
x

y

y

12

x

(ht ))

The proof quite similar to Bellman’s principle of optimality (see e.g. Stokey and Lucas
(1989), Section 4.1).
Proof. We …rst proof that if there exists a solution to the saddle point problem, the value
must coincide with F . For this de…ne
X
;:::;
t
F ( 0 ; x0 ; s0 ) = sup inf E0 0
0 ;:::;

0 ;:::;

r(xt ; at ; st ) +

X

t=0

i
i
t;at (g (xt ; at ; st )

i

+E0 0

;:::;

+1

F

0

X

n

n=0
t

subject to supp( t )
Note that for

+

!

;x

g1i ) + g i (xt ; at ; st )(
+1 ; s +1

i
0

+

x0 ; s0 given:

1
X

t (a)

i
n)

n=0

!

A(h ) for all ht ;

t 1
X

!!

= 1 we obtain

F1 ( 0 ; x0 ; s0 ) = sup inf E0
0; 1

0; 1

r(xt ; a; st ) +

X

2

F

0

t=0

a2supp(

i
i
a;t (g (xt ; a; st )

i

+E0

X

t

+

1
X

n

n=0

subject to supp( t )

!

; x(h3 ); s3

t)

g i ) + g i (xt ; a; st )(

i
0

+

t 1
X

!!

i
n)

n=0

!

A(ht ) for all ht ;

x0 ; s0 given:

By the Fan’s minmax theorem and since the problem is additively separable this can be
rewritten as
X
F1 ( 0 ; x0 ; s0 ) = sup inf E0
0 (a)
0

0

r(x0 ; a; s0 ) +

X

a2supp(

0)

i
i
0a (g (x0 ; a; s0 )

g1i + g i (x0 ; a; st ) i0 )

i

+ E0 sup inf
1

1

X

a2supp(

r(x1 ; a; s1 ) +

X

1 (a)
1)

i
1a

g i (x1 ; a; s1 )

i

+E0

2

F

0

+

1
X
n=0

subject to supp( t )

!

n

!

; x(h2 ); s3

g1i + g i (x1 ; a; s1 )(
!

A(ht ) for all ht ;

i
0

+

i
0)

!

x0 ; s0 given:

Clearly F1 = F and in fact by induction, F ( 0 ; x0 ; S0 ) = F ( 0 ; s0 ; s0 ) for each
.
13

= 1; 2; :::

On the other hand, we can write
F ( 0 ; x0 ; s0 ) = sup

inf E0 0

0 ;:::;

0 ;:::;

r(xt ; at ; st ) +

I
X

;:::;

+1

X

t

t=0

Xt
i i
g (xt ; at ; st ) + it;at (

i=1

E0 0

;:::;

F

0

+

X
n=0

n

!

n i

g (xt+n ; at+n ; st+n )

n=0

;x

+1 ; s +1

!

g1i )

!!

+

:

Recall that the value of the saddle point is
L ( 0 ; x0 ; s0 ) = sup inf E0
( t) (

r(xt ; at ; st ) +

I
X

t)

ig

1
X

t

(12)

t=0

i

(xt ; at ; st ) +

i
t;at (

1
X

n

g(xt+n ; at+n ; st+n )

n=0

i=1

!!

g1i )

:

Since F is bounded, the reward functions are bounded and since the optimal multipliers
to the saddle point problem are bounded (see Rustichini (1998)) it follows that
lim L ( 0 ; x0 ; s0 )

F ( 0 ; x0 ; s0 ) = 0:

!1

Therefore the value of the saddle point must be equal to F .
Furthermore the solution to the saddle point equation is certainly feasible for the recursive
social planner problem. Since it gives the optimal value, F , it must satisfy ( t ; t ) 2
C( ( 0 ; :::; t 1 ); x(ht ); st ) for all t.

4

Recovering policy-functions

The previous theorem shows that among the (possibly many) solutions to the functional
equation (11) there is one which also solves our original problem. The question still remains
how to …nd it. In this section, we explicitly construct a solution to the maximization problem,
using the solution to the functional equation (11).
We proceed in two steps. First we consider the general problem, here it will become clear
that we are mixing the promised utility approach with the recursive multiplier approach
of Marcet and Marimon (1994) to obtain well de…ned policy functions6 . We then make an
assumption on the solution set of the problem which simpli…es the analysis considerably and
allows us to derive a simple characterization of the solution.
6

In Section 4 below we make explicit how our approach compares to the promised utility approach.

14

4.1

The general approach

By the maximum principle it is easy to see that the correspondence C(:), as de…ned in the
previous section, is upper hemi-continuous. We can therefore de…ne
A( ; x; s) = f(a; ) : 9( ; ) 2 C( ; x; s) : a 2 supp( )g:
Note that all (a; ) 2 A( ; x; s) yield the same value of the objective function (of the social
planner’s problem, given ). Note also that if ( t ; t )1
t=0 solves the saddle point problem,
then in fact each a0 2 supp( 0 ) is feasible and optimal. As the example in Section 3.3.1
shows, the problem comes about because one needs to establish a link between periods. It
is well known that one way to do this is to carry agents’promised utilities.
For a given ’state7 ’( ; x; s), there might be several possible utilities that can be generated
by the several possible actions (a; ) 2 A( ; x; s) and by the randomizations over these
actions. In a slight abuse of notation, we use G both for the function and the correspondence
of possible utility levels and collect these in a set G( ; x; s). The correspondence, G, from
the state to the set of possible (i.e. feasible) utilities, is de…ned by the following system.
G( ; x; s) = convf

(G0 ; G1 ; : : : ; GI ) 2 RI+1 : 9(a; ) 2 A( ; x; s)
I0
0
0
9(G00
s0 ; : : : ; Gs0 ) 2 G( + ; (x; a; s); s ) for all s
G0 = r(x; a; s) + Es G00
G1 = g 1 (x; a; s) + Es G10 ; G1 g11
..
.
GI = g I (x; a; s) + Es GI0 ; GI g1I g;

were conv(A) denotes the convex hull of a set A.
For each admissible state, ; x; s, the set G( ; x; s) is a non-empty and convex
P subset of
the I-dimensional hyperplane with normal-vector (1; 1 ; : : : ; I ). That is G0 + Ii=1 i GI =
const: for all (G0 ; : : : ; GI ) 2 G( ; x; s). If the problem is strictly concave, this set will contain
only one point –in general, however, it has ’full’dimension I.
Obviously there must be a (possible set valued map) from ( ; x; s) and a given G 2
G( ; x; s) to randomized actions, , multipliers , and next period’s utilities (G0s0 )s0 =1:::S .
Note that by Caratheodory’s theorem (Caratheodory (1911)), each point in the set can be
obtained by randomizing over at most I + 1 actions a. If we de…ne the state-space to contain
utilities promised in the last period as well as ; x and s, we can easily construct policies that
are optimal and feasible. Given an enlarged state ( ; x; s) and G 2 G( ; x; s) we now say
that a policy consists of ; as well as (G0s0 )s0 =1:::S that is consistent with the requirements
above, i.e.
G0 = r(x; a; s) + Es G00 ; : : : ; GI = g I (x; a; s) + Es GI0 g1I :
In order to tie this in with our results in the previous sections, it is useful, however, to
choose a slightly di¤erent approach. This appears more complicated at …rst, but will simplify
e ; x; s) all utilities
things greatly once we make additional assumptions. We denote by G(
7

Formally we now enlarge the state-space to contain agents’utilities, however, when we refer to the state,
we mean the current multiplier, current physical state and current shock.

15

that can be obtained at a state ( ; x; s) without using randomization in the current period,
e ; x; s)) and each G 2 G(
e ; x; s) is associated with a degenerate
i.e. G( ; x; s) = conv(G(
probability distribution over current actions. A non-standard way of deriving policies now is
that a policy speci…es (a; ) 2 A( ; x; s) as well as G0s0 k and s0 ;k , s0 = 1; :::; S and k = 1; :::; I
e + ; x0 ; s0 ) and that
with the property that each G0s0 k 2 G(
X
00
G0 = r(x; a; s) + Es
s0 k Gs0 k
k

1

1

G = g (x; a; s) + Es

X

10
s0 k Gs0 k

g11

I0
s0 k Gs0 k

g1I :

k

..
.
X
GI = g I (x; a; s) + Es
k

In other words, the policy today is given by a non-randomized action today and a randomization over all actions tomorrow, shock by shock. By Caratheodory’s theorem one only
has to randomize over at most I actions in each state tomorrow. In order to make clear
that we randomize over a …xed set of continuation utilities, we now use to denote the
probabilities of these randomization, instead of , the randomization over actions which are
obviously implied by this.

4.2

A special case

A special case obtains when the sets G( ; x; s) are simple in the sense that they can be
written as the convex hull of a …nite number of points. In this case, we do not have to
introduce an additional continuous state variable.
To illustrate this, we now make the following strong assumptions on the solution set of
the problem.
Assumption 2 For each admissible ; x 2 X ; x 2 S, the set G( ; x; s) is a convex polytope,
i.e. the convex hull of a …nite set of points.
Note that for the case I = 1, i.e. the case of one agent and one principal, the assumptions
always hold since G( ; x; s) is simply a closed interval. We consider this case in some detail
in the next subsection. For more than one agent, this obviously depends on the formulation
of the problem. Even if the set of actions A is …nite, it is not guaranteed that G( ; x; s) will
be a polytope –it is quite likely though. However, if the true solution set is not a polytope,
the most obvious approach to approximate it numerically is to use a polytope. Furthermore,
we will argue below that while solving the problem, one can verify whether or not the true
solution set, G is a polytope. It is clear that with this assumption, the state does not has to
be enlarged by an in…nite set but simply by a …nite set of discrete states.
For a given set of points, P , whose convex hull forms a polytope, let V(P ) denote the
(…nite set) of extreme points (i.e. vertices) of this polytope, i.e. the smallest set of points for
which conv(P ) = conv(V(P )). Furthermore, let g(x; a; s) = (g 0 (x; a; s); :::; g I (x; a; s)) and
let F = f(G0 ; :::; GI ) : G1
g11 ; :::; GI
g1I g be the set of utilities satisfying the incentive
16

constraint (i.e. feasible utilities). We de…ne a correspondence G, mapping the current state
to a …nite set of points, G : [0; 1)I X S ) RI+1 , by requiring that it satis…es the following
functional equation equations
G( ; x; s) =
2
[
V4

(a; )2A( ;x;s)

F \ conv(fg(x; a; s)g +

x0 = (x; a; s):

X
s0

!3

(13)

(s; s0 )G( + ; x0 ; s0 )) 5 ;

In this equation the sum of sets is the Minkowski sum with A + B = f a + b : a 2 A; b 2
Bg.
A solution exists precisely when G( ; x; s) is a polytope –in this case G( ; x; s) is simply
the set of its extreme points. The advantage of writing it like this (as opposed to the system
that de…ned G( ; x; s) in the previous subsection) is that it becomes evident, that these sets
can be constructed relatively easily from the knowledge of the correspondence A(:).
When a solution to (13) exists an optimal policy can be constructed as follows. For each
v 2 G( ; x; s), de…ne
A( ; x; s; v) = f(a; ) 2 A( ; x; s); ( 1 ; :::;
X
X
v = g(x; a; s) +
(s; s0 )
s0

v 0 2G( + ;x0 ;s0 )

S)

:

s0 (v

0

)v 0 g

This gives a quasi-policy function in the following sense. The current state now consists
of ; x; s as well as a v 2 G( ; x; s). The policy prescribes the current non-randomized action,
the current multiplier, as well as (a possibly degenerate) randomization over next periods
states (which in e¤ect means a randomization over actions in the next period). There is
in general no guarantee that A is single valued, but each selection will produce an optimal
policy.
Note that in general one needs to choose a randomization for each shock s0 in the next
period, i.e. will di¤er across shocks next period. We will show below that for the special
case of one agent and one principal this is not the case –there the same randomization can
be chosen for each shock. It is clear why in this general framework this cannot be expected
–there is no guarantee that the number of vertices is the same for each shock next periods,
therefore it is obviously impossible to …nd a randomization that does not depend on the
shock.
Note that there is only randomization if today at least one incentive constraint is binding.
Theorem 4 There is an optimal solution (a; ; ) with each
distribution if (v1 ; :::; vI )
(g11 ; :::; g1I ).

s

being a degenerate probability

To prove the result, observe that in order for v to be an element of
"
!#
X
V [(a; )2A( ;x;s) fg(x; s; a)g +
(s; s0 )G( + ; x0 ; s0 )
s0

17

it has to be generated by an action next period that is not randomized –each vertex of the
sum of polytopes must be the sum of vertices (see e.g. Fukuda (2004)). The result does not
seem completely obvious since one might have thought that it could be optimal to randomize
in two periods if a IC constraint is only binding this period.
From a computational aspect, polytopes are easy to deal with. There is a sizable literature
on polyhedral computation. For example, Fukuda (2004) discusses an algorithm for the
Minkowski addition of polytopes that can be applied to very large scale problems. For
solving problems with large I, the key problem lies in solving for the correspondence A,
if the true solution set of utilities is a polytope, the computation of G(:) and A is quite
straightforward. If one is to solve the functional equation (13) iteratively, one can easily see
if the number of extreme points grows with each iteration or if eventually this stabilizes.

4.3

A principal and one agent

If there is only one agent (in additional to the principal), Assumption 2 is trivially satis…ed
and we can set up the problem somewhat more intuitively. Since G( ; x; s) is at most an
interval the sets G( ; x; s) contain at most two points. We can characterize these two points as
the one that maximizes the agent’s utility and then one that minimizes the agents utility and
denote them by GH ( ; x; s) and GL ( ; x; s). Furthermore the probabilities over next periods’
continuation payo¤s (and hence next periods actions) can now be chosen independently of
the shock.
We can consider the following functional equation.
GL ( ; x; s) =
min

g(x; a; s) + Es

GH ( ; x; s) =
max

g(x; a; s) + E

(a; )2A( ;x;s); 2[0;1]

GL ( + ; x0 ; s0 ) + (1
GL ( + ; x0 ; s0 ) + (1

)GH ( + ; x0 ; s0 )
)GH ( + ; x0 ; s0 )

(a; )2A( ;x;s); 2[0;1]

subject to
x0 = (x; a; s0 ); x0 = (x; a; s0 );

GH ( ; x; s)

g1 ; GL ( ; x; s)

g1 :

Given a solution GH ; GL to this functional equation we de…ne
! L ( ; x; s) =
arg
min

(a; )2A( ;x;s); 2[0;1]
0

g(x; a; s) + E

GL ( + ; x0 ; s0 ) + (1

)GH ( + ; x0 ; s0 )

GL ( + ; x0 ; s0 ) + (1

)GH ( + ; x0 ; s0 )

subject to x = (x; a; s):
and
! H ( ; x; s) =
arg
max

(a; )2A( ;x;s); 2[0;1]
0

g(x; a; s) + E

subject to x = (x; a; s):
18

Note that ! L and ! H are not policy functions in the classical sense. The policy should
specify a randomization and a transition as a function of the current state. But we
have seen that this is not possible. Instead we change the timing and the state. The state
now consists of ( ; x; s) as well as an element of fH; Lg. The timing now says that given
the state, we choose a deterministic action, but also the randomization across actions next
period. Note that this randomization will not be conditional on the realized shock next
period - but this does not matter since the (IC) holds today.
By construction, these policies are feasible. The mapping could still be set valued, but
then we can simply pick any point in that set. Since the solution is feasible, the argument
from Theorem 3 can be applied to show that it must also be optimal, i.e. an optimal solution
to the original problem can be generated from !, given an initial 0 that ensures that the
(PC) holds.
Finally, note that in the solution to these functional equations ! H ( ; x; s) will trivially
set = 0 so as to put the maximum probability onto the high continuation payo¤ for the
agent. In the solution to ! L ( ; x; s); it will be optimal to set = 1 so long as this does
not lead to a violation of the incentive constraint that GL ( ; x; s) g1 : If this constraint is
violated at = 1; then it may be optimal to lower since this allows the solution to satisfy
the incentive constraint at no additional cost to the overall value of the Lagrangian.
4.3.1

The optimal contract

Given these constructions, we can now characterize the dynamics of the optimal contract in
terms of ( ; x; s) and an element of fH; Lg:
In the initial period the initial physical state is x0 ; and the promised utility g2 associated
with the participation constraint must be satis…ed. To do so, we select 0 by the requirement
that
Es GH ( 0 ; x; s) g2 Es GL ( 0 ; x; s);
and we select the initial lottery probabilities over fH; Lg; and 1
; by the requirement
that
g2 = Es GH ( 0 ; x; s) + (1
) Es GL ( 0 ; x; s):
This then determines the full initial state ( ; x; s) and an element of fH; Lg: Each period
thereafter begins with this initial state.
Within each period thereafter the contract evolves as follows:
1. The element of fH; Lg determines which policy function ! H and ! L is used within the
period.
2. The policy function ! i determines a continuation triplet ( + ; x0 ; s0 ) for next period
and a probability for the lottery over elements of fH; Lg: If the neither the participation or incentive constraint bind, then is simply 0 if the element is H and 1 if
the element is L: The participation constraint cannot bind if the element is H and if
it binds when the element is L then will be less than one. If the incentive constraint
binds, then is chosen so that
g(x; ai ; s) + Es

GL ( + ; x0 ; s0 ) + (1
19

)GH ( + ; x0 ; s0 ) = g1 :

where ai is the minimizing element of the solution set for actions if i = L and maximizing if i = H.
3. This lottery is undertaken at the end of the period and an element is selected. This
determines the state tomorrow as ( + ; x0 ; s0 ) and the element a 2 fH; Lg:
The key thing to note here is that the determination of the lottery probability is very
mechanical and does not require solving an ex ante simultaneous equation problem such as
would be the case under the utility approach, which we discuss next.

4.4

Relation to ’promised utility’approach

It is useful to connect the multiplier approach to the standard ’promised utility method’,
which was originally developed by Spear and Srivastava (1987), and where the state consists
of the physical states as well as of an additional endogenous state variable, promised utility,
G. This section is somewhat less formal. We focus on the case I = 1 and we assume
that the promised utility approach has a solution and that the resulting value function is
di¤erentiable. With this, we can demonstrate that the value of the Pareto-problem must be
identical to the value of the recursive problem of the previous section.
It is useful to change the timing and write value of the Pareto problem as a function
of (G; x) before the current shock s is realized. In a Markov setup, we then need to carry
around s as a state variable and we can write our Pareto problem recursively using the
promised utility approach as
X
X
V (G; x; s ) =
max
min
(s ; s)
s (a)
(

s )2

S ;(G0 )
s;a

0

s2S

a2supp(

r(x; a; s) + V (G0s;a ; x0s ; s) + (g(x; a; s) + G0s;a
subject to
x0s = (x; as ; s)
p(x; a; s) 0; 8a 2 supp( s );
g(x; a; s) + G0s;a g1 ; 8a 2 supp( s ):

s)

G)

(14)

Note that we have included the promised utility of the agent G through a Lagrangian multiplier formulation where is the multiplier on this constraint. Obviously, at the optimal
solution it must hold that (g(x; a; s) + G0s;a G) = 0.
Assuming di¤erentiability, each G gives us a unique (G), since by the envelope theorem
VG (G; x; s ) =
. Unfortunately that the map is not invertible. For each 2
[0; 1);
we can de…ne (assuming di¤erentiability)
V L ( ; x; s) = min fV (G; x; s) : VG (G; x; s) =
V H ( ; x; s) = max fV (G; x; s) : VG (G; x; s) =

g;
g:

We can also de…ne the associated payo¤s to the agent as
GL ( ; x; s) = arg min fV (G; x; s) : VG (G; x; s) =
GH ( ; x; s) = arg max fV (G; x; s) : VG (G; x; s) =
20

g;
g:

Note that that we must have that for all ; x; s,
V L ( ; x; s) + GL ( ; x; s) = V H ( ; x; s) + GH ( ; x; s):

(15)

Given these functions, we can rewrite our recursive Pareto problem in terms of these functions, using the insight from before that the derivative of V in the next period must be equal
to plus the value of the Lagrange parameter attached to this period’s (IC). Writing to
denote the optimal associated with G we obtain
X
V (G; x; s ) = max min
(s ; s)
(16)
(as );(

r(xs ; as ; s) +

s)

sV

L

(

s)

s

0
s ; xs ) + (1
+ s ; x0s ) + (1
+ s ; x0s ) + (1

( +

(g(xs ; as ; s) + ( s GL (
L
s (g(xs ; as ; s) + ( s G (
subject to x0s = (xs ; as ; s); p(xs ; as ; s)

s )V

H

0
s ; xs ) +
+ s ; x0s ))
+ s ; x0s ))

( +

H
s )G (
H
s )G (

G) +
g1 )

0; 8s

~ that give the same value of
Note that the value V (G; x; s) is linear in G for all G
have that
~ x; s ) + G
~ = constant ;
V (G;
~ x; s ) =
whenever VG (G;
.
Moreover, if we de…ne SP ( (G); x; s ) = V (G; x; S ) + (G)G, we obtain
SP ( ; x; s ) = max min
(as );(

r(xs ; as ; s) +

s
H

s)

(

s)

L

V ( +

X

–i.e. we

(17)

(s ; s)

s

0
s ; xs )

+( +

L
s )G (

+

0
s ; xs )

0
H
0
+ (1
s ) V ( + s ; xs ) + ( + s )G ( + s ; xs ) +
g(xs ; as ; s) + s (g(xs ; as ; s) g1 )
subject to x0s = (xs ; as ; s); p(xs ; as ; s) 0; 8s

But note that with Equation (15) the optimal value of this problem can be derived from the
optimal value the social planning problem from the previous section. Simply note that
SP ( + s ; x; s) = V L ( + s ; x0s )+( + s )GL ( + s ) = V H ( + s ; x0s )+( + s )GH ( + s ; x0s ):
By changing the timing in the problem of Section 3, it is now clear that we obtain the
same value.

5

Three simple examples

Here we return to our simple partnership model in which there is a principal and an agent,
and the principal must have the participation of the agent in order to run a project which
produces a pie of size 1 every period that can be split between the two of them. Each period
the principal gets to choose the amount of the pie he eats, a; and this implies the amount
21

that the agent eats, 1 a: The within period reward functions for the principal is log(a)
and for the agent is log(1 a): Both are expected utility maximizers and both discount the
future at rate :
The agent has an initial outside opportunity, and then each period draws an outside
opportunity. These opportunities put an initial ex ante lower bound on his payo¤, which we
call the participation constraint, and a conditional lower bound, which we can an incentive
constraint. The ex ante outside opportunity which goes into the participation constraint is
g2 = 1 1 log(1=5). Assume that in every period there are two states of the world s 2 fl; hg
which are i.i.d. and equi-probable. The conditional opportunity of the agent depends on
the shock, g1 (l) = 1 1 log( ) and g1 (h) = 1 1 log(2=3). These opportunities determine the
conditional incentive constraint.
We will assume that a is bounded between " and 1 "; where " is a small number that
serves to bound the payo¤s of the agent and the principal. We will consider two cases: (i)
the set of actions a is convex and equal to ["; 1 "]; and (ii) the set of actions is not convex.
In the nonconvex case, we will allow for public randomization in order to convexify the set
of payo¤s. If the set of possible actions a by the principal is convex, then one can show that
the Pareto frontier is strictly concave, however if the action set is not convex, then one can
easily construct examples in which the Pareto frontier is weakly concave but not strictly so.

5.1

Example 1: No ‡at spots

Assume that the set of possible actions are a 2 [ ; 1 ] for some small > 0. If we let V (G)
denote the ex ante Pareto frontier, then the Lagrangian for this problem can be written as
Z
[log(a) + V (G(s; a))] d (a; s)
L(G) =
max E
(a;s);G(s;a)
a
Z
[log(1 a) + G(s; a)] d (a; s) G
+ E
a
XZ
(s; a) [log(1 a) + G(s; a) g1 (s)] d (a; s)
+
s

+E

Z

a

(a; s) (a; s)da + E '(s) 1

a

Z

(a; s)da

:

a

We have added constraints to ensure the probabilities (a; s) are nonnegative and sum to one;
(a; s) and '(s) are multipliers on these constraints. In addition, the incentive constraint is
multiplied by the p.d.f. over the action set to ensure that it holds for all actions taken with
positive probability. The solution to this problem implicitly determines the Pareto frontier
since V (G) = L(G): And, when we set G = g2 ; the value of the PC, we generate the solution
to our contracting problem.
This problem leads to the following f.o.c.s with respect to the probabilities
log(a(h)) + V (G(h; a)) + ( + (h; a)) [log(1
(h; a)g1 (s) + (a; h) '(h) = 0;
log(a(l)) + V (G(l; a)) + [log(1

a(h)) + G(h; a)]

a(l)) + G(l; a)] + (a; l)
22

'(l) = 0;

and the following f.o.c.s with respect to continuation payo¤s
V 0 (G(h; a)) +

(18)

+ (h; a) = 0;

and
V 0 (G(l; a)) +

(19)

= 0:

When the incentive constraint does not bind, as it cannot in state l, the f.o.c. for the
continuation payo¤ uniquely determines it as the solution to
V 0 (G) = :
Given this, the f.o.c. w.r.t. the probability implies that
a 2 arg max flog(a) + log(1
a

a)g ;

which implies that
1
1
=
:
a(l)
1 a(l)

(20)

When the incentive constraint binds, as it can in state h; we get that
G(a) =

1

[log(1

a))

g1 ] ;

and using our f.o.c.s for a(h) we get that the optimal choice is a solution to
a 2 arg max log(a) + V (
a

1

[log(1

a)

g1 ]) ;

and hence that

1
1
+ V 0 ( 1 [log(1 a) g1 ])
= 0:
a
1 a
This condition care be can be rewritten in terms of our multipliers as
1
= ( + (h; a))
a(h)
1

1
:
a(h)

(21)

The solution here is unique when the choice set for a is convex,8 and conditions (18-21)
determine the solution given the multipliers and (h); and the that (l) = 0.
Since the continuation utility is G(a; s) next period, it follows that the multiplier on
the promised utility condition in the update problem will be + (h; a): Continuing in
this manner, we can construct the future sequence of multiplier values. If we denote the
8
The optimal a here is unique if the action space is convex since V is weakly concave. To see this
assume that there were two combinations (ai ; Gi ) i = 1; 2 which were being used with positive probability.
Then note that since each combination satis…es the IC constraint with equality, and since the agent has
concave preferences over a; then the convex combination of choices does so strictly. Furthermore, the convex
combination is strictly preferred by the principal since he too has a concave payo¤ function over a and at
least weakly concave one over G:

23

multiplier on the PC by 0 and the multipliers on the IC constraint in history state (ht 1 ; h)
and (ht 1 ; l) by t (ht ) Pr(ht ); we get the …rst-order condition for a(ht );
2
3
X
1
1
4 0+
(hj )5
= 0;
t
a(h )
1 a(ht )
j
t
h

h

where " " means predecessor history and includes the current history. The fact that the
IC multipliers for each state along an event tree come in additively motivated Marcet and
Marimon to focus on the recursive multiplier
X
(ht 1 ) = 0 +
(hj );
hj ht

1

in which case the multiplier in ht become (ht 1 ) + (ht ) in the above expression. Since
this alternative representation is equivalent to the original representation, and since that
representation was necessary and su¢ cient, it follows that this representation is as well.
If we go a step further and consider a recursive planning-problem in which the state
variable for the agent is his recursive multiplier ; we get Marcet and Marimon’s formulation
F ( ; s) = max min log(a) + log(1
a2A

a) + [log(1

0

a)

g1 (s)] + Es F ( + ; s~);

where for notational simplicity we have consider only deterministic action choices. The …rst
order conditions with respect to a is simply
1

1
a

1

1

a

The …rst order condition with respect to

is

log(1

a)

1

a

=0

g1 (s) + Es F 0 = 0:

We obtain directly from these conditions that a = 1+ 1+ , if the resulting a 2 [ ; 1 ], which
will always be the case below. So we only need to solve for ( ; s) and as well as the period
zero 0 which is needed to satisfy the participation constraint.
Working recursively at = 2, we want to verify that ( ) = 0, independently of the
shock. Note that we can write the utility the IC constraint promises the agent in h as
g1 (h) = 1 1 log( 1+ ): Furthermore, under the assumption that = 0, the value-function is
given by
1
1
Fl ( ) = Fh ( ) =
log(
) + log(
) :
1
1+
1+
In shock h, the …rst order condition with respect to
log(

1+

)

1
1

log(

1

)+

1

at

= 0 becomes

(log( )

log(1 + )) = 0;

so in fact = 0 is the optimal policy if = 2. In shock l it is easy to see that the
constraint becomes binding and so = 0 is also the optimal policy.
24

0

Working backwards, for any < it is easy to verify, by the above argument, that in
shock h the optimal policy is given by ( ) =
and so a = 1+1 . The value function is
then
Fh ( ) =
=
In shock l, we claim that

1

1
) + log(
)
(
1+
1+
1
log(
) + log(
) :9
1+
1+
log(

1
1
1

= 0 is the optimal policy, whenever

Fl ( ) = log(

)g2

1
1

1. If

= 0,

1
) + log(
) + [Fl ( ) + Fh ( )] ;
1+
1+
2

and substituting for Fh ( ) and , the value function is given by
Fl ( ) =

1
1

=2

log(

1
) + log(
)+
1+
1+
2

2

(log(1=3) + log(2=3)) :

One can easily check that the …rst order conditions with respect to give a corner solution
= 0.
Finally the initial 0 is now pinned down by the participation constraint through the
value function of the agent.10

5.2

Example 2: Flat spots

We now change the previous example and assume that a 2 f g [ [0:5; 1
potential ‡at spot in the Pareto-frontier at
=

log(0:5)
log(1
)

]. There is now a

log( )
> 1:
log(0:5)

For an >
the corner solution a = is optimal, and the constraint that
0 binds,
1
hence = 0; the (IC) constraint does not bind and Fs ( ) = 1 (log( ) + log(1
)) for
both s = 1; 2.
At =
the optimal solutions (to the recursive Pareto-problem) are all randomizations
over 0:5 and , yielding 0:5 or 1
to the agent. Since any >
implies the agent getting
1 " forever, the IC constraints cannot possibly bind at
and = 0: Note that if is so
small that
1
log(0:5) +
log(1
)
log(2=3) < 0;
1
1
it will never be optimal to put positive weight on 0.5 in the h state and randomization
only occurs in the l state. Clearly not all these randomizations are optimal solutions to the
principal agent problem (even if they only occur in the l-state).
10
0

Marcet and Marimon don’t include an initial participation constraint and hence in their formulation,
= 0:

25

L
If we let GH
s ( ) denote the high payo¤ to the agent in state s and Gs ( ) the low, then
randomizing over extreme payo¤s in the appropriate manner will allow us to achieve the
e¢ cient outcome. Clearly at = ; the highest possible outcome for the agent is to set his
1
log(1
), s = l; h;
consumption equal to 1
forever, and this implies that GH
s ( ) = 1
H
and that the ex ante high payo¤ G ( ) is equal to this as well.
Next consider recursively de…ning GLs ( ) as

GLs ( ) =

min

2[0;1];a2f0:5;1

g

log(1

GH ( ) + (1

a) +

)GL ( )

subject to
GLs ( )

g1 (s)

and where
GL ( ) = :5 GLl ( ) + GLh ( ) :
Clearly, we will want to make and a as small as possible subject to the incentive constraint.
Since the incentive constraint has to hold ex-post, we cannot randomize over a to satisfy this
constraint. However, we can do so by randomizing over GL and GH , which means that we
are randomizing over actions tomorrow. Since the incentive constraint doesn’t bind in the
low state, it follows that = 0, a = 0:5, and
GLl ( ) = log(0:5) +

2

GLl ( ) + GLh ( ) :

(22)

Since the incentive constraint binds in the high state GLh ( ) = g1 (h). Equation (22) then
yields directly
1
(log(0:5) + 0:5 g1 (h)):
GLl ( ) =
1
=2
The associated actions are clear in the low state –in the high state they depend on and
the discount factor. If there exists a such that
log(0:5) +

H
(0:5GH
h + 0:5Gl ) + (1

)(0:5GLh + 0:5GLl ) = g1 (h);

then the optimal action is 0:5, otherwise it must be . Note that there might be a region
where the action is not unique and both 1=2 and to the job with the right probabilities.
For any 0 <
in shock h the (IC) constraint binds, the optimal ( 0 ) =
0 and
the associated action is ! H
(h)
together
with
a
randomization
over
next
periods’actions
that
L
gives g1 (h) for the agent. Because there is a kink in the Pareto frontier at a = 0:5 there are
1
1
two cases when 0 <
and the shock is l: If 0 2 (1; ) then a = 0:5: If 1
0
1
1
the optimal = 0 and the unique policy is determined by a = 1+ . The participation
0
constraint pins down 0 .
The reason why one does not have to keep track of GH and GL for <
is simple: At
h, there is initially no randomization, the + brings one to the ‡at spot but it is clear
that the strategy is uniquely determined as the one that yields g1 (h):
Note that the randomization is identical for both shocks in the subsequent period.
This is only possible because we randomize over the two extreme points of two intervals,
describing the sets G(s0 ) for the two possible shocks s0 = 1; 2 in the next period.
26

5.3

Example 3: How do Flat Spots Propagate

We now change the example by assuming that the action space can stochastically switch
from the continuous action space (c) to the discrete action space (d) and stay there forever.
The state space is now s 2 f(l; c) ; (h; c) ; (l; d) ; (h; d)g ; where the draws of l and h continue
to be i.i.d. and equiprobable, and the probability of switching from c to d next period is
; while d is an absorbing state. If the state is (i; c) for = l; h then the action space is
A(i; c) = [ ; 1
]; while if it is (i; d) it is A(i; d) = f g [ [0:5; 1
]: We assume that the
initial states are either (l; c) or (l; d).
The recursive planning problem is
F ( ; s) = max min log(a) + log(1
a2A(s)

a) + [log(1

0

a)

g1 (s)] + Es F ( + ; s~);

and the …rst-order conditions are still
1
a

1
1

1
a

1

a

=0

with respect to a and
log(1

a)

g1 (s) + Es F 0 = 0

with respect to ; which implies that a = 1+ 1+ , if the action space is A(i; c) and the resulting
a 2 [ ;1
]:
Clearly, when the action space is A(i; d); the solution to this problem will work just like
in the example 2, and there will be a ‡at spot on the conditional Pareto frontier at slope
: For this reason, continue to let Gjl for j = H and L denote the high and low payo¤s
respectively in the discrete case when s = l: In state (l; c); the payo¤ to the agent, Wl ( );
will be implicitly given by
Wl ( ) = log(1
+

2

+ [(1
2

(1

)Wl ( + )

GH
l ( + ) + (1

)GLl ( + )

a( + )) +

2

) + ] g1 (h);

where is chosen to satisfy the incentive constraint that this payo¤ must be at least g1 (l):
If > 0 then this payo¤ must be g1 (l), and hence is uniquely de…ned. If 6=
and = 0;
H
L
then Gl ( ) = Gl ( ) and the payo¤ is again uniquely de…ned. However if =
and = 0;
L
then the GH
(
)
>
G
(
)
and
the
ability
to
randomize
over
this
two
continuation
payo¤s
will
l
l
induce a ‡at section on the Pareto frontier today.
Similarly, in state (h; c) the payo¤ to the agent, Wh ( ) will be implicitly given by
Wh ( ) = log(1
+

2

+ [(1
2

a( + )) +

(1

)Wh ( + )

GH
l ( + ) + (1

)GLl ( + )

2

) + ] g1 (h);
27

where is chosen to satisfy the incentive constraint that this payo¤ must be at least g1 (h):
If with = ; = 0 and = 1 the payo¤ Wh ( ) > g1 (h); then here too the payo¤ to the
agent will be indeterminate when = .
This example illustrates that ‡at sections in the continuation Pareto frontier do induce
‡at spots in the current Pareto frontier, but only at the speci…c value of the multiplier.11

6

Computational Considerations

We show in this section that the insights from the simple example above carry over to more
complicated models. For simplicity we assume that there is no physical state. For models
with a physical state variable (such as for example capital), many of the insights of this
section will still hold true as long as one makes strong enough assumptions on the law of
motion, .
We propose a simple value function iteration to solve for the optimal policy even when
there are ‡at spots in the Pareto-frontier. In the case without physical state, the recursive
planner problem simpli…es to
F ( ; s) = max min
2

"

r(a; s) +

I
X

0

X

(a)

a2supp( )
i
i
a (g (a; s)

i i

g (a; s) +

i
g1s
) + Es F ( +

#

0
a; s )

i=1

subject to
p(a; s) 0; 8a 2 supp( ):

(23)

Standard arguments show that for a given s, F is continuous in . To see that it is also
convex it is useful to consider the non-recursive social planning problem (8).
!#
"1
I
X
X
t
i
SP ( 0 ; s0 ) = max1 Es0
r(at ; st ) +
subject to
i g (at ; st )
(

supp( t )

t )2

t 1

t=0

i=1

t 1

A(h ; st ) for all h ; st
1
X
n i
i
g (at ; st ) + Et
g (at+n ; st+n ) g1i
n=1

8at 2 supp( t ); 8t; 8i

We proved above that SP ( ; s) = F ( ; s) and furthermore it is easy to see that that for any
s 2 s, ; ~ 0 and 2 [0; 1], SP ( + (1
)~ ; s)
SP ( ) + (1
)SP (~ ). For this just
=
+ (1
)~ , it is feasible for
note that if ( t ) solves the social planner problem for
11

This result is sensitive to the assumption that the principal and the agent discount at the same rate.

28

both

and ~ . Therefore
SP ( ; s) + (1
)SP (~ ; s)
!
I
1
X
X
t
i
r(at ; st ) +
+ (1
E s0
i g (at ; st )
t=0

SP (

i=1

+ (1

)~ )

)Es0

1
X

t

r(at ; st ) +

t=0

I
X
i=1

!

~ i g i (at ; st )

Let rF ( ; s) denote the subdi¤erential of F with respect to . This possibly multivalued
map is (cyclically) monotonically increasing in . If F is di¤erentiable at , the map is singlevalued and equal to the gradient of F (see Rockefellar (1970)). The points where F fails to
be di¤erentiable are precisely those that correspond to a ‡at spot in the Pareto-frontier. It
is also standard to see that given an action a, an interior solution for must satisfy the …rst
order condition
g(a; s) g1s + Es @(s0 ) = 0;
(24)
with @(s0 ) 2 rF ( + ; s0 ) for all s0 .
It is clear that for a given ( ; s) and a given the set of optimal actions (that are
associated with that ) can be determined without knowledge of F . If the problem is
smooth it is simply the set of zeros of the equations
Da r(a; s) + ( + )Dg(a; s) + Dp(a; s) = 0;

i pi (a; s)

= 0;

(25)

that satisfy p(a; s) 0. In the strictly concave case, this system has a unique solution which
can be found with standard methods, otherwise all solutions to this system can be obtained
using Gröbner bases if the reward functions are algebraic and the dimension of the action
space is not too large (see Kubler and Schmedders (2010)) –this method has the advantage
that ( + ) can be treated as a parameter and the Gröbner basis only needs to be computed
once.
If the problem is not smooth, as in the example, one needs to look …rst for solutions in
the smooth region and then go through all endpoints (or discrete choices) to enumerate the
support of all possible randomizations.
Note that by the …rst order conditions (24) for a given , if one of the reward functions
is strictly monotone in the action, there can only be more than one solution if = 0 or if we
are at a point where F ( + ; s0 ) is not di¤erentiable for some s0 .
Also note that if a( + ) and a( + ~ ) and solve Equation (25) for and ~ respectively,
and if < ~ , we must have g(a( + ); s) < g(a( + ~ ); s). Since rF is monotone, the …rst
order condition (24) then implies directly that there cannot be two di¤erent (a; ) and (~
a; ~ )
with ~ 6= that both solve the Bellman equation (11).
These observations suggest a simple value function iteration to solve the functional Equation (11) and to implement our approach computationally.
We start with an initial guess of a single-valued policy and di¤erentiable and convex
value function. As in standard value function iteration (see e.g. Judd (1998)), we iterate
backwards, solving the Bellman equation at a …nite number of predetermined points in each
iteration. This is possible, because we can control for possibility of multiplicity in a simple
fashion. We check separately if there are solutions for = 0. If not, we use standard methods
29

=

to …nd a solution for an interior . Only if the associated > 0 leads to a nondi¤erentiability
of tomorrow value function we need to again use all solution methods to …nd all solutions
in a for that given . At the same time, we perform value function iteration on the agents’
utilities G( ; s) and approximate the optimal policy A( ; s; v) For the case I > 1 it can be
veri…ed at each stage that if there is a ‡at spot, the set of agents’utility in fact does form
a polytope. For the simple case I = 1, in the case of a ‡at spot, we simply need to keep
track both of GL and GH and include into the policy the optimal randomization over next
periods’continuation. Value function iteration can be done parallel to keeping track of the
vertices of the set G( ; s).

7

Conclusion

The recursive multiplier approach in Marcet and Marimon (1994) can be extended to models with ‡at spots in the Pareto-frontier. In order for our method to be computationally
tractable, the ‡at spots must be polytopes. This is obviously always the case if there is only
one agent in addition to the principal. In the case of several agents, this will naturally be
the case if the ‡at spots are generated lotteries over a …nite number of actions. On the other
hand, it is clearly possible to construct problems where the ‡at spot is a general convex set.
For problems like this, neither the promised utility approach nor our method is likely to be
feasible.

References
[1] Aiyagari, R., A. Marcet, T. Sargent and J. Seppala (2002), Journal of Political Economy,
110(6), 1220-1254.
[2] Atkeson, A. and H. Cole, (2005) A Dynamic Theory of Executive Compensation and
Optimal Capital Structure, N.B.E.R. working paper #11083.
[3] Attanasio, O. and J. Rios-Rull (2000), Consumption Smoothing in Island Economies:
Can Public Insurance Reduce Welfare?, European Economic Review, 44(7), 1225-1258.
[4] Chien, Y., H. Cole and H. Lustig, (2007), A Multiplier Approach to Understanding the
Macro Implications of Household Finance, N.B.E.R. working paper #13555.
[5] Cole, H. and E. Prescott (1997), Valuation Equilibrium with Clubs, Journal of Economic
Theory, 74, 19-39.
[6] Cooley, T., R. Marimon and V. Quadrini (2004), Aggregate Consequences of Limited
Contract Enforceability, Journal of Political Economy, 112(4), 817-847.
[7] Dechert, W.D., (1982), Lagrange multiplier in in…nite horizon discrete time optimal
control models, Journal of Mathematical Economics, 9, 285-302.
[8] Fukuda, K, (2004) From the zonotope construction to the Minkwoski sum of convex
polytopes, Journal of Symbolic Computation, 38, 1261-1272.
30

[9] Judd, K. (1998), Numerical Methods in Economics, MIT Press, Cambridge.
[10] Kehoe, P. and F. Perri (2002), International Business Cycles with Endogenous Incomplete Markets, Econometrica, 70(3), 907-928.
[11] Khan, A., R. King and A. Wolman (2003), Optimal Monetary Policy, Review of Economic Studies, 70(4), 825-860.
[12] Kubler, F. and K. Schmedders (2010), Tackling Multiplicity of Equilibria with Gröbner
Bases, Operations Research 58, 1037-1050.
[13] Luenberger, D.G. (1969), Optimization by vector space methods, Wiley.
[14] Marcet, A. and R. Marimon, (1992), Communication, Commitment, and Growth, Journal of Economic Theory, 58, 219-49.
[15] Marcet, A. and R. Marimon, (1994), Recursive contracts., discussion paper Universitat
Pompeu Fabra.
[16] Messner,M. and N. Pavoni, (2004), On the recursive saddle point method, IGIER working paper 255, Universita Bocconi.
[17] Messner,M., N. Pavoni, and C. Sleet, (2011), Recursive Methods for Incentive Problems,
research memo.
[18] Prescott, E. and R. Townsend (1984A), General Competitive Analysis in an Economy
with Private Information, International Economic Review, 25(1), 1-20.
[19] Prescott, E. and R. Townsend (1984A), Pareto Optima and Competitive Equilibria with
Adverse Selection and Moral Hazard, Econometrica, 52(1), 21-46.
[20] Rockefellar, R.T., (1970), Convex Analysis, Princeton University Press.
[21] Rogerson, R. (1988), Indivisible Labor, Lotteries and Equilibrium, Journal of Monetary
Economics, 21, 3-16.
[22] Rustichini, A. (1998), Lagrange multiplier in incentive-constrainted problems, Journal
of Mathematical Economics, 29, 365-380.
[23] Spear, S. and S. Srivastava (1987), On Repeated Moral Hazard with Discounting, Review
of Economic Studies, 54, 599-617.
[24] Stokey, N., and R. Lucas, with E. Prescott (1989) Recursive Methods in Economic
Dynamics, Harvard University Press.

31

