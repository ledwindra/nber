NBER WORKING PAPER SERIES

PREFERENCES, SELECTION, AND VALUE ADDED:
A STRUCTURAL APPROACH
Şaziye Pelin Akyol
Kala Krishna
Working Paper 20013
http://www.nber.org/papers/w20013
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
March 2014

We would like to thank Nikhil Agarwal, Verónica Frisancho, Paul Grieco, Susumu Imai, Sung Jae
Jun, Corinne Jones and Cemile Yavas for comments on an earlier draft. We would also like to thank
participants of the CES-IFO Area Conference on Applied Microeconomics 2013 and the Penn State
Applied Econ JMP Conference 2013 for their useful comments on an earlier draft. We benefited from
the helpful comments of seminar participants at University of Kentucky and Warwick University.
All errors are our own. The views expressed herein are those of the authors and do not necessarily
reflect the views of the National Bureau of Economic Research.
At least one co-author has disclosed a financial relationship of potential relevance for this research.
Further information is available online at http://www.nber.org/papers/w20013.ack
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2014 by Şaziye Pelin Akyol and Kala Krishna. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.

Preferences, Selection, and Value Added: A Structural Approach
Şaziye Pelin Akyol and Kala Krishna
NBER Working Paper No. 20013
March 2014
JEL No. I20,I21
ABSTRACT
This paper investigates two main questions: i) What do applicants take into consideration when choosing
a high school? ii) To what extent do schools contribute to their students’ academic success? To answer
these questions, we model students’ preferences and derive demand for each school by taking each
student’s feasible set of schools into account. We obtain average valuation placed on each school from
market clearing conditions. Next, we investigate what drives these valuations by carefully controlling
for endogeneity using a set of creative instruments suggested by our model. Finally, controlling for
mean reversion bias, we look at each school’s value-added.
We find that students infer the quality of a school from its selectivity and past performance on the
university entrance exam. However, the evidence on the value- added by schools shows that highly
valued or selective schools do not have high value- added on their students’ academic outcomes.
Şaziye Pelin Akyol
Department of Economics
303 Kern Graduate Building
University Park, PA 16802
spd161@psu.edu
Kala Krishna
Department of Economics
523 Kern Graduate Building
The Pennsylvania State University
University Park, PA 16802
and NBER
kmk4@psu.edu

An online appendix is available at:
http://www.nber.org/data-appendix/w20013

“The C student from Princeton earns more than the A student from Podunk not mainly
because he has the prestige of a Princeton degree, but merely because he is abler. The golden
touch is possessed not by the Ivy League College, but by its students.”
Shane Hunt, “Income Determinants for College Graduates and the Return to Educational
Investment,” Ph.D. thesis, Yale University, 1963, p. 56.

1

Introduction
In much of the world, elite schools are established and very often subsidized by the

government. Entry into these schools is based on performance in open competitive entrance
exams. Applicants leave no stone unturned in their quest for higher scores on these entrance
exams creating enormous stress. The belief seems to be that getting into these schools
is valuable, presumably because future outcomes are better in this event. Students, it is
argued, will do better by going to an exam school where they are challenged by more difficult
material and exposed to better peers. What actually happens? Students of these elite exam
high schools, without a doubt, do better on college entrance exams and are more likely to
be placed at the best university programs. But is this due to selection or value-added by
these schools? It is quite possible that the success of students from exam schools creates the
belief that these schools add value. This belief results in better students sorting into exam
schools so that students from these schools do better, which perpetuates the belief system.
The usual way of ranking schools is in terms of their selectivity, how hard they are to
get into in terms of some performance measure like the SATs in the US1 , or in terms of how
well students who graduate from them do as measured by wages, eminence in later life, or
admission into further schooling. However, schools may do well in all of these dimensions
merely because they admit good students and not because they provide value added and
1

Schools are sometimes less than honest: some inflate their statistics on the performance of their entering
class. Some schools manipulate the system by keeping their class size small, thereby having high SATs and
looking very selective. See “Academic integrity should count in rankings” in the Kansas City Star, 2/8/2013.
http://www.centredaily.com/2013/02/12/3499088/editorial-academic-integrity-should.html

2

thereby improve the performance of the students they admit.2 How can we control for such
selection and estimate value added? In this paper, we develop a simple model of student’s
school preferences and use it to understand equilibrium sorting across schools. We estimate
preferences with a view to understanding what students seem to value in a school. Finally,
we estimate the extent to which a school contributes to their students’ academic success.
Turkey is a good place to look for answers to these questions for a number of reasons.
To begin with, the Turkish admissions system is exam-driven. Admissions are rationed on
the basis of performance on open competitive national central exams at the high school and
university level. In addition, the allocation of students to high schools is based on strategyproof Deferred Acceptance algorithm on the high school entrance exam which eliminates
incentive problems.3 Second, as education is highly subsidized in public institutions, educational options outside the country or at private institutions are much more expensive so that
these exams are taken seriously by the applicants.4 When the stakes are high, as in Turkey,
it is less likely that outcomes are driven just by noise.
We develop a way to answer the questions of interest by taking a more structural approach
than much of the literature. Using data on the admission cutoff scores, the size of each high
school class, and the overall distribution of scores, we estimate a nested logit model of
preferences over high schools taking into account that exam schools only admit the highest
scoring students who apply. Thus, students choose their best school from schools whose
cutoff is below their score. We do this in multiple steps. First, by using information on
the minimum cutoff scores, we derive demand for each school conditional on the correlation
of shocks within a nest. We obtain the mean valuation for each school by setting demand
equal number of available seats. Second, we pin down the correlation of shocks within a
nest using information on the maximum and minimum cutoff scores in each school. This
2

There has recently been considerable effort in determining value-added by a school as part of the
accountability in the No Child Left Behind legislation. See Darling et al. (2012) for a critique of the
approach usually taken.
3
Students prefer to report their true preferences, no matter what other students report.
4
Many experiments, especially non-natural ones, rely on performance measures or evaluations that do
not matter for the student, which makes the effects hard to interpret.

3

twist, to our knowledge, is novel. The idea is quite simple. If preference shocks are perfectly
correlated within a nest, then preferences are purely vertical and the minimum score in the
most valued school in the nest cannot be lower than the maximum score in the second most
valued school in the nest. Thus, the extent of overlap in the scores between schools within a
nest identifies the correlation in preference shocks in the nest. Third, to see what applicants
care about in a school, we regress mean valuation of schools on the schools’ characteristics
using clever instruments suggested by our model to correct for endogeneity. We use the
data on the overall distribution of scores on the high school entrance exam, along with
the estimated preference parameters to allocate students to high schools and obtain the
simulated distribution of students’ scores on the high school entrance exam in each school.
Then, by using information on university entrance exam scores and the simulated high school
entrance exam scores in a school, we obtain a contaminated estimate of the value-added by
a school. The contamination comes from mean reversion and is especially severe at the top
and bottom of the school hierarchy. This mean reversion is a consequence of randomness in
performance. Students in the best (worst) schools disproportionately include those who are
just lucky (unlucky) so that their performance in the university entrance exams will tend to
be below (above) that in the high school entrance exams even if there is zero value added.
We use simulation-based methods as well as information on each student in a single school
to estimate the average value-added by a school while controlling for mean reversion. Note
that the extent of the mean reversion depends on both preferences and the extent of noise
in the high school entrance exam score so that correcting for it can only be done by taking
a structural approach.
Our results suggest that students care about a school’s selectivity, its students’ past
performance on the university entrance exam, and they value elite science schools highly.
However, the evidence on the value-added by schools shows that highly valued schools do
not all have high value-added on their students’ academic outcomes. Some have negative
value added while others have positive value added. Our results suggest that students like

4

more selective, better performing elite schools so that better students are sorted into these
schools, even when they need not add value to the students in terms of their performance on
the university entrance exam. This may also be because of signaling and/or the consumption
value of going to such schools.
Exam schools in Turkey are given more funding per student, and they have better teacher
to student ratios. The better off are also more likely to be able to get into these schools (For
example, see Caner and Okten (2012)) so that such funding is likely to be a regressive force.5
Providing funding based on value-added by a school may make such funding less regressive
as well as better align the incentives of schools and society. Though our data is on Turkey,
the issues raised in this paper are of universal interest.
We proceed as follows. First, we relate our work to the literature. In Section 2, we
provide the necessary background regarding the Turkish system and the data. Section 3 lays
out the model, the estimation of preferences and the results. In Section 4, we estimate the
value-added by the schools. Then, we conclude. Additional figures, tables and details about
the estimation strategy can be found in the Appendix.

1.1

The Literature

There is a large literature that deals with school choice and school effects in the US, as
well as in other developed and developing countries. In the US, the consensus seems to be
that attending a better school does not have much of an impact on a student’s academic
achievement. Abdulkadiroglu, Angrist, and Pathak (2011), and Dobbie and Fryer (2011)
investigate the effects of attending Boston and New York exam schools by using a RegressionDiscontinuity approach. They look at students who were just below the cutoff and those
that were just above and find no significant effect of being above the cutoff and thereby
going to exam schools.
Cullen, Jacob and Levitt (2005) and (2006) use data from randomized lotteries that
5

This regressive nature is common across countries as the better off are advantaged in many ways.

5

determine the allocation of students in the Chicago public school system. Students who win
the lottery attend the better schools. They find that winning this lottery does not improve
students’ academic performance. Clark (2010) investigates the effect of attending a selective
high school (Grammar School) in the UK (where selection is based on a test given at age 11
and primary school merit) and finds no significant effect on performance in courses taken by
students, although the probability of attending a university is positively affected.
Dale and Krueger (2002) and (2011) look at the effect of attending elite colleges on labor
market outcomes. Their work is among the most careful and well-cited on the topic. Much
of the work in this area controls for selection by using a two step Heckman approach or
matching estimators. Unobservables are typically controlled for by allowing the error terms
in the selection and outcome variables to be correlated.6 What is unique about their work
is that they control for selection by controlling for the colleges to which the student applied
and was accepted. The former provides an indication of how the student sees himself while
the latter provides a way of controlling for how the colleges rank the student. Intuitively,
the effect of selective schools on outcomes is identified by the performance of students who
go to a less selective school despite being admitted to a more selective one, relative to
those who go to the more selective one. Of course, if this choice is based on unobservables,
this estimate would be biased.7 They find that black and Hispanic students in addition to
students from disadvantaged backgrounds, less-educated or low-income families, do seem to
gain from attending elite colleges. However, for most students the effect is small and fades
over time.
In contrast to these results, Pop-Eleches and Urquiola (2013) and Jackson (2010) estimate
the effect of elite school attendance in Romania and Trinidad and Tobago, respectively. They
find a large positive effect on students’ exam performance in the university entrance exams.
From the school choice literature, Hastings, Kane, and Staiger (2009) and Burgess et
6

See Frisancho and Krishna (2012) for an application using Indian data.
For example, if confident students go to the selective school and less confident ones do not, and confident
students do better, the effect of selective schools would be overestimated.
7

6

al. (2009) investigate what parents care about in a school using data from the CharlotteMecklenburg School District and Millennium Cohort Study (UK), respectively. Hastings,
Kane, and Staiger (2009) take a structural approach and estimate a mixed logit model
of preferences. A major contribution of their work is to use information on the stated
preferences for schools and compare these to what was available to them to back out the
weight placed on factors like academics, distance from home, and so on. They are then able
to see whether the impact of a school differs according to “type”. For example, they can
determine whether students who put a high value on academics do better in a good school
than students who place a high value on being close to the school. If such differences are
large, the reduced form effects estimated for attending good schools could be biased if such
selection is not properly accounted for. If students in developing countries place greater
value on good schools than do students from developed countries, this insight could explain
why we see such different results for attending better schools between the two. Burgess et
al. (2009) also compare the first choice school to the set that was available, constructed
by the authors by using students’ residence areas, and estimate trade-offs between school
characteristics.
Although we don’t estimate peer effects separately, our estimate of the school’s valueadded includes peer effects. Ding and Lehrer (2007) estimate peer effects using data from
a county in China, where students are allocated to high schools based on a criteria that is
mainly based on students’ entrance exam scores.8 They find a positive peer effect on students’
college entrance exam scores. Several other papers (Hanushek et al. (2003), Hoxby (2000),
Kang (2007), Zabel (2008), and Zimmerman (2003)) also study peer effects on academic
achievements.9 Duflo, Dupas and Kremer (2011) suggest that the behavior of teachers is
crucial. They use data from a randomized experiment in Kenya to investigate how tracking
students affects outcomes, and find that tracking helps both high achieving and low achieving
students if teachers adjust their instruction level, but not otherwise.
8
9

This differs slightly from the Turkish system where allocation solely depends on exam scores.
Epple and Romano (2010) present a detailed survey about peer effects.

7

In sum, the evidence available suggests that selective schools/tracks can have a positive
impact on disadvantaged groups who care about quality schooling and would otherwise have
had a low quality education, or who live in developing countries.
Our contribution to the literature is twofold. First, much of the work described above is
reduced form rather than structural. An advantage of the slightly more structural approach
taken here is that we can estimate preferences, understand what seems to drive them, look
at sorting over schools, as well as estimate the value-added by a school. In other words, we
examine the whole process and not just one of its components. Second, despite the lack of
panel data, i.e., not having the high school entrance exam score and the college entrance
exam score for each student, we show how one can use fairly limited data on each high school,
along with data on university entrance exam takers along with the model, to get around this
deficiency. That is to say, our approach allows us to economize on data in the estimation.
With richer data that includes information on each students’ performance in both exams, as
well as some background information on them, we could estimate students’ preferences using
standard techniques in industrial organization such as those developed in Berry, Levinsohn
and Pakes (1995), or variations that also use information on stated preferences as in Hastings,
Kane and Staiger (2009). This would help mitigate the impact of unobservables that remain
an issue even when selection is controlled for as in Dale and Krueger (2002).

2

Background
In Turkey, competitive exams are everywhere. Unless a student chooses to attend a

regular public high school, he must take a centralized exam at the end of 8th grade to
get into an “exam school”. These are analogous to magnet schools in the US, though the
competition for placement into them is national and widespread, rather than local as in the
US. After high school there is an open competitive university entrance exam given every
year. So many students retake these university entrance exams that only a third of the 1.5

8

0

.02

density

.04

.06

Figure 1: Distribution of ÖSS-SAY score

80

90

100

110

120

130 140 150
OSS−SAY score

Science Lycee
Public Regular−Science Track

160

170

180

190

Anatolian−Science Track

Source: ÖSYM data on 2002 ÖSS applicants

million students taking the exam in a given year do so for the first time. Most students go to
cram schools (dershanes) to prepare for the university entrance exam. Much of high school
is also spent preparing students for this exam. Such exams weaken the formal schooling
system as schools focus on teaching to the exam rather than on the curriculum or fostering
the ability to think. If exam schools, in fact, have little value-added, then the system itself
may have adverse welfare effects. This is especially so if such schools are subsidized relative
to the alternatives, as is often the case.10 In this event, students expend possibly wasteful
effort to capture these rents which reduces welfare.11
Students from exam schools do perform much better in university entrance exams. Figure
1 shows the distribution of average scores (ÖSS-SAY) in the university entrance exam of
science track students coming from the different kinds of high schools. Science high schools
are clearly doing better, followed by the almost as selective Anatolian high schools, while
10

The best teachers are allocated to these schools, their facilities are better, and their class sizes are
smaller than that of regular schools. In addition, Caner and Ökten (2012) shows that school subsidies are
regressive as better off students tend to do better on exams and so go to better schools which are more
highly subsidized.
11
See Krishna and Tarasov (2013) for more on this subject.

9

regular Public schools seem to do the worst. However, this says little about the contribution
of exam schools in terms of value-added.

2.1

The Institutional Structure

The educational system in Turkey is regulated by the Ministry of Education. All children
between the ages of 6 and 14 must go to school. At 14 they take the high school entrance
exam (OKS) if they want to be placed in public exam schools. Performance on this exam
determines the options open to a student. The better the performance, the greater the
number of schools with a cutoff score below what the student has obtained. There are
four types of public exam schools: Anatolian high schools, Anatolian Teacher Training high
schools, Science high schools, and Anatolian Vocational high schools.
Anatolian high schools place a strong emphasis on foreign language education although
their specific goals may vary across the different types of Anatolian schools. The main goal
of Anatolian high schools is to prepare their students for higher education while teaching
them a foreign language at a level that allows them to follow scientific and technological
developments in the world. Anatolian Vocational high schools aim to equip their students
with skills for certain professions and prepare them both for the labor market and higher
education. Anatolian Teacher Training high schools train their students to become teachers
though they can choose other paths as well.
The most prestigious of the exam schools are the Science high schools. These were
established in the mid 1980s to educate the future scientists of Turkey and initially accepted
very few students. Over the next decade, the success of their students on the university
entrance exams, as well as the rigorous education these schools were famous for, created
considerable demand for these schools and they spread throughout the country.
In public high schools, Anatolian high schools and Anatolian Teacher Training high
schools, students can choose between four tracks: the Science track, the Turkish-Math track,
the Social Science track and the Language track. In Science high schools they must take the
10

Figure 2: Education System in Turkey
8th grade

High School Entrance
Exam

Anatolian
Teacher Training
Schools

Non-exam
Schools

TurkishMath
Track

Social
Science
Track

Anatolian
Schools

Science
Track

Language
Track

Science
Schools

Anatolian
Vocational
Schools

Science
Track

11th grade
University Entrance
Exam (ÖSS )

science track. In Anatolian Vocational high schools there are no tracks, which puts them a
little outside the mainstream. All of this is depicted in Figure 2.
After 11th grade, students who wish to pursue higher education take a centralized nationwide university entrance exam (ÖSS), which is conducted by the Student Selection and
Placement Center (ÖSYM). This exam is highly competitive and placement of students into
colleges is based on their ÖSS score, high school grade point average (GPA), and their preferences. For each student a placement score is constructed as a weighted average of the
ÖSS score and the GPA and students choose from schools with cutoffs no higher than their
placement scores.
Below, we use high school and university entrance exam scores to infer the value-added
of schools. For this reason, it is important to explain what these exams consist of and how
similar they are. Both high school and university entrance exams are multiple choice tests
that are held once a year. The high school entrance exam is taken by students at the end

11

of eighth grade. There are four tests, Turkish, social science, math, and science, with 25
questions on each test. Students are given 120 minutes to answer the 100 questions. The
University entrance exam is similar. It is a nationwide central exam with four parts, Turkish,
social science, math, and science, with 45 questions in each part. Students are given 180
minutes. The questions on both exams are based on the school curriculum and are meant
to measure the ability to use the concepts taught in school. To discourage guessing, there is
negative marking for incorrect answers in both exams.

2.2

The Data

The data we use comes from several public sources. To measure students’ academic
performance at the end of high school, we rely on information on the performance of each
school on the university entrance exam from 2002 to 2007. This information is published
by the Student Selection and Placement Center (ÖSYM) and is made available to schools
and families so that they are informed about the standing of each school. The information
includes the number of students who took university entrance exam from each school, as well
as the mean and standard deviation of their scores in each field of the exam.
A student’s performance in the high school entrance exam is seen as a (noisy) measure
of his performance prior to attending high school. We obtained data on the minimum and
maximum scores and on the number of seats in 2001 for each exam high school from the
Ministry of Education’s website.12 The summary statistics for these variables are presented
in Table A.5. We also collected data on the average ÖSS performance of each high school
on each part of the exam in the previous year, 2000, from ÖSYM’s Results booklet for that
year, which is publicly available from their website. This is used as one possible quality
dimension along which schools vary. Additional high school characteristics were collected
from the Ministry of Education’s website (education language, dormitory availability, and
location) and the high schools’ websites (age of the schools). We use this data along with
12

This data was collected using the website http://archive.org/web/web.php, which provides previous
versions of websites.

12

the score distribution of all students who took the high school entrance exam in 2005 (see
Table A.6) in our analysis below. Ideally we would have liked to have this information for
2001, but as this was not available and as these distributions are very stable, we use data
from 2005.
In the next section we show how to use information on the allocation process, seats available, the distribution of scores overall on the high school entrance exam, and the preference
structure to back out the mean valuation placed on each high school.

3

The Model
Seats in public exam schools are allocated according to students’ preferences and their

performance on a centralized exam (conducted once a year). All schools have an identical
ranking over students based on their test scores. Each exam school has a fixed quota,
qj , which is exogenously determined.13 The allocation process basically assigns students to
schools according to their stated preferences, with higher scoring students placed before lower
scoring ones. Students know past cutoffs for schools when they put down their preferences.
They are allowed to put down up to 12 schools.14
We model preferences as follows. Student i’s utility from attending school j takes the
form

Uij (Xj , ξj , εij ; β) = βXj + ξj + εij
where Xj are the observed school characteristics, ξj are the unobserved school characteristics, and εij is a random variable which has a Generalized Extreme Value (GEV) distribution.
13

In general, the seats available are close to the size of the graduating class as schools are capacity
constrained.
14
Students do face a location restriction in listing their Anatolian high school preferences. They are not
allowed to list preferences on Anatolian high schools in Ankara, İstanbul, İzmir, and their current city: they
have to pick one of these locations and make all their Anatolian high school preferences from their chosen
location. However, if preferences are stated after the score is known, and cutoffs are stable over time (as in
our setting) this restriction should not have any impact. A student would put his most preferred school with
a cutoff below his score at the top of his list and be assigned there.

13

Let δj denote the school specific mean valuation where

δj = βXj + ξj
so that

Uij (Xj , ξj , εij , β) = δj + εij
This structure implies that variation in individual preferences comes from the error term,
conditional on the students having the same feasible choice set. If two alternatives are in the
same nest, their errors are allowed to be correlated. Otherwise, the errors are assumed to be
independent.
In general, the cumulative distribution function of ε = hεi0 , εi1 , . . . , εiN i is given by

H(εi0 , εi1 , . . . , εiN ) = exp −

K
X
k=1

X
j∈Bk

exp(−

εij
)
λk

!λk 


(1)

where Bk is the set of alternatives within nest k, K is the number of nests, and λk measures
the degree of independence among the alternatives within nest k (see Train (2009)). As
λk increases, the correlation between alternatives in nest k decreases. If λk is equal to 1,
there is no correlation between alternatives within nest k, whereas as λk goes to 0, there
is perfect correlation among all alternatives in the same nest. In this case, the choice of
alternatives for any individual is driven by the δj component alone so that there is pure
vertical differentiation among schools in a nest.
We partition the set of high schools in Turkey according to their type and location.
Figure 3 shows the nesting structure we adopt. Since we want to allow for vertical and
horizontal differentiation, it makes sense to put similar schools in the same nest. Thus, at
the upper level of the nest, students have seven options: Science high schools, Anatolian
Teacher Training high schools, Anatolian high schools in Ankara, in İstanbul and in İzmir,

14

Figure 3: School Choice in Turkey

Local
Schools

Regular
Public
Schools

Anatolian
Vocational

Anatolian
in Ankara

Anatolian
in Istanbul

Anatolian
in Izmir

Anatolian
Teacher

Science
Schools

Local
Anatolian

Anatolian Vocational high schools, and the local school option. The local school option
for a student includes a local Anatolian school and a public regular high school which is
modeled as the outside option. Since computational intensity will increase with the size of
the choice set, we aggregate Anatolian Vocational high schools into five subgroups according
to their types with seats equal to the sum of seats of schools in that subgroup. We define
the maximum and minimum score of each subgroup as the maximum and minimum of the
cutoff scores of the schools in that subgroup. Other nests include all schools in Turkey of
a given type: 91 Teacher Training high schools, 48 Science high schools, 24 Anatolian high
schools in Ankara, 38 Anatolian high schools in İstanbul, and 18 Anatolian high schools in
İzmir.15 Thus, we have 226 options overall.16
Each student chooses a school that maximizes his utility given his feasible set of schools,
which is determined by his own score, si , and the cutoff scores of each school

max Uij (Xj , ξj , εij ; β)
j∈Fi

where
15

These schools are located in the center of the Ankara, İstanbul, and İzmir. Anatolian high schools
located in a town in the provinces are defined as local Anatolian high schools by Ministry of Education.
16
We ignore private exam high schools as they comprise less than 5% of the total and there is no data on
them.

15

Fi = {j : cj ≤ si }
The feasible set of a student, Fi , includes all the schools whose cutoff score is below the
student’s score. Given the demand for each school and the number of seats available, the
cutoff score, cj , is endogenously determined in equilibrium.
Let the set of N schools be partitioned into K mutually exclusive sets (nests) where the
elements of each of these sets correspond to schools within that nest. For example, Bk , where
k = 1, 2, . . . , K, would have as its elements all schools that are in nest k. If there were no
rationing, the probability that school j in nest k was chosen by student i would be given
by17

!λk −1
δ
exp( λjk )

Pij (δ, λ) =

K
P
n=1

P
l∈Bk

exp( λδkl )
λn


P
l∈Bn

exp( λδnl )

which would be equivalent to the fraction of students whose best alternative was alternative
j.
However, students’ choices are constrained by the cutoff scores in each school, cj , and by
their own exam performance, si . Suppose that there are N + 1 choices (including the outside
option) and let the cutoff scores for each alternative be ordered in ascending order

c0 = 0 < c1 < c2 < . . . < cN −1 < cN
where 0 indexes the outside option. Students whose score is in the interval [cm , cm+1 ) have
the first m schools in their feasible choice set and we call this interval Im . Similarly, students
whose scores are below c1 have scores in interval I0 and have their choice set containing only
the outside option, while students with si ≥ cN get to choose from all the N + 1 alternatives
and have scores in interval IN . Thus, the probability that student i with a score in interval
17

The derivation of the nested logit probability, Pij , can be found in the Appendix A.1

16

Ij chooses school t, t ≤ j, in nest k from his feasible set will be



δ

 exp( λt )
k

Pjt(k) (δ, λ) =





Kj
P

k −1

λ
P
l∈Bk (Ij )

n=1

)
λn




δ
exp( λ l
k

P
l∈Bn (Ij )

δ
exp( λ l )
n

if si ∈ Ij









where bold variables denote vectors and where Bk (Ij ) denotes the restriction placed on
the elements of nest k when the individuals’ score is in the interval Ij . λk is the extent of
independence between alternatives in nest k, and Kj is the total number of nests available
to a student whose score is in interval Ij .
Aggregate demand for each school will thus depend on the distribution of scores, F (s),
the minimum entry cutoff scores of all other schools whose cutoff score is higher, and the
observed and unobserved characteristics of all schools. Using the equilibrium cutoff scores
and the students’ score distribution we can get the density of students that are eligible for
admission to each school.
For simplicity, we will write the demand function for school j in nest s, dj(s) (δ, λ), as
dj(s) . The demand for school N, the best school, which is in nest k comes only from those
in IN :

dN (k) = PN N (k) (δ, λ)[1 − F (cN )]

Only students with scores above cN have the option to be in school N which gives the
term [1 − F (cN )]. In addition, N in nest k has to be their most preferred school; hence the
term PN N (k) (δ, λ). Similarly, the demand for school j which is in nest s comes from those in

17

Ij ,... IN

dj(s) = PN j(s) (δ, λ)[1 − F (cN )]
+P(N −1)j(s) (δ, λ)[F (cN ) − F (cN −1 )]
+.. + P(j)j(s) (δ, λ)[F (cj+1 ) − F (cj )]
=

N
−1
X

Pwj(s) [F (cw+1 ) − F (cw )] + PN j(s) (δ, λ)[1 − F (cN )]

w=j

Students with higher scores have more options open to them which is what makes higher
scores valuable to a student in this setting.

3.1

Estimation Strategy and Results

Given the preference parameters and the number of seats in each school, the real world
cutoffs are determined by setting the demand for seats, as explained above, equal to their supply and obtaining the market clearing score cutoffs. This is not what we will do. For us, the
cutoffs and the number of seats are data. We want to use this data and the nesting structure
imposed to obtain the preference parameters. In particular, we want to estimate the coefficients of school characteristics (β) and the parameter vector λ,where λ = [λ1 , λ2 , . . . , λK ],
that best fit the data and respect the solution of the model that equates demand (d) with
supply (q).
We do this in three steps. In Step 1, we back out the values of δj for each school j for a
given λ. In essence, the minimum cutoff in each school denoted by the vector c = (c1 , ..cN ),
the number of seats in each school denoted by the vector q = (q1 , ..qN ), together with the
market clearing conditions of the model, pin down the mean valuation of each school for a
given vector, λ = (λ1 ,...λK ). In step 2, we find λ so as to best match the extent of overlap in
the scores of schools in the same nest. A higher correlation in the errors within a nest means
that there is less of a role for preference shocks to play in choice, so that preferences are
driven by the non-random terms. This corresponds to having more of a vertical preference
18

structure. As a result there is less overlap in the range of student scores across schools in a
nest. If there is perfect correlation, the maximum score in a worse school will be less than the
minimum score in a better one. In step 3, we relate our estimates of δj to the characteristics
of each school to see what drives the preferences for schools.
We do not use the standard nested logit setup because the cutoff score constrains choice.
Only those students with scores above the cutoff for a school have the option of attending it.
Had we ignored this constraint, we would have obtained biased estimates of δj . For example,
small and selective colleges would be wrongly seen as undesirable.18

3.2

Step 1

Our model includes unobserved school characteristics, and these unobserved characteristics enter the demand function non linearly, which complicates the estimation process.
Berry (1994) proposed a method to transform the demand functions so that unobserved
school characteristics appear as school fixed effects. By normalizing the value of the outside
option to zero, δ0 = 0, we have N demand equations with N unknowns. This permits us to
get the vector δ(q, c, λ) for given vectors q and c, conditional on a vector λ such that

q = d(δ(q, c, λ), λ).
On the left hand side we have supply of seats, and on the right hand side we have the
demand for seats for a given vector of mean school valuations and school cutoffs (denoted
by δ and c respectively) and correlation of shocks within nests (λ). For a given λ, and with
q and c coming from the data, we can invert the above to obtain δ(q, c, λ). Our setup is
more complex than the models presented in Berry (1994) so we cannot solve for δ(q, c, λ)
analytically. Our setup is closer to that in Bresnahan, Stern and Trajtenberg (1997) who
18

There is a growing literature on the structural estimation of matching models that uses data on who is
matched with whom (See Fox (2009)). Since we do not observe all matches and only see the minimum and
maximum scores associated with each school as well as the number of seats, our approach has to differ from
theirs.

19

solve the system numerically as we do. Thus, inverting the demand function numerically
gives us the vector of the mean valuation of the alternatives, δ(q, c, λ).

3.3

Step 2

Once we get δ(q, c, λ), we can specify individual i’s utility from alternative j as

Uij (λ, q, εij ) = δj (q, c, λ) + εij

At this stage, the only unknown in the utility function is the vector λ. As the λ for a nest
falls, the correlation of the utility shock within the nest will increase. In the extreme case,
when the correlation is perfect, if one agent values a particular school highly so do all other
agents, which can be interpreted as pure vertical differentiation. In this case, there will be
no overlap in the score distributions of different schools within the nest. If correlation is
low, then some students will choose one school and others will choose another and there will
be overlap in the score distributions. The extent of overlap in the minimum and maximum
scores of schools that are next to each other in cutoffs within a nest helps to pin down the
λ in the nest.
Figure 4 shows how different values of λ affect the fit of the model to the data for the
Science high school nest. For each λ, the simulated minimum scores lie exactly on top of the
actual minimum scores as depicted in Figure 4, a consequence of our estimation strategy.
The figure shows the actual maximum score and simulated maximum scores for λ = 0.25,
0.5 and 1. Note how the lines move up as λ rises (or correlation falls) so that the extent of
overlap increases.
We pin down λ using a simulation-based approach. The simulation algorithm works as
follows: For a given vector λ, we obtain the vector δ(q, λ) and simulate the minimum and
maximum cutoff scores, cj , and c̄j for each school. Then we find the vector λ that best
¯
matches the actual maximum (and minimum) cutoff scores.

20

Figure 4: Real and Simulated Cutoff Scores for λ = 0.25, 0.5, 1
960

940

Cutoff Scores

920

900

880

Simulation Min (λ=1)
Simulation Min (λ=0.5)
Simulation Min (λ=0.25)
Actual Min
Simulation Max (λ=1)
Simulation Max (λ=0.5)
Simulation Max (λ=0.25)
Actual Max

860

840

820

0

5

10

15

20

25
Schools

30

35

40

45

50

Simulating the error terms in the nested logit model creates some difficulties: taking
a draw from the GEV distribution with the standard Markov Chain Monte Carlo Method
is computationally intensive. We use a method proposed by Cameron and Kim (2001)
which takes a draw from the GEV distribution using a far less computationally intensive
procedure.19
We draw M (= 100) sets of error terms εij from the distribution function given in equation
1 by using the parameters, λ. For each of the M sets of errors drawn, εk = hεkij i, k = 1, ..M,
we allocate students to schools by using the placement rule. After drawing each set of errors
we get a distribution of scores for students in each school. Let gjk be the set of scores in
school j in simulation k, ordered to be increasing.20

gjk (λ) = hskj1 (λ), skj2 (λ), . . . , skjqj (λ)i

19

This method is explained in Appendix A.2.
In the method proposed by Cameron and Kim (2001), a change in λ only affects the coefficients. This
allows us to keep the random seeds drawn from the extreme value distribution over simulations and only
change coefficients.
20

21

After ordering scores in ascending order for each school j and simulation M , we find the
expected value of the score for each rank within each school across the M simulations. The
expected score of student with rank r in school j is thus:

s∗jr (λ) =

M
1 X k
s (λ)
M k=1 jr

Let gj∗ (λ) be
gj∗ (λ) = hs∗j1 (λ), s∗j2 (λ), . . . , s∗jqj (λ)i.
We take the lowest and highest rank mean simulated score in each school. We find the
λ that gives the least square distance between these simulated minimum and maximum
cutoff scores and observed minimum and maximum cutoff scores. In effect, we are matching
the maximum scores as the minimum scores are matched on average given our estimation
procedure for obtaining δ.

λ̂ = arg min
λ

1 X ∗
1 X ∗
(sjqj (λ) − c̄j )2 +
(s (λ) − cj )2
N j
N j j1

Table 1 shows the λ values for each nest that minimize the distance between simulated
and real maximum and minimum cutoff scores. As we mentioned before, λ is a measure of
dissimilarity in preferences within a nest. If λ is small, students rank schools in the same
nest according to their perceived quality (δ) so that students tend to agree on the ranking of
schools. However as λ gets bigger, students differ in their preferences and no such ranking
exists as their tastes for schools differ.
The correlation in shocks is low for vocational, teacher and local schools, suggesting that
preferences are more horizontal there. The correlation is highest in the Izmir, Ankara, and
Istanbul Anatolian high school nests (as λ is lowest). Note that the smaller the city, the
higher the correlation in the city nest, as might be expected.21 Science high schools are also
more vertically differentiated than Local schools, Vocational schools and Teachers schools.
21

Large cities are more likely to have the room for niche schools which are horizontally differentiated.

22

Table 1: Nesting Parameters: λ
Variable
λloc
λvoc
λank
λist
λizm
λteach
λsci

Coefficient
0.958
0.986
0.795
0.837
0.777
0.999
0.897

These findings suggest that students’ preferences are vertical for selective Anatolian and
Science high schools, but less so for less selective vocational, teacher and local schools.
The real and simulated cutoff scores for λ presented in Table 1 are given in Figure 5.
As we can see simulated maximum scores track the real maximum cutoffs quite well. Note
that the actual maximum score is more variable than that estimated one. This comes from
differences in preferences only coming from one source: the error term. This is a consequence
of our lack of information about students. We expect that students have preferences over
school location relative to where they themselves come from. For example, a very good
student may choose a less selective Anatolian School just because it is close. This would
raise the maximum score there above what the model predicts. If we had better information
of this sort on students, we expect that we could do better at matching the maximum score.
Figure 6 depicts the relationship between the perceived valuation and the selectivity of
schools. More selective schools clearly seem to be more valued. Close to the top of the score
distribution a small increase in the score raises utility a lot while a similar increase at low
scores has little effect. In the next step, we investigate the factors affecting the students’
perceived valuations of the schools.

23

Figure 5: Real and Simulated Cutoff Scores
950

850

800

Simulation Min
Actual Min
Simulation Max
Actual Max

750

700

0

50

100

150

200

250

15

Schools

0

δ

5

10

Figure 6: Perceived Valuation w.r.t. Minimum Cutoff Score

−5

Cutoff Scores

900

650

700

750
800
Minimum cutoff score

24

850

900

3.4

Step 3

Once we pin down the λ that gives the best match of the actual and the simulated cutoffs,
we get δ̂(q, λ̂). Returning to the definition of δ, the vector of mean valuations for schools,

δ̂ = βX + ξ
where X is the observed characteristics of the school, and ξ is the school specific component of
mean valuations. ξ is the unobserved, common across all agents, school specific preference
shock. X includes the school’s success on the ÖSS the previous year, its age/experience,
type, education language, dormitory availability, whether it is located in a big city (Ankara,
İstanbul, or İzmir), the number of seats, and the cutoff score of the school. The dummy for
being a Science or Anatolian high school incorporates the possibility that such schools have
a good reputation and this makes people value them. This need not be for what they add
in value: it could be for consumption purposes, perhaps for the bragging rights associated
with going there.
There is an econometric issue associated with including the cutoff score as an explanatory
variable. If ξ, the school specific shock, is large and positive, then the cutoff score will be
high as well, so that the cutoff will be correlated with ξ. This will bias the estimates of
β obtained. This is the familiar endogeneity problem. To deal with this we need a good
instrument for the cutoff score.
We can partition X as [X̃, c] so that

δ̂ = β̃ X̃ + γc + ξ

A good instrument is an exogenous variable that shifts the cutoff score, but does not affect
a school’s average valuation δ directly. The first variable that comes to mind that shifts the
minimum cutoff score is the number of available seats in a school. However, the available
number of seats may affect the valuation of the school directly. In addition, it may be a
25

response to a high ξ which makes it less than optimal. This is less of a concern in Turkey,
where the number of seats is usually equal to the size of the graduating class as the overall
school size is set by the central authority and can be thought of as exogenous. Fortunately,
the model suggests which instruments to use for the minimum cutoff score. Next, we explain
what these are and how we construct them, and then present our results.
The model predicts that the number of available seats in schools worse than a given school
has no effect on the demand for the school. However, the number of seats in better schools
does affect the demand for a school: more seats in better schools is predicted to reduce
the cutoff score of a school. This result comes from the observation that the demand for a
school comes from those who like it the most among the alternatives that are open to them.
Changing the cutoff in worse schools has no effect on the alternatives open to a student going
to a better school and hence on their demand. In other words, if Podunk University offers
more seats, there is no effect on the demand for Harvard since everyone choosing to go to
Harvard had, and continues to have, Podunk in their choice set. But if Harvard offers more
seats, it may well reduce the demand for Podunk University. It could be that someone chose
Podunk because they could not get into Harvard. Once Harvard increases its seats and so
reduces its own cutoff, Harvard may become feasible for such a student. As we use seats in
other schools to instrument for a school’s cutoff we need not worry about any correlation
with ξ.
To construct the instrumental variable, we need a ranking of schools free of ξ. We will
use the schools’ success on the verbal and quantitative part of the ÖSS in the previous year
to rank schools.
We construct our instrumental variable as follows:
1. For each school, we find the schools that have better average test scores in both dimensions, verbal and quantitative.
2. We find the total number of seats in all of the schools found in step 1. The available
number of seats in the school itself is not included.
26

The second set of instruments we use is constructed using a different insight. A large
positive draw of ξ, the school specific demand shock, would raise demand for the school
and so raise both the cutoff or minimum score and the maximum score. As a result, the
residual from the regression of the cutoff score on a flexible form of the maximum score will
be correlated with the minimum cutoff, but orthogonal to the school specific demand shock,
ξ. This makes it a good instrument.22 We thus use the residual of the minimum score on
a polynomial function of maximum cutoff score as an instrument for the minimum cutoff
score23 .
c = λ0 + λ1 c̄ + λ2 c̄2 + λ3 c̄3 + ν
Table 2 shows our first stage estimation:

c = η X̃ + κ1 ∗ Seats in better schools + κ1 ∗ ν + ε

Note that the number of seats in better schools has a negative coefficient: more seats in better
schools reduces the school’s own cutoff as expected. The second instrumental variable, the
residual from the regression of minimum cutoff on a polynomial function of maximum cutoff
score, has a positive coefficient as expected since the minimum score would be increased by
a positive shock as captured by a positive residual.
We also validate the use of the number of seats in better schools. According to the model,
the number of seats in worse schools should have no effect on the school’s own cutoff. Table
A.7 presented in Appendix A.4 shows the first stage estimation with our instruments and
the instrument constructed by using the number of seats in lower scoring schools. Only the
instruments constructed with higher scoring schools and the residual from the regression of
minimum cutoff on a polynomial function of maximum cutoff score are significant. This is
exactly what the model predicts!
22

One might ask what could affect the maximum score and not the minimum score. The score distribution
around the cutoff score of a school does not affect its maximum score but it affects its minimum cutoff score.
23
Hoxby (2000) uses an instrumental variable constructed with similar logic.

27

Table 2: First Stage Estimation

Variable

Coefficients

Number of Available Seats

0.086
(0.062)
Average Quantitative Score in 2000 ÖSS
1.212*
(0.532)
Average Verbal Score in 2000 ÖSS
0.971
(0.653)
Age
0.198
(0.419)
Science High School
57.46***
(14.900)
Teacher High School
45.86**
(16.770)
Anatolian High School in Istanbul
24.340
(18.120)
Anatolian High School in Izmir
19.980
(19.800)
Education Language- English
12.610
(14.220)
Education Language- German
-1.576
(14.030)
Dormitory Availability
12.750
(6.847)
Ankara
26.88*
(13.100)
Istanbul
23.98*
(11.200)
Izmir
26.62*
(12.830)
Seats in better schools
-0.00395*
(0.002)
Residual from min regression
0.718***
(0.089)
Constant
715.8***
(33.170)
Note: Standard errors are reported in parentheses. *, **,
*** indicate significance at the .90, .95 and .99 levels, respectively.

28

Table 3: School Choice: Estimation Results

Variable
Number of Available Seats

(OLS)

(OLS)

0.005
(0.007)
0.218***
(0.053)
0.306***
(0.053)
0.026
(0.062)
8.422***
(1.764)
4.039*
(1.931)
1.928
(2.152)
1.715
(2.280)
2.671
(2.192)
1.198
(2.254)
1.617
(0.893)
4.235**
(1.494)
4.485***
(1.297)
3.746*
(1.449)

0.00842*
(0.004)
0.0680*
(0.033)
0.0865*
(0.038)
0.032
(0.031)
3.237***
(0.765)
0.867
(0.763)
-0.544
(0.728)
0.392
(0.679)
-0.028
(1.118)
0.330
(1.175)
0.500
(0.455)
0.733
(0.485)
1.687**
(0.522)
0.608
(0.381)
0.0846***
(0.006)
-76.18***
(4.363)

(2SLS)

( LIML)

0.007
0.007
(0.004)
(0.004)
Average Quantitative Score in 2000 ÖSS
0.120**
0.121**
(0.036)
(0.037)
Average Verbal Score in 2000 ÖSS
0.163***
0.164***
(0.040)
(0.040)
Age
0.030
0.030
(0.041)
(0.041)
Science High School
5.031***
5.072***
(1.078)
(1.091)
Teacher High School
1.965
1.990
(1.092)
(1.103)
Anatolian High School in Istanbul
0.312
0.331
(1.192)
(1.204)
Anatolian High School in Izmir
0.850
0.860
(1.077)
(1.091)
Education Language- English
0.906
0.927
(1.451)
(1.462)
Education Language- German
0.630
0.637
(1.485)
(1.493)
Dormitory Availability
0.886
0.895
(0.561)
(0.564)
Ankara
1.945*
1.972*
(0.810)
(0.821)
Istanbul
2.655**
2.677**
(0.795)
(0.802)
Izmir
1.694**
1.719**
(0.607)
(0.617)
Minimum Cutoff Score
0.0553*** 0.0547***
(0.008)
(0.008)
Constant
-22.95***
-57.76*** -57.34***
(3.121)
(4.823)
(4.883)
F-statistic (excluded instruments)
37.773
37.773
Note: Corrected standard errors are reported in parentheses. *, **, *** indicate significance at the .90, .95 and .99 levels, respectively.

29

The first column of Table 3 shows our baseline estimates, where we regress average
valuation on the exogenous variables and do not include the minimum cutoff score of a
school. This column suggests that past performance on the university entrance exam (ÖSS
scores) and school type drive preferences. The second column of Table 3 shows the results
of the regression of the average valuation on the exogenous variables and the school’s cutoff
score. The coefficient on the minimum score is positive and highly significant suggesting that
a more selective school is highly valued. The significance of past scores on the university
entrance exam are less significant, as would be expected given that the cutoff is positively
correlated with the past performance of a school so that including it picks up some of this
variation. However, as explained above, cutoffs are not exogenous. As cutoffs are high when
the school preference shocks are high, cutoffs are positively correlated with the error term
which imparts an upward bias to the coefficient. The third and fourth columns show the
results when we instrument for cutoffs. The third column reports the 2SLS estimates, and
the fourth column reports the limited information maximum likelihood (LIML) estimates.
The latter has better small sample properties. It is reassuring that the estimates from both
methods are very similar. In addition, note that after instrumenting for the cutoff score,
the coefficient on it falls (as expected) but remains positive and significant. This suggests
that students value the selectivity of a school and blindly put greater value on more selective
ones. Past performance on the university entrance exam becomes more significant suggesting
that, conditional on the cutoff, a school’s performance on the university entrance exam is
an important determinant of its valuation. Thus, students do look at how well students
graduating, or the output of a school, in forming their valuation of a school. These results
are consistent with the findings of Burgess et al. (2009) and Hastings, Kane, and Staiger
(2009) who reach a similar conclusion using data from the Millennium Cohort Study in the
UK, and school choice data from the Charlotte-Mecklenburg School District, respectively.
Science high schools and schools in Istanbul, Ankara, and Izmir are also valued beyond
what they would be based on their selectivity alone. As mentioned before, Science high

30

schools are very prestigious. It could be that attending such schools gives one contacts in
the future as well as a consumption value in the present.
Macleod and Urquiola (2013) show that a school’s reputation can affect wages as the
identity of the school attended gives information about a student’s ability. This could also
rationalize the high valuation placed on Science high schools. It could also be that the high
valuation of Science high schools comes from the students’ use of school type as a proxy for
school quality. In the next section we look at the value-added of each Science high school by
estimating the effect or the value added of the high school on their students’ performance
on the university entrance exam.

4

High School’s Value-Added
In the previous section, we estimated the preference parameters and recovered the high

school entrance exam scores for students in each school. We allocated students to schools
using the estimated preference parameters and the overall score distribution by simulation.
The goal in this section is to estimate the value-added by a school to the students’ academic
performances. Here we are limited by the data. We do not have a panel, so we cannot match
the score the student obtained on the high school entrance exam to what he obtained on
the university entrance exam. Rather, we infer the effects of schools on student performance
by comparing the mean high school entrance exam (OKS) score to the mean university
entrance exam (ÖSS) scores for each school. We have many years for the latter, but only
one year for the former. We discover patterns that suggest that better schools are resting
on their laurels, while the schools at the bottom are scrambling to improve. However, we
argue that this could be reflecting mean reversion. By using simulation based methods as
well as information on each student in a single school, we estimate the average value-added
by a school while controlling for mean reversion.

31

4.1

The Approach

In this section, we look only at science high school students because their program is
homogeneous since all students follow the science track. Students in these schools will placed
on the basis of a score that gives greater weight to the science and math part of the exam,
the ÖSS-SAY score, which is what we use as the performance measure on the university
entrance exam. We standardize scores by using the mean and the standard deviation of
scores within all Science high schools. Thus a score of −1 means the school is 1 standard
deviation below the mean.
We assume that student i’s high school entrance exam score depends on his ability, αi ,
and his i.i.d. mean zero shock, εhs.
i , and that his university entrance exam score depends
on his ability, the value-added of the school he attended, and the shock to the university
entrance exam score, εcij . Thus
hs
shs
i = αi + εi

and

scij = αi + γj + εcij
|{z}

uj +vic

where j indexes schools and γj is the school value-added. Assume that αi , γj , uj , vic and εhs
i are
independent of each other, and uj and vic which are the school specific and individual specific
components of the university entrance exam score shock, are independently distributed, mean
zero error terms. The school level common shock, uj , is a shock affecting the performance
of all students in the school. We do not observe the individual students’ scores, but only the
school level average scores for the university entrance exam. Thus, aggregating to the school
level in the model above, we get the mean scores in the OKS and ÖSS from school j :

hs
E(shs
i |j) = E(αi |j) + E(εi |j)

32

and

E(scij |j, t) = E (αi |j, t) + γj,t + E(εcij |j, t)
The t is a time index as we have more than a single year’s data on the university entrance
exam. Under the following assumptions, we can get a consistent estimate of the school valueadded, γj,t , by using the data on the performance of the schools over time.
Assumption 1: E(εhs
i |j) = 0
Assumption 1 is a heroic one and is unlikely to hold in the data we have. Students with
better scores, and hence with better shocks to their scores on the high school entrance exam,
get into a better school while those with worse ones do not. As a result, it is to be expected
that the mean scores of students in the best (worst) high schools will look like they have
fallen (risen) in the university entrance exam even if there is actually no value added by any
school. This is the familiar mean reversion issue. All we are saying here is that if Assumption
1 holds, then we can easily estimate value added. If it is grossly untrue then our estimates
will be biased due to mean reversion and we will need to correct for this.
Assumption 2: E(αi |j, t) = E(αi |j) ∀t
Assumption 2 states that students placed in a school have the same ability on average
over time. This is a reasonable assumption in the Turkish system. The cutoff scores are
fairly stable as the educational environment in Turkey has been unchanged over the last few
decades. In Appendix A.3 we present evidence on the stability of cutoff scores.
Assumption 3: γj,t = γj ∀t
Assumption 3 says that school value-added is time-invariant. Assumptions 1-3 imply that
the variation in the performance of a school comes from the shock, ujt , received by that school
in that year.
33

Table 4: Correlation of the Mean OKS score with the Mean ÖSS Scores
Mean Score
OKS
ÖSS 2002
ÖSS 2003
ÖSS 2004
ÖSS 2005
ÖSS 2006
ÖSS 2007

OKS
1
0.6667
0.7239
0.7242
0.7146
0.838
0.811

ÖSS 2002

ÖSS 2003

ÖSS 2004

ÖSS 2005

ÖSS 2006

ÖSS 2007

1
0.8474
0.8816
0.6666
0.7761
0.7902

1
0.8949
0.7522
0.8231
0.8807

1
0.7989
0.863
0.8851

1
0.8027
0.7893

1
0.8664

1

Table 5: Correlation of the Rank of the Mean OKS and the Mean ÖSS Scores
Rank of Mean Score
OKS
ÖSS 2002
ÖSS 2003
ÖSS 2004
ÖSS 2005
ÖSS 2006
ÖSS 2007

OKS
1
0.8341
0.8191
0.8347
0.7438
0.8758
0.851

ÖSS 2002

ÖSS 2003

ÖSS 2004

ÖSS 2005

ÖSS 2006

ÖSS 2007

1
0.7988
0.8228
0.6365
0.8171
0.7983

1
0.8533
0.7479
0.8971
0.8489

1
0.8013
0.8927
0.87

1
0.7968
0.7754

1
0.8825

1

Under these assumptions the average performance of students in school j at time t can
be written as
c
E(scij |j, t) = E(shs
i |j) + γj + E(uj + vi |j, t)

E(scij |j, t) − E(shs
i |j) = γj + E(uj |j, t)
To account for the correlation in the shocks received by a school over time, we cluster
standard errors at the school level.

4.2

Results

Before we present our results, we examine the patterns in the data on the schools’ average
performance on the university and high school entrance exam, to understand the effect of
34

−3

−2

OSS score
−1 0
1

2

Figure 7: Average ÖSS Score by Average OKS Score

−2

−1

0
OKS score
µOSS,2002
µOSS,2003
µOSS,2004
µOSS,2005
µOSS,2006
µOSS,2007
µHS

1

2

linear fit (2002)
linear fit (2003)
linear fit (2004)
linear fit (2005)
linear fit (2006)
linear fit (2007)

noise on the average performance of schools. Looking at the raw patterns in the data we
see that there seems to be a role for ability in the sorting between schools. If ability did
not affect the scores on the high school and the university entrance exams, then allocation
of students to schools would be independent of ability. In that case, the correlation between
the average ÖSS score and the average OKS score would be zero if there is no value-added
by schools. The correlation and rank correlation of the mean OKS and the mean ÖSS scores
are strongly positive as in Table 4 and Table 5. Thus, in the absence of value-added, this is
evidence of sorting on the basis of ability between schools. However, if better schools add
value, then the correlation could be coming from the value-added component and not the
ability sorting one. Hence given our data we cannot separate between these two. We need,
at the very least, information over time for the schools’ mean OKS scores to have a hope
of pinning down a school-level value-added effect. When we plot normalized OKS and ÖSS
scores as in Figure 7, the fitted line is flatter than the 45 degree line, which is in red. This
is exactly what we would see with mean reversion and/or with better more selective high
schools adding less value than less selective ones.
The less the randomness in the OKS score relative to the variation in ability, the more

35

−4

−2

score
0

2

4

Figure 8: Average OKS and ÖSS scores

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46
School
Positive Value−added
µOSS,2002
µOSS,2004
µOSS,2006
µOKS

Negative Value−added
µOSS,2003
µOSS,2005
µOSS,2007

informative is the high school entrance exam score and the lower the extent of mean reversion
bias. If we knew, or could assume something about the this, we might be able to pin down
the value-added by a school.
Figure 8 presents the same data in a slightly different way. It orders schools on the basis
of their cutoffs with School 1 being the least selective one. Thus, the schools are ordered
from worst to best. Each school’s score on the university entrance exam from 2002-2007
as well as the high school entrance exam score in 2001 is plotted. The high schools with
positive and significant value-added are highlighted in blue, while those with significantly
negative value-added are highlighted in red. No highlight means the estimated value-added
is not significantly different from zero.
Note that the worst schools seem to add value on average and the best ones reduce it,
although School 4, one of the worst schools, reduces value. As discussed above, this broad
pattern could be just a reflection of mean reversion. In the next section, by using auxiliary
student level data from a school, we estimate the magnitude of the mean reversion bias and

36

correct for it.

4.3

Mean Reversion Bias

In the previous section we noted that the mean difference in school performance in the
OKS and OSS exams captures both mean reversion and value-added. In this section, by using
some auxiliary student-level data we were able to obtain for only one school, we develop a
way to correct for the bias due to mean reversion. This auxiliary data contains each student’s
name, their high school score and their college entrance exam score.
As in the previous section, we normalize scores within the school so that the mean score
is zero and its standard deviation is 1 on both exams. If the value-added by a school is
assumed to be constant across students as is assumed, then student i’s high school and
university entrance exam scores are given by24
hs
shs
i = αi + εi

sci = αi + vic
where αi is the ability of student i. We assume that αi , vic and εhs.
i are independent of each
other. Students’ scores on the high school entrance exam and university entrance exam differ
from each other only by the difference in the shocks received.
We used the approach introduced in Chay, McEvan and Urquiola (2005) to understand if
there is mean reversion in the data. We look at the ”regression” coefficient relating the score
difference between high school and university entrance exam scores to high school entrance
exam scores.

hs
sci − shs
i = ρsi + ωi
24

As the scores are normalized, value-added is wiped out.

37

Table 6: ”Regression” Coefficient: ρ
ρ
-0.663***
(0.0559)

hs
Cov(sci − shs
Cov(sci , shs
i , si )
i )
=
−1
hs
hs
V ar(si )
V ar(si )
−σε2hs
σα2
=
−1= 2
σα2 + σε2hs
σα + σε2hs

ρ̂ =

If there is no mean reversion, ρ̂ is zero. To build intuition, consider two extreme cases that
show how ρ̂ is related to mean reversion. Firstly, if we assume that high school entrance
exam scores depend only on students’ abilities and there is no noise, then σε2hs = 0 so ρ̂ = 0.
In this case, we don’t expect to see mean reversion bias since there is no randomness in the
high school entrance exam scores. Secondly, if we assume that high school entrance exam
hs
scores are just noise, shs
i = εi , then ability does not affect score variation, which results in

ρ̂ being equal to −1. In this case, the mean reversion bias is at its highest level. Therefore,
ρ̂ can be thought as showing how severe the mean reversion is.
We observe students’ scores on the high school and university entrance exams, so we can
run the following regression to estimate ρ :

hs
sci − shs
i = ρsi + ωi

Table 6 shows the estimate of ρ. As a result of our normalization process the variance
of each of the scores is unity. As ability and shocks are orthogonal, σα2 + σε2hs = 1. So we
can recover the variance of the error distribution and the variance of the ability distribution
from our estimate of ρ.

38

−2

−1

Score
0

1

2

Figure 9: Mean Reversion Bias: εhs ∼ N (0,−ρ̂), εhs ∼ N (0,−ρ̂ ± 2σ−ρ̂ )

1

3
2

5
4

7
6

9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45
8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46
School
µOSS if ε ∼ N(0, −ρ−2σ−ρ)
HS
µOSS if ε ∼ N(0, −ρ)
HS
µOSS if ε ∼ N(0, −ρ+2σ−ρ)
Simulated µOKS
Actual µOKS
HS

σε2hs = −ρ̂
σα2 = 1 + ρ̂
The estimate of ρ is quite large in absolute terms: noise accounts for 66% of the variance
in the OKS score.
In our system, the allocation rule of students to schools is known, and in the previous
section, we estimated students’ preferences over high schools. We can recover the average
ability and shock received by students in each school by making a parametric assumption
on the distribution of ability and noise on the high school entrance exam. We will assume
that the ability has a normal distribution with the mean equal to zero and the variance
equal to (1 + ρ̂). Similarly the distribution of the error term, εhs
i , is normal with mean equal
to zero and variance −ρ̂. Under these assumptions, we generate high school entrance exam
scores and allocate students to schools based on the estimated preferences. This gives the

39

−4

−2

score
0

2

4

Figure 10: Value-added: εhs ∼ N (0,−ρ̂)

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46
School
Positive Value−added
µOSS,2002
µOSS,2004
µOSS,2006
µOKS−E(εOKS)

Negative Value−added
µOSS,2003
µOSS,2005
µOSS,2007

orange line (connecting the dots) in Figure 9. We also present the mean scores on university
entrance exams for this simulated allocation of students to schools when there is no value2
added for different levels of variance in εhs
i , i.e., σεhs . The middle curve corresponds to the

university entrance exam score with σε2hs = −ρ̂. The ones above and below it correspond to
the simulations where σε2hs is set at the 95% bands. In addition, we present the actual mean
scores on the high school entrance exam to see if the simulated mean scores deviate from the
actual ones. It is comforting to see that they look remarkably similar. They differ slightly
for the most and least selective schools.
It is worth noting that preferences also affect the extent of mean reversion bias: the more
vertical the preferences, the more the bias. With purely horizontal preferences, students who
get lucky in their high school entrance exam performance are less likely to end up in the
more selective schools reducing the extent of mean reversion bias.
Now we adjust for the mean reversion bias in our estimates by adding E(εhs
i |j) to our
estimates of value added to correct for mean reversion. In the figures we also depict µOKS −
E(εOKS ) which is the mean high school entrance exam score adjusted for the mean reversion
40

−4

−2

score
0

2

4

Figure 11: Value-added: εhs ∼ N (0,−ρ̂ + 2σ−ρ̂ )

1

3
2

5
4

7
6

9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45
8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46
School
Positive Value−added
µOSS,2002
µOSS,2004
µOSS,2006
µOKS−E(εOKS)

Negative Value−added
µOSS,2003
µOSS,2005
µOSS,2007

−4

−2

score
0

2

4

Figure 12: Value-added: εhs ∼ N (0,−ρ̂ − 2σ−ρ̂ )

1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46
School
Positive Value−added
µOSS,2002
µOSS,2004
µOSS,2006
µOKS−E(εOKS)

41

Negative Value−added
µOSS,2003
µOSS,2005
µOSS,2007

bias which rises far less slowly than the unadjusted one. Figure 10 shows the schools’
value-added estimates when we correct for the mean reversion bias. There is no particular
pattern in value-added estimates according to selectivity. The most selective schools do
not seem to have a positive effect on their students’ test scores. However, it is also clear
that some schools, such as schools 13, 29 and 35, improve their students scores, while others
have negative value-added, such as schools 4, 11, 33 and 45. Figures 11 and 12 do the
same thing but allow for higher and lower levels of variance in εhs
i respectively as defined
by the confidence intervals above. As can be seen, with higher variance, there is more mean
reversion to correct for so that more schools on the right add value and more on the left
reduce value. With lower variance, we get the opposite happening.
These results show that there is reason to think that the circular causation hypothesis
has some merit. Although better schools do not seem to have any significant effect on their
students’ test scores, students act like they do! It is also important to note that we are
only investigating the effect of exam schools on academic achievement. However, students
attending exam schools may have other benefits that are valuable to them, but unobservable
to us.25

5

Conclusion
Schools are hard to evaluate in the real world. Unlike most experience goods, where

consumers can know how much they like the good upon consuming it, with schooling, liking
the experience is only part of what people care about. They care about attributes, like
reputation or selectivity that might signal something, as well as the value-added by the
teaching in the school. Since consumers are unlikely to have information about the latter,
even if they have information about the former, information frictions are likely to be rampant
in this market. This may well result in the market working poorly. Schools with high value25

Alstadsæter (2011), and Jacob, McCall, and Stange (2011) show the importance of the role of consumption value in students’ school choices in different contexts.

42

added may thus be ranked below those that are adding little value but are very selective.
School choice programs are thought to increase the productivity of public schools by
encouraging competition in the market. Just like firms producing better products can charge
a higher price for them, it is tempting to think of schools competing in their products with
good schools delivering a better product, i.e., adding more value to their students, and
as a result being more selective and having greater status. However, as argued above,
quality is hard to infer in this market. As a result, the market may work poorly if quality
information is not made available. In this paper, we use data available from public sources
to show that, indeed, consumers value academic success on the university entrance exam,
the selectivity of the school, elite school status and location. However, what people like and
value-added are not related. Our results suggest it is hard to acquire information on the
quality of the product by the schools so that families/students cannot infer the quality of
a school. Therefore providing better information on value-added by a school, rather than
just information on the performance of its students is essential to the market working well
in this area. Elite schools seem to get better students because everyone wants to go to them,
even when they need not add value to the students in terms of their performance on the
university entrance exam. This may also be because of signaling and/or the consumption
value of going to such schools: bragging rights or networks formed in such schools that are
of value later. In this case, especially because better-off students are more likely to be able
to get into such schools, it is hard to defend the subsidies received by elite schools.
Finally, our results illustrate the value of taking a structural, model based approach.
First, as is well understood, by using the model, one can do more with less data. Second,
even if we had better data, we would still need to correct for much of what we describe
above. For example, if we had data at the student level on high school and university entrance exams, just looking at the difference in student performance by school would not give
a bias free estimate of value added. Mean reversion as above would still be an issue. Its
extent depends on the signal used and the extent of noise in the signal as explained above.

43

If U.S. schools use a host of factors in deciding on their admissions, not just high school performance or SAT scores, the noise in their admissions could rise worsening mean reversion
bias. However, if preferences are horizontal more than vertical, as may well be the case in
a large country like the U.S. where schools find a niche for themselves, the extent of mean
reversion bias could be lower. Thus, preferences, the allocation system, and the strength of
the signal present in the scores are critical inputs when developing measures of value added.
They can only be obtained by taking a structural approach.

44

References
Abdulkadiroglu, A., Angrist, J. D., and Pathak, P. A. (2011). The elite illusion: Achievement
effects at Boston and New York exam schools (No. 17264). National Bureau of Economic
Research.
Alstadsæter, A. (2011). Measuring the consumption value of higher education. CESifo Economic Studies, 57(3), 458-479.
Berry, S. T. (1994). Estimating discrete-choice models of product differentiation. The RAND
Journal of Economics, 242-262.
Berry, S., Levinsohn, J., and Pakes, A. (1995). Automobile prices in market equilibrium.
Econometrica, 841-890.
Bresnahan, T. F., Stern, S., and Trajtenberg, M. (1997). Market Segmentation and the
Sources of Rents from Innovation: Personal Computers in the Late 1980s. RAND Journal of
Economics, S17-S44.
Burgess, S., Greaves, E., Vignoles, A., and Wilson, D. (2009). What parents want: School
preferences and school choice. CMPO.
Cameron, A. C., and Kim, N. (2001). Simulation Methods for Nested Logit Models. Department of Economics, University of California, Davis, California.
Caner, A., and Okten, C. (2013). Higher education in Turkey: Subsidizing the rich or the
poor?. Economics of Education Review.
Chay, K. Y., McEwan, P. J., and Urquiola, M. (2005). The Central Role of Noise in Evaluating Interventions That Use Test Scores to Rank Schools. American Economic Review,
1237-1258.
Clark, D. (2010). Selective schools and academic achievement. The BE Journal of Economic
Analysis & Policy, 10(1)
45

Cullen, J. B., Jacob, B. A., and Levitt, S. D. (2005). The impact of school choice on student
outcomes: an analysis of the Chicago Public Schools. Journal of Public Economics, 89(5),
729-760.
Cullen, J. B., Jacob, B. A., and Levitt, S. (2006). The effect of school choice on participants:
Evidence from randomized lotteries. Econometrica, 74(5), 1191-1230.
Dale, S. B., and Krueger, A. B. (2002). Estimating the payoff to attending a more selective
college: An application of selection on observables and unobservables. The Quarterly Journal
of Economics, 117(4), 1491-1527.
Dale, S., and Krueger, A. B. (2011). Estimating the return to college selectivity over the career using administrative earnings data (No. 17159). National Bureau of Economic Research.
Darling-Hammond, L., Amrein-Beardsley, A., Haertel, E., and Rothstein, J. (2012). Evaluating teacher evaluation. Phi Delta Kappan, 93(6), 8-15.
Ding, W., and Lehrer, S. F. (2007). Do peers affect student achievement in China’s secondary
schools?. The Review of Economics and Statistics, 89(2), 300-312.
Dobbie, W., and Fryer Jr, R. G. (2011). Exam high schools and academic achievement:
Evidence from New York City (No. 17286). National Bureau of Economic Research.
Duflo, E., Dupas, P., and Kremer, M. (2006). Peer Effects, Teacher Incentives, and the
Impact of Tracking: Evidence from a Randomized Evaluation in Kenya. American Economic
Review, 101(5), 1739-74.
Epple, D., and Romano, R. (2011). Peer effects in education: A survey of the theory and
evidence. Handbook of Social Economics, 1(11), 1053-1163.
European Commission, (2009/2010). Organization of the Education System in Turkey.

46

Frisancho Robles, V., and Krishna, K. (2012). Affirmative Action in Higher Education in
India: Targeting, Catch Up, and Mismatch (No. 17727). National Bureau of Economic Research.
Fox, J. T. (2009). Structural Empirical Work Using Matching Models. In S. N. Durlauf and
L. E. Blume (Eds.), New Palgrave Dictionary of Economics (Online ed.).
Hanushek, E. A., Kain, J. F., Markman, J. M., and Rivkin, S. G. (2003). Does peer ability
affect student achievement?. Journal of Applied Econometrics, 18(5), 527-544.
Hastings, J. S., Kane, T., and Staiger, D. (2009). Heterogeneous preferences and the efficacy
of public school choice. NBER Working Paper, No. 12145 and No. 11805.
Heckman, J. J. (1979). Sample selection bias as a specification error. Econometrica: Journal
of the econometric society, 153-161.
Hoxby, C. (2000). Peer effects in the classroom: Learning from gender and race variation
(No. 7867). National Bureau of Economic Research.
Hoxby, C. M. (2000). The effects of class size on student achievement: New evidence from
population variation. The Quarterly Journal of Economics, 115(4), 1239-1285.
Jackson, C. K. (2010). Do Students Benefit from Attending Better Schools? Evidence from
Rule-based Student Assignments in Trinidad and Tobago. The Economic Journal, 120(549),
1399-1429.
Jacob, B., McCall, B., and Stange, K. (2011). The consumption value of postsecondary
education. Working Paper
Kang, C. (2007). Classroom peer effects and academic achievement: Quasi-randomization
evidence from South Korea. Journal of Urban Economics, 61(3), 458-495.
Krishna, K., and Tarasov, A. (2013). Affirmative Action: One Size Does Not Fit All (No.
19546). National Bureau of Economic Research.
47

Macleod, W. B., and Urquiola, M. (2013). Anti-Lemons:

Reputation and Educa-

tional Quality. Working paper. Retrieved from http://www.columbia.edu/~msu2101/
MacLeod-Urquiola(2013).pdf
Pop-Eleches, C., and Urquiola, M. (2013). Going to a better school: Effects and Behavioral
Responses. American Economic Review, 103(4): 1289-1324.
ÖSYM (2002). Ortaöğretim Kurumlarına Göre 2002 Öğrenci Seçme Sınavı Sonuçları Kitabı.
Train, K. (2009). Discrete choice methods with simulation. Cambridge University Press.
Zabel, J. E. (2008). The Impact of Peer Effects on Student Outcomes in New York City
Public Schools. Education Finance and Policy, 3(2), 197-249.
Zimmerman, D. J. (2003). Peer effects in academic outcomes: Evidence from a natural
experiment. Review of Economics and Statistics, 85(1), 9-23.

48

A

Appendix

A.1

The Nested Logit Model

Suppose that individual i’s choice set, C, contains N + 1 alternatives. These alternatives
are partitioned into K nests according to certain characteristics. Therefore we can write the
choice set as:
C = {B1 , B2 , . . . , Bk }
Let utility of the individual i from alternative j in nest k be

Uij = δkj + εij

where δkj is the mean valuation of the alternative j. We can decompose δkj as:

δkj = Wk + Vj

where Wk is the valuation related only to the nest characteristics and Vj is the valuation
related to alternative j’s attributes.
Let λk be the scale parameter of nest k, which is inversely related to the correlation of
error terms within nest k.
The probability alternative j is chosen conditional on nest k being chosen is given by:
V

exp( λkj )
P (j|Bk ) = P
exp( λVkl )
l∈Bk

The probability of nest k being chosen depends on the nest characteristics Wk , and
inclusive value Ik , which depends on all the alternatives in the nest k.

P (Bk ) =

X
exp(Wk + λk Ik )
Vl
where
I
=
log(
exp(
))
k
K
P
λ
k
l∈Bk
exp(Wn + λn In )
n=1

49

We can write P (j) as:

P (j) = P (j|Bk )P (Bk )
V

exp( λkj )
exp(Wk + λk Ik )
= P
Vl P
K
exp( λk )
exp(Wn + λn In )
l∈Bk
n=1

Replace Ik by log(

P

l∈Bk

exp( λVkl ))

P (j) =

=

Multiply both sides by

P
exp(Wk + λk log(
exp( λVkl )))
Vj
exp( λk )
l∈Bk
P
Vl P
K
P
exp( λk )
exp(Wn + λn log(
exp( λVnl )))
l∈Bk
n=1
l∈Bn
P
exp( λVkl ))λk
(exp(Wk ))(
Vj
exp( λk )
l∈Bk
P
Vl P
K
P
exp( λk )
(exp(Wn ))(
exp( λVnl ))λn
l∈Bk
n=1
l∈Bn
Wk
λk
W
exp( λ k
k

exp(

)
)

:
V

P (j) =

k
)
exp( W
λk
k
)
exp( W
λk

(exp(Wk ))(exp( λkj ))(

l∈Bk

K
P

(exp(Wn ))(

P

n=1

=

k
exp( W
)
λk
Wk
exp( λk )

K
P

n λn
(exp( W
) )(
λn

n=1

K
P

+

n λn
(exp( W
) )(
λn

P

l∈Bn

δ

(exp( λkjk ))(
P (j) =

exp( λVnl ))λn

P

l∈Bk

K
P

(

P

n=1 l∈Bn

50

P
l∈Bn

P
Wk
))(
λk
l∈Bk

n=1

Therefore

exp( λVkl ))λk −1

l∈Bn
P
Vj
W k λk
(exp( λk ) )(exp( λk ))(
l∈Bk

V
k λk −1
exp( W
)
(exp( λkj
λk

=

P

exp( λVkl ))λk −1

exp( λVnl ))λn

exp( λVkl ))λk −1

exp( λVnl ))λn

exp( δλklk ))λk −1

exp( δλnln ))λn

A.2

Cameron and Kim (2001)

Suppose that ε1 and ε2 are jointly distributed with bivariate extreme value distribution

 
ε2  λ
ε1
H(ε1 , ε2 ) = exp − exp(− ) + exp(− )
λ
λ
Cameron and Kim (2001) propose that

ε1 = aξ + bv1 + c
ε2 = aξ + bv2 + c
where ξ, v1 , v2 are independently distributed with extreme value distribution, and a, b
and c are the weights that match the moments of extreme value distribution.

E(εi ) = E(aξ + bv1 + c) = aγ + bγ + c = γ
V ar(εi ) = a2

π2
π2
π2
+ b2
=
6
6
6

Corr(ε1 , ε2 ) = [1 − λ2 ] =
This results in
a=
b=

√
√

1 − λ2

1 − a2

c = (1 − a − b)γ
where γ is the Euler constant.

51

a2
a2 + b2

Table A.1: Correlation in Minimum Cutoff Scores
Min Score 2000 2001 2002 2003
2004
2000
1.00 0.97 0.97 0.96
0.96
2001
0.97 1.00 0.97 0.96
0.95
2002
0.97 0.97 1.00 0.98
0.97
2003
0.96 0.96 0.98 1.00
0.98
2004
0.96 0.95 0.97 0.98
1.00
Source: Science and Anatolian high school’s cutoff
scores from 2000 - 2004 from the Ministry of Education
website

This method is generalized to the multivariate extreme value distribution,

H(εi0 , εi1 , . . . , εiN ) = exp −

K
X
k=1

X
j∈Bk

exp(−

εij
)
λk

!λk 


such that
εj = ak ξ + bk vj + ck
where
ak =

A.3

q
q
1 − λ2k , bk = 1 − a2k , ck = (1 − ak − bk )γ

Stability of Exam Schools’ Cutoff Scores

The following tables show the correlation of cutoff scores over the five year period from
2000 to 2004. As Tables A.1 and A.2 show the correlation between minimum cutoff scores
over the years is never less than 0.95. The correlation between maximum cutoff scores is
lower than between minimum cutoff scores, but it is still around 0.8. Similarly we also look at
how the ranks of schools with respect to their minimum and maximum scores are correlated
over time. Table A.3 shows the correlation in rank of schools’ minimum cutoff scores over
the five year period. Similarly, Table A.4 shows the corresponding table for the maximum
cutoff scores. These tables show that exam schools’ cutoff scores are stable in Turkey.

52

Table A.2: Correlation in Maximum Cutoff Scores
Max Score 2000 2001 2002 2003
2004
2000
1.00 0.82 0.83 0.83
0.82
2001
0.82 1.00 0.80 0.82
0.78
2002
0.83 0.80 1.00 0.87
0.85
2003
0.83 0.82 0.87 1.00
0.86
2004
0.82 0.78 0.85 0.86
1.00
Source: Science and Anatolian high school’s cutoff
scores from 2000 - 2004 from the Ministry of Education
website

Table A.3: Correlation in Rank of Minimum Cutoff Scores
Rank of Min Score 2000 2001 2002 2003
2004
2000
1.000 0.953 0.946 0.943
0.946
2001
0.953 1.000 0.973 0.969
0.968
2002
0.946 0.973 1.000 0.985
0.979
2003
0.943 0.969 0.985 1.000
0.979
2004
0.946 0.968 0.979 0.979
1.000
Source: Science and Anatolian high school’s cutoff scores from
2000 - 2004 from the Ministry of Education website

Table A.4: Correlation in Rank of Maximum Cutoff Scores
Rank of Max Score 2000 2001 2002 2003
2004
2000
1.000 0.785 0.800 0.793
0.771
2001
0.785 1.000 0.829 0.837
0.798
2002
0.800 0.829 1.000 0.858
0.838
2003
0.793 0.837 0.858 1.000
0.847
2004
0.771 0.798 0.838 0.847
1.000
Source: Science and Anatolian high school’s cutoff scores from
2000 - 2004 from the Ministry of Education website

53

Table A.5: Descriptive Statistics: High School Entrance Exam
Variable

Obs

Mean

Std.Dev.

Min

Max

Anatolian High Schools in Ankara
Number of Available Seats

24

85.000

49.782

30

240

Minimum Cutoff Score

24

813.573

30.792

768.819

872.254

Maximum Cutoff Score

24

859.001

21.543

825.171

912.31

Age

24

10.292

6.182

5

30

Average Math Score in 2000 ÖSS*

17

29.071

3.598

23.07

34.84

Average Science Score in 2000 ÖSS*

17

18.425

6.691

3.1

28.41

Average Turkish Score in 2000 ÖSS*

17

34.656

1.857

31.35

37.81

Average Social Science Score in 2000 ÖSS*

17

25.920

2.477

21.78

30.21

Language offered: English

24

0.792

0.415

0

1

Language offered: German

24

0.167

0.381

0

1

Language offered: French

24

0.042

0.204

0

1

Dormitory Availability

24

0.167

0.381

0

1

Anatolian High Schools in Istanbul
Number of Available Seats

38

100.658

48.186

30

240

Minimum Cutoff Score

38

827.916

41.686

654.059

898.332

Maximum Cutoff Score

38

874.426

23.135

830.076

933.735

Age

38

10.105

6.501

1

26

Average Math Score in 2000 ÖSS*

23

29.201

4.152

18.72

37.61

Average Science Score in 2000 ÖSS*

23

19.553

4.568

11.85

32.48

Average Turkish Score in 2000 ÖSS*

23

35.819

2.524

29.19

41.05

Average Social Science Score in 2000 ÖSS*

23

26.359

3.446

20.83

34.74

Language offered: English

38

0.763

0.431

0

1

Language offered: German

38

0.184

0.393

0

1

Language offered: French

38

0.053

0.226

0

1

Dormitory Availability

38

0.184

0.393

0

1

Anatolian High Schools in Izmir
Number of Available Seats

18

90.000

63.431

30

300

Minimum Cutoff Score

18

810.994

32.805

762.369

878.236

Maximum Cutoff Score

18

868.863

30.033

818.16

915.172

Age

18

14.500

16.111

1

48

Average Math Score in 2000 ÖSS*

12

26.553

6.916

12.61

31.03

Average Science Score in 2000 ÖSS*

12

17.968

4.550

9.45

22.17

Average Turkish Score in 2000 ÖSS*

12

33.875

4.792

24.02

37.48

Average Social Science Score in 2000 ÖSS*

12

25.014

5.621

14.59

33.3779

Language offered: English

18

0.556

0.511

0

1

Language offered: German

18

0.278

0.461

0

1

(continued on next page)

54

Variable

Obs

Mean

Std.Dev.

Min

Max

Language offered: French

18

0.167

0.383

0

1

Dormitory Availability

18

0.278

0.461

0

1

Science High Schools
Number of Available Seats

48

83.000

21.556

48

96

Minimum Cutoff Score

48

878.010

18.120

837.949

920.268

Maximum Cutoff Score

48

910.355

14.484

879.825

941.566

Age

48

8.250

6.380

1

38

Average Math Score in 2000 ÖSS*

38

37.270

2.652

29.22

41.4

Average Science Score in 2000 ÖSS*

38

32.973

3.919

22.54

39.59

Average Turkish Score in 2000 ÖSS*

38

35.945

3.908

26.07

41.35

Average Social Science Score in 2000 ÖSS*

38

27.843

6.209

12.92

38.06

Language offered: English

48

1

0

1

1

Language offered: German

48

0

0

0

0

Language offered: French

48

0

0

0

0

Dormitory Availability

48

1

0

1

1

Anatolian Teacher Training High Schools
Number of Available Seats

91

56.703

21.322

24

120

Minimum Cutoff Score

91

798.716

35.943

712.758

864.296

Maximum Cutoff Score

91

864.419

16.861

827.49

902.864

Age

91

8.571

3.763

1

12

Average Math Score in 2000 ÖSS*

71

15.401

4.704

5.2

27.65

Average Science Score in 2000 ÖSS*

71

9.745

3.340

2.26

18.35

Average Turkish Score in 2000 ÖSS*

71

31.726

3.405

22.65

37.87

Average Social Science Score in 2000 ÖSS*

71

23.476

3.483

10.81

30.19

Language offered: English

91

1

0

1

1

Language offered: German

91

0

0

0

0

Language offered: French

91

0

0

0

0

Dormitory Availability

91

0.846

0.363

0

1

* : The differences in the number of observations across variables comes from some schools
being new so that there are no students graduating in 2000.

Table A.6: Descriptive Statistics: High School Entrance Exam Scores
Quantiles
Variable
OKS Score

Number of Students

Mean

Std.Dev.

Min

0.25

Median

0.75

Max

553495

592.35

86.34

442.53

526.76

572.83

637.44

941.49

55

Table A.7: Validity Check: Instrumental Variables

Variable

Coefficients

Number of Available Seats

0.083
(0.061)
Average Quantitative Score in 2000 ÖSS
0.631
(0.750)
Average Verbal Score in 2000 ÖSS
-0.071
(1.212)
Age
0.166
(0.420)
Science High School
58.23***
(14.450)
Teacher High School
45.77**
(16.310)
Anatolian High School in Istanbul
23.990
(17.990)
Anatolian High School in Izmir
18.230
(19.210)
Education Language- English
11.720
(13.900)
Education Language- German
-2.157
(13.680)
Dormitory Availability
12.360
(6.808)
Ankara
26.90*
(12.590)
Istanbul
23.89*
(11.640)
Izmir
27.06*
(12.300)
Instrument for Minimum Score (with better schools)
-0.00465*
(0.002)
Instrument for Minimum Score (with worse schools)
0.002
(0.002)
Residual from min regression
0.720***
(0.090)
Constant
756.5***
(52.540)
Note: Standard errors are reported in parentheses. *, **, *** indicate significance at the .90,
.95 and .99 levels, respectively.

56

Figure A.1: Model Fit: Science High Schools Nest
960

940

900

880

860
Simulation Min
Actual Min
Simulation Max
Actual Max

840

820

0

10

20

30

40

50

Schools

Figure A.2: Model Fit: Teacher High Schools Nest
950

900

Cutoff Scores

Cutoff Scores

920

850

800

Simulation Min
Actual Min
Simulation Max
Actual Max

750

700

0

20

40

60
Schools

57

80

100

Figure A.3: Model Fit: Ankara Anatolian High Schools Nest
960
Simulation Min
Actual Min
Simulation Max
Actual Max

940
920

Cutoff Scores

900
880
860
840
820
800
780
760

0

5

10

15
Schools

20

25

30

Figure A.4: Model Fit: Istanbul Anatolian High Schools Nest
960
940
920

Cutoff Scores

900
880
860
840
820
Simulation Min
Actual Min
Simulation Max
Actual Max

800
780
760

0

5

10

15

20
Schools

58

25

30

35

40

Figure A.5: Model Fit: Izmir Anatolian High Schools Nest
960
Simulation Min
Actual Min
Simulation Max
Actual Max

940
920

Cutoff Scores

900
880
860
840
820
800
780
760

0

5

10

15
Schools

59

20

25

