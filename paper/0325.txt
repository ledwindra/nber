NBER WORKING PAPER SERIES

Analysis

of

Covariance with Qualitative Data

Gary Chamberlain

Working Paper No. 325

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge MA 02138
March 1979

The research reported here is part of the NBER's research
program in Labor Economics. Any opinions expressed are
those of the author and not those of the National Bureau
of Economic Research. Financial Support was provided by
the National Science Foundation (Grant No. SOC77'—l562').

NBER Working Paper 325
March 1979

Analysis of Covariance with Qualitative Data
ABSTRACT

In data with a group structure, incidental parameters are

to control for missing variables. Applications include
longitudinal data and sibling data. In general, the joint maxincluded

imurn likelihood estimator of the structural parameters is not
consistent as the number of groups

increases,

of observations per group. Instead

with

a fixed number

a conditional likelihood

function is maximized, conditional on sufficient statistics for

the incidental parameters. In

tional

the logit case, a standard condi-

logit program can be used. Another solution is a random

effects rwdel, in which the distribution of the incidental parameters may depend upon the exogenous variables.

Gary Chamberlain
Department of Economics
Harvard University
Littauer Center

Cambridge, MA 02138
6l7/L95_3203

ANALYSIS OF COVARIANCE WITH QUALITATIVE DATA
by
Gary Chamberlain
Harvard University

1. Introduction
This paper deals with data that has a group structure. A simple
example in the context of a linear regression model is

E(yjtlx, , ct) =

'x

+ c (i=l..

.

., N;

t=l, .

. ., T),

where there are T observations within each of N groups. The

are group

specific parameters. Our primary concern is with the estimation of ,

a parameter vector conunon to all groups. The role of the

is to control

for group specific effects; i.e., for omitted variables that are constant

within a group. The regression function that does not condition on the
group will not in general flentlfy :

E(y1Jx
In this case there is an omitted variable bias.

An important application is generated by longitudinal or panel data,

in which there are two or more observations on each individual. Then the
group is the individual, and the c capture iiicH.vidual differences. If
these person effects are correlated with x, then a regression function that

fails to control for them will not identify 3. In another important application
the group is a family, with observations on two or more siblings within the
family. Then the ct. capture omitted variables that are family specific,
and t1iy give a concrete representation 10 family background.

We shall assume that observations from different groups are independent.

2

Then the c. are incidental parameters (Neyman and Scott 13111, and ,

which is common to the independent sampling units, is a vector of structural
parameters. In the application to sibling data, T is small,

typically

T=2, whereas there may be a large number of families. Small T and large N
are also characteristic of many of the currently available longitudinal

data sets. So a basic statistical issue is to develop an estimator for 3
that

has

good properties in this case. In particular, the estimator ought

to be consistent as N -

for

fixed T.

It is well—known that analysis of covariance in the linear regression

model does have this consistency property. The problem of finding consistent
estimators in other models is non—trivial, however, since the number of

incidental parameters is increasing with sample size. We shall work with

the following probability model: y. is a binary variable with

Prob(y =
where F( )

is

lix, '

=

+

ci.),

a cumulative distribution function such as a unit normal or

a logistic. For example, y may indicate labor force participation,
unemployment, job change, marital status, health status, or a college

degree. Section 2 considers maximum likelihood (ML) estimation of the fixed
effects version of this model. A simple algorithm is available which
involves a weighted analysis of covariance at each iteration. The ML
estimator of

is not consistent (for fixed T), however, and we present a

simple example with T=2 in which the I'IL estimator of

converges to 2.

Section 3 presents one solution to this problem by working with a

conditional likelihood function that conditios on sufficient statistics
for the incidental parameters. This likelihood function does not depend
upon the incidental paraneters, and hence standard asymptotic theory for

maximum likelihood estimation applies. This approach is applied to a

3

multinomial logit model for grouped data and to the inultivariate log—linear

probability model. Section 4 develops an alternative approach, based on
a random effects model in which the incidental parameters are assumed to

follow a distribution. The important point here is that the distribution

of the c is not assumed to be independent of x; otherwise the problem of
omitted variable bias would be assumed away from the beginning. Throughout
the paper we shall use the familiar linear regression case to guide the
exposition.

2. Fixed Effects: Maximization of the Joint Likelihood Function
We shall begin with a brief review of the linear regression case.
Let

+ a. +

=
where

is i.i.d. N(O,

So in addition to assuming independence across

the groups, we are assuming that observations within a group are independent

as well, conditional on the group effects. The dependence of different
observations within a group is assumed to be due to their common dependence

on the group specific aj. More general forms of dependence are, of course,
possible; for example, there could be serial correlation in addition to

the c in the longitudinal case.
Maximum likelihood for this model is simply a multiple regression of

y on x and a set of group indicator dummy variables. A useful computational
simplification is that the ML estimator of

can be obtained from a

regression of y—y. on iti' where y. and . are group means
In the case of T=2, this is equivalent to a regression of y.2—y11 on

x —x. .
..i2 -.il

Since we have

i2 — y11

=

i2 il
—

+

—

£11,

Iy1/T).

4

with the 's independent of x, it is clear this provides a consistent
(provided that there is sufficient variation in

as N -

estimator of

-

There is a comparable computational simplification for the probability
models. We shall discuss ML estimation using either a Newton—Raphson or
a scoring algorithm, and shall show that each iteration reduces to a weighted

analysis of covariance. The binary y are assumed to be independent
(conditional on x, , and a) both between and within groups, with

lix, ,

Prob(yi =

a) =

F('x.it + a.).
1

Let O'z.
=
-'it

'x.- it ÷ a..1
-

Then

the log—likelihood function is

L =

it

hF(6'z. )

E

i,t

Note that if y

= 1 for all t

it

0 for all t

y it

+ (1

then the

lnIl

-

—

F(Ezj)]}.

then the ML estimate ofia is °,

ML estimate of a is —no.
i

and if

Hence the observations

on such groups do not affect the ML estimate of , and we can simplify by
only including in L the groups within which y varies.
We have the following score vector and Hessian:

y

2

l—y

(it
i,t F

1—F

it)

F'it OOT

= ih1

where F and its derivatives are evaluated at Oz
l—y
y
l—y
it
at —
it
_____
=
h
(F')2 +

—i + - it]

it

(1—F)

1—F

)

it

,

F".

It is well—known that L is concave for probit [F(u)

or for

and

=

e

dr/]

eU/(l + eU)]. Hence a Newton—Raphson algorithm is

logit IF(u)

expected to be effective:

AU

Pu

J

—(,) L—1

Also of interest is a scoring algQrithm which replaces

5

by its expectation:1

=

E(h1)

where

E(hit)

—

(F')2

—

F(l—F)

In either case the computational burden at each iteration comes from

where s1 is either h.t or E(h.). Simplifying

inverting

the partitioned inverse gives the following formulas for up-dating
—13 and a.:
1

s

=

Aa =

- E

'*)

Si

—(L13)' . (i=1,.

.

it

it - S

., N),

where

— F)/F'

it =

i = .it,

—

=—..,
1

-'-1

S.

1

'-

it -it

—*
1
p.
1 =S.—
t .itp.
it
1

At each iteration, F and its derivatives are evaluated at the current values

it

for 13'x. + a..
—
1

This iterated, weighted analysis of covariance algorithm is computationally
effective. 2 Unfortunately, the consistency property (for fixed T) of the NL
estimator of 13

in

the linear regression model does not carry over to this

case. That maximum likelihood need not be consistent in the presence of
incidental parameters can be illustrated in the linear regression model.
The ML estimator of a2
plim a2 =

doe; not adjust for degrees of freedom, and hence

T-l

For T=2, the ML estimator is inconsistent by a factor of two.3

6

thexampl e is a. tour egJi on

Ano

it =
We

i,t_i +

+ Ei.

shall condition on y10. In that case the likelihood

6it

function with

i.i.d. N(O, a2) is formally identical to the previous case. The log—

likelihood function is quadratic in 13 and a (given a2), and the ML estimator

is

of 13

analysis of covariance. With T=2, it can be obtained from a least

squares regression of y12y1 On Y1i—Yo. Given that the log—likelihood
function is quadratic, it is rather surprising that the ML est:finator

for 13 is not consistent. The inconsistency follows immediately since

= 13(y.1 — y.0) +

i2 —
and

i2

is correlated with y1. If the joint distribution of

(y0. y1, y2)

is stationary, then the estimator converges to (13—1)12 as N-°.

As an example of the inconsistency of maximum likelihood in the probability

models, consider the following logit model: F(u) =

x.1=O, x.2=l, i1, .

. ., N.

e'I(l

+ eU), T=2,

So the "treatment" is administered only to

the second observation in the group. Assume that the sequence of a1ts is
such that the following limits exist:

llm*
E[y.1(l—y2)Ja.]
1
N-°°

1im E[(l—y.1)y.2a.]

m1

=
m2

where

).

Then

satisfies

E1y11(1—y.2) a] = F(ai)F(—ct1 —13) and E[(l_y11)y12Ia] F(_a)F(a +
Andersen 11973, P. 66] shows that the ML estimator of 13 almost surely
13

=

213

as N-'°°. A simple extension of his argument shows that if F is a distribution

function corresponding to a symmetric, continuous, non—zero probability
density, then
1

13

= 2F'(

m.)
L

m1+m2

7

almost surely as N-. The logit case is special in that m2/m1 = e1,
independently of the sequence of ctj's.

In general the limiting

depends on this sequence; but if all of the
obtain

=

c = 0,

then once again we

2 almost surely as N-oo.

We conclude that the linear regression model is very special. The
consistency of the ML est:imator of

does not carry over to other models.

The next section interprets this result by introducing a conditional
likelihood function that conditions on sufficient statistics for the

incidental parameters. In the linear regression case, the conditional ML
estimator of

is identical to the ML estimator based on the original joint

likelihood function. Then we show that the idea of using such a conditional
likelihood function can be applied to other models.

3. Fixed Effects: Lhe Conditional Likelihood Function
We have seen that maximization of the fixed effects likelihood
function can give seriously inconsistent estimators if there are only a

small number of observations per group. This section will develop an
alternative approach using a conditional likelihood function. The key idea
is to base the likelihood function on the conditional distribution of the
data, conditioning on a set of sufficient statistics for the incidental
4
parameters.
We shall begin by applying this idea to the familiar linear regression

case. Let

+

yit =

with

+

i.i.d. N(0, 02). Then a sufficient statistic for

a is

It is straightforward to check that the conditional density for

conditional

on

is

y1, .

. .,

8

•

'

=

(2S-(T-l)/2(T-J) exp{Note

that

22

- '(X

-

(Yft

-

this conditional density does not depend upon ..

lo;—likclihood function depends only upon

L

-

N(T-l)

Thu -

i

Hence

the

conditional

and a:

[(y.it - y.)
1

'(x.

it

-.fl2;
L

there is no incidental parameter problem, and so maximum likelihood will
give consistent estimates provided that the usual regularity conditions are

satisfied. The conditional ML estimator of

is the analysis of covariance

estimator that results from maximization of the joint likelihood function.
Hence the consistency of that estimat:or, which was surprising given the

incidental parameter problem, follows immediately from the coincidence of
the joint and the conditional ML estimators.

The advantage of the conditional likelihood approach can be seen in
the conditional ML estimator for a
2 =

—

1

—

2

N(T—1)

Unlike the

joint ML estimator, here there is a correction for degrees of

freedom which ensures that â2 isa consistent estimator of cY2.

The conditional likelihood approach can be applied directly to the
fixed effects logit probability model, since
statistic for ct..

1

5

is again a sufficient

Consider first the case of T=2. If

il +

the only case of

then y and y.2 are both determined given their sum.

So

interest is y11 + y2 = 1.

are w1 =

il' y12)

Then the two possibilities

(0,1) and w. = 0

if (y.1, 'i2 =

= ( or 2,

1

if

(1,0). The conditional

9

density is

Prob(w =

= e

1

= 1)

÷

' i2il

Prob(w1 =

= F['(x

,

l)/[Prob(w1

= 0) + Prob(w

1)1

—x.

-.12 -.il

l+e

which does not depend upon c. The conditional log—likelihood function is
L =

J1 {w1

lnF[8'(x12—x1)] + (l_w) lnF[—8'(x.2—x.1)]},

1

where I =

{ijy11

+ y12 =

l}.

This conditional likelihood function does not depend upon the incidental

parameters. In fact, it is in the form of a binary logit likelihood function
in which the two outcomes are (0,1) and (1,0) with explanatory variables
x,,—x1.

This

is the analog of differencing in the two period regression

model. The conditional ML estimate of 8 can be obtained simply from a standard
ML binary logit program.
The conditional ML estimator of B

Is

consistent provided that the conditional

likelihood function satisfies regularity conditions, which impose mild

restrictions on the a. These restrictions, which constrain the rate at
which the sequence of a1's is allowed to become unbounded, are discussed in

Andersen [1], [2]. Furthermore, the inverse of the information matrix based
on the conditional likelihood function provides an a9ymptotIc (as N-*)

covariance matrix

for

the conditional ML estimator of 8 6 In deriving this

information matrix, one must be careful to note that I is a random set

of indices. This can be made more explicit by defining d1 = 1 if

+

= 1 and d1 = 0
otherwise. Then we have
=

_dF(l_F)

izn

i2il

10

where F and its derivatives are evaluated at (x.2.-xii). The
information matrix is
J =

LP1F(l-F)(x.2-x11)(x12-x11,

where

This

il(1

+

=

+ F(—ci.

i?(c

+

information matrix fs difficult to evaluate since we do not have

a consistent estimator for e., which appears in

Moreover, a standard

ML binary logit program will be evaluating
-

E

2
___

2
___

(since the Hessian of the logit log—likelihood function is non—stochastic),

(given d). In fact,

which depends only upon

is an appropriate asymptotic

covariance matrix for the conditional ML estimator of ,

since

we can apply

the strong law of large numbers to establish that

NdJ N

a. s.

J

-

0

asN-

m.m/i2 <

if

i -.J--,1

wherein., replaces each element of (x-x) by its square. This follows
since the d are independent with Ed.
=
1
1

P.,
1

and both F and the variance of

d1 are uniformly bounded. The condition for convergence clearly holds if the
are uniformly bounded.7

11, .

For general T, conditioning on
conditional log—likelihood function:

L =

ln
[exp('x
I

i t)/

exp('x. d )],
dEB1

t' t

. ., i,

gives the following

11.

where B1 = {d =

(d1,

.

.

., d)1d

0 or 1 and td =

is in conditional logit form with the alternative set

yft}.

L

(B1) varying

across the observations.8 There are T+l distinct alternative sets corresponding

to yft = 0,

1, .

. ., T.

Groups for which it: = 0 or T contribute zero

to L, however, and so only T—l alternative sets are relevant. The alternative

set for groups with it = s has () elements, corresponding to the distinct
sequences of T trials with s successes. For example, with T=3 and s=l
there are three alternatives with the following conditional probabilities:

Prob(l,0,0jy. = 1)
tit

=

Prob(O,1,0Jy. = 1) =

it

Prob(0,0,lJy. = 1) =

it

Since

- —il —xi3)
D

e' i2j3

, D = e3'(x11—x.3) +

e'(x.2—x.3)+ 1.

L is in the form of a conditional logit log—likelihood function,

it can be maximized by standard programs. The information matrix evaluated
by such a program will Implicitly condition on the alternative sets, which

are random in our problem. So the program will evaluate

B =

—E(2L/'IB).

Since the Hessian of the log—likelihood function in conditional logit is

non—stochastic, we have B =

—2L/'.

Hence

is an appropriate asymptotic

covariance matrix for the conditional ML estimator of
JB/N

law

provided that

converges to its expectation. This ili follow from the strong
of large numbers if, for

example, the

are uniformly bounded.

12

In the remainder of this section we :ha1l first extend our conditional
likelihood approach from the binary to the multinomial case; then we shall
apply our approach to the multivariate lcg-lincar probability model, thereby
relaxing the assumption that the observations within a group are independent.

Multinomial Logit for Grouped Data. Say that it can take on three values:

a, b, c. Then we have
a..

Prob(y. =

j) =

e

1J

+

-3'x- itj

aii +

(j = a, b, .

-itj

We assume that the y's are independent both within and between groups. We
shall condition on the number of occurrences within the ith group of each of
the three events.

If T=2, then the only cases of interest are those in which two of the
three events each occurs once, for otherwise there is no stochastic variation.
Conditioning on a and b each occurring once gives (suppressing the i subscript):
-

PfObka,b)I (a,b)
where z =

or (b,a)] =

—x ).
—x ) — (x
(x
.2b .2a
-lb .la

1

+ e'

Hence we have a binary logit problem

with (a,b) and (b,a) as the two alternatives and with z as the explanatory

variables. The incidental parameters do not appear in this conditional probability.
There is a similar result when we condition on a and c each occurring once,
and also when b arid c each occur once.

13

In the general case of T independent observations on each group with

y.
it taking on J values, we define w. .

it]

j=l,

We condition on 5.. =

.

.

= 1 if y. = j and
it

., J.

w.

it]
.

=0

otherwise.

This gives the following

conditional log—likelihood function:

L =

where
Bi =

{d

=

(d11,

.

.

., d,.)1d1

= 0 or 1, dt.

s r1

1,

This is in the form of a conditional logit log—likelihood function and can
be maximized by standard programs.

The Log—Linear Probability Model. We shall relax the assumption that the
are independent within a group by extending the conditional likelihood
9

approach to the general log—linear model. We begin by illustrating the log—

=0

linear model for the binary case

or 1) with T=3 (the i subscripts

are suppressed):
ln Prob(y1, y2, y3) = p

+ 112y1

+ y1y + y2y +

+

where y* = 1 if y = 1 and y* =

+
—1

23yy + 123yyy,

if y =

0.

This is a saturated model

since there are 2—l = 7 independent probabilities, and there are seven
free parameters with p determined by the constraint that the probabilities
sum to one.

A common way to impose structure on this model is to specify the main
effects in terms of a set of explanatory variables: 1jt =
that the interaction terms are constant:

=

for

and to assume

s, t=1, 2, 3, and

il23 l23 Additional structure can be imposed by specifying that the

14

lnteraction

It

terms beyond some order arc zero; for example, that

We shall introduce group specific effects by letting
is straightforward to check that
Prob(y.1 =

ly.2,

in

0.
-Sit

*

* *

2a1 + 2'x + 2'y12y12 + 2l3i3 + 2il23Yf2vfl.

l—Prob(y11=1jy12, •3)

=

=

So If the interaction terms

it = çi +

*

y13) =

=

0, then y1 is independent of

and y3, and the probability of y1=l takes the logistic form that we have
been using (except for a scale factor of 2).

For the general case of T binary variables we have (suppressing the i
subscripts):

where Mk =

{(t1,

T

+

in Prob(y1,

•

k=l tEM
.

.

., t)}

•Y

is the set consisting of the () groups of

k integers that can be formed from the integers 1,

specify the first order terms asy =

it

aI + -'x
=it.

.

. ., T.

Th

We shall

interaction terms

may depend upon x but with coefficients that do not vary in i, so that the
incidental parameters are confined to the first order terms.

Since 1.yft is a sufficient statistic for a., we form the following
conditional density:
Prob(y.1, .

'IT't

=

exp[(a.1 + f'x.
—it )yit + g(y.)]
exp[(a. +
dEB1

it )d*
t + g(d)]

exp[B'Ex.y+ g(y.)]
+

where B. =

=

{d = (d1,

.

.

. ., dT)Idt

g(d)I

= 0 or 1 and tdt =

and g( ) does not depént upon a.. We see that

the conditional density does not depend upon a.. The corresponding
log—likelihood function differs from the one for independent y's

15

only in the g( )
g(l,

+

terms.

l2 l3 + 23 + l23

0, 0) =

-

123 g(0, 0,

=

= 1 we have

For example, with T=3 and

l2 l3
—

—

g(O, 1, 0) =

l2

+

l3

-

23 + l23 Rescaling all the coefficients

by one—half, we can write the conditional probabilities as

Prob(l, 0, 0Iy = 1) =

expt'(x.1

—

i3 +

Prob(O, 1, OIy = 1) =

exp['(x12

—

j3) + 113

Prob(0, 0, ly = 1)

with

23 12
— 1121

=

D determined so that the probabilities sum to one. So this differs

from the independence case by introducing alternative specific constants
into the conditional probabilities.

We have seen that it is fruitful to base the likelihood function on a
conditional distribution that conditions on sufficient statistics for the

incidental parameters. It is not always possible, however, to find a sufficient
statistic for

about

such that the conditional distribution is sufficiently informative

The next section examines a random effects model in which a consistent

estimator for

can be obtained without relying upon sufficient statistics for

the cz1.

4. Random Effects: the Marginal Likelihood Function
An alternative approach is to assume thai: the incidental parameters

follow a distribution. Then the likelihood function can be based on the

density for y, given x, , and G, the distribution function for ct.

If

we specify a parametric family for C, indexed by a fthite parameter vector
T,

then

we have the following log—likelihood function for ,

i:

16
L

inff(yj, , )dG(cx,

So the density function for y

i).

conditional on

density function that is marginal on c.

has been replaced by a

The maximization of this i.ikeiThood

function will, under weak regularity conditions, give consistent. (as N estimators

for

and •

2

This approach introduces additional information and is most naturally

formulated in Bayesian terms. A potentially appealing prior distribution
specifies that the ci's are independent and identically distributed.

This can often be justified bydeFinetti's 116]

exchangeability

criterion.

If (for arbitrary N) the distribution of the a.'s is not affected by
permuting them, so that the subscript is purely a labeling device with no

substantive content, then the joint distribution of the &s must be ex—
pressable as random sampling from a univariate distribution. This criterion
will often be satisfied when i indexes individuals (longitudinal data) or
families (sibling data).

The main point I want to make here is that the random sampling

on is appropriate only as a marginal distribution for a. We
must, however, specify a distribution for a conditional on x. The convent1uiii
random effects model assumes that a is independent of x. But our interest
in introducing the incidental parameters was motivated by missing variables

that are correlated with x. If one mistakenly models a as independent of x,
then the omitted

variable bias is not eliminated. So we want

to specify a

13
conditional distribution for a given x that allows for dependence.

A

convenient possibility is to assume that the dependence is only via a linear
regression function: c. = Tr'x.
1
.i

+ v.,
withx =
1

(x
-.il

,

.

. ., x-iT), and where v

is independent of x. We appeal to exchangeability to argue that the v are
independent and identically distributed. A restriction on the regression
function that may be appropriate is n'x. =

17

We shall illustrate this approach with a production function example

that leads to a linear regression model.14 Say that a farmer is producing
a product under the following Cobb—Douglas technology: Y =

LQ'e6,

where Y is output, L is a variable factor (labor), Q is a fixed factor (soil
is stochastic (rainfall), and 0 <

quality),

< 1.

Assume that c is

distributed independently of Q; persistent differences in average rainfall

can be incorporated into Q. We assume that the farmer knows the product
price (P) and the factor price (W), which do not depend on his decisions,

and that he knows Q. The factor input decision, however, is made before
knowing E, and we assume that L is chosen to maximize expected profit:

E(PY — WLP, W, Q).

There are observations on il, .

. ., N

farms in each of t=l, .

.

., T

periods. Assume that Q is constant over the period of the sample and tiat
the distribution of c conditional on Q, W, and P is

i.i.d. N(0, a2).

Then we have the following production and factor demand functions:

=

x1 +

x1 = p +
where y =

mY,

1

x =

+ 61t

+ c) +

lnL, c=ylnQ, p =

(ln ÷ 42)/(l_),

z =

ln(P/W),

and u is a random term, reflecting optimization and other errors, which is

independent of c and c. Although Q is krown to the farmer and affects his
factor demand decisions, we assume that it is not observed by the econometrician;

is included in order to capture this omitted variable. The example is
useful in showing explicitly how a correlation between x and a might arise.

We shall focus on tiiing th roduction function without using
whatever price data is available. A pooled least squares regression of y
on x, which does not allow for farm effects, is inconsistent. If a is
independent of z, then as N-° this estimator converges to

18
a

2

a

(l—) (Vw + VB)

where

2

1
V = plim-—-

—

(x.

N-*° NTj,t it-

5L)

plim —

VB

N

—

j I

and a Is the marginal variance of a. Now consider a random effects
approach, a1 i.i.d. N(i, az), that incorrectly assumes that a is independent

of x. Then the ML estimator of , conditional on A = a2/a2, is generalized

least

squares. This is equivalent to ordinary least squares using deviations
—
—
—
—
from fractional means: regress
where y=l — (1 + AT) —1/2
yy. on

ix.,

This estimator converges,as N-*o, to

(l—y) 22

a

+

(1)
Hence It

(1—y) VB1

is consistent only as T-°.

it

So

Let w1 =

is

essential to allow for a dependence between a and x.

z./(l)

conditional

distribution of a
2

-

2

a

aa.

i is a Txl

(l-.)
vector

on x is given by a1 = K + 'rr'x. +

2

does no imply that IT'x1

A sufficient condition is that is
The ML estimator of (, Tr),

x1 and given A = a2/a2,

—

the

ix.

and (l—y)x..

regression

of

where

of ones, and v is independent of x with v1 i.i.d. N(O, a2).

Note that assuming a stationary

on

v1,

+] -l2

a

—

Then the

+ u1 and assume that w. is i.i.d. N(m, ).

—

can

equic)rrelated:

allowing

=

Sx. If T > 2.

p11 + p29Q'.

or several variables in

be obtained From the regression of

The resulting estimator for

—

can be obtained from

yy1 on the residual from the regression of x1 —

on

x1. This residual is

is

equivalent to the regression of

—

hut the regression of
—

y1

—

on it —

We have obtained the interesting result that a random effects

—

19

that

specification can give a ML estimator of 8

is identical to the fixed

effects estimator, if we allow the distribution of the incidental parameters
to depend upon x)-50f course the linear regression case is special, since

the fixed effects estimator is consistent. This is not true for the (joint)
ML estimator of 8

in

the linear autoregressive model or in the probability

models. So the random effects specification leads to new estimators In those
cases.

In the autoregressive case, let

io +a I +c Ii

i2 =

il

+ a1 +

where, conditional on y

10

and a , we have
I

(c.1,

c.i2 )

i.1.d.

a normal

distribution with mean 0 and diagonal covariance matrix: d1ag{o,
=

Let

Try10 + v1, where, conditional on y10,we have v1 i.i.d. N0, 02). Then

y12) =
where

i'

62'io + (u11, u12),

= + 2 =

+

IF,

and u1 is i.i.d. N(0, E). This is a inultivar late

regression model in which the ML estimator
squares regressions of

estimator of $ from

of 6 is obtained from the least

y1 and y2 on y0. Then we can solve for the ML

8 =

A

(62

—

A

A

61)1(61

—

1).

if the y10 have sufficient variation and if 8
to taking first differences,

2

=

This estimator is consistent

+

8(y11

using y0 as an instrumental variable for y11 —
assumption that

can
of 8

=

02

—

y10)

+ £12 —

If

c,

and

we add the

then an additional consistent estimator of 8

be obtained from a consistent

will

1. It is equivalent

IF

estimator of E. Now the ML estimator

combine the estimator obtained from

with the estimator obtained from the

the regression coefficients

residual covariance matrix.

The likelihood function for the joint distribution of

(v0, y1, y2)

20

is obtained by multiplying the likelihood conditional on y0 by the

marginal density of y0. If the parameters of this marginal density are
left unconstrained, then the ML estimator of

is unaffected. Imposing

stationarity on the joint distribution will, however, imply constraints.

is i.i.d. normal with variance p, then stationarity implies

If

/I(1rj.

that P =

In

the binary data case, let Prob(y4 =

lix, ,

a) =

F('j

+ a.). Then

the log—likelihood function under our random effects specification is

ElnfflF('x
t — _it

L =

i

wherekl( i)

is

y

l-y

÷ ir'x1 + v) it[1 —

F('x1

+irx
——— + v)]

dH(vIp),
—

a family of univariate distribution functions indexed by

the parameter vector P.

For example, if F is a unit normal distribution

function and we choose H to be the distribution function of a N(0, 02)

random variable, then our specification gives a multivariate probit model:
= 1

1f

i.i.d.
where

+

+

>

N(O, 0vTT +

is a T x 1 vector of ones. The novel feature of this model is

the inclusion of the term 7r'x. to capture the dependence between the
incidental parameters and x.

For example,.consider estimating the effect of ability on the

probability of attending college, controlling for family background. There
is

per

a sample of N families with test scores (x) for
famli, y. The family effect:

each of T2

brothers

Is Intended to capture omitted variables

such as family wealth and parents' s.hooling. Under this interpretation,

a is likely to be correlated with x Our

procedure

n the probit ce

is to fit a (constrained) bivariate robit model.for y11 and y12 on

and

21

x12. This provides estimates of

+ir1 r2

1

from which we obtain an estimate of

by taking the coefficient of sib l's

test score in sib l's equation minus the coefficient of sib l's test score

in sib 2's equation. We can do the same with sib 2's test score and hence
the constraint on the matrix of probit coefficieits.

From the symmetry of this example (ignoring birth order effects),
it is appropriate to set

=

2

Then

can be consistently estimated

by taking the coefficient of sib l's test score in sib l's equation minus

the coefficient of sib 2's test score in sib l's equation. Hence we only
require y for one of the sibs provided that we have x for both. For
example, the Michigan Panel Study of Income Dynamics 1261 has extensive
information on the respondent and much less complete information on his

siblings. There is schooling data for the respondent and his oldest
brother, but earnings and occupation data only for the respondent.

Nevertheless, we can control for family backgiound In assessing the
relationship between schooling and earnings by including the schooling of

sib 2 in a regression of sib l's earnings on his schooling. Then
is estimated by the excess of sib l's schooling coefficient over that of

his brother. A probit example could arise in studying the relationship
between schooling and occupation, where occupations are classified into
two groups corresponding to production and non—production workers.

5. Conclusion
The paper has discussed three approaches t

the analysis of grouped data:

the joint likelihood function, the conditional likelihood function, and the

marginal likelihood function. Throughout the paper, our concern has been with

22

the parameters ()

that

are common to all of the

(a1) are intended to capture

estimates of .
to

groups; the incidental parameters

group effects whose omission would

result in biased

The objective has been to obtain
estimators that converge

as the number of groups (N)

increases, even if the number of observations
per group (T) is small. Important
applications include longitudinal
data,

in which there are two or more observations on each individual,
and the a. capture person effects; and sibling dnia, in which the

a1

capture family effects, such as omitted family background variables.

have illustrated the inconsistency or the olnt ML estimator iii
the fixed effects probability model s. One solution, within the fixed
We

effects model,

is to maximize a conditional

likelihood function that

conditions on sufficient statistics for the incidental parameters.

This

conditional likelihood function does not depend upon the incidental
and so standard asymptotic

theory can be applied. In the

linear regression model, the

parameters,

(normal—theory)

consistency of the joint ML estimator of

corresponds to the coincidence of the

joint and the conditional ML estimators.

In the log:Lt case, however, the conditional ML estimator of

is consistent

whereas the joint ML estimator is not (for fixed T). The conditional
ML
estimator for the logit case can be implemented with a standard conditional
logit program, which allows the alternative
Finally, we discussed random effects
distribution on the incidental

set to vary across the observations.

models which impose a (prior)

parameters. Then the likelihood function

is based on the distribution for

y that is marginal on the incidental

parameters. The important point here is that the specification of the
conditional distribution fora. given x should allow for dependence;
the
common assumption that a. is independent of x assumes away omitted
varIable

bias. In the linear

regression model, the ML estimator for

random effects specification is
in this special case, all three

under our

once again analysis of covariance.

So

of our approaches give identical estimators

23

for .

In

the probability models, however, the marginal likelihood

specification leads to new estimators.

The marginal likelihood approach has the advantage of not requiring

simple sUfficient statistics for the incidental parameters. Furthermore,
it imposes (stochastic) restrictions on the fixed effects model, which will
lead to more precise estimators if the restrictions are valid. The dis—
advantage is that in order to specify that the c are independent of each
other (conditional on x), our approach requires a particular parametric

class of conditional distributions for . given x. Hence some
-1

analysis

sensitivity

is called for. The fixed effects model allows for a very general

relationship between the incidental parameters and the explanatory variables.

24

Footno tes

11n the logit case the Hessian does not depend upon y, and so scoring is
identical to the Newton—Raphson algorithm.

program to implement this algorithm is described in Hall [21],
along with an example of the computational efficiency of the program.
A labor force participation application of a fixed effects probit model
is presented in Heckman [221.

3mis example is discussed in Neyrnan and Scott [31].
4The use of conditional likelihood functions for incidental parameter
problems is discussed in Bartlett [8], [9], [10], Andersen [1], [5],
Kalbfleisch and Sprott [23], and Barndorff—Nielsen [7].
5The conditional likelihood approach in the logit case is closely related

to R. A. Fisher's [17] exact test for independence in a 2x2 table. This
exact significance test has been extended by Cox [15] and others to the

case of several contingency tables. Additional references are in Cox [15]
and in Bishop et al. [111. A conditional likelihood approach was used
by Rasch [321, f 331 in his model for intelligence tests. The
probability that person i gives a correct answer to item number t is

exp(czi + )/[l + exp(ai +

this is a special case in which

is a

set of dummy indicator variables. An algorithm for conditional maximum
likelihood estimation in this model is described in Andersen [4].
(3

The efficiency of the conditional ML estimator is maximized by conditioning
on minimal sufficient statistics for the incidental parameters. Zy
is a minimal sufficient statistic for

both in the linear regression

model and in the logit model. Even so the conditional ML estimator need
not attain the asymptotic Cramer—Rao bound as N-*oo for fixed T. It does

in the linear regression case but not in the logit model. However, I

25

doubt whether there is another consistent estimator that has smaller

asymptotic variance in the fixed effects logit model. The random effec:s
model of section 4, which introduces additional (stochastic) restrictions,

can lead to a more efficient estimator of .
7An alternative justification for the use of —E(2L/B'Id) can be based on

stating the limiting distribution properties in terms of the conditionaL
distribution, in which the observed values of the sufficient statistics

are treated as parameters. This approach is pursued in Andersen [3].
8The conditional logit model is developed in McFadden 1 25].
9The log—linear model is developed in Goodman 118], [19], Haberman [20],

and Nerlove and Press 130]. Additional references are in Bishop et al. [11].
101n the probit model, for example, there does not appear to be such a
sufficient statistic.

11Kalbfleisch and Sprott [23] call this an integrated likelihood function.
A marginal likelihood function can also be useful in a fixed effects
approach, in which we consider the distribution of some function of
conditional on a.. For example, in the linear regression case with T=2,
the distribution of y12—y11 does not depend upon ct1. Hence maximizing
the associated likelihood function gives consistent (as N-oo) estimators
of

and .

Once again the ML estimator of

is the standard analysis of

covar lance estimator.

Note that

the

original Kiefer and Wolfowitz [24] results were not limited

to the parametric case.

13Note that the empirical work by Chamberlain and Griliches [13], 114]
and Chamberlain [121 does allow the random effects to be correlated with

the explanatory variables. Also in the original Balestra and Nerlove
[6] model, the autoregressive component is correlated with the random effects.

26

14Thjs example is discussed in Mundlak [271, [281.

15This result is discussed in Mundlak [29] for the case ir'x.

=

S

27

References

[1] Andersen, E. B.

"Asymptotic Properties of Conditional Maximum

Likelihood Estimators", Journal of the Royal Statistical Society,
Series

B, 32 (1970), 283—301.

[2] Andersen, E. B. "Asymptotic Properties of Conditional Likelihood

Ratio Tests", Journal of the American Statistical Association,

66

(1971), 630—633.

13]

Andersen, E. B.

"A Strictly Conditional Approach in Estimation

Theory", Skandinavisk Aktuarietidskrjft, (1971), 39—49.

14] Andersen, E. B. "The Numerical Solution of a Set of Conditional

Equations", Journal of the Royal Statistical Society,
Series B, 34 (1972), 42—54.
Estimation

[5] Andersen, E. B. Conditional Inference and Models for Measuring
(Copenhagen: Mentalhygiejnisk Forlag, 1973).

[6] Balestra, P, and Nerlove, M. "Pooling Cross Section and Time Series
Data in the Estimation of a Dynamic Model: The Demand for Natural
Gas", Econometrica, 34 (1966), 585—612.

17] Barndorff—Nielsen, 0. Information and Exponential Families in

Statistical Theory (New York: Wiley, 1978).
[8] Bartlett, M. S. "The Information Available in Small Samples",
Proceedings of the Cambridge Philosophical Society, 32 (1936), 560—566.

[9] Bartlett, M. S. "Statistical Information and Properties of Sufficiency",
Proceedings of the Royal Society, Series A, 154 (1936), 124—137.

[10] Bartlett, M. S. "Properties of Sufficiency and Statistical Tests",
Proceedings -f the Royal Society, Series A, 160 (1937), 268—282.

[11] Bishop, Y. M. N., Fienberg, S. E. and Holland, P. W. Discrete
Multivariate Analysis: Theory and Practice (Cambrige, Mass.:N.I.T. Press,
1975).

28

[12] Chamberlain, G. "Omitted Variable Bias in Panel Data: Estimating
the Returns to Schooling", Annales de l'INSEE, 30-31 (1978), 49—82

[13] Chamberlain, G. and Griliches, Z. "Unobservables with a Variance—
Components Structure: Ability, Schooling, and the Economic Success
of Brothers", International Economic Review, 16 (1975), 422—449.

[14] "Chamberlain, G. and Griliches, Z. "More on Brothers" in Taubman, P.
(ed.), Kinometrics: The Determinants of Socio—economic Success Within
and Between Families (Amsterdam: North Holland Publishing Company, 1977).

[15] Cox, D. R. Analysis of Binary Data (Jce, 4ylethuen, l?7O).

[16]

deFinetti, B. "La Prvision: Les Lois Logiques, ses Sources Subjectives",
Annales de l'Institut Henri Poincar, 7 (1937). English translation in

Kyburg, H. E. and Smokier, H. E. (eds.), Studies in Subjective Probability
(New York: Wiley, 1964).,

[17] Fisher, R. A. "The Logic of Inductive Inference",Journal of the
Royal Statistical Society, Series B, 98 (1935), 39—54.

[18] Goodman, L. "The Multivariate Analysis of Qualitative Data:
Interactions Among Multiple Classifications", Journal of the American
Statistical Association, 65 (1970), 226—256.

[19] Goodman, L "A Modified Multiple Regression Approach to the Analysis
of Dichotomous Variables", American Sociological Review, 37 (1972),
28—4 6.

[20] Haberman, S. J.

(Chicago: University

of Chicago Press, 1974).

[21] Hall,' B. H. "A General Framework for Tine Series—Cross Section

Estimation",

Annales de l'INSEE, 303i (978), i77—202
[22] Heckman, J. J. "Statistical Models for Discrete Longitudinal Data",
(tniversity of Chicago, 1978).

29

[23] Kalbfleisch, J. D. and Sprott, D. A. "Application of Likelihood
Methods to Models Involving Large Numbers of Parameters", Journal of
the Royal Statistical Society, Series B, 32 (1970), 175—208.

[24] Kiefer, J. and Wolfowitz, J. "Consistency of the Maximum Likelihood
Estimator in the Presence of Infinitely Many Incidental Parameters",
Annals of Mathematical Statistics, 27 (1956), 887—906.

[25] McFadden, D. "Conditional Logit Analysis of Qualitative Choice
Behavior" in Zarembka, P (ed.), Frontiers in Econometrics (New York:
Academic Press, 1974).

[261 Morgan, J. N. et al. A Panel Study of Income Dynamics (Ann Arbor:
Institute for Social Pesearch, 1972).

[27] Mundlak, Y. "Empirical Production Function Free of Management Bias",
Journal of Farm Economics, 43 (1961), 44—56.

[28] Mundlak, Y. "Estimation of Production and Behavioral Functions from
a Combination of Cross—Section and Time—Series Data" in Christ, C. et
al., Measurement in Economics (Stanford University Press, 1963).

[29] Mundlak, Y. "On the Pooling of Time Series and Cross Section Data",
Econometrica, 46 (1978), 69—85.

[30] Nerlove, M. and Press, S. J. "Multivariate Log—Linear Probability
Models for the Analysis of Qualitative Data", Center for Statistics
and Probability Discussion Paper,No. 1 (Northwestern University, 1976).

[31] Neyman,

J. and

Scott, E. L. "Consistent Estimates Based on Partially

Consistent Observations", Econoinetrica, 16 (1948), 1—32.

[32] Rasch, G. Probabilistic Models for SOineIntelligence and Attaient-Tests
(Copenhagen: Danmarks Paedagogiske Institut, 1960).

[33] Rasch, G. "On General Laws and the Meaning of Measurement in Psychology",
Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics
and Probability, 4 (1961), 321—333.

