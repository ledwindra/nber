NBER WORKING PAPER SERIES

DISENTANGLING VOLATILITY FROM JUMPS
Yacine Aït-Sahalia
Working Paper 9915
http://www.nber.org/papers/w9915
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
August 2003

Financial support from the NSF under grant SES-0111140 is gratefully acknowledged. I am also grateful to
Arnaud Godard and Jialin Yu for excellent research assistance. The views expressed herein are those of the
authors and not necessarily those of the National Bureau of Economic Research.
©2003 by Yacine Aït-Sahalia. All rights reserved. Short sections of text, not to exceed two paragraphs, may
be quoted without explicit permission provided that full credit, including © notice, is given to the source.

Disentangling Volatility from Jumps
Yacine Aït-Sahalia
NBER Working Paper No. 9915
August 2003
JEL No. G12, C22
ABSTRACT
Realistic models for financial asset prices used in portfolio choice, option pricing or risk
management include both a continuous Brownian and a jump components. This paper studies our
ability to distinguish one from the other. I find that, surprisingly, it is possible to perfectly
disentangle Brownian noise from jumps. This is true even if, unlike the usual Poisson jumps, the
jump process exhibits an infinite number of small jumps in any finite time interval, which ought to
be harder to distinguish from Brownian noise, itself made up of many small moves.

Yacine Aït-Sahalia
Department of Economics
and Bendheim Center for Finance
26 Prospect Avenue
Princeton, NJ 08540-5296
and NBER
yacine@princeton.edu

1.

Introduction

From an asset pricing perspective, being able to decompose the total amount of noise into a continuous
Brownian part and a discontinuous jump part is useful in a number of contexts: for instance, in option pricing,
the two types of noise have diﬀerent hedging requirements and possibilities; in portfolio allocation, the demand
for assets subject to both types of risk can be optimized further if a decomposition of the total risk into a
Brownian and a jump part is available; in risk management, such a decomposition makes it possible over short
horizons to manage the Brownian risk using Gaussian tools while assessing VaR and other tail statistics based
on the identified jump component. In fact, the ability to disentangle jumps from volatility is the essence of risk
management, which should focus on controlling large risks leaving aside the day-to-day Brownian fluctuations.
This paper shows that it is possible to use likelihood-based statistical methods to distinguish volatility from
jumps with (asymptotically) perfect accuracy, thereby focusing on the part of the overall risk that should be
the object of concern in risk management or asset allocation.
The fact that jumps play an important role in many variables in finance, such as asset returns, interest
rates or currencies, as well as a sense of diminishing marginal returns in studies of the “simple” diﬀusive
case, has led to a flurry of recent activity dealing with jump processes. This activity has developed in three
broad directions: estimating ever more complex and realistic financial models incorporating jumps (see e.g.,
Schaumburg (2001) using maximum-likelihood, Eraker et al. (2003) using MCMC, Chernov et al. (2002) using
EMM, and the references therein), testing from discrete data whether jumps are present (see Aït-Sahalia
(2002b) using a characterization of the transition function of a diﬀusion and Carr and Wu (2003b) using short
dated options) and studying the behavior of interesting statistics, such as the quadratic variation and related
quantities, in the presence of jumps (see Barndorﬀ-Nielsen and Shephard (2002)).
The present paper asks a diﬀerent yet basic question, which, despite its importance and apparent simplicity,
appears to have been overlooked in the literature: how does the presence of jumps impact our ability to estimate
the diﬀusion parameter σ 2 ? I start by presenting some intuition that seems to suggest that the identification of
σ2 is hampered by the presence of the jumps, before showing that maximum-likelihood can actually perfectly
disentangle Brownian noise from jumps provided one samples frequently enough. I first show this result in the
context of a compound Poisson process, i.e., a jump-diﬀusion model as in Merton (1976).
One may wonder whether this result is driven by the fact that Poisson jumps share the dual characteristic
of being large and infrequent. Is it possible to perturb the Brownian noise by a Lévy pure jump process
other than Poisson, and still recover the parameter σ2 as if no jumps were present? The reason one might
expect this not to be possible is the fact that, among Lévy pure jump processes, the Poisson process is the
only one with a finite number of jumps in a finite time interval. All other pure jump processes exhibit an
infinite number of small jumps in any finite time interval. Intuitively, these tiny jumps ought to be harder
to distinguish from Brownian noise, which is itself made up of many small moves. Perhaps more surprisingly
then, I find that maximum likelihood can still perfectly discriminate between Brownian noise and a Cauchy

1

process, a canonical example of such processes.
Indeed, while the early use on jumps in finance has focused exclusively on Poisson jumps (see Press
(1967), Merton (1976), Beckers (1981) and Ball and Torous (1983)), the literature is rapidly moving towards
incorporating other types of Lévy processes, such as the Cauchy jumps which are considered here. This is the
case either for theoretical option pricing (see e.g., Madan et al. (1998), Chan (1999) and Carr and Wu (2003a)),
risk management (see e.g., Eberlein et al. (1998)), or as a means of providing more accurate description of
asset returns data (see e.g., Carr et al. (2002)). In term structure modelling, diﬀerent Central Bank policies
can give rise to diﬀerent types of jumps and recent models do also allow for Lévy jumps other than Poisson
(see e.g., Eberlein and Raible (1999)). Given this literature, the results I present provide statistical support
for the use of non-Poisson jump processes: in the various contexts that matter in finance, it is possible to mix
such jump processes with the usual Brownian volatility and still distinguish one from the other.
The paper is organized as follows: in Section 2, I briefly present the basic Poisson model, before giving in
Section 3 diﬀerent types of intuition which all suggest that it would be diﬃcult to distinguish the volatility
from the jumps. In Section 4, I show that the intuition is actually misleading, at least in the Poisson case. I
then look in Section 5 at the extent to which GMM estimators using absolute moments of various non-integer
orders can recover the eﬃciency of maximum likelihood (the answer is no, but they do better than traditional
moments such as the variance and kurtosis). The next question is whether any of this is specific to Poisson
jumps. I show that this is not the case by studying more general Lévy pure jump processes in Section 6.
Finally, I present in Section 7 Monte Carlo evidence to show that the asymptotic results in the theorems of
the previous sections provide a close approximation to the behaviors that we are likely to encounter in daily
data. Section 8 concludes. All proofs are in the Appendix.
2.

The Model and Setup

Most of the points made in this paper are already apparent in a Poisson-based jump-diﬀusion model. So,
for clarity, I will start with the simple Merton (1976) model where the jump term is Poisson-driven; in this
section, I collect a number of useful results about this basic model. Later, I will turn to the more complex
situation where the jump term is Cauchy-driven. Consider for now the jump-diﬀusion specification
dXt = µdt + σdWt + Jt dNt

(2.1)

where Xt denotes the log-return derived from an asset. Wt denotes a standard Brownian motion and Nt a
Poisson process with arrival rate λ. The log-jump size Jt is a Gaussian random variable with mean β and
variance η. By Itô’s Lemma, the corresponding model for the asset price St = S0 exp(Xt ) is
¢
dSt ¡
= µ + σ 2 /2 dt + σdWt + (exp (Jt ) − 1) dNt
St

(2.2)

For further simplicity, assume that Wt , Nt and Jt are independent stochastic processes. As noted above,

2

extensions to dependent drift, diﬀusion, and jump arrival intensity functions, as well as to other distributions
of the jump size Jt , pose no conceptual diﬃculties but are notationally more cumbersome with little associated
gain. The objective is to study our ability to estimate the parameter vector θ = (µ, σ 2 , λ, β, η)0 , where µ is
the drift of the Brownian process, σ the volatility of the Brownian process, λ the arrival rate of the Poisson
process, β the average size of the jumps J and η their variance. θ is an unknown parameter in a bounded
set Θ ⊂ R5 . I focus in particular on our ability to distinguish information about the diﬀusive part (σ 2 ) from
information about the jump part (λ, η), the respective means (µ, β) being largely inconsequential.
2.1. The Transition Density
The transition density for the model under consideration has a known form which I briefly review. The solution
of the stochastic diﬀerential equation (2.1) is
Z∆ = X∆ − X0 = µ∆ + σW∆ +

Z

∆

Js dNs

(2.3)

0

which implies in particular that, for this simple model, the log-returns are i.i.d. This is a consequence of the
assumptions made that the parameters and distribution of the jump term are state-independent. Since the
distribution of the Poisson process is discrete, and
Pr (N∆ = n; θ) =

exp(−λ∆)(λ∆)n
.
n!

Conditioning on the number of possible jumps between 0 and ∆ and applying Bayes’ Rule, we have
Pr (X∆ ≤ x|X0 = x0 , ∆; θ) =

+∞
X

n=0

Pr (X∆ ≤ x | X0 = x0 , ∆, N∆ = n; θ) × Pr (N∆ = n; θ) .

Conditioned upon the event N∆ = n, there must have been exactly n times, say τ i , i = 1, ..., n, between 0
and ∆ such that dNτ i = 1. Thus
Z

∆

Js dNs =

0

n
X

Jτ i

i=1

is the sum of n independent jump terms. Under the assumption that each one has the distribution J ∼ N (β, η),
it follows that the transition density of X∆ given X0 is given by
p(x|x0 , ∆; θ) =

+∞
X

p(x|x0 , ∆, N∆ = n; θ) Pr (N∆ = n; θ)

n=0
+∞
X

Ã
!
exp(−λ∆)(λ∆)n
(x − x0 − µ∆ − nβ)2
=
exp −
.
√ p
2 (nη + ∆σ2 )
2π nη + ∆σ2 n!
n=0

(2.4)

As expected in the presence of jumps, the density exhibits excess kurtosis: see Figures 1 and 2 (at ∆ = 1/12
with parameters µ = β = 0, σ = 0.3, λ = 0.2 and η 1/2 = 0.6). Early examples of the use of this or similar
formulae for maximum likelihood in finance are contained in Press (1967), Beckers (1981) and Ball and Torous
3

(1983). A non-zero value of the mean jump size β would add skewness. Note that, for purposes of maximumlikelihood estimation, care must be taken to ensure that the mixture of normals remains bounded by properly
restricting the admissible parameters. Otherwise, setting the mean of one of the elements to be exactly equal
to the observations, the variance parameter of that element can be driven to zero thereby increasing the
likelihood to arbitrarily high levels (see Kiefer (1978) and Honoré (1998) for further discussion).
2.2. Moments of the Process
The first four conditional moments of the process X were calculated by Press (1967) using the transition
density. They are E [Y∆ ] = ∆ (µ + βλ) and, with
M (∆, θ, r) ≡ E [(Y∆ − ∆ (µ + βλ))r ]
we have
¡
¢
M (∆, θ, 2) = ∆ σ 2 + (β 2 + η)λ
¢
¡
M (∆, θ, 3) = ∆λβ β 2 + 3η
¡
¡
¢
¢2
M (∆, θ, 4) = ∆ β 4 λ + 6β 2 ηλ + 3η 2 λ + 3∆2 σ2 + (β 2 + η)λ

(2.5)

More generally, to evaluate moments of the process, for (2.1) and more complex stochastic diﬀerential equations, let A denote the infinitesimal generator of the process X, defined by its action on functions f (δ, x, x0 )
in its domain:
A · f (∆, x, x0 ) =

∂f (∆, x, x0 ) 1 2 ∂ 2 f (∆, x, x0 )
∂f (∆, x, x0 )
+µ
+ σ
∂∆
∂x
2
∂x2
+λEJ [f (∆, x + J, x0 ) − f (∆, x, x0 )] .

(2.6)

To evaluate a conditional expectation, I use the Taylor expansion
E [f (∆, X∆ , X0 )|X0 = x0 ] =

K
X
∆k
k=0

k!

¢
¡
Ak · f (δ, x, x0 )| x=x0 ,δ=0 + O ∆K+1

(2.7)

In all cases, this expression is a proper Taylor series (as in Aït-Sahalia (2002a)); whether the series is analytic
at ∆ = 0 is not guaranteed. In the present case, the moments of the process of integer order all lead to a finite
series, which is therefore exact: applying (2.7) to f(δ, x, x0 ) = (x − x0 )i , i = 1, ..., 4, yields exact expressions.
2.3. Absolute Moments of Non-Integer Order
It turns out that the absolute value of the log returns is less sensitive than the quadratic variation to large
deviations, which makes them suitable in the context of high frequency data with the possibility of jumps.
This has been noted by e.g., Ding et al. (1993) and Andersen and Bollerslev (1997). Consider the quadratic

4

variation of the X process
n
X
¡
¢2
Xti − Xti−1
[X, X]t = plimn→∞

(2.8)

i=1

for any increasing sequence 0 = t0 , ..., tn = t. We have
[X, X]t

c

= [X, X]t +

X

0≤s≤t

2

= σ t+

X

0≤s≤t

= σ2 t +

Nt
X

2

(Xs − Xs− )

Js2 (Ns − Ns− )

2

Js2i

i=1

where si , i = 1, ..., Nt denote the jump dates of the process, with the continuous part of the quadratic variation
given by [X, X]ct = σ2 t and Xs − Xs− = Js (Ns − Ns− ) (i.e., X only jumps when N jumps; when N jumps,
it jumps by one unit).
Not surprisingly, the quadratic variation in this case no longer estimates σ2 (see e.g., the related discussion
in Andersen et al. (2003)). However, Lepingle (1976) studied the behavior of the power variation of the process,
i.e., the quantity
r

[X, X]t = plimn→∞

n
X
¯
¯
¯Xti − Xti−1 ¯r

(2.9)

i=1

and showed that the contribution of the jump part to r [X, X]t is, after normalization, zero when r ∈ (0, 2),
PNt 2
i=1 Jsi when r = 2 and infinity when r > 2. Barndorﬀ-Nielsen and Shephard (2002) use this result to

show that the full

r

[X, X]t depends only on the diﬀusive component when r ∈ (0, 2). They also compute

the asymptotic distribution of the sample analog of r [X, X]t constructed from discrete approximations to the
continuous-time process.
I will use these insights when forming GMM moment conditions to estimate the parameters in the presence
of jumps, with the objective of studying their ability to reproduce the eﬃciency of MLE. I will consider in
particular absolute moments of order r (i.e., the plims of the power variations). To form unbiased moment
conditions, I will need an exact expression for these moments, which is given in the following result:

Proposition 1. For any r ≥ 0, the centered absolute moment of order r is:
Ma (∆, θ, r) ≡ E [|Y∆ − ∆ (µ + βλ)|r ]
´¡
¢ ³
¡
¢r/2
2
2
n
∞ 2r/2 Γ 1+r F 1+r , 1 , β (n−∆λ)
nη + σ2 ∆
(λ∆) −λ∆− (nβ−∆βλ)2
X
2
2
2
2 2(nη+σ ∆)
2(∆σ2 +nη)
=
e
1/2 n!
π
n=0
where Γ denote the gamma function and F denotes the Kummer confluent hypergeometric function 1 F1 (a, b, ω).1
1 See

Chapter 13 in Abramowitz and Stegun (1972) for definitions and properties of 1 F1 .

5

In particular, when β = 0, F

¡ 1+r
2

¢
, 12 , 0 = 1. The expansion of Ma (∆, X0 , r) in ∆ is, at the leading order,


¢ r r/2
¡

π−1/2 2r/2 Γ 1+r
σ ∆ + o(∆r/2 ) if r < 2


2
 ¡
¡
¢
¢
Ma (∆, θ, r) =
σ2 + β 2 + η λ ∆ if r = 2

´
³
2


 π−1/2 2r/2 ηr/2 λΓ ¡ 1+r ¢ H 1+r , 1 , β 2 e− β2η ∆ + o(∆)
2

3.

2

2 2η

if r > 2

Intuition for the Diﬃculty in Identifying the Parameters

Before turning to the formal study of estimators in the context of this model, I describe intuitively in this
section why distinguishing the volatility parameter from the jump component could be expected to be diﬃcult.
3.1. Isonoise Curves
The first intuition I provide is based on the traditional method of moments, combined with non-linear least
squares. We know that, in the nonlinear least squares context, the asymptotic variance of the estimator is
proportional to the inverse of the partial derivative of the moment function (or conditional mean) with respect
to the parameter. In other words, if small changes in the parameter value result in large changes in the moment
function then the parameter will be estimated precisely. If on the other hand large changes in the parameter
result in small changes in the moment function, then the parameter will not be estimated precisely.
I plot in Figure 3 what can be called isonoise curves. These are combinations of parameters of the process
that result in the same observable conditional variance of the log returns; excess kurtosis is also included. These
h
i
h
i
are the curves E (X∆ − X0 )2 |X0 = constant and E (X∆ − X0 )4 |X0 = constant, with the other parameters
fixed. Intuitively, any two combinations of parameters on the same isonoise curve cannot be distinguished by

the method of moments using these moments. (An additional issue is that in practice kurtosis is estimated with
little precision.) The top row of the figure looks at distinguishing σ2 from λ, the bottom one at distinguishing
λ from η (in the figure, ∆ = 1/12 and the other parameters are µ = β = 0, η1/2 = 0.6 in the top row and
σ = 0.3 in the bottom one). Combinations of the two parameters (λ, η) that are on the same isonoise curve
result in the same amount of “jumpiness” from the perspective of these two moments. This analysis provides
further arguments for including moments other than the variance and kurtosis in an a GMM-type setting (see
Section 5 below).
3.2. Inferring Jumps from Large Realized Returns
In discretely sampled data, every change in the value of the variable is by nature a discrete jump, yet we wish
to estimate jointly from these data the underlying continuous-time parameters driving the Brownian and jump
terms. So the next question I examine, still with the objective of providing some intuition, is the following:
given that we observe in discrete data an asset return of a given magnitude z or larger, what does that tell us

6

about the likelihood that such a change involved a jump (as opposed to just a large realization of the Brownian
noise)?
To investigate that question, let’s use Bayes’ Rule to calculate
Pr (B∆ = 1 | Z∆ ≥ z; θ) =

Pr (Z∆ ≥ z, B∆ = 1; θ)
Pr (Z∆ ≥ z; θ)

Pr (B∆ = 1; θ)
= Pr (Z∆ ≥ z | B∆ = 1; θ)
Pr (Z∆ ≥ z; θ)
´´
³
³
z−µ∆−β
exp (−λ∆) λ∆ 1 − Φ 2(η+∆σ
2 )1/2
³
³
´´
= P
+∞ exp(−λ∆)(λ∆)n
z−µ∆−nβ
1 − Φ 2(nη+∆σ2 )1/2
n=0
n!

since

Pr (B∆ = 1; θ) = exp (−λ∆) λ∆
!!
Ã
Ã
Z +∞
+∞
X
exp(−λ∆)(λ∆)n
z − µ∆ − nβ
q(z, ∆; θ)dz =
Pr (Z∆ ≥ z; θ) =
1−Φ
n!
z
2 (nη + ∆σ2 )1/2
n=0
Ã
!
z − µ∆ − β
Pr (Z∆ ≥ z | B∆ = 1; θ) = 1 − Φ
2 (η + ∆σ2 )1/2
where Φ denotes the Normal cdf and
q (y, ∆; θ) ≡ p(x0 + y|x0 , ∆; θ).
The probability of seeing more than one jump is:

since

Pr (B∆ ≥ 1; θ)
Pr (B∆ ≥ 1 | Z∆ ≥ z; θ) = Pr (Z∆ ≥ z | B∆ ≥ 1; θ)
Pr (Z∆ ≥ z; θ)
³
´´
P+∞ exp(−λ∆)(λ∆)n ³
z−µ∆−nβ
1
−
Φ
1/2
2
n=1
n!
³
³ 2(nη+∆σ ) ´´
= P
+∞ exp(−λ∆)(λ∆)n
z−µ∆−nβ
1 − Φ 2(nη+∆σ2 )1/2
n=0
n!
Pr (Z∆ ≥ z , B∆ ≥ 1; θ)
Pr (B∆ ≥ 1; θ)
P+∞
n=1 Pr (Z∆ ≥ z , B∆ = n; θ)
=
Pr (B∆ ≥ 1; θ)
+∞
X
Pr (B∆ = n; θ)
.
Pr (Z∆ ≥ z | B∆ = n; θ)
=
Pr (B∆ ≥ 1; θ)
n=1

Pr (Z∆ ≥ z | B∆ ≥ 1; θ) =

Then
Pr (B∆ = 0 | Z∆ ≥ z; θ) = 1 − Pr (B∆ ≥ 1 | Z∆ ≥ z; θ) .

7

(3.1)

If we look at jumps of a given size, irrespective of the direction, then:
Pr (B∆ = 1 | |Z∆ | ≥ z; θ) =
=
=

=

Pr (|Z∆ | ≥ z, B∆ = 1; θ)
Pr (|Z∆ | ≥ z; θ)
Pr (Z∆ ≥ z, B∆ = 1; θ) + Pr (Z∆ ≤ −z, B∆ = 1; θ)
Pr (Z∆ ≥ z; θ) + Pr (Z∆ ≤ −z; θ)
(Pr (Z∆ ≥ z | B∆ = 1; θ) + Pr (Z∆ ≤ −z | B∆ = 1; θ)) Pr (B∆ = 1; θ)
Pr (Z∆ ≥ z; θ) + Pr (Z∆ ≤ −z; θ)
³
³
´
³
´´
z−µ∆−β
−z−µ∆−β
1 − Φ 2(η+∆σ2 )1/2 + Φ 2(η+∆σ
exp (−λ∆) λ∆
2 )1/2
³
´
³
´´
P+∞ exp(−λ∆)(λ∆)n ³
z−µ∆−nβ
−z−µ∆−nβ
1 − Φ 2(nη+∆σ
+ Φ 2(nη+∆σ
2 )1/2
2 )1/2
n=0
n!

If the process is symmetric (µ = β = 0), then
!
Ã
!
Ã
z
−z
=1−Φ
Φ
2 (nη + ∆σ 2 )1/2
2 (nη + ∆σ2 )1/2

and it makes no diﬀerence whether we condition on |Z∆ | ≥ z or Z∆ ≥ z, in which case:
´´
³
³
z
exp (−λ∆) λ∆
2 1 − Φ 2(η+∆σ
1/2
2)
´´
³
³
Pr (B∆ = 1 | |Z∆ | ≥ z; θ) = P
+∞ exp(−λ∆)(λ∆)n
z
2
1
−
Φ
n=0
n!
2(nη+∆σ2 )1/2
³
³
´´
z
1 − Φ 2(η+∆σ
exp (−λ∆) λ∆
2 )1/2
³
³
´´
= P
n
+∞ exp(−λ∆)(λ∆)
z
1
−
Φ
1/2
n=0
n!
2(nη+∆σ 2 )

and similarly for Pr (B∆ ≥ 1 | |Z∆ | ≥ z; θ) .

Figure 4 plots the functions Pr (B∆ = 1 | Z∆ ≥ z; θ) (as well as the matching probabilities of zero and two
¡
¢1/2
jumps) evaluated at z = u∆1/2 σ2 + (β 2 + η)λ
, so u measures the size of the log-return observed in terms

of number of standard deviations away from the mean, at the same parameter values as above. The figure

shows that as far into the tail as 3.5 standard deviations, it is still more likely that a large observed log-return
was produced by Brownian noise only (since the probability of zero jump is higher than that of one jump).
Since 3.5 standard deviation moves are unlikely to begin with, and hence few of them will be observed in any
given series of finite length, this underscores the diﬃculty of relying on large observed returns as a means of
identifying jumps. Implicit in these calculations is also the interaction between the unconditional arrival rate
of the jumps and our ability to properly identify a large move as having been generated by a jump: if the
unconditional jump probability is low, then it takes an even bigger observed log-return before we can assign
its origin to a jump.
This said, it is intuitively clear that our ability to visually pick out the jumps from the sample path
increases when the time interval ∆ between successive observations on the path decreases. Figure 5 shows
this eﬀect by plotting the dependence of Pr (B∆ = 1 | |Z∆ | ≥ z; θ) , evaluated at z = 10%, as a function of the
sampling interval ∆. The smaller ∆, the higher the probability that an observed log-return of fixed magnitude
10% of greater was caused by a jump. Note however from the figure that our ability to infer the provenance of

8

the large move tails oﬀ very quickly as we move from ∆ equal to 1 minute to 1 hour to 1 day. At some point,
enough time has elapsed that the 10% move could very well have come from the sum over the time interval
(0, ∆) of all the tiny Brownian motion moves.
3.3. The Time-Smoothing Eﬀect
The final intuition for the diﬃculty in telling Brownian noise apart from jumps lies in the eﬀect of time
aggregation, which in the present case takes the form of time smoothing. Just like a moving average is
smoother than the original series, log returns observed over longer time periods are smoother than those
observed over shorter horizons. In particular, jumps get averaged out.
This eﬀect can be severe enough to make jumps visually disappear from the observed time series of log
returns. As an example, albeit extreme but real world, of this phenomenon, consider the eﬀect of the 1987
crash on the Dow Jones Industrials Average. As Figure 6 shows, there was no 1987 crash as far as the annual
data were concerned. Of course, the crash is quite visible at higher frequencies, the more so the higher the
frequency.
4.

Disentangling the Diﬀusion from the Jumps Using the Likelihood

Armed with these various intuitions, I now turn to the question of determining formally what is the eﬀect of
the presence of the jumps on our ability to estimate the value of σ2 . The ability to pick out jumps from the
sample path as well as the time-smoothing eﬀect suggest that our best chance of disentangling the Brownian
noise from the jumps lies in high frequency data. I will show that it is actually possible to recover the value of
σ2 with the same degree of precision as if there were no jumps and the only source of noise were the Brownian
motion. In other words, the various intuitions suggesting otherwise are misleading, at least in the limit of
infinitely frequent sampling.
4.1. Asymptotics
Since I will consider both likelihood and non-likelihood types of estimators below, I embed both types into
the GMM framework. Let Yn∆ = Xn∆ − X(n−1)∆ denote the first diﬀerences of the process X. They are i.i.d.
under this simple model. To estimate the d-dimensional parameter vector θ, consider a vector of m moment
conditions h(y, δ, θ), m ≥ d,continuously diﬀerentiable in θ (ḣ denotes the gradient of h with respect to θ).
Then form the sample average
mT (θ) ≡ N −1

N
X

h(Yn∆ , ∆, θ)

(4.1)

n=1

and obtain θ̂ by minimizing the quadratic form
QT (θ) ≡ mT (θ)0 GT mT (θ)
9

(4.2)

where GT is an m × m positive definite weight matrix assumed to converge in probability to a positive definite
limit G. If the system is exactly identified, m = d, the choice of GT is irrelevant and minimizing (4.2) amounts
to setting mT (θ) to 0.
To insure consistency of θ̂, h is assumed to satisfy
E [h(Y∆ ∆, θ 0 )] = 0.
It follows from standard arguments, subject to regularity conditions (see Hansen (1982)) that

(4.3)
√
T (θ̂ − θ0 )

converges in law to N (0, Ω), with
Ω−1 = ∆−1 D0 GD(D0 GSGD)−1 D0 GD

(4.4)

h
i
D ≡ E ḣ(Y∆ , ∆, θ 0 )

(4.5)

S ≡ E [h(Y∆ , ∆, θ0 )h(Y∆ , ∆, θ 0 )0 ]

(4.6)

where

is m × d, and

is m × m. The weight matrix GT can be chosen optimally to minimize the asymptotic variance Ω, by taking it

to be any consistent estimator of S −1 . A consistent first-step estimator of θ, needed to compute the optimal

weight matrix, can be obtained by minimizing (4.2) with GT = Id. When GT is then chosen optimally,
G = S −1 and as a result equation (4.4) reduces to
Ω−1 = ∆−1 D0 S −1 D.

(4.7)

In particular, (4.7) applies when the system is exactly identified (r = d) since the choice of the weight matrix
is irrelevant in that case.
4.2. Fisher’s Information in the Presence of Jumps
Of course, by choosing h to be the score vector, this class of estimator encompasses maximum likelihood. If
we let
l(y, δ, θ) ≡ ln p(x0 + y|x0 , δ; θ)
˙ δ, θ). Then S = E[l˙l̇0 ], D = −E[¨l] and
denote the log-likelihood function, this corresponds to h(y, δ, θ) = −l(y,
S=D

10

(4.8)

is Fisher’s Information matrix. The asymptotic variance of θ̂MLE takes the form
AVARMLE (θ) = ∆(DS −1 D)−1 = ∆D−1 .

(4.9)

The following theorem shows that, despite the diﬃculties described earlier, it is still possible, using maximum likelihood, to identify σ 2 with the same degree of precision as if there were no jumps:

Theorem 1. When the Brownian motion is contaminated by Poisson jumps, it remains the case that
¡ ¢
AVARMLE σ 2 = 2σ 4 ∆ + o(∆)

(4.10)

so that in the limit where sampling occurs infinitely often (∆ → 0), the MLE estimator of σ2 has the same
asymptotic distribution as if no jumps were present.

Theorem 1 says that maximum-likelihood can theoretically perfectly disentangle σ 2 from the presence of
the jumps, when using high frequency data. I will show in Section 7 below Monte Carlo evidence that suggests
that this holds true in practice, too.
Note also that the result of Theorem 1 states that the presence of the jumps imposes no cost on our ability
to estimate σ2 : the variance which is squared in the leading term is only the diﬀusive variance σ2 , not the
total variance σ 2 + (β 2 + η)λ. This can be contrasted with what would happen if, say, we contaminated the
Brownian motion with another independent Brownian motion with known variance s2 . In that case, we could
¢2
¡
also estimate σ2 , but the asymptotic variance of the MLE would be 2 σ2 + s2 ∆.

What is happening here is that, as ∆ gets smaller, our ability to identify price discontinuities improves

(recall Figure 5).This is because these Poisson discontinuities are, by construction, discrete, and there are
few of them relative to the diﬀusive moves. Then if we can see them, we can exclude them, and do as if
they did not happen in the first place. More challenging therefore will be the case where the jumps occur
infinitely often and are infinitely frequent (see Section 6 below). But before examining that question, I will
investigate the ability of a large class of GMM estimators to approach the eﬃciency of MLE. Indeed, in light
of the Cramer Rao lower bound, Theorem 1 establishes 2σ 4 ∆ as the benchmark for alternative methods that
attempt to estimate σ 2 (based on the quadratic variation, absolute variation, absolute power variation, etc.)
and it is interesting to know how closely GMM using such moment functions can approximate MLE.
5.

Using Moments: How Close Does GMM Come to MLE?

The first question I now address is whether the identification of σ 2 achieved by the likelihood, despite the
presence of jumps, can be reproduced by conditional moments of the process of integer or non-integer type,
and which moments or combinations of moments come closest to achieving maximum likelihood eﬃciency
.While it is clear that MLE is the preferred method, and as discussed above has been used extensively in
11

that context, it is nevertheless instructive to determine which specific choices of moment functions do best
in terms of approximating its eﬃciency. So, in GMM estimation, I form moment functions of the type
h(y, δ, θ) = y r − M (δ, θ, r) and/or h(y, δ, θ) = |y|r − Ma (δ, θ, r) for various values of r. By construction,
these moment functions are unbiased and all the GMM estimators considered will be consistent. The question
becomes one of comparing their asymptotic variances among themselves, and to that of MLE.
I will refer to diﬀerent GMM estimators of θ by listing the moments M (∆, θ, r) and/or Ma (∆, θ, r) that
are used for that particular estimator. For example, the estimator of σ2 obtained by using the single moment
M (∆, θ, 2) corresponds to the discrete approximation to the quadratic variation of the process. Estimators
based on the single moment Ma (δ, θ, r) correspond to the power variation, etc. By using Taylor expansions
in ∆, I characterize in closed form the properties of these diﬀerent GMM estimators.
5.1. Estimating σ2 Alone
I start with the case where only σ2 is to be estimated. The jump term, while present, has known parameters,
or one could think of it as being a nuisance process that is of no interest. For simplicity, I will assume here
and in the rest of the paper that the drift and mean jump are centered at zero (µ = β = 0). This assumption
is largely inconsequential, except that it greatly simplifies the expressions below. It also makes the standard
moments of odd order zero.
The technique I use to obtain tractable closed form expressions for the asymptotic variances of the diﬀerent
estimators under consideration is to Taylor-expand them in ∆ around ∆ = 0 (see Aït-Sahalia and Mykland
(2003a) for another use of this technique in a diﬀerent context). Indeed, computation of AVARGMM requires
the separate computation of the matrices D and S in (4.5)-(4.6). These matrices are expected values of
functionals of the moment vector h, taken with respect to the law of the observed process Y∆ . In the present
example, this law has density (2.4). With polynomial moment functions in h (including possibly absolute
values and non integer powers), the functionals ḣ and hh0 retain the polynomial form. Thus D and S can be
computed explicitly using the moments calculated in Proposition 1. If non-polynomial moment functions were
to be used, the corresponding calculations would involve Taylor-expanding using the infinitesimal generator
of the process, as described in (2.7). The results for D and S can then be combined to form the matrix Ω
in (4.7), which in turn has a natural expansion in powers of ∆.(again, possibly non-integer when non-integer
moments are used).
With this method, it becomes possible to compare diﬀerent estimators by looking at the Taylor expansions
of their respective asymptotic variances. I find that, although it does not restore full maximum likelihood
eﬃciency, using absolute moments in GMM helps greatly. In particular, the next proposition shows that
¡ ¢
when σ 2 is estimated using exclusively moments of the form M (∆, θ, r), then AVARGMM σ2 = O(1), a full

order of magnitude bigger than achieved by MLE. When moments of the form Ma (∆, θ, r) with r ∈ (0, 1) are
¡ ¢
used, however, AVARGMM σ2 = O(∆), i.e., the same order as achieved by MLE, although the constant of
proportionality is always greater than 2∆σ4 as should be the case in light of the Cramer-Rao lower bound.
12

¡ ¢
When σ2 is estimated based on the moment Ma (∆, θ, r) with r ∈ (1, 2] are used, AVARGMM σ 2 = O(∆2−r ).
Specifically:

Proposition 2. The following table gives the asymptotic variance of the GMM estimator of σ2 using diﬀerent
combinations of moment functions:
¡ ¢
AVARGMM σ2 with jumps

Moment(s)

¡
¢2
3η2 λ + 2∆ σ2 + ηλ

M (∆, θ, 2)



M (∆, θ, 2)
M (∆, θ, 4)




³
+ ∆ 2σ4 +

6η 2 λ
7

4

∆ 4σ
r2

Ma (∆, θ, r) , r ∈ (0, 1)

Ma (∆, θ, r) , r ∈ (1, 2]



M (∆, θ, 2)
Ma (∆, θ, 1)

µ

44η 2 λ2
7

π1/2 Γ( 12 +r)
Γ( 1+r
2 )

2

+

2∆σ4

100ηλσ 2
49

´

2∆σ4

+ o(∆)

¶
− 1 + o(∆)

4

∆ 4σ
r2

¡
¢
2∆σ 2 (π − 2) σ2 + πηλ

Ma (∆, θ, 1)



¡ ¢
AVARGMM σ 2 no jumps




2−r 4π

∆

1/2 r

η λσ2(2−r) Γ( 12 +r)
r 2 Γ( 1+r
2 )

³
2∆σ2 (π − 2) σ2 +

2

+ o(∆

´

π1/2 Γ( 12 +r )
2

Γ( 1+r
2 )

¶
− 1 + o(∆)

2 (π − 2) ∆σ 4
2−r

(3π−8)
ηλ
3

µ

)

4
∆ 4σ
r2

+ o(∆)

µ

π1/2 Γ( 12 +r )
2

Γ( 1+r
2 )

¶
− 1 + o(∆)

2∆σ4

¡ ¢
When the moments (Ma (∆, θ, r) , Ma (∆, θ, q))0 are used jointly, AVARGMM σ2 is given in the no jumps

case by

¡ ¢
2π1/2 A (r, q)
AVARGMM σ2 = 2∆σ4
+ o(∆)
B (r, q)

where

¶Ã
¶
µ
µ
µ
¶2 !
¶ µ
¶ µ
¶
1
1
+
r
1
1+q
1+r
1+q+r
1/2
A(r, q) = Γ
+q
π Γ
+r −Γ
+ 2Γ
Γ
Γ
2
2
2
2
2
2
¶
µ
µ
¶2
¶2 µ
1+q+r
1+q
1
+r
−Γ
Γ
−π1/2 Γ
2
2
2
¶ µ
¶
µ
µ
µ
µ
¶2
¶2 Ã
¶2 !
1
+
r
1
1
+
q
1
1
+
r
2
B(r, q) = π 1/2 r2 Γ
+q Γ
+ r − (q − r) Γ
+Γ
π1/2 q 2 Γ
2
2
2
2
2
µ
¶ µ
¶ µ
¶
1+q
1+r
1+q+r
−2π 1/2 qrΓ
Γ
Γ
2
2
2
µ

13

Figure 7 plots the eﬃciency of the GMM estimator of σ 2 using .Ma (∆, θ, r), relative to MLE, as a function
of r. In light of the table in Proposition 2, this is given by the function
!
Ã
¡
¢
2 π1/2 Γ 12 + r
r 7→ 2
¢2 − 1 .
¡
r
Γ 1+r
2

In the absence of jumps, the minimum is achieved by selecting r = 2, which reproduces the MLE’s asymptotic

variance of 2∆σ4 . This is not surprising since the MLE for σ2 in the absence of jumps is simply the quadratic
variation of the process (at frequency ∆−1 ). When jumps are present, however, absolute moments taken
individually (even though they do better than regular moments) are no longer capable of attaining the eﬃciency
of MLE. Figure 8 plots the corresponding picture, at the weekly frequency and the same parameters as above.
However, taking such absolute moments of diﬀerent orders in combination improves upon any single one.
Figure 9 plots the relative eﬃciency, as a function of (r, q) that results from estimating σ2 using the overidentified GMM system based on the vector of moment conditions (Ma (∆, θ, r) , Ma (∆, θ, q))0 . Given Proposition
2, this is the surface
(r, q) 7→

2π1/2 A (r, q)
B (r, q)

In the no jumps case, using two functions improves upon one when r 6= 2, and achieves MLE eﬃciency provided
one of the two is the quadratic variation. When jumps are present, however, it is only asymptotically, as the
number of these absolute moment functions increases, that GMM can reproduce MLE.
Finally, note that at the leading order in ∆, GMM makes little use of the quadratic variation M (∆, θ, 2)
when an absolute moment of the type Ma (∆, θ, r) is also part of the h vector. Comparing the asymptotic
variance in the case where (M (∆, θ, 2) , Ma (∆, θ, 1)0 are both used together to that where only Ma (∆, θ, 1)
is used, we see that the decrease in variance is only
¶
µ
¡
¢
16
(3π − 8)
2∆σ 2 (π − 2) σ 2 + πηλ − 2∆σ2 (π − 2) σ 2 +
ηλ = ∆σ 2 ηλ.
3
3
5.2. Estimating σ2 and λ Together
I now turn to the case where both σ 2 and λ are to be jointly estimated.

0
Proposition 3. When (σ 2 , λ) are estimated using the moments (M (∆, θ, 2) , M (∆, θ, 4)) , the resulting
¡ 2 ¢
AVARGMM σ , λ is




14η2 λ
3

+

2∆(70η2 λ2 +22ηλσ 2 +3σ 4 )
3

+ O(∆2 )

−20ηλ
3
35λ
3

•

14

2∆λ(79ηλ+28σ 2 )
+ O(∆2 )
3
2
2∆λ(91ηλ+40σ )
+ O(∆2 )
3η

−

+




(5.1)

¡
¢
When the moments (M (∆, θ, 2) , Ma (∆, θ, 1/2))0 are used, the resulting AVARGMM σ 2 , λ is



¡
¢
2∆σ 2 σ2 + ηλ (π − 2) + O(∆3/2 )
•


2∆σ2 ((π−2)ηλ+(π−3)σ 2 )
3/2
−2∆1/2 η 1/2 λσ −
+
O(∆
)
η

´
³
4
2
4∆1/2 λσ
2πλσ2
+ O(∆2 )
3λ + η1/2 + ∆ 2λ + η + 2(π−3)σ
η2

(5.2)

As in the σ2 alone situation, the introduction of an absolute moment of the type Ma (∆, θ, r) reduces the
asymptotic variance of the GMM estimator of σ2 by an order of magnitude, from O(1) in (5.1) to O(∆) in
(5.2), which is the same rate as MLE but with a higher constant.
5.3. Estimating σ2 , λ and η Together
The following result gives the asymptotic variance of the GMM estimator of (σ2 , λ, η), estimated jointly.

Proposition 4. When (σ 2 , λ, η) are estimated using the moments (M (∆, θ, 2) , M (∆, θ, 4) , Ma (∆, θ, 1/2))0 ,
¡
¢
the resulting matrix AVARGMM σ2 , λ, η has the following elements
¡
¢
2∆σ 2 3 (π − 2) σ2 + (3π − 7) ηλ
+ O(∆3/2 )
3
¡
¢
¡ 2 ¢
2∆σ2 (6π − 13) ηλ + 6 (π − 3) σ 2
1/2 1/2
σ , λ : −2∆ η λσ −
+ O(∆3/2 )
3η
¡
¢
¡ 2 ¢
2∆σ2 (π − 2) ηλ + (π − 3) σ2
1/2 3/2
σ , η : 2∆ η σ +
+ O(∆3/2 )
λ
´
³

¡ 2 2¢
σ ,σ
:

2

2

4

+ 24πλσ
− 24(3−π)σ
110λ2 − 16λσ
η
η
η2
11λ 8∆1/2 λσ
+
+ O(∆3/2 )
+∆
(λ, λ) :
1/2
3
3
η
µ
¶
8η
4(3 − π)σ 4 122ηλ 10σ2
1/2 1/2
2
−
−
− 4πσ + O(∆3/2 )
(λ, η) : − − 6∆ η σ + ∆
3
ηλ
3
3
¡
¢
2
14η2 4∆1/2 η3/2 σ 2∆ 70η2 λ + 17ηλσ2 + 3πηλσ 2 + 3(π − 3)σ4
+
+
+ O(∆3/2 )
(η, η) :
3λ
λ
3λ2

Comparing the asymptotic variance of σ 2 in the case where only λ is estimated along with σ2 (the upper
left element of (5.2)) to that obtained in Proposition 4 measures the cost associated with not knowing η, given
the moment functions used. That cost is given here by:
¡
¢
¡
¢
2∆σ2 3 (π − 2) σ 2 + (3π − 7) ηλ
4
− 2∆σ 2 σ2 + ηλ (π − 2) = ∆σ2 ηλ.
3
3
6.

Disentangling the Diﬀusion from Other Jump Processes: The Cauchy Case

Theorem 1 demonstrated the ability of maximum-likelihood to fully distinguish the diﬀusive component from
the jump component on the basis of the full sample path. I then showed under what circumstances (i.e., choices
of moment functions) GMM was able to approach this result, although not fully reproduce the eﬃciency of
MLE. I now examine whether the perfect distinction aﬀorded by MLE is specific to the fact that the jump
15

process considered so far was a compound Poisson process, or whether it extends to other types of jump
processes. Among the class of continuous-time Markov processes, it is natural to look at Lévy processes. As I
will discuss below, Poisson jumps are a unique case in the Lévy universe. Yet, it is possible to find examples
of other pure jump processes for which the same result continues to hold, which cannot be explained away as
easily as in the Poisson case.
6.1. Lévy Processes
I start by briefly reviewing the main properties of Lévy processes that I will use in the rest of the paper
(see e.g., Bertoin (1998), for further details). A process is a Lévy process if it has stationary and independent increments and is continuous in probability. A Lévy process can be decomposed as the sum of three
independent components: a linear drift, a Brownian motion and a pure jump process. Correspondingly, the
log-characteristic function of a sum of independent random variables being the sum of their individual characteristic functions, the characteristic function of a Lévy process is given by the Lévy-Khintchine formula, which
states that there exist constants γ c ∈ R, σ ≥ 0 and a positive sigma-finite measure ν(·) on R\{0} (extended
to R by setting v ({0}) = 0) satisfying
Z

+∞

−∞

¢
¡
min 1, z 2 ν(dz) < ∞

(6.1)

such that the log-characteristic function ψ(u), defined by

E[eiuX∆ |X0 = 0] = eψ(u)∆
has the form for u ∈ R :
ψ(u) = iγ c u −

σ2 2
u +
2

Z

+∞ ¡

−∞

¢
eiuz − 1 − iuzc(z) ν(dz).

(6.2)

The three quantities (γ c , σ, ν(·)), called the characteristics of the Lévy process, completely describe the
probabilistic behavior of the process. γ c is the drift rate of the process, σ its volatility from the Brownian
component and the measure ν(·) describes the pure jump component. It is known as the Lévy measure and has
the interpretation that ν(E) for any subset E ⊂ R is the rate at which the process takes jumps of size x ∈ E,
i.e., the number of jumps of size falling in E per unit of time. Sample paths of the process are continuous
if and only if ν ≡ 0. Note that ν(·) is not necessarily a probability measure, in that ν(R) may be finite or
infinite.
The function c(z) is a weighting function whose role is to make the integrand in (6.2) integrable. When
|z|ν(dz) is integrable near 0, it is enough to have
eiuz − 1 − iuzc(z) = O(|z|) as z → 0
and this can be achieved simply by setting c = 0 near z = 0. But if |z|v(dz) is not integrable near 0 (and only
16

z 2 v(dz) is, as required by (6.1)), then one needs a non-zero c(z) function, which must insure in light of (6.1)
that
eiuz − 1 − iuzc(z) = O(z 2 ) as z → 0,

(6.3)

that is, c(z) ∼ 1 near z = 0. When |z|ν(dz) is integrable near ∞, it is enough to have
eiuz − 1 − iuzc(z) = O(|z|) as z → ±∞
and this can be achieved simply by setting c = O(1) near z = ∞. But if |z|v(dz) is not integrable near ∞ (and
only v(dz) is, as required by (6.1)), then one needs
eiuz − 1 − iuzc(z) = O(1) as z → ±∞,

(6.4)

that is, c(z) = O (1/|z|) near z = ±∞. Typical examples include c(z) = 1/(1 + z 2 ), c(z) = 1(|z| < ε) for some
ε > 0, etc.
The function c(z) can be replaced by another one. Any change in the weighting function from c(z) to c0 (z)
is absorbed by a matching change in γ c , which is replaced by γ 0c in such a way that
γc −

Z

+∞

zc(z)ν(dz) =

γ 0c

−∞

−

Z

+∞

zc0 (z)ν(dz).

−∞

The infinitesimal generator of the process is given by
µ
¶
Z +∞
∂f (∆, x, x0 )
∂f (∆, x, x0 )
A · f (∆, x, x0 ) =
+ γc −
zc(z)ν(dz)
∂∆
∂x
−∞
Z +∞
2
1 ∂ f (∆, x, x0 )
+
{f (∆, x + z, x0 ) − f(∆, x, x0 )} ν(dx).
+ σ2
2
∂x2
−∞
Examples of Lévy processes include the Brownian motion (c = 0, γ c = 0, σ = 1, ν = 0), the Poisson
process (c = 0, γ c = 0, σ = 0, ν(dx) = λδ 1 (dx) where δ 1 is a Dirac point mass at x = 1) and the Poisson
jump diﬀusion I considered above in (2.1), corresponding to c = 0, γ c = µ, σ > 0, ν(dx) = λn(x; β, η)dx where
n(x; β, η) is the Normal density with mean β and variance η.
The question I now address is whether it is possible to perturb the Brownian noise by a Lévy pure jump
process other than Poisson, and still recover the parameter σ2 as if no jumps were present. The reason one
might expect this not to be possible is the fact that, among Lévy pure jump processes, the Poisson process
is the only one with a finite ν(R), i.e., a finite number of jumps in a finite time interval (and the sample
paths are piecewise constant). In that case, define λ = ν(R) and the distribution of the jumps has measure
n(dx) = v(dx)/λ. All other pure jump processes are such that ν([−ε, +ε]) = ∞ for any ε > 0, so that the

process exhibits an infinite number of small jumps in any finite time interval.2 Intuitively, these tiny jumps

ought to be harder to distinguish from Brownian noise, which is itself made up of many small moves. Can the
2 The

number of “big” jumps remains finite: ν((−∞, −ε) ∪ (ε, +∞)) < ∞.

17

likelihood still tell them perfectly apart from Brownian noise?
I will consider as an example the Cauchy process, which is the pure jump process (σ = 0) with Lévy
measure
α
dx
x2

ν(dx) =

(6.5)

and, with weight function c(z) = 1/(1 + z 2 ), γ c = 0. This is an example of a symmetric stable distribution of
index 0 < ξ < 2 and rate α > 0, with log characteristic function proportional to ψ(u) = −(α |u|)ξ , and Lévy
measure
ν(dx) =

αξ ξ
1+ξ

|x|

dx.

(6.6)

The Cauchy process corresponds to ξ = 1, while the limit ξ → 2 (from below) produces a Gaussian distribution.
While, as a result of (6.1), all Lévy processes have finite quadratic variation almost surely, the absolute
variation of the process will be finite only if σ = 0 and if |z| ν(dz) is integrable near 0, a condition that fails
for the Cauchy process but is satisfied by the Poisson process (and gamma, beta, and simple homogeneous
examples). More generally, for r > 0,


Z
X
r


Pr
|Xs − Xs− | < ∞ = 1 ⇐⇒

+∞

−∞

0≤s≤t

min (1, |z|r ) ν(dz) < ∞

(6.7)

which in the case (6.6) is equivalent to r > ξ.

6.2. Mixing Cauchy Jumps with Brownian Noise
So I now look at the situation where
dXt = µdt + σdWt + dCt

(6.8)

where Ct is a Cauchy process independent of the Brownian motion Wt . Focusing on the ability to disentangle
σ2 from the jumps, let’s consider again the case where µ = 0. The solution of the stochastic diﬀerential
equation (6.8) is
X∆ − X0

= σW∆ + C∆
√
= σ ∆Z∆ + C∆

(6.9)

where Z∆ ∼ N (0, 1). Equation (6.9) implies again that the log-returns Yn∆ = Xn∆ − X(n−1)∆ are i.i.d.
By independence of C and W, the transition density of the process X is given by the convolution of their
respective densities
fX∆ (y) =

fσ√∆Z∆ +C∆ (y)

=

Z

+∞

−∞

18

fσ√∆Z∆ (y − z)fC∆ (z)dz.

To obtain the density fC∆ (z), recall that the log-characteristic function of C is given by (6.2),

ψC (u) =

Z

+∞

−∞

µ
¶
izu
α
iuz
dz = −πα |u| ,
e −1−
1 + z2 z2

(6.10)

with the density following by Fourier inversion since the characteristic function exp(ψC (u)) is integrable:
fC∆ (z) =
=

1
2π

Z

+∞

exp (−iuz + ψC (u)∆) du

−∞

∆α

∆2 α2 π2

+ z2

.

Finally, for this process the density q(y|∆; θ) = p(x + y|x0 , ∆; θ) is given by the convolution
Ã
!
Z +∞
(y − z)2
∆α
1
fX∆ (y) =
exp −
dz.
√ √
2
2
2
2
2∆σ
∆ α π2 + z2
2π ∆σ
−∞

(6.11)

(6.12)

which is known as the Voigt function.
The question now becomes whether it is still possible, using maximum likelihood, to identify σ2 with the
same degree of precision as if there were no jumps, despite the fact that the Cauchy process contaminates the
Brownian motion with infinitely many infinitesimal jumps. The answer is, surprisingly, yes:

Theorem 2. When the Brownian motion is contaminated by Cauchy jumps, it still remains the case that
¡ ¢
AVARMLE σ2 = 2σ4 ∆ + o(∆).

(6.13)

6.3. Intuition for the Result: How Big is That Infinite Number of Small Jumps?
Theorem 2 has shown that Cauchy jumps do not come close enough to mimicking the behavior of the Brownian
motion to reduce the accuracy of the MLE estimator of σ2 . The intuition behind this surprising result is the
following: while there is an infinite number of small jumps in a Cauchy process, this “infinity” remains relatively
small (just like the cardinal of the set of integers is smaller than the cardinal of the set of reals) and while
the jumps are infinitesimally small, they remain relatively bigger than the increments of a Brownian motion
during the same time interval ∆. In other words, they are harder to pick up from inspection of the sample
path than Poisson jumps are, but with a fine enough microscope, still possible. And the likelihood is the best
microscope there is, per Cramer-Rao.
I now show formally how this works:

Lemma 1. Fix ε > 0. If Y∆ is the log-return from a pure Brownian motion, then
r
µ
¶
ε2
∆1/2 σ 2
exp −
Pr (|Y∆ | > ε) =
(1 + o(1))
ε
π
2∆σ2
19

(6.14)

is exponentially small as ∆ → 0. However, if Y∆ results from a Lévy pure jump process with jump measure
v(dz), then under regularity conditions
Pr (|Y∆ | > ε) = ∆ ×

Z

v(dy) + o(∆)

(6.15)

|y|>ε

which decreases only linearly in ∆.
For example, for a Cauchy process a direct calculation based on the density (6.11) yields
Pr (|Y∆ | > ε) =

Z

fC∆ (y)dy = ∆

|y|>ε

2α
2α3 π2
− ∆3
+ O(∆5 )
ε
3ε3

(6.16)

whose leading term coincides with (6.15) when v is replaced by its Cauchy expression (6.5). More generally,
with a symmetric stable process with order ξ, whose Lévy measure is given in (6.6), we have
Pr (|Y∆ | > ε) = ∆ ×

2αξ
+ o(∆).
εξ

(6.17)

The key aspect here is that the order in ∆ of Pr (|Y∆ | > ε) for a pure jump Lévy process is always O(∆).
In other words, Lévy jump processes will always produce moves of size greater than ε at a rate far greater than
the Brownian motion. Brownian motion will have all but an exponentially small fraction of its increments of
size less than any given ε. Lévy pure jump processes with infinite ν(R) (i.e., all except the compound Poisson
process), despite producing an infinite amount of small jumps will not produce quite as many small moves as
Brownian motion does: “only” a fraction 1 − O(∆) of their increments are smaller than ε. It’s a question of
two “infinities” one growing linearly, the other exponentially.
In that sense, all of these Lévy pure jump processes produce tiny jumps (those of size less than ε) at the
same rate 1 − O(∆) as a compound Poisson process does:
Pr (|Y∆ | > ε) = ∆ × λ

Z

n(y; β, η)dy + o(∆)

(6.18)

|y|>ε

since in the example considered above the jumps J have density n(x; β, η). The probability of seeing a move
greater than ε is at the first order in ∆ the probability that one jump occurs, i.e.,
Pr (N∆ = 1) = ∆λ + O(∆2 )
times the probability that J will be of size at least ε.
Do jumps always have to behave that way? The answer is yes, in light of the following. Ray (1956) showed
that the sample paths of a Markov process are almost surely continuous if and only if, for every ε > 0,
Pr (|Y∆ | > ε) = o (∆)

(6.19)

Ray’s condition maps out the continuity of the sample path into a bound on the size of the probability of leaving
a given neighborhood in the amount of time ∆; intuitively, this probability must be small as ∆ goes to zero if
20

the sample paths are to remain continuous. Condition (6.19) says how small this probability must be as ∆ gets
smaller. But, since condition (6.19) is necessary and suﬃcient, it also establishes a lower bound for how big the
probability of making a move greater than ε must be if the process is not continuous, i.e., can jump. Based on
what we have seen, it is therefore natural to have for a Lévy pure jump process Pr (|Y∆ | > ε) = O(∆) as stated
in (6.15), and not o(∆). Further, while I wrote (6.19) assuming that the process has independent increments
(i.e., be Lévy), this condition is valid also for processes with dependent increments: replace Pr (|Y∆ | > ε) with
Pr (|X∆ − X0 | > ε|X0 = x0 ) and add the requirement that it be satisfied uniformly for x0 in a compact.
Since
Pr (|Y∆ | ≤ ε) = 1 − Pr (|Y∆ | > ε)
is the probability of making small moves (the ones that look like Brownian motion), this eﬀectively puts an
upper bound on the ability of a jump process to imitate the behavior of Brownian volatility. So the result
is likely not driven by the fact that the divergence of ν(dx) near 0 is only O(|x|−2 ) for the Cauchy process,
instead of for instance O(|x|−(1+ξ) ) with ξ greater than 1 but smaller than 2 (ξ → 2 provides the maximum
admissible amount of small jumps per unit of time, while still satisfying the requirement (6.1)).
7.

Monte Carlo Simulations

A legitimate question at this point is whether Theorems 1 and 2, which are statements about the behavior
of the estimators at high frequency, have relevance at the observation frequencies that we typically encounter
in asset pricing. So, in this section, I report the results of Monte Carlo simulations designed to examine the
empirical adequacy of the theoretical results when we observe asset prices once a day. The daily frequency is
generally considered to be low enough to be largely unaﬀected by the market microstructure issues that can
substantially derail the performance of high frequency quantities such as the realized quadratic variation, etc.
(see Aït-Sahalia and Mykland (2003b) for an analysis of the eﬀect of market microstructure noise on high
frequency estimates).
Starting with the jump-diﬀusion (2.1), I simulate 5, 000 sample paths, each of length n = 1, 000 at the daily
frequency (∆ = 1/252). To demonstrate the ability of the likelihood to disentangle the volatility parameter
from the jumps, I purposefully set the arrival rate of the jumps at a high level, λ = 5 in the Poisson case. Five
jumps per year on average is much higher than would be realistic based on actual estimates for stock index
returns (I use λ = 0.2 in all figures above). I set the value of σ at a realistic level, σ = 0.3. So there is relatively
little volatility given the amount of jumps, which should make it more diﬃcult to distinguish volatility from
jumps among the overall amount of noise. The standard deviation of the jump size is η1/2 = 0.6. The process
is symmetric (µ = β = 0). In the Cauchy case, I set α = 0.2 and σ = 0.3. Again, jumps are plentiful.
I then estimate the parameter σ2 using MLE. I also repeated the experiment estimating both (σ2 , λ) in the
Poisson case, (σ 2 , α) in the Cauchy case to investigate the eﬀects of joint estimation on our ability to distinguish

21

the volatility parameter from the jump component. Figures 10 and 11 report the small sample and asymptotic
distributions for (σ2 , λ) and (σ2 , α) respectively. The histograms show that despite the large number of jumps
the estimates of σ 2 remain in a tight interval around the true value of 0.09. And the sample variance is quite
close to the values predicted by Theorems 1 and 2 (corresponding to the asymptotic distribution) despite
the fact that the data are only sampled once a day. These results show that the theoretical asymptotic
distribution of MLE as derived above provides a good approximation to the small sample behavior of the
estimators at the daily frequency. Finally, Figures 12 and 13 show the resulting confidence regions for the two
joint estimation problems in the Poisson and Cauchy cases. Note that in the Poisson case, the estimators are
largely uncorrelated hence the roughly circular shape of the joint confidence interval. This is not the case in
the Cauchy situation however.
8.

Conclusions

I studied the eﬀect of the presence of jumps on our ability to identify the volatility component of the log
returns process and found that, somewhat surprisingly, jumps had no detrimental eﬀect as far as maximum
likelihood estimation was concerned. I also discussed which moment conditions are better than others in terms
of approaching the eﬃciency of maximum likelihood for this problem.
Even more surprisingly, the result did not depend on the jumps being large and infrequent, i.e., Poisson
jumps. It remains valid in the case of Cauchy jumps which can be infinitely small in magnitude, and infinitely
frequent. Finally, I provided an explanation of this phenomenon based on the fact that all jump processes,
despite having an infinite number of small jumps, have distinguishing characteristics relative to Brownian
motion that ultimately can be picked up by the likelihood. As discussed in the introduction, this result means
that the recent literature in finance that introduces Lévy processes in option pricing, portfolio choice or risk
management will not face major obstacles from the econometric side, despite what the intuition might have
initially suggested.

22

Appendix A:

Proof of Proposition 1

Writing the expected value in terms of the density of the discrete increments Y∆ = X∆ − X0 , we have, with
b ≡ ∆ (µ + βλ)
∆a (∆, θ, r) = E [|Y∆ − ∆ (µ + βλ)|r ]
Z +∞
=
|y − ∆ (µ + βλ)|r q (y, ∆; θ) dy
−∞
b

=

Z

−∞

(b − y)r q (y, ∆; θ) dy +

Z

b

+∞

(y − b)r q (y, ∆; θ) dy

Then recall from (2.4) that
Ã
!
+∞
X
exp(−λ∆)(λ∆)n
(y − µ∆ − nβ)2
q (y, ∆; θ) =
exp −
√ p
2 (nη + ∆σ 2 )
2π nη + ∆σ 2 n!
n=0

and compute each of the two integrals term by term.

Each term can be computed from the form
Ã
Ã
!
!
Z b
Z +∞
(y − a)2
(y − a)2
1
1
r
r
(b − y) √
(y − b) √
exp −
exp −
dy +
dy
2v
2v
2πv
2πv
−∞
b
Ã
! µ
!
¶ Ã
(b − a)2
1+r
1 + r 1 (b − a)2
r/2 −1/2
, ,
exp −
= (2v) π
Γ
F
2v
2
2
2
2v
based on Section 13.2 in Abramowitz and Stegun (1972). Summing the terms over n after replacing the values
a and v by their expressions a = µ∆ + nβ and v = ∆σ2 + nη yields the result.
Appendix B:

Proof of Theorem 1

Fisher’s Information for σ2 is
Iσ 2

= E

"µ

∂ ln q (y|∆; θ)
∂σ2

"µ

¶2 #

¶2 #
∂q (y|∆; θ)
1
= E
∂σ 2
q (y|∆; θ)
¶2
Z +∞ µ
dy
∂q (y|∆; θ)
=
2
∂σ
q
(y|∆;
θ)
−∞

(B.1)

where q is given by (2.4):
µ
¶
+∞
X
exp(−λ∆)(λ∆)n
y2
q(y|, ∆; θ) =
exp −
√ p
2 (nη + ∆σ2 )
2π nη + ∆σ2 n!
n=0
≡

+∞
X

n=0

qn (y|, ∆; θ)

exp(−λ∆)(λ∆)n
n!

23

(B.2)

so that
µ
¶
+∞
¡ 2
¢
y2
∂q (y|∆; θ) X exp(−λ∆)(λ∆)n ∆
exp
−
y − (nη + ∆σ 2 ) .
=
√
2
2
5/2
2
∂σ
2 (nη + ∆σ )
n!
n=0 2 2π (nη + ∆σ )
Since the presence of the jumps cannot increase the information we have about σ2 relative to the no-jumps
case, it must be that
Iσ2 ≤

1
.
2σ4

(B.3)

The idea is now to integrate (B.1) on a restricted subset of the real line, (−a∆ , +a∆ ), yielding from the
positivity of the integrand

Iσ2 =

Z

+∞

−∞

³

∂q(y|∆;θ)
∂σ2

´2

q (y|∆; θ)

dy ≥

Z

+a∆

−a∆

³

∂q(y|∆;θ)
∂σ2

´2

q (y|∆; θ)

dy

(B.4)

and then to select a∆ small enough that q (y|∆; θ) has a simpler expression on (−a∆ , +a∆ ) yet with enough
of the support of the density included in (−a∆ , +a∆ ) that
Z

+a∆

−a∆

³

∂q(y|∆;θ)
∂σ2

´2

q (y|∆; θ)

dy =

1
+ o(∆).
2σ 4

Combining the upper and lower bounds (B.3)-(B.4) will give the desired result
Iσ2 =

1
+ o(∆)
2σ 4

(B.5)

which, in light of (4.9), will prove the Theorem.
Set a∆ to be the positive solution of
q0 (a∆ |, ∆; θ) = q1 (a∆ |, ∆; θ),

(B.6)

that is
a∆

h ³
¡
¢
= ∆1/2 η + ∆σ 2 ση −1/2 ln 1 +
= σ [−∆ ln (∆)]1/2 (1 + o(1)).

η ´i1/2
∆σ2

For all y ∈ (−a∆ , +a∆ ), we have
q0 (y|, ∆; θ) > q1 (y|, ∆; θ) > ... > qn (y|, ∆; θ) > ...
and so from (B.2)
1
1
≥
.
q (y|∆; θ)
q0 (y|∆; θ)

24

Therefore
³
´2
Z +a∆ ∂q(y|∆;θ)
2
∂σ

q (y|∆; θ)

−a∆

dy

≥

Z

=

Z

+a∆

−a∆
+a∆

−a∆

=

Z

+a∆

−a∆

≡
=

Z

+a∆

−a∆

³

∂q(y|∆;θ)
∂σ 2

´2

dy
q0 (y|∆; θ)
³P
+∞ exp(−λ∆)(λ∆)n ∆

´¡
³
¢´2
y2
y 2 − (nη + ∆σ2 )
exp − 2(nη+∆σ
2)
´
³
dy
y2
1
√ √
exp
−
2
2∆σ
2π ∆σ2

√
n=0 2 2π(nη+∆σ 2 )5/2 n!

Ã+∞ √
¢
¡
X 2πe−λ∆ (λ∆)n ∆ ∆σ2 1/4
2 (nη + ∆σ2 )5/2 n!
!2

n=0

Ã+∞
X

fn (y|, ∆; θ)

n=0

+∞ X
+∞ Z
X

n=0 m=0

2

2

y
−
+ y
e 2(nη+∆σ2 ) 4∆σ2

!2
¡ 2
¢
2
y − (nη + ∆σ )
dy

dy

+a∆

fn (y|, ∆; θ)fm (y|, ∆; θ)dy

−a∆

The leading term in that double sum comes from n = m = 0 and is the only one with a final limit as
∆→0:

+∞ X
+∞ Z
X

n=0 m=0

with
Z +a∆

f02 (y|, ∆; θ)dy

−a∆

+a∆

fn (y|, ∆; θ)fm (y|, ∆; θ)dy =
−a∆

Z

+a∆

f02 (y|, ∆; θ)dy + o(1)

−a∆

e−2∆λ
2
√
¡
¢ η+∆σ
η
2η
2 2πη 3/2 1 + ∆σ
(σ2 )3/2
2
Ã
µ
¶
¶
η+∆σ2 µ
³
h ³
p
√
η ´ 2η
η ´i1/2
−1/2
2 ln 1 +
×
2πη3/2 1 +
η
+
∆σ
2Φ
η
−
1
∆σ 2
∆σ2
¶
h ³
p
¡
¢ ³
η ´i1/2 ³
η ´´
2
η
+
η
+
∆σ
ln
1
+
− η + ∆σ 2 ln 1 +
∆σ2
∆σ2
1
+ o(∆)
=
2σ 4
=

Therefore
Z

+a∆

−a∆

³

∂q(y|∆;θ)
∂σ 2

´2

q0 (y|∆; θ)

dy =

1
+ o(∆)
2σ4

which achieves the proof.
Appendix C:

Proof of Propositions 2, 3 and 4

In all cases, what needs to be computed are the matrices D and S. With polynomial moment functions in h
(including possibly absolute values and non integer powers) of the type
h(y, δ, θ) = yr − ∆ (δ, θ, r)
25

(C.1)

or
h(y, δ, θ) = |y|r − ∆a (δ, θ, r) ,

(C.2)

the functionals ḣ and hh0 retain the polynomial form in y. Thus D and S can be computed explicitly using
the moments ∆ (∆, θ, r) and ∆a (∆, θ, r) calculated in Proposition 1. Indeed, say we used the single (C.2) as
our moment condition. Then
h
i
D = E ḣ(Y∆ , ∆, θ) = −∆a (δ, θ, r)

(C.3)

which can be calculated by diﬀerentiating with respect to the parameter of the expression for ∆a (∆, θ, r)
given in Proposition 1 and
S

= E [h(Y∆ , ∆, θ)h(Y∆ , ∆, θ)0 ]
i
h
2
= E (|Y∆ |r − ∆a (∆, θ, r))
i
h
= E |Y∆ |2r − 2E [|Y∆ |r ] ∆a (∆, θ, r) + ∆a (∆, θ, r)2
= ∆a (∆, θ, 2r) − ∆a (∆, θ, r)2

(C.4)

which is calculated using again the expressions for the moments in Proposition 1.
Given D and S in (C.3)-(C.4), I then calculate a Taylor expansion in ∆ for Ω in (4.7),
¡
¢−1
S2
Ω = ∆ D0 S −1 D
= ∆ 2.
D

The leading terms of the Taylor expansions of the moments are given in Proposition 1. In the vector case,
repeat the calculations (C.3)-(C.4) for each element of the matrices D and S. The leading term of the Taylor
expansion of the AVAR matrix Ω is reported for each combination of moments and parameters in the three
propositions.
Appendix D:

Proof of Theorem 2

The essence of the argument is to compute the leading term of Fisher’s Information by using the convergence
of the Cauchy density as ∆ → 0 to a Dirac delta function, using the Brownian density as the test function,
after a change of variable, to get a fixed function. Fisher’s Information for σ 2 is
Iσ 2 =

Z

+∞ µ

−∞

∂q (y|∆; θ)
∂σ2

¶2

dy
q (y|∆; θ)

Replace now q, the density of the Brownian plus Cauchy process, by its expression (6.12), yielding
³
´
2
2
Z +∞
(y
−
z)
−
∆σ
2
(y−z)
α
∂q (y|∆; θ)
√
=
e− 2∆σ2
dz
1/2
2
5/2
∂σ 2
∆2 α2 π 2 + z 2
(σ )
−∞ 2 2π∆

26

and therefore
Z

+∞ µ

∂q (y|∆; θ)
∂σ 2

¶2

dy
q (y|∆; θ)
−∞
µ
¶2 √ √ 2
α
2π σ
√
=
1/2
2
5/2
∆1/2 α
2 2π∆ (σ )
¶2
µ
´
³
2
2
R +∞
(y−z)2 ((y−z) −∆σ )
Z +∞
exp
−
dz
2∆σ 2
∆2 α2 π2 +z 2
−∞
´
³
dy
×
R +∞
(y−z)2
1
−∞
∆2 α2 π2 +z 2 dz
−∞ exp − 2∆σ 2
¶2
µ
´
³
2
2
R +∞
(y−z)2 ((y−z) −∆σ )
Z +∞
exp
−
dz
2∆σ2
∆2 α2 π2 +z 2
−∞
α
´
³
√
=
×
dy
R
2
+∞
(y−z)
1
4 2π∆3/2 (σ 2 )9/2
−∞
exp
−
dz
2
2
2
2
2
2∆σ
∆ α π +z
−∞

Iσ2

=

Concentrating on the numerator inside the integral, we have

Z


+∞

2
− (y−z)
2∆σ 2

e
−∞

³
´
(y − z)2 − ∆σ2
∆2 α2 π2 + z 2

2

dz 

ÃZ

=

+∞

2

− (y−z)
2∆σ2

e

−∞

(y − z)2
dz
∆2 α2 π2 + z 2

!2

!
(y − z)2
−2∆σ
e
dz
∆2 α2 π2 + z 2
−∞
µZ +∞
¶
2
1
− (y−z)
2
×
e 2∆σ
dz
∆2 α2 π2 + z 2
−∞
¶2
µZ +∞
2
1
− (y−z)
2 4
2
+∆ σ
e 2∆σ
dz
∆2 α2 π2 + z 2
−∞
2

ÃZ

+∞

2

− (y−z)
2∆σ 2

so that
Z

+∞
−∞

¶2
´
³
2
2
(y−z)2 ((y−z) −∆σ )
exp
−
dz
2
2
2
2
2
2∆σ
∆ α π +z
−∞
´
³
dy
R +∞
2
(y−z)
1
exp
−
dz
2
2
2
2
2
2∆σ
∆ α π +z
−∞

µ
R +∞

´
³
´2
(y−z)2
(y−z)2
exp
−
dz
2
2
2
2
2
2∆σ
∆ α π +z
−∞
´
³
=
dy
R +∞
2
(y−z)
1
−∞
exp
−
dz
2
2
2
2
2
2∆σ
∆ α π +z
−∞
Z +∞ Z +∞
2
(y−z)
(y − z)2
e− 2∆σ2 2 2 2
dzdy
−2∆σ2
∆ α π + z2
−∞
−∞
Z +∞ Z +∞
(y−z)2
1
e− 2∆σ2 2 2 2
dzdy
+∆2 σ4
∆ α π + z2
−∞
−∞
≡ A+B +C
Z

+∞

³R
+∞

From
Z

+∞

q (y|∆; θ) dy = 1,

−∞

it follows that
Z

+∞ Z +∞

−∞

−∞

2

− (y−z)
2∆σ 2

e

1
dzdy =
2
2
∆ α π2 + z 2

27

√
2π(σ 2 )1/2
∆1/2 α

(D.1)

and hence
!
Ã√
√
∆3/2 2π(σ2 )5/2
2π(σ2 )1/2
C=∆ σ
=
.
α
∆1/2 α
2 4

Similarly, from the expected value of the score being zero, i.e.,
Z

+∞

−∞

it follows that
Z +∞ Z
−∞

+∞

2

− (y−z)
2∆σ 2

e

−∞

∂q (y|∆; θ)
dy = 0,
∂σ2

(y − z)2
dzdy
2
∆ α2 π 2 + z 2

= ∆σ

2

Z

+∞ Z +∞

−∞

−∞

√
∆1/2 2π(σ2 )3/2
α

=

e−

(y−z)2
2∆σ2

1
dzdy
∆2 α2 π2 + z 2
(D.2)

and hence
B = −2∆σ

2

Ã

!
Ã
!
√
√
∆1/2 2π(σ2 )3/2
∆3/2 2π(σ2 )5/2
= −2
.
α
α

Thus
Iσ2

=

α
√
× (A + B + C)
4 2π∆3/2 (σ 2 )9/2
α

√
×A+
4 2π∆3/2 (σ 2 )9/2
α
√
×A−
=
3/2
4 2π∆ (σ 2 )9/2
1
≡ Ã − 4
4σ
=

Ã

α

√
4 2π∆3/2 (σ2 )9/2
1
4σ 4

−2

Ã

!
!
√
√
∆3/2 2π(σ 2 )5/2
∆3/2 2π(σ 2 )5/2
+
α
α

(D.3)

That leaves us with the computation of
α

α

Ã ≡ √
×A= √
4 2π∆3/2 (σ 2 )9/2
4 2π∆3/2 (σ 2 )9/2

Z

+∞

−∞

³R
+∞

´
³
´2
2
(y−z)2
exp − (y−z)
2∆σ2
∆2 α2 π2 +z 2 dz
´
³
dy.
R +∞
(y−z)2
1
exp
−
dz
2
2
2
2
2
2∆σ
∆ α π +z
−∞
−∞

To handle the integral A, I first do two changes of variable: from z to w = z/(∆σ2 )1/2 in the two inner integrals

28

and from y to x = y/(∆σ 2 )1/2 in the outer integral, yielding

A =

Z

+∞

−∞

=

Z

+∞

−∞

µ
R +∞

µ

µ
R +∞

µ

(y−w(∆σ2 )1/2 )2

−∞

exp −

2

(x(∆σ2 )1/2 −w(∆σ2 )1/2 )

2 1/2

¶2

2∆σ 2

¶

(x(∆σ2 )1/2 −w(∆σ2 )1/2 )2
∆2 α2 π2 +∆σ 2 w2

dy

dw

µ
¶
R +∞
(x(∆σ2 )1/2 −w(∆σ2 )1/2 )2
1
2∆σ 2
∆2 α2 π2 +∆σ2 w2 dw
−∞ exp −

¶2

(∆σ 2 )1/2 dx(∆σ2 )1/2

´
³
´2
(x−w)2
(x−w)2
exp
−
dw
¢3
¡
2
∆2 α2 π2 +∆σ 2 w2
−∞
´
³
dx
= ∆σ 2
R +∞
2
(x−w)
1
−∞
2
∆2 α2 π2 +∆σ2 w2 dw
−∞ exp −
³
´
³
´2
(x−w)2
Z +∞ R +∞ exp − (x−w)2
2 π 2 +σ 2 w 2 dw
2
∆α
−∞
´
³
dx
= ∆2 σ 6
R +∞
2
1
−∞
exp − (x−w)
2
∆α2 π2 +σ2 w2 dw
−∞
+∞

³R
+∞

´
³
´2
(x−w)2
(x−w)2
exp
−
dw
2
∆α2 π2 +σ 2 w2
−∞
∆ α
´
³
√
dx
R +∞
2
2
3/2
(x−w)
1
4 2π(σ )
−∞
2
∆α2 π2 +σ2 w2 dw
−∞ exp −
³
´
³
´2
Z +∞ R +∞ exp − (x−w)2 (x − w)2 ω ∆ (w) dw
2
−∞
1
´
³
√
dx
R +∞
(x−w)2
4 2πσ4 −∞
ω ∆ (w) dw
2
−∞ exp −
1/2

Ã =

=

where

¶

exp −
2∆σ2
∆2 α2 π2 +∆σ 2 w2 dw(∆σ )
µ
¶
R +∞
(y−w(∆σ2 )1/2 )2
1
2 1/2
2∆σ2
∆2 α2 π2 +∆σ2 w2 dw(∆σ )
−∞ exp −
−∞

Z

so that

2

(y−w(∆σ2 )1/2 )

Z

+∞

³R
+∞

ω∆ (w) ≡

α∆1/2 σ
.
∆α2 π2 + σ2 w2

The function ω ∆ integrates to 1, and as ∆ tends to zero, it converges to a Dirac point mass at w = 0. Therefore
Ã
!
µ 2¶
Z +∞
(x − w)2
x
exp −
ω ∆ (w) dw = exp −
+ o(1)
2
2
−∞
Ã
!
µ 2¶
Z +∞
(x − w)2
x
2
exp −
(x − w) ω ∆ (w) dw = exp −
x2 + o(1)
2
2
−∞
Let

h∆ (x) ≡

³R
+∞
−∞

´
³
´2
2
(x − w)2 ω∆ (w) dw
exp − (x−w)
2
´
³
.
R +∞
(x−w)2
ω
exp
−
(w)
dw
∆
2
−∞

From the limits above we have that
³
³ 2 ´ ´2
exp − x2 x2
¢
¡
h∆ (x) =
+ o(1) = exp −x2 x4 + o(1) ≡ h0 (x) + o(1).
¡ x2 ¢
exp − 2
29

To establish that
Z

Z

+∞

h∆ (x)dx =

−∞

+∞

h0 (x)dx + o(1)

(D.4)

−∞

√
= 3 2π + o(1)

I apply Fatou’s Lemma (see e.g., 6.8.8 in Haaser and Sullivan (1991)) to yield
Z +∞
Z +∞
h0 (x)dx ≤ lim
h∆ (x)dx
−∞

(D.5)

−∞

To obtain an upper bound for the r.h.s. of (D.5), I use the Cauchy-Schwarz Inequality as follows
³R
+∞

u(w, x)dw

´2

−∞
R +∞
−∞ v(w, x)dw

to obtain
Z

+∞

−∞

h∆ (x)dx ≤
=
=

Z

+∞ Z +∞

−∞

Z

−∞

+∞ Z +∞

−∞
−∞
Z +∞ Z +∞
−∞

≤

Z

+∞

−∞

u(w, x)2
dw
v(w, x)

³
³
´
´2
2
exp − (x−w)
(x − w)2 ω ∆ (w)
2
´
³
dwdx
2
ω ∆ (w)
exp − (x−w)
2

e−
e−

(x−w)2
2

(x−w)2
2

−∞

(x − w)4 ω∆ (w) dwdx
(x − w)4

α∆1/2 σ
dwdx
∆α2 π2 + σ2 w2

with the change of variable from w to z = w − x, followed by a change of variable from x to y = −x.
From
0 =
=

Z

+∞

∂ 2 q (y|∆; θ)
dy
∂ (σ2 )2
−∞
Ã
!
Z +∞ Z +∞
4
2
2
2 4
2
1
(y
−
z)
−
6
(y
−
z)
∆σ
+
3∆
σ
− (y−z)
√
e 2∆σ2
dzdy
∆2 α2 π 2 + z 2
4 2πσ 4 ∆5/2 σ 9 −∞ −∞

it follows that
Z

+∞ Z +∞

−∞

−∞

e−

(y−z)2
2∆σ2

(y − z)4
dzdy
2
∆ α2 π 2 + z 2

= 6∆σ 2

Z

+∞ Z +∞

−∞

Z

e−

−∞
+∞ Z +∞

(y−z)2
2∆σ2

(y − z)2
dzdy
+ z2

∆2 α2 π2

(y−z)2
1
e− 2∆σ2 2 2 2
dzdy
−3∆2 σ 4
∆ α π + z2
−∞
−∞
!
Ã
Ã
!
√
√
1/2
2 3/2
2 1/2
∆
2π(σ
)
2π(σ
)
= 6∆σ 2
− 3∆2 σ4
α
∆1/2 α
√
∆3/2 2π(σ2 )5/2
(D.6)
= 3
α

with the last step from (D.1) and (D.2). Using now the changes of variables from w to z = w(∆σ2 )1/2 in the

30

inner integral and from x to y = x(∆σ2 )1/2 in the outer integral yields
Z
=

+∞ Z +∞

−∞
−∞
+∞ Z +∞

Z

−∞

=

(x−w)2
2

e−

(y−z)2
2∆σ2

e−

−∞

α
∆3/2 (σ2 )5/2

α
=
∆3/2 (σ2 )5/2
√
= 3 2π

Z

e−

(y−z)2
2∆σ2

−∞

3/2

3

α∆1/2 σ
dwdx
∆α2 π 2 + σ 2 w2

(y − z)4
α∆1/2 σ
dy
dz
(∆σ2 )2 ∆α2 π2 + σ 2 z 2 /(∆σ 2 ) (∆σ2 )1/2 (∆σ 2 )1/2

+∞ Z +∞

−∞

Ã

(x − w)4

∆

√
2π(σ2 )5/2
α

(y − z)4
dzdy
∆2 α2 π2 + z 2
!

proving that
Z

+∞

√
h∆ (x)dx ≤ 3 2π.

−∞

Combined with (D.5), we have therefore
Z
√
3 2π =

+∞
−∞

h0 (x)dx ≤ lim

Z

+∞

−∞

√
h∆ (x)dx ≤ 3 2π

which establishes (D.4):
lim

Z

+∞

√
h∆ (x)dx = 3 2π.

−∞

Hence I have obtained that
1
Ã = √
4 2πσ4

Z

+∞

h∆ (x)dx =

−∞

3
+ o(1)
4σ4

and from (D.3) Fisher’s Information for σ2 is
Iσ2 = Ã −

1
1
= 4 + o(1)
4
4σ
2σ

which, in light of (4.9), proves the Theorem.
Appendix E:

Proof of Lemma 1

For a Brownian motion with density
µ
¶
1
y2
fW∆ (y) = √ √
exp −
2∆σ 2
2π ∆σ2

31

(D.7)

we have
Pr (|Y∆ | > ε) =

Z

fW∆ (y)dy

|y|>ε

= 2Φ
=

³

ε

´

−1
∆1/2
rσ
µ
¶
ε2
∆1/2 σ 2
exp −
(1 + o(1))
ε
π
2 ∆ σ2

with the last equation following from the known asymptotic behavior of the Normal cdf Φ near infinity (see
e.g., 26.2.12 in Abramowitz and Stegun (1972)):
1 − Φ (x) =

φ (x)
(1 + o(1))
x

as x → +∞, where φ is the Normal pdf.
For a Lévy pure jump process with jump measure v(dz) and probability measure fL∆ (dy) it is known that
for points y 6= 0, under regularity conditions,
fL∆ (dy) = ∆ × v(dy) + o(∆)
(see e.g., Corollary 1 in Rüschendorf and Woerner (2002) as a special case for Lévy processes of Léandre (1987)
and Picard (1997) for points that can be reached in one jump from 0). The regularity conditions referred to
in the statement of the lemma are those of Theorem 1 in Rüschendorf and Woerner (2002). They are satisfied
for instance by the symmetric stable class emphasized here.
Then, by Fatou’s Lemma,
Pr (|Y∆ | > ε) =

Z

fL∆ (dy)

|y|>ε

= ∆×

32

Z

|y|>ε

v(dy) + o(∆).

References
Abramowitz, M., Stegun, I. A., 1972. Handbook of Mathematical Functions. Dover, New York, N.Y.
Aït-Sahalia, Y., 2002a. Maximum-likelihood estimation of discretely-sampled diﬀusions: A closed-form approximation approach. Econometrica 70, 223—262.
Aït-Sahalia, Y., 2002b. Telling from discrete data whether the underlying continuous-time model is a diﬀusion.
Journal of Finance 57, 2075—2112.
Aït-Sahalia, Y., Mykland, P. A., 2003a. The eﬀects of random and discrete sampling when estimating
continuous-time diﬀusions. Econometrica 71, 483—549.
Aït-Sahalia, Y., Mykland, P. A., 2003b. How often to sample a continuous-time process in the presence of
market microstructure noise. Tech. rep., Princeton University.
Andersen, T. G., Bollerslev, T., 1997. Intraday periodicity and volatility persistence in financial markets.
Journal of Empirical Finance 4, 115—158.
Andersen, T. G., Bollerslev, T., Diebold, F. X., Labys, P., 2003. Modeling and forecasting realized volatility.
Econometrica 71, 579—625.
Ball, C. A., Torous, W. N., 1983. A simplified jump process for common stock returns. The Journal of Financial
and Quantitative Analysis 18, 53—65.
Barndorﬀ-Nielsen, O. E., Shephard, N., 2002. Power variation with stochastic volatility and jumps. Tech. rep.,
University of Aarhus.
Beckers, S., 1981. A note on estimating the parameters of the diﬀusion-jump model of stock returns. The
Journal of Financial and Quantitative Analysis 16, 127—140.
Bertoin, J., 1998. Lévy Processes. Cambridge University Press, Cambridge, UK.
Carr, P., Geman, H., Madan, D. B., Yor, M., 2002. The fine structure of asset returns: An empirical investigation. Journal of Business 75, 305—332.
Carr, P., Wu, L., 2003a. Time-changed Lévy processes and option pricing. Journal of Financial Economics,
forthcoming , xxx—xxx.
Carr, P., Wu, L., 2003b. What type of process underlies options? A simple robust test. Journal of Finance,
forthcoming , xxx—xxx.
Chan, K., 1999. Pricing contingent claims on stocks driven by Lévy processes. Annals of Applied Probability
9, 504—528.
Chernov, M., Gallant, A. R., Ghysels, E., Tauchen, G. T., 2002. Alternative models of stock price dynamics.
Journal of Econometrics, forthcoming , xxx—xxx.
Ding, Z., Granger, C. W., Engle, R. F., 1993. A long memory property of stock market returns and a new
model markets. Journal of Empirical Finance 1, 83—106.
Eberlein, E., Keller, U., Prause, K., 1998. New insights into smile, mispricing and Value at Risk: The hyperbolic model. Journal of Business 71, 371—405.
Eberlein, E., Raible, S., 1999. Term structure models driven by general Lévy processes. Mathematical Finance
9, 31—53.
Eraker, B., Johannes, M. S., Polson, N., 2003. The impact of jumps in equity index volatility and returns.
Journal of Finance, forthcoming , xxx—xxx.
Haaser, N. B., Sullivan, J. A., 1991. Real Analysis. Dover, New York.
33

Hansen, L. P., 1982. Large sample properties of generalized method of moments estimators. Econometrica 50,
1029—1054.
Honoré, P., 1998. Pitfalls in estimating jump-diﬀusion models. Tech. rep., Aarhus School of Business.
Kiefer, N. M., 1978. Discrete parameter variation: Eﬃcient estimation of a switching regression model. Econometrica 46, 427—434.
Léandre, R., 1987. Densite en temps petit d’un processus de sauts. In: Séminaire de Probabilités, XXI, Lecture
Notes in Mathematics, 1247. Springer, Berlin, Germany, pp. 81—99.
Lepingle, D., 1976. La variation d’ordre p des semi-martingales. Zeitschrift fur Wahrscheinlichkeitstheorie und
Verwandte Gebiete 36, 295—316.
Madan, D. B., Carr, P. P., Chang, E. E., 1998. The variance gamma process and option pricing. European
Finance Review 2, 79—105.
Merton, R. C., 1976. Option pricing when underlying stock returns are discontinuous. Journal of Financial
Economics 3, 125—144.
Picard, J., 1997. Density in small time for lévy processes. ESAIM Probability and Statistics 1, 357—389.
Press, S. J., 1967. A compound events model for security prices. The Journal of Business 40, 317—335.
Ray, D., 1956. Stationary markov processes with continuous paths. Transactions of the American Mathematical
Society 82, 452—493.
Rüschendorf, L., Woerner, J. H. C., 2002. Expansions of transition distributions of lévy processes in small
time. Bernoulli 8, 81—96.
Schaumburg, E., 2001. Maximum likelihood estimation of jump processes with applications to finance. Ph.D.
thesis, Princeton University.

34

Fig. 1
The Transition Density

Fig. 2
Tail of the Transition Density

35

Fig. 3
Isonoise Curves

36

Fig. 4
Jump Probabilities Inferred from Observing a Jump Greater than a Given Threshold

Fig. 5
Probability that a 10% Log-Return Involved 1 Jump as a Function of the Sampling Interval

37

Fig. 6
Log-Returns on the Dow Jones Industrial Average at Diﬀerent Observation Frequencies

38

Fig. 7
Eﬃciency of the Ma (∆, θ, r) Moment Condition Relative to MLE in the Absence of Jumps

Fig. 8
Eﬃciency of the Ma (∆, θ, r) Moment Condition Relative to MLE When Jumps Are Present

39

Fig. 9
Eﬃciency of the (Ma (∆, θ, r), Ma (∆, θ, q))0 Moment Conditions Used Jointly Relative to MLE in the Absence
of Jumps

40

Fig. 10
Distinguishing Volatilty from Poisson Jumps: Small Sample and Asymptotic Distributions of the MLE Estimators of (σ 2 , λ)

41

Fig. 11
Distinguishing Volatilty from Cauchy Jumps: Small Sample and Asymptotic Distributions of the MLE Estimators of (σ 2 , α)

42

Fig. 12
Distinguishing Volatilty from Poisson Jumps: Confidence Regions for the MLE Estimates of (σ 2 , λ)

Fig. 13
Distinguishing Volatilty from Cauchy Jumps: Confidence Regions for the MLE Estimates of (σ 2 , α)

43

