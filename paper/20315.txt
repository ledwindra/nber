NBER WORKING PAPER SERIES

CORRUPTION, INTIMIDATION, AND WHISTLE-BLOWING:
A THEORY OF INFERENCE FROM UNVERIFIABLE REPORTS
Sylvain Chassang
Gerard PadrÃ³ i Miquel
Working Paper 20315
http://www.nber.org/papers/w20315
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
July 2014

We are grateful to Johannes H\H orner for a very helpful discussion. We are indebted to Nageeb Ali,
Abhijit Banerjee, Michael Callen, Yeon Koo Che, Hans Christensen, Ray Fisman, Matt Gentzkow,
Bob Gibbons, Navin Kartik, David Martimort, Andrea Prat, Jesse Shapiro, as well as seminar audiences
at Berkeley, Columbia, Essex, Hebrew University, the Institute for Advanced Study, the 2013 Winter
Meeting of the Econometric Society, MIT, MIT Sloan, the Nemmers Prize Conference, NYU, NYU
IO day, Paris School of Economics, Pompeu Fabra, ThReD, and the UCSD workshop on Cellular
Technology, Security and Governance for helpful conversations. Chassang gratefully acknowledges
the hospitality of the University of Chicago Booth School of Business, as well as support from the
Alfred P. Sloan Foundation and the National Science Foundation under grant SES-1156154. Padro
i Miquel acknowledges financial support from the European Union's Seventh Framework Programme
(FP/2007-2013) / ERC Starting Grant Agreement no. 283837. The views expressed herein are those
of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
Â© 2014 by Sylvain Chassang and Gerard PadrÃ³ i Miquel. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including Â© notice, is given to the source.

Corruption, Intimidation, and Whistle-blowing: a Theory of Inference from Unverifiable Reports
Sylvain Chassang and Gerard PadrÃ³ i Miquel
NBER Working Paper No. 20315
July 2014
JEL No. D73,D82,D86
ABSTRACT
We consider a game between a principal, an agent, and a monitor in which the principal would like
to rely on messages by the monitor to target intervention against a misbehaving agent. The difficulty
is that the agent can credibly threaten to retaliate against likely whistleblowers in the event of an
intervention. In this setting intervention policies that are very responsive to the monitor's message
provide very informative signals to the agent, allowing him to shut down communication channels.
Successful intervention policies must garble the information provided by monitors and cannot be
fully responsive. We show that even if hard evidence is unavailable and monitors have heterogeneous
incentives to (mis)report, it is possible to establish robust bounds on equilibrium corruption using
only non-verifiable reports. Our analysis suggests a simple heuristic to calibrate intervention policies:
first get monitors to complain, then scale up enforcement while keeping the information content of
intervention constant.
Sylvain Chassang
Department of Economics
Bendheim Hall 316
Princeton University
Princeton, NJ 08544-1021
chassang@princeton.edu
Gerard PadrÃ³ i Miquel
STICERD
London School of Economics
Houghton Street
London, WC2A 2AE
UNITED KINGDOM
and NBER
g.padro@lse.ac.uk

1

Introduction

This paper explores anti-corruption mechanisms in which a principal relies on messages by
an informed monitor to target intervention against a potentially misbehaving agent.1 The
difficulty is that the agent can credibly threaten to retaliate against likely whistleblowers.
When messages are exogenously informative, intervention policies that are more responsive
to the monitorâ€™s messages naturally provide greater incentives for the agent to behave well.
However, when messages are endogenous, making intervention responsive to the monitorâ€™s
message facilitates effective retaliation by corrupt agents and limits information provision.
This generates a novel trade-off between eliciting information and using that information
efficiently. In addition, this makes evaluating intervention policies difficult: imagine that no
corruption is reported; does this mean that there is no underlying corruption, or does it mean
that would-be whistleblowers are being silenced by threats and intimidation? We investigate
the relationship between intervention, corruption, and whistleblowing, and suggest ways to
identify effective intervention strategies using only unverifiable reports.
The paper considers a dynamic game played by a principal, an agent and a monitor.
Both the principal and the agent have commitment power, and they act sequentially. The
principal first commits to an intervention strategy as a function of the information obtained
from the monitor, i.e. to a likelihood of intervention as a function of messages â€œcorruptâ€ and
â€œnon-corrupt.â€ The agent then commits to a retaliation strategy against the monitor as a
function of subsequent observables â€” including whether or not an intervention is triggered
by the principal â€” and makes a binary corruption decision. The monitor observes the
corruption behavior of the agent and chooses what message to send to the principal. Finally,
intervention and retaliation are realized according to the commitments of both the principal
and the agent.
Our modeling approach emphasizes three issues that are important in practical applications. First, we take seriously the idea that in the long run, corrupt agents will undermine
the effectiveness of institutions by side-contracting with the monitors in charge of evaluating
1

Throughout the paper we refer to the principal and monitor as she, and to the agent as he.

2

them. In our model, this side-contracting takes the form of contingent retaliation profiles.
Second, departing from much of the literature on collusion, we do not assume that messages are verifiable, reflecting the fact that hard measures of corruption are rarely available.2
When it comes to anti-corruption policy, the outcome of interest need not be directly measurable, and policies may have to be evaluated using non-verifiable messages. Third, we
do not assume that the principal has precise control over the payoffs of the agent and the
monitor following intervention: rewards and punishment may be determined by imperfect
and stochastic institutional processes; whistleblower protection schemes may not fully shield
the monitor against ostracism, or harassment; supposedly anonymous information may be
leaked; the judiciary may fail to act against corrupt agents, and so on. Finally, we do not
think it is realistic to assume that the principal has prior knowledge of the agent and the
monitorâ€™s payoffs, and therefore our model allows for rich heterogeneity. In particular, our
framework allows for â€œmaliciousâ€ monitors who benefit from triggering intervention against
honest agents. Hence, the principal should be concerned that measures taken to protect
whistleblowers may end up empowering scoundrels.
We provide two main sets of results. The first is that any effective intervention strategy
must limit its responsiveness to the monitorâ€™s messages. Indeed, consider a principalâ€™s strategy that launches an intervention with high probability when receiving message â€œcorruptâ€,
and launches it with low probability when the message is â€œnot corruptâ€. Under such a strategy, if there is an intervention, the agent knows that it is likely due to a monitorâ€™s accusing
message. In this case we say that the information content of intervention â€” measured by the
ratio of intervention rates under the two messages â€” is high. This simplifies the incentive
problem of the agent: committing to very painful retaliation conditional on intervention
ensures that the monitor never reports corruption at little equilibrium cost. It follows that if
the principal wants to receive informative messages from the monitor, the information content of intervention cannot be too high. In particular, the likelihood of intervention against
agents reported as not corrupt must be bounded away from zero. In addition, it may be
2

See Bertrand et al. (2007) or Olken (2007) for innovative approaches to measuring corruption.

3

optimal not to always intervene against agents reported as corrupt. This allows the principal
to reduce intervention on non-corrupt agents, which is costly in equilibrium, and still keep
the information content of intervention low. The basic take-away is that since collusion is a
side-contracting problem, it can be addressed by exploiting imperfect-monitoring contracting
frictions between colluding parties.
Our second set of results shows how to use equilibrium properties of corruption and reporting decisions to infer bounds on the underlying levels of corruption using non-verifiable
reports alone. We show that for any given type of the agent, the region of the interventionstrategy space in which corruption occurs is star-shaped around the origin. Moreover, keeping
corruption behavior constant, the messages sent by monitors depend only on the information
content of intervention (the ratio of intervention rates), and not on the intensity of intervention (the absolute values of intervention rates). Using these properties, we show that
policy experiments which vary the level of intensity while keeping the information content of
intervention constant yield bounds on unobservable corruption. These bounds can be used
for prior-free policy design and suggest the following rule-of-thumb: first provide enough
plausible deniability so that monitors are willing to complain, and then scale up intensity
while keeping the information content of intervention constant.
This paper speaks to a growing class of information technology (IT) policy interventions
that hope to reduce corruption by improving the recording and transmission of complaints.3
A recent example is Punjabâ€™s Citizen Feedback Model under which Punjabi citizens availing
themselves of a broad class of public services get surveyed on whether they experienced corruption or not.4 The data is then analyzed by the Punjabi Information Technology Board
which brings up particularly suspect cases to the attention of relevant authorities.5 In a recent field experiment also taking place in Punjab, Callen et al. (2013) study absenteeism in
3
As is reflected in our modeling choices, our analysis is orthogonal to that of policies designed to directly
change the payoffs of involved parties â€“ for instance the large cash rewards awarded by the SECâ€™s Office of
the Whistleblower in exchange for actionable high-quality information.
4
See http://www.punjabmodel.gov.pk/ as well as Callen and Hasanain (2013) and the Economist (2013)
for recent descriptions and discussions of the program. The website www.ipaidabribe.com functions along
a similar spirit although the data it generates is not formally tied to institutional interventions.
5
See http://www.pitb.gov.pk/.

4

public health clinics and evaluate the impact of sending senior health officials reports highlighting poorly performing facilities. They find that flagging out a facility reduces subsequent
absenteeism by 18%. We share the belief that IT represents a first-order opportunity for
anti-corruption efforts. Still, we raise the concern that in the long run, such schemes may be
undermined by side-contracting between agents and monitors (here in the form of retaliation
threats).6 Our analysis provides operational guidelines on how to address this issue.
This paper contributes to a growing effort to understand the effectiveness of countercorruption measures. In recent years, the World Bank, the OECD and the United Nations
have launched new initiatives to improve governance, in the belief that a reduction in corruption can improve the growth trajectory of developing countries.7 Growing micro-economic
evidence confirms the importance of corruption issues affecting public service provision and
public expenditure in education or health (see Olken and Pande (2012) and Banerjee et al.
(2012) for recent reviews), and recent experimental evidence suggests that appropriate incentive design can reduce misbehavior (Olken (2007), Duflo et al. (2012, 2013)). In our view,
a key aspect of corruption is that although there is strong suspicion that it is occurring,
there is generally little direct and actionable evidence flowing back to the relevant principals
due to implicit threats of retaliation.8 We show that correct policy design is essential to
keep information channels open under these threats. Relying on robust implications from
our structural model, we provide a method to measure underlying corruption using only
unverifiable messages generated by appropriately chosen policy experiments. In this respect,
we contribute to a growing literature which takes a structural approach to experiment design
6

eBayâ€™s feedback mechanism is an example of an initially promising monitoring scheme that was rendered
essentially uninformative by retaliatory threats (in their analysis of eBay feedback, Resnick and Zeckhauser
(2002) find 99.1% of positive feedback, versus .9% of neutral and negative feedback). See Appendix A for
additional anecdotal evidence.
7
See Mauro (1995) for early work highlighting the association of corruption and lack of growth. Shleifer
and Vishny (1993) and Acemoglu and Verdier (1998, 2000) provide theories of corruption that introduce
distortions above and beyond the implicit tax that corruption imposes.
8
See for instance Ensminger (2013) who emphasizes the role of threats and failed information channels in
recent corruption scandals affecting community driven development projects. Also, in a discussion of why
citizens fail to complain about poor public service, Banerjee and Duflo (2006) suggest that â€œthe beneficiaries
of education and health services are likely to be socially inferior to the teacher or healthcare worker, and a
government worker may have some power to retaliate against them.â€

5

in order to make inferences about unobservables.9
This paper also contributes to the contract theory literature on collusion in organizations
(see for instance Tirole (1986), Laffont and Martimort (1997, 2000), Prendergast (2000), or
Faure-Grimaud et al. (2003)).10 Our insight is that whenever collusion is an issue, then it
will be in the principalâ€™s interest to make side-contracting between the agent and the monitor
difficult. The forces that make contracting difficult are well known: adverse selection and
moral hazard. Here we focus on the latter, and show how the principal can make the
agentâ€™s own incentive provision problem more difficult by garbling the information content
of the monitorâ€™s responses.11 This creates a novel practical rationale for the use of random
mechanisms, and we believe that this simple idea has applications in other settings, for
instance to fight collusion in auctions. Our paper also emphasizes a novel set of questions
in this literature. Rather than solving for optimal contracts in a Bayesian environment,
we study the inference of unobserved but payoff-relevant behavior, and the extent to which
unverifiable message data can be used for prior-free policy design.12
Finally, our work is related to that of Myerson (1986) or more recently Rahman (2012)
who consider mechanism design problems with non-verifiable reports, and emphasize the
value of random recommendation-based incentives to jointly incentivize mutliple agents, and
in particular to incentivize both effort provision and the costly monitoring of effort. The
key difference is that this strand of literature excludes the possibility of side contracting
between players. As a result, the role of mixed strategies in our work is entirely different:
monitoring itself is costless and randomization occurs only to garble the information content
of the principalâ€™s intervention behavior and make side-contracting between the agent and
the monitor difficult.13 Our work also shares much of its motivation with the seminal work
9

See for instance Karlan and Zinman (2009) or Chassang et al. (2012).
A particularly active theoretical and empirical strand of this literature focuses on collusion in auctions.
See, among others, Skrzypacz and Hopenhayn (2004), Che and Kim (2006, 2009) or Asker (2010).
11
This echoes the point made by Dal BoÌ (2007) in a legislative context, that making votes anonymous can
help prevent influence activities and vote-buying.
12
Our perspective here echoes that developed in work by Hurwicz and Shapiro (1978), Segal (2003),
Hartline and Roughgarden (2008), MadaraÌsz and Prat (2010), Chassang (2013), Frankel (2014), Carroll
(2013).
13
Eeckhout et al. (2010) propose a different theory of optimal random intervention based on budget
10

6

of Warner (1965) on the role of plausible deniability in survey design, and the recent work
of Izmalkov et al. (2011), Ghosh and Roth (2010), Nissim et al. (2011), or Gradwohl (2012)
on privacy in mechanism design.
The paper is structured as follows: Section 2 introduces our framework and presents the
main points of our analysis in the context of a simple example; Section 3 introduces our
general framework; Section 4 establishes robust properties of corruption and reporting in
equilibrium, and shows how they can be exploited to form estimates of underlying corruption levels as well as make policy recommendations; Section 5 concludes with a discussion
of potential implementation challenges. Appendix A presents several extensions, studying
among other things settings with multiple monitors, and short-term out-of-equilibrium inference. Proofs are contained in Appendix B.

2

An Example

This section introduces our framework and illustrates the mechanics of corruption, intimidation and whistleblowing through a simple but detailed example. In the interest of clarity,
we make several restrictive assumptions which are relaxed in Sections 3 and 4.

2.1

Setup

Players, timing, and actions. There are three players: a principal P , an agent A and a
monitor M .14 The timing of actions is as follows.
1. The agent chooses whether to be corrupt (c = 1) or not (c = 0). The monitor observes
corruption c and sends a message m âˆˆ {0, 1} to the principal.15
2. The principal observes the monitorâ€™s message m and triggers an intervention (i = 1)
constraints, and non-linear responses of criminal behavior to the likelihood of enforcement.
14
See Appendix A for an extension to the case of multiple monitors.
15
In this simple setting, this binary message space is without loss of efficiency: collecting messages from
the agent, or richer messages from the monitor (for instance about threats of retaliation) is not helpful. See
Appendix B, Lemma B.1 for details.

7

or not (i = 0). Intervention has payoff consequences for the principal, the agent, and
the monitor that are detailed below.
3. The agent can retaliate with intensity r âˆˆ [0, +âˆ) against the monitor.
This timing of actions is associated with a specific commitment structure: the principal
commits first to an intervention policy, following which the agent commits to a retaliation
strategy (see further description below).
At this point, it is useful to illustrate the types of interactions we are interested in capturing. Corruption may include bribe collection by state officials, arrangements between police
officers or judges and organized crime, fraud by sub-contractors in public good projects,
breach of fiduciary duty by a firmâ€™s top executives, and so on. Retaliation can also take
several forms: an honest bureaucrat may be socially excluded by his colleagues and denied promotion; whistleblowers may be harrassed, see their careers derailed, or get sued for
defamation; police officers suspected of collaborating with Internal Affairs may have their life
threatened by lack of prompt support, and so on.16 In many cases retaliation is facilitated by
the fact that only a few colleagues, subordinates, or frequent associates are informed about
the agentâ€™s misbehavior. However, group punishments may also be used. For instance, entire
communities may be denied access to public services following complaints to authorities.17
In addition, monitors may fear that anonymity is not properly ensured and that imperfect
institutions may leak the source of complaints to the agent or one of his associates. In hierarchical 360â—¦ evaluations, subordinates may not be willing to complain about their superior
to their superiorâ€™s boss if they worry that the two may share information. All these situations exhibit two features that are key to our analysis: (1) there is significant information
about corrupt agents which the principal wants to obtain; (2) the individuals who have this
information and are able to pass it on to the principal can be punished by the agent.
16

See Punch (2009) for examples of punishment of informants in a study of police corruption.
For instance, Ensminger (2013) suggests that egregious corruption affecting the World Bankâ€™s arid land
program were not reported by the local Kenyan communities that suffered from it for fear of being cut off
from subsequent projects.
17

8

Observables and payoffs. The monitor costlessly observes the agentâ€™s corruption decision
c âˆˆ {0, 1}, and can send a message m âˆˆ {0, 1} to the otherwise uninformed principal. The
agent does not observe the monitorâ€™s message m, but observes whether the principal triggers
an intervention i âˆˆ {0, 1}.18
As a function of c âˆˆ {0, 1}, i âˆˆ {0, 1} and r â‰¥ 0, payoffs uA , uP and uM to the agent,
principal and monitor take the form
uM = Ï€M Ã— c + vM (c, m) Ã— i âˆ’ r
uA = Ï€A Ã— c + vA (c) Ã— i âˆ’ kA (r)
uP = Ï€P Ã— c + vP (c) Ã— i
where Ï€M , Ï€A , and Ï€P capture the expected payoff consequences of corruption, vM , vA , and
vP capture the expected payoffs associated with intervention, r is the level of retaliation
imposed by the agent on the monitor, and kA (r) is the cost of such retaliation to the agent.
Payoffs conditional on corruption are such that Ï€A > 0 and Ï€P < 0. The cost of retaliation
kA (r) is strictly increasing in r, with kA (0) = 0. Payoffs are common-knowledge. We make
the following assumption.
Assumption 1. Expected continuation payoffs following intervention (i = 1) satisfy
âˆ€m âˆˆ {0, 1},

vM (c = 0, m) < 0

(non-malicious monitor);

Ï€A + vA (c = 1) < vA (c = 0) = 0
Ï€P â‰¤ vP (c = 0) < 0
âˆ€c âˆˆ {0, 1},

(effective intervention);
(optimality of intervention);

vM (c, m 6= c) â‰¤ vM (c, m = c)

(weak preference for the truth);

The first three assumptions are made for simplicity and are relaxed in our general analysis. The assumption that there are no malicious monitors requires that the monitor gets a
negative continuation payoff vM (c = 0, m) < 0 following intervention on an honest agent; effective intervention requires that certain intervention does not hurt the agent if he is honest,
18

Our general framework allows the agent to observe leaks from the institutional process that can be
informative of the message m sent by the monitor.

9

and hurts him sufficiently when dishonest to dissuade corruption; optimality of intervention
guarantees that it is always optimal for the principal to pick an intervention profile that
induces the agent to be honest. The last assumption (weak preference for the truth) is
maintained throughout the paper. We assume that taking intervention as given, the monitor is weakly better off telling the truth. This gives an operational meaning to messages
m âˆˆ {0, 1}. It typically comes for free in direct mechanism design problems.
Strategies and commitment. Both the principal and the agent can commit to strategies
ex ante. Though we do not provide explicit micro-foundations, we think of this commitment
power as either arising from reputational concerns, or enforced by institutions. The principal
is the first mover and commits to an intervention policy Ïƒ : m âˆˆ {0, 1} 7â†’ Ïƒm âˆˆ [0, 1], where
Ïƒm â‰¡ prob(i = 1|m) is the likelihood of intervention given message m.19 Without loss of
generality, we focus on strategies such that Ïƒ1 â‰¥ Ïƒ0 .20
Knowing the principalâ€™s intervention strategy Ïƒ, the agent takes a corruption decision
c âˆˆ {0, 1} and commits to a retaliation policy r : i âˆˆ {0, 1} 7â†’ r(i) âˆˆ [0, +âˆ) as a function of whether or not he observes intervention. The monitor moves last and chooses the
message m âˆˆ {0, 1} maximizing her payoffs given the strategic commitments of both the
principal and the agent.21 Note that we assume that retaliation, rather than payments, is
the side-contracting instrument available to the agent, and this plays an important role in
the analysis.22
We are interested in characterizing patterns of corruption and information transmission
as a function of the principalâ€™s intervention policy Ïƒ. We also solve for the principalâ€™s optimal
policy and show that it must be interior. For simplicity, we assume throughout the paper that
19

We assume that the principal can commit to using a mixed strategy. Section 5 discusses credible ways
for the principal to do so. In particular, we suggest that mixing can be achieved by garbling the messages
provided by the monitor directly at the surveying stage, before it even reaches the principal.
20
See Appendix B, Lemma B.1 for details.
21
The order of moves is essential for the analysis. Intuitively, it reflects the various partiesâ€™ ability to make
more or less public commitments. The principal can make fully public commitments, whereas the agent can
only commit vis-aÌ€-vis the monitor: fully public commitments to retaliate would be directly incriminating.
22
See Appendix A for a detailed discussion, as well as sufficient conditions for this to be optimal even if
side-payments are available, building on the fact that rewards must be paid on the equilibrium path.

10

whenever the agent is indifferent, he chooses to not be corrupt, and whenever the monitor
is indifferent, she reveals the truth. This convention does not matter for any of our results.
Reduced-form payoffs. It is important to note that while we take payoffs upon intervention as exogenous, this does not mean that our approach is inconsistent with a broader
mechanism design problem in which payoffs upon intervention vA and vM are also policy
instruments available to the principal. Indeed, we place few restrictions on reduced-form
payoffs, and they can be thought of as being determined in a first optimization stage, before
determining intervention patterns Ïƒ. This is especially true in the more general framework
of Section 3.
More formally, if V denotes the set of feasible payoff structures v â‰¡ (vA , vM ), Î£ the
set of possible intervention policies Ïƒ, and câˆ— (v, Ïƒ) an appropriate selection of the agentâ€™s
equilibrium behavior under payoff structure v and policy Ïƒ, the principal can be thought of
as solving
max E[uP |Ïƒ, câˆ— (v, Ïƒ)] = max max E[uP |Ïƒ, câˆ— (v, Ïƒ)].

vâˆˆV,ÏƒâˆˆÎ£

vâˆˆV

ÏƒâˆˆÎ£

Provided that payoffs in V satisfy Assumption 1 (or the more general assumption made
in Section 3), our analysis applies within the broader mechanism design problem in which
payoffs are endogenously determined by the principal. In particular, our reduced-form payoffs
capture schemes under which the monitor receives reward vM (c = 1, m = 1) > 0 for correctly
informing the principal that the agent is corrupt, and is instead punished for erroneous
statements: vM (c, m 6= c) â‰¤ 0.
Our decision to eschew endogenizing payoffs reflects what we perceive as great heterogeneity in the ability of principals to reliably affect the payoffs of involved parties. While payoffs
are a first order determinants of behavior, they are rarely available as policy instruments.
Even powerful international organizations such as the World Bank need to go through local
judiciary systems to target corrupt agents, which severely constrains their ability to deliver
rewards and punishments. For this reason, we choose to focus on the decision to trigger
intervention, in whatever form it may take, as our main policy dimension of interest.

11

2.2

The Trade-off Between Eliciting and Using Information

To frame the analysis, it is useful to contrast the effectiveness of intervention policies when
messages are exogenously informative, i.e. when the monitor is an automaton with strategy
m(c) = c, and when messages are endogenous.
Fact 1 (basic trade-off).

(i) If messages are exogenously informative, i.e. m(c) =

c, setting Ïƒ0 = 0 and Ïƒ1 = 1 is an optimal policy. There is no corruption and no
retaliation in equilibrium.
(ii) If messages are endogenous, there exists Î» > 1 such that for any intervention
policy Ïƒ satisfying

Ïƒ1
Ïƒ0

â‰¥ Î»,

â€¢ the agent is corrupt and commits to retaliate conditional on intervention;
â€¢ the monitor sends message m = 0.
Point (i) follows from Assumption 1, which ensures that the agent refrains from corruption if intervention occurs with high enough probability. Since messages are exogenous,
intervention can be fully responsive to the monitorâ€™s message: it provides the strongest
incentives for the agent to be honest, and avoids costly intervention on the equilibrium path.
Point (ii) shows that this is no longer the case when messages are endogenous. In this
case, when the likelihood ratio of intervention rates Î» â‰¡

Ïƒ1
Ïƒ0

is high, intervention itself becomes

a very informative signal of which message the monitor sent. When Î» is too high, the agent
can dissuade the monitor to send message m = 1 while keeping equilibrium retaliation
costs low, simply by threatening the monitor with high levels of retaliation conditional on
intervention.
To prevent corruption, the principal must therefore commit to trigger intervention with
sufficiently high probability when she receives message m = 0. This gives the monitor
plausible deniability when intervention takes place, and therefore makes the agentâ€™s own
incentive problem vis-aÌ€-vis the monitor more costly to resolve, since retaliation must be
carried out with positive probability in equilibrium.
12

2.3

Intervention, Reporting and Corruption

We now study in greater detail the patterns of corruption and information flow as a function
of intervention policy Ïƒ. We proceed by backward induction.
Reporting by the monitor. We begin by clarifying the conditions under which the
monitor will report corruption or not. Fix an intervention profile Ïƒ = (Ïƒ0 , Ïƒ1 ), with Ïƒ0 < Ïƒ1 ,
and a level of retaliation r conditional on intervention.
We first note that when the agent is not corrupt (c = 0), it is optimal for the monitor
to send message m = 0 regardless of retaliation level r. Indeed, given c = 0, her expected
payoffs conditional on messages m = 1 and m = 0 necessarily satisfy
E[uA |m = 1] = Ïƒ1 [vM (c = 0, m = 1) âˆ’ r] â‰¤ Ïƒ0 [vM (c = 0, m = 0) âˆ’ r] = E[uA |m = 0].
As a result, a non-corrupt agent will find it optimal to set retaliation level r = 0. Note that
this relies on the assumption that the monitor is non-malicious (vM (c = 0, m = 1) â‰¤ 0).
When the monitor is malicious (vM (c = 0, m = 1) > 0), even honest agents may need to use
threats to ensure that message m = 0 is sent.
Consider now the case where the agent chooses to be corrupt, i.e. c = 1. The monitor
will report corruption and send message m = 1 if and only if
Ïƒ1 [vM (c = 1, m = 1) âˆ’ r] â‰¥ Ïƒ0 [vM (c = 1, m = 0) âˆ’ r].
This holds whenever


Ïƒ1 vM (c = 1, m = 1) âˆ’ Ïƒ0 vM (c = 1, m = 0)
r â‰¤ rÏƒ â‰¡
Ïƒ1 âˆ’ Ïƒ 0

+
(1)

where x+ â‰¡ max{x, 0} by convention. Note that whenever vM (c = 1, m = 1) < 0 (i.e. the
monitor suffers from intervention against a corrupt agent), there will be intervention profiles
Ïƒ such that rÏƒ = 0: the monitor prefers to send message m = 0 even in the absence of

13

retaliation. This possibility is a concern in the context of foreign aid if corruption scandals
cause aid to be withheld (Ensminger, 2013).
Information manipulation and corruption. We now characterize the agentâ€™s decisions.
Note first that rÏƒ can be expressed as a function of likelihood ratio Î» â‰¡


Î»vM (c = 1, m = 1) âˆ’ vM (c = 1, m = 0)
rÏƒ = rÎ» â‰¡
Î»âˆ’1

Ïƒ1
:
Ïƒ0

+
.

Threshold rÎ» is decreasing in Î»: when the information content of intervention is large,
moderate threats of retaliation are sufficient to shut down reporting.
Consider now the agentâ€™s incentives to influence reporting conditional on being corrupt
(c = 1). Since retaliation r is costly to the agent, he either picks r = 0 and lets the monitor
send her preferred message, or picks r = rÏƒ and induces message m = 0 at the lowest possible
cost. Hence, the agent will manipulate messages through the threat of retaliation if and only
if:
Ïƒ1 vA (c = 1) â‰¤ Ïƒ0 [vA (c = 1) âˆ’ kA (rÏƒ )]
â‡â‡’ Î»vA (c = 1) â‰¤ vA (c = 1) âˆ’ kA (rÎ» ).

(2)

Whenever the information content of intervention Î» is high enough, the agent will induce
message m = 0, and there will be unreported corruption.
Fact 2 (unreported corruption). There exists Î»0 â‰¥ 1 such that a corrupt agent induces
message m = 0 if and only if

Ïƒ1
Ïƒ0

> Î»0 .

Altogether, the agent will choose to be corrupt if and only if
Ïƒ0 vA (c = 0) < Ï€A + max{Ïƒ1 vA (c = 1), Ïƒ0 [vA (c = 1) âˆ’ kA (rÏƒ )]}.
This can be further simplified, since by Assumption 1, vA (c = 0) = 0.

14

(3)

Optimal intervention. It is now straightforward to characterize the optimal intervention
profile. One notable property is that it involves interior rates of intervention conditional on
both messages m = 0 and m = 1. The reason for this is that by setting Ïƒ1 < 1 one can
lower baseline intervention rate Ïƒ0 , while keeping the likelihood-ratio of intervention

Ïƒ1
Ïƒ0

low

enough that messages from the monitor are informative.
Fact 3 (optimal intervention). The optimal intervention profile Ïƒ âˆ— satisfies (2) and (3) with
equality:
Ïƒ1âˆ— =

Ï€A
âˆ’vA (c = 1)

and

Ïƒ0âˆ— =

Ïƒ1âˆ—
.
Î»0

Profile Ïƒ âˆ— is interior: Ïƒ0âˆ— âˆˆ (0, 1) and Ïƒ1âˆ— âˆˆ (0, 1). Under policy Ïƒ âˆ— , there is no corruption
and no retaliation on the equilibrium path.
Inference, and data-driven policy design. We now ask whether it is possible to make
inferences about underlying corruption c on the basis of unverifiable messages m alone.
It turns out that even though messages are unverifiable and unreported corruption is a
possibility, variation in messages across different policy choices provides sharp information
about underlying levels of corruption.
Consider old and new intervention profiles Ïƒ O and Ïƒ N such that
Ïƒ0O < Ïƒ0N ,

Ïƒ1O â‰¤ Ïƒ1N ,

and

Ïƒ1N
Ïƒ1O
â‰¤
.
Ïƒ0N
Ïƒ0O

(4)

We think of these two intervention profiles as policy experiments implemented on different subsamples of a population of agents and monitors.23 Intervention profile Ïƒ N involves
strictly more intervention than Ïƒ O while having a lower information content of intervention
Î». As a result it may reasonably be expected to yield both less corruption and more reliable
messages. Let cO , cN and mO , mN denote the respective corruption and reporting decisions
in equilibrium conditional on Ïƒ O and Ïƒ N .
23

Taking seriously this population view of the agency problem, we allow for heterogeneity across agents
and monitors in Sections 3 and 4.

15

Patterns of corruption and reporting implied by conditions (2) and (3) are illustrated in
Figure 1. The following property holds.
Fact 4. For every pair of policies Ïƒ O , Ïƒ N satisfying (4)
cO â‰¥ mO ;

(5)

cO â‰¥ cN ;

(6)

mO > mN â‡’ cO > cN .

(7)

In words, across policy changes that increase the frequency of intervention while also
decreasing the information content of intervention: (i) reported corruption is always a weak
underestimate of true corruption; (ii) the amount of underlying corruption can only diminish;
and (iii) drops in reported corruption are a reliable indicator of drops in true corruption.

Figure 1: corruption and messages (c, m) as a function of intervention profiles (Ïƒ0 , Ïƒ1 ); payoff
specification Ï€A = 5, vA (c) = âˆ’10c, vM (c, m) = âˆ’2 + c(3 + 3m), kA (r) = 20r.
Fact 4 implies that messages and changes in messages can be used to make sharp inferences about underlying levels of corruption. A corollary is that one can identify the optimal
intervention policy using unverifiable message data. Denote by mâˆ— (Ïƒ) equilibrium reports
16

at policy profile Ïƒ.
Fact 5. The optimal policy Ïƒ âˆ— solves
min{Ïƒ0N | with Ïƒ N s.t. mâˆ— (Ïƒ N ) = 0 and âˆƒÏƒ O satisfying (4) s.t. mâˆ— (Ïƒ O ) = 1}.
ÏƒN

(8)

In words, the optimal policy is the one that requires the lowest level of baseline intervention Ïƒ0âˆ— consistent with: (1) message m = 0 being sent at Ïƒ âˆ— ; (2) message m = 1 being sent
at an intervention profile that involves less frequent intervention and a higher information
content of intervention Î». Point (2) ensures that there is no unreported corruption occurring
at Ïƒ âˆ— and that reports of no-corruption can be trusted.
Fragility. Some of the properties highlighted in Facts 4 and 5 are intuitive and seem
like they should be robust: maintaining or increasing intervention levels, greater plausible
deniability should diminishes corruption; and once there is sufficient plausible deniability that
monitors report corruption, drops in reported corruption should be reliable. Unfortunately,
it turns out that these useful properties do not extend to more general environments, and
the following possibility results should serve as cautionary tales for policy design. Consider
policies Ïƒ O and Ïƒ N satisfying (4). The following can happen:
discouraging the honest â€“ if vA (c = 0) < 0, i.e. intervention is costly to an honest agent,
it may be that cO = 0 < cN = 1: corruption increases with policy Ïƒ N ; this may happen
if corrupt agents are being reported under Ïƒ O , so that increasing baseline intervention
rate Ïƒ0 does not affect the payoff of corrupt agents but diminishes that of honest ones;
empowering the scoundrels â€“ if vM (c = 0, m = 1) > 0, i.e. the monitor is malicious and
benefits from intervention against non-corrupt agents, it may be that cO < mO (there
is over-reporting) and that mO > mN 6â‡’ cO > cN (drops in complaints do not imply
drops in corruption); indeed, greater plausible deniability may help malicious monitors
send inaccurate messages about honest agents;

17

unreliable drops in reports â€“ with malicious monitors and uncertainty over payoffs, it
may be that E[mO ] > E[mN ] and E[cO ] < E[cN ], i.e. drops in complaints are unreliable:
average complaint rates can decrease while underlying levels of corruption increase.
See Appendix A for examples illustrating these different possibilities. Our general framework
allows us to tackle these challenges head-on, and identify robust ways in which unverifiable
messages can serve to inform policy decisions.

3

General Framework

In order to better assess the robust inferences that can be drawn from our model, we generalize the framework of Section 2 in three important ways: first, we allow for arbitrary
incomplete information over the types of the agent and the monitor; second we allow for
the possibility of malicious monitors, i.e. monitors who benefit from intervention against an
honest agent; third we allow for the possibility of leaks which may reveal information over
messages sent by the monitor following intervention.
Types. Payoffs take the same general form as in Section 2, but we relax the complete
information assumption of Section 2 and allow for rich incomplete information. Monitors and
agents have types Ï„ = (Ï„M , Ï„A ) âˆˆ TM Ã— TA = T such that the monitorâ€™s type Ï„M determines
her payoffs (Ï€M , vM ), while the agentâ€™s type Ï„A determines his payoffs (Ï€A , vA , kA ), and his
belief over the type Ï„M of the monitor, which we denote by Î¦(Ï„M |Ï„A ) âˆˆ âˆ†(TM ). We assume
that TM is a compact subset of Rn . The only assumptions imposed on the model are the
following common knowledge restriction on payoffs.
Assumption 2 (general payoffs). It is common-knowledge that payoffs satisfy
Ï€A â‰¥ 0;
âˆ€c âˆˆ {0, 1},

vA (c) â‰¤ 0;

âˆ€c âˆˆ {0, 1},

vM (c, m = c) â‰¥ vM (c, m 6= c).
18

We note that under Assumption 2, a positive mass of agents may get no benefits from
corruption (Ï€A = 0), the certainty of intervention need not dissuade corruption (Ï€A + vA (c =
1) > vA (c = 0)), and monitors may be malicious in that they benefit from intervention
happening against an honest agent (vM (c = 0, m = 1) > 0). We continue to assume that
conditional on intervention, monitors have weak preferences for telling the truth. Note
that this doesnâ€™t preclude the possibility of malicious monitors. In accordance with this
assumption, we consider policy profiles such that Ïƒ1 â‰¥ Ïƒ0 .
We denote by ÂµT âˆˆ âˆ†(T ) the true distribution of types Ï„ âˆˆ T in the population. Distribution ÂµT may exhibit arbitrary correlation between the types of the monitor and the
agent, and is unknown to the principal. We think of this underlying population as a large
population from which it is possible to sample independent (agent, monitor) pairs.
Leaks. We generalize the assumption that the agent can observe the principalâ€™s intervention decisions. The agent now observes an abstract signal z âˆˆ Z âˆª {âˆ…} on which he can
condition his retaliation policy. We think of signal z as leaks from the institutional process triggered by intervention. We assume that z = âˆ… conditional on no intervention and
follows some distribution F (Â·|m, c) conditional on intervention. Note that âˆ… remains a possible outcome conditional on intervention. In that case intervention yields no observable
consequences.
The only restriction we impose on F is that for all c âˆˆ {0, 1},
probF (z = âˆ…|m = 0, c) â‰¥ probF (z = âˆ…|m = 1, c),
that is, message m = 0 is weakly more likely to lead to no consequences. This ensures that
in equilibrium, retaliation only occurs if intervention has been triggered. Allowing for leaks
makes our analysis applicable to settings in which investigating institutions are not entirely
trustworthy, resulting in information being revealed to the agent. Note that since leaks are
possible, the principal has only limited commitment power and the revelation principle does

19

not apply.24 This is inherently an indirect mechanism design problem where messages have
hard-wired institutional meaning.

4

Patterns of Corruption and Reporting

4.1

The Basic Trade-off

The basic trade-off between using information efficiently and keeping information channels
open is the same as in Section 2. Denote by câˆ— (Ïƒ, Ï„A ) the optimal corruption decision by an
agent of type Ï„A under policy Ïƒ, and by mâˆ— (Ïƒ, Ï„ ) the optimal message sent by a monitor
of type Ï„M facing an agent of type Ï„A under policy Ïƒ. As before, let Î» =

Ïƒ1
Ïƒ0

denote the

likelihood ratio of intervention rates.
Proposition 1. Assume that messages are exogenously informative, i.e. that the monitor
is an automaton following strategy m(c) = c. Any optimal intervention profile Ïƒ âˆ— 6= 0 must
be such that Ïƒ0âˆ— = 0 and Ïƒ1âˆ— > 0.25
If instead messages are endogenous, we have that
Z

câˆ— (Ïƒ, Ï„A )dÂµT (Ï„A ) â‰¥ probÂµT (Ï€A > 0);
TA
Z
âˆ€Ï„A s.t. vA (Â·) < 0, lim
mâˆ— (Ïƒ, Ï„ )dÎ¦(Ï„M |Ï„A ) = 0.
lim inf
Î»â†’âˆ

Î»â†’âˆ

As Î» =

Ïƒ1
Ïƒ0

TM

gets arbitrarily large, all agents with strictly positive value for being corrupt

choose to be corrupt, and all agents who suffer strictly from intervention shut down reporting
(from either malicious or non-malicious monitors).
24
See Bester and Strausz (2001) for a partial extension of the revelation principle in principal-agent settings
where the principal does not have commitment power. Note that in our setting leaks are not under the control
of the principal.
25
The optimal intervention policy Ïƒ âˆ— may be equal to zero if the equilibrium cost of intervention overwhelms
the gains from dissuading corruption.

20

4.2

The Geometry of Corruption and Reporting

Consider a given agent of type Ï„A . Without loss of generality, we can restrict attention to
retaliation schemes that involve retaliation only conditional on intervention leading to some
consequences, i.e. z 6= âˆ….26
A retaliation profile r : Z â†’ [0, +âˆ) and a corruption decision c induce a messaging
profile m : TM â†’ {0, 1} such that for all Ï„M âˆˆ TM ,
m(Ï„M ) âˆˆ arg max Ïƒm
b âˆ’ E(r|c, m)].
b
b [vM (c, m)

(9)

mâˆˆ{0,1}
b

We denote by M = {0, 1}TM the set of message profiles, and for any message m âˆˆ {0, 1}
define Â¬m to be the other message. For any corruption decision c, and any message profile
Ï„A
m âˆˆ M, consider the normalized cost Kc,m
(Ïƒ) of implementing report profile m defined by

Ï„A
Kc,m
(Ïƒ)

1
=
Ïƒ0

Z
Ïƒm(Ï„M ) kA (r(z))dF (z|c, m(Ï„M ))dÎ¦(Ï„M |Ï„A )

inf
r:Zâ†’[0,+âˆ)

(10)

ZÃ—TM

s.t. âˆ€Ï„M , m â‰¡ m(Ï„M ) satisfies,
Ïƒm [E(vM |m, c) âˆ’ E(r|m, c)] â‰¥ ÏƒÂ¬m [E(vM |Â¬m, c) âˆ’ E(r|Â¬m, c)]
By convention, this cost is infinite whenever message profile m is not implementable, i.e.
when there is no retaliation profile r such that (9) holds for all Ï„M âˆˆ TM . Noting that for
all m âˆˆ {0, 1},

Ïƒm
Ïƒ0

Ïƒm
ÏƒÂ¬m

= Î»m and

Ï„A
(Ïƒ) of implementing
= Î»2mâˆ’1 , it follows that the cost Kc,m

Ï„A
message profile m can expressed as a function Kc,m
(Î») of the likelihood ratio Î» of intervention

rates. Altogether, an agent with type Ï„A will choose to be honest if and only if
Z
Ï€A + Ïƒ0 sup

m(Ï„M )

Î»
Z

mâˆˆM



vA (c = 1)dÎ¦(Ï„M |Ï„A ) âˆ’

Ï„A
Kc=1,m
(Î»)

TM

â‰¤ Ïƒ0 sup
mâˆˆM

Î»

m(Ï„M )



vA (c = 0)dÎ¦(Ï„M |Ï„A ) âˆ’

Ï„A
Kc=0,m
(Î»)

.

(11)

TM

This implies several useful properties of corruption and reporting decisions in equilibrium.
26

See Lemma B.2 for a proof.

21

Proposition 2 (patterns of manipulation and corruption).

(i) Pick an agent of

type Ï„A and consider old and new intervention profiles Ïƒ O , Ïƒ N such that Ïƒ O =
ÏÏƒ N , with Ï > 0. Denote by cO , cN and mO , mN the corruption decisions and
message profiles implemented by the agent in equilibrium at Ïƒ O and Ïƒ N . If cO =
cN , then mO = mN .
(ii) Consider an agent of type Ï„A . The set of intervention profiles Ïƒ such that
the agent chooses to be corrupt is star-shaped around (0, 0): if câˆ— (Ïƒ, Ï„A ) = 1, then
câˆ— (ÏÏƒ, Ï„A ) = 1 for all Ï âˆˆ [0, 1].
(iii) Fix the ratio of intervention rates Î» â‰¥ 1 and consider the ray {(Ïƒ0 , Î»Ïƒ0 ) with Ïƒ0 âˆˆ
[0, 1]}. Along this ray, under the true distribution ÂµT , the mass of corrupt agents
Z

câˆ— (Ïƒ, Ï„A )dÂµT (Ï„A )

TA

is decreasing in baseline intervention rate Ïƒ0 .
In words, point (i) states that whenever intervention profiles have the same information
content Î», message profiles change if and only if the underlying corruption behavior of the
agent changes. Points (ii) and (iii) show that keeping the information content of intervention
Î» constant, agents are less likely to be corrupt as the intensity of intervention increases.

4.3

Inference and Policy Evaluation from Unverifiable Reports

We now investigate the extent to which unverifiable reports can be used to make inferences
about the underlying levels of corruption, and to inform policy choices. Note that the only
data observable to the principal is the aggregate mass of corruption messages
Z

mâˆ— (Ïƒ, Ï„ )dÂµT (Ï„ ).

T

We first highlight that in our rich environment, unverifiable messages at a single policy profile
Ïƒ imply no restrictions on underlying levels of corruption.
22

Proposition 3. Take as given any interior policy profile Ïƒ, and a true distribution ÂµT
R
yielding aggregate complaint rate T mâˆ— (Ïƒ, Ï„ )dÂµT (Ï„ ). We have that
Z

Z

âˆ—

c (Ïƒ, Ï„A )db
ÂµT (Ï„ ),

for

Âµ
bT s.t

T

Z

âˆ—

m (Ïƒ, Ï„ )db
ÂµT (Ï„ ) =
T


m (Ïƒ, Ï„ )dÂµT (Ï„ ) = [0, 1].
âˆ—

T

In words, taking as given a policy profile and an observable level of aggregate complaints,
one can find an underlying environment that rationalizes both the given level of complaints
and any arbitrary underlying degree of corruption.
While reports at a single policy profile are uninformative, we now show that variation in
the mass of corruption messages across appropriately chosen policy profiles can imply useful
bounds on the underlying levels of corruption.
Proposition 4. Consider policies Ïƒ O and Ïƒ N such that Ïƒ N = ÏÏƒ O , with Ï > 1. For all
possible true distributions ÂµT âˆˆ âˆ†(T ), we have that
Z



âˆ—

O

âˆ—

N


c (Ïƒ , Ï„A ) âˆ’ c (Ïƒ , Ï„A ) dÂµT (Ï„ ) â‰¥

T

Z

[mâˆ— (Ïƒ N , Ï„ ) âˆ’ mâˆ— (Ïƒ O , Ï„ )]dÂµT (Ï„ ) .

T

When policy profiles move along a ray, observable changes in message patterns provide a
lower bound for changes in underlying levels of corruption. An immediate corollary is that
R
changes in aggregate complaint levels T [mâˆ— (Ïƒ N , Ï„ ) âˆ’ mâˆ— (Ïƒ O , Ï„ )]dÂµT (Ï„ ) provide a lower
R
bound for both the mass T [1 âˆ’ câˆ— (Ïƒ N , Ï„A )]dÂµT (Ï„ ) of honest agents at policy Ïƒ N as well as
R
a lower bound for the mass T câˆ— (Ïƒ O , Ï„A )dÂµT (Ï„ ) of corrupt agents at policy Ïƒ O .
We now show that Proposition 4 can be used to inform policy design. Imagine that
some set of policy experiments Ïƒ âˆˆ Î£ can be performed, where Î£ is a set of feasible policy
profiles. Proposition 4 suggests the following heuristic to specify intervention policies. Define
b : [0, 1]2 â†’ [0, 1] the function defined by
v P = mincâˆˆ{0,1} vP (c), and denote by C
2

âˆ€Ïƒ âˆˆ [0, 1] ,

(Z
b
C(Ïƒ)
â‰¡ 1âˆ’max

)
âˆ—

âˆ—

0

0

[m (Ïƒ, Ï„ ) âˆ’ m (Ïƒ , Ï„ )]dÂµT (Ï„ ) Ïƒ âˆˆ Î£ âˆ© {ÏÏƒ|Ï âˆˆ [0, 1]} .
T

b
From Proposition 4 we know that C(Ïƒ)
is an upper bound to the amount of underlying
23

corruption at Ïƒ. Noting that for a given intervention profile Ïƒ, the principalâ€™s payoff is
âˆ—

Z

âˆ—

EÂµT [uP |c , m , Ïƒ] = Ï€P

Z

âˆ—

c (Ïƒ, Ï„A )dÂµT (Ï„ ) +
T

vP (câˆ— (Ïƒ, Ï„A ))Ïƒmâˆ— (Ïƒ,Ï„ ) dÂµT (Ï„ ),

T

we obtain the following corollary.
Corollary 1. For any intervention profile Ïƒ, we have that


Z
âˆ—
b
EÂµT [uP |c , m , Ïƒ] â‰¥ Ï€P C(Ïƒ)
+ v P Ïƒ0 + (Ïƒ1 âˆ’ Ïƒ0 ) m (Ïƒ, Ï„ )dÂµT (Ï„ ) .
âˆ—

âˆ—

T

Furthermore, if Î£ = [0, 1]2 , then the data-driven heuristic policy Ïƒ
b(ÂµT ) defined by

b
Ïƒ
b(ÂµT ) âˆˆ arg max2 Ï€P C(Ïƒ)
+ v P Ïƒ0 + (Ïƒ1 âˆ’ Ïƒ0 )
Ïƒâˆˆ[0,1]

Z

âˆ—



m (Ïƒ, Ï„ )dÂµT (Ï„ )
T

is a weakly undominated strategy with respect to the unknown true distribution ÂµT .
While finding policy Ïƒ
b(ÂµT ) requires many policy experiments, the underlying logic can
be exploited in more practical ways. The basic insight is to first, find an intervention profile
with information content Î» low enough that monitors are willing to send complaints; second,
scale up intervention rates, keeping the information content of intervention Î» constant, until
complaints diminish by a sufficient amount.

5
5.1

Discussion
Summary

We model the problem of a principal who relies on messages from informed monitors to target
intervention against a possibly corrupt agent. The difficulty is that the agent can dissuade the
monitor from informing the principal by threatening to retaliate conditional on observables.
In this setting, intervention becomes a signal which the agent can exploit to effectively
dissuade the monitor from complaining. As a consequence, effective intervention strategies
24

must garble the information content of messages. In particular, there needs to be a positive
baseline rate of intervention following the message â€œnon-corruptâ€. This creates an imperfect
monitoring problem between the agent and the monitor which limits the effectiveness with
which they can side-contract.
Because hard-evidence of corruption is hard to come by, we explore the extent to which
one can make inferences about unobservable corruption, as well as evaluate policies, on the
basis of unverifiable messages alone. We consider a general framework wich allows for near
arbitrary incomplete information and heterogeneity across agents and monitors. We establish
general properties of reporting and corruption patterns which can be exploited to derive
bounds on underlying corruption as a function of unverifiable reports. These bounds suggest
heuristics to identify robust intervention policies which can be described as follows: first find
intervention profiles that guarantee sufficient plausible deniability for monitors to complain,
then increase intervention rates proportionally until complaints fall to an acceptable level.

5.2

Implementation

A strength of our analysis is that it does not presume that the principal has extensive control
over the payoffs of the agent and the monitor. This accommodates environments in which
the relevant principal has to rely on existing institutional channels to carry out interventions.
This scenario is particularly relevant for IT-driven counter-corruption policies, for instance
Punjabâ€™s Citizen Feedback Model, or policies similar to the one studied in Callen et al.
(2013). Still our policy suggestions raise some practical concerns.
Commitment to mixed strategies. Our analysis assumes the principal is able to commit
to mixed strategies which is admittedly more demanding than committing to pure strategies. One way to justify this assumption is to invoke reputational concerns in an unmodelled
continuation game, committing to mixed strategies being equivalent to forming a reputation
under imperfect public monitoring (Fudenberg and Levine, 1992). A more practical observation is that commitment to mixed strategies can be achieved through hard-wired garbling

25

of messages at the surveying stage. Specifically, instead of recording messages directly, the
principal may record the outcomes of two Bernoulli lotteries l0 and l1 such that
ï£±
ï£² 1 with proba Ïƒ
0
l0 =
ï£³ 0 with proba 1 âˆ’ Ïƒ
0

ï£±
ï£² 1 with proba Ïƒ
1
and l1 =
ï£³ 0 with proba 1 âˆ’ Ïƒ .
1

The monitor communicates by privately picking a lottery, with observed realized outcome y.
Conditional on y the principal intervenes according to pure strategy i(y) = y. This approach
has the benefit of making plausible deniability manifest to participating monitors. Crucially,
one can recover aggregate submitted reports from outcome y data alone: for any mapping
m : T â†’ {0, 1},
R

Z
m(Ï„ )dÂµT (Ï„ ) =

T

T

y(Ï„ )dÂµT (Ï„ ) âˆ’ Ïƒ0
.
Ïƒ1 âˆ’ Ïƒ0

Hence the analysis of Section 4 continues to apply as is. Note that this implementation
of mixed strategies is closely related to the randomized response techniques introduced by
Warner (1965).27
Destroying information. A salient concern with the policies we consider is that they
require the government to explicitly garble valuable information. In particular, we show
that the optimal policy may involve Ïƒ1 < 1 and Ïƒ0 > 0, i.e. triggering intervention against
agents for whom there have been no complaints, while not investigating all agents against
which a complaint has been filed. This is supect behavior that governments may prefer to
avoid. This reinforces the argument that garbling should occur at the recording stage, with
the caveat that the garbling procedure needs to appear natural and legitimate to participants.
In addition, one may choose to focus on the subset of policies such that Ïƒ1 = 1, so that the
government cannot be suspected of abetting corruption.
27

The main difference is that typical randomized response techniques simply enjoin the monitor to garble
his response, but the monitor can always guarantee his preferred message. Hence, in our fully rational
framework, traditional randomized response techniques do not guarantee plausible deniability in all equilibria.
This difference is important when messages are used for equilibrium incentive design, rather than for one-shot
surveys.

26

Validating structural inference. Our analysis emphasizes the possibility of making
structural inferences over underlying corruption on the basis of â€œsoftâ€ unverifiable messages. This is motivated by the fact that in many environments corruption itself is very
difficult to observe. While it is encouraging that in a fairly general setting, theory allows us
to place bounds on underlying corruption on the basis of unverifiable messages alone, it is
legitimate to worry whether equilibrium inferences from our model can be trusted. In this
respect, obtaining â€œhardâ€ direct measures of corruption is valuable, even though cost limits
their scalability. Indeed, even a limited sample of direct measures could be used to calibrate
the meaning of unverifiable messages obtained from agents, as well as confirm or not the
structural implications of our analysis.

Appendix - For Online Publication
A

Extensions

A.1

Examples

In this appendix we explicitly solve the model in the case where payoffs are complete information between the agent and the monitor, but allow for the more general payoffs described in
Assumption 2. Specifically, intervention may be costly even to honest agents (vA (c = 0) < 0)
and monitors may be malicious (vM (c = 0, m = 1) > 0). We describe explicitly environments
for which the possibility results discussed at the end of Section 2 are true. Again, the model
is solved by backward induction.
Reporting by the monitor. Take as given an intervention profile Ïƒ = (Ïƒ0 , Ïƒ1 ), with
Ïƒ0 < Ïƒ1 , and a level of retaliation r conditional on intervention.
When the agent is not corrupt (c = 0), the monitor sends message m = 0 if and only if
Ïƒ1 [vM (c = 0, m = 1) âˆ’ r] < Ïƒ0 [vM (c = 0, m = 0) âˆ’ r].

27

This holds if and only if
râ‰¥

rÏƒ0



Ïƒ1 vM (c = 0, m = 1) âˆ’ Ïƒ0 vM (c = 0, m = 0)
â‰¡
Ïƒ1 âˆ’ Ïƒ 0

+
.

Because the monitor is malicious, a non-corrupt agent may now have to threaten the monitor
with positive retaliation rÏƒ0 to induce the monitor to send message m = 0.
When the agent is corrupt, i.e. c = 1, the monitor will report corruption and send
message m = 1 if and only if
Ïƒ1 [vM (c = 1, m = 1) âˆ’ r] â‰¥ Ïƒ0 [vM (c = 1, m = 0) âˆ’ r].
This will hold whenever
râ‰¤

rÏƒ1



Ïƒ1 vM (c = 1, m = 1) âˆ’ Ïƒ0 vM (c = 1, m = 0)
â‰¡
Ïƒ1 âˆ’ Ïƒ 0

As before, rÏƒ1 is decreasing in the ratio
of ratios

Ïƒ1
Ïƒ0

Ïƒ1
.
Ïƒ0

+
.

In addition rÏƒ0 is decreasing in

Ïƒ1
Ïƒ0

over the range

such that rÏƒ0 > 0. As before, the information content of intervention affects the

level of retaliation needed to influence messaging.
Information manipulation and corruption. We now examine the agentâ€™s behavior.
Consider the agentâ€™s incentives to manipulate information given a corruption decision c âˆˆ
{0, 1}. Since retaliation r is costly to the agent, he either picks r = 0 and does not influence
the monitor, or picks r = rÏƒc and induces message m = 0 at the lowest possible cost. Hence,
the agent will induce a message m(Ïƒ, c) such that
m(Ïƒ, c) âˆˆ arg max Ïƒm [vA (c) âˆ’ 1m=0 kA (rÏƒc )].
mâˆˆ{0,1}

28

(12)

It follows that the agent will choose not to be corrupt if and only if
Ï€A + max{Ïƒ1 vA (c = 1), Ïƒ0 [vA (c = 1) âˆ’ kA (rÏƒ1 )]} â‰¤ max{Ïƒ1 vA (c = 0), Ïƒ0 [vA (c = 0) âˆ’ kA (rÏƒ0 )]}.
(13)
We can now provide explicit illustrations of the possibility results discussed in Section 2.
Consider policies Ïƒ O and Ïƒ N satisfying condition (4). Figure 2(a) illustrates the fact that
when vA (c = 0) < 0, it may be that increasing intervention (even without increasing the
likelihood ratio of intervention rates) can result in greater corruption. In this example,
increasing the baseline intervention rate Ïƒ0 diminishes the payoffs of non-corrupt agents
without affecting the payoffs of corrupt ones.
Figures 2(b) and 2(c) show that when the monitor is malicious, messages of corruption
are no longer a lower bound to true corruption. In fact, policy changes from Ïƒ O to Ïƒ N
can generate both increases and decreases in reports without corresponding changes in the
underlying level of corruption.
Finally, the environments of Figures 2(a) and 2(b) can be used to construct a stochastic
example in which a policy change from Ïƒ O to Ïƒ N induces a strict drop in reported corruption and a strict increase in underlying corruption. Indeed imagine that while payoffs
(vM , vA , Ï€A ) are common-knowledge between the agent and the monitor, they are uncertain
for the principal. In particular, say that with probability .3 payoffs are those of Figure 2(a)
and with probability .7 payoffs are those of Figure 2(b). Then one can pick policies Ïƒ O and
Ïƒ N satisfying condition (4) such that E[mO ] = .7 and E[cO ] = 0, while E[mN ] = .3 and
E[cN ] = .3. In this case, a strict drop in complaints is associated with a strict increase in
corruption.

A.2

Multiple Monitors

Our analysis can be extended to settings with multiple monitors. Imagine that there are
now L monitors indexed by i âˆˆ {1, Â· Â· Â· , L}, each of which observes the agentâ€™s corruption
decision c âˆˆ {0, 1} and can send a binary message mi âˆˆ {0, 1} to the principal. We denote by
29

(a) unproductive intervention: vA (c = 1) =
âˆ’4.5 âˆ’ 5.5c, vM (c, m) = âˆ’2 + c(3 + 3m)
1
0.9
0.8
0.7

Ïƒ1

0.6
0.5
0.4

(c,m)

0.3

0,0
0,1
1,0
1,1
NA

0.2
0.1

0.00.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Ïƒ0

1

(b) malicious monitor 1: vA (c = 1) = âˆ’10c,
vM (c, m) = 2 + c(âˆ’1 + 2.5m)
1
0.9
0.8
0.7

Ïƒ1

0.6
0.5
0.4

(c,m)

0.3

0,0
0,1
1,0
1,1
NA

0.2
0.1

0.00.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Ïƒ0

1

(c) malicious monitor 2: vA (c = 1) = âˆ’2 âˆ’ 8c,
vM (c, m) = 2 + c(âˆ’1 + 2.5m)

Figure 2: Corruption decisions and messages (c, m) as a function of intervention profiles
(Ïƒ0 , Ïƒ1 ); common parameters: Ï€A = 5, kA (r) = 20r.

30

â†’
âˆ’
m âˆˆ {0, 1}L the vector of messages sent by the monitors. We abuse notation and denote by 0
the message profile in which all monitors report mi = 0, and by 1 the message profile in which
all monitors report mi = 1. An intervention policy Ïƒ is now a map Ïƒ : {0, 1}L â†’ [0, 1]. For
example, likelihood of intervention may be an affine function of the number of complaints,
PL
1
â†’
Ïƒâˆ’
m = Ïƒ0 + (Ïƒ1 âˆ’ Ïƒ0 ) L
i=1 mi . Alternatively, it may follow a threshold rule, with threshold
P
â†’
Î˜ âˆˆ N, i.e. Ïƒâˆ’
. For simplicity, we consider intervention policies
m = Ïƒ0 + (Ïƒ1 âˆ’ Ïƒ0 )1 L
i=1 mi >Î˜
âˆ’
â†’ â‰¥ Ïƒ
such that for all â†’
m, Ïƒâˆ’
m

0

As in Section 3, the agent and monitors have arbitrary types, except for the fact that
Assumption 2 is common knowledge among players. We assume that each monitor iâ€™s value
conditional on intervention vi,M depends only on c and her own message mi . The agent now
âˆ’
commits to a profile of vector-valued retaliation intensities â†’
r : Z â†’ [0, +âˆ)L associated
âˆ’
âˆ’
with a cost function kA (â†’
r ) that is increasing in all components of â†’
r.
The vector of monitorsâ€™ types is denoted by âˆ’
Ï„â†’
M = (Ï„i,M )iâˆˆ{1,Â·Â·Â· ,L} . Note that now, each
monitorâ€™s type must include a belief over other monitorsâ€™ types. Furthermore, the agentâ€™s
L
belief over âˆ’
Ï„â†’
M is now a joint distribution over (TM ) . Finally, the distribution of leaks z may
L
âˆ’
âˆ’
depend on the vector of messages â†’
m. We denote by â†’
m âˆˆ {0, 1}TM
profiles of message
vectors as a function of the monitorsâ€™ types. Note that for all i âˆˆ {1, Â· Â· Â· , L} monitor iâ€™s
message profile mi (Ï„i,M ) is only a function of monitor iâ€™s type (i.e. we donâ€™t consider richer
mechanisms that would let monitorsâ€™ exchange information about their type).
The main properties identified in Section 4 continue to hold: for messages to be informative, it must be that all likelihood ratios of intervention rates be bounded away from
0; when policies Ïƒ are ordered along a ray, message profiles change only when corruption
decisions change, and corruption must decrease along a ray going away from the origin. One
difficulty is that there may now be multiple messaging equilibria among agents conditional
on a given retaliation policy. We work under the assumption that given a retaliation policy,
the agent is able to select the equilibrium that most benefits him, and that this equilibrium is
âˆ’
unique. We continue to think of the agent as selecting a message profile â†’
m under constraints
corresponding to the monitorsâ€™ incentive compatibility conditions.

31

Fact A.1. If Ïƒ0 = 0 then all agents that benefit from corruption will be corrupt, and induce
âˆ’
message profile â†’
m = 0.
Proof. The proof is identical to that of Proposition 1. By setting r(z = âˆ…) = 0 and r(z 6=
âˆ’
0) = r arbitrarily high, the agent is able to induce message â†’
m = 0 at no cost in equilibrium,
which insures that there is no intervention.
 
â†’
âˆ’
â†’
âˆ’
Given an interior intervention profile Ïƒ, define Î» = ÏƒÏƒm0 âˆ’
â†’

mâˆˆ{0,1}L

the vector of likelihood

ratios of intervention.
â†’
âˆ’
Proposition A.1. Fix a vector of intervention ratios Î» and consider the ray of intervention
â†’
âˆ’
policies {Ïƒ0 Î» for Ïƒ0 âˆˆ [0, 1]}. Along this ray the following properties hold:
âˆ’
(i) conditional on a corruption decision c, the message profile â†’
m that a given
agent chooses to induce is constant along the ray;
(ii) the agentâ€™s decision to be corrupt is decreasing in Ïƒ0 along the ray.
Proof. The proof is essentially identical to that of Proposition 2. Let us begin with point
âˆ’
(i). Conditional on a corruption decision c âˆˆ {0, 1}, for any message profile â†’
m, we define
âˆ’
the cost Kc,Ï„Aâˆ’
(Ïƒ) of inducing message profile â†’
m as
â†’
m
(Ïƒ)
Kc,Ï„Aâˆ’
â†’
m

1
=
Ïƒ0

ZZ
inf
r:Zâ†’[0,+âˆ)

L
ZÃ—TM

âˆ’
âˆ’
â†’
â†’
k (r(z))dF (z|c, â†’
m(âˆ’
Ï„â†’
Ïƒâˆ’
M ))dÎ¦(Ï„M |Ï„A )
m(âˆ’
Ï„â†’
M) A

â†’
âˆ’
s.t. âˆ€âˆ’
Ï„â†’
M = (Ï„i,M )iâˆˆ{1,Â·Â·Â· ,L} , (mi )iâˆˆ{1,Â·Â·Â· ,L} = m(Ï„i,M ) satisfies
âˆ€i âˆˆ {1, Â· Â· Â· , L},




â†’
âˆ’
â†’
âˆ’
â†’
â†’
E Ïƒ(mi ,âˆ’
mâˆ’i ) vi,M (mi , c) âˆ’ ri mi , m âˆ’i , c â‰¥ E Ïƒ(Â¬mi ,âˆ’
mâˆ’i ) vi,M (Â¬mi , c) âˆ’ ri Â¬mi , m âˆ’i , c .
â†’
âˆ’
It follows from inspection that Kc,Ï„Aâˆ’
is a function of Î» only. By convention Kc,Ï„Aâˆ’
is set to
â†’
â†’
m
m
âˆ’
+âˆ whenever message profile â†’
m is not implementable. Given a corruption decision c, the
âˆ’
agent chooses to induce the message profile â†’
m solving
n hâ†’
i
âˆ’
âˆ’ o
Ï„A â†’
âˆ’
â†’
âˆ’
â†’
Ïƒ0 max
E
Î»
v
(c)
âˆ’
K
(
Î») .
âˆ’
â†’
m(Ï„M ) A
c,m
âˆ’
â†’
m

32

â†’
âˆ’
It follows that the optimal message induced by the agent is a function of Î» only, and
therefore remains constant along rays.
We now turn to point (ii). An agent chooses to be non-corrupt if and only if
n hâ†’
i
n hâ†’
i
âˆ’
âˆ’ o
âˆ’
âˆ’ o
Ï„A â†’
Ï„A â†’
âˆ’
â†’
âˆ’
â†’
âˆ’
â†’
âˆ’
â†’
Ï€A + Ïƒ0 max
E
Î»
v
(1)
âˆ’
K
(
Î»
)
â‰¤
Ïƒ
max
E
Î»
v
(0)
âˆ’
K
(
Î» ) . (14)
â†’
â†’
0 âˆ’
m(Ï„M ) A
m(Ï„M ) A
1,âˆ’
m
0,âˆ’
m
âˆ’
â†’
â†’
m

m

Since Ï€A â‰¥ 0 it follows that whenever (14) holds for Ïƒ0 , it must also hold for all Ïƒ00 â‰¥ Ïƒ0 .
This proves point (ii).
An implication is that changes in reporting patterns along a ray can be assigned to
â†’
âˆ’
â†’
âˆ’
â†’
âˆ’
changes in corruption. Consider two policies Ïƒ O , Ïƒ N such that Î» O = Î» N = Î» and
âˆ’
m âˆˆ {0, 1}L 7â†’ x âˆˆ Rn computing a summary statisÏƒ O < Ïƒ N . For any function X : â†’
0

0

tic of messages, denote by Âµ
bÏƒX the distribution over x âˆˆ X({0, 1}L ) defined by Âµ
bÏƒX (x) =
R
âˆ’
â†’âˆ—
1 âˆ’
dÂµT (Ï„ ), where â†’
m âˆ— (Ïƒ, Ï„ ) is the equilibrium vector of messages for a realT X( m (Ïƒ,Ï„ ))=x
âˆ’
ized profile of types Ï„ = (Ï„ , â†’
Ï„ ) given intervention policy Ïƒ. Given policies Ïƒ O , Ïƒ N ,
A

M

let D denote the distance between message distributions induced by Ïƒ O and Ïƒ N defined by
P
N
O
D â‰¡ 12 xâˆˆX({0,1}L ) |ÂµÏƒX (x) âˆ’ ÂµÏƒX (x)|. Note that D can be computed from message data
alone. Proposition 4 extends as follows.
Proposition A.2 (inference). For all possible true distributions ÂµT , we have that
Z




câˆ— (Ïƒ O , Ï„A ) âˆ’ câˆ— (Ïƒ N , Ï„A ) dÂµT (Ï„ ) â‰¥ D

T

which implies that D is a lower bound for the mass

R

at policy Ïƒ N as well as a lower bound for the mass

R

T

[1 âˆ’ câˆ— (Ïƒ N , Ï„A )]dÂµT (Ï„ ) of honest agents

T

câˆ— (Ïƒ O , Ï„A )dÂµT (Ï„ ) of corrupt agents at

policy Ïƒ O .
Proof. The proof is essentially identical to that of Proposition 4. From Proposition A.1, it

33

follows that
Z
T

âˆ—

âˆ—

O

Z

N

[c (Ïƒ , Ï„A )âˆ’c (Ïƒ , Ï„A )]dÂµT (Ï„ ) â‰¥

Z
â‰¥
max 1RÏ„ 1
TA xâˆˆX

â‰¥

1
2

Z

TA xâˆˆX

Ï„M

âˆ’
âˆ’âˆ’
â†’âˆ— (Ïƒ N ) dÂµT (Ï„A )
âˆ’
âˆ’âˆ’
â†’âˆ— (Ïƒ O )6=m
1m
Ï„
Ï„
A

A



âˆ’
âˆ’âˆ’
â†’
âˆ’
âˆ’
â†’
X m
Ï„A âˆ— (Ïƒ O ,Ï„M ) =x

M

X Z

T

(

)

R
dÂµT (Ï„M |Ï„A )6= Ï„

M

1

dÂµT (Ï„M |Ï„A ) âˆ’
1X ( âˆ’
âˆ’âˆ’
â†’âˆ— (Ïƒ O ,âˆ’
Ï„â†’
m
Ï„
M ))=x
A

dÂµT (Ï„M |Ï„A )
âˆ’
âˆ’âˆ’
â†’
âˆ’
âˆ’
â†’
X m
Ï„A âˆ— (Ïƒ N ,Ï„M ) =x

(
Z

Ï„M

)

dÂµT (Ï„A )

dÂµT (Ï„M |Ï„A ) dÂµT (Ï„A )
1X (âˆ’
âˆ’âˆ’
â†’âˆ— (Ïƒ N ,âˆ’
Ï„â†’
m
Ï„
M ))=x
A

â‰¥D

which concludes the proof.

A.3

Retaliation and Side Payments

The paper assumes that the agent uses only retaliation to provide incentives to the monitor. It is immediate that the analysis of Section 4 can be extended to allow for sidepayments (modeled as r(z) < 0), provided that there are no rewards given conditional on
no-intervention, i.e. provided that r(z = âˆ…) = 0.
We now provide sufficient conditions for this to be true in the general framework of Section
3, even if we allow for retaliation as well as side payments, i.e. r âˆˆ R. The cost of retaliation
kA (Â·) â‰¥ 0 is extended over R. For simplicity, we assume that kA is everywhere differentiable,
except at r = 0, where there is a kink: kA0 (0âˆ’ ) < 0 â‰¤ kA0 (0+ ). Recall that state z = âˆ… occurs
with probability 1 if there is no intervention, and with probability probF (z = âˆ…|c, m) if there
is intervention. Let us define p = min(c,m)âˆˆ{0,1}2 probF (z = âˆ…|c, m). The following holds.
Proposition A.3. Whenever
âˆ’p Ã— sup kA0 (r) > (1 âˆ’ p) Ã— sup kA0 (r) > 0,
r<0

(15)

r>0

for any intervention profile Ïƒ and any type Ï„A , the agentâ€™s optimal retaliation strategy is such
that r(âˆ…) = 0.
Whenever the marginal cost of retaliation is low and the probability of intervention
yielding additional information is low, it is optimal for the agent never to give out rewards
34

when there are no observed consequences, i.e. z = âˆ…. Note that it may still be optimal for
the agent to give out rewards, for instance if he gets a particularly informative signal z that
the monitor sent message m = 0.
Proof. By an argument identical to that of Lemma B.2 (see Appendix B), it follows that
at any optimal retaliation profile, r(âˆ…) â‰¤ 0. Assume that r(âˆ…) < 0. We show that for
 > 0 small enough, it is welfare improving for the agent to reduce rewards by  conditional
on z = âˆ…, and increase retaliation by  at all states z 6= âˆ…, i.e. to use retaliation policy
r (Â·) â‰¡ r(Â·) + .
We first show that this change in retaliation policy induces the same messages from
monitors. This is immediate since payoffs have been shifted by a constant: for all m âˆˆ {0, 1},
we have
âˆ’(1 âˆ’ Ïƒm )r (âˆ…) + Ïƒm [vM (c, m) âˆ’ E(r |i = 1, m, c)] = âˆ’(1 âˆ’ Ïƒm )r(âˆ…) + Ïƒm [vM (c, m) âˆ’ E(r|i = 1, m, c)] âˆ’ ,

which implies that the monitorâ€™s IC constraints are unchanged, and retaliation profile r
induces the same message profile as r.
We now show that using r rather than r reduces the agentâ€™s expected retaliation costs.
Indeed, the change in the agentâ€™s retaliation costs is given by
ZZ





âˆ—

[kA (r (z)) âˆ’ kA (r(z))]f (z|m (Ï„M ), c)dzdÎ¦A (Ï„M ) â‰¤ 
TM Ã—Z

0
p sup kA
(r)
r<0



+ (1 âˆ’

0
p) sup kA
(r)
r>0

<0

where we used condition (15). This implies that it is not optimal for the agent to choose a retaliation
strategy such that r(âˆ…) < 0.

A.4

An Anecdote

A basic implication from our paper is that a strictly positive baseline rate of intervention
Ïƒ0 > 0 is needed to ensure that information will flow from the monitor to the principal. This
provides the monitor with plausible deniability should her message lead to an intervention,
which makes incentive provision by the agent harder.

35

We use anecdotal evidence from recent changes in British accounting-oversight policy to
provide a plausible illustration of how this mechanism may play out in practice.28 We emphasize that the goal here is only to describe the trade-off identified in Fact 1 and Proposition
1 sufficiently realistically that it can be used to rationalize existing data. This, however, is
merely a suggestive anecdote and there are clearly alternative interpretations of the data we
discuss.29
Between 2004 and 2005 the UKâ€™s Financial Reporting Review Panel â€” the regulatory
authority in charge of investigating the accounts of publicly owned firms â€” radically changed
its investigation policy. It moved from a purely reactive policy â€” in which investigations
were only conducted in response to complaints filed by credible agents (in our terminology,
Ïƒ0 = 0) â€” to a proactive policy, under which a significant number of firms were investigated
each year regardless of whether complaints were filed or not (i.e. Ïƒ0 > 0); credible complaints
continuing to be investigated as before (Financial Reporting Council, 2004). The change in
the number of complaints is large, going from an average of 4 a year in the period from 1999
to 2004, to an average of 50 a year in the period from 2005 to 2011.30
This striking pattern can be mapped to our framework as follows. The natural monitor
of a firmâ€™s aggregate accounting behavior is the firmâ€™s own auditor. Under a purely reactive
system, following intervention, the firm knows that its auditor must have reported it. Of
course, this puts the auditor in a difficult position, and is likely to disrupt future business.
In contrast, under a proactive system, baseline intervention rates give the auditor plausible deniability should its client be investigated, thereby limiting the damages to long-run
relationships. As a consequence, proactive investigations result in higher rates of complaints.
28

We are grateful to Hans Christensen for suggesting this example.
In fact, concurrent changes make this example unsuitable for proper identification. For instance, over a
time period covering the data we bring up, accounting standards were being unified across Europe.
30
The data is obtained from Brown and Tarca (2007) for years 1999 to 2004, and from the Financial
Reporting Review Panel (2005â€“2011) for years 2005 to 2011.
29

36

80

proactive investigations
(180 to 270 per year)

0

20

complaints
40
60

no proactive investigations

2000

A.5

2005

2010

Short-run inference

The analysis of Section 4 emphasized inference in equilibrium. We now study inference under
a partial equilibrium in which the monitor adjusts her behavior, while the corruption and
retaliation decisions of the agent remain fixed. It is plausible that this partial equilibrium
model may be better suited to interpret data collected in the short-run.
We assume that corruption, retaliation, and reporting policies (cO , rO , mO ) under policy
Ïƒ O are at equilibrium. Under the new policy Ïƒ N , we consider the short-run partial equilibrium in which the agentâ€™s behavior is kept constant equal to cO , rO , while the monitorâ€™s
O O
N
reporting strategy mN
SR is a best-reply to c , r under the new policy Ïƒ .

We first note that in the short run, the policy experiment considered in Section 4 is
uninformative.
Fact A.2 (no short-run inferences). Consider policies Ïƒ O and Ïƒ N such that Ïƒ N = ÏÏƒ O with
Ï > 1. In the short-run equilibrium, message patterns are not affected by new policy Ïƒ N :
Z
âˆ€Ï„ âˆˆ T,

Z

O

m (Ï„ )dÂµT (Ï„ ) =
T

mN
SR (Ï„ )dÂµT (Ï„ ).

T

Proof. The result follows from the fact that given a retaliation strategy, the monitorâ€™s report37

ing decision, described by (9), depends only on the likelihood ratio of intervention rates.
This is not necessarily a negative result: it can serve as a test of whether the short-run
or long-run equilibrium is most suited for analysis. It also implies that the bounds given in
Proposition 4 remain valid if players play a mixture of long-run and short-run equilibria. We
now show that under additional assumptions, other experimental variation may be used to
extract useful information from short-run data. We first describe variation useful to place
bounds on unreported corruption.
Proposition A.4 (a lower bound on unreported corruption). Consider policies Ïƒ O and Ïƒ N
such that Ïƒ0O < Ïƒ0N and Ïƒ1O = Ïƒ1N . Under the assumption that there are no malicious monitors
and agents know it, we have that
Z

O

Z

O

c (Ï„A )[1 âˆ’ m (Ï„ )]dÂµT (Ï„ ) â‰¥

O
[mN
SR (Ï„ ) âˆ’ m (Ï„ )]dÂµT (Ï„ ).

T

T

In words, the increase in reports is a lower bound for the amount of unreported corruption.
Proof. The fact that there are no malicious monitors and the agent knows it implies that
conditional on being non-corrupt, i.e. choosing c = 0, the agent never threatens to retaliate,
i.e. r(Â·) = 0. In addition, since there are no malicious monitors, it must be that mN
SR (Ï„ ) = 1
O
implies cO (Ï„A ) = 1. As a consequence, whenever mN
SR (Ï„ ) âˆ’ m (Ï„ ) > 0, it must be that
O
mO (Ï„ ) = 0 and cO (Ï„A ) = 1. Therefore cO (Ï„A )(1 âˆ’ mO (Ï„ )) â‰¥ mN
SR (Ï„ ) âˆ’ m (Ï„ ). This

concludes the proof.
We now describe variation allowing to obtain a bound on the number of malicious monitors.
Proposition A.5 (a lower bound on the mass of malicious monitors). Assume that there
are no leaks, i.e. f (z|c = 1, m) = f (z|c = 1). Consider policies Ïƒ O and Ïƒ N such that
Ïƒ0O = Ïƒ0N

and

Ïƒ1O < Ïƒ1N . We have that
Z

Z
1vM (c=0,m=1)>0 dÂµT (Ï„ ) â‰¥

T

T

38

 N

mSR (Ï„ ) âˆ’ mO (Ï„ ) dÂµT (Ï„ ).

O
N
O
Proof. Whenever mN
SR (Ï„ ) âˆ’ m (Ï„ ) > 0, it must be that mSR (Ï„ ) = 1 and m (Ï„ ) = 0. We

show that this can only occur when c = 0 and vM (c = 0, m = 1) > 0.
Indeed, consider first the case where c = 0 and vM (c = 0, m = 1) â‰¤ 0. The fact that
mO (Ï„ ) = 0 implies that
Ïƒ1O [vM (c = 0, m = 1) âˆ’ E(r|m = 1, c = 0)] â‰¤ Ïƒ0O (vM (c = 0, m = 0) âˆ’ E[r|m = 0, c = 0]) .
Since in this case vM (c = 0, m = 1) âˆ’ E[r|m = 1, c = 0] â‰¤ 0, the fact that Ïƒ1N > Ïƒ1O and
Ïƒ0N = Ïƒ0O implies
Ïƒ1N [vM (c = 0, m = 1) âˆ’ E(r|m = 1, c = 0)] â‰¤ Ïƒ0N [vM (c = 0, m = 0) âˆ’ E(r|m = 0, c = 0)] .
Hence we also obtain that mN
SR (Ï„ ) = 0.
Consider now the case where c = 1. Using the fact that f (z|c = 1, m = 1) = f (z|c =
1, m = 0), the fact that mO (Ï„ ) = 0 implies that
Ïƒ1O [vM (c = 1, m = 1) âˆ’ E(r|c = 1])] â‰¤ Ïƒ0O (vM (c = 1, m = 0) âˆ’ E[r|c = 1]) .

(16)

By Assumption 2, we have that vM (c = 1, m = 1) â‰¥ vM (c = 1, m = 0). Given that Ïƒ1O > Ïƒ0O ,
condition (16) can only hold if vM (c = 1, m = 1) âˆ’ E[r|c = 1] â‰¤ 0. This implies that
necessarily,
Ïƒ1N [vM (c = 1, m = 1) âˆ’ E(r|c = 1])] â‰¤ Ïƒ0N (vM (c = 1, m = 0) âˆ’ E[r|c = 1]) .
O
We have now established that mN
SR (Ï„ ) âˆ’ m (Ï„ ) > 0 implies vM (c = 0, m = 1) > 0.

Proposition A.5 follows by integration over Ï„ âˆˆ T .

39

B

Proofs

B.1

Proofs for Section 2

We begin by establishing the simplifying claims made in Section 2.
Lemma B.1. It is without loss of efficiency for the principal to: (i) not elicit messages from
the agent; (ii) offer the monitor only binary messages 0, 1; (iii) use an intervention policy
satisfying Ïƒ0 â‰¤ Ïƒ1 .
Proof. We begin by showing point (i): it is without loss of efficiency not to elicit messages
from the agent. The agent has commitment power and therefore can commit to the messages
he sends. When the agent sends a message, we can think of him as choosing the intervention
profile Ïƒ that he will be facing, as well as the messages sent by the monitor. If a non-corrupt
agent chooses intervention profile Ïƒ, then giving additional choices can only increase the
payoffs of a corrupt agent. Hence the principal can implement the same outcome by offering
only the profile Ïƒ chosen by a non-corrupt agent.
We now turn to point (ii) and consider enlarging the set of messages submitted by the
monitor. The monitor observes only two pieces of information: the corruption status c âˆˆ
{0, 1} of the agent, and the level of retaliation r âˆˆ R that he is threatened with in the event
of intervention. A priori, the principal may elicit messages (m, Ï) âˆˆ {0, 1} Ã— [0, +âˆ) about
both the corruption status of the agent and the retaliation level she has been threatened
with. This means that intervention rates now take the form Ïƒm,Ï âˆˆ [0, 1].
Take as given an intervention profile Ïƒ = (Ïƒm,Ï )mâˆˆ{0,1},Ïâˆˆ[0,+âˆ) . First, note that we can
focus on the case where the agentâ€™s optimal decision is to be non-corrupt, otherwise nointervention is the optimal policy. Second, noting that the value of Ï submitted by the
monitor must solve maxÏâˆˆ[0,+âˆ) Ïƒm,Ï (vM (c, m) âˆ’ r) it follows that without loss of generality
one can focus on binary values of Ï âˆˆ {âˆ’, +} such that Ïƒm,âˆ’ = inf Ïâˆˆ[0,+âˆ) Ïƒm,Ï and Ïƒm,+ =
supÏâˆˆ[0,+âˆ) Ïƒm,Ï .31 Finally, without loss of efficiency, one can consider intervention profiles
31

When the monitor is indifferent, she must be inducing the lowest possible intervention rate, otherwise
the agent would increase retaliation by an arbitrarily small amount.

40

such that for all Ï âˆˆ {âˆ’, +}, Ïƒ0,Ï â‰¤ Ïƒ1,Ï . Indeed, given Ï, define Ïƒ = maxmâˆˆ{0,1} Ïƒm,Ï and
Ïƒ = minmâˆˆ{0,1} Ïƒm,Ï , as well as m and m the corresponding messages. Given Ï, the level of
retaliation needed to induce Ïƒ rather than Ïƒ must satisfy


ÏƒvM (c, m) âˆ’ ÏƒvM (c, m)
Ïƒ(vM (c, m) âˆ’ r) â‰¤ Ïƒ(vM (c, m) âˆ’ r) â‡â‡’ r â‰¥
Ïƒâˆ’Ïƒ

+
.

setting m = 1 and m = 0 maximizes the cost of inducing Ïƒ for the corrupt agent and
minimizes the cost of inducing Ïƒ for the non corrupt agent. Note that this proves point (iii).
Given a profile Ïƒ satisfying the properties established above, we now establish the existence of a binary intervention profile Ïƒ
b = (b
Ïƒm )mâˆˆ{0,1} which keeps the payoff of a non-corrupt
agent the same and can only decrease the payoff of a corrupt agent. Specifically set Ïƒ
b0 = Ïƒ0,âˆ’
and set Ïƒ
b1 as the intervention rate that would occur under Ïƒ if a corrupt agent chose retaliation level r = 0. First note that the assumption that the monitor is not malicious implies
that a non-corrupt agent will induce intervention rate Ïƒ0,âˆ’ without using retaliation under
both Ïƒ and Ïƒ
b. Hence the payoff of a non-corrupt agent remains unchanged, and the equilibrium intervention rate remains the same in both settings. Consider now the problem of
the corrupt agent under Ïƒ
b. The respective costs of inducing intervention rates Ïƒ0,âˆ’ and Ïƒ
b1
havenâ€™t changed. However the agent now has less choice regarding the intervention rates she
can induce. It follows that the corrupt agent must be weakly worse-off. Hence profile Ïƒ
b also
induces the agent to be non-corrupt. This concludes the proof.

Proof of Fact 1: We begin with point (i). Note that 0 is the highest payoff the principal
can attain. Under intervention policy Ïƒ0 = 0, Ïƒ1 = 1, Assumption 1 implies that it is optimal
for the agent to choose c = 0. As a result, there will be no intervention on the equilibrium
path. Hence the principal attains her highest possible payoff, and Ïƒ0 = 0, Ïƒ1 = 1 is indeed
an optimal intervention policy.
Let us turn to point (ii). Consider policies Ïƒ such that

Ïƒ1
Ïƒ0

> 2 and the retaliation profile

under which the agent retaliates by an amount r â‰¡ 2vM (c = 1, m = 1) âˆ’ vM (c = 1, m = 0).

41

Retaliation level r is chosen so that whenever the agent is corrupt, the monitor prefers to
send message m = 0. Indeed, the monitor prefers to send message m = 0 if and only if
Ïƒ1 [vM (c = 1, m = 1) âˆ’ r] â‰¥ Ïƒ0 [vM (c = 1, m = 0) âˆ’ r]
â‡â‡’ r â‰¥
where Î» =

Ïƒ1
.
Ïƒ0

Î»vM (c = 1, m = 1) âˆ’ vM (c = 1, m = 0)
Î»âˆ’1

(17)

Noting that the right-hand side of (17) is decreasing in Î» and that Î» > 2, we

obtain that the monitor indeed sends message m whenever r â‰¥ 2vM (c = 1, m = 1) âˆ’ vM (c =
1, m = 0).
It follows that a corrupt agentâ€™s expected payoff under this retaliation strategy is
1
Ï€A + Ïƒ0 [vA (c = 1) âˆ’ kA (r)] â‰¥ Ï€A + [vA (c = 1) âˆ’ kA (r)].
Î»
Since Ï€A > 0, it follows that this strategy guarantees the agent a strictly positive payoff for
Î» sufficiently large. Given that the highest possible payoff for an agent choosing c = 0 is
equal to 0, it follows that for Î» large enough the agent will be corrupt.
Given corruption, we now show that the agent will also use retaliation. Under no retaliation the agent obtains an expected payoff equal to Ï€A + Ïƒ1 vA (c = 1). Under the retaliation
strategy described above, the agent obtains a payoff equal to Ï€A +

Ïƒ1
[vA (c
Î»

= 1) âˆ’ kA (r)].

Since vA (c = 1) < 0 it follows that for Î» large enough, it is optimal for the agent to commit
to retaliation.



Proof of Fact 2:

Recall that Î» =

Ïƒ1
.
Ïƒ0

As shown in the text, the corrupt agent induces

message m = 0 if and only if (2) holds, i.e. if
Î»vA (c = 1) â‰¤ vA (c = 1) âˆ’ kA (rÎ» ).
From the fact that rÎ» is decreasing in Î» and vA (c = 1) < 0, it follows that there exists Î»0
such that (2) holds if and only if Î» > Î»0 .


42

Proof of Fact 3: By Assumption 1, the optimal intervention profile must discourage corruption in equilibrium (Ïƒ0 = Ïƒ1 = 1 guarantees no corruption and is preferred to corruption
in spite of high intervention costs). Since there wonâ€™t be corruption in equilibrium, the equilibrium rate of intervention is Ïƒ0 . The principalâ€™s problem is therefore to find the smallest
value of Ïƒ0 for which there exists Ïƒ1 â‰¥ Ïƒ0 satisfying
Ï€A + max{Ïƒ1 vA (c = 1), Ïƒ0 [vA (c = 1) âˆ’ kA (rÎ» )]} â‰¤ Ïƒ0 vA (c = 0).

(18)

Let us first show that at the optimal policy, it must be that Ïƒ1 vA (c = 1) = Ïƒ0 [vA (c =
1) âˆ’ kA (rÎ» )]. Indeed, if we had Ïƒ1 vA (c = 1) > Ïƒ0 [vA (c = 1) âˆ’ kA (rÎ» )], then one could
decrease Ïƒ0 while still satisfying (18), which contradicts optimality. If instead we had that
Ïƒ1 vA (c = 1) < Ïƒ0 [vA (c = 1) âˆ’ kA (rÎ» )], then diminishing Ïƒ1 increases rÎ» which allows to
diminish Ïƒ0 while still satisfying (18). Hence it must be that Ïƒ1 vA (c = 1) = Ïƒ0 [vA (c =
1) âˆ’ kA (rÎ» )]. By definition of Î»0 , this implies that Ïƒ1 = Î»0 Ïƒ0 .
Hence (18) implies that Ï€A + Ïƒ1 vA (c = 1) â‰¤ Ïƒ0 vA (c = 0). Furthermore this last inequality
must be an equality, otherwise one would again be able to diminish the value of Ïƒ0 while
satisfying (18). This implies that Ï€A + Ïƒ1 vA (c = 1) = Ïƒ0 vA (c = 0). This proves the first part
of Fact 3.
We now show that this optimal policy is necessarily interior. We know that Ïƒ0 âˆˆ (0, 1)
from Fact 1 and the assumption that Ï€A + vA (c = 1) < vA (c = 0). Let us show that Ïƒ1 < 1.
The first part of Fact 3 allows us to compute Ïƒ1 explicitly as
Ïƒ1 =

Ï€A
1
Ï€A
1
â‰¤
v
(c=0)
v
âˆ’vA (c = 1) 1 âˆ’ A
âˆ’vA (c = 1) 1 âˆ’ A (c=0)
Î»0 vA (c=1)

vA (c=1)

Ï€A
< 1,
â‰¤
âˆ’vA (c = 1) + vA (c = 0)
where the last inequality uses the assumption that Ï€A + vA (c = 1) < vA (c = 0). This concludes the proof of Fact 3.



43

Proof of Fact 4:

Condition (5) follows from the proof the fact that since there are no

malicious monitors, an non-corrupt agent can induce message m = 0 at no retaliation cost.
Let us now show that if cN = 1 then cO = 1. It follows from (5) that mN = 1 implies
cN = 1. Let us define Î»N â‰¡

Ïƒ1N
Ïƒ0N

and Î»O â‰¡

Ïƒ1O
.
Ïƒ0O

Assume that cN = 1. Since corruption is

optimal for the agent at Ïƒ N , we obtain that
Ï€A + max{Ïƒ1N vA (c = 1), Ïƒ0N [vA (c = 0) âˆ’ kA (rÎ»N )]} â‰¥ 0.
Since Î»N < Î»O , rÎ» is decreasing in Î», vA (Â·) â‰¤ 0 and Ïƒ N > Ïƒ O for the usual vector order, we
obtain that
Ï€A + max{Ïƒ1O vA (c = 1), Ïƒ0O [vA (c = 0) âˆ’ kA (rÎ»O )]} â‰¥ 0.
Hence, it must be optimal for the agent to be corrupt at Ïƒ O : cO = 1.
Finally, we prove (7). Since mO = 1, we know that cO = 1. Since the agent chooses not
to induce message m = 0 at Ïƒ O , it must be that Î»O â‰¤ Î»0 , where Î»0 was defined in Fact
2. Since Î»N < Î»O , it follows from point (i) above that a corrupt agent would not induce
message m = 0 at Ïƒ N . Hence, it must be that cN = 0.



Proof of Fact 5: Fact 4 implies that any profile Ïƒ N satisfying the condition in (8) is such
that c(Ïƒ N ) = 0.
We now show that there exists a sequence of intervention profiles converging to optimal
policy Ïƒ âˆ— that satisfies the conditions in (8). We know from Fact 3 that policy Ïƒ âˆ— satisfies
mâˆ— (Ïƒ âˆ— ) = 0 and Ïƒ1âˆ— = Î»0 Ïƒ0âˆ— . Consider sequences (ÏƒnO )nâˆˆN and (ÏƒnN )nâˆˆN such that
N
Ïƒ0,n
N
Ïƒ1,n


1
= 1+
Ïƒ0âˆ— ,
n


1
N
= Î»0 1 âˆ’
Ïƒ0,n
,
n


O
Ïƒ0,n
O
Ïƒ1,n

44




1
= 1âˆ’
Ïƒ0âˆ— ,
n


1
O
= Î»0 1 +
Ïƒ0,n
.
n

For n sufficiently large, the pair (ÏƒnO , ÏƒnN ) satisfies the condition in (8), and sequence (ÏƒnN )nâˆˆN
converges to Ïƒ âˆ— . This concludes the proof.

B.2



Proofs for Section 4

Proof of Proposition 1:

Consider the case where the monitor is an automaton sending

exogenously informative messages m(c) = c. We show that it is optimal to set Ïƒ0 = 0.
Since messages are exogenous, it is optimal for the agent not to engage in retaliation
regardless of his type. Therefore the agent will be corrupt if and only if
Ï€A + Ïƒ1 vA (c = 1) â‰¥ Ïƒ0 vA (c = 0).
Hence we obtain that the principalâ€™s payoff is
Z

Z
1Ï€A +Ïƒ1 vA (c=1)â‰¤Ïƒ0 vA (c=0) Ïƒ0 vP (c = 0)dÂµT +

T

1Ï€A +Ïƒ1 vA (c=1)>Ïƒ0 vA (c=0) [Ï€P + vP (c = 1)Ïƒ1 ]dÂµT
ZT

â‰¤

1Ï€A +Ïƒ1 vA (c=1)>Ïƒ0 vA (c=0) [Ï€P + vP (c = 1)Ïƒ1 ]dÂµT ,
T

where we used the assumption that vA (c) â‰¤ 0 for all c âˆˆ {0, 1}, and Ï€P < 0. Hence it follows
that setting Ïƒ0 = 0 is optimal for the principal when messages are exogenously informative.
We now consider the case where messages are endogenous. A proof identical to that of
Fact 1 shows that whenever Ï€A > 0 for Î» sufficiently high, câˆ— (Ïƒ, Ï„A ) = 1. Hence by dominated
convergence, it follows that
Z
lim

Î»â†’âˆ

T

câˆ— (Ïƒ, Ï„A )dÂµT (Ï„ ) â‰¥ probÂµT (Ï€A > 0).

We now show that for all types Ï„A such that vA (Â·) < 0, the agent will induce the monitor
to send message m = 0. The proof is by contradiction. Consider an agent of type Ï„A and

45

assume that there exists  > 0 such that for Î» arbitrarily large,
Z

mâˆ— (Ïƒ, Ï„ )dÎ¦(Ï„M |Ï„A ) > .

TM

This implies that given a corruption decision c, the agentâ€™s payoff is bounded above by


Z



âˆ—

m (Ïƒ, Ï„ )dÎ¦(Ï„M |Ï„A ) vA (c) < Ï€A Ã— c + Ïƒ0 [1 + (Î» âˆ’ 1)]vA (c).

Ï€A Ã— c + Ïƒ0 + (Ïƒ1 âˆ’ Ïƒ0 )
TM

Consider the alternative strategy in which the agent chooses corruption status c but commits
to retaliate with intensity
r=

sup

[2vM (c, m = 1) âˆ’ vM (c, m = 0)]

vM âˆˆsupp Î¦(Â·|Ï„A )

1
minm,c probF (z 6= âˆ…|m, c)

whenever z 6= âˆ…. This retaliation strategy ensures that all types Ï„M in the support of Î¦(Â·|Ï„A )
choose to send message m = 0. Under this strategy the agent obtains a payoff greater than
Ï€A Ã— c + Ïƒ0 [vA (c) âˆ’ kA (r)].
For Î» sufficiently large that (Î» âˆ’ 1)vA (c) â‰¥ kA (r), this contradicts the hypothesis that
mâˆ— is an optimal message manipulation strategy for the agent. Hence it must be that
R
limÎ»â†’âˆ TM mâˆ— (Ïƒ, Ï„ )dÎ¦(Ï„M |Ï„A ) = 0. This concludes the proof of Proposition 1. 

Lemma B.2. For any corruption decision c, it is optimal for the agent to retaliate only
conditional on intervention: for any intervention policy Ïƒ, the agentâ€™s optimal retaliation
policy is such that r(âˆ…) = 0.
Proof of Lemma B.2:

Taking a corruption decision c as given, the agentâ€™s expected

46

payoff under an optimal retaliation profile r : Z â†’ [0, +âˆ) is
Ï€A Ã— c + prob(m = 0|r, c, Ïƒ)Ïƒ0 [vA (c) âˆ’ E(kA (r)|m = 0, c)]
+ prob(m = 1|r, c, Ïƒ)Ïƒ1 [vA (c) âˆ’ E(kA (r)|m = 1, c)].
Therefore, if it is optimal for the agent to engage in a positive amount of retaliation, it must
be that
Ïƒ0 [vA (c) âˆ’ E(kA (r)|m = 0, c)] â‰¥ Ïƒ1 [vA (c) âˆ’ E(kA (r)|m = 1, c)],
since otherwise, no retaliation would guarantee the agent a greater payoff. We now show
that setting r(âˆ…) to 0 increases the probability with which the monitor sends message m = 0.
Since it also reduces the cost of retaliation, it must increase the agentâ€™s payoff.
A monitor sends a message m = 0 if and only if
âˆ’(1 âˆ’ Ïƒ0 )r(âˆ…) + Ïƒ0 [vM (c, m = 0) âˆ’ E(r|m = 0, z 6= âˆ…, c)probF (z 6= âˆ…|m = 0, c)

(19)

âˆ’ r(âˆ…)prob(z = âˆ…|m = 1, c)]
â‰¥ âˆ’(1 âˆ’ Ïƒ1 )r(âˆ…) + Ïƒ1 [vM (c, m = 1) âˆ’ E(r|m = 1, z 6= âˆ…, c)probF (z 6= âˆ…|m = 1, c)
âˆ’ r(âˆ…)prob(z = âˆ…|m = 1, c)].
Since Ïƒ1 â‰¥ Ïƒ0 and, by assumption, probF (z 6= âˆ…|m = 1, c) â‰¥ probF (z 6= âˆ…|m = 0, c), it
follows that whenever (19) holds for a retaliation profile such that r(âˆ…) > 0, it continues
to hold when r(âˆ…) is set to 0, everything else being kept equal. Hence optimal retaliation
profiles are such that r(âˆ…) = 0.

Proof of Proposition 2:



We begin with point (i). We know from Section 4 that the

agentâ€™s payoff conditional on a corruption decision c and a message profile m can be written
as
Z
Ï€ A Ã— c + Ïƒ0

m(Ï„M )

Î»



vA (c)dÎ¦(Ï„M |Ï„A ) âˆ’

Ï„A
Kc,m
(Î»)

.

TM

It follows that given a corruption decision c, the agent induces a message profile m that
47

solves
Z

Ï„A
Î»m(Ï„M ) vA (c)dÎ¦(Ï„M |Ï„A ) âˆ’ Kc,m
(Î»).

max

mâˆˆM

TM

Since this problem depends only on ratio Î» =

Ïƒ1
,
Ïƒ0

it follows that mO = mN .

Let us turn to point (ii). Assume that it is optimal for the agent to take decision c = 0
at intervention profile Ïƒ. It must be that
Z

m(Ï„M )

Ï€ A + Ïƒ0

Î»



vA (c = 1)dÎ¦(Ï„M |Ï„A ) âˆ’

Ï„A
Kc=1,m
(Î»)

TM

Z
â‰¤ Ïƒ0

m(Ï„M )

Î»



vA (c = 0)dÎ¦(Ï„M |Ï„A ) âˆ’

Ï„A
Kc=0,m
(Î»)

.

TM

Since Ï€A â‰¥ 0, this implies that
Z

m(Ï„M )

Î»

Z

vA (c =

Ï„A
0)dÎ¦(Ï„M |Ï„A )âˆ’Kc=0,m
(Î»)âˆ’

m(Ï„M )

Î»



vA (c = 1)dÎ¦(Ï„M |Ï„A ) âˆ’

Ï„A
Kc=1,m
(Î»)

TM

TM

which implies that keeping Î» constant
Ï€A +

Ïƒ00

Z

m(Ï„M )

Î»



vA (c = 1)dÎ¦(Ï„M |Ï„A ) âˆ’

Ï„A
(Î»)
Kc=1,m

TM

â‰¤

Ïƒ00

Z

m(Ï„M )

Î»



vA (c = 0)dÎ¦(Ï„M |Ï„A ) âˆ’

Ï„A
(Î»)
Kc=0,m

.

TM

for any Ïƒ00 â‰¥ Ïƒ0 . This implies that the agent will choose not to be corrupt at any profile ÏÏƒ,
with Ï > 1.
Point (iii) follows from point (ii). For any Ïƒ O , Ïƒ N such that Ïƒ N = ÏÏƒ O with Ï > 1, we
have that for all types Ï„A âˆˆ TA , câˆ— (Ïƒ O , Ï„A ) â‰¥ câˆ— (Ïƒ N , Ï„A ). Integrating against ÂµT yields point
(iii).



R
Fix Ïƒ and a distribution ÂµT such that T mâˆ— (Ïƒ, Ï„ )dÂµT (Ï„ ) =
R
M âˆˆ [0, 1]. Fix C âˆˆ [0, 1]. We show that there exists Âµ
bT such that T mâˆ— (Ïƒ, Ï„ )db
ÂµT (Ï„ ) = M
R âˆ—
and T c (Ïƒ, Ï„A )dÂµT (Ï„ ) = C.

Proof of Proposition 3:

It is sufficient to work with type spaces such that the agent knows the type of the monitor,
48

â‰¥ 0,

provided we allow payoffs to be correlated. A possible environment is as follows. The agent
observes intervention and no other signal. With probability C, the agent gets a strictly
positive payoff Ï€A > 0 from corruption. Conditional on Ï€A > 0, with probability Î±, the
monitor has positive value for intervention against corrupt agents, i.e. vM (c = 1, m) = v >
0 = vM (c = 0, m); with probability 1 âˆ’ Î±, the monitor has a low value for intervention on
corrupt agents: vM (c, m) = 0 for all (c, m) âˆˆ {0, 1}2 . The cost of retaliation for the agent is
such that kA is convex and strictly increasing. For vA (c = 1) > 0 appropriately low, it will
be optimal for the agent to be corrupt, and commit to an arbitrarily low retaliation profile
so that the monitor with a low value for intervention sends message m = 0 and the monitor
with a high value for intervention sends message m = 1.
With complementary probability 1 âˆ’ C the agent gets a payoff Ï€A = 0 from corruption
and has an arbitrarily high cost of retaliation. The agentâ€™s values upon intervention are
such that vA (c = 1) < vA (c = 0). With probability Î², the monitor has negative value for
intervention against a non-corrupt agent vM (c = 0, m) < 0. With probability 1 âˆ’ Î² the
monitor gets a positive payoff v > 0 from intervention against the agent, regardless of his
corruption status. For v and a cost of retaliation kA sufficiently high, the agent will choose
not to be corrupt, the non-malicious monitor will send message m = 0, and the malicious
monitor will send message m = 1.
For any C âˆˆ [0, 1] and M âˆˆ [0, 1], one can find Î± and Î² such that CÎ± + (1 âˆ’ C)Î² = M.
This concludes the proof.



Proof of Proposition 4: From Proposition 2 (ii), we obtain that c(Ïƒ O , Ï„A ) âˆ’ c(Ïƒ N , Ï„A ) âˆˆ
{0, 1}. Using Proposition 2 (i), this implies that c(Ïƒ O , Ï„A ) âˆ’ c(Ïƒ N , Ï„A ) â‰¥ |m(Ïƒ O , Ï„ ) âˆ’
m(Ïƒ N , Ï„ )|. Integrating against ÂµT implies that
Z

O

Z

N

[c(Ïƒ O , Ï„A ) âˆ’ c(Ïƒ N , Ï„A )]dÂµT (Ï„A )

|m(Ïƒ , Ï„ ) âˆ’ m(Ïƒ , Ï„ )|dÂµT (Ï„ ) â‰¤
T

Z
â‡’

TA

m(Ïƒ O , Ï„ ) âˆ’ m(Ïƒ N , Ï„ )dÂµT (Ï„ ) â‰¤

T

Z
TA

49

[c(Ïƒ O , Ï„A ) âˆ’ c(Ïƒ N , Ï„A )]dÂµT (Ï„A ).

This concludes the proof.

Proof of Corollary 1:



The first part of the corollary follows directly from Proposition

4. The second part of the corollary follows from Fact 5. Indeed, the strategy profile Ïƒ
b(ÂµT )
coincides with the optimal strategy profile whenever payoffs are complete information and
Assumption 1 holds.



References
Acemoglu, D. and T. Verdier (1998): â€œProperty rights, Corruption and the Allocation
of Talent: a general equilibrium approach,â€ Economic Journal, 108, 1381â€“1403.
â€”â€”â€” (2000): â€œThe Choice between Market Failures and Corruption,â€ American Economic
Review, 194â€“211.
Asker, J. (2010): â€œA study of the internal organization of a bidding cartel,â€ The American
Economic Review, 724â€“762.
Banerjee, A. and E. Duflo (2006): â€œAddressing Absence,â€ Journal of Economic Perspectives, 20, 117â€“132.
Banerjee, A., S. Mullainathan, and R. Hanna (2012): â€œCorruption,â€ in Handbook of
Organizational Economics, ed. by R. Gibbons and J. Roberts, Princeton University Press.
Bertrand, M., S. Djankov, R. Hanna, and S. Mullainathan (2007): â€œObtaining a
driverâ€™s license in India: an experimental approach to studying corruption,â€ The Quarterly
Journal of Economics, 122, 1639â€“1676.
Bester, H. and R. Strausz (2001): â€œContracting with imperfect commitment and the
revelation principle: the single agent case,â€ Econometrica, 69, 1077â€“1098.

50

Brown, P. and A. Tarca (2007): â€œAchieving high quality, comparable financial reporting: A review of independent enforcement bodies in Australia and the United Kingdom,â€
Abacus, 43, 438â€“473.
Callen, M., S. Gulzar, A. Hasanain, and M. Y. Khan (2013): â€œThe political economy of public employee absence: Experimental evidence from Pakistan,â€ Available at
SSRN 2316245.
Callen, M. and A. Hasanain (2013): â€œThe Punjab Model of Proactive Governance:
Empowering Citizens Through Information Communication Technology,â€ .
Carroll, G. (2013): â€œRobustness and Linear Contracts,â€ Stanford University Working
Paper.
Chassang, S. (2013): â€œCalibrated incentive contracts,â€ Econometrica, 81, 1935â€“1971.
Chassang, S., G. PadroÌ i Miquel, and E. Snowberg (2012): â€œSelective Trials: A
Principal-Agent Approach to Randomized Controlled Experiments,â€ American Economic
Review, 102, 1279â€“1309.
Che, Y.-K. and J. Kim (2006): â€œRobustly Collusion-Proof Implementation,â€ Econometrica, 74, 1063â€“1107.
â€”â€”â€” (2009): â€œOptimal collusion-proof auctions,â€ Journal of Economic Theory, 144, 565â€“
603.
Dal BoÌ, E. (2007): â€œBribing voters,â€ American Journal of Political Science, 51, 789â€“803.
Duflo, E., M. Greenstone, R. Pande, and N. Ryan (2013): â€œTruth-telling by Thirdparty Auditors and the Response of Polluting Firms: Experimental Evidence from India*,â€
The Quarterly Journal of Economics, 128, 1499â€“1545.
Duflo, E., R. Hanna, and S. P. Ryan, N. (2012): â€œIncentives work: Getting teachers
to come to school,â€ The American Economic Review, 102, 1241â€“1278.
51

Eeckhout, J., N. Persico, and P. Todd (2010): â€œA theory of optimal random crackdowns,â€ The American Economic Review, 100, 1104â€“1135.
Ensminger, J. (2013): â€œInside Corruption Networks: Following the Money in Community
Driven Development,â€ Unpublished manuscript, Caltech.
Faure-Grimaud, A., J.-J. Laffont, and D. Martimort (2003): â€œCollusion, delegation and supervision with soft information,â€ The Review of Economic Studies, 70, 253â€“279.
Financial Reporting Council (2004): â€œPolicy Update,â€ http://www.frc.org.uk/Newsand-Events/FRC-Press/Press/2004/December/Financial-Reporting-Review-PanelAnnounces-2005-Ri.aspx.
Financial

Reporting

Review

Panel

(2005â€“2011):

â€œAnnual

Report,â€

http://www.frc.org.uk.
Frankel, A. (2014): â€œAligned delegation,â€ The American Economic Review, 104, 66â€“83.
Fudenberg, D. and D. K. Levine (1992): â€œMaintaining a reputation when strategies
are imperfectly observed,â€ The Review of Economic Studies, 59, 561â€“579.
Ghosh, A. and A. Roth (2010):

â€œSelling privacy at auction,â€ Arxiv preprint

arXiv:1011.1375.
Gradwohl, R. (2012): â€œPrivacy in Implementation,â€ .
Hartline, J. D. and T. Roughgarden (2008): â€œOptimal Mechanism Design and Money
Burning,â€ in Symposium on Theory Of Computing (STOC), 75â€“84.
Hurwicz, L. and L. Shapiro (1978): â€œIncentive structures maximizing residual gain
under incomplete information,â€ The Bell Journal of Economics, 9, 180â€“191.
Izmalkov, S., M. Lepinski, and S. Micali (2011): â€œPerfect implementation,â€ Games
and Economic Behavior, 71, 121â€“140.

52

Karlan, D. and J. Zinman (2009): â€œObserving unobservables: Identifying information
asymmetries with a consumer credit field experiment,â€ Econometrica, 77, 1993â€“2008.
Laffont, J. and D. Martimort (1997): â€œCollusion under asymmetric information,â€
Econometrica: Journal of the Econometric Society, 875â€“911.
Laffont, J.-J. and D. Martimort (2000): â€œMechanism design with collusion and correlation,â€ Econometrica, 68, 309â€“342.
MadaraÌsz, K. and A. Prat (2010): â€œScreening with an Approximate Type Space,â€
Working Paper, London School of Economics.
Mauro, P. (1995): â€œCorruption and Growth,â€ Quartely Journal of Economics, 110, 681â€“
712.
Myerson, R. B. (1986): â€œMultistage games with communication,â€ Econometrica: Journal
of the Econometric Society, 323â€“358.
Nissim, K., C. Orlandi, and R. Smorodinsky (2011): â€œPrivacy-aware mechanism
design,â€ Arxiv preprint arXiv:1111.3350.
Olken, B. (2007): â€œMonitoring corruption: evidence from a field experiment in Indonesia,â€
Journal of Political Economy, 115, 200â€“249.
Olken, B. and R. Pande (2012): â€œCoruption in Developing Countries,â€ Annual Review
of Economics, 4, 479â€“505.
Prendergast, C. (2000): â€œInvestigating Corruption,â€ working paper, World Bank development group.
Punch, M. (2009): Police Corruption: Deviance, Accountability and Reform in Policing,
Willan Publishing.
Rahman, D. (2012): â€œBut who will monitor the monitor?â€ The American Economic Review, 102, 2767â€“2797.
53

Resnick, P. and R. Zeckhauser (2002): â€œTrust among strangers in Internet transactions:
Empirical analysis of eBayâ€™s reputation system,â€ Advances in applied microeconomics, 11,
127â€“157.
Segal, I. (2003): â€œOptimal pricing mechanisms with unknown demand,â€ The American
economic review, 93, 509â€“529.
Shleifer, A. and R. W. Vishny (1993): â€œCorruption,â€ Quartely Journal of Economics,
108, 599â€“617.
Skrzypacz, A. and H. Hopenhayn (2004): â€œTacit collusion in repeated auctions,â€ Journal of Economic Theory, 114, 153â€“169.
the Economist (2013): â€œTechnology and government: How the clever use of mobile phones
is helping to improve government services in Pakistan,â€ the Economist, Technology Quarterly, June.
Tirole, J. (1986): â€œHierarchies and bureaucracies: On the role of collusion in organizations,â€ Journal of Law, Economics, and Organizations, 2, 181.
Warner, S. L. (1965): â€œRandomized response: A survey technique for eliminating evasive
answer bias,â€ Journal of the American Statistical Association, 60, 63â€“69.

54

