NBER WORKING PAPER SERIES

INCENTIVE STRENGTH AND TEACHER PRODUCTIVITY:
EVIDENCE FROM A GROUP-BASED TEACHER INCENTIVE PAY SYSTEM
Scott A. Imberman
Michael F. Lovenheim
Working Paper 18439
http://www.nber.org/papers/w18439

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
October 2012

We wish to thank seminar participants at Aarhus University, CESifo, Cornell University, the Institute
for Research on Poverty, NBER Summer Institute, Purdue University, Tilburg University, the University
of Copenhagen, and the University of Houston for helpful comments and suggestions. We gratefully
acknowledge the help and input of Kiel Albrecht, Jack Barron and Steve Coate in the development
of our theoretical model. We further thank Aimee Chin, Steven Craig, Steve Rivkin, Gary Solon and
Lesley Turner for helpful comments and suggestions. Finally, we would like to thank the employees
at Houston Independent School District for their help and assistance. All errors, omissions and conclusions
are our own. Copyright 2012 by Scott Imberman and Michael Lovenheim. The views expressed herein
are those of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2012 by Scott A. Imberman and Michael F. Lovenheim. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.

Incentive Strength and Teacher Productivity: Evidence from a Group-Based Teacher Incentive
Pay System
Scott A. Imberman and Michael F. Lovenheim
NBER Working Paper No. 18439
October 2012
JEL No. H41,I21,J33,J38
ABSTRACT
Using data from a group incentive program that provides cash bonuses to teachers whose students
perform well on standardized tests, we estimate the impact of incentive strength on student achievement.
These awards are based on the performances of students within a grade, school and subject, providing
substantial variation in group size. We use the share of students in a grade-subject enrolled in a teacher's
classes as a proxy for incentive strength since, as the teacher share increases, a teacher's impact on
the probability of award receipt rises. We find that student achievement improves when a teacher becomes
responsible for more students post program implementation: mean effects are between 0.01 and 0.02
standard deviations for a 10 percentage point increase in share for math, English and social studies,
although mean science estimates are small and are not statistically significant. As predicted in our
theoretical model, we also find larger effects at smaller shares that fall towards zero as share increases.
For all four subjects studied, effect sizes start at 0.05 to 0.09 standard deviations for a 10 percentage
point increase in share when share is initially close to zero and fade out as share increases. These findings
suggest that small groups provide productivity gains over large groups. Further, they suggest that the
lack of effects found in US teacher incentive pay experiments probably are in some part due to specific
aspects of program design rather than failure of teachers to respond to incentives more generally.

Scott A. Imberman
Michigan State University
486 W. Circle Drive
110 Marshall-Adams Hall
East Lansing, MI 48824-1038
and NBER
imberman@msu.edu
Michael F. Lovenheim
Department of Policy Analysis and Management
Cornell University
135 Martha Van Rensselaer Hall
Ithaca, NY 14853
and NBER
mfl55@cornell.edu

1

Introduction

Teacher incentive pay has become an increasingly popular policy in the United States. A key
component of many of these incentive schemes, and of performance pay systems in general,
is group incentives. In education, this typically amounts to paying teachers based on gradeor school-speciﬁc performance on standardized exams in a given subject. The literature to
date has found mixed results with respect to the eﬀectiveness of these programs. Lavy (2002)
ﬁnds evidence that school-wide incentives in Israeli schools boost student achievement as does
Muralidharan and Sundararaman (2011) in India. Nonetheless, studies focusing on the United
States generally have found small or negligible eﬀects of group incentive pay on performance
(Fryer, 2011; Springer, et. al., 2010; Sojourner, West and Mykerezi, 2011; Goodman and
Turner, 2011), although Ladd (1999) ﬁnds positive eﬀects of a school-based incentive pay system
implemented in the early 1990s in Dallas and Fryer et al. (2012) ﬁnd eﬀects when there is loss
aversion.
One drawback of providing rewards based on group performance is the “ n1 ” problem (Holmstrom, 1982; Kandel and Lazear, 1992), whereby teachers in larger groups may be less responsive
to ﬁnancial incentives. The “ n1 ” problem comes about because of “free riding,” where workers have an incentive to reduce their eﬀort when others in the group exert more eﬀort, and
because of “award salience” eﬀects, where the incentive to work hard is reduced when group
size increases, which causes each worker to have less of an impact on the likelihood of winning
the award.1 Further, larger groups make it more diﬃcult for workers to monitor each other’s
eﬀort, which can encourage shirking.2 On the other hand, in the case of group incentives for
teachers, the existence of larger groups can have positive eﬀects through spillovers, such as
the development of team teaching practices, mentoring, technology adoption, peer eﬀects, and
mutual professional support. How individual performance responds to group-based incentives
thus is an empirical question about which little currently is known in general, and especially
1

In the context of this paper we deﬁne “eﬀort” broadly to include not only an increase in quantity (e.g.
time working) but also quality (more eﬀective use of time) and actions that increase the use of productivity
enhancing technologies.
2
In the context of supermarket workers, Mas and Moretti (2009) show that peer-monitoring can substantially
reduce free-riding behavior.

1

with regard to teachers.
In this paper, we test directly for whether the strength of a group-based incentive a teacher
faces aﬀects her productivity using the implementation of the ASPIRE3 teacher incentive pay
program in the Houston Independent School District (HISD). In the 2006-2007 school year,
HISD began a rank-order tournament incentive pay program that pays teachers based on the
relative value-added of their students’ performance on math, English, science and social studies
state exams. In 2007-08 and later for high school teachers, the incentives are group-based,
rewarding teachers for the performance of all students in each year-school-grade-subject group.
The awards are allocated using a rank-order tournament for each subject, with sharp cutoﬀs in
award amounts at the 50th and 75th percentiles of the district-wide, subject-grade value-added
distribution. The award amounts are substantial: the maximum award in the 2009-2010 school
year was $7,700.4
Our analysis begins with a simple model of worker eﬀort in which we develop predictions
for how teachers will respond to the implementation of an output-based rank-order tournament
incentive pay system as a function of the proportion of the output for which they are responsible.
We model a group incentive pay system with two teachers per group, where teachers diﬀer only
in the share of students they teach. We focus on the eﬀect of a teacher’s share on her own
eﬀort and ﬁnd that, in general, the model yields ambiguous predictions for how teacher share
should aﬀect teacher eﬀort. When we parameterize the model and solve it numerically, our
results point to increasing share having mostly a positive eﬀect on eﬀort but that the eﬀect is
non-linear in share. At small shares, increasing share has large, positive eﬀects on eﬀort that
declines with the proportion of students in a group teachers instruct, such that it approaches
zero or even becomes negative for some parameter values.
We then take these theoretical predictions to the data, using individual student-level data
from before and after implementation of the incentive pay program. We test for responses
to incentive strength by examining whether teachers who are responsible for a larger share
3

ASPIRE stands for “Accelerating Student Progress, Increasing Results and Expectations.”
The maximum award includes a 10% bonus for perfect teacher attendance. Additional smaller awards based
on school-wide performance metrics also are provided. The maximum for these awards combined was $3,410 in
2009-10 when applying the attendance bonus.
4

2

of students in each grade and subject generate more achievement gains after implementation
of the award system than those who are responsible for teaching fewer students. Under a
group incentive scheme, the share of students a teacher instructs is a strong proxy for incentive
strength because, as the teacher share increases, a teacher’s impact on the probability of award
receipt rises. We argue that this share is a more direct measure of cross-teacher incentive
diﬀerences than the number of workers in the group, which is the measure typically used to
examine group-size eﬀects. Thus, our key explanatory variable is the share of a subject-schoolgrade cell enrolled in each teacher’s classes, and we identify how the eﬀect of this share changes
when the incentive pay program is implemented using a diﬀerence-in-diﬀerence methodology.
By controlling for pre-ASPIRE share, lagged student test scores, student demographics, schoolyear and grade-year ﬁxed eﬀects, we argue our empirical models account for the non-random
sorting of students into classrooms with teachers of diﬀering quality and who teach a larger
or smaller share of students. The key identifying assumption we invoke is that the eﬀect of
share taught on student achievement is not shifting systematically when the incentive system
is implemented for reasons not having to do with the program. We present extensive evidence
that this assumption holds in our data.
Doubts about whether teachers respond to ﬁnancial incentives, at least in the United States,
arise from the fact that recent experimental studies in the US have found little overall impact of
incentive pay on achievement (Fryer, 2011; Springer et al., 2010; Goodman and Turner, 2011).
Hence, a major contribution of our analysis is to establish that teachers are responding to
incentive pay. In particular, we ﬁnd that teacher productivity increases as ﬁnancial incentives
become stronger. The estimates are largest for math, where we ﬁnd that a 10 percentage point
increase in the share of students taught post-ASPIRE increases test scores by 0.024 standard
deviations. A similar increase in teacher share increases achievement in English and social
studies by 0.014 and 0.020 standard deviations, respectively. There is no eﬀect on science
scores, on average. We show as well that teachers shift their focus across grades in response
to the program, such that among students in diﬀerent grades in the same year with the same
teacher, test performance increases more for students in the higher-share classroom. Further,
using another math exam that is not incentivized under this system, we test whether the
3

achievement gains in math are speciﬁc to the incentivized exam. Despite the large eﬀects we
ﬁnd on the incentivized math exam, there is no impact on the non-incentivized exam.
Consistent with our theoretical model, our estimates show that there are large, positive
eﬀects of increasing share on achievement at low shares in all four subjects post-ASPIRE and
that this eﬀect declines with share. The eﬀect of increasing share by 10 percentage points is
between 0.05 to 0.09 standard deviations at very low shares and falls until reaching zero at
shares between 0.2 to 0.3. These results are unlikely to be driven by monitoring: conditional on
share, there is no change in the relationship between the size of the department and student test
scores. Our results thus suggest that there are large returns in terms of student achievement
to incentivizing smaller groups of teachers but that these returns disappear as group sizes
decline suﬃciently. A back-of-the-envelope calculation based on our estimates, along with
some assumptions about the nature of the groups, points to achievement-maximizing group
sizes of between 3-5 teachers, which is far smaller than the school-wide groups common in
teacher incentive pay programs.
In showing that there are heterogeneous eﬀects of group incentive pay on teacher eﬀectiveness, this paper demonstrates that previous work examining average eﬀects may miss an
important part of how group incentives operate. While we cannot determine whether our results are due to award salience or free-riding behavior, they nonetheless indicate that targeting
incentives to smaller groups likely yields larger achievement improvements than using large
groups. To date, there has been little work on how to optimally structure teacher incentive
pay,5 and the results from this analysis indicate that design features matter a lot in determining
how eﬀective an incentive system is in increasing student achievement. Our results underscore
the importance of focusing on such design issues in future work.
The rest of this paper is organized as follows: Section 2 describes the previous literature
on teacher incentive pay and on free riding in group incentive programs. Section 3 describes
our theoretical model. In Section 4 we describe the HISD incentive pay program, and our data
are discussed in Section 5. We present our empirical methodology in Section 6. All results are
5

Barlevy and Neal (2012) provide the only theoretical work on optimal teacher incentive pay of which we
are aware.

4

discussed in Section 7, and Section 8 concludes.

2

Previous Literature

The prior literature on teacher incentive pay mostly focuses on group-level incentives. However,
these studies typically examine whether there is an average eﬀect of these incentive pay systems
on student achievement, not how individuals respond to their speciﬁc incentives for increasing
output.6
Lavy (2002) studies a school-wide performance incentive program in Israeli Public Schools
that was implemented in 1995. Schools received bonuses based on dropout rates, the average
number of credit units per student and the proportion of students receiving a matriculation
certiﬁcate. Eligibility for the program is based on school type and geography, creating treatment
and control groups that are of the same type and that are observationally similar. His main
ﬁnding is that the school-based incentives led to an increase in student test scores, a decrease in
dropout rates and an increase in the proportion of students receiving a matriculation certiﬁcate.
He does not examine whether teachers in schools with more teachers are more or less responsive
to the implementation of the award system.
Lavy (2009) analyzes another teacher bonus tournament in Israeli schools that began in 2000.
He ﬁnds that among teachers in eligible schools, test-taking, passing, and scores on a math high
school exit test increase signiﬁcantly due to exposure to ﬁnancial incentives. While Israel is a
developed country, there are substantial diﬀerences between the Israeli and US public education
systems, making it unclear how relevant these ﬁndings, as well as the ﬁndings in Lavy (2002),
are to the United States.7 In addition, experimental studies in developing countries have found
positive eﬀects of both group and individual incentive pay on student outcomes (Muralidharan
and Sundararaman, 2011; Glewwe, Ilias and Kremer, 2010). However, the educational contexts
in these countries diﬀer substantially from the United States, which makes their results diﬃcult
to generalize to the US schooling environment.
6

See Neal (2011) for a review of the teacher incentive pay literature.
For example, Lavy (2009) reports that the predominant way in which teachers reacted to the ﬁnancial
incentive was to increase instruction time, which would be diﬃcult for teachers in the US system to do without
district- or school-level policy changes.
7

5

In the United States, several studies have use randomized experiments to assess the average
impact of school-level group incentive pay in New York (Fryer, 2011; Goodman and Turner,
2011) and individual incentive pay in Nashville (Springer, et al., 2010). They ﬁnd no signiﬁcant
impact of teacher incentives on student performance on average. Sojourner, West and Mykerezi
(2011) examine the eﬀect of Minnesota’s Q-Comp pay for performance system. In this system,
schools can choose whether to provide incentives at the individual teacher or school level. They
ﬁnd very small but signiﬁcant positive eﬀects. Ladd (1999) estimates the eﬀect of a schoolbased, rank-order incentive pay system that was implemented in Dallas from 1991 through
1995. She compares trends in academic achievement in Dallas to schools in other large cities
over this period, and she shows evidence that academic performance in Dallas rose relative
to these other cities. Her empirical methodology cannot diﬀerentiate between incentive eﬀects
and diﬀerential secular trends or shocks across cities, though.8 Finally, Fryer, et al. (2012)
conduct an experiment that gave some teachers award bonuses and other teachers ﬁxed cash
pay-outs prior to the school year who were then required to return some of the money based
on performance. Consistent with the rest of the literature, they ﬁnd no signiﬁcant impacts
from the ﬁrst group, however they do ﬁnd improvements from the second group. This paper
highlights how the particular design of a program matters. It is also interesting to note that
they ﬁnd some larger impacts for two-teacher group awards relative to individual awards in the
second group.
The most closely related study to our own is Goodman and Turner (2011). They use variation in the number of math and English teachers in each school in a school-level randomized
teacher incentive pay experiment in New York City to identify whether free-riding reduces the
program’s eﬀectiveness. They present evidence that achievement declined slightly in larger
schools and may have increased by a small amount in smaller schools. However, given the
aggregate nature of their data at the school-year level, they cannot test whether the diﬀerences in responsiveness by school size are due to free-riding or whether they are due to school
8

Jackson (2010, 2012) examines the eﬀects of the Advanced Placement (AP) Incentive Program in Texas and
ﬁnds that oﬀering student and teachers incentives to pass AP exams increases test taking, test passing, collegegoing and future earnings. However, given the structure of the award program, he is unable to disentangle the
impact of teacher-speciﬁc awards, per se, on student outcomes from the eﬀect of oﬀering both students and
teachers ﬁnancial incentives to pass AP exams.

6

attributes that are correlated with school size. Furthermore, without individual teacher data,
their analysis cannot examine whether there are non-linear eﬀects of group size. Nonetheless,
the results from this analysis point to the potentially important role that group size plays in
determining how teachers respond to group-based incentives.9
Outside of education, there has been more work examining how group incentive schemes
inﬂuence productivity. Prendergast (1999) provides an overview of this literature. Several of
these studies suggest that workers respond less to group incentives when they are part of a
larger group (Newhouse, 1973; Liebowitz and Tollison, 1980; Gaynor and Pauly, 1990). While
suggestive of the existence of free-rider behavior, none of these analyses are able to control fully
for the endogeneity of group size nor do they have exogenous variation in award amounts (i.e.,
in the returns to eﬀort). Hamilton, Nickerson and Owan (2003) do have exogenous variation
driven by a garment plant switching from individual to group-based piece rate pay. They ﬁnd
positive productivity eﬀects of this switch, which are due to increased worker cooperation.
Thus, despite the sizable previous literature on group-based merit pay, little is known about
how group incentives impact worker behavior. In education, very little attention has been paid
to the eﬀects of group-based teacher incentive pay on teacher behavior when there are many
teachers that dilute each worker’s impact on the likelihood of receiving an award. Given the
pervasive nature of group-based incentive pay in education and in the private sector, understanding how group size interacts with teacher behavior is critical to developing optimal merit
pay systems. The unique structure of the HISD teacher incentive pay system for high school
teachers provides an unusually clean test of the impact of the strength of group incentives on
individual behavior that will allow us help ﬁll this gap in the literature.
9

Ahn (2011) also presents evidence that free-riding may exist in group incentive pay systems in education.
He estimates a structural model of teacher eﬀort and student achievement in which he proxies for teacher eﬀort
with teacher absences to analyze a school-level incentive pay system in elementary schools in North Carolina.
He estimates free-rider eﬀects by simulating optimal eﬀort responses by teachers in response to a change from
school to classroom level incentives. While his parameter estimates point to an increase in average bonus receipt,
average teacher eﬀort declines due to a change in which teachers ﬁnd themselves marginal to an award threshold.
These estimates are consistent with a role for group size, but they are only suggestive because he is unable to
disentangle group-size eﬀects from changes in the marginal incentives of teachers.

7

3

A Model of Teacher Responses to Group Incentives

In order to illustrate the role of teacher share in a rank-order incentive pay tournament and
to motivate our empirical analysis, we start with a theoretical model based on Kandel and
Lazear (1992) that we modify to account for the tournament structure of the award system.
We assume workers are homogenous, except for the share of the output for which they are
responsible. We impose this simpliﬁcation in order to isolate the role of the output share
in driving eﬀort diﬀerences across workers in a tournament model. Throughout, we assume
that share is exogenously assigned.10 Teachers choose eﬀort, e, which determines value-added
through the function S(e), where S ′ (e) > 0 and S ′′ (e) < 0. This function does not diﬀer across
teachers by assumption. The cost of eﬀort, C(e), also is the same for each teacher and is convex
in eﬀort: C ′ (e) > 0 and C ′′ (e) > 0.
Teachers compete in a rank-order tournament for a monetary award, A. Teachers only win
the award if the teacher’s group achieves value-added at or above a critical value S ∗ , which
is randomly drawn from an i.i.d., single peaked continuously diﬀerentiable distribution with
mean µ, variance σ 2 and highest density at µ. The CDF for this distribution is deﬁned by
F (x), with pdf f (x) = F ′ (x). While S ∗ is potentially endogenously determined by strategic
interaction of teachers, in practice since there are many teachers and many groups competing in
the tournament we assume that teachers take S ∗ to be exogenous.11 S* is unknown to teachers,
but they do know the distribution that generates S*. The teacher’s utility is

Ui = A × 1

N
(∑

θk S(ek ) > S

∗

)

− C(ei )

(1)

k

where 1(·) is the indicator function, θ is the teacher’s share of students in a school-subject-grade
∑
and N
k θk = 1. Hence, if the group exceeds the critical value, the teacher receives a ﬂat award
of A.
10

This is an innocuous assumption in this model, because with equal ability teachers, principals will have no
incentives to assign higher shares to the “best” teachers.
11
Note that even with this simplifying assumption, this model yields quite complex predictions about the
relationship between teacher share and eﬀort. Endogenizing S ∗ will add considerable complexity without, we
believe, adding much more insight into this relationship because each teacher would exert at most a small eﬀect
on S ∗ due to the large number of teachers and groups.

8

The eﬀect of the award incentive on each teacher’s behavior will be a function both of the
teacher’s own share of output and the eﬀort other teachers in the group exert. We therefore
must solve for the Nash equilibrium in this static game. In order to make this problem easier to
solve, consider a group with two teachers, 1 and 2. Each has the same expected utility function
given by equation (1). Deﬁne S̄ = θS(e1 ) + (1 − θ)S(e2 ) − µ as the group-average value-added
less the mean of the cutoﬀ distribution (e.g., S̄ > 0 if the mean value-added is greater than the
expected cutoﬀ). The expected utility functions for teachers 1 and 2 are given by:

E(U1 ) = AF (S̄) − C(e1 )

(2)

E(U2 ) = AF (S̄) − C(e2 )

(3)

These expected utility functions lead to two ﬁrst order conditions, which are the best response
functions for teacher 1 and teacher 2, given the award structure and exogenously assigned
teacher share:
AθF ′ (S̄)S ′ (e1 ) − C ′ (e1 ) = 0

(4)

A(1 − θ)F ′ (S̄)S ′ (e2 ) − C ′ (e2 ) = 0

(5)

The intersection of these best response functions implicitly deﬁne the optimal amount of eﬀort
for each teacher, e∗1 and e∗2 . The empirical focus of this paper is on identifying

∂e∗1
.
∂θ

However,

solving for optimal eﬀort through applying the implicit function theorem to equations (4) and
(5) and then calculating

∂e∗1
∂θ

yields a complicated expression that cannot be signed in general.12

Thus, in order to give us insight into how optimal eﬀort in this tournament setting varies with
output share, we parameterize the model and solve it numerically.
√
Let S = e, C = 21 e2 and A=1. These functions are chosen to be concave and convex,
respectively, and we note that the resulting comparative statics are similar using alternative
( )
concave and convex functions for S and C. We parameterize F (·) with a normal CDF, Φ S̄σ ,
and we will explore below how the model predictions vary with the mean and variance of this
12

The full closed-form solution is provided in Appendix B. We note that there is clearly no general sign here
∂e∗
as below we show examples where ∂θ1 is positive and examples where it is negative.

9

distribution. The ﬁrst order conditions using these parameters are:
( )
θϕ S̄σ
√ − e1 = 0
2 e1
( )
(1 − θ)ϕ S̄σ
− e2 = 0
√
2 e2

(6)
(7)

where ϕ(·) is the standard normal PDF. The set of equilibrium achievement values (and by
extension teacher eﬀort) of teacher 1 as a function of θ, which is teacher 1’s share, is shown
in Figure 1 for diﬀerent values of µ and σ.13 The relationship between eﬀort and own share
is concave for most values of θ in each µ-σ combination, although these parameters aﬀect the
speciﬁc shapes of these curves.14 Note that since the share for teacher 2 is 1 − θ, the equilibrium
achievement values of that teacher’s students would be deﬁned by the mirror image ﬁgure. Also
note that, since achievement is a concave function of eﬀort, the shape of this relationship will
show the same patterns as the direct relationship between θ and eﬀort.
In each case, increasing teacher share for teachers with low share increases achievement by
much more than increasing share for those with a higher share. This non-linearity is more
apparent in Figure 2, which shows

∂S(e∗1 )
,
∂θ

calculated empirically for each 0.001 change in θ.

The eﬀect of increasing teacher share on achievement is much larger at lower share levels, and
it declines with share. This ﬁnding is robust to diﬀerent assumptions about the mean and
standard deviation of the cutoﬀs for award eligibility, although for σ = 0.5 and µ = 1.5 the
eﬀect of share on achievement approaches zero at lower share levels. This non-linear eﬀect is
driven by several factors. First, free-rider eﬀects are larger at lower levels of θ. Increasing θ for
those with low share reduces the incentive to free ride more than increasing θ for a high share
teacher, which translates into a larger eﬀort increase. Second, because the marginal cost of
eﬀort is increasing in eﬀort and the marginal beneﬁt of eﬀort is decreasing in eﬀort, there is a
limit to how much a teacher will increase her eﬀort, regardless of the strength of the incentives.
As share increases, teachers increase eﬀort, but because eﬀort is costly and due to diminishing
marginal returns to eﬀort, this increase occurs at a decreasing rate.
13

The values of µ are diﬀerent for each σ in order to make the range of means examined be roughly proportional
to the standard deviation.
14
The exceptions are for σ = 0.5 and µ = 1 or µ = 1.5, where the curve becomes convex when θ > 0.7.

10

As discussed above, in general,

∂e∗1
∂θ

cannot be signed. While the derivatives shown in Figure

2 are mostly positive, the eﬀect of increasing share can become negative when share is high and
σ is small and/or µ is small. This result stems from the fact that increasing θ reduces the share
for the other teacher (1 - θ). Since at high θ the ﬁrst teacher is exerting a lot of eﬀort while the
second teacher is exerting little eﬀort, the convexity of the cost function implies that teacher
1 can get large utility gains from reducing eﬀort. This eﬀect is strongest when the cutoﬀ is
known with more certainty, as then it becomes easier for teachers to target eﬀort. Furthermore,
the model predicts that teacher eﬀort is particularly sensitive to share variation at low shares.
Thus, reducing teacher 2’s share when she only teaches a small proportion of the students could
lead to a sizable reduction in her own eﬀort, which would reduce the group value-added. If
the group is far from the expected cutoﬀ, this reduction in eﬀort could place the group too far
to reasonably expect to be in contention for the award, which also could induce teacher 1 to
reduce eﬀort.
With the exception of these more extreme cases, Figures 1 and 2 show that in a theoretical
model in which there are two teachers in each group and heterogeneity is only a function of share,
increasing share tends to have a positive eﬀect on student achievement that declines with share
(e.g.

∂S(e∗1 )
∂θ

> 0 and

(∂S(e∗1 ))2
∂2θ

< 0). As a result, this model provides two testable predictions.

First, an increase in the share of students for whom a teacher is responsible should generate an
increase in average achievement on the incentivized exam. Second, this eﬀect becomes smaller
as the teacher share increases. We will test both of these predictions in our empirical analysis
below. The model also predicts that under certain conditions increasing teacher shares when
they are already high could have a negative impact on achievement. While this is an intriguing
result, unfortunately we will not be able to test this prediction directly as our data do not have
suﬃcient variation at high teacher shares (see Figure 3).

4

The ASPIRE Teacher Incentive Pay Program

The Houston Independent School District is one of the largest school districts in the United
States, with more than 200,000 students enrolled.
11

The district began providing teachers

bonus compensation for the performance of their students on standardized exams in 200506. The initial program contained a mix of school-level and individual teacher rewards based
on student achievement growth on the Stanford Achievement Test and the state accountability
test. In total, teachers who taught “core” courses - math, reading, science, social studies and
English\language arts - could receive up to $6,000 in payments above their base pay. There
were no rewards provided at the department level that year; all awards were either individual
or school-wide. In the 2006-2007 academic year, all merit-based bonuses were awarded at the
school-wide or school-subject level.
The current incarnation of ASPIRE started in the 2007-08 academic year, when HISD modiﬁed the teacher award for high school teachers so that they are determined within grade and
subject rather than by school. The district contracted with the SAS Corporation and moved
to a more complex method of calculating teacher value-added using the Education ValueAdded Assessment System (EVAAS). The system is based on a model developed by William
Sanders and co-authors originally under the moniker “Tennessee Value-Added Assessment System” (Sanders, Saxton and Horn, 1997; Wright, Sanders and Rivers, 2006). For departmentbased awards, where a department is deﬁned by school-grade-subject, the model estimates a
department-grade-year ﬁxed eﬀect that accounts for prior teachers’ or departments’ contributions to achievement.15 The current department ﬁxed eﬀect is captured and then adjusted via
a Bayesian shrinkage estimator so that estimates for departments with fewer observations are
biased towards the mean. This adjusted department ﬁxed eﬀect is the department value-added
score. The value-added measures are then ranked within grade, subject and year.16 Departments that receive value-added scores greater than zero (indicating value-added greater than
the mean) and that are above the median value-added in their group receive an award. The
award doubles if the department is within the top quartile of value-added.
Table 1 provides details on the awards available to teachers each year and the requirements
for receiving them for high-school teachers who teach core courses - the focus of this study. As
the table indicates, although a teacher would receive the awards for all grades in his subject
15

This procedure amounts to including ﬁxed eﬀects for the teachers\departments each student had in each of
the last three academic years.
16
See Wright, White, Sanders and Horn (2010) for a detailed technical treatment of both methods.

12

regardless of whether he teaches each grade, each award is based on grade and subject speciﬁc
performance. For example, a teacher may only teach 9th grade students in science and hence
her actions only can contribute to the 9th grade portion of the science award. However, if
her department meets the requirements for the 10th grade science award, she will receive that
award money as well. Our identiﬁcation strategy relies on the fact that how much each teachers’
actions contribute to the probability of award receipt diﬀers by the share of students she teaches
in each grade. Despite the fact that teachers may receive bonus money due the actions of
teachers in other grades, the incentive system is designed such that each core teacher’s own
students enter into some award tournament. This setup means that every core high school
teacher is incentivized, to some degree, to get over some award threshold.17 And, the most
salient measure of the speciﬁc incentives a teacher faces is the share of students in the group
she teaches, as her impact on the likelihood of award receipt is directly proportional to this
share.
In addition to the teacher award, there are a series of awards for school-wide performance.18
Each of these awards is relatively small, ranging from $150 to $750 a piece, hence we do not
consider them in our analyses.19
In 2006-07 and 2007-08, teachers could earn up to $5,500 from the departmental awards.20
The maximum total award a teacher could receive was $8,030. In 2008-09, HISD increased award
amounts substantially. The maximum award on the department portion jumped to $7,700,
with a total maximum award of $11,330. Given that the base salary for a new teacher with a
17

This design also could lead teachers within a department to act strategically across grades by reducing
performance in earlier grades in order to increase growth in later grades. Due to accountability pressures, it
is unlikely principals would allow such behavior to persist for very long. However, we have estimated models
by grade to examine whether eﬀects are indeed smaller in earlier grades. We ﬁnd no statistically signiﬁcant
diﬀerences across grades, which suggests teachers are not engaging in this cross-grade gaming behavior. These
results are provided in Appendix Table A-4.
18
Each year there are four types of campus-wide awards for which teachers are eligible. Initially, these awards
included a bonus for school-wide performance, an award for being in the top half of a state-wide comparison
group of schools determined by the state education agency, an award for the school being given one of the two
highest accountability ratings, and a writing performance award. In 2009-10, the second campus-wide award
was disbanded and replaced with bonuses for participation in and performance on Advanced Placement and
International Baccalaureate exams.
19
Principals and assistant principals also were given awards for school-wide performance in each subject.
The incentives under these awards were only partially aligned with those of teachers, as they were based on
performance by the entire school in each subject, not by each grade and subject.
20
This amount includes a 10% attendance bonus that is given to teachers who take no sick days during the
year.

13

bachelor’s degree in that year was $44,027, up to 20% of a teacher’s total wage compensation
was determined by performance bonuses, with up to 14% from the department award portion.
Even teachers at the highest step in the pay scale, $71,960, could have received up to 14%
of their salary from incentive pay. The average award across all core teachers in HISD (not
only high school) was $3,614 in 2009-10. Thus, the large bonus amounts relative to base pay
suggests there is substantial scope in this system for teachers to respond to the incentives.
One potential problem with the ASPIRE program is that the use of the EVAAS value-added
methodology for determining award receipt might make the award formula complex and diﬃcult
for teachers to understand. However, there is some evidence that teachers in HISD were well
informed and had a good understanding of the system. In surveys conducted by the district,
teachers were asked about their level of understanding of the program parameters.21 Although
the surveys had relatively low response rates (30% - 50%), those who responded generally
indicated that they understood the program. For example, in May 2009, 90% of teachers
indicated they had very high, high, or suﬃcient understanding of the program. Nonetheless, we
note that teachers do not need to fully understand the value-added system in order to respond
to the incentives we study in this paper. A suﬃcient condition for us to detect responses to
student share incentives is that teachers understand that increasing their students’ achievement
on speciﬁc tests leads to an increase in value-added and that their students’ contribution to
the value-added score is proportional to the share of students they teach in the given subject.
Since detailed documents that explain the value-added system are easily accessible to teachers
online, we believe this condition is likely met and if anything, a lack of understanding would
bias us towards not ﬁnding eﬀects.
The survey responses also provide some insight into whether teachers responded to the
incentives in the ASPIRE program. In May of 2009, teachers were asked a series of questions
about whether they agree that the award program changed various aspects of their teaching.
In each case, at least 47% of teachers responded that they changed a particular aspect. For
example, 47% of teachers indicated they devoted more time to professional development, while
60% indicated they used value-added data to make instructional decisions.
21

The survey results can be found at http://www.houstonisd.org/portal/site/researchaccountability.

14

5

Data

Our data come from matched student and teacher records that cover the 2002-03 through
2009-10 academic years. Since the department-level awards only are provided in high school,
we restrict our analysis to grades 9 through 11 (students in grade 12 are not tested unless
they fail the grade 11 exams). We further restrict the analysis sample to 2003-04 and after
to allow us to control for prior achievement. The data include achievement results from two
types of exams. The ﬁrst is Texas’ criterion referenced exam used for accountability called the
“Texas Assessment of Knowledge and Skills” (TAKS). Unfortunately, we do not know whether
a given seating of this exam is the ﬁrst or a retake after failing the ﬁrst exam. Nonetheless, since
students often undergo intensive test preparation before retakes, a reasonable assumption is that
a student’s lowest score in a year is the initial score. Hence, we use each student’s lowest score in
a year as our achievement outcome for the TAKS exam. The scaled scores are then standardized
within grade and year across the district to have a mean of zero and standard deviation of
one. The second exam type is the Stanford Achievement Test (SAT), a nationally normed
standardized exam. This exam is “low stakes,” since it does not contribute to accountability
or graduation requirements, although as we explain below it is used in English\language arts
and 9th grade science and social studies to determine awards. We also standardize the scaled
scores on these exams within grade and year. In addition to the achievement tests, the data
have information on student course taking, demographics and grades. Students are linked to
teacher id’s via course records. Teachers are matched to awards based on a list compiled in
2009 of courses that count for each award.22
Each observation in the data is for a student-course unit. As a result, some students who
take multiple courses in a subject with either the same or diﬀerent teachers will be observed
multiple times. For example, a student might take a class on US history and a second class on
world cultures with two diﬀerent teachers, both of whom would be eligible for the social studies
22

Course names were standardized across the district in 2006-07 and remained consistent afterwards. However,
prior to 2006-07 some courses had diﬀerent names. Additionally, some new courses were created and old courses
discontinued. Generally, this is not a problem since the awards are only based oﬀ of core subjects – math,
science, social studies, language arts, and reading – for which course oﬀerings change little over time. Thus, we
visually inspected courses that did not match directly to the list to determine whether they should be included
as an award eligible course had the ASPIRE program existed at the time.

15

awards. In this case, the student’s achievement only would count towards the value-added
metric that determines awards once even though the student would appear in our data twice.
In order to ensure that such students are not given excess inﬂuence on the estimates, in all
of our regressions we assign weights to each observation equal to the inverse of the number of
courses the student takes in a subject.23
The data are split into four subjects - math, English & language arts (ELA), science, and
social studies. Teachers for each of these subjects are eligible for the departmental awards.
While reading teachers also are eligible for awards, by high school few students take reading
as most have moved on to English literature. Although reading and ELA are combined into
a single award, students who take reading enter into the departmental value-added calculation
based on reading scores, while students who take ELA courses enter based on language scores.
Since very few students take reading in high school, estimates of impacts on reading achievement
are very noisy. Hence, we do not provide results for reading. Note that this implies that only
students who take an ELA course are included in our analysis of language scores.24
We assign teachers to students based on current academic year assignments for both spring
and fall regardless of which test is used to determine awards. The TAKS exam is given in
late March or early April, making the appropriate teachers for this exam the fall and spring
teachers of the current school year. The Stanford exam is given in January, however, making the
appropriate teacher assignment more ambiguous. We use the same assignment throughout for
purposes of consistency as well as because, for the January exam, the spring semester teachers
in academic year t can inﬂuence the score through test preparation, extra teaching sessions
and review for the exams. On the other hand, the teacher from the spring of academic year
t − 1 may not be inclined to change her eﬀort in response to the year t award, since many of
her students in that term may not count towards her year t award due to school switching,
dropouts, and students exiting 11th grade. Since the best method for linking Stanford tests to
teachers is not entirely clear, we provide robustness checks from models that link students to
23

Results are similar without weighting and are provided in the Appendix Table A-4.
Since reading scores contribute towards award determination, teacher shares for ELA teachers are calculated
as the number of students that teacher has in ELA courses divided by the total number of students in ELA &
reading courses in the grade.
24

16

the fall teacher of year t and the spring teacher from year t−1 as well as estimates that use only
the fall semester teachers to identify award impacts. These results are provided in Appendix
Table A-4 and show that our conclusions are robust to the speciﬁc manner in which we match
teachers and students.
Since HISD had an individual award system for high school in 2005-06, we drop this year
from our main analysis as it is unclear whether this should be considered a treatment or
comparison year. Furthermore, we drop 2006-07 as awards during this year were based on
school-wide value-added in a subject rather than grade-level value-added. Nonetheless, we will
show later that including these years with 2005-06 as a “pre” year and 2006-07 as a “post” year
has little impact on our estimates. We further limit the sample by dropping charter schools and
alternative schools as the former tend to be very small and the latter serve special populations.
In both cases, this makes these schools relatively incomparable to traditional high schools. We
also drop observations for all teachers who instruct fewer than 10 students in a subject as these
are likely to be part-time teachers who are ineligible for the awards. Finally, we exclude teachers
for whom more than 80% of their students are limited English proﬁcient or more than 80% are
special education, because these classes tend to be small and specialized. In all of these cases,
we estimate models without the restriction and ﬁnd results - described in more detail below that are similar to baseline. Our ﬁnal sample includes approximately 240,000 student-course
observations in 33 high schools with between 263 and 356 teachers in each subject per year.25
Table 2 provides summary statistics and exact observation counts for the data, split by
subject. In general, student characteristics are similar regardless of the subject. This result is
not surprising, as most students are required to take at least one course in math, science, social
studies and English/language arts each year. Note that the smaller sample size for English is
due to the exclusion of students in reading classes. HISD is a heavily minority district - only
11% of high school students are white. The racial composition is mainly a mix of Hispanic
(54%) and black (31%) students. Students in HISD are also relatively low income, with 70%
25

HISD’s teacher identiﬁcation system changed in 2006-07. Hence, we are not able to match all teachers
across pre and post periods and as such we cannot identify how many teachers there are in total. However,
from 2006-07 through 2009-10, the period in our data during which teachers had unique identiﬁcation numbers
that were inviolate over time, we have 745 math, 728 language arts, 695 science and 683 social studies teachers.

17

being economically disadvantaged.26 Furthermore, 63% of students are classiﬁed as being at
risk for dropping out, 7% of students in the sample have limited English proﬁciency and 17%
of the sample is classiﬁed as gifted. While the gifted population may seem large, it is likely
upward biased relative to the underlying population, as a substantial portion of the non-gifted
students drop out prior to or during high school. In Panel [B] we see that, on average, teachers
are responsible for between 12% to 14% of students in a subject-grade, and there are between
12 - 15 teachers in each grade and subject. Note that the mean share is not equal to the inverse
of the number of teachers because teachers with rates of LEP or special education students over
80%, who generally have smaller classes, are dropped from the sample.

6

Empirical Methodology

Our theoretical model indicates that when a teacher is responsible for a small portion of a group,
an increase in that responsibility as measured by the share of students the teacher instructs
should generate increases in achievement. The goal of this analysis is to identify whether
teachers who are responsible for a larger share of students increase test scores more postASPIRE than pre-ASPIRE. If students were randomly assigned to classrooms, we could simply
compare teachers with higher and lower shares after program implementation. However, since
students sort non-randomly into classrooms we need to control for underlying characteristics of
students and teachers that might be correlated with their teachers’ shares. We accomplish this
task via a diﬀerence-in-diﬀerences speciﬁcation.
We use administrative data from HISD on student test scores, student demographics and
teacher assignments as described in Section 5 to estimate the following model:

Aisgjt =β0 + β1 Sharesgjt + β2 Sharesgjt ∗ P ostt +
∑∑
′
γgt Apre
isgjt × Y eart × Gradeg + Xit Φ + λgt + νjt + εisgjt ,
t

(8)

g

where Asigjt is test score in subject s of student i in grade g with teacher j in year t, Share
26

Economically disadvantaged means that a student qualiﬁes for free-lunch, reduced-price lunch, or some
other Federal or state anti-poverty program.

18

is the proportion of students teacher j teaches in year t, grade g and subject s, P ost is a
dummy variable equal to 1 if the incentive pay program is in eﬀect (2006-07 and later), and
Apre
isgjt is lagged student test score. In order to avoid conditioning on scores that could have
been inﬂuenced by ASPIRE, we condition on each student’s 2004-05 achievement score for
2005-06 and later. For 2003-04 and 2004-05, we use once lagged achievement.27 Since the role
of our lagged achievement measure may change by year and grade level, we interact Apre
isgjt with
year-by-grade indicators. The vector X contains student demographic characteristics, such as
race, gender, participation in special education, participation in gifted and talented programs,
limited English proﬁciency, and whether the student is economically disadvantaged, along with
a quartic in total enrollment in the school. In addition to these controls, equation (8) contains
grade-by-year ﬁxed eﬀects (λgt ) and school-by-year ﬁxed eﬀects (νjt ). We estimate this model
separately for math, English, science and social studies tests. Because of the likelihood that
errors are correlated across students within schools and within schools over time, all estimates
in the analysis are accompanied by standard errors that are clustered at the school level.28
The coeﬃcient of interest in equation (8) is β2 , which shows how the eﬀect of teacher share
shifts when the incentive pay program is implemented. In order to interpret β2 as a causal
estimate, we must control for the non-random sorting of students into classes with diﬀerent
teacher shares. It is important to emphasize that we control for lagged student test scores. To
the extent that these scores pick up ﬁxed diﬀerences in student academic ability, any residual
selection would have to be a function of student test score growth, not student test score levels.
We also control for Share, which estimates the eﬀect of teacher share on student test scores
in the absence of ASPIRE. Students in classes in which the teachers teach a large proportion
of students may perform better if the teacher is of higher quality or could perform worse if
the large volume of students causes her to under-perform. The parameter β1 picks up this
27

Results are similar if we use 2002-03 achievement as the lagged score for all years and grades and are
provided in Appendix Table A-4.
28
Clustering standard errors may still cause one to over-reject null hypotheses when the number of clusters
is small (Cameron, Gelbach and Miller, 2008; Bertrand, Duﬂo and Mullainathan, 2004). Using monte-carlo
simulations, Bertrand, Duﬂo and Mullainathan (2004) show only very small over-rejection rates with 20 clusters
and Cameron, Gelbach and Miller ﬁnd similar results with 30 clusters. These simulations suggest that clustering
our standard errors at the school level will not be problematic for the purposes of hypothesis testing, as we have
33 clusters.

19

underlying relationship between teacher share and student achievement and thus β2 provides a
diﬀerence-in-diﬀerences estimate.
Conditional on the school-by-year and grade-by-year ﬁxed eﬀects in addition to the other
controls in the model, the identifying variation in Share comes from several sources. The
ﬁrst is year-to-year diﬀerences in share within teachers over time. The share of students for
whom a given teacher is responsible may vary from year to year due to population variation,
idiosyncratic demand diﬀerences for speciﬁc subjects across cohorts, and teacher turnover. The
variation in Share in equation (8) also comes from diﬀerences in teacher share across teachers
in a given subject and grade within each school.29 Although it is not possible to know perfectly
why diﬀerent teachers are responsible for diﬀerent proportions of students, for our identiﬁcation
strategy to be valid what must be true is that the reason for these diﬀerences does not change
when the program is implemented.
Thus, in order for β2 to provide an unbiased estimate of responses to stronger merit pay
incentives, it must be the case that students with diﬀerent test score growth patterns are
not diﬀerentially sorting post-ASPIRE relative to pre-ASPIRE into classrooms with teachers
who teach a larger (or smaller) share of students. Note that the value added methodology
by which the awards are administered uses statistical adjustments based on multiple years of
prior achievement. Thus, a principal would not be able to manipulate class assignment to
maximize award receipt by simply sorting students based on raw achievement or growth rates.
Further, it is important to stress that due to school accountability rules principals already
had strong incentives to maximize total achievement prior to the implementation of ASPIRE.
Nonetheless, if administrators did try to reallocate high performing students to high share
teachers in response to the incentive program, we would expect that the information principals
use to sort students is correlated with student demographics and achievement. If this is true,
teacher share interacted with being in a post award period would be related to the demographic
characteristics and prior achievement scores of students. In Table 3, we present balancing tests
29

In Appendix Table A-2, we provide results from an analysis of variance for teacher share in 2006 and later.
After accounting for observables and all ﬁxed eﬀects in our model, the results indicate that, depending on the
subject, between 40% and 58% of the remaining variance in teacher share is across teachers while the rest is
within teachers.

20

that show the correlation between our key explanatory variable and demographic characteristics
of students. In particular, we estimate regressions of the following form:

xisgjt = α0 + α1 Sharesgjt + α2 Share ∗ P ostsgjt + λgt + νjt + εisgjt ,

(9)

where x is a speciﬁc student characteristic and all other variables are as previously deﬁned.
In Table 3, we show estimates of α2 that test whether shifts in teacher share surrounding the
implementation of the incentive pay program are correlated with shifts in student observable
characteristics.
The estimates in Table 3 suggest there were no signiﬁcant changes in the relationship between student demographics and teacher share when ASPIRE was implemented. We test
whether there are “impacts” on gender, race, economic status, at-risk status, special education, LEP, and gifted and talented status. We also examine the “impact” of ASPIRE on pretreatment achievement levels and gains (one-year growth in test scores). In no case are these
estimates signiﬁcant at the 5% level and only one, LEP status for science exams, is signiﬁcant
at even the 10% level. The one potentially troublesome estimate is for science achievement.
While not statistically signiﬁcant, it is nonetheless large and indicates that teachers with higher
shares tend to get higher achieving students in science. While this result may give us some
pause in the interpretation of the science results, it is nonetheless comforting that we see no
similar estimates in any of the three other subjects, and in fact the math and English point
estimates have negative signs. Further, we stress that we control for lagged achievement and
other student observables in all of our models, which helps address the potential sorting in
science.
Another identiﬁcation concern is that, even if student sorting did not change as a function
of share when ASPIRE was implemented, teacher shares could have adjusted in response to the
awards. For example, a principal may decide that, in order to maximize award receipt, she will
increase shares for good teachers while decreasing shares for low-performing teachers. While
principals have very limited ability in HISD to ﬁre teachers due to low value-added, this goal
could be achieved by assigning teachers in core subjects to teach in non-core subjects instead or

21

to teach lower-share core classes. Such re-assignment is likely to be diﬃcult, however, as by high
school most teachers specialize in speciﬁc subjects and have high levels of speciﬁc human capital
in those subjects, which makes it costly for them to switch. Also, as mentioned above, due to
accountability pressures, the principal already had an incentive to maximize group achievement
by assigning the best teachers the highest shares before ASPIRE.30
Nonetheless, we can check on the empirical relevance of this theory by assessing whether
there is a change after implementation of ASPIRE in how teachers of varying quality are
assigned shares. We link teachers over time to calculate teacher value added for a subset of
teachers. Unfortunately, while we have unique teacher id’s for 2006-07 and later, prior to 200607 the teachers were not linked over time or as they changed schools by ID numbers. For these
years, we have teacher names and gender but we were unable to acquire teacher name data
(along with teacher characteristics) for the 2007-08 academic year. Further, these data from
2008-09 and 2009-10 are incomplete, with missing information for many teachers. Thus, in
order to get a reasonable sample of teachers linked over time, we create a sample that links
teachers by name and gender from 2002-03 through 2006-07 and then matches to their 200607 id number. Teachers from 2007-08 and later are then matched to earlier years via the id
number, and we restrict the sample to those who are in the data in 2006-07. We note though
that this method has two key limitations. First, we are left with a select sample of teachers
who were in HISD in 2006-07. Second, matching by names leaves us with some teachers with
the same name who will be grouped together and some teachers who change names, mostly due
to marriage, who will be identiﬁed as two separate teachers.
With these caveats in mind, we calculate teacher value-added using data from prior to the
implementation of ASPIRE. In particular, we estimate the following model for each subject,
applying the weights described in Section 5:

Yisgjt = γ0 + γ1 Yisgj,t−1 + Xit′ Φ + λgt + νjt + εisgjt ,
30

(10)

Note that if principals re-organized shares across teachers in a way that increased aggregate test scores,
this would be a positive causal eﬀect of the program. However, it would be coming through altering teacher
assignments rather than through increasing teacher eﬀort.

22

For each grade, we use the standardized score on the exam that would eventually be used
to determine ASPIRE awards. After estimating equation (10) using data from 2003-04 and
2004-05, we generate residuals for each student-course linkage and average over all (weighted)
observations for each teacher. These average residuals are used as the “Teacher Value-Added”
dependent variable in Table 3. The last row of Table 3 provides the estimates for the “impact”
of teacher share after ASPIRE implementation on teacher value-added. For all four exams,
the point estimates are small in magnitude and are not statistically diﬀerent from zero at
conventional levels. Further, there is no consistent pattern in the point estimates, with English
being positive and the other three subjects negative.31 Thus, we ﬁnd no evidence that teachers
with higher pre-ASPIRE performance were being given higher shares when the incentive pay
program was implemented.
To develop more evidence on whether principals are altering teacher shares endogenously
in response to ASPIRE, we compare teacher share distributions before and after ASPIRE is
implemented. If there is a push to place more students with the high performing teachers
in core subjects, we would expect to see a shift in the teacher share distribution towards
having more teachers with large teacher shares. Figure 3 provides these distribution plots in
each subject during the pre-ASPIRE (2003-04 to 2004-05) and post-ASPIRE (2007-08 to 200910) periods. In all four subjects, the distributions are very similar across time periods, with
little evidence of any shift towards higher teacher shares. Hence, these results, combined with
the teacher value-added results in Table 3, suggest that teacher assignments were unlikely to
have been adjusted in a way systematically related to teacher share concurrent with program
implementation. Furthermore, we show below that our results are similar if we instrument for
share using pre-ASPIRE inverse department size, which cannot be inﬂuenced by endogenous
sorting in response to the program.
31

In Appendix Table A-1, we also provide estimates of the impacts of share on whether a student is new to
the school or was not enrolled in the district in the prior year. In the former case only the math sample shows a
signiﬁcant eﬀect at the 10% level, while only the science sample shows signiﬁcant eﬀects for the latter. We also
look at whether the number of courses taught by a teacher is correlated with P ost ∗ Share and only the English
estimate is signiﬁcant at even the 10% level, but the coeﬃcient is positive. Having more courses requires more
work on the part of teachers, and so without any eﬀort adjustment achievement should be lower. Thus, we
would expect that, if anything, this eﬀect would generate a downward bias in the English estimates.

23

7

Results

7.1

Baseline Estimates

Before presenting our estimates of equation (8), we examine the correlation between teacher
share and achievement by year in order to see whether there are pre-treatment trends and
whether a break in any pre-treatment relationship between these variables is evident around
2006-07 when the group incentive pay system started. We estimate models similar to (8) except
Share and P ost ∗ Share are replaced by interactions of Share with year indicators. Note that
while in our main regressions we omit 2005-06 and 2006-07, we include them here to better
measure trends. Figure 4 presents estimates of the eﬀect of a 10 percentage point change in
teacher share by year, separately by exam. The estimates for math, shown in the ﬁrst panel, are
the most notable. Prior to 2006, teacher share was uncorrelated with student achievement, while
after the incentive pay system was enacted teachers who were responsible for more students
performed better than those responsible for fewer students. The estimates for English (second
panel) also show a clear level shift after 2005. For science and social studies,32 the year-by-year
estimates after 2006 are more mixed. Nonetheless, the ﬁgures show that there is no trend in
estimated eﬀects of teacher share prior to implementation of ASPIRE, providing support for
our diﬀerence-in-diﬀerences identiﬁcation strategy. Indeed, F-tests of the joint signiﬁcance of
the pre-ASPIRE years (2003-04 through 2005-06) do not reject the null of equality, with test
statistics of 0.3, 0.0, 0.0 and 0.0 for math, English, science and social studies, respectively.
Thus, we ﬁnd no evidence of pre-treatment trends in the share-achievement relationship prior
to ASPIRE implementation. In particular, the ﬁgure indicates that any falsiﬁcation test that
uses pre-treatment data and involves setting the treatment year to 2005-2006 or before would
show no change in the relationship between test scores and share when the false program was
implemented. The ﬁgure also provides evidence that the ASPIRE program generated a positive
shift in the relationship between teacher share and achievement, particularly for math.
Table 4 presents the baseline estimates of equation (8). The estimates in each column of
32

We do not have data for performance on the state exam in social studies for 2006-07, so we omit that year
from the social studies regressions.

24

each panel come from separate regressions, with controls added sequentially across panels. For
brevity, only the estimates for Share (β1 ) and Post*Share (β2 ) are shown. Consistent with
Figure 4, Table 4 shows that the eﬀect of teacher share on math test scores increases after the
incentive pay system went into eﬀect. In the ﬁrst panel, we include grade-year ﬁxed eﬀects
and lagged achievement as controls. In Panel [2], we add student level controls. Then in Panel
[3] we add school ﬁxed eﬀects. In Panel [4], we have our preferred speciﬁcation that replaces
school ﬁxed eﬀects with school-by-year ﬁxed eﬀects. The ﬁrst four columns provide results for
the exams that are linked to the incentives. Both math and social studies show similar results
in all four speciﬁcations. In Panel [4], the math estimate is 0.24 and is signiﬁcant at the 5%
level. It indicates that a 10 percentage point increase in share increases average achievement
amongst that teacher’s students by 0.024 standard deviations. Similarly for social studies, the
estimate is 0.20 and is signiﬁcant at the 10% level. For English and science, the inclusion of
school year ﬁxed eﬀects makes a notable diﬀerence, increasing the estimate for English from an
insigniﬁcant 0.05 to a signiﬁcant 0.14. For science the opposite occurs, as the school-year ﬁxed
eﬀects drop the science estimate from 0.13 to essentially zero. With signiﬁcant and positive
impacts for math, English and social studies, the results indicate that teachers do respond to
changes in the share of students for whom they are responsible in the direction predicted by
the theoretical analysis in Section 3.
The results in Table 4 also help address whether the bonuses incentivize teachers to focus on
speciﬁc tests or whether they lead to a general increase in knowledge.33 We examine whether
students in classrooms with teachers who have a higher share post-2006 score higher on the
Stanford math exam, which is administered to all students but is not part of the incentive
pay system. The last column of Table 4 presents estimates of equation (8) using standardized
Stanford math scores as the dependent variable. Focusing on panel [3], the estimate is 0.22.
While it is not statistically signiﬁcantly diﬀerent from zero at conventional levels, the estimate
is close to the estimate for the state math exam. However, when we include school-year ﬁxed
33

Another possibility is that incentives encourage cheating. For example, Jacob and Levitt (2003) ﬁnd nontrivial amounts of teacher cheating on standardized tests in Chicago in response to accountability incentives.
See Barlevy and Neal (2012) for a discussion of the design of optimal teacher incentive mechanisms that avoid
this problem.

25

eﬀects, the estimate falls considerably, to 0.03. Hence, we ﬁnd little evidence of impacts on this
non-incentivized exam. We note though, that while this could be indicative of teachers focusing
speciﬁcally on the incentivized exam, it is also the case that TAKS and Stanford do not fully
overlap in topics studied. Because the curriculum is targeted towards TAKS, it may be that
teachers focus on topics in the curriculum that are not well covered in the Stanford exam.34
Indeed, our estimates show that Stanford math performance did not decline as a function of
share post-ASPIRE, which suggests teachers were not completely shifting their focus to the
incentivized exam. That the relationship between Stanford exam scores and share does not
shift post-ASPIRE also provides support for our main identiﬁcation assumption that principals
did not sort students diﬀerentially into classrooms as a function of share post-ASPIRE. Such a
change in sorting should show up on all test scores, not just on the incentivized exams.
In Panel [5], we provide a set of estimates that relies speciﬁcally on variation within teachers
and years. The unique design of the ASPIRE program leaves many teachers with diﬀerent
incentives across grades, depending on the proportion of students they teach in each grade.
For example, a teacher may instruct 50% of 9th grade students but only 20% of 10th . Thus,
the teacher will face stronger 9th grade incentives than 10th . This setup provides a diﬀerent
source of identifying variation than our baseline diﬀerence-in-diﬀerence models, as it leverages
diﬀerences within teacher in share in the same year by controlling for teacher-by-year ﬁxed
eﬀects. Furthermore, these estimates are of interest to the extent that they show teachers
shifting focus or eﬀort across grades due to the ﬁnancial incentives they face. The results in
Panel [5] show estimates that are positive and signiﬁcant for all four incentivized exams, with
no impact on the non-incentivized Stanford math exam, mirroring the estimates in Panel [4].35
The only estimate that is notably diﬀerent from those in Panel [4] is science, which is now large,
positive and signiﬁcant. These results suggest that teachers do indeed shift focus across grades
to the grade in which they have a higher share. They also provide further support for our
34

Scores on the state math exam and Stanford math have a correlation in our data of only 0.63, which leaves
substantial room for diﬀerences in outcomes across the two exams.
35
In Appendix Table A-4, we also provide estimates that use teacher ﬁxed eﬀects and school ﬁxed eﬀects.
With this analysis we have to use the restricted sample of teachers in HISD in 2006-07, so they may not be
directly comparable to estimates in Table 4. Nonetheless, we obtain similar results with this model, which shows
positive eﬀects for all tests that are signiﬁcant for TAKS math, science and Stanford math.

26

contention that our estimates are driven by teacher responses to ASPIRE, as it is diﬃcult to
tell an alternative story that would lead to within-teacher and year increases in the relationship
between share and student achievement post-ASPIRE. For example, these estimates also are
suggestive that our results are not being driven by increased resources being given to teachers
with higher shares, as it is unlikely that principals can target resources in such a way that
teachers can only use them in one grade.36 Given the similarity of the estimates between
Panels [4] and [5], we will use the model in Panel [4] throughout the rest of the analysis unless
noted otherwise, as these estimates are considerably more precise.

7.2

Heterogenous Treatment Eﬀects by Teacher Share

Thus far, we have estimated the mean eﬀect of post-ASPIRE teacher share over the entire
distribution of shares. These estimates may hide important information, however, as our theoretical model predicts larger eﬀects at lower shares that fall to zero as share rises. To test
for heterogeneous responses as a function of share, we estimate local linear regressions of the
eﬀect of teacher share post-2006 on achievement at diﬀerent parts of the share distribution in
Figure 5. This method allows us to examine non-parametrically how the eﬀect of teacher share
changes when the incentive pay system is implemented.37 The ﬁgure shows point estimates and
95% conﬁdence intervals from a series of regressions of equation (8) centered at each percentage
point of the teacher share distribution and restricted to a bandwidth of 0.15 on each side using
a rectangular (uniform) kernel. We show regression estimates up to a share of 0.5, as sample
sizes become too small at larger shares for reasonable inference. Since 95% of the distribution
has a share below 0.4, the standard errors tend to grow considerably at larger shares.38
Consistent with the predictions from our theoretical model, Figure 5 shows that the estimate
36

We stress that if changes in resource targeting were a driver of the eﬀects we ﬁnd, our estimates still would
be showing the causal eﬀect of the incentive pay program on student achievement and how this eﬀect varies
with teacher share. For policy purposes, this is the relevant parameter. But, the interpretation of our estimates
would diﬀer: instead of being driven by changes in teacher eﬀort, changes in resources also would play a role.
While we believe our estimates are most consistent with eﬀort changes by teachers as a function of share postASPIRE, our results are valid even in the presence of resource changes across the share distribution in response
to ASPIRE.
37
While there is parametric structure on the linear models we estimate, we impose no structure on the
heterogeneity with respect to teacher share.
38
In Appendix Figure A-1, we provide ﬁgures that use a bandwidth of 0.1 instead of 0.15. Although noisier,
the basic pattern remains.

27

for Share∗P ost starts out positive at low shares and then falls to zero for all four subjects.39 In
particular, for a teacher with a share close to zero, the impact on achievement from increasing
share by 0.1 would be between 0.05 and 0.09 standard deviations. With the exception of
language, all estimates are statistically signiﬁcantly diﬀerent from zero from a 0.0 share to a 0.2
share. The point estimates ﬁrst cross the zero eﬀect line between 0.2 and 0.3 in each subject,
including ELA. Hence, Figure 5 shows that achievement increases substantially for teachers who
are responsible for small shares of the class as that share increases; that is, the marginal impact
of increasing share falls as the teacher’s share increases. The eﬀects at low shares are sizable,
representing about half to a quarter of the eﬀect of reducing class sizes by seven (Krueger, 1999)
and are about the same size as a one standard deviation increase in teacher quality (Rivkin,
Hanushek and Kain, 2005; Rockoﬀ, 2004).
The estimates shown in Figure 5 do not lend themselves simply to statistical tests that the
eﬀect of share on test scores post-ASPIRE declines with share. Thus, in Appendix Table A-3 we
estimate equation (8) separately for teachers with shares above and below 0.15 and then test for
the equality of the Post*Share coeﬃcients across the two models. As the table demonstrates,
the eﬀect of increasing share post-ASPIRE among teachers with shares less than 0.15 is much
larger than among teachers with shares more than 0.15. For English, science and social studies,
this diﬀerence is statistically signiﬁcant at the 5% level, and for math, while insigniﬁcant, the
p-value is only 0.11.
In Figure 6, we provide local linear regression estimates for the non-incentivized Stanford
math exam. As expected, given the estimates in Table 4, there is no signiﬁcant eﬀect of share on
Stanford math throughout the share distribution. In particular, in ranges where in Figure 5 we
see signiﬁcant eﬀects for math, the estimate for Stanford math is virtually zero. These results
indicate that there are no spillovers from improvements in TAKS into the Stanford test due to
larger incentive strength. Nonetheless, it remains to be seen whether this is due to “teaching
to the test” or because the Stanford exam does not cover the material on which teachers focus.
Although we model the teacher response as a function of the share of the students they teach,
39

While there appears to be an uptick for math starting at around 0.3, the lack of precision at this range
prevents us from being able to test whether this is a true eﬀect. Indeed, except for a small range around 0.4,
these estimates are not statistically signiﬁcantly diﬀerent from zero at the 5% level.

28

there also is a potential direct role for the department size. For example, if teachers monitor each
other’s performance, the number of teachers in each department should be directly related to
teacher eﬀectiveness.40 In Figure 7, we provide local linear regressions of equation (8) with the
addition of a variable for the number of teachers in the department (DepartmentSize) and the
interaction of DepartmentSize with being in the ASPIRE period (P ost ∗ DepartmentSize).
The left column shows the impact estimates for teacher share while the right column shows
impact estimates for department size. Department size only has an independent signiﬁcant eﬀect
on language scores post-ASPIRE. Nonetheless, inclusion of these additional controls strengthens
the share results. In particular, the estimates for language are now statistically signiﬁcantly
diﬀerent from zero at shares between 0.0 and 0.2 and social studies eﬀects stay positive at
slightly higher levels of teacher share. Most importantly, however, is that the graphs show
the same downward sloping relationship between the eﬀect estimate and share as in Figure
5. These results indicate that teachers are responding directly to incentive strength rather
than being inﬂuenced by other factors associated with diﬀerent department sizes, which is a
rather unique ﬁnding in this literature. Furthermore, these results show that teacher share is
a stronger proxy for incentive strength than is department size, which is what has been used
previously to examine group-size eﬀects in education and in other labor markets. Our linked
student-teacher data thus allow us to identify how teachers respond to group-based incentives
with much more precision than has been feasible in previous analyses.
The estimates in Figures 5 and 7 are particularly important as they provides us with a measure of the achievement maximizing (optimal) group size. The results suggest that, assuming
all teachers in a group have equal teacher shares, the optimal group size is between 3 and 5
teachers.41 However, there are a few notes of caution here. First, the teachers in the groups we
measure do not have the same shares, so if how students are allocated across teachers within the
group matters for our calculations, then the estimates may not extend to the equal distribution
40

If the incentive pay program leads to increased monitoring of higher-share teachers, then the Post*Share
coeﬃcients could be picking up monitoring as well. This would be one potential mechanism that would lead to
higher eﬀort among higher-share teachers post-ASPIRE.
41
The 3 teacher group size is calculated using the share where the local linear estimates ﬁrst crosses the zero
eﬀect line for social studies in Figure 7 (0.35), while the 5 teacher group size is calculated using the corresponding
estimate for language in Figure 7 (0.20). All other estimates ﬁrst cross the zero eﬀect line between these two
values.

29

case. Second, the groups in this study are all teachers in a grade who teach a given subject the department. It is not clear that the achievement maximizing group size would remain the
same if smaller groups are generated within departments. Nonetheless, these estimates do give
us a starting point for thinking about optimal group size. To our knowledge, no other paper has
been able to provide even rough estimates of such a parameter. Finally, and most importantly,
even if these values do not precisely identify the optimal group size, the results in Figures 5 and
7 clearly show that if group sizes are large there are eﬃciency gains from reducing group size
under a group incentive scheme. For example, these estimates suggest that school-level incentive pay programs may have little eﬀect on teacher behavior because each teacher’s school-level
share is so low that the incentive she faces is quite weak.

7.3

Robustness Checks

As discussed in Section 6, the interpretation of the shift in the eﬀect of teacher share after
program implementation as causal is predicated on our extensive set of ﬁxed eﬀects and student
background controls being suﬃcient to account for any changes in the underlying relationship
between teacher share and achievement growth coincidental with ASPIRE implementation. In
Table 5, we present a series of robustness checks that help shed light on the validity of this
assumption.
First, we add school-grade ﬁxed eﬀects to the regressions, which has little impact on the
estimates. We then control for the number of students each teacher teaches in Panel [2]. A
teacher who has more students may be able to beneﬁt from economies of scale in responding to
the awards. Including this variable has a negligible eﬀect on our estimates, however, suggesting
that our results are not driven by economies of scale.
HISD has a number of charter and alternative schools. Teachers in these schools are eligible
for the incentive pay awards, but we exclude them from our main analysis because of the
diﬃcult selection problems associated with these schools, given that teachers, administrators
and students in these schools likely diﬀer substantially from those in traditional public schools.
When we include these schools, the estimates are attenuated for math, English and social

30

studies, although they remain positive and statistically signiﬁcant.
Throughout the analysis, we have excluded school years 2005-2006 and 2006-2007 because
in those years the incentive pay system diﬀered substantially from the subject-grade-speciﬁc
tournaments of later years. As in the previous panel, when we add them back into the sample our
estimates become attenuated - which is not surprising as we are essentially adding measurement
error. Nonetheless, the estimates are qualitatively similar to baseline. In Panel [5], we relax
our restriction on the minimum number of students teachers can have to be included in the
regressions. The results change little. In Panel [6], we add back in teacher-courses with more
than 80% Special Education or 80% LEP and ﬁnd results that are in-line with those shown in
Table 4. In Panels [7] and [8], we drop all special education and LEP students, respectively,
and ﬁnd results similar to baseline.42
Finally, we check the robustness of our estimates to potential sorting of students and teachers
in response to the program by estimating diﬀerence-in-diﬀerence models using

1
2004DepartmentSize

to specify treatment intensity. This model tests whether departments that were larger (and
thus had lower teacher shares) in the pre-ASPIRE period experienced larger increases in test
scores when the program was implemented. Panel [A] of Appendix Table A-5 contains these
estimates, and they are very similar to those shown in Table 4. While the estimates are less
precise than in the baseline models, which underscores the value of using the share variation
we have in our main estimates, the similarity of the point estimates in this model to the
baseline model suggests limited scope for bias from endogenous student and teacher sorting
in reaction to ASPIRE. In Panel [B], we use

1
2004DepartmentSize

and P ost ∗

1
2004DepartmentSize

as

instruments for Share and P ost ∗ Share and also ﬁnd estimates that are, on the whole, similar
to our main results. These estimates serve to complement the results shown above by providing
evidence from an identiﬁcation strategy that is not subject to the same potential biases from
post-ASPIRE selection. Together with our baseline estimates, Table A-5 suggests teachers
42

In the appendix we also estimate models that allow for diﬀerent estimates by grade. In no case is an estimate
in a given grade signiﬁcantly diﬀerent from other grades. We also estimate models that match students to their
spring of the prior year and fall of the current year teachers instead of matching to spring and fall of current
years. We further estimate models only matching student to their fall of current year teachers and, that use
2002-03 scores as the lagged score for all observations, and that are unweighted. All these models are similar
to baseline. Finally we estimate models with teacher and school ﬁxed-eﬀects (but no school-year FE) and ﬁnd
similar estimates to baseline with the exception of Stanford math which is positive and signiﬁcant in this model.

31

responded to the incentives they faced under ASPIRE and were more responsive when they
were responsible for a larger proportion of the output.

8

Conclusion

Numerous school districts and states have implemented programs linking teacher compensation to student exam performance. Despite their widespread popularity, the evidence on the
eﬀectiveness of these programs is mixed. Particularly troublesome is that recent experimental
analyses have found little impact of incentive pay on achievement (Fryer, 2011; Goodman and
Turner, 2011; Springer,et al., 2010). One potential explanation for these ﬁndings is that these
programs are not designed in a way that induces teachers to respond to the incentives. Unfortunately, while Barlevy and Neal (2012) provide useful theoretical analyses, there is a severe
lack of empirical analysis into the optimal design of such programs. This paper takes a ﬁrst
step in understanding the role of program design in the development of incentive pay programs
by testing for individual teachers’ responses to incentive strength in a group-based teacher
incentive pay program in the Houston Independent School District. The program we study,
called ASPIRE, provides a unique opportunity to examine how teachers respond to free-riding,
award salience and peer-monitoring incentives embedded within the program, since high school
teachers are provided cash awards based on the performance of all students in the teacher’s
grade-school-subject cell. The cash awards are large, accounting for up to 14% of a teacher’s
total wage compensation. This is a useful program for studying teachers’ responses to incentive
strength since, unlike in cases where awards are determined on a school-wide basis, there is
substantial variation in the share of students within a grade-subject that a teacher instructs.
This “share” value is directly related to incentive strength because, as the share increases, the
potential impact of teachers on award receipt increases as well. We use this teacher share as a
proxy for incentive strength and evaluate diﬀerence-in-diﬀerence models that estimate the shift
in the relationship between achievement and teacher share when the teacher incentive pay program is implemented. In addition to informing optimal program design, our analysis establishes
whether teachers respond to the incentives at all. This is important to study as evaluations of
32

overall programs cannot distinguish between whether the speciﬁc program is poorly designed
or the more general problem that teachers simply may not respond to incentive pay.
Our study establishes that teachers do indeed respond to incentives when they are strong
enough. In particular, we ﬁnd evidence that student achievement increases in response to
stronger group incentives, which we interpret as coming from increases in teacher eﬀort. That
is, teachers’ eﬀort increases as their contribution to the probability of award receipt increases.
On average, our preferred estimates indicate that a 10 percentage point increase in teacher
share increases math and social studies achievement by 0.02 standard deviations, while language
scores increase by 0.014 standard deviations. There is no eﬀect on science scores. However,
these pooled estimates hide a substantial amount of heterogeneity. Using local linear regression
techniques we ﬁnd that, at very low levels of teacher share, math, language, science, and
social studies achievement increases by 0.05 to 0.09 standard deviations for each 10 percentage
point increase in teacher share post-ASPIRE. This treatment eﬀect fades out as teacher share
increases and reaches zero at teacher shares between 0.2 and 0.3. These results are consistent
with our theoretical model and are indicative of substantial free-riding or response to award
salience when teachers are responsible for small portions of the relevant student population.
Furthermore, the results provide a basis for estimating achievement maximizing group size. If
students are distributed equally amongst teachers in a group, and assuming there are no eﬀects
from splitting teachers within a department into separate groups, the estimates indicate that
there are beneﬁts to reducing group size until the teachers are in groups of 3 to 5.
More importantly however, is that our analysis suggests that the design of group teacher
incentives has important implications for productivity. In particular, the results indicate that
when implementing group incentive pay it is better to provide awards on the basis of small
groups and that there is substantial potential for schools with group awards to improve productivity by reducing group size. This is an important ﬁnding because most group incentive
pay schemes use the school as the group level. Our results suggest that a group that large,
and even groups based on all teachers in a grade-subject, are likely less eﬀective than smaller
groups with a handful of teachers.

33

References
[1] Ahn, Thomas, 2011. “The Missing Link: Estimating the Impact of Incentives on Eﬀort and Eﬀort on
Production Using Teacher Accountability Legislation.” University of Kentucky, mimeo.
[2] Barlevy, Gadi and Derek Neal, 2012. “Pay for Percentile.” American Economic Review 102(5): 1805-31.
[3] Bertrand, Marianne, Esther Duﬂo and Sendhil Mullainathan, 2004. “How Much Should We Trust
Diﬀerences-In-Diﬀerences Estimates?” Quarterly Journal of Economics 119(1): 249-275.
[4] Cameron, Colin A., Jonah B. Gelbach and Douglas L. Miller, 2008. “Bootstrap-Based Improvements for
Inference with Clustered Errors.” The Review of Economics and Statistics 90(3): 414-427.
[5] Fryer, Roland G., 2011. “Teacher Incentives and Student Achievement: Evidence from New York City
Public Schools.” NBER Working Paper No. 16850.
[6] —, Steven D. Levitt, John List and Sally Sadoﬀ. 2012. “Enhancing the Eﬃcacy of Teacher Incentives
through Loss Aversion: A Field Experiment.” NBER Working Paper No. 18237.
[7] Gaynor, Martin and Mark V. Pauly, 1990. “Compensation and Productive Eﬃciency in Partnerships: Evidence from Medical Groups Practice.” Journal of Political Economy 98(3): 544-573.
[8] Glewwe, Paul, Nauman Ilias, and Michael Kremer, 2010. “Teacher Incentives.” American Economic Journal:
Applied Economics 2(3): 205-227.
[9] Goodman, Sarena F. and Lesley J. Turner, 2011. “Teacher Inventive Pay and Educational Outcomes:
Evidence from the New York City Bonus Program.” Columbia University, mimeo.
[10] Hamilton, Barton H., Jack A. Nickerson, Hideo Owan, 2003. “Team Incentives and Worker Heterogeneity:
An Empirical Analysis of the Impact of Teams on Productivity and Participation.” Journal of Political
Economy 111(3): 465-497.
[11] Holmstrom, Bengt, 1982. “Moral Hazard in Teams.” The Bell Journal of Economics 13(2): 324-340.
[12] Jackson, C. Kirabo, 2010. “A Little Now for a Lot Later: A Look at a Texas Advanced Placement Incentive
Program.” Journal of Human Resources 45(3): 591-639.
[13] Jackson, C. Kirabo, 2012. “Do College-Prep Programs Improve Long-Term Outcomes?” National Bureau
of Economic Research Working Paper No. 17859.
[14] Jacob, Brian and Steven Levitt, 2003. “Rotten Apples: An Investigation of The Prevalence and Predictors
of Teacher Cheating.” Quarterly Journal of Economics 118(3): 843-877.
[15] Kandel, Eugene and Edward P. Lazear, 1992. “Peer Pressure and Partnerships.” Journal of Political Economy 100(4): 801-817.
[16] Krueger, Alan B, 1999. “Experimental Estimates of Education Production Functions.” Quarterly Journal
of Economics 114(2): 497-532.
[17] Ladd, Helen F., 1999. “The Dallas School Accountability and Incentive Program: an Evaluation of its
Impacts on Student Outcomes.” Economics of Education Review 18(1): 1-16.
[18] Lavy, Victor, 2002. “Evaluating the Eﬀect of Teachers’ Group Performance Incentives on Pupil Achievement.” Journal of Political Economy 110(6): 1286-1317.
[19] Lavy, Victor, 2009. “Performance Pay and Teachers’ Eﬀort, Productivity and Grading Ethics.” American
Economic Review 99(5): 1979-2021.
[20] Lazear, Edward P. and Sherwin Rosen, 1981. “Rank-Order Tournaments as Optimal Labor Contracts.”
Journal of Political Economy 89(5): 841-864.

34

[21] Leibowitz, Arleen and Robert Tollison, 1980. “Free Riding, Shirking and Team Production in Legal Partnerships.” Economic Inquiry 18: 380-394.
[22] Mas, Alexandre and Enrico Moretti, 2009. “Peers at Work.” American Economic Review 99(1): 112-145.
[23] Muralidharan, Karthik and Venkatesh Sundararaman, 2011. “Teacher Performance Pay: Experimental
Evidence from India.” Journal of Political Economy 119(1): 39-77.
[24] Neal, Derek, 2011. “The Design of Performance Pay in Education” in Eric A. Hanushek, Stephen Machin
and Ludger Woessmann (Eds.) Handbook of the Economics of Education, vol. 4. North-Holland: Amsterdam.
[25] Newhouse, Joseph P., 1973. “The Economics of Group Practice.” Journal of Human Resources 8(1): 37-56.
[26] Prendergast, Candice, 1999. “The Provision of Incentives in Firms.” Journal of Economic Literature 37(1):
7-63.
[27] Rivkin, Steven G., Eric A. Hanushek and John F. Kain. “Teachers, Schools and Academic Achievement.”
Econometrica 73(2): 417-458.
[28] Rockoﬀ, Jonah, 2004. “The Impact of Individual Teachers on Student Achievement: Evidence from Panel
Data.” American Economic Review 94(2): 247-252.
[29] Sanders, William L., Arnold M. Saxton and Sandra P. Horn, 1997. “The Tennessee Value-Added Assessment
System: A Quantitative, Outcomes-Based Approach to Educational Assessment.” In Grading Teachers,
Grading Schools, J. Millman, ed.: 137-162.
[30] Springer, Matthew G., Dale Ballou, Laura Hamilton, Vi-Nhuan Le, J. R. Lockwood, Daniel F. McCaﬀrey, Matthew Pepper and Brian M. Stecher, 2010. “Teacher Pay For Performance: Experimental Evidence from the Project on Incentives in Teaching.” National Center on Performance Incentives:
http://www.performanceincentives.org/data/ﬁles/pages/POINT%20REPORT 9.21.10.pdf.
[31] Sojourner, Aaron, Kristine West and Elton Mykerezi, 2011. “When Does Teacher Incentive Pay Raise
Student Achievement? Evidence from Minnesota’s Q-Comp Program.” Mimeo.
[32] Wright, S. Paul, William L. Sanders and June C. Rivers, 2006. “Measurement of Academic Growth of
Individual students toward Variable and Meaningful Academic Standards.” In Longitudinal and Value Added
Models of Student Performance, R. W. Lissitz, ed.: 385-406.
[33] Wright, S. Paul, John T. White, William L. Sanders, and June C. Rivers, 2010. “SAS EVAAS Statistical Models.” Technical report. Available at http://www.sas.com/resources/asset/SAS-EVAAS-StatisticalModels.pdf.

35

Figure 1: Equilibrium Achievement Under Various Teacher Shares in a Two-Person
Group-Based Rank-Order Tournament

.2

Average Score of Teacher 1
.4
.6
.8

1

σ = 0.2

0

.2

.4
.6
Student Share for Teacher 1
µ=0
µ = 0.5

.8

1

.8

1

.8

1

µ = 0.3
µ = 0.7

0

Average Score of Teacher 1
.2
.4
.6

.8

σ = 0.5

0

.2

.4
.6
Student Share for Teacher 1
µ=0
µ=1

µ = 0.5
µ = 1.5

.1

Average Score of Teacher 1
.2
.3
.4
.5

.6

σ=1

0

.2

.4
.6
Student Share for Teacher 1
µ = −1
µ=1

µ=0
µ=2

√
The graphs show the relationship between teacher eﬀort and share (for teacher 1), assuming S = e,
C = 12 e2 , A=$1,500 and that the score cutoﬀ function is distributed normally. The diﬀerent panels show
how this relationship varies with the standard deviation and mean of the cutoﬀ distribution.

36

Figure 2: The Eﬀect of Changing Share on Equilibrium Achievement in a TwoPerson Group-Based Rank-Order Tournament

Change in Average Score of Teacher 1
For a 0.01 Increase in Share
0
.001
.002
.003
.004

σ = 0.2

0

.2

.4
.6
Student Share for Teacher 1
µ=0
µ = 0.5

.8

1

.8

1

.8

1

µ = 0.3
µ = 0.7

Change in Average Score of Teacher 1
For a 0.01 Increase in Share
0
.001
.002
.003

σ = 0.5

0

.2

.4
.6
Student Share for Teacher 1
µ=0
µ=1

µ = 0.5
µ = 1.5

Change in Average Score of Teacher 1
For a 0.01 Increase in Share
0
.0005 .001 .0015 .002 .0025

σ=1

0

.2

.4
.6
Student Share for Teacher 1
µ = −1
µ=1

µ=0
µ=2

The graphs show derivatives of the curves in Figure 1, calculated empirically for each 0.001 change in share.
The diﬀerent panels show how this derivative varies with the standard deviation and mean of the cutoﬀ
distribution.

37

Density

8

6

4

0

.5

2003−04 to 2004−05

1

Math

0

.5

2007−08 to 2008−09

1

.5

1

0

Share of Students in Grade

.5

2007−08 to 2008−09

1

Density

0

0

.5

1

0

0

.5

.5

2007−08 to 2008−09

Share of Students in Grade

1

Social Studies

Share of Students in Grade

2003−04 to 2004−05

.5

2007−08 to 2008−09

English Language Arts
2003−04 to 2004−05

1

1

Graphs show distribution of unweighted teacher shares of students. The teacher is the unit of observation. Teachers with fewer than 10 students in a
subject are dropped.

0

2003−04 to 2004−05

Science

Share of Students in Grade

2

0

8

6

4

2

0

Density

4

6

8

Figure 3: Distribution of Teacher Shares During Pre- and Post-Incentive Pay Periods

Density
2
0
10
5
0

38

39

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.02
0
.02
.04
.06

03

20

03

20
Estimate

05
20

7

0
20

08
20

95% Confidence Interval

06
20
Year

4

0
20
Estimate

05
20

7

0
20

08
20

95% Confidence Interval

06
20
Year

Science − By Year
Stanford (9th), TAKS (10th, 11th) Science

4

0
20

Math − By Year
TAKS Math

9
0
20

9

0
20

03
20

03

20
Estimate

20

05

7
0
20

8
0
20

95% Confidence Interval

06
20
Year

09
20

4
0
20

Estimate

20

05

7
0
20

8
0
20

95% Confidence Interval

06
20
Year

09
20

Social Studies − By Year
Stanford (9th), TAKS (10th, 11th) Social Studies

4

0
20

English − By Year
Stanford Language

Figure 4: Eﬀects of Teacher Share by Year

Data for social studies in 2006-07 are unavailable. Each point shows the average eﬀect in a given year of raising the proportion of students a teacher is
responsible for by 0.1 on standardized student test scores. These estimates come from models that include school-year and grade-year ﬁxed eﬀects as well
as controls for school-grade-speciﬁc enrollment, lagged student test scores and student demographics. The bars extending from each point show the 95%
conﬁdence interval of each estimate that is calculated from standard errors clustered at the school level.

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.02
0
.02
.04

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.02
0
.02
.04
Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.02
0
.02
.04
.06

40

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.2
−.1
0
.1
.2

0

0
95% CI

Estimate

95% CI

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

Stanford Science for 9th, TAKS Science for
10th/11th Grade Science Awards

Estimate

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

.5

.5

Estimate

95% CI

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

.5

0

Estimate

95% CI

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

.5

Stanford Social Studies for 9th, TAKS Social Studies
for 10th/11th Grade Social Studies Awards

0

Stanford Language for Language Arts Awards

Each line shows local linear regression estimates of Share*Post from models that include school-year and grade-year ﬁxed eﬀects as well as controls for
school-grade-speciﬁc enrollment, lagged student test scores and student demographics. Rectangular kernels are used with a bandwidth of 0.15. The dashed
lines show the bounds of the 95% conﬁdence interval that are calculated from standard errors that are clustered at the school level.

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.1
−.05
0
.05
.1
.15

TAKS Math for Math Awards

Figure 5: Local Linear Regressions of the Eﬀect of Teacher Share Post-ASPIRE on Student Achievement
Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.1
−.05
0
.05
.1
Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.1 −.05
0
.05
.1
.15

41
Estimate

95% CI

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

.5

Each line shows local linear regression estimates of Share*Post from models that include school-year and grade-year ﬁxed eﬀects as well as controls for
school-grade-speciﬁc enrollment, lagged student test scores and student demographics. Rectangular kernels are used with a bandwidth of 0.15. The dashed
lines show the bounds of the 95% conﬁdence interval that are calculated from standard errors that are clustered at the school level.

0

Stanford Math

Figure 6: Local Linear Regressions of the Eﬀect of Teacher Share Post-ASPIRE on the Non-Incentivized Math Exam
Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.1
0
.1
.2

Figure 7: Local Linear Regressions of the Eﬀect of Teacher Share Post-ASPIRE on
Student Achievement, Controlling for Department Size

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

Change in Award Impact from Additional Teacher
Achievement − Standard Deviation Units
−.04 −.02
0
.02
.04
.06
.08

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher
Estimate

.5

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher
Estimate

.5

95% CI

.5

95% CI

Stanford Science for 9th, TAKS Science for
10th/11th Grade Science Awards

0

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher
Estimate

Change in Award Impact from Additional Teacher
Achievement − Standard Deviation Units

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.1 −.05
0
.05
.1
.15

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher
Estimate

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

95% CI

Stanford Social Studies for 9th, State Exam Social Studies
for 10th/11th Grade Social Studies Awards

0

0

Estimate

.5

.5

95% CI

Stanford Language for Language Arts Awards

95% CI

Stanford Science for 9th, State Exam Science for
10th/11th Grade Science Awards

0

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher
Estimate

Stanford Language for Language Arts Awards

0

0

95% CI

Change in Award Impact from Additional Teacher
Achievement − Standard Deviation Units
−.04 −.02 0 .02 .04 .06 .08

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.1
−.05
0
.05
.1
.15

.5

TAKS Math for Math Awards

.5

95% CI

Stanford Social Studies for 9th, TAKS Social Studies
for 10th/11th Grade Social Studies Awards
.02 .04 .06 .08

0

Change in Award Impact from Additional Teacher
Achievement − Standard Deviation Units
−.04 −.02 0 .02 .04 .06 .08

State Exam Math for Math Awards

Share − Estimate

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.1
−.05
0
.05
.1
.15

P ost ∗ DepartmentSize

−.04 −.02 0

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.1 −.05
0
.05
.1
.15

P ost ∗ Share

0

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher
Estimate

.5

95% CI

Each solid line shows local linear regression estimates of Share*Post or Department Size*Post from models
that include school-year and grade-year ﬁxed eﬀects as well as controls for school-grade-speciﬁc enrollment,
lagged student test scores and student demographics. Each row of ﬁgures comes from a separate regression.
Rectangular kernels are used with a bandwidth of 0.15. The dashed lines show the bounds of the 95%
conﬁdence interval that are calculated from standard errors that are clustered at the school level.

42

43

One subject taught:
$5500
$1667 per grade
Two subjects taught:
$833 per grade

One
$833
Two
$417

Separate award for each subject.
2007-2008 Determined by department-wide
value-added within grade. Must have
value-added > 0 to receive award.
Compared to departments in same subject
and grade in all high schools (grades 9 - 11
only). All teachers in department receive
award regardless of which grades they teach.

One subject taught:
$7700
$2333 per grade
Two subjects taught:
$1167 per grade

One subject taught:
$7700
$2333 per grade
Two subjects taught:
$1167 per grade

One subject taught:
$1167 per grade
Two subjects taught:
$833 per grade

One subject taught:
$1167 per grade
Two subjects taught:
$833 per grade

Separate award for each subject.
2008-2009 Determined by department-wide
value-added within grade. Must have
value-added > 0 to receive award.
Compared to departments in same subject
and grade in all high schools (grades 9 - 11
only). All teachers in department receive
award regardless of which grades they teach.

Separate award for each subject.
2009-2010 Determined by department-wide
value-added within grade. Must have
value-added > 0 to receive award.
Compared to departments in same subject
and grade in all high schools (grades 9 - 11
only). All teachers in department receive
award regardless of which grades they teach.

subject taught:
per grade
subjects taught:
per grade

One subject taught:
$5500
$5000
Two subjects taught:
$2500
Three subjects taught:
$1666

One subject taught:
$2500
Two subjects taught:
$1250
Three subjects taught:
$833

Max Award (with 10%
Attendance Bonus)

Separate award for each subject.
2006-2007 Determined by department-wide
value-added. Must have value-added > 0 to
receive award. Compared to departments in
same subject in all high schools.

Description

Per-Subject Award
For Being in Top 25%

Year

Per-Subject Award
For Being in Top 50%

Table 1: Characteristics of Department Award Portion of the HISD Teacher Incentive Pay Program for 9th to 12th
Grade Teachers

Table 2: Descriptive Statistics

Panel [A]: Student Characteristics:

Variable
Asian
Black
Hispanic
White
Economically Disadvantaged
At Risk
Special Education
Limited English Proﬁciency
Gifted & Talented
Observations

Math
English
Science Social Studies
Students Students Students
Students
0.04
(0.20)
0.29
(0.46)
0.55
(0.50)
0.11
(0.32)
0.70
(0.46)
0.62
(0.48)
0.05
(0.22)
0.07
(0.17)
0.17
(0.38)

0.04
(0.20)
0.31
(0.46)
0.53
(0.50)
0.12
(0.32)
0.69
(0.46)
0.61
(0.49)
0.07
(0.25)
0.03
(0.18)
0.17
(0.38)

0.04
(0.19)
0.31
(0.46)
0.54
(0.50)
0.11
(0.31)
0.70
(0.46)
0.63
(0.48)
0.09
(0.28)
0.07
(0.26)
0.16
(0.37)

0.04
(0.19)
0.31
(0.46)
0.54
(0.50)
0.11
(0.31)
0.70
(0.46)
0.63
(0.48)
0.09
(0.28)
0.07
(0.26)
0.16
(0.37)

241,694

230,099

240,572

243,161

Math
Teachers

English
Teachers

Science
Teachers

Social Studies
Teacher

0.12
(0.13)
13.6
(6.8)

0.13
(0.14)
15.4
(7.7)

0.13
(0.13)
11.9
(5.3)

0.14
(0.15)
12.2
(5.6)

3,518

2,902

3,281

3,053

Panel [B]: Teacher Characteristics:

Variable
Teacher Share
Department Size
Observations

Source: HISD administrative data from 2003-2009. Standard deviations are shown in parentheses below the means.

44

Table 3: OLS Estimates of the Relationship Between Student Background Characteristics and a Teacher’s Share Post-ASPIRE

Independent Variable
Female
White
Black
Hispanic
Economically Disadvantaged
At Risk
Special Education
Limited English Proﬁciency
Gifted & Talented
Achievement Levels†

Math
0.020
(0.038)
0.084
(0.051)
-0.035
(0.051)
-0.056
(0.039)
0.010
(0.048)
0.005
(0.106)
-0.014
(0.020)
0.032
(0.036)
-0.007
(0.105)
-0.091
(0.178)

Observations

241,694

Achievement Value Added††

-0.105
(0.108)

Observations

224,167

Pre-ASPIRE Teacher Value-Added†††

-0.007
(0.061)

Observations

127,161

Test Subject:
English &
Language Science

Social
Studies

-0.046
(0.064)
0.034
(0.039)
-0.039
(0.043)
0.027
(0.046)
-0.006
(0.054)
-0.028
(0.123)
0.025
(0.032)
0.017
(0.026)
0.090
(0.092)
-0.090
(0.229)

0.010
(0.044)
0.009
(0.030)
0.009
(0.047)
-0.036
(0.042)
-0.007
(0.031)
-0.111
(0.086)
0.018
(0.038)
0.022
(0.027)
0.053
(0.073)
0.400
(0.237)

0.034
(0.039)
0.040
(0.051)
0.001
(0.039)
-0.044
(0.043)
-0.002
(0.058)
-0.085
(0.096)
-0.006
(0.023)
0.056∗
(0.029)
0.135
(0.092)
0.212
(0.195)

240,472

242,001

0.135
(0.117)

0.081
(0.100)

219,566

220,010

-0.060
(0.058)

-0.053
(0.078)

123,975

138,627

224,044
0.079
(0.094)
205,995
0.008
(0.075)
124,929

† We use the most recent pre-program (2004 and earlier) lagged achievement. For math and English in the exam
that determines the awards (TAKS and Stanford, respectively.) For science and social studies the TAKS exam is
not given in every grade, so we use Stanford.
†† Value-added regressions include the the most recent lagged achievement from 2003 and earlier as a regressor
interacted with indicators for current grade and year.
† † † Teacher value-added is calculated using 2004-05 and 2005-06 data for the subset of teachers in HISD in
2006-07 as these are the only teachers we can link across both pre and post-incentive pay periods. Value-added is
calculated as the mean residual for each teacher from a regression of student achievement in the relevant exam for
each subject on lagged achievement, gender, ethnicity, economic disadvantaged, at-risk, special education , LEP,
gifted, and grade-by-year and school ﬁxed eﬀects.
Notes: Each cell comes from a separate estimation of equation (9) and shows the estimate of α2 , which is the
coeﬃcient on Share ∗ P ost. Regressions also include school-year and grade-year ﬁxed eﬀects along with a quartic
in enrollment. Standard errors clustered at the school level are in parentheses: ***,**,* indicates statistical
signiﬁcance at the 1%, 5% and 10% levels, respectively.

45

Table 4: OLS Estimates of the Eﬀect of a Teacher’s Share PostASPIRE on Student Test Scores
Test Subject:
TAKS English &
Social
Independent Variable
Math
Language Science
Studies
Panel [1]: Grade-Year Fixed Eﬀects & Lagged Achievement:
0.201
0.053
0.121
0.152∗
Post*Teacher Share
(0.128) (0.088)
(0.079)
(0.086)
-0.136
0.334∗∗
0.123
0.432∗∗∗
Teacher Share
(0.120) (0.148)
(0.144)
(0.154)

0.150
(0.163)
-0.390∗∗∗
(0.133)

Panel [2]: [1] + Individual Controls:
0.177∗
0.033
Post*Teacher Share
(0.097) (0.075)
-0.025
0.075
Teacher Share
(0.093) (0.104)

0.152∗∗
(0.061)
0.126
(0.107)

0.164∗∗∗
(0.060)
0.218∗
(0.117)

0.144
(0.138)
-0.305∗∗
(0.117)

Panel [3]: [2] + School Fixed Eﬀects:
0.215∗∗ 0.051
Post*Teacher Share
(0.099) (0.068)
0.034
0.073
Teacher Share
(0.058) (0.062)

0.131∗
(0.070)
0.099
(0.067)

0.166∗∗
(0.064)
0.268∗∗∗
(0.076)

0.223
(0.137)
-0.270∗∗∗
(0.089)

Panel [4]: [2] + School-Year Fixed Eﬀects:
0.238∗∗ 0.142∗∗∗
Post*Teacher Share
(0.089) (0.049)
0.047
0.004
Teacher Share
(0.061) (0.047)

-0.010
(0.092)
0.184∗∗
(0.078)

0.200∗
(0.104)
0.268∗∗∗
(0.075)

0.032
(0.083)
-0.154∗∗
(0.065)

Panel [5]: [2] + Teacher-Year Fixed Eﬀects:
0.289∗∗ 0.187∗∗
Post*Teacher Share
(0.137) (0.087)
0.226∗∗ 0.024
Teacher Share
(0.091) (0.058)

0.363∗∗∗
(0.105)
0.236∗∗∗
(0.071)

0.364∗∗
(0.151)
0.478∗∗∗
(0.073)

0.017
(0.079)
-0.106∗
(0.079)

Observations

240,472

242,001

239,350

241,694

224,044

Stanford
Math

Source: HISD administrative data as described in the text. The math test in the ﬁrst column
is the state administered TAKS math exam. The Language exams are Stanford tests. For 10th
and 11th grade science and social studies, the TAKS exams are used, while for 9th grade they
are Stanford tests. All estimates are in terms of scale scores standardized across the district
within grade and year. Individual controls include student gender, race, at-risk, special education, LEP, and gifted status. Standard errors clustered at the school level are in parentheses:
***,**,* indicates statistical signiﬁcance at the 1%, 5% and 10% levels, respectively.

46

Table 5: Robustness Checks

Independent Variable

TAKS
Math

Test Subject:
English &
Social
Language Science Studies

[1] Add School-Grade Fixed Eﬀects:
0.221∗∗
0.161∗∗∗
Post*Teacher Share
(0.089)
(0.054)
Observations
241,694
224,044

Stanford
Math

0.091
0.192∗∗
(0.084) (0.091)
240,472 242,001

0.033
(0.080)
239,350

[2] Control for # of Students Each Teacher Has:
0.236∗∗
0.117∗∗
-0.010
0.179∗
Post*Teacher Share
(0.087)
(0.050)
(0.092) (0.103)
Observations
241,694
224,044
240,472 242,001

0.036
(0.081)
239,350

[3] Include Charters and Alternative Schools:
0.144∗
0.110∗∗
-0.054
0.169∗
Post*Teacher Share
(0.083)
(0.046)
(0.080) (0.095)
Observations
254,141
235,200
252,064 252,582

0.003
(0.077)
251,198

[4] Include 2005-06 as Pre and 2006-07 as Post Years:
0.197∗∗
0.051
-0.043
0.131∗
Post*Teacher Share
(0.071)
(0.040)
(0.079) (0.076)
Observations
377,248
346,518
374,473 343,467

0.101∗
(0.058)
373,150

[5] Include Teachers with Fewer than 10 Students:
0.245∗∗∗ 0.127∗∗∗
-0.004
0.203∗
Post*Teacher Share
(0.088)
(0.045)
(0.088) (0.103)
Observations
243,792
226,082
242,136 243,967

0.028
(0.081)
241,414

[6] Keep Classrooms with > 80% Special Ed or LEP:
0.179∗
0.040
0.096
0.142∗∗
Post*Teacher Share
(0.097)
(0.067)
(0.066) (0.060)
Observations
244,699
238,429
242,386 243,658

0.229
(0.136)
244,540

[7] Drop Special Education Students:
0.251∗∗∗ 0.139∗∗∗
Post*Teacher Share
(0.086)
(0.048)
Observations
229,817
209,091

0.047
0.187∗
(0.083) (0.107)
219,536 220,670

0.038
(0.089)
223,778

0.004
0.194∗
(0.100) (0.104)
223,344 224,395

0.048
(0.087)
222,925

[8] Drop LEP Students:
Post*Teacher Share
Observations

0.235∗∗
(0.088)
225,565

0.151∗∗∗
(0.049)
216,705

Source: HISD administrative data as described in the text. The math test in the ﬁrst column is the state
administered TAKS math exam. The English and Language Arts exams are Stanford tests. For 10th and
11th grade science and social studies, the TAKS exams are used, while for 9th grade they are Stanford tests.
All estimates are in terms of standardized scores. Controls include student gender, race, at-risk, special
education, LEP, and gifted status along with lagged achievement interacted with grade-year indicators and
grade-year and school-year ﬁxed eﬀects. To ease presentation we do not show the estimate for the “teacher
share” main eﬀect. Standard errors clustered at the school level are in parentheses: ***,**,* indicates
statistical signiﬁcance at the 1%, 5% and 10% levels, respectively.

47

Appendix

48

49

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.2 −.15 −.1 −.05 0 .05 .1 .15 .2 .25

0

0
95% CI

Estimate

95% CI

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

Stanford Science for 9th, TAKS Science for
10th/11th Grade Science Awards

Estimate

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

.5

.5

Estimate

95% CI

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

.5

0

Estimate

95% CI

.1
.2
.3
.4
Share of Students in Subject Assigned to Teacher

.5

Stanford Social Studies for 9th, TAKS Social Studies
for 10th/11th Grade Social Studies Awards

0

Stanford Language for Language Arts Awards

Each line shows local linear regression estimates of Share*Post from models that include school-year and grade-year ﬁxed eﬀects as well as controls for
school-grade-speciﬁc enrollment and lagged student test scores. Rectangular kernels are used with a bandwidth of 0.1. The dashed lines show the bounds
of the 95% conﬁdence interval that are calculated from standard errors that are clustered at the school level.

Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.2 −.15 −.1 −.05 0 .05 .1 .15 .2 .25

TAKS Math for Math Awards

Figure A-1: Local Linear Regressions of the Eﬀect of Teacher Share Post-ASPIRE on Student Achievement - Bandwidth of 0.1
Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.2 −.15 −.1 −.05 0 .05 .1 .15 .2 .25
Change in Award Impact from 0.1 Share Increase
Achievement − Standard Deviation Units
−.2 −.15 −.1 −.05 0 .05 .1 .15 .2 .25

Table A-1: Additional Tests of “Impact” of Teacher Share on Student and Teacher
Characteristics

Independent Variable

Math

Student is New to School (Grades 10, 11 Only)

-0.046∗
(0.026)

Observations

144,955

Student was Not Enrolled in District in Prior Year †

0.002
(0.009)

Observations

241,694

Number of Courses Taught

1.113
(1.036)

Observations

241,694

†

Test Subject:
English &
Language Science

Social
Studies

-0.008
(0.052)

-0.018
(0.030)

0.034
(0.034)

143,319

145,723

-0.030∗∗
(0.012)

-0.015
(0.015)

240,544

242,196

-0.149
(0.931)

0.797
(0.539)

240,472

242,001

134,576
-0.021
(0.015)
224,964
2.463∗
(1.300)
224,044

Since the data are restricted to students having achievement data from 2004, these students would have been
in the district prior, left and then returned; i.e. returning dropouts.
Each cell comes from a separate estimation of equation (8) and shows the estimate of α2 , which is the coeﬃcient
on Share ∗ P ost. Each independent variable is a dummy variable that indicates whether a student falls into
the given category except where noted. Regressions also include school-year and grade-year ﬁxed eﬀects along
with a quartic in enrollment. Standard errors clustered at the school level are in parentheses: ***,**,* indicates
statistical signiﬁcance at the 1%, 5% and 10% levels, respectively.

50

Table A-2: Analysis of Variance of Teacher Share Between and Within Teachers 2006 and Later
Test Subject:
English &
Math Language Science

Social
Studies

[1] Raw Variance
Between Teachers
Within Teachers

69%
31%

86%
14%

77%
23%

81%
19%

[2] Residual Variance - No School-Year FE
Between Teachers
Within Teachers

50%
50%

69%
31%

58 %
42%

59%
41 %

[3] Residual Variance - With School-Year FE
Between Teachers
Within Teachers

40%
60%

58%
42%

46%
54 %

44%
56 %

Percentages are calculated by conducting one-way ANOVA and
then calculating the ratio of between teacher and within teacher
sum-of-squares to total sum-of-squares. The ﬁrst row uses raw
Teacher Share across teachers. The second row uses the residuals
from a regression of Teacher Share on student gender, race, atrisk, special education, LEP, and gifted status along with lagged
achievement interacted with grade-year indicators and grade-year
ﬁxed eﬀects.The third row uses residuals from a regression including the controls in panel [2] plus school-year ﬁxed eﬀects.

51

Table A-3: Tests of Heterogeneity by Share

Independent Variable

TAKS
Math

English &
Language

Science

Social
Studies

Stanford
Math

Panel A: Estimates for Shares ≤ 0.15
Post*Share

0.699**
(0.279)

0.453***
(0.148)

0.540**
(0.246)

1.172***
(0.294)

0.151
(0.490)

Share

0.272**
(0.121)

-0.162
(0.104)

0.468**
(0.185)

0.748***
(0.255)

-0.311
(0.287)

Observations

128,014

101,336

119,081

89,008

126,824

Panel B: Estimates for Shares > 0.15
Post*Share

0.213
(0.127)

0.075
(0.062)

-0.302**
(0.141)

-0.150
(0.152)

0.139
(0.117)

Share

-0.089
(0.077)

-0.008
(0.075)

0.221
(0.146)

0.098
(0.099)

-0.231**
(0.098)

Observations

113,680

122,708

121,391

152,993

112,526

Panel C: Tests of Diﬀerences for Post*Share Between (A) and (B)
Chi2
P(Chi2 )

2.57
0.11

5.68
0.02

8.85
0.00

17.96
0.00

0.00
0.98

Source: HISD administrative data as described in the text. The math test in the ﬁrst
column is the state administered TAKS math exam. The Language exams are Stanford
tests. For 10th and 11th grade science and social studies, the TAKS exams are used,
while for 9th grade they are Stanford tests. All estimates are in terms of scale scores
standardized across the district within grade and year. Individual controls include student
gender, race, at-risk, special education, LEP, gifted status and year ﬁxed eﬀects. Standard
errors clustered at the school level are in parentheses: **,* indicates statistical signiﬁcance
at the 5% and 10% levels, respectively.

52

Table A-4: Additional Robustness Checks

TAKS
Math

Independent Variable

[1] Interactions with Grade Level:
0.268**
Post*Teacher Share
(0.121)
-0.075
Post*Teacher Share*10th
(0.182)
-0.020
Post*Teacher Share*11th
(0.128)
Observations
219,430

Test Subject:
English &
Social
Language Science Studies

Stanford
Math

0.153**
(0.057)
-0.029
(0.052)
-0.013
(0.052)
197,560

-0.008
(0.101)
0.087
(0.078)
0.037
(0.114)
208,282

-0.189
(0.116)
0.319
(0.213)
0.250
(0.268)
202,017

0.140
(0.190)
0.162
(0.247)
0.024
(0.243)
203,793

[2] Assign Students to Spring of t - 1 and Fall of t Teachers for Grade/Subjects:
that Use Stanford
0.156∗∗∗
-0.003
0.220∗∗ 0.029
Post*Teacher Share
(0.051)
(0.109)
(0.109) (0.074)
Observations
124,412
195,532 195,968 139,534
[3] Assign Students to Fall of t Teachers Only for
that Use Stanford:
0.146∗∗
Post*Teacher Share
(0.054)
Observations
112,468

Grade/Subjects
0.240∗∗
(0.115)
192,491

0.023
(0.087)
118,672

[4] Use 2002-03 Score as Lagged Achievement for
0.174∗
0.160∗∗∗
Post*Teacher Share
(0.093)
(0.055)
Observations
219,430
197,560

All Years:
0.107
0.223∗
(0.083)
(0.117)
202,017 203,793

0.032
(0.094)
208,282

0.210∗
(0.106)
242,001

0.057
(0.080)
239,350

0.011
(0.104)
190,166

[5] Unweighted Regressions:
Post*Teacher Share
Observations

0.281∗∗∗
(0.090)
241,694

0.140∗∗
(0.051)
224,044

-0.012
(0.098)
240,472

[6] Add Teacher Fixed Eﬀects (School FE Instead of School-Year FE):
Post*Teacher Share
0.332∗∗∗ 0.068
0.345∗∗∗ 0.174
0.355∗∗∗
(0.103)
(0.092)
(0.121)
(0.104) (0.119)
Observations
175,119
169,076
177,575 186,210 173,420
Source: HISD administrative data as described in the text. The math test in the ﬁrst column is
the state administered TAKS math exam. The English and Language Arts exams are Stanford
tests. For 10th and 11th grade science and social studies, the TAKS exams are used, while for
9th grade they are Stanford tests. All estimates are in terms of standardized scores. Controls
include student gender, race, at-risk, special education, LEP, and gifted status along with lagged
achievement interacted with grade-year indicators and grade-year and school-year ﬁxed eﬀects.
To ease presentation we do not show the estimate for the “teacher share” main eﬀect. Standard
errors clustered at the school level are in parentheses: ***,**,* indicates statistical signiﬁcance at
the 1%, 5% and 10% levels, respectively.

53

Table A-5: Estimates of the Eﬀect of ASPIRE on Student Test
Scores Using 2004 Department Size and Share
Panel A: Diﬀerences in Diﬀerences
Independent Variable
1
Post* 2004 Dept.
Size
1

2004 Dept. Size

TAKS
Math
0 .413∗∗
(0.155)
-0.100
(0.370)

English &
Language

Science

Social Stanford
Studies
Math

0 .147
(0.255)
1 .303∗∗
(0.522)

0 .406∗
(0.227)
0 .897
(0.581)

0 .293∗
(0.161)
0 .605∗
(0.330)

0 .753∗∗
(0.335)
-0.108
(0.311)

1
Panel B: IV Using 2004 Dept.
Size as an Instrument for Share

Independent Variable
Post*Teacher Share
Teacher Share

TAKS
Math
0 .383∗∗
(0.153)
-0.062
(0.508)

English &
Language

Science

Social Stanford
Studies
Math

0 .120
(0.140)
1 .056
(0.606)

0 .270∗∗
(0.130)
0 .924∗
(0.512)

0 .581
(0.875)
6 .149
(8.708)

0 .719∗∗
(0.351)
0 .227
(0.435)

Source: HISD administrative data as described in the text. School ﬁxed-eﬀects are
excluded as there is too little variation within schools in the instrument to generate
precise estimates with these controls. The math test in the ﬁrst column is the state
administered TAKS math exam. The Language exams are Stanford tests. For 10th and
11th grade science and social studies, the TAKS exams are used, while for 9th grade they
are Stanford tests. All estimates are in terms of scale scores standardized across the
district within grade and year. Individual controls include student gender, race, at-risk,
special education, LEP, gifted status and year ﬁxed eﬀects. Standard errors clustered
at the school level are in parentheses: **,* indicates statistical signiﬁcance at the 5%
and 10% levels, respectively.

54

Appendix B
Closed-Form Solution for Comparative Statics of
Theoretical Model
Deﬁnitions:

S̄ = θS(e1 ) + (1 − θ)S(e2 ) − µ
F = F (S̄), F ′ = F ′ (S̄), F ′′ = F ′′ (S̄)
S1 = S(e1 ), S1′ = S ′ (e1 ), S1′′ = S ′′ (e1 )
S2 = S(e2 ), S2′ = S ′ (e2 ), S2′′ = S ′′ (e2 )
C1 = C(e1 ), C1′ = C ′ (e1 ), C1′′ = C ′′ (e1 )
C2 = C(e2 ), C2′ = C ′ (e2 ), C2′′ = C ′′ (e2 )
(11)

First order conditions:

G1 = AθF ′ S1′ − C1′ = 0

(12)

G2 = A(1 − θ)F ′ S2′ − C2′ = 0

(13)

Take derivatives of G1 wrt θ, e1 and e2 :
∂G1
=AF ′ S1′ + AθF ′′ S1′ S1
∂θ
∂G1
2
=Aθ2 F ′′ (S1′ ) + AθF ′ S1′′ − C1′′
∂e1
∂G1
=Aθ(1 − θ)F ′′ S1′ S2′
∂e2

55

(14)
(15)
(16)

Take derivatives of G2 wrt θ, e1 and e2 :
∂G2
= − AF ′ S1′ + A(1 − θ)F ′′ S2′ S2
∂θ
∂G2
=Aθ(1 − θ)F ′′ S1′ S2′
∂e1
∂G2
2
=A(1 − θ)2 F ′′ (S2′ ) + A(1 − θ)F ′ S2′′ − C2′′
∂e2

(17)
(18)
(19)

Implicit function theorem implies that:
1
− ∂G
∂θ

∂G2
∂e1

2
− ∂G
∂e1
∂θ
=
∂θ
∂G1

∂G2
∂e2

∂e1

∂G2
∂e1

∂G1
∂e2

∂G2
∂e2

(20)

(21)
∂G ∂G
∂G ∂G
∂e1 − ∂θ1 ∂e22 + ∂θ2 ∂e12
= ∂G1 ∂G1 ∂G2 ∂G2
∂θ
− ∂e ∂e
∂e ∂e
1

2

Solve for elements of equation (22):

56

1

2

(22)

−

∂G1 ∂G2
2
= − A2 (1 − θ)2 F ′ S1′ F ′′ (S2′ )
∂θ ∂e2

(23)

− A2 (1 − θ) (F ′ ) S1′ S2′′
2

+ AF ′′ S1′ C ′′ (e2 )
− A2 θ(1 − θ)2 (F ′′ ) (S2′ ) S1′ S1
2

2

− A2 θ(1 − θ)2 F ′ F ′′ S2′′ S1′ S1
+ AθF ′′ S1′ S1 C2′′

∂G2 ∂G2
2
= − A2 θ(1 − θ)F ′ F ′′ (S1′ ) S2′
∂θ ∂e1

(24)

− A2 θ(1 − θ)2 (F ′′ ) (S2′ ) S1′ S2
2

∂G1 ∂G1
=A2 θ3 (1 − θ)(F ′′ )2 (S1′ )3 S2′ +
∂e1 ∂e2

(25)

A2 θ2 (1 − θ)F ′′ F ′ S1′′ S1′ S2′ −
Aθ(1 − θ)F ′′ S1′ S2′ C1′′

−

∂G2 ∂G2
= − A2 θ(1 − θ)3 (F ′′ )2 (S2′ )3 S1′ −
∂e1 ∂e2

(26)

A2 θ(1 − θ)2 F ′′ F ′ S2′′ S1′ S2′ +
Aθ(1 − θ)F ′′ S1′ S2′ C2′′
(27)

57

Solution for numerator:
[
∂G1 ∂G2 ∂G2 ∂G2
2
2
2
′
−
+
= − A (1 − θ)S1 (1 − θ)F ′ F ′′ (S2′ ) + (F ′ ) S2′′
∂θ ∂e2
∂θ ∂e1

(28)

+θ(1 − θ) (F ′′ ) (S2′ ) S1 + θF ′ F ′′ S2′′ S1
]
2
+θF ′ F ′′ S1′ S2′ + θ(1 − θ) (F ′′ ) S2′ S2
2

2

+ AC2′′ S1′ F ′′ [1 + θS1 ]

Solution for denominator:
∂G2 ∂G2 ∂G1 ∂G2
−
=Aθ(1 − θ)F ′′ S1′ S2′ ×
∂θ ∂e1
∂θ ∂e2
[ 2 ′′ ′ 2
Aθ F (S1 ) + AθF ′ S1′′ − C1′′
A(1 − θ)2 F ′′ (S2′ )2 + A(1 − θ)F ′ S2′′ + C2′′

(29)

]

Full closed form solution:

∂e1
∂θ

=

[
]
2
2
−A2 (1−θ)S1′ (1−θ)F ′ F ′′ (S2′ ) +(F ′ )2 S2′′ +θ(1−θ)(F ′′ )2 (S2′ ) S1 +θF ′ F ′′ S2′′ S1 +θF ′ F ′′ S1′ S2′ +θ(1−θ)(F ′′ )2 S2′ S2
Aθ(1−θ)F ′′ S1′ S2′ [Aθ2 F ′′ (S1′ )2 +AθF ′ S1′′ −C1′′ A(1−θ)2 F ′′ (S2′ )2 +A(1−θ)F ′ S2′′ +C2′′ ]

58

