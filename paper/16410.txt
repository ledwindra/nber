NBER WORKING PAPER SERIES

DYNAMICS OF INFORMATION EXCHANGE IN ENDOGENOUS SOCIAL NETWORKS
Daron Acemoglu
Kostas Bimpikis
Asuman Ozdaglar
Working Paper 16410
http://www.nber.org/papers/w16410

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2010

We thank seminar participants at Columbia, Cornell, Microsoft Research, MIT, Stanford, University
of Chicago, University of Michigan, the SED Meeting at Montreal and the Quebec Workshop on the
Economics of Social Networks for useful comments and suggestions. The views expressed herein
are those of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
¬© 2010 by Daron Acemoglu, Kostas Bimpikis, and Asuman Ozdaglar. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including ¬© notice, is given to the source.

Dynamics of Information Exchange in Endogenous Social Networks
Daron Acemoglu, Kostas Bimpikis, and Asuman Ozdaglar
NBER Working Paper No. 16410
September 2010
JEL No. D83,D85
ABSTRACT
We develop a model of information exchange through communication and investigate its implications
for information aggregation in large societies. An underlying state determines payoffs from different
actions. Agents decide which others to form a costly communication link with incurring the associated
cost. After receiving a private signal correlated with the underlying state, they exchange information
over the induced communication network until taking an (irreversible) action. We define asymptotic
learning as the fraction of agents taking the correct action converging to one in probability as a society
grows large. Under truthful communication, we show that asymptotic learning occurs if (and under
some additional conditions, also only if) in the induced communication network most agents are a
short distance away from "information hubs", which receive and distribute a large amount of information.
Asymptotic learning therefore requires information to be aggregated in the hands of a few agents.
We also show that while truthful communication may not always be a best response, it is an equilibrium
when the communication network induces asymptotic learning. Moreover, we contrast equilibrium
behavior with a socially optimal strategy profile, i.e., a profile that maximizes aggregate welfare. We
show that when the network induces asymptotic learning, equilibrium behavior leads to maximum
aggregate welfare, but this may not be the case when asymptotic learning does not occur. We then
provide a systematic investigation of what types of cost structures and associated social cliques (consisting
of groups of individuals linked to each other at zero cost, such as friendship networks) ensure the emergence
of communication networks that lead to asymptotic learning. Our result shows that societies with too
many and sufficiently large social cliques do not induce asymptotic learning, because each social clique
would have sufficient information by itself, making communication with others relatively unattractive.
Asymptotic learning results if social cliques are neither too numerous nor too large, in which case
communication across cliques is encouraged.
Daron Acemoglu
Department of Economics
MIT, E52-380B
50 Memorial Drive
Cambridge, MA 02142-1347
and CIFAR
and also NBER
daron@mit.edu
Kostas Bimpikis
MIT - Sloan School of Management
50 Memorial Drive
Cambridge, MA 02142
kostasb@MIT.EDU

Asuman Ozdaglar
Department of Electrical Engineering
and Computer Science
Massachusetts Institute of Technology
77 Massachusetts Ave, E40-130
Cambridge, MA 02139
asuman@mit.edu

1

Introduction

Most social decisions, ranging from product and occupational choices to voting and political behavior,
rely on information agents gather through communication with friends, neighbors and co-workers as
well as information obtained from news sources and prominent webpages. A central question in social
science concerns the dynamics of communication and information exchange and whether such dynamics lead to the effective aggregation of dispersed information. Our objective in this paper is to develop
a tractable benchmark model to study the dynamics of belief formation and information aggregation
through communication and the choices that individuals make concerning whom to communicate with.
A framework for the study of these questions requires communication to be strategic, time-consuming
and/or costly, since otherwise all information could be aggregated immediately by simultaneous communication among the agents. Our approach focuses on dynamic and costly communication (and we
also allow strategic communication, though this turns out to be less important in the present context).
An underlying state of the world determines which action has higher payoff (which is assumed to
be the same for all agents). Because of discounting, earlier actions are preferred to later ones. Each
agent receives a private signal correlated with the underlying state. In addition, she can communicate
with others, but such communication first requires the formation of a communication link, which
may be costly. Therefore, our framework combines elements from models of social learning and
network formation. The network formation decisions of agents induce a communication graph for
the society. Thereafter, agents communicate with those whom they are connected to until they take
an irreversible action. Crucially, information acquisition takes time because the ‚Äúneighbors‚Äù of an
agent with whom she communicates acquire more information from their own neighbors over time.
Information exchange will thus be endogenously limited by two features: the communication network
formed at the beginning of the game, which allows communication only between connected pairs, and
discounting, which encourages agents to take actions before they accumulate sufficient information.
We characterize the equilibria of this network formation and communication game and then investigate the structure of these equilibria as the society becomes large (i.e., for a sequence of games). Our
main focus is on how well information is aggregated, which we capture with the notion of asymptotic
learning. We say that there is asymptotic learning if the fraction of agents taking the correct action
converges to one (in probability) as the society becomes large.
Our analysis proceeds in several stages. First, we take the communication graph as given and
assume that agents are non-strategic in their communication, i.e., they disclose truthfully all the
information they possess when communicating. Under these assumptions, we provide a condition that
is sufficient and (under an additional mild assumption) necessary for asymptotic learning. Intuitively,
this condition requires that most agents are a short distance away from information hubs, which are

1

agents that have a very large (in the limit, infinite) number of connections.1 Two different types
of information hubs can be conduits of asymptotic learning in our benchmark model. The first are
information mavens who receive communication from many other agents, enabling them to aggregate
information. If most agents are close to an information maven, asymptotic learning is guaranteed.
The second type of hubs are social connectors who communicate to many agents, enabling them to
spread their information widely.2 Social connectors are only useful for asymptotic learning, if they
are close to mavens so that they can distribute their information. Thus, asymptotic learning is also
obtained if most agents are close to a social connector, who is in turn a short distance away from a
maven. The intuition for why such information hubs and almost all agents being close to information
hubs are necessary for asymptotic learning is instructive: were it not so, a large fraction of agents
would prefer to act before waiting for sufficient information to arrive. But then a nontrivial fraction
of those would take the incorrect action, and moreover, they would also disrupt the information flow
for the agents to whom they are connected. The advantage of the first part of our analysis is that it
enables a relatively simple characterization of equilibria and the derivation of intuitive conditions for
asymptotic learning.
Second, we show that even if individuals misreport their information (which they may want to do
in order to delay the action of their neighbors and obtain more information from them in future communication), it is an equilibrium of the strategic communication game to report truthfully whenever
truthful communication leads to asymptotic learning. Interestingly, the converse is not necessarily
true: strategic communication may lead to asymptotic learning in some special cases in which truthful
communication precludes learning. From a welfare perspective, we show a direct connection between
asymptotic learning and the maximum aggregate welfare that can be achieved by any strategy profile:
when asymptotic learning occurs, all equilibria are (asymptotically) socially efficient, i.e., they achieve
the maximum welfare. However, when asymptotic learning does not occur, equilibrium behavior can
lead to inefficiencies that arise from the fact that agents do not internalize the positive effect of delaying their action and continuing information exchange. Thus, our analysis identifies a novel information
externality that is a direct product of the agents being embedded in a network: the value of an agent
to her peers does not only originate from her initial information but also from the paths she creates
between different parts of the network through her social connections. It is precisely the destruction
of these paths when the agent takes an action that may lead to a welfare loss in equilibrium.
Our characterization results on asymptotic learning can be seen both as ‚Äúpositive‚Äù and ‚Äúnegative‚Äù.
On the one hand, to the extent that most individuals obtain key information from either individuals or
news sources (websites) approximating such hubs, efficient aggregation of information may be possible
1

We also derive conditions under which , Œ¥-asymptotic learning occurs at an equilibrium strategy profile. We say that
, Œ¥-asymptotic learning occurs when at least 1 ‚àí  fraction of the population takes an -optimal action with probability
at least 1 ‚àí Œ¥.
2
Both of these terms are inspired by Gladwell (2000).

2

in some settings. We show in particular that hierarchical graph structures where agents in the higher
layers of the hierarchy can communicate information to many agents at lower layers lead to asymptotic
learning.3 On the other hand, communication structures that do not feature such hubs appear more
realistic in most contexts, including communication between friends, neighbors and co-workers.4 Our
model thus emphasizes how each agent‚Äôs incentive to act sooner rather than later makes information
aggregation significantly more difficult.
Third, armed with the analysis of information exchange over a given communication network, we
turn to the study of the endogenous formation of this network. We assume that the formation of
communication links is costly, though there also exist social cliques, groups of individuals that are
linked to each other at zero cost. These can be thought of as ‚Äúfriendship networks‚Äù that are linked
for reasons unrelated to information exchange and thus act as conduits of such exchange at low cost.
Agents have to pay a cost at the beginning in order to communicate (receive information) from those
who are not in their social clique. Even though network formation games have several equilibria, the
structure of our network formation and information exchange game enables us to obtain relatively
sharp results on what types of societies lead to endogenous communication networks that ensure
asymptotic learning. In particular, we show that societies with too many (disjoint) and sufficiently
large social cliques induce behavior inconsistent with asymptotic learning. The reason why relatively
large social cliques may discourage efficient aggregation of information is that because they have
enough information, communication with others (from other social cliques) becomes unattractive, and
as a consequence, the society gets segregated into a large number of disjoint social cliques that do not
share information. In contrast, asymptotic learning obtains in equilibrium if social cliques are neither
too numerous nor too large so that it is worthwhile for at least some members of these cliques to
communicate with members of other cliques, forming a structure in which information is shared across
(almost) all members of the society.
These results also illustrate an interesting feature of the information exchange process: an agent‚Äôs
willingness to perform costly search (which here corresponds to forming a link with another social
clique) is decreasing with the precision of the information that is readily accessible to her. This gives a
natural explanation for informational segregation: agents do not internalize the benefits for the group
of forming an additional link, leading to a socially inefficient information exchange structure. It further
suggests a form of informational Braess‚Äô paradox,5 whereby the introduction of additional information
may have adverse effects for the welfare of a society by discouraging the formation of additional links
for information sharing (see also Morris and Shin (2002) and Duffie, Malamud, and Manso (2009) for
3

An additional challenge when significant information is concentrated in the hands of a few hubs may arise because
of misalignment of interests, which our approach ignores.
4
In particular, the popular (though not always empirically plausible) random graph models such as preferential
attachment and Poisson (ErdoÃãs-Renyi) graphs do not lead to asymptotic learning.
5
In the original Braess‚Äô paradox, the addition of a new road may increase the delays faced by all motorists in a Nash
equilibrium.

3

a related result). Consider, for example, the website of a film critic that can be viewed as a good
but still imprecise information source (similar to a reasonable-sized social clique in our model). Other
agents can access the critic‚Äôs information and form an opinion about a movie quickly. However, this
precludes information sharing among the agents and may lead to a decrease in the aggregate welfare.
Our paper is related to several strands of the literature on social and economic networks. First,
it is related to the large and growing literature on social learning. Much of this literature focuses
on Bayesian models of observational learning, where each individual learns from the actions of others
taken in the past. A key impediment to information aggregation in these models is the fact that actions
do not reflect all of the information that an individual has and this can induce a pattern reminiscent
to a ‚Äúherd,‚Äù where individuals ignore their own information and copy the behavior of others (see,
for example, Bikhchandani, Hirshleifer, and Welch (1992), Banerjee (1992), and Smith and S√∏rensen
(2000), as well as Bala and Goyal (1998), for early contributions, and Smith and S√∏rensen (2010),
Banerjee and Fudenberg (2004) and Acemoglu, Dahleh, Lobel, and Ozdaglar (2010) for models of
Bayesian learning with richer observational structures). While observational learning is important in
many situations, a large part of information exchange in practice is through communication.
Several papers in the literature study communication, though typically using non-Bayesian or ‚Äúmyopic‚Äù rules (for example, Ellison and Fudenberg (1995), DeMarzo, Vayanos, and Zwiebel (2003) and
Golub and Jackson (2010)). A major difficulty faced by these approaches, often precluding Bayesian
and dynamic game theoretic analysis of learning in communication networks, is the complexity of
updating when individuals share their ex-post beliefs (because of the difficulty of filtering out common sources of information). We overcome this difficulty by adopting a different approach, whereby
individuals can directly communicate their signals and there is no restriction on the total ‚Äúbits‚Äù of
communication. This leads to a tractable structure for updating of beliefs and enables us to study perfect Bayesian equilibria of a dynamic game of network formation, communication and decision-making.
It also reverses one of the main insights of these papers, also shared by the pioneering social learning
work by Bala and Goyal (1998), that the presence of ‚Äúhighly connected‚Äù or ‚Äúinfluential‚Äù agents, or
what Bala and Goyal (1998) call a ‚Äúroyal family,‚Äù acts as a significant impediment to the efficient
aggregation of information. On the contrary, in our model the existence of such highly connected
agents (information hubs, mavens or connectors) is crucial for the efficient aggregation of information.
Moreover, the existence of such ‚Äúhighly connected‚Äù also reduces incentives for non-truthful communication, and is the key input into our result that truthful communication can be an equilibrium. The
recent paper by Duffie, Malamud, and Manso (2009) is also noteworthy: in their model agents are
randomly matched according to endogenously determined search intensities, and because they focus
on an environment with a continuum of agents, communication of beliefs in their setup is equivalent
to exchanging signals, and thus enables them to avoid the issues arising in the previous literature.
Their main focus is on characterizing equilibrium search intensities as a function of the information
4

that an agent already has access to. In contrast to our work, there is no explicit network structure.
Finally, Mobius, Phan, and Szeidl (2010) empirically compare a non-Baysian model of communication
(similar to the one adopted by Golub and Jackson (2010)) with a model in which, similar to ours,
signals are communicated and agents are Bayesian. Although their study is not entirely conclusive
on whether agents behave according to one or the other model, their evidence broadly supports the
Bayesian alternative.
Our work is also related to the growing literature on network formation, since communication takes
place over endogenously formed networks. Bala and Goyal (2000) model strategic network formation as
a non-cooperative game and study its equilibria under various assumptions on the benefits of forming
a link. In particular, they distinguish between one-way and two-way flow of benefits, depending
on whether a link benefits only the agent that decides to form it or both participating agents. They
identify a number of simple structures that arise at equilibrium: the empty network, the wheel, the star
and the complete network. More recently, Galeotti, Goyal, and Kamphorst (2006) and Galeotti (2006)
study the role of heterogeneity among agents in the network structures that arise at equilibrium.
Closer to our work is Hojman and Szeidl (2008) who study a network formation model where the
benefits from connecting to other agents have decreasing returns to scale (which is also the case in
our model of information exchange because of endogenous reasons). The main focus of the network
formation literature is on characterizing equilibrium structures and comparing them with patterns
observed in real world networks (e.g., small distances between agents, high centrality etc.). However,
in most of the literature the benefits and costs associated with forming a link are exogenous. A novelty
in our work is that the benefits of forming links are endogenously determined through the subsequent
information exchange. Our focus is also different: although we also obtain characterization results
on the shape of the network structures that arise in equilibrium (which are similar to those in the
literature), our focus is on whether these structure lead to asymptotic learning. Interestingly, while
network formation games have a large number of equilibria, the simple structure of our model enables
us to derive relatively sharp results about environments in which the equilibrium networks lead to
asymptotic learning.
Finally, our paper is related to the literature on strategic communication, pioneered by the cheap
talk framework of Crawford and Sobel (1982). While cheap talk models have been used for the study
of information aggregation with one receiver and multiple senders (e.g. Morgan and Stocken (2008))
and multiple receivers and single sender (e.g. Farrell and Gibbons (1989)), most relevant to our paper
are two recent papers that consider strategic communication over general networks, Galeotti, Ghiglino,
and Squintani (2010) and Hagenbach and Koessler (2010). A major difference between these works
and ours is that we consider a model where communication is allowed for more than one time period,
thus enabling agents to receive information outside their immediate neighborhood (at the cost of a
delayed decision) and we also endogenize the network over which communication takes place. On the
5

other hand, our framework assumes that an agent‚Äôs action does not directly influence others‚Äô payoffs,
while such payoff interactions are the central focus of Galeotti, Ghiglino, and Squintani (2010) and
Hagenbach and Koessler (2010). Our paper is also related to the existing work by Ambrus, Azevedo,
and Kamada (2010), where the sender and the receiver communicate strategically through a chain of
intermediators. Their primary focus is information intermediation, thus communication takes place
over multiple rounds but it is restricted on a ordered line from the sender to the receiver, where each
agent only sends information once.
The rest of the paper is organized as follows. Section 2 develops a general model of information
exchange among rational agents, that are embedded in a communication network. Also, it introduces
the two main environments we study. Section 3 contains our main results on social learning given
a communication graph. It also includes a welfare discussion that draws the connection between
learning and efficient communication. Finally, it illustrates how our results can be applied to a number
of random graph models. Section 4 incorporates endogenous network formation to the information
exchange model. Our main result in this section shows the connection between incentives to form
communication links and asymptotic learning. Section 5 concludes. All proofs are presented in the
Appendix.

2

A Model of Information Exchange in Social Networks

In the first part of the paper, we focus on modelling information exchange among agents over a given
communication network. In the second part (Section 4), we investigate the question of endogenous
formation of this network. We start by presenting the information exchange model for a finite set
N n = {1, 2, ¬∑ ¬∑ ¬∑ , n} of agents. We also describe the limit economy as n ‚Üí ‚àû.

2.1

Actions, Payoffs and Information

Each agent i ‚àà N n chooses an irreversible action xi ‚àà R. Her payoff depends on her action and an
underlying state of the world Œ∏ ‚àà R, which is an exogenous random variable. In particular, agent i‚Äôs
payoff when she takes action xi and the state of the world is Œ∏ is given by f (xi , Œ∏) = œÄ ‚àí (x ‚àí Œ∏)2 ,
where œÄ is a constant.
The state of the world Œ∏ is unknown and agents observe noisy signals about it. In particular, we
assume that Œ∏ is drawn from a Normal distribution with known mean ¬µ and precision œÅ. Each agent
receives a private signal si = Œ∏ + zi , where the zi ‚Äôs are idiosyncratic and independent from one another
and Œ∏, with common mean ¬µÃÑ (normalized to 0) and precision œÅÃÑ.

2.2

Communication

Our focus is on information aggregation, when agents are embedded in a network that imposes communication constraints. In particular, agent i forms beliefs about the state of the world from her
private signal si , as well as information she obtains from other agents through a given communication

6

network Gn , which, as will be described shortly, represents the set of communication constraints imposed on them. We assume that time t ‚àà [0, ‚àû) is continuous and there is a common discount rate
r > 0. Communication times are stochastic. In particular, communication times are exponentially
distributed with parameter Œª > 0.6 At a given time instant t, agent i decides whether to take action
xi (and receive payoff f (xi , Œ∏) discounted by e‚àírt ) or ‚Äúwait‚Äù to obtain more information in subsequent
communication rounds from her peers. Throughout the rest of the paper, we say that the agent ‚Äúexits‚Äù
at time t, if she chooses to take the irreversible action at time t. Discounting implies that an earlier
exit is preferred to a later one. We define Uin as the discounted payoff of agent i (from the viewpoint
of time t = 0) when the size of the society is n. For example, when the underlying state is Œ∏ and the
agent takes action xi at time t, we would have that

Uin = e‚àírt œÄ ‚àí (xi ‚àí Œ∏)2 .
As mentioned above, each agent obtains information from other agents through a communication
network represented by a directed graph Gn = (N n , E n ), where E n is the set of directed edges with
which agents are linked. We say that agent j can obtain information from i or that agent i can send
n denote the
information to j if there is an edge from i to j in graph Gn , i.e., (i, j) ‚àà E n . Let Ii,t
n denote the set of all possible information sets. Then, for
information set of agent i at time t and Ii,t

every pair of agents i, j, such that (i, j) ‚àà E n , we say that agent j communicates with agent i or that
agent i sends a message to agent j, and define the following mapping
n
mnij,t : Ii,t
‚Üí Mnij,t for (i, j) ‚àà E n ,

where Mnij,t ‚äÜ Rn denotes the set of messages that agent i can send to agent j at time t. Note that
without loss of generality the k-th component of mnij,t represents the information that agent i sends to
agent j at time t regarding the signal of agent k 7 . Moreover, the definition of mnij,t captures the fact that
communication is directed and is only allowed between agents that are linked in the communication
network, i.e., j communicates with i if and only if (i, j) ‚àà E n . The direction of communication should
be clear: when agent j communicates with agent i, then agent i sends a message to agent j, that could
in principle depend on the information set of agent i as well as the identity of agent j.
Importantly, we assume that the cardinality (‚Äúdimensionality‚Äù) of Mnij,t is such that communication can take the form of agent i sharing all her information with agent j. This has two key
implications. First, an agent can communicate (indirectly) with a much larger set of agents than just
her immediate neighbors, albeit with a time delay. For example, the second time agent j communicates
6

Equivalently, agents ‚Äúwake‚Äù up and communicate simultaneously with their neighbors, when a Poisson clock with
rate Œª ticks.
7
As will become evident in subsequent discussion, we assume that communication involves exchange of signals and not
posterior beliefs. Moreover, information is tagged, i.e., the receiver of the message understands that its k-th component
is associated with agent k.

7

with agent i, then j can send information not just about her direct neighbors, but also their neighbors
(since presumably she obtained such information during the first communication). Second, mechanical
duplication of information can be avoided. In particular, the second time agent j communicates with
agent i, she can repeat her original signal, but this is not recorded as an additional piece of information
by agent j, since given the size of the message space Mnij,t , each piece of information is ‚Äútagged‚Äù.
This ensures that there need be no confounding of new information and previously communicated
information.
Let Tt denote the set of times that agents communicated with their neighbors before time t. That
defines the information set of agent i at time t > 0 as:
n
Ii,t
= {si , mnji,œÑ , for all œÑ ‚àà Tt and j such that (j, i) ‚àà E n }
n = {s }. In particular, the information set of agent i at time t > 0 consists of her private signal
and Ii,0
i

and all the messages her neighbors sent to i in previous communication times. Agent i‚Äôs action at
time t is a mapping from her information set to the set of actions, i.e.,
n
œÉ ni,t : Ii,t
‚Üí {‚Äúwait‚Äù} ‚à™ R.

The tradeoff between taking an irreversible action and waiting, should be clear at this point. An
agent would wait, in order to communicate indirectly with a larger set of agents and choose a better
action. On the other hand, future is discounted, therefore, delaying is costly.
We close the section with a number of definitions. We define a path between agents i and j in
network Gn as a sequence i1 , ¬∑ ¬∑ ¬∑ , iK of distinct nodes such that i1 = i, iK = j and (ik , ik+1 ) ‚àà E n for
k ‚àà {1, ¬∑ ¬∑ ¬∑ , K ‚àí 1}. The length of the path is defined as K ‚àí 1. Moreover, we define the distance of
agent i to agent j as the length of the shortest path from i to j in network Gn , i.e.,
distn (i, j) = min{length of P

P is a path from i to j in Gn }.

Finally, the k-step neighborhood of agent i is defined as
n
Bi,k
= {j

distn (j, i) ‚â§ k},

n = {i}, i.e., B n consists of all agents that are at most k links away from agent i in graph
where Bi,0
i,k

Gn . Intuitively, if agent i waits for k communication steps and all of the intermediate agents receive
n .
and communicate information truthfully, i will have access to all of the signals of the agents in Bi,k

2.3

Equilibria of the Information Exchange Game

We refer to the game defined above as the Information Exchange Game. We next define the equilibria
of the information exchange game Œìinf o (Gn ) for a given communication network Gn . We use the

8

standard notation œÉ ‚àíi to denote the strategies of agents other than i and we let œÉ i,‚àít denote the vector
of actions of agent i at all times except t. Also, let PœÉ and EœÉ denote the conditional probability,
conditional expectation respectively when agents behave according to profile œÉ.
Definition 1. An action strategy profile œÉ n,‚àó is a pure-strategy perfect Bayesian Equilibrium of the
information exchange game Œìinf o (Gn ) if for every i ‚àà N n and time t, œÉ n,‚àó
i,t maximizes the expected
payoff of agent i given the strategies of other agents œÉ n,‚àó
‚àíi , i.e.,
œÉ n,‚àó
i,t ‚àà arg

max
y‚àà{‚Äúwait‚Äù}‚à™R

n
E((y,œÉn,‚àó ),œÉn,‚àó ) (Uin Ii,t
).
i,‚àít
‚àíi

We denote the set of equilibria of this game by IN F O(Gn ).
For the remainder, we refer to a pure-strategy perfect Bayesian Equilibrium simply as an equilibrium (we do not study mixed strategy equilibria). It is important to note here that although equilibria
depend on the discount rate r, we do not explicitly condition on r (through the use of a subscript) for
convenience.
If agent i decides to exit and take an action at time t, then the optimal action would be:
n
n
xn,‚àó
i,t = arg max E[f (x, Œ∏) Ii,t ] = E[Œ∏ Ii,t ],
x

where the second equality holds as f (x, Œ∏) = œÄ ‚àí (x ‚àí Œ∏)2 . Since actions are irreversible, the agent‚Äôs
decision problem reduces to determining the timing of her action. It is straightforward to see that
at equilibrium an agent takes the irreversible action immediately after some communication step
concludes. Thus, an equilibrium strategy profile œÉ induces an equilibrium timing profile œÑ n,œÉ , where
œÑ n,œÉ
designates the communication step after which agent i exits by taking an irreversible action. The
i
œÑ notation is convenient to use for the statement of some of our results below. Finally, similar to
n , we define the k-step neighborhood of agent i under equilibrium œÉ as follows: a path P œÉ between
Bi,k

agents i and j in Gn under œÉ is a sequence i1 , ¬∑ ¬∑ ¬∑ , iK of distinct nodes such that i1 = i, iK = j,
(ik , ik+1 ‚àà E n ) and œÑ n,œÉ
ik ‚â• k ‚àí 1, which ensures that the information from j will reach agent i before
any of the agents in the path take an irreversible action. Then, we can define
distn,œÉ (i, j) = min{length of P œÉ

P œÉ is a path from i to j in Gn under equilibrium œÉ}

and
n,œÉ
Bi,k
= {j

2.4

distn,œÉ (j, i) ‚â§ k}.

Assumptions on the Information Exchange Process

The communication model described in Section 2.2 is fairly general. In particular, the model does not
restrict the set of messages that an agent can send. Throughout, we maintain the assumption that the
communication network Gn is common knowledge. Also, we focus on the following two environments
9

4

2

4

1

2

s3
3

5

4

s4
s2

1

3

7

(a) Time t = 0.

1

(s6 , s7 )
5

6

I1,0 = (s1 )

s3

s5
s6

6

2

3

s7
7

6

I1,1 = (s1 , s2 , s4 , s5 )
(b) First communication step.

5

7

I1,2 = (s1 , s2 , s4 , s5 , s3 , s6 , s7 )
(c) Second communication step.

Figure 1: The information set of agent 1 under truthful communication.
defined by Assumptions 1 and 2 respectively.
Assumption 1 (Truthful Communication). Communication between agents is truthful, i.e.,
 n
if |Tt | ‚â§ œÑ n,œÉ
mÃÇ ij,t
i
mnij,t =
otherwise.
mÃÇn ij,œÑ n,œÉ
i
and

(mÃÇn

ij,t )`

=

s`
‚ààR

if distn,œÉ
i,` ‚â§ |Tt |
otherwise

Intuitively, this assumption compactly imposes three crucial features: (1) Communication takes
place by sharing signals, so that when agent j communicates with agent i at time t, then agent i
sends to j all the information agent i has obtained thus far (refer to Figure 1 for an illustration of the
communication process centered at a particular agent); (2) Agents cannot strategically manipulate the
messages they sent, i.e., an agent‚Äôs private signal is hard information. Moreover, they cannot refuse to
disclose the information they possess; (3) When an agent takes an irreversible action, then she no longer
obtains new information and, thus, can only communicate the information she has obtained until the
time of her decision. The latter feature captures the fact that an agent, who engages in information
exchange to make a decision, would have weaker incentives to collect new information after reaching
that decision. Nevertheless, she can still communicate the information she had previously obtained to
other agents. An interesting consequence of this feature is that it imposes dynamically new constraints
to communication: agent i can communicate with agent j only if there is a directed path between
them in the original communication network Gn and the agents in the path do not exit early. We call
this type of communication truthful to stress the fact that the agents cannot strategically manipulate
the information they communicate.8 We discuss the implications of relaxing Assumption 1 by allowing
8
Yet another variant of this assumption would be that agents exit the social network after taking an action and stop
communicating entirely. In this case, the results are essentially identical when their action is observed by their neighbors.

10

strategic communication in Subsection 3.4.

2.5

Learning in Large Societies

We are interested in whether equilibrium behavior leads to information aggregation. This is captured
by the notion of ‚Äúasymptotic learning‚Äù, which characterizes the behavior of agents over communication
networks with growing size.
n
n
n
We consider a sequence of communication networks {Gn }‚àû
n=1 , where G = {N , E }, and refer

to this sequence of communication networks as a society. A sequence of communication networks
induces a sequence of information exchange games, and with a slight abuse of notation we use the
term equilibrium to denote a sequence of equilibria of the information exchange games, or of the society
n ‚àû
n
n
{Gn }‚àû
n=1 . We denote such an equilibrium by œÉ = {œÉ }n=1 , which designates that œÉ ‚àà IN F O(G ) for

all n. For any fixed n ‚â• 1 and any equilibrium of the information exchange game œÉ n ‚àà IN F O(Gn ),
we introduce the indicator variable:

1 if agent i takes an action that is -close to the optimal,
Min, =
0 otherwise.

(1)

In other words, Min, = 1 (for some ) if and only if agent i chooses irreversible action xi , such that
|xi ‚àí Œ∏| ‚â§ .
Next definition introduces , Œ¥-asymptotic learning for a given society.9
Definition 2. We say that , Œ¥-asymptotic learning occurs in society {Gn }‚àû
n=1 along equilibrium œÉ if
we have:

"
lim PœÉ

n‚Üí‚àû

#
!
n
1X
(1 ‚àí Min, ) >  < Œ¥.
n
i=1

This definition states that , Œ¥-asymptotic learning occurs when the probability that at least (1 ‚àí )fraction of the agents take an action that is -close to the optimal action (as the society grows infinitely
large) is at least 1 ‚àí Œ¥.
Definition 3. We say that perfect asymptotic learning occurs in society {Gn }‚àû
n=1 along equilibrium
œÉ if we have:
"
lim PœÉ

n‚Üí‚àû

#
!
n
1X
n,
(1 ‚àí Mi ) >  = 0.
n
i=1

for any  > 0.
Perfect asymptotic learning is naturally a stronger definition (corresponding to  and Œ¥ being
arbitrarily small in the definition of , Œ¥-asymptotic learning) and requires all but a negligible fraction
of the agents taking the optimal action in the limit as n ‚Üí ‚àû.
However, if their action is not observable, then the analysis needs to be modified in particular, there exist other equilibria
where several agents might exit together expecting others to exit. We do not analyze these variants in the current version
to save space.
9
Note that we could generalize Definition 2 by introducing
yet another
and study , Œ¥, Œ∂-asymptotic learning,
 P
 parameter

n,
in which case we would require that limn‚Üí‚àû PœÉ n1 n
i=1 (1 ‚àí Mi ) > Œ∂ < Œ¥.

11

3

Learning and Efficient Communication

In this section, we present our main results on learning and discuss their implications for the aggregate
welfare. Before doing so, we discuss the decision problem of a single agent, i.e., determining the best
time to take an irreversible action given that the rest of the agents behave according to strategy profile
œÉ. Later, we contrast the single agent problem with that of a social planner, whose objective is to
maximize the expected aggregate welfare. The analysis in the next three subsections assumes that
communication is truthful (cf. Assumption 1).

3.1

Agent i‚Äôs problem

The (non-discounted) expected payoff of agent i taking an action after observing k truthful private
signals (including her own) is given by:
œÄ‚àí

1
,
œÅ + œÅÃÑk

where recall that œÅ, œÅÃÑ are the precisions of the state Œ∏ and the idiosyncratic noise respectively. To
see this, note that if agent i takes her irreversible action, then the optimal such action would be
n ] and the associated non-discounted payoff would be equal to:
Œ∏ÃÇ = E[Œ∏ Ii,t

n
n
E[œÄ ‚àí (Œ∏ÃÇ ‚àí Œ∏)2 Ii,t
] = œÄ ‚àí var(Œ∏ÃÇ ‚àí Œ∏ Ii,t
) = œÄ ‚àí var

k
X
s(i)
i=1

k

!
n
‚àí Œ∏ Ii,t

=œÄ‚àí

1
,
œÅ + œÅÃÑk

where s(i) denotes the i-th signal observed by the agent and Œ∏ÃÇ is equal to the sum of k private signals
normalized by k.
n and assuming
By the principle of optimality, the value function for agent i at information set Ii,t

that the rest of the agents behave according to profile œÉ is given by:
(
1
œÄ ‚àí œÅ+œÅÃÑk
(when she takes the optimal irreversible action),
n,œÉ
n n
i,t
EœÉ (Ui Ii,t ) = max
‚àírdt
n
n
n
e
E[EœÉ (Ui Ii,t+dt ) Ii,t ] (when she decides to wait, i.e., x = ‚Äúwait‚Äù),
n,œÉ
where ki,t
denotes the number of distinct private signals agent i has observed up to time t. The first

line is equal to the expected payoff for the agent when she chooses the optimal irreversible action
n , i.e., E[Œ∏|I n ], and she has observed k n,œÉ private signals, while the second
under information set Ii,t
i,t
i,t

line is equal to the discounted expected continuation payoff.
The following lemma states that an agent‚Äôs optimal action takes the form of a threshold rule: there
n,œÉ ‚àó
exists a threshold (ki,T
) , such that an agent decides to take an irreversible action at time t as long
|t|
n,œÉ ‚àó
as she has observed more that (ki,T
) private signals. Like all other results in the paper, the proof
|t|

of this lemma is provided in the Appendix.
Lemma 1. Suppose Assumption 1 holds. Given communication network Gn and equilibrium œÉ ‚àà
n,œÉ ‚àó ‚àû
IN F O(Gn ), there exists a sequence of signal thresholds for each agent i, {(ki,œÑ
) }œÑ =0 , that depend on

the current communication round, the identity of the agent i, the communication network Gn and œÉ
12

n by taking action xn (I n ) defined
such that agent i maximizes her expected utility at information set Ii,t
i,t i,t

as
n
)
xni,t (Ii,t


=

n ],
E[Œ∏ Ii,t
‚Äúwait‚Äù,

n,œÉ
n,œÉ ‚àó
if ki,t
‚â• (ki,|T
) ,
t|
otherwise,

A consequence of Lemma 1 is that an equilibrium strategy profile œÉ defines both a time in which
agent i acts (immediately after communication step œÑ n,œÉ
i ), but also the number of signals that agent i
has access to when she acts.

3.2

Asymptotic Learning

We begin the discussion by introducing the concepts that are instrumental for asymptotic learning:
the observation radius and k-radius sets. Recall that an equilibrium of the information exchange game
on communication network Gn , œÉ n ‚àà IN F O(Gn ), induces a timing profile œÑ n,œÉ , such that agent i
takes an irreversible action after œÑ n,œÉ
communication steps. We call œÑ n,œÉ
the observation radius of
i
i
agent i under equilibrium profile œÉ n . We also define agent i‚Äôs perfect observation radius, œÑ ni , as the
communication round that agent i would exit assuming that all other agents never exit. Note that
an agent‚Äôs perfect observation radius is equilibrium independent and depends only on the network
structure. On the other hand, œÑ n,œÉ
is an endogenous object and depends on both the network as well
i
as the specific equilibrium profile œÉ. Given the notion of an observation radius, we define k-radius sets
(and similarly perfect k-radius sets) as follows.
Definition 4. Let Vkn,œÉ be defined as
n,œÉ
Bi,œÑ
n,œÉ ‚â§ k}.

Vkn,œÉ = {i ‚àà N

i

We refer to Vkn,œÉ as the k-radius set (along equilibrium œÉ). Similarly, we refer to
n
n, ‚â§ k}
Bi,œÑ

Vkn = {i ‚àà N

i

as the perfect k-radius set.
Intuitively, Vkn,œÉ includes all agents that take an action before they receive signals from more than
k other individuals at equilibrium œÉ. Equivalently, the size of their (indirect) neighborhood by the
time they take an irreversible action is no greater than k. From Definition 4 it follows immediately
that
i ‚àà Vkn,œÉ ‚áí i ‚àà Vkn,œÉ
for all k 0 > k.
0

(2)

The following proposition provides a necessary and a sufficient condition for , Œ¥-asymptotic learning
Rx
2
to occur in a society under equilibrium profile œÉ. Recall that erf (x) = ‚àö2œÄ 0 e‚àít dt denotes the error
function of the normal distribution.
Proposition 1. Suppose Assumption 1 holds. Then,
13

(a) , Œ¥-asymptotic learning does not occur in society {Gn }‚àû
n=1 under equilibrium profile œÉ if there
exists k > 0 such that
1
Œ∑ = lim sup ¬∑ Vkn,œÉ >  and erf
n‚Üí‚àû n

r


kœÅÃÑ
2

!
< (1 ‚àí Œ¥)(1 ‚àí /Œ∑).

(3)

(b) , Œ¥-asymptotic learning occurs in society {Gn }‚àû
n=1 under equilibrium profile œÉ if there exists
k > 0 such that
1
Œ∂ = lim inf
¬∑ Vkn,œÉ <  and erf
n‚Üí‚àû n

r


kœÅÃÑ
2

!
>1‚àí

Œ¥( ‚àí Œ∂)
.
1‚àíŒ∂

(4)

This proposition provides conditions such that , Œ¥-asymptotic learning takes place (or does not take
place). Intuitively, asymptotic learning is precluded if there exists a significant fraction of the society
that takes an action before seeing a large set of signals, since in this case there is a large enough
probability that these agents will take an action far away from the optimal one. The proposition
quantifies the relationship between the fraction of agents taking actions before seeing a large set of
signals and the quantities  and Œ¥. Because agents are estimating a normal random variable from noisy
observations (where the noise is also normally distributed), their probability of error is captured by
the error function erf (x), which is naturally decreasing in the number of observations. In particular,
the probability thatan
 with k signals takes an action at least  away from the optimal action
qagent
kœÅÃÑ
is no less than erf  2 (see Lemma 2 in the Appendix), and this enables us to characterize the
fraction of agents that will take an action at least  away from the optimal one in terms of the set
Vkn,œÉ as well as  and Œ¥. We thus obtain sufficient conditions for both , Œ¥-learning to take place and
for it to be incomplete. Finally, recall that equilibria and subsequently k-radius sets depend on the
discount rate (thus, different discount rates result in different answers for , Œ¥-learning).
Proposition 1 is stated in terms of the sets Vkn,œÉ , which depend on the equilibrium (as the conditioning on œÉ makes clear). Our next proposition provides a necessary and sufficient condition for perfect
asymptotic learning to occur in any equilibrium profile as a function of only exogenous objects, i.e.,
the perfect k-radius sets, that depend exclusively on the original network structure. Before stating the
proposition, we define the notion of leading agents. Intuitively, a society contains a set of leading agents
if there is a negligible fraction of the agents (the leading agents) whose actions affect the equilibrium
n |, outdeg n = {j i ‚àà B n }
behavior of a much larger set of agents (the followers). Let indegin = |Bi,1
i
j,1

denote the in-degree, out-degree of agent i in communication network Gn respectively.
Definition 5. A collection {S n }‚àû
n=1 of sets of agents is called a set of leading agents if
n

(i) There exists k > 0, such that S nj ‚äÜ Vk j for all j ‚àà J, where J is an infinite index set.
(ii) limn‚Üí‚àû

1
n

¬∑ S n = 0, i.e., the collection {S}‚àû
n=1 contains a negligible fraction of the agents as the
14

¬∑¬∑¬∑
A1

1

A

..
.

¬∑¬∑¬∑

n

B

An
Figure 2: Leading agents and asymptotic learning.
society grows.
(iii) limn‚Üí‚àû

1
n

¬∑ Sfnollow > , for some  > 0, where Sfnollow denotes the set of followers of S n . In

particular,
n
}.
Sfnollow = {i there exists j ‚àà S n such that j ‚àà Bi,1

Proposition 2. Suppose Assumption 1 holds. Then,
(i) Perfect asymptotic learning occurs in society {Gn }‚àû
n=1 in any equilibrium œÉ if
lim lim

k‚Üí‚àû n‚Üí‚àû

1
¬∑ Vkn = 0.
n

(5)

(ii) Conversely, if condition (5) does not hold for society {Gn }‚àû
n=1 and the society does not contain
a set of leading agents, then perfect asymptotic learning does not occur in any equilibrium œÉ.
Proposition 2 is not stated as an if and only if result because the fact that condition (5) does not
hold in a society does not necessarily preclude perfect asymptotic learning in the presence of leading
agents. In particular, depending on their actions, a large set of agents may exit early before obtaining
enough information to learn, or delay their actions and learn. Figure 2 clarifies this point: if the
leading agents (agents A and B) delay their irreversible decision for one communication round, then
a large fraction of the rest of the agents (agents 1 to n) may take (depending on the discount rate)
an irreversible action as soon as they communicate with the leading agents and their neighbors (i.e.,
after the second communication round concludes), thus, perfect asymptotic learning fails. However, if
the leading agents do not ‚Äúcoordinate,‚Äù then they exit early and this may lead the rest of the agents
to take a delayed (after the third communication round), but more informed action. Generally, in the
presence of leading agents, asymptotic learning may occur in all or some of the induced equilibria,
15

even when condition (5) does not hold.
In the rest of this section, we present two corollaries that help clarify the intuition of the asymptotic
learning result and identify the role of certain types of agents on information spread in a given society.
We focus on perfect asymptotic learning, since we can obtain sharper results, though we can state
similar corollaries for , Œ¥-asymptotic learning for any  and Œ¥. All corollaries are again expressed in
terms of the original network topology.10
In particular, Corollary 1 identifies a group of agents, that is crucial for a society to permit
asymptotic learning: information mavens, who have high in-degrees and can thus act as effective
aggregators of information (a term inspired by Gladwell (2000)). Information mavens are one type
of hubs the importance of which is clearly illustrated by our learning results. Our next definition
formalizes this notion.
Definition 6. Agent i is called an information maven of society {Gn }‚àû
n=1 if i has an infinite in-degree,
i.e., if
lim indegin = ‚àû.

n‚Üí‚àû

n ‚àû
Let MAVEN ({Gn }‚àû
n=1 ) denote the set of mavens of society {G }n=1 .
,n
For any agent j, let dMAVEN
denote the shortest distance defined in communication network Gn
j
n
between j and a maven k ‚àà MAVEN ({Gn }‚àû
n=1 ). Finally, let W denote the set of agents that are at

distance at most equal to their perfect observation radius from a maven in communication network
Gn , i.e., W n = {j

,n
dMAVEN
‚â§ œÑ nj }.
j

The following corollary highlights the importance of information mavens for asymptotic learning.
Informally, it states that if almost all agents have a short path to a maven, then asymptotic learning
occurs.
Corollary 1. Suppose Assumption 1 holds. Then, asymptotic learning occurs in society {Gn }‚àû
n=1 if
lim

n‚Üí‚àû

1
¬∑ W n = 1.
n

Corollary 1 thus clarifies that asymptotic learning is obtained when there are information mavens
and almost all agents are at a ‚Äúshort distance‚Äù away from one (less than their observation radius).
As mentioned in the Introduction, a second type of information hub also plays an important role
in asymptotic learning. While mavens have high in-degree and are thus able to effectively aggregate
dispersed information, they may not be in the right position to distribute this aggregated information.
If so, even in a society that has several information mavens, a large fraction of the agents may not
benefit from their information. Social connectors, on the other hand, are defined as agents with a high
10

The corollaries are stated under the additional assumption, that the in-degree of an agent is non-decreasing with n.
This is simply a technicality that allows us to simplify the statement of the corollaries.

16

out-degree, and thus play the role of spreading the information aggregated by the mavens. Before
stating the proposition, we define social connectors.
Definition 7. Agent i is called a social connector of society {Gn }‚àû
n=1 if i has an infinite out-degree,
i.e., if
lim outdegin = ‚àû.

n‚Üí‚àû

The following corollary illustrates the role of social connectors for asymptotic learning.
Corollary 2. Suppose Assumption 1 holds. Consider a society {Gn }‚àû
n=1 , such that the set of information mavens does not grow at the same rate as the society itself, i.e.,
lim

n‚Üí‚àû

MAVEN ({Gn }‚àû
n=1 )
= 0.
n

Then, for asymptotic learning to occur, the society should contain a social connector within a short
distance to a maven, i.e.,
,n
dMAVEN
‚â§ œÑ ni , for some social connector i.
i

Corollary 2 thus states that unless a large fraction of the agents belongs to the set of mavens and,
subsequently, the rest can obtain information directly from a maven, then, information aggregated
at the mavens is spread through the out-links of a connector (note that an agent can be both a
maven and a connector). These two corollaries highlight two ways in which society can achieve perfect
asymptotic learning. First, it may contain several information mavens who not only collect and
aggregate information but also distribute it to almost all the agents in the society. Second, it may
contain a sufficient number of information mavens, who pass their information to social connectors,
and almost all the agents in the society are a short distance away from social connectors and thus
obtain accurate information from them. This latter pattern has a greater plausibility in practice than
one in which the same agents collect and distribute dispersed information. For example, if a website or
a news source can rely on information mavens (journalists, researchers or analysts) to collect sufficient
information and then reach a large number of individuals, then information can be aggregated.
The results summarized in Propositions 1 and 2 as well as in Corollaries 1 and 2 can be seen
both as positive and negative, as already noted in the Introduction. On the one hand, communication
structures that do not feature information mavens (or connectors) do not lead to perfect asymptotic
learning, and information mavens may be viewed as unrealistic or extreme. On the other hand, as
already noted above, much communication in modern societies happens through agents that play the
role of mavens and connectors (see again Gladwell (2000)). These are highly connected agents that are
able to collect and distribute crucial information. Perhaps more importantly, most individuals obtain
17

Layer 1

Layer 2

Layer 3
Figure 3: Hierarchical Society.
some of their information from news sources, media, and websites, which exist partly or primarily for
the purpose of acting as information mavens and connectors.11

3.3

Asymptotic Learning in Random Graphs

As an illustration of the results we outlined in Subsection 3.2, we apply them to hierarchical graphs, a
class of random graphs defined below. Note that in the present section we assume that communication
n then j ‚àà B n .
networks are bidirectional, or equivalently that if agent i ‚àà Bj,1
i,1
n
Definition 8 (Hierarchical graphs). A sequence of communication networks {Gn }‚àû
n=1 , where G =

{N n , E n }, is called Œ∂-hierarchical (or simply hierarchical) if it was generated by the following process:
(i) Agents are born and placed into layers. In particular, at each step n = 1, ¬∑ ¬∑ ¬∑ , a new agent is
born and placed in layer `.
(ii) Layer index ` is initialized to 1 (i.e., the first node belongs to layer 1). A new layer is created
(and subsequently the layer index increases by one) at time period n ‚â• 2 with probability

1
,
n1+Œ∂

where Œ∂ > 0.
(iii) Finally, for every n we have
P ((i, j) ‚àà E n ) =

p
,
|N n
`|

independently for all i, j ‚àà N n that belong to the same layer `,

where N`n denotes the set of agents that belong to layer ` at step n and p scalar, such that
0 < p < 1. Moreover,
P ((i, k) ‚àà E n ) =

1
and
|N<` |

X

n
P ((i, k) ‚àà E n ) = 1 for all i ‚àà N`n , k ‚àà N<`
, ` > 1,

k‚ààN<`

n denotes the set of agents that belong to a layer with index lower than ` at step n.
where N<`
11

For example, a news website such as cnn.com acts as a connector that spreads the information aggregated by
the journalists-mavens to interested readers. Similarly, a movie review website, e.g., imdb.com, spreads the aggregate
knowledge of movie reviewers to interested movie aficionados.

18

Intuitively, a hierarchical sequence of communication networks resembles a pyramid, where the
top contains only a few agents and as we move towards the base, the number of agents grows. The
following argument provides an interpretation of the model. Agents on top layers can be thought of as
‚Äúspecial‚Äù nodes, that the rest of the nodes have a high incentive to connect to. Moreover, agents tend
to connect to other agents in the same layer, as they share common features with them (homophily).
As a concrete example, academia can be thought of as such a pyramid, where the top layer includes
the few institutions, then next layer includes academic departments, research labs and finally at the
lower levels reside the home pages of professors and students.
Proposition 3. Suppose Assumption 1 holds and consider society {Gn }‚àû
n=1 . There exist rÃÑ > 0 and a
function Œ∂(Œ∑) such that perfect asymptotic learning occurs in society {Gn }‚àû
n=1 with probability at least
1 ‚àí Œ∑, if the sequence of communication networks {Gn }‚àû
n=1 is Œ∂(Œ∑)‚àíhierarchical and the discount rate
r < rÃÑ.
The probability Œ∑ that perfect asymptotic learning fails is related here to the stochastic process
that generated the graph. The results presented provide additional insights on the conditions under
which asymptotic learning takes place. It can also be proved that the popular preferential attachment
and ErdoÃãs-Renyi graphs do not lead to asymptotic learning (we omit these results to save space). This
can be interpreted as implying that asymptotic learning is unlikely in several important networks.
Nevertheless, these network structures, though often used in practice, do not provide a good description
of the structure of many real life networks. In contrast, our results show that asymptotic learning takes
place in hierarchical graphs, where ‚Äúspecial‚Äù agents are likely to receive and distribute information to
lower layers of the hierarchy. Although this result is useful in pointing out certain structures where
information can be aggregated efficiently, our analysis on the whole suggests that the conditions for
both perfect asymptotic learning and for , Œ¥-learning are somewhat stringent.

3.4

Strategic Communication

Next we explore the implications of relaxing the assumption that agents cannot manipulate the messages they send. In particular, we replace Assumption 1 with the following:
Assumption 2 (Strategic Communication). Communication between agents is strategic if
mnij,t ‚àà Rn ,
for all agents i, j and time t.
This assumption makes it clear that in this case the messages need not be truthful. Allowing
strategic communication adds an extra dimension in an agent‚Äôs strategy, since the agent can choose
to ‚Äúlie‚Äù about (part) of her information set in the hope that this increases her expected payoff. Note
that, in contrast with ‚Äúcheap talk‚Äù models, externalities in our framework are purely informational
19

9
10

6
8

A

B

1

2

11

3
4

7

5

Figure 4: Agents may have an incentive to misreport/not disclose their information.
as opposed to payoff relevant. Thus, an agent may have an incentive to ‚Äúlie‚Äù as a means to obtain
more information from the information exchange process (by inducing a later exit decision from her
neighbors).
Figure 4 illustrates how incentives for non-truthful communication may arise. Here, agent B may
have an incentive not to disclose her information to agent A. In particular, for a set of parameter
values we have that if agent B is truthful to A, then A takes an action after the first communication
round. On the other hand, if B does not disclose her information to A, then A waits for an additional
time period and B obtains access to the information of agents 9, 10 and 11.
Let (œÉ n , mn ) denote an action-message strategy profile, where mn = {mn1 , ¬∑ ¬∑ ¬∑ , mnn } and mni =
n . Also let P n n refer to the conditional probability when agents
[mnij,œÑ ]t=0,1,¬∑¬∑¬∑ , for j such that i ‚àà Bj,1
œÉ ,m

behave according to the action-message strategy profile (œÉ n , mn ).
Definition 9. An action-message strategy profile (œÉ n,‚àó , mn,‚àó ) is a pure-strategy perfect Bayesian Equilibrium of the information exchange game Œìinf o (Gn ) if for every i ‚àà N n and communication round œÑ ,
we have
n)‚â•E
E(œÉn,‚àó ,mn,‚àó ) (Uin Ii,t
((œÉ n

n,‚àó
n,‚àó
n
n
n
i,œÑ ,œÉ i,‚àíœÑ ,œÉ ‚àíi ),(mi,œÑ ,mi,‚àíœÑ m‚àíi ))

n ),
(Uin Ii,t

for all mni,œÑ , mni,‚àíœÑ , and œÉ ni,œÑ , œÉ ni,‚àíœÑ . We denote the set of equilibria of this game by IN F O(Gn ).
Similarly we extend the definitions of asymptotic learning [cf. Definitions 2 and 3]. We show that
strategic communication does not harm perfect asymptotic learning. The main intuition behind this
result is that it is weakly dominant for an agent to report her private signal truthfully to a neighbor
with a high in-degree (maven), as long as others are truthful to the maven.
Proposition 4. If perfect asymptotic learning occurs in society {Gn }‚àû
n=1 under Assumption 1, then
there exists an equilibrium (œÉ, m), such that perfect asymptotic learning occurs in society {Gn }‚àû
n=1
along equilibrium (œÉ, m) when we allow strategic communication (cf. under Assumption 2).
This proposition therefore implies that the focus on truthful reporting was without much loss of
generality as far as perfect asymptotic learning is concerned. In any communication network in which
there is perfect asymptotic learning, even if agents can strategically manipulate information, there is
arbitrarily little benefit in doing so. Thus, the main lessons about asymptotic learning derived above
apply regardless of whether communication is strategic or not.
20

¬∑¬∑¬∑
A1

1

A

..
.

¬∑¬∑¬∑

n

B

An
Figure 5: Strategic communication may lead to better actions.
However, this proposition does not imply that all learning outcomes are identical under truthful and strategic communication. In particular, interestingly, as illustrated in Figure 5, strategic
communication may lead agents to take a better action with higher probability than under nonstrategic communication (cf. Assumption 1). The main reason for this (counterintuitive) fact is that
under strategic communication an agent may delay taking an action compared to the non-strategic
environment. Therefore, the agent obtains more information from the communication network and,
consequently, chooses an action, that is closer to optimal. In particular, in the example illustrated in
Figure 5, if agents A, B decide not to disclose their information, then agents 1, ¬∑ ¬∑ ¬∑ , n may delay their
action so as to communicate with the neighbors of A1 , ¬∑ ¬∑ ¬∑ , An and thus take an action based on more
information.

3.5

Welfare

In this subsection, we turn to the question of efficient communication and compare equilibrium allocations (communication and action profiles in equilibrium) with the timing of agents‚Äô actions and
communications that would be dictated by the welfare-maximizing social planner. We identify conditions under which a social planner can improve over an equilibrium strategy profile. In doing so,
we illustrate that communication over social networks might be inefficient because agents do not
internalize the positive externality that delaying their action generates for their peers.
A social planner whose objective is to maximize the aggregate expected welfare of the population
of n agents can implement the timing profile that is a solution to the optimization:
max
n
sp

n
X

Espn [Uin ]

(6)

i=1

n,sp
We call the resulting timing profile as the optimal allocation and we denote it by spn = (œÑ n,sp
1 , ¬∑ ¬∑ ¬∑ , œÑ n ).

21

Similarly with the asymptotic analysis for equilibria, we define a sequence of optimal allocations for
societies of growing size, sp = {spn }‚àû
n=1 . We are interested in identifying conditions under which the
social planner can / cannot achieve an asymptotically better allocation than an equilibrium (sequence
of equilibria) œÉ, i.e., we are looking at the expression:
P
P
n
n
i‚ààN n Espn [Ui ] ‚àí
i‚ààN n EœÉ [Ui ]
lim
.
n‚Üí‚àû
n
The next proposition shows a direct connection between learning and efficient communication.
Proposition 5. Consider society {Gn }‚àû
n=1 . If perfect asymptotic learning occurs at the optimal allocation sp = {spn }‚àû
n=1 , then all equilibria are asymptotically efficient, i.e.,
P
P
n
n
i‚ààN n Espn [Ui ] ‚àí
i‚ààN n EœÉ [Ui ]
lim
= 0,
n‚Üí‚àû
n
for all equilibria œÉ.
Therefore if perfect learning occurs at the optimal allocation, then perfect learning occurs in all
equilibria œÉ.
We next provide a partial converse to Proposition 5. Before stating this result, we contrast the
decision problem an individual agent i with that of the social planner. With a slight abuse of notation,
Uin (k, œÉ) denotes the expected payoff of agent i when agents behave according to profile œÉ and the
agent has observed k signals. Agent i decides to take an irreversible action at time t and not to wait
for an additional dt, when other agents behave according to œÉ, if (cf. Appendix)
!
1
r+Œª
n,œÉ
n,œÉ
n,œÉ
œÄ‚àí
‚â• Uin (ki,t
+ |Bi,|T
| ‚àí |Bi,|T
|, œÉ)
n,œÉ
|+1
t
t|
Œª
œÅ + œÅÃÑki,t

(7)

Similarly, in the corresponding optimal allocation agent i exits at time t and does not wait if:
!
1
r+Œª
œÄ‚àí
n,sp
Œª
œÅ + œÅÃÑki,t
X
n,sp
n,sp
n,sp
‚â• Uin (ki,t
+ |Bi,|T
|
‚àí
|B
|,
sp)
+
Esp [Ujn i ‚Äúwaits‚Äù at t] ‚àí Esp [Ujn i ‚Äúexits‚Äù at t],
i,|Tt |
t |+1
j6=i

(8)
The comparison of (7) to (8) shows the reason for why equilibria may be inefficient in this setting:
when determining when to act, agent i does not take into account the positive externality that a later
action exerts on others. This externality is expressed by the summation on the right hand side of (8).
We next derive sufficient conditions under which a social planner outperforms an equilibrium allocation
n and œÑ n,œÉ > œÑ n,œÉ + 1, which implies that B n
n
œÉ. Consider agents i and j such that i ‚àà Bj,1
‚äÉ Bi,œÑ
n,œÉ
j
i
j,œÑ n,œÉ
j

i

(i.e., agent j communicates with a superset of the agents that i communicates with before taking an
n,œÉ
action). Also, let kij,œÑ
n,œÉ denote the additional agents that j would observe if i delayed her irreversible
i

22

action by dt and communication took place. Then, the aggregate welfare of the two agents increases
if the following condition holds:
n,œÉ
n,œÉ
n,œÉ
n,œÉ
n,œÉ
) > Ujn (kj,œÑ
) + Uin (ki,œÑ
Ujn (kj,œÑ
n,œÉ ) +
n,œÉ + k
n,œÉ + k
ij,œÑ n,œÉ
ij,œÑ n,œÉ
j

i

i

i

j

r + Œª n n,œÉ
Ui (ki,œÑ n,œÉ ),
i
Œª

(9)

n,œÉ
n,œÉ
Let set Dk,`
denote the following set of agents: j ‚àà Dk,`
, if
n,œÉ
(i) kj,œÑ
n,œÉ ‚â§ k.
j

n,œÉ
(ii) There exists an agent i ‚àà Bj,1
such that

(i) œÑ n,œÉ
> œÑ n,œÉ
+ 1.
j
i
(ii) If i exits at œÑ n,œÉ
+ 1, then j gains access to at least an additional ` signals.
i
n,œÉ
Intuitively, set Dk,`
contains agents that would obtain higher payoff in expectation if one of their

neighbors delayed taking her irreversible action. In particular, under equilibrium profile œÉ, agent
n,œÉ
j ‚àà Dk,`
takes an action after observing at most k signals. If her neighbor i delayed her action by one

communication round, then she would have access to at least k + ` signals by the time of her action.
The following proposition provides a sufficient condition for an equilibrium to be inefficient.
n ‚àû
Proposition 6. Consider society {Gn }‚àû
n=1 and equilibrium œÉ = {œÉ }n=1 . Assume that limn‚Üí‚àû

n,œÉ
|Dk,`
|
n

>

Œæ > 0, for k, ` that satisfy the following:

2
r 1
r
œÄ+
< 2+
.
Œª
œÅ + œÅÃÑ(k + `)
Œª œÅ + œÅÃÑk
Then, there exists an Œ∂ > 0, such that
P
P
n
n
i‚ààN n Espn [Ui ] ‚àí
i‚ààN n EœÉ [Ui ]
lim
> Œ∂,
n‚Üí‚àû
n
i.e., equilibrium œÉ is asymptotically inefficient. Moreover, there exist , Œ¥ such that , Œ¥-asymptotic
learning fails at equilibrium œÉ.
We close this section with a discussion on the implications of increasing the information that agents
have access to at the beginning of the information exchange process. Consider the following setting:
agents at time t = 0 have access to k public signals in addition to their private signal. This results
in the following tradeoff: on the one hand, agents are better informed about the underlying state,
but then, on the other hand, they will have less incentive to delay taking an action and thus obtain
and share information with others. In particular, one can show that when all agents have access to
the same k public signals, then information sharing will be reduced compared to a setting without
public signals, in the sense that agents take an irreversible action earlier. Moreover, in some cases
the presence of public signals leads to a strictly smaller aggregate welfare. Thus, more information
23

is not necessarily better for the aggregate welfare of the agents. This result is similar to those in
Duffie, Malamud, and Manso (2009) and in Morris and Shin (2002), both of which show how greater
availability of public information may reduce welfare.

4

Network Formation

We have so far studied information exchange among agents over a given communication network
Gn = (N n , E n ). We now analyse how this communication network emerges. We assume that link
formation is costly. In particular, communication costs are captured by an n √ó n nonnegative matrix
n denotes the cost that agent i has to incur in order to form the directed link (j, i) with
C n , where Cij

agent j. As noted previously, a link‚Äôs direction coincides with the direction of the flow of messages.
In particular, agent i incurs a cost to form in-links. We refer to C n as the communication cost
matrix. We assume that Ciin = 0 for all i ‚àà N n . Our goal in this section is to provide conditions
under which the network structures that emerge as equilibria of the network formation game defined
below guarantee asymptotic learning. Our results indicate that easy access to information (i.e., low
cost to form links with some information sources) may preclude asymptotic learning, as it reduces
the incentives for further information sharing. Moreover, asymptotic learning may depend on how
well agents coordinate at equilibrium: we show that there may be multiple equilibria that induce
sparser/denser network structures and lead to different answers for asymptotic learning.
n = 1
We define agent i‚Äôs link formation strategy, gin , as an n-tuple such that gin ‚àà {0, 1}n and gij

implies that agent i forms a link with agent j. The cost agent i has to incur if she implements strategy
gin is given by
Cost(gin ) =

X

n
n
Cij
¬∑ gij
.

j‚ààN

The link formation strategy profile

gn

=

(g1n , ¬∑ ¬∑ ¬∑

, gnn ) induces the communication network Gn =

n = 1.
(N n , E n ), where (j, i) ‚àà E n if and only if gij

We extend our environment to the two-stage Network Learning Game Œì(C n ), where C n denotes
the communication cost matrix. The two stages of the network learning game can be described as
follows:
Stage 1 [Network Formation Game]: Agents choose their link formation strategies. The link
formation strategy profile g n induces the communication network Gn = (N n , E n ).
We refer to stage 1 of the network learning game, when the communication cost matrix is C n as the
network formation game and we denote it by Œìnet (C n ).
Stage 2 [Information Exchange Game]: Agents communicate over the induced network Gn as
studied in previous sections.
We next define the equilibria of the network learning game Œì(C n ). Note that we use the standard
notation g‚àíi and œÉ ‚àíi to denote the strategies of agents other than i. Also, we let œÉ i,‚àít denote the

24

vector of actions of agent i at all times except t.
Definition 10. A pair (g n,‚àó , œÉ n,‚àó ) is a pure-strategy perfect Bayesian Equilibrium of the network
learning game Œì(C n ) if
(a) œÉ n,‚àó ‚àà IN F O(Gn ), where Gn is induced by the link formation strategy g n,‚àó .
(b) For all i ‚àà N n , gin,‚àó maximizes the expected payoff of agent i given the strategies of other agents
n,‚àó
g‚àíi
, i.e.,

gin,‚àó ‚àà arg

max

gin ‚àà{0,1}n

n,‚àó
n
EœÉ [Œ†i (gin , g‚àíi
)] ‚â° EœÉ (Uin Ii,0
) ‚àí Cost(gin ).

n,‚àó
for all œÉ ‚àà IN F O(GÃÉn ), where GÃÉn is induced by link formation strategy (gin , g‚àíi
).

We denote the set of equilibria of this game by N ET (C n ).
Similar to the analysis of the information exchange game, we consider a sequence of communication
cost matrices {C n }‚àû
n=1 , where for fixed n,
n+1
n
C n : N n √ó N n ‚Üí R+ and Cij
= Cij
for all i, j ‚àà N n .

(10)

For the remainder of the section, we focus our attention to the social cliques communication cost
structure. The properties of this communication structure are stated in the next assumption.
Assumption 3. Let cnij ‚àà {0, c} for all pairs (i, j) ‚àà N n √ó N n , where c <

1
œÅ+œÅÃÑ .

Moreover, let cij = cji

for all i, j ‚àà N n (symmetry), and cij + cjk ‚â• cik for all i, j, k ‚àà N n (triangular inequality).
The assumption that c <

1
œÅ+œÅÃÑ

rules out the degenerate case where no agent forms a costly link.

The symmetry and triangular inequality assumptions are imposed to simplify the definition of a social
clique, which is introduced next. Suppose Assumption 3 holds. We define a social clique (cf. Figure
6) H n ‚äÇ N n as a set of agents such that
i, j ‚àà H n

if and only if cij = cji = 0.

Note that this set is well-defined since, by the triangular inequality and symmetry assumptions, if an
agent i does not belong to social clique H n , then cij = c for all j ‚àà H n . Hence, we can uniquely
n }.
partition the set of nodes N n into a set of K n pairwise disjoint social cliques Hn = {H1n , ¬∑ ¬∑ ¬∑ , HK
n

We use the notation Hkn to denote the set of pairwise disjoint social cliques that have cardinality
greater than or equal to k, i.e., Hkn = {Hin , i = 1, . . . , K n | |Hin | ‚â• k}. We also use SC n (i) to denote
the social clique that agent i belongs to.
We consider a sequence of communication cost matrices {C n }‚àû
n=1 satisfying condition (10) and
Assumption 3, and we refer to this sequence as a communication cost structure. As shown above,
n ‚àû
the communication cost structure {C n }‚àû
n=1 uniquely defines the following sequences, {H }n=1 and

25

Social clique 1

Social clique 2
c

0

0

0

0

c

0
0

c

Figure 6: Social cliques.
{Hkn }‚àû
n=1 for k > 0, of sets of pairwise disjoint social cliques. Moreover, it induces network equilibria
n
n
n
(g, œÉ) = (g n , œÉ n )‚àû
n=1 such that (g , œÉ ) ‚àà N ET (C ) for all n. Our goal is to identify conditions on

the communication cost structure that lead to the emergence of networks which guarantee asymptotic
learning. We focus entirely on perfect asymptotic learning, as this enables us to obtain sharp results.
Similar results can be obtained for , Œ¥-asymptotic learning.
Proposition 7. Let {C n }‚àû
n=1 be a communication cost structure and let Assumptions 1 and 3 hold.
Then, there exists a constant kÃÑ = kÃÑ(c) such that the following hold:
(a) Suppose that
lim sup
n‚Üí‚àû

HkÃÑn
‚â•  for some  > 0.
n

(11)

Then, perfect asymptotic learning does not occur in any network equilibrium (g, œÉ).
(b) Suppose that
lim

n‚Üí‚àû

HkÃÑn
= 0 and lim H`n = ‚àû for some `.
n‚Üí‚àû
n

(12)

Then, perfect asymptotic learning occurs in all network equilibria (g, œÉ) when the discount rate
r satisfies 0 < r < rÃÑ, where rÃÑ > 0 is a constant.
(c) Suppose that there exists M > 0 such that
lim

n‚Üí‚àû

HkÃÑn
= 0 and lim sup H`n < M for all `,
n
n‚Üí‚àû

(13)

and let agents be patient, i.e., consider the case, when the discount rate r ‚Üí 0. Then, there
exists a cÃÑ > 0 such that
(i) If c ‚â§ cÃÑ, perfect asymptotic learning occurs in all network equilibria (g, œÉ).
(ii) If c > cÃÑ, there exists at least one network equilibrium (g, œÉ), where there is no perfect
26

: Clique with size > kÃÑ
: Clique with infinite size

: Individual Agent
: Small Clique
...

...

...

sender

receiver

...

(a) Equilibrium network, when (12) holds.

(b) Equilibrium network, when (13) holds.

Figure 7: Network formation among social cliques.
asymptotic learning and there exists at least one network equilibrium (g, œÉ) where perfect
asymptotic learning occurs.
The results in this proposition provide a fairly complete characterization of what types of environments lead to the formation of networks that subsequently induce perfect asymptotic learning. The
key concept is that of a social clique, which represents groups of individuals that are linked to each
other at zero cost. These can be thought of as ‚Äúfriendship networks,‚Äù which are linked for reasons
unrelated to information exchange and thus can act as conduits of such exchange at low cost. Agents
can exchange information without incurring any costs (beyond the delay necessary for obtaining information) within their social cliques. However, if they wish to obtain further information, from outside
their social cliques, they have to pay a cost at the beginning in order to form a link. Even though
network formation games have several equilibria, the structure of our network formation and information exchange game enables us to obtain relatively sharp results on what types of societies lead to
endogenously formed communication networks that ensure perfect asymptotic learning. In particular,
the first part of Proposition 7 shows that perfect asymptotic learning cannot occur in any equilibrium
if the number of sufficiently large social cliques increases at the same rate as the size of the society.
This is intuitive; when this is the case, there are many social cliques of sufficiently large size that none
of their members wish to engage in further costly communication with members of other social cliques.
But since several of these do not contain an information hub social learning is precluded.
In contrast, the second part of the proposition shows that if the number of disjoint and sufficiently
large social cliques is limited (grows less rapidly than the size of the society) and some of them are
large enough to contain information hubs, then perfect asymptotic learning takes place (provided
that future is not heavily discounted). In this case, as shown by Figure 7(a), sufficiently many social
cliques connect to the larger social cliques acting as information hubs, ensuring effective aggregation
of information for the great majority of the agents in the society. It is important that the discount
27

factor is not too small, otherwise smaller cliques do not find it beneficial to form links with the larger
cliques.
Finally, the third part of the proposition outlines a more interesting configuration, potentially
leading to perfect asymptotic learning. In this case, many small social cliques form an ‚Äúinformational
ring‚Äù(Figure 7(b)). Each is small enough that it finds it beneficial to connect to another social clique,
provided that this other clique also connects to others and obtain further information. This intuition
also clarifies why such information aggregation takes place only in some equilibria. The expectation
that others do not form the requisite links leads to a coordination failure. Interestingly, however, if
agents are sufficiently patient and the cost of link formation is not too large, the coordination failure
equilibrium disappears, because it becomes beneficial for each clique to form links with another one,
even if further links are not forthcoming. Finally, the ring structure is a direct consequence of the
fact that agents are patient (and has been shown to emerge as an equilibrium configuration in other
models of network formation, e.g., Bala and Goyal (2000))

5

Conclusion

We have developed a framework for the analysis of information exchange through communication
and investigated its implications for information aggregation in large societies. An underlying state
determines the payoffs from different actions. Agents decide which agents to form a communication
link with incurring the associated cost. After receiving a private signal correlated with the underlying
state, they exchange information over the induced communication network until taking an (irreversible)
action.
Our focus has been on asymptotic learning, defined as the fraction of agents taking the correct
action converging to one in probability as a society grows large. We showed that asymptotic learning
occurs if and, under some additional mild assumptions, only if the induced communication network
includes information hubs and most agents are at a short distance from a hub. Thus asymptotic
learning requires information to be aggregated in the hands of a few agents. This kind of aggregation
also requires truthful communication, which we show is an equilibrium of the strategic communication
in large societies (partly as a consequence of the fact there is no conflict among the agents concerning
which action is best).
Our analysis also provides a systematic investigation of what types of cost structures, and associated social cliques which consist of groups of individuals linked to each other at zero cost (such
as friendship networks), ensure the emergence of communication networks that lead to asymptotic
learning. Our main result on network formation shows that societies with too many (disjoint) and
sufficiently large social cliques do not form communication networks that lead to asymptotic learning,
because each social clique would have sufficient information to make communication with others not
sufficiently attractive. Asymptotic learning results if social cliques are neither too numerous nor too

28

large so as to encourage communication across cliques. Our analysis was conducted under a simplifying
assumption that all agents have the same preferences. Interesting avenues for research include investigation of similar dynamic models of information exchange and network formation in the presence
of ex ante or ex post heterogeneity of preferences as well as differences in the quality of information
available to different agents, which may naturally lead to the emergence of hubs.

29

Appendix
Proofs from Section 3
Proof of Lemma 1.
Recall that, by the principle of optimality, agent i‚Äôs optimal continuation payoff at information set
n , when the rest of the agents behave according to strategy profile œÉ, is given by:
Ii,t

(
EœÉ (Uin

n
Ii,t
)

= max

1
n,œÉ
œÅ+œÅÃÑki,t
‚àírdt
n
e
E[EœÉ (Uin |Ii,t+dt
)

œÄ‚àí

(when she takes the optimal irreversible action),
n]
Ii,t

(when she decides to wait, i.e., x = ‚Äúwait‚Äù),

n,œÉ
where ki,t
denotes the number of distinct private signals agent i has observed up to time t. The first

line is equal to the expected payoff for the agent when she chooses the optimal irreversible action
n , i.e., E[Œ∏|I n ], and she has observed k n,œÉ private signals, while the second
under information set Ii,t
i,t
i,t

line is equal to the discounted expected continuation payoff.
For the latter, we have that with probability Œªdt, communication takes place in time interval [t, t +
dt], thus the information set of agent i expands; with probability (1 ‚àí Œªdt), there is no communication
and the value function for agent i remains unchanged. If communication takes place in interval [t, t+dt],
n,œÉ
n,œÉ
then agent i observes |Bi,|T
| ‚àí |Bi,|T
| additional signals.
t |+1
t|

Note that since we assume that signals are identically distributed and independent, the value
n , k n,œÉ and profile
function can simply be expressed as a function of the number of distinct signals in Ii,t
i,t

œÉ. The agent chooses to take an irreversible action and not to wait if
œÄ‚àí

1
n
‚àírdt
n
) Ii,t
]
E[EœÉ (Uin |Ii,t+dt
n,œÉ ‚â• e
œÅ + œÅÃÑki,t


n,œÉ
n,œÉ
n,œÉ
n n,œÉ
+ |Bi,|T
|
‚àí
|B
|,
œÉ)
+
(1
‚àí
Œªdt)U
(k
,
œÉ)
.
‚â• e‚àírdt ŒªdtUin (ki,t
i
i,t
i,|Tt |
t |+1

Thus, we obtain that the agent chooses not to wait if:
r+Œª
n,œÉ
n,œÉ
n,œÉ
Uin (ki,t
+ |Bi,|T
| ‚àí |Bi,|T
|, œÉ) ‚â§
t |+1
t|
Œª

1
œÄ‚àí
n,œÉ
œÅ + œÅÃÑki,t

!
.

(14)

The left hand side of (14) is upper bounded by œÄ, whereas the right hand side is increasing in the
n,œÉ
number of private signals ki,t
and in the limit is equal to

r+Œª
Œª œÄ

> œÄ. This establishes the lemma.

The next lemma will be used in the rest of the Appendix. It shows that the probability of choosing
an action that is more than  away from the optimal for agent i ‚àà Vkn,œÉ , i.e., PœÉ (Min, = 0), is
uniformly bounded away from 0 in terms of the error function.
Lemma 2. Let k > 0 be a constant, such that the k-radius set Vkn,œÉ is non-empty. Then,
r !
kœÅÃÑ
n,
n
P(Mi = 0) ‚â• erf 
for all i ‚àà Vk,œÉ
,
2

30

where erf (x) =

‚àö2
œÄ

Rx
0

2

e‚àít dt is the error function.

Proof. Note that because of our normality assumption the empirical mean Œ∏ÃÇ after observing ` private
signals is normally distributed around Œ∏ with precision œÅŒ∏ÃÇ = `œÅÃÑ. Then, the probability that Min, = 0
is simply equal to the probability that the error does not belong to the interval [‚àí, ], i.e.,
r !
`œÅÃÑ
n,
.
P(Mi = 0) = erf 
2
The lemma follows since agent i ‚àà Vkn,œÉ , thus she takes an irreversible action after observing at most
k private signals.
Proof of Proposition 1. First, we show that learning fails if condition (3) holds, i.e., there exists
a k > 0, such that
1
Œ∑ = lim sup ¬∑ Vkn,œÉ >  and erf
n‚Üí‚àû n

r


kœÅÃÑ
2

!
< (1 ‚àí Œ¥)(1 ‚àí /Œ∑).

(15)

From condition (15) we obtain that there exists an infinite index set J such that
n

Vk j ‚â• Œ∑ ¬∑ nj for j ‚àà J.
Now restrict attention to index set J, i.e., consider n = nj for some j ‚àà J. Then,
 hP
i


P
P
n,
n,
n,œÉ M
n,œÉ M
PœÉ n1 ni=1 Min, > 1 ‚àí  = PœÉ n1
+
>
1
‚àí

i‚ààV
/ k
i
ii

 hPi‚ààVk
n,
n,œÉ
1
n,œÉ M
+
n
‚àí
V
>
1
‚àí

‚â§ PœÉ n
i‚ààVk
i
k


n,œÉ
P
V
n,
k
1
= PœÉ n i‚ààV n,œÉ Mi > n ‚àí 

,

k

where the inequality follows since we let Min, = 1 for all i ‚àà
/ Vkn,œÉ . Next we use Markov‚Äôs inequality
hP
i
Ô£´
Ô£∂
n,
n,œÉ
n,œÉ M
E
X
œÉ
i‚ààVk
i
V
1
.
PœÉ Ô£≠
Min, > k
‚àí Ô£∏ ‚â§
n,œÉ
n
n
n ¬∑ Vk /n ‚àí 
n,œÉ
i‚ààVk

We can view each summand
above as an independent Bernoulli variable with success probability
 q 
kœÅÃÑ
bounded above by erf  2 from Lemma 2. Thus,
EœÉ


P

n¬∑



n,
n,œÉ Mi
i‚ààV
k

Vkn,œÉ /n‚àí



‚â§
‚â§

 q 
Vkn,œÉ erf  kœÅÃÑ
2


n,œÉ
/n‚àí
n¬∑ Vk
Œ∑
Œ∑‚àí erf

 q 
 kœÅÃÑ
< 1 ‚àí Œ¥,
2

where the second inequality follows from the fact that n was chosen such that Vkn,œÉ ‚â• Œ∑ ¬∑ n. Finally,
the last expression follows from the choice of k (cf. Condition (3)). We obtain that for all j ‚àà J it

31

holds that

"
PœÉ

#
!
nj
1 X
nj , 
1 ‚àí Mi
>  ‚â• Œ¥.
nj
i=1

Since J is an infinite index set we conclude that
" n
#
!
1X
n,
(1 ‚àí Mi ) >  ‚â• Œ¥,
lim inf PœÉ
n‚Üí‚àû
n
i=1

thus , Œ¥-asymptotic learning is incomplete when (3) holds.
Next, we prove that Condition (4) is sufficient for , Œ¥-asymptotic learning. As mentioned above, if
agent i takes an irreversible action after observing ` signals, then the probability that Min, = 1 is
equal to
r
PœÉ (Min,

= 1) = erf



`œÅÃÑ
2

!
.

(16)

Similarly with above, we have

"
PœÉ

#
!
n
1X
n,
(1 ‚àí Mi ) > 
‚â§ PœÉ
n

!
#
V
1X
n,
(1 ‚àí Mi ) >  ‚àí
n
n
i‚ààV
/
P

n,
EœÉ
i‚ààV
/ (1 ‚àí Mi )

,
n  ‚àí V /n

i=1

‚â§
where V =

n
i

n
Bi,œÑ
n,œÉ ‚â§ k

o

"

(17)

and the second inequality follows from Markov‚Äôs inequality. By

i

combining Eqs. (16) and (17) and letting kin,œÉ denote the number of private signals that agent i
observed before taking an action,

EœÉ

n, 
i‚ààV
/ (1 ‚àí Mi )

‚â§
n  ‚àí V /n

P

We have

r
erf



kin,œÉ œÅÃÑ
2

P

i‚ààV
/

 q n,œÉ 
ki œÅÃÑ

2

.
n  ‚àí V /n
1 ‚àí erf

!
>1‚àí

Œ¥( ‚àí Œ∂)
,
1‚àíŒ∂

(18)

(19)

for all i ‚àà
/ V from the definition of k (cf. Condition (4)). Thus, combining Eqs. (17),(18) and (19), we
obtain

"
PœÉ

#
!
n
1X
(1 ‚àí Min, ) >  < Œ¥ for all n > N,
n
i=1

where N is a sufficiently large constant, which implies that condition (4) is sufficient for asymptotic
learning.
Similar to perfect k-radius sets, we define sets Xkn for scalar k as
n
n
n with indeg` > k},
Xkn = {i ‚àà N n there exists ` ‚àà Bi,œÑ
i

32

n
Agent i‚Äôs observation set Bi,œÑ
n
i

Agent j‚Äôs observation set

n
Bj,œÑ
n
j

Node `

Node j

Node i
œÑ nj

œÑ nj
dist(`, j)

dist(`, i) ‚â§ œÑ ni

Figure 8: Proof of Proposition 8.
i.e., the set Xkn consists of all agents, which have an agent with in-degree at least k within their perfect
observation radius.
Proposition 8. Suppose Assumption 1 holds. Then, perfect asymptotic learning occurs in society
{Gn }‚àû
n=1 in any equilibrium œÉ if
lim lim

k‚Üí‚àû n‚Üí‚àû

1 n
|X | = 1.
n k

1
n
Proof. Consider equilibrium profile œÉ and society {Gn }‚àû
n=1 such that limk‚Üí‚àû limn‚Üí‚àû n |Xk | = 1.

Define Zkn,œÉ as the following set of agents
n,œÉ
n
Zkn,œÉ = {i ‚àà N n there exists ` ‚àà Bi,œÑ
n,œÉ with indeg` > k},
i

i.e., the agents that at equilibrium œÉ, communicate with an agent with in-degree at least k. Next, we
show that for k large enough (and consequently n large enough), Xkn = Zkn,œÉ .
Consider i ‚àà Xkn and let P n = {`, i1 , ¬∑ ¬∑ ¬∑ , iK , i} denote the shortest path in communication network
Gn between i and any agent `, with indeg`n ‚â• k. First we show the following (refer to Figure 8)
i ‚àà Xkn ‚áí j ‚àà Xkn for all j ‚àà P n .

(20)

Assume for the sake of contradiction that condition (20) does not hold. Then, let
{distn (`, j 0 ) j 0 ‚àà P n and distn (`, j 0 ) > œÑ nj0 },
j = arg min
0
j

where recall that œÑ ni denotes the perfect observation radius of agent i. For agents i, j we have œÑ ni > œÑ nj
n
n
and dist(j, i) + dnj < dist(`, i) ‚â§ œÑ ni , since otherwise j ‚àà Xkn . This implies that Bj,œÑ
n ‚äÇ Bi,œÑ n .
j

i

Furthermore,
1
œÄ‚àí
n |) >
œÅ + œÅÃÑ(|Bj,œÑ
n
j



Œª
Œª+r

dist(`,j)‚àíœÑ nj

1
œÄ‚àí
n | + k)
œÅ + œÅÃÑ(|Bj,œÑ
n

!
,

(21)

j

In particular, the left hand side is equal to the expected payoff of agent j if she takes an irreversible
33

n | observations, whereas the right hand side is a lower bound on
action at time œÑ nj after receiving |Bj,œÑ
n
j

the expected payoff if agent j delays taking an action until after she communicates with agent `. The
inequality follows, from the definition of the observation radius for agent j. On the other hand, since
n , we have
for agent i, ` ‚àà Bi,œÑ
n
i

1
œÄ‚àí
n |) <
œÅ + œÅÃÑ(|Bj,œÑ
n
j



Œª
Œª+r

dist(`,i)‚àídist(j,i)‚àíœÑ nj

1

œÄ‚àí

n | + k + k0 )
œÅ + œÅÃÑ(|Bj,œÑ
n

!
, for some k 0 > 0. (22)

j

For k large enough we conclude that dist(`, j) < dist(`, i)‚àídist(j, i), which is obviously a contradiction.
This implies that (20) holds.
Next we show, by induction on the distance from agent ` with in-degree ‚â• k that Xkn = Zkn,œÉ for
equilibrium œÉ. The claim is obviously true for all agents with distance equal to 0 (agent `) and 1 (her
neighbors). Assume that the claim holds for all agents with distance at most t from agent `, i.e., if
i ‚àà Xkn and dist(`, i) ‚â§ t then i ‚àà Zkn,œÉ . Finally, we show the claim for an agent i such that i ‚àà Xkn
and dist(`, i) = t + 1. Consider a shortest path P n from i to `. Condition (20) implies that all agents
j in the shortest path are such that j ‚àà Xkn , thus from the induction hypothesis we obtain j ‚àà Zkn,œÉ .
Thus, for k sufficiently large we obtain that i ‚àà Zkn,œÉ , for any equilibrium œÉ.
Finally, by the hypothesis of the proposition, i.e., limk‚Üí‚àû limn‚Üí‚àû n1 |Xkn | = 1, we conclude that
limk‚Üí‚àû limn‚Üí‚àû n1 |Zkn,œÉ | = 1, for any equilibrium œÉ. The latter implies that limk‚Üí‚àû limn‚Üí‚àû n1 |Vkn,œÉ | =
0, thus asymptotic learning occurs along equilibrium œÉ from Proposition 1.
Proof of Proposition 2.
The first part of Proposition 2 follows directly from Proposition 8, since
lim lim

k‚Üí‚àû n‚Üí‚àû

1
1 n
|V | = 0 ‚áí lim lim |Xkn | = 1.
k‚Üí‚àû n‚Üí‚àû n
n k

To conclude the proof we need to show that if asymptotic learning occurs along some equilibrium œÉ
when condition (4) does not hold, then the society contains a set of leading agents. In particular,
n ‚àû
consider a society {Gn }‚àû
n=1 in which condition (4) does not hold and equilibrium œÉ = {œÉ }n=1 along

which asymptotic learning occurs in the society. This implies that there should exist a subset {Rn,œÉ }‚àû
n=1
of agents such that limn‚Üí‚àû n1 |Rn,œÉ | >  and there is an infinite index set J for which
n

n ,œÉ

i ‚àà Rnj ,œÉ and œÑ i j < œÑ i j , for j ‚àà J.

(23)

This further implies that
|B

nj
n

i,œÑ i j

| > |B

nj ,œÉ
n

i,œÑ i j

|.

(24)

From equations (23) and (24) we obtain that there should exist a collection of agents {S n }‚àû
n=1 such
that (we restrict attention to index set J):
(i) Rn,œÉ ‚äÜ Sfnollow .
34

(ii) There exists a k > 0 such that S n ‚äÜ Vkn,œÉ .
(ii) limn‚Üí‚àû n1 |S n | = 0, since otherwise asymptotic learning would not occur under equilibrium œÉ.
Note that collection {S n }‚àû
n=1 satisfies the definition of a set of leading agents [cf. Definition 5] and
Proposition 2 (ii) follows.

Proof of Proposition 3. Consider the following two events A and B.
Event A: Layer 1 (the top layer) has more than k agents, where k > 0 is a scalar.
Event B: The total number of layers is more than k.
From the definition of a hierarchical sequence of communication networks, we have
!

k 
k
Y
X
1
1
P(A) =
1 ‚àí 1+Œ∂ < exp ‚àí
.
i
i1+Œ∂
i=2

Also,

(25)

i=2

‚àû
1X 1
E(L)
=
P(B) ‚â§
,
k
k
i1+Œ∂

(26)

i=2

from Markov‚Äôs inequality, where L is a random variable that denotes the number of layers in the
hierarchical society. Let Œ∂(Œ∑) be small enough and k (and consequently n) large enough such that
P‚àû 1
Pk
k¬∑Œ∑
1
4
i=2 i1+Œ∂ < 4 . For those values of Œ∂ and k we obtain P(A) < Œ∑/4 and
i=2 i1+Œ∂ > log Œ∑ and
P(B) < Œ∑/4. Next, consider the event C = Ac ‚à© B c , which from Eqs. (25) and (26) has probability
P(C) > 1 ‚àí Œ∑/2 for the values of Œ∂ and k chosen above. Moreover, we consider
Event D: The agents on the top layer are information hubs, i.e.,
n
lim |Bi,1
| = ‚àû, for all i ‚àà N1n .

n‚Üí‚àû

We claim that event D occurs with high probability if C occurs, i.e., P(D C) > 1 ‚àí Œ∑/2, which implies
P(C ‚à© D) = P(D C)P(C) > (1 ‚àí Œ∑/2)2 > 1 ‚àí Œ∑.

(27)

In particular, note that conditional on event C occurring, the total number of layers and the total
number of agents in the top layer is at most k. From the definition of a hierarchical society, agents
in layers with index ` > 1 have an edge to a uniform agent that belongs to a layer with lower index,
with probability one. Therefore, if we denote the degree of an agent in a top layer by D1n we have
n

n

D1n

=

T2
X

level2
Ii,1

+ ¬∑¬∑¬∑ +

i=1

TL
X

levelL
Ii,1
,

(28)

i=1

levelj
where Tin denotes the random number of agents in layer i and Ii,1
is an indicator variable that

takes value one if there is an edge from agent i to agent 1 (here levelj simply denotes that agent i

35

levelj
belongs to level j). Again from the definition, we have P(Ii1
= 1) =

1
Pj‚àí1
`=1

T`n

, where the sum in

the denominator is simply the total number of agents that lie in layers with lower index, and finally,
T1n + ¬∑ ¬∑ ¬∑ TLn = n.
We can obtain a lower bound on the expected degree of an agent in the top layer conditional on
event C by viewing (28) as the following optimization problem:
min
s.t.

xk
x2
+ ¬∑¬∑¬∑ +
x1
x1 + ¬∑ ¬∑ ¬∑ + xk‚àí1
P
k
x
=
n,
j
j=1
0 ‚â§ x1 ‚â§ k,
0 ‚â§ x2 , ¬∑ ¬∑ ¬∑ , xk‚àí1 ,

where we make use of the fact that the total number of layers is bounded by k, since we condition
on event C. By solving the problem we obtain that the objective function is lower bounded by œÜ(n),
where œÜ(n) = O(n1/k ) for every n. Then,
E[D1n C] =
=

k
X

X

P(L = `, T1n = k1 , ¬∑ ¬∑ ¬∑ , T`n = k` |C) ¬∑ E[D1n C, L = `, T1n = k1 , ¬∑ ¬∑ ¬∑ , T`n = k` ]

`=2 k1 ‚â§k,¬∑¬∑¬∑ ,k`
k1 +¬∑¬∑¬∑+k` =n

‚â•

k
X

X

P(L = `, T1n = k1 , ¬∑ ¬∑ ¬∑ , T`n = k` |C) ¬∑ œÜ(n) = œÜ(n),

(29)

`=2 k1 ‚â§k,¬∑¬∑¬∑ ,k`
k1 +¬∑¬∑¬∑+k` =n

where Eq. (29) follows since E[D1n C, L = `, T1n = k1 , ¬∑ ¬∑ ¬∑ , T`n = k` ] ‚â• œÜ(n) for all values of ` (2 ‚â§ ` ‚â§ k)
and k1 , ¬∑ ¬∑ ¬∑ , k` (k1 ‚â§ k, k1 + ¬∑ ¬∑ ¬∑ + k` = n) from the optimal solution of the optimization problem. The
same lower bound applies for all agents in the top layer. Similarly we have for the variance of the
degree of an agent in the top layer (we use `, k1 , ¬∑ ¬∑ ¬∑ , k` as a shorthand for L = `, T1n = k1 , ¬∑ ¬∑ ¬∑ , T`n = k` )
V

ar[D1n

C] =

k
X

X

P(`, k1 , ¬∑ ¬∑ ¬∑ , k` |C) ¬∑ V ar[D1n C, `, k1 , ¬∑ ¬∑ ¬∑ , k` ]

`=2 k1 ‚â§k,¬∑¬∑¬∑ ,k`
k1 +¬∑¬∑¬∑+k` =n

=

k
X

X



level2
level`
P(`, k1 , ¬∑ ¬∑ ¬∑ , k` |C) ¬∑ k2 V ar(Ii,1
) + ¬∑ ¬∑ ¬∑ + k` V ar(Ii,1
)

(30)



level2
level`
P(`, k1 , ¬∑ ¬∑ ¬∑ , k` |C) ¬∑ k2 E(Ii,1
) + ¬∑ ¬∑ ¬∑ + k` E(Ii,1
) = E[D1n C],

(31)

`=1 k1 ‚â§k,¬∑¬∑¬∑ ,k`
k1 +¬∑¬∑¬∑+k` =n

‚â§

k
X

X

`=1 k1 ‚â§k,¬∑¬∑¬∑ ,k`
k1 +¬∑¬∑¬∑+k` =n

where Eq. (30) follows by noting that conditional on event C and the number of layers and the agents
in each layer being fixed, the indicator variables (defined above) are independent and Eq. (31) follows
since the variance of an indicator variable is smaller that its expectation. We conclude that the variance

36

of the degree is smaller than the expected value and from Chebyschev‚Äôs inequality we conclude that
\

P(D) ‚â• P(

i‚ààN1n

Din
> Œ∂) > 1 ‚àí Œ∑/2,
œÜ(n)

where Œ∂ > 0, i.e., with high probability all agents in the top layer are information hubs (recall that
limn‚Üí‚àû œÜ(n) = ‚àû).
We have shown that when event C ‚à©D occurs, there is a path of length at most k (the total number
of layers) from each agent to an agent at the top layer, i.e., an information hub with high probability.
Therefore, if the discount rate r is smaller than some bound (r < rÃÑ), then perfect asymptotic learning
occurs. Finally, we complete the proof by noting that P(C ‚à© D) > (1 ‚àí Œ∑/2)2 > 1 ‚àí Œ∑.

Proof of Proposition 4.
Proposition 4 is a direct consequence of the next lemma, which intuitively states that there is no
incentive to lie to an agent with a large number of neighbors, assuming that everybody else is truthful.
Lemma 3 (Truthful Communication to a High Degree Agent). There exists a scalar k > 0, such
that truth-telling to agent i, with indegin ‚â• k, in the first time period is an equilibrium of IN F O(Gn ).
Formally,
(œÉ n,truth , mn,truth ) ‚àà IN F O(Gn ),
n .
where mn,truth
= sj for j ‚àà Bi,1
ji,0
n except j report
Proof. The proof is based on the following argument. Suppose that all agents in Bi,1
n | ‚â• k, where k is a large constant. Then, it is an weakly
their signals truthfully to i. Moreover, let |Bi,1

dominant strategy for j to report her signal truthfully to i, since j‚Äôs message is not pivotal for agent i,
i.e., i will take an irreversible action after the first communication step, no matter what j reports.
Proof (sketch) of Proposition 5. Assume that asymptotic learning occurs at the optimal allocation
sp = {spn }‚àû
n=1 . Then,
lim lim

k‚Üí‚àû n‚Üí‚àû

1 n
|V | = 0.
n k

(32)

This follows since if Equation (32) were not true, then a social planner could replicate the allocation
induced by the perfect observation radius and achieve a higher aggregate welfare. This is possible
n ‚äá B n,sp for every œÑ , where sp denotes the socially optimal strategy profile. From Equation
since Bi,œÑ
i,œÑ

(32) and Proposition 1 we obtain that asymptotic learning occurs in all equilibria œÉ. Finally, the
proposition follows using similar arguments as those used in the proof of Proposition 8.
Proof of Proposition 6.

The claim follows by noting that the social planner could choose the

n,œÉ
following strategy profile: for each j ‚àà Dk,`
delay i‚Äôs irreversible action by at least one time period,

where i is an agent such that if i delays then j gains access to a least ` additional signals. Moreover,
it is straightforward to see that there exist , Œ¥ for which , Œ¥-learning fails.

37

Proofs from Section 4
Proof of Proposition 7
First we make an observation which will be used frequently in the subsequent analysis. Consider an
n
agent i such that HSC(i)
‚àà HkÃÑn , where kÃÑ is an integer appropriately chosen (see below), i.e., the size of
n
the social clique of agent i is greater than or equal to kÃÑ, |HSC(i)
| ‚â• kÃÑ. Suppose agent i does not form a

link with cost c with any agents outside her social clique. If she makes a decision at time t = 0 based
on her signal only, her expected payoff is œÄ ‚àí

1
œÅ+œÅÃÑ .

If she waits for one period, she has access to the

signals of all the agents in her social clique (i.e., she has access to at least kÃÑ signals), implying that


Œª
1
her expected payoff would be bounded from below by r+Œª
. Hence, her expected payoff
œÄ ‚àí œÅ+œÅÃÑ
kÃÑ
E[Œ†i (g n )] satisfies


E[Œ†i (g )] ‚â• max œÄ ‚àí
n

1
Œª
,
œÅ + œÅÃÑ r + Œª


œÄ‚àí

1
œÅ + œÅÃÑkÃÑ


,

for any link formation strategy g n and along any œÉ ‚àà IN F O(Gn ) (where Gn is the communication
network induced by g n ). Suppose now that agent i forms a link with cost c with an agent outside her
social clique. Then, her expected payoff is bounded from above by
(
)

2
1
Œª
E[Œ†i (g n )] < max œÄ ‚àí
,
œÄ‚àíc ,
œÅ + œÅÃÑ Œª + r
where the second term in the maximum is an upper bound on the payoff she could get by having access
to the signals of all agents she is connected to in two time steps (i.e., signals of the agents in her social
clique and in the social clique that she is connected to). Combining the preceding two relations, we
n
see that an agent i with HSC(i)
‚àà HkÃÑn will not form any costly links in any network equilibrium, i.e.,
n
n
gij
= 1 if and only if SC(j) = SC(i) for all i such that |HSC(i)
| ‚â• kÃÑ.

for kÃÑ such that
Œª
r+Œª
(a)


œÄ‚àí

1
œÅ + œÅÃÑkÃÑ




‚â•

Œª
Œª+r

(33)

2
œÄ ‚àí c.

Condition (11) implies that for all sufficiently large n, we have
HkÃÑn ‚â• Œæn,

where Œæ > 0 is a constant. For any  with 0 <  < Œæ, we have
Ô£´Ô£Æ
!
n
n,
X1‚àíM
X
1 ‚àí Min,
Ô£¨Ô£Ø
i
P
>
=
P Ô£≠Ô£∞
+
n
n
n
i=1

i| |HSC(i) |<kÃÑ

Ô£´
‚â•

Ô£¨
PÔ£≠

(34)

Ô£π
X
n
i| |HSC(i)
|‚â•kÃÑ

1‚àí

Ô£∂

Min, Ô£∫

n

Ô£∑
Ô£ª > Ô£∏

Ô£∂
X
n
i| |HSC(i)
|‚â•kÃÑ

38

1 ‚àí Min,
Ô£∑
> Ô£∏ .
n

(35)

The right-hand side of the preceding inequality can be re-written as
Ô£´
Ô£∂
Ô£´
Ô£∂
n,
n,
X
X
1 ‚àí Mi
1 ‚àí Mi
Ô£¨
Ô£∑
Ô£¨
Ô£∑
PÔ£≠
> Ô£∏ = 1 ‚àí P Ô£≠
‚â§ Ô£∏
n
n
n
n
i| |HSC(i) |‚â•kÃÑ

i| |HSC(i) |‚â•kÃÑ

Ô£´

Ô£∂
X

Ô£¨
= 1 ‚àí PÔ£≠

Min,

n
i| |HSC(i)
|‚â•kÃÑ

where r =

P

1
n
i| |HSC(i)
|‚â•kÃÑ n .

n

Ô£∑
‚â• r ‚àí Ô£∏ ,

By Eq. (34), it follows that for n sufficiently large, we have r ‚â• Œæ. Using

Markov‚Äôs inequality, the preceding relation implies
Ô£´
Ô£∂
P
n,
n
X
i| |HSC(i)
|‚â•kÃÑ E[Mi ]
1 ‚àí Min,
1
Ô£¨
Ô£∑
PÔ£≠
> Ô£∏ ‚â• 1 ‚àí
¬∑
.
n
n
r‚àí
n

(36)

i| |HSC(i) |‚â•kÃÑ

n
By Lemma 2 and observation (33), E[Min, ] for an agent i with |HSC(i)
| ‚â• kÃÑ is upper bounded by

Ô£´ s
P(Min, = 0) ‚â• erf Ô£≠
and therefore

Ô£´ s
E[Min, ] ‚â§ 1 ‚àí erf Ô£≠

n
|HSC(i)
|œÅÃÑ

2

n
|HSC(i)
|œÅÃÑ

2

Ô£∂
Ô£∏,

Ô£∂
Ô£∏.

Now assuming that social cliques are ordered by size (H1n is the biggest), we can re-write Eq. (36) as
Ô£´
Ô£∂
n,
X
1 ‚àí Mi
Ô£¨
Ô£∑
PÔ£≠
> Ô£∏ ‚â•
n
n
i| |HSC(i) |‚â•kÃÑ

P|HkÃÑn |

n
j=1 |Hj |


 q n 
|Hj |œÅÃÑ
1 ‚àí erf 
2

‚â•1‚àí

(r ‚àí ) ¬∑ n
r ¬∑ (1 ‚àí Œ∂)
Œæ ¬∑ (1 ‚àí Œ∂)
‚â•1‚àí
‚â•1‚àí
>Œ¥
r‚àí
Œæ‚àí

(37)

Here, the second inequality is obtained
 q since the largest value for the sum is achieved when all summands are equal and Œ∂ = erf  kÃÑœÅÃÑ
2 . The third inequality holds using the relation r ‚â• Œæ and
choosing appropriate values for , Œ¥.
This establishes that for all sufficiently large n, we have
!
n
X
1 ‚àí Min,
P
>  > Œ¥ > 0,
n
i=1

39

which implies
lim sup P
n‚Üí‚àû

n
X
1 ‚àí M n,
i

n

i=1

!
>

> Œ¥,

and shows that perfect asymptotic learning does not occur in any network equilibrium.
(b)

We show that if the communication cost structure satisfies condition (12), then asymptotic learn-

ing occurs in all network equilibria (g, œÉ) = ({g n , œÉ n })‚àû
n=1 . For an illustration of the resulting communication networks, when condition (13) holds, refer to Figure 7(a). Let Bin (Gn ) be the neighborhood
of agent i in communication network Gn (induced by the link formation strategy g n ),
Bin (Gn ) = {j

there exists a path P in Gn from j to i},

i.e., Bin (Gn ) is the set of agents in Gn whose information agent i can acquire over a sufficiently large
(but finite) period of time.
n
We first show that for any agent i such that lim supn‚Üí‚àû HSC(i)
< kÃÑ, her neighborhood in any

network equilibrium satisfies limn‚Üí‚àû Bin = ‚àû. We use the notion of an isolated social clique to show
this. For a given n, we say that a social clique H`n is isolated (at a network equilibrium (g, œÉ)) if no
agent in H`n forms a costly link with an agent outside H`n in (g, œÉ). Equivalently, a social clique H`n is
not isolated if there exists at least one agent j ‚àà H`n , such that j incurs cost c and forms a link with
an agent outside H`n .
n
n
We show that for an agent i with lim supn‚Üí‚àû HSC(i)
< kÃÑ, the social clique HSC(i)
is not isolated

in any network equilibrium for all sufficiently large n. Using condition (12), we can assume without loss
of generality that social cliques are ordered by size from largest to smallest and that limn‚Üí‚àû |H1n | = ‚àû.
n
Suppose that HSC(i)
is isolated in a network equilibrium (g, œÉ). Then the expected payoff of agent i

is upper bounded (similarly with above)

n
E[Œ†i (g )] ‚â§ max œÄ ‚àí

1
Œª
,
œÅ + œÅÃÑ r + Œª



1
œÄ‚àí
œÅ + œÅÃÑ(kÃÑ ‚àí 1)



Using the definition of kÃÑ, it follows that for some  > 0,
(
)

2
1
Œª
E[Œ†i (g n )] ‚â§ max œÄ ‚àí
,
œÄ‚àíc‚àí
œÅ + œÅÃÑ r + Œª

(38)

Suppose next that agent i forms a link with an agent j ‚àà H1n . Her expected payoff E[Œ†i (g n )]
satisfies
E[Œ†i (g n )] ‚â•



Œª
r+Œª

2
¬∑

1
œÄ‚àí
œÅ + œÅÃÑ H1n

!
‚àí c,

since in two time steps, she has access to the signals of all agents in the social clique H1n . Since

40

limn‚Üí‚àû |H1n | = ‚àû, there exists some N1 such that
n



E[Œ†i (g )] >

Œª
Œª+r

2
œÄ‚àíc‚àí

for all n > N1 .

Comparing this relation with Eq. (38), we conclude that under the assumption that r < rÃÑ (for appron
priate rÃÑ), the social clique HSC(i)
is not isolated in any network equilibrium for all n > N1 .

Next, we show that limn‚Üí‚àû |Bin | = ‚àû in any network equilibrium. Assume to arrive at a contradiction that lim supn‚Üí‚àû |Bin | < ‚àû in some network equilibrium. This implies that lim supn‚Üí‚àû |Bin | <
n
|H1n | for all n > N2 > N1 . Consider some n > N2 . Since HSC(i)
is not isolated, there exists some
n
n
j ‚àà HSC(i)
such that j forms a link with an agent h outside HSC(i)
. Since lim supn‚Üí‚àû |Bin | < |H1n |,
n = 0 and g n = 1 for h0 ‚àà H n , i.e., j is
agent j can improve her payoff by changing her strategy to gjh
1
jh0

better off deleting her existing costly link and forming one with an agent in social clique H1n . Hence,
for any network equilibrium, we have
lim |Bin | = ‚àû

n
for all i with lim sup |HSC(i)
| < kÃÑ

n‚Üí‚àû

(39)

n‚Üí‚àû

We next consider the probability that a non-negligible fraction (-fraction) of agents takes an action
that is at least -away from optimal with probability at least Œ¥ along a network equilibrium (g, œÉ). For
any n, we have from Markov‚Äôs inequality
P

n
X
1 ‚àí M n,
i

n

i=1

!
>

n
1 X E[1 ‚àí Min, ]
‚â§ ¬∑

n

(40)

i=1

We next provide upper bounds on the individual terms in the sum on the right-hand side. We have
!
r
œÅÃÑ|Bin |
n,
E[1 ‚àí Mi ] ‚â§ erf 
.
(41)
2
n
n
Consider an agent i with lim supn‚Üí‚àû |HSC(i)
| < kÃÑ (i.e., |HSC(i)
| < kÃÑ for all n large). By Eq. (39),

we have limn‚Üí‚àû |Bin | = ‚àû. Together with Eq. (41), this implies that for some Œ∂ > 0, there exists
some N such that for all n > N , we have
E[1 ‚àí Min, ] <

Œ∂
2

n
for all i with lim sup |HSC(i)
| < kÃÑ.

(42)

n‚Üí‚àû

n
Consider next an agent i with lim supn‚Üí‚àû |HSC(i)
| ‚â• kÃÑ, and for simplicity, let us assume that the
n
limit exists, i.e., limn‚Üí‚àû |HSC(i)
| ‚â• kÃÑ.12
12

The case when the limit does not exist can be proven by focusing on different subsequences. In particular, along
n
any subsequence Ni such that limn‚Üí‚àû,n‚ààNi |HSC(i)
| ‚â• kÃÑ, the same argument holds. Along any subsequence Ni with
n
limn‚Üí‚àû,n‚ààNi |HSC(i) | < kÃÑ, we can use an argument similar to the previous case to show that limn‚Üí‚àû,n‚ààNi |Bin | = ‚àû,
and therefore E[1 ‚àí Min, ] < 2Œ∂ for n large and n ‚àà Ni .

41

n
This implies that |HSC(i)
| ‚â• kÃÑ for all large n, and therefore,

Min, ]

X
n
i| lim supn‚Üí‚àû |HSC(i)
|‚â•kÃÑ

E[1 ‚àí
n

|Hkn |

‚â§

X

Ô£´ s
|Hjn |

¬∑ erf Ô£≠

œÅÃÑ|Hjn |

j=1

2

Ô£∂
Ô£∏‚â§

|HkÃÑn |
¬∑ kÃÑ,
n

where the first inequality follows from Eq. (41). Using condition (12), i.e., limn‚Üí‚àû

HkÃÑn
n

= 0, this

relation implies that there exists some NÃÉ such that for all n > NÃÉ , we have
X
n
i| lim supn‚Üí‚àû |HSC(i)
|‚â•kÃÑ

E[1 ‚àí Min, ]
Œ∂
<
.
n
2

(43)

Combining Eqs. (42) and (43) with Eq. (40), we obtain for all n > max {N, NÃÉ },
!
n
X
1 ‚àí Min,
>  < Œ∂,
P
n
i=1

where Œ∂ > 0 is an arbitrary scalar. This implies that
lim P

n‚Üí‚àû

n
X
1 ‚àí M n,
i

i=1

n

!
>

= 0,

for all , showing that perfect asymptotic learning occurs along every network equilibrium.
(c)

The proof proceeds in two parts. First, we show that if condition (13) is satisfied, learning occurs

in at least one network equilibrium (g, œÉ). Then, we show that there exists a cÃÑ > 0, such that if c < cÃÑ,
then learning occurs in all network equilibria. We complete the proof by showing that if c > cÃÑ, then
there exist network equilibria, in which asymptotic learning fails, even when condition (13) holds. We
consider the case when agents are patient, i.e., the discount rate r ‚Üí 0. We consider kÃÑ, such that
c>

1
œÅ+œÅÃÑkÃÑ

and c <

1
œÅ+œÅÃÑ(kÃÑ‚àí1)

‚àí 0 , for some 0 > 0 (such a kÃÑ exists). Finally, we assume that c <

1
œÅ+œÅÃÑ ,

since otherwise no agent would have an incentive to form a costly link.
Part 1: We assume, without loss of generality, that social cliques are ordered by size (H1n is the smalln denote the set of social cliques of size less than kÃÑ, i.e., Hn = {H n , i = 1, . . . , K n | |H n | <
est). Let H<
i
i
kÃÑ
<kÃÑ

kÃÑ}. Finally, let rec(j) and send(j) denote two special nodes for social clique Hjn , the receiver and the
sender (they might be the same node). We claim that (g n , œÉ n ) described below and depicted in Figure
7(b) is an equilibrium of the network learning game Œì(C n ) for n large enough and Œ¥ sufficiently close
to one.
Ô£±
1 if SC(i) = SC(j), i.e., i, j belong to the same social clique,
Ô£¥
Ô£¥
Ô£≤ 1 if i = rec(` ‚àí 1) and j = send(`) for 1 < ` ‚â§ |Hn |,
n
<kÃÑ
gij
=
n |) and j = send(1),
1 if i = rec(|H<
Ô£¥
Ô£¥
kÃÑ
Ô£≥
0 otherwise
and œÉ n ‚àà IN F O(Gn ), where Gn is the communication network induced by g n . In this communication
network, social cliques with size less than kÃÑ are organized in a directed ring, and all agents i, such
42

n
that |HSC(i)
| < kÃÑ have the same neighborhood, i.e., Bin = B n for all such agents.

Next, we show that the strategy profile (g n , œÉ n ) described above is indeed an equilibrium of the network
learning game Œì(C n ). We restrict attention to large enough n‚Äôs. In particular, let N be such that
N |
P|H<
kÃÑ
N
i=1 |Hi | > kÃÑ and consider any n > N (such N exists from condition (13)). Moreover, we assume
that the discount rate is sufficiently close to zero. We consider the following two cases.
n = 1 if and only if SC(j) = SC(i). Agent i‚Äôs neighborhood
Case 1: Agent i is not a connector. Then, gij
1
as noted above is set B n , which is such that œÄ ‚àí œÅ+œÅÃÑ|B
n | > œÄ ‚àí c from the assumption on n, i.e., n > N ,
N |
P|H<
where N such that i=1 kÃÑ |HiN | > kÃÑ. Agent i can communicate with all agents in B n in at most |H<kÃÑ |

communication steps. Therefore, her expected payoff is lower-bounded by


n

E[Œ†i (g )] ‚â•

Œª
Œª+r



n
H<
kÃÑ


¬∑ œÄ‚àí

1
œÅ + œÅÃÑkÃÑ


> œÄ ‚àí c,

under any equilibrium œÉ n for r sufficiently close to zero. Agent i can deviate by forming a costly
link with agent m, such that SC(m) 6= SC(i). However, this is not profitable since from above her
expected payoff under (g n , œÉ n ) is at least œÄ ‚àí c (which is the maximum possible payoff if an agent
chooses to form a costly link).
n = 1.
Case 2: Agent i is a connector, i.e., there exists exactly one j, such that SC(j) 6= SC(i) and gij

Using a similar argument as above we can show that it is not profitable for agent i to form an additional
costly link with an agent m, such that SC(m) 6= SC(i). On the other hand, agent i could deviate by
n = 0. However, then her expected payoff would be
setting gij



Œª
1
1
,
œÄ‚àí
E[Œ†i (g )] = max œÄ ‚àí
œÅ + œÅÃÑ r + Œª
œÅ + œÅÃÑ|Hin |



1
Œª
1
‚â§ max œÄ ‚àí
,
œÄ‚àí
< œÄ ‚àí c ‚àí 0
œÅ + œÅÃÑ r + Œª
œÅ + œÅÃÑ(kÃÑ ‚àí 1)

 Hn 

<kÃÑ
Œª
1
<
œÄ‚àí
‚àí c ‚àí ,
r+Œª
œÅ + œÅÃÑ|B n |
n



(44)

for discount rate sufficiently close to zero. Therefore deleting the costly link is not a profitable deviation. Similarly we can show that it a (weakly) dominant strategy for the connector not to replace her
costly link with another costly link.
We showed that (g n , œÉ n ) is an equilibrium of the network learning game. Note that we described a
link formation strategy, in which social cliques connect to each other in a specific order (in increasing
n | cliques is an
size). There is nothing special about this ordering and any permutation of the first |H<
kÃÑ

equilibrium as long as they form a directed ring. Finally, any node in a social clique can be a receiver
or a sender.
Next, we argue that asymptotic learning occurs in network equilibria (g, œÉ) = {(g n , œÉ n )}‚àû
n=1 ,
where for all n > N , N is a large constant, g n has the form described above. As shown above,

43

H`n1

X

H`n

H`n2

X

(a) Deviation for i ‚àà H`n1 - property (i).

(b) Deviation for i ‚àà H`n - property (ii).

Figure 9: Communication networks under condition (13).
n
all agents i for which HSC(i)
< kÃÑ have the same neighborhood, which we denoted by B n . Moreover,

limn‚Üí‚àû |B n | = ‚àû, since social cliques with size less than kÃÑ are connected to the ring and, by condition
P
(13), limn‚Üí‚àû i| |H n |<kÃÑ |Hin | = ‚àû. For discount rate r sufficiently close to zero and from arguments
i

similar to those in the proof of part (b), we conclude that asymptotic learning occurs in network
equilibria (g, œÉ).
Part 2: We have shown a particular form of network equilibria, in which asymptotic learning occurs.
The following proposition states that for discount rate sufficiently close to zero network equilibria fall
in one of two forms.
Proposition 9. Suppose Assumptions 1, 3 and condition (13) hold. Then, an equilibrium (g n , œÉ n ) of
the network learning game Œì(C n ) can be in one of the following two forms.
n |,
(i) (Incomplete) Ring Equilibrium: Social cliques with indices {1, ¬∑ ¬∑ ¬∑ , j}, where j ‚â§ |H<
kÃÑ

form a directed ring as described in Part 1 and the rest of the social cliques are isolated. We
call those equilibria ring equilibria and, in particular, a ring equilibrium is called complete if
n |, i.e., if all social cliques with size less than kÃÑ are not isolated.
j = |H<
kÃÑ
n |, and clique
(ii) Directed Line Equilibrium: Social cliques with indices {1, ¬∑ ¬∑ ¬∑ , j}, where j ‚â§ |H<
kÃÑ
n | (the largest clique) form a directed line with the latter being the endpoint. The
with index |HK
n

rest of the social cliques are isolated.
Proof. Let (g n , œÉ n ) be an equilibrium of the network learning game Œì(C n ). Monotonicity of the
expected payoff as a function of the number of signals observed implies that if clique H`n is not
isolated, then no clique with index less than ` is isolated in the communication network induced by g n .
In particular, let conn(`) be the connector of social clique H`n and E[Œ†conn(`) (g n )] be her expected
payoff. Consider an agent i such that SC(i) = `0 < ` and, for the sake of contradiction, H`n0 is
isolated in the communication network induced by g n . Social cliques are ordered by size, therefore,
|H`n0 | ‚â§ |H`n |. Now, we use the monotonicity mentioned above. Consider the expected payoff of i:



1
Œª
1
n
E[Œ†i (g )] = max œÄ ‚àí
,
œÄ‚àí
œÅ + œÅÃÑ Œª + r
œÅ + œÅÃÑ|H`n0 |



1
Œª
1
‚â§ max œÄ ‚àí
,
œÄ‚àí
< E[Œ†conn(`) (g n )],
(45)
œÅ + œÅÃÑ Œª + r
œÅ + œÅÃÑ|H`n |
44

where the last inequality follows from the fact that agent conn(`) formed a costly link. Consider a
n,deviation
n,deviation
n , i.e., agent i forms a
deviation, gin,deviation for agent i, in which gi,conn(`)
= 1 and gij
= gij

costly link with agent conn(`). Then,
E[Œ†i (g n,deviation )] ‚â•

Œª
E[Œ†conn(`) (g n )] > E[Œ†i (g n )],
Œª+r

from (45) and for discount rate sufficiently close to zero. Therefore, social clique H`n0 will not be
isolated in any network equilibrium (g n , œÉ n ).
Next, we show two structural properties that all network equilibria (g n , œÉ n ) should satisfy, when
the discount rate r is sufficiently close to one. We say that there exists a path P between social cliques
H`n1 and H`n2 , if there exists a path between some i ‚àà H`n1 and j ‚àà H`n2 . Also, we say that the in-degree
(out-degree) of social clique H`n1 is k, if the sum of in-links (out-links) of the nodes in H`n1 is k, i.e.,
P
P
n
H`n1 has in-degree k if
i‚ààH n
j ‚ààH
/ n gij = k.
`1

`1

(i) Let H`n1 , H`n2 be two social cliques that are not isolated. Then, there should exist a directed path
P in Gn induced by g n between the two social cliques.
(ii) The in-degree and out-degree of each social clique is at most one.
Figure 9 provides an illustration of why the properties hold for patient agents. In particular, for
property (i), let i = conn(H`n1 ) and j = conn(H`n2 ) and assume, without loss of generality, that
|Bin | ‚â§ |Bjn |. Then, for discount rate sufficiently close to zero and from monotonicity of the expected
payoff, we conclude that i has an incentive to deviate, delete her costly and form a costly link with
agent j. Property (ii) follows due to similar arguments. From the above, we conclude that the only
two potential equilibrium topologies are the (incomplete) ring and the directed line with the largest
clique being the endpoint under the assumptions of the proposition.
So far we have shown a particular form of network equilibria that arise under condition (13), in
which asymptotic learning occurs. We also argued that under condition (13) only (incomplete) ring
or directed line equilibria can arise for network learning game Œì(C n ). In the remainder we show that
there exists a bound cÃÑ > 0 on the common cost c for forming a link between two social cliques, such
that if c < cÃÑ all network equilibria (g, œÉ) that arise satisfy that g n is a complete ring equilibrium for
all n > N , where N is a constant. In those network equilibria asymptotic learning occurs as argued
in Part 1. On the other hand, if c > cÃÑ coordination among the social cliques may fail and additional
equilibria arise in which asymptotic learning does not occur. Let
)
(
1
1
+
cÃÑn = min ‚àí
P
n |)
n |)
k
œÅ + œÅÃÑ|Hk+1
œÅ + œÅÃÑ( kj=1 |Hjn | + |Hk+1

45

(46)

n | and
where k1 ‚â§ k < |H<
kÃÑ

Pk1

n
j=1 |Hj |

n | (size of the largest social clique). Moreover, let
‚â• |HK
n

cÃÑ = lim inf cÃÑn .
n‚Üí‚àû

The following proposition concludes the proof.
Proposition 10. Suppose Assumptions 1, 3 and condition (13) hold. If c < cÃÑ asymptotic learning
occurs in all network equilibria (g, œÉ). Otherwise, there exist equilibria in which asymptotic learning
does not occur.
Proof. Let the common cost c be such that c < cÃÑ, where cÃÑ is defined as above, and consider a network
equilibrium (g, œÉ). Let N be a large enough constant and consider the corresponding g n for n > N .
We claim that g n is a complete ring equilibrium for all such n. Assume for the sake of contradiction
that the claim is not true. Then, from Proposition 9, g n is either an incomplete ring equilibrium or
a directed line equilibrium. We consider the former case (the latter case can be shown with similar
arguments). There exists an isolated social clique H`n , such that |H`n | < kÃÑ and all cliques with index
less than ` are not isolated and belong to the incomplete ring. However, from the definition of cÃÑ we
obtain that an agent i ‚àà H`n would have an incentive to connect to the incomplete ring, thus we reach a
n,deviation
contradiction. In particular, consider the following link formation strategy for agent i: gim
=1
n,deviation
n for j 6= m. Then,
n
and gij
= gij
for agent m ‚àà H`‚àí1

E[Œ†ni (g n,deviation )]

!
|H n |
<kÃÑ
Œª
1
‚â•
œÄ‚àí
‚àíc
P
n | + |H n |)
Œª+r
|H
œÅ + œÅÃÑ( `‚àí1
j=1
j
`



1
Œª
1
> max œÄ ‚àí
,
œÄ‚àí
= E[Œ†ni (g n )],
œÅ + œÅÃÑ Œª + r
œÅ + œÅÃÑ|H`n |


where the strict inequality follows from the definition of cÃÑ for r sufficiently close to zero. Thus, we
conclude that if c < cÃÑ, g n is a complete ring for all n > N , where N is a large constant, and from
Part 1 asymptotic learning occurs in all network equilibria (g, œÉ). On the contrary, if c > cÃÑ, then there
exists an infinite index set W , such that for all n in the (infinite) subsequence, {nw }w‚ààW , there exists
a k, such that
1

‚àíc<

1
n |.
œÅ + œÅÃÑ|Hk+1

(47)
œÅ + œÅÃÑ(
+
Pk
n | < kÃÑ and
n
n
Moreover, |Hk+1
j=1 |Hj | ‚â• |HK n |. We conclude that for (47) to hold it has to be that
Pk

n
j=1 |Hj |

Pk

n
j=1 |Hj |

n |)
|Hk+1

< R, where R is a uniform constant for all n in the subsequence. Consider (g, œÉ)‚àû
n=1 , such

that for every n in the subsequence, g n is such that social cliques with index greater than k (as described
above) are isolated and the rest form an incomplete ring or a directed line and œÉ n = IN F O(Gn ), where
Gn is the communication network induced by g n . From above, we obtain that for c > cÃÑ, (g n , œÉ n ) is an
equilibrium of the network learning game Œì(C n ). perfect assymptotic learning, however, fails in such
an equilibrium, since for every i ‚àà N n , |Bin | ‚â§ R, where Bin denotes the neighborhood of agent i.
46

References
Acemoglu, D., M. Dahleh, I. Lobel, and A. Ozdaglar (2010): ‚ÄúBayesian learning in social
networks,‚Äù Working paper.
Ambrus, A., E. Azevedo, and Y. Kamada (2010): ‚ÄúHierarchical Cheap Talk,‚Äù Working paper.
Angeletos, G., and A. Pavan (2007): ‚ÄúEfficient use of information and social value of information,‚Äù Econometrica, 75(4), 1103‚Äì1142.
Austen-Smith, D. (1990): ‚ÄúInformation transmission in debate,‚Äù American Journal of Political
Science, 34(1), 124‚Äì152.
Austen-Smith, D., and J. Banks (1996): ‚ÄúInformation aggregation, rationality, and the Condorcet
jury theorem,‚Äù American Political Science Review, 90(1), 34‚Äì45.
Bala, V., and S. Goyal (1998): ‚ÄúLearning from neighbours,‚Äù Review of Economic Studies, 65(3),
595‚Äì621.
(2000): ‚ÄúA noncooperative model of network formation,‚Äù Econometrica, 68(5), 1181‚Äì1229.
Banerjee, A. (1992): ‚ÄúA simple model of herd behavior,‚Äù Quarterly Journal of Economics, 107(3),
797‚Äì817.
Banerjee, A., and D. Fudenberg (2004): ‚ÄúWord-of-mouth learning,‚Äù Games and Economic
Behavior, 46, 1‚Äì22.
BarabaÃÅsi, A., and R. Albert (1999): ‚ÄúEmergence of scaling in random networks,‚Äù Science,
286(5439), 509‚Äì512.
Bikhchandani, S., D. Hirshleifer, and I. Welch (1992): ‚ÄúA theory of fads, fashion, custom,
and cultural change as informational cascades,‚Äù Journal of Political Economy, 100(5), 992‚Äì1026.
Crawford, V., and J. Sobel (1982): ‚ÄúStrategic information transmission,‚Äù Econometrica, 50(6),
1431‚Äì1451.
DeMarzo, P., D. Vayanos, and J. Zwiebel (2003): ‚ÄúPersuasion Bias, Social Influence, and
Unidimensional Opinions,‚Äù Quarterly Journal of Economics, 118(3), 909‚Äì968.
Duffie, D., S. Malamud, and G. Manso (2009): ‚ÄúInformation Percolation with Equilibrium
Search Dynamics,‚Äù Econometrica, 77(5), 1513‚Äì1574.
Ellison, G., and D. Fudenberg (1995): ‚ÄúWord-of-mouth learning,‚Äù The Quarterly Journal of
Economics, 110, 93‚Äì126.
Farrell, J., and R. Gibbons (1989): ‚ÄúCheap talk with two audiences,‚Äù American Economic
Review, 79(5), 1214‚Äì1223.
Gale, D., and S. Kariv (2003): ‚ÄúBayesian learning in social networks,‚Äù Games and Economic
Behavior, 45(2), 329‚Äì346.
Galeotti, A. (2006): ‚ÄúOne-way flow networks: the role of heterogeneity,‚Äù Economic Theory, 29(1),
163‚Äì179.

47

Galeotti, A., C. Ghiglino, and F. Squintani (2010): ‚ÄúStrategic information transmission in
networks,‚Äù Working paper.
Galeotti, A., and S. Goyal (2010): ‚ÄúThe law of the few,‚Äù forthcoming in the American Economic
Review.
Galeotti, A., S. Goyal, and J. Kamphorst (2006): ‚ÄúNetwork formation with heterogeneous
players,‚Äù Games and Economic Behavior, 54(2), 353‚Äì372.
Gladwell, M. (2000): The Tipping Point: How little things can make a big difference. Little Brown.
Golub, B., and M. Jackson (2010): ‚ÄúNaƒ±Ãàve Learning in Social Networks and the Wisdom of
Crowds,‚Äù American Economic Journal: Microeconomics, 2(1), 112‚Äì149.
Goyal, S. (2007): Connections: an introduction to the economics of networks. Princeton University
Press, Princeton, New Jersey.
Hagenbach, J., and F. Koessler (2010): ‚ÄúStrategic communication networks,‚Äù Review of Economic Studies, 77(3), 1072‚Äì1099.
Hojman, D., and A. Szeidl (2008): ‚ÄúCore and periphery in networks,‚Äù Journal of Economic
Theory, 139(1), 295‚Äì309.
Jackson, M. (2004): A survey of models of network formation: stability and efficiency. In group
formation in Economics; Networks, clubs and coalitions, edited by Gabrielle Demange and Myrna
Wooders. Cambridge University Press.
(2008): Social and economic networks. Princeton University Press, Princeton, New Jersey.
Jackson, M., and B. Rogers (2006): ‚ÄúSearch in the formation of large networks: How random
are social networks?,‚Äù American Economic Review, 97(3), 890‚Äì915.
Mobius, M., T. Phan, and A. Szeidl (2010): ‚ÄúTreasure Hunt: Social Learning in Real-World
Social Networks,‚Äù Working paper.
Morgan, J., and P. Stocken (2008): ‚ÄúInformation aggregation in polls,‚Äù American Economic
Review, 98(3), 864‚Äì896.
Morris, S., and H. S. Shin (2002): ‚ÄúSocial value of public information,‚Äù American Economic
Review, 92(5), 1521‚Äì1534.
Smith, L., and P. S√∏rensen (2000): ‚ÄúPathological outcomes of observational learning,‚Äù Econometrica, 68(2), 371‚Äì398.
(2010): ‚ÄúRational social learning with random sampling,‚Äù Working paper.

48

