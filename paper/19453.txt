NBER WORKING PAPER SERIES

CAUSAL ANALYSIS AFTER HAAVELMO
James J. Heckman
Rodrigo Pinto
Working Paper 19453
http://www.nber.org/papers/w19453
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2013

We thank Olav Bjerkholt for very helpful comments, participants at the Haavelmo symposium, Oslo,
Norway, December, 2011, as well as Steven Durlauf, Maryclare Griffin and Cullen Roberts, three
anonymous referees, participants in the seminar on Quantitative Research Methods in Education, Health
and Social Sciences at the University of Chicago, March 2013, and participants at a seminar at UCL,
September 4, 2013. Steve Stigler gave us helpful bibliographic references. This research was supported
in part by the American Bar Foundation, a grant from the European Research Council DEVHEALTH269874, and NICHD R37-HD065072. The views expressed in this paper are those of the authors and
not necessarily those of the funders or commentators mentioned here, nor of the National Bureau of
Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2013 by James J. Heckman and Rodrigo Pinto. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including
© notice, is given to the source.

Causal Analysis after Haavelmo
James J. Heckman and Rodrigo Pinto
NBER Working Paper No. 19453
September 2013
JEL No. C10,C18
ABSTRACT
Haavelmo's seminal 1943 paper is the ˝first rigorous treatment of causality. In it, he distinguished
the definition of causal parameters from their identification. He showed that causal parameters are
defined using hypothetical models that assign variation to some of the inputs determining outcomes
while holding all other inputs ˝fixed. He thus formalized and made operational Marshall's (1890) ceteris
paribus analysis. We embed Haavelmo's framework into the recursive framework of Directed Acyclic
Graphs (DAG) used in one influential recent approach to causality (Pearl, 2000) and in the related
literature on Bayesian nets (Lauritzen, 1996). We compare an approach based on Haavelmo's methodology
with a standard approach in the causal literature of DAGs– the "do-calculus" of Pearl (2009). We discuss
the limitations of DAGs and in particular of the do-calculus of Pearl in securing identification of economic
models. We extend our framework to consider models for simultaneous causality, a central contribution
of Haavelmo (1944). In general cases, DAGs cannot be used to analyze models for simultaneous causality,
but Haavelmo's approach naturally generalizes to cover it.
James J. Heckman
Department of Economics
The University of Chicago
1126 E. 59th Street
Chicago, IL 60637
and University College Dublin and IZA
and also NBER
jjh@uchicago.edu
Rodrigo Pinto
Department of Economics
The University of Chicago
1126 E. 59th Street
Chicago, IL 60637
rodrig@uchicago.edu

1

Trygve Haavelmo’s Causality

Trygve Haavelmo made fundamental contributions to understanding the formulation and
identiﬁcation of causal models. In two seminal papers (1943, 1944), he formalized the distinction between correlation and causation,1 laid the foundation for counterfactual policy analysis
and distinguished the concept of “ﬁxing” from the statistical operation of conditioning—a
central tenet of structural econometrics. He developed an empirically operational version of
Marshall’s notion of ceteris paribus (1890) which is a central notion of economic theory.
In Haavelmo’s framework, the causal eﬀects of inputs on outputs are determined by
the impacts of hypothetical manipulations of inputs on outputs which he distinguishes from
correlations between inputs and outputs in observational data. The causal eﬀect of an input is
deﬁned using a hypothetical model that abstracts from the empirical data generating process
by making hypothetical variation in inputs that are independent of all other determinants of
outputs. As a consequence, Haavelmo’s notion of causality relies on a thought experiment
in which the model that governs the observed data is extended to allow for independent
manipulation of inputs, irrespective of whether or not they vary independently in the data.
Haavelmo formalized Frisch’s notion that “causality is in the mind.”2 Causal eﬀects
1
To our knowledge, the ﬁrst recorded statement of the distinction correlation and causation is due
to Fechner (1851), who distinguished “causal dependency” from what he called “functional relationship”.
See Heidelberger (2004, p. 102). In later work, Yule (1895, footnote 2, p. 605) discussed the distinction
between correlation and causation in a discussion of the eﬀect of relief payments on pauperism. We thank,
respectively, Olav Bjerkholt and Steve Stigler for these references.
2
This notion is central to structural econometrics. It was developed by Frisch and participants in his
laboratory going back to at least 1930:

“. . . we think of a cause as something imperative which exists in the exterior world. In my
opinion this is fundamentally wrong. If we strip the word cause of its animistic mystery, and
leave only the part that science can accept, nothing is left except a certain way of thinking, an
intellectual trick . . . which has proved itself to be a useful weapon . . . the scientiﬁc . . . problem
of causality is essentially a problem regarding our way of thinking, not a problem regarding the
nature of the exterior world.” (Frisch 1930, p. 36, published 2011)
Writing in the heyday of the Frisch-Haavelmo-inspired Cowles Commission in the late 1940’s, Koopmans and
Reiersøl distinguished descriptive statistical inference form structural estimation in the following statement.
“In many ﬁelds the objective of the investigator’s inquisitiveness is not just a “population” in
the sense of a distribution of observable variables, but a physical structure projected behind this
distribution, by which the latter is thought to be generated. The word “physical” is used merely
to convey that the structure concept is based on the investigator’s ideas as to the “explanation”
or “formation” of the phenomena studied, brieﬂy, on his theory of these phenomena, whether

3

are not empirical statements or descriptions of actual worlds, but descriptions of hypothetical worlds obtained by varying—hypothetically—the inputs determining outcomes. Causal
relationships are often suggested by observed phenomena, but they are abstractions from
it.

3

This paper revisits Haavelmo’s notions of causality using the mathematical language of
Directed Acyclic Graphs (DAGs). We start with a recursive framework less general than
that of Haavelmo (1943). This allows us to represent causal models as Directed Acyclic
Graphs which are intensively studied in the literature on Bayesian networks (Howard and
Matheson, 1981; Lauritzen, 1996; Pearl, 2000). We then consider the general non-recursive
framework of Haavelmo (1943, 1944) which cannot, in general, be framed within the context
of DAGs.
Following Haavelmo, we distinguish hypothetical models that are used to deﬁne causal
parameters as idealizations of empirical models that govern data generating processes. This
enables us to discuss causal concepts such as “ﬁxing” using an intuitive approach that draws
on Haavelmo’s notion of causality. Identiﬁcation relies on linking the parameters deﬁned in
a hypothetical model using data generated by an empirical model.
This paper makes the following contributions to the literature on causality: (1) We build
a framework for the study of causality inspired by Haavelmo’s concept of hypothetical variation of inputs; (2) In doing so, we express Haavelmo’s notion of causality in the mathematical
language of DAGs; (3) For this class of models, we compare the simplicity of Haavelmo’s
they are classiﬁed as physical in the literal sense, biological, psychological, sociological, economic
or otherwise.” (Koopmans and Reiersøl 1950, p. 165)
See Simon (1953), Heckman (2008) and Freedman et al. (2010), for later statements of this point of view.
3
All models—empirical or hypothetical—are idealized thought experiments. There are no formalized
rules for creating models, causal or empirical. Analysts may diﬀer about the inputs and relationships in
either type of model. A model is more plausible the more phenomena it predicts and the deeper are its
foundations in established theory. Causal models are idealizations of empirical models which are in turn
idealizations of phenomena. Some statisticians reject the validity of hypothetical models and seek to deﬁne
causality using empirical methods (Sobel, 2005). As an example we can cite the “Rubin model” of Holland
(1986), which equates establishing causality with the empirical feasibility of conducting experiments. This
approach confuses deﬁnition of causal parameters with their identiﬁcation from data. We refer to Heckman
(2005, 2008) for a discussion of this approach.

4

framework with the well-known causal framework of the do-calculus proposed by Pearl (2000)
which is beginning to be used in economics (see e.g. Margolis et al., 2012; White and Chalak,
2009); (4) We then discuss the limitations of the use of DAGs for econometric identiﬁcation.
We show that even in recursive models, the methods that rely solely on the information
in DAGs do not exploit identiﬁcation strategies based on functional restrictions and exclusion restrictions that are generated by economic theory. This limitation produces apparent
non-identiﬁcation in classically identiﬁed econometric models. We show how Haavelmo’s
approach naturally extends to notions of simultaneous causality while the DAG approach is
fundamentally recursive.
Our paper is mainly on the methodology of causality. We do not create a new concept
of causality, but rather propose a new framework within which to discuss it. We show that
Haavelmo’s approach is a complete framework for the study of causality which accommodates
the main tools of identiﬁcation used in the current literature in econometrics whereas other
approaches do not.
We show that the causal operation of ﬁxing described in Haavelmo (1943) and Heckman
(2005, 2008) is equivalent to statistical conditioning when embedded in a hypothetical model
that assigns independent variation to inputs with regard to all variables not caused by those
inputs. Pearl (2009) uses the term do for the concept of ﬁxing a variable. We show the
relationship between statistical conditioning in a hypothetical model and the do-operator.
Fixing, in our framework, diﬀers from the operation of the do-operator because it targets
speciﬁc causal links instead of variables that operate across multiple causal links. A beneﬁt
of targeting causal links is that it simpliﬁes the analysis of the subsets of causal relationships
associated with an input variable when compared to the do-operator.
Haavelmo’s approach allows for a precise yet intuitive deﬁnition of causal eﬀects. With
it, analysts can identify causal eﬀects by applying standard statistical tools. In contrast
with the do-calculus, application of Haavelmo’s concepts eliminates the need for additional
extra-statistical graphical/statistical rules to achieve identiﬁcation of causal parameters.

5

Haavelmo’s approach also covers the case of simultaneous causality in its full generality
whereas frameworks for causal analysis currently used in statistics cannot, except through
introduction and application of ad hoc rules.
This paper is organized in the following way. Section 2 reviews Haavelmo’s causal framework. Section 3 uses a modern framework of causality to assess Haavelmo’s contributions to
the literature. Section 4 examines how application of this framework diﬀers from Pearl’s docalculus (2009) and enables analysts to apply the standard tools of probability and statistics
without having to invent extra-statistical rules. It gives an example of the identiﬁcation of
causal eﬀects that considers Pearl’s “Front-Door” criteria. Section 5 discuss the limitations
of DAGs in implementing the variety of sources of identiﬁcation available to economists. We
focus on the simplest cases of confounding models where instrumental variables are available. Section 6 extends the discussion to a simultaneous equations framework. Section 7
concludes.

2

Haavelmo’s Causal Framework

We review the key concepts of causality developed by Haavelmo (1943, 1944)—starting with
a recursive model. A causal model is based on a system of structural equations that deﬁne
causal relationships among a set of variables. In the language of Frisch (1938), these structural equations are autonomous mechanisms represented by deterministic functions mapping
inputs to outputs. By autonomy we mean, as did Frisch, that these relationships remain
invariant under external manipulations of their arguments. They are functions in the ordinary usage of the term in mathematics. They produce the same values of the outcomes
when inputs are assigned to a ﬁxed set of values, however those values are determined. Even
though the functional form of a structural equation may be unknown, the causal directions
among the variables of a structural equation are assumed to be known. They are determined
by thought experiments that may sometimes be validated in data. The variables chosen as

6

arguments in a structural equation are assumed to account for all causes of the associated
output variable.
Haavelmo developed his work on causality for aggregate economic models. He considered
mean causal eﬀects and, for the sake of simplicity, invoked linearity, assumed uniformity
of responses to inputs across agents, and focused on continuous variables. More recent
approaches generalize his framework.
Haavelmo formalized the distinction between correlation and causation using a simple
model. In order to examine his ideas, consider three variables Y, X, U associated with error
terms  = (U , X , Y ) such that X, Y are observed by the analyst while variable U,  are
not.4 He assumed that U is a confounding variable that causes Y and X. We represent this
model through the following structural equations:

Y = fY (X, U, Y ),

X = fX (U, X ),

and U = fU (U ),

where  is a vector of mutually independent error terms with cumulative distribution function
Q . Thus, if X, U, Y take values of x, u, eY , then Y must take the value y = fY (x, u, eY ).
By iterated substitution we can express all variables in terms of . Moreover, the mutual
independence assumption of error terms implies that Y is independent of (X, U ) as X =
fX (fU (U ), X ) and U = fU (U ). Notationally, we write (X, U ) ⊥⊥ Y where ⊥⊥ denotes
statistical independence. In the same fashion, we have that X ⊥⊥ U but X is not independent
of U .
Haavelmo deﬁnes the causal eﬀect of X on Y as being generated by a hypothetical manipulation of variable X that does not aﬀect the values that U or  take. This is called ﬁxing X
by a hypothetical manipulation.5 Notationally, outcome Y when X is ﬁxed at x is denoted
4

This framework allows for uncertainty on the part of agents if realizations of the uncertain variables are
captured through variables X and U . In that sense the model can be characterized as a method for examining
ex-post relationships between variables. For a discussion of causal analysis of ex-post versus ex-ante models,
see, e.g., Hansen and Sargent (1980) and Heckman (2008).
5
Haavelmo (1943) did not explicitly use the term “ﬁxing.” He set U (in our notation) to a speciﬁed
value and manipulated X in his “hypothetical model.” Speciﬁcally, Haavelmo set U = 0 but the point of
evaluation is irrelevant in the linear case he analyzed.

7

by Y (x) = fY (x, U, Y ) and its expectation is given by E(U,Y ) (Y (x)) = E(f (x, U, Y )), where
E(U,Y ) (·) means expectation over the distribution of random variables U and Y . The average
causal eﬀect of X on Y when X takes values x and x is given by E(U,Y ) (Y (x))−E(U,Y ) (Y (x )).
For notational simplicity, we henceforth suppress the subscript on E denoting the random
variable with respect to which the expectation is computed.
Conditioning is a statistical operation that accounts for the dependence structure in the
data. Fixing is an abstract operation that assigns independent variation to the variable
being “ﬁxed”. The standard linear regression framework is convenient for illustrating these
ideas and in fact is the one used by Haavelmo (1943).
Consider the standard linear model Y = Xβ +U +Y where E(Y ) = 0 represent the data
generating process for Y. The expectation of outcome Y when X is ﬁxed at x is given by
E(Y (x)) = xβ + E(U ). This equation corresponds to Haavelmo’s (1943) hypothetical model.
The expectation of Y when X is conditioned on x is given by E(Y |X = x) = xβ + E(U |X =
x), as E(Y |X = x) = 0 because Y ⊥⊥ X. If E(U |X = x) = 0 and elements of X are
not collinear, then OLS identiﬁes β and E(Y |X = x) = E(Y (x)) = xβ and β generates
the average treatment eﬀect of a change in X on Y . Speciﬁcally, (x − x )β is the average
diﬀerence between the expectation of Y when X is ﬁxed at x and x .
The diﬃculty of identifying the average causal eﬀect of X on Y when E(U |X) = 0 (and
thereby E(Y |X = x) = E(Y (x))) stems from the potential confounding eﬀects of unobserved
variable U on X. In this case, the standard Least Squares estimator does not generate
an autonomous causal or structural parameter because plim(β̂) = β + cov(X, U )/ var(X)
depends on the covariance between X and U . While the concept of a causal eﬀect does not
rely on the properties of the data generating process, the identiﬁcation of causal eﬀects does.
Without linearity, one needs an assumption stronger than E(U |X = x) = 0 to obtain
E(Y |X = x) = E(Y (x)). Indeed if one assumes no confounding eﬀects of U , that is to say
that X and U are independent (X ⊥⊥ U ), then one can show that ﬁxing is equivalent to

8

statistical conditioning:

E(Y |X = x) =



=

fY (x, u, Y )dQ(U,Y )|X=x (u, Y )
fY (x, u, Y )dQU (u)dQY (Y )

= E(fY (x, U, Y ))
= E(Y (x)),

where dQ(U,Y )|X=x (u, Y ) denotes the cumulative joint distribution function of U, Y conditional on X = x and the second equality comes from as the fact that U, X and Y are
mutually independent. If X ⊥⊥ (U, Y ) holds, we can use observational data to identify the
mean value of Y ﬁxing X = x by evaluating the expected value of Y conditional on X = x.
Note that in general, the value obtained depends on the functional form of fY (x, u, Y ).
Haavelmo’s notation has led to some confusion in the statistical literature. His argument
was aimed at economists of the 1940s and does not use modern notation. Haavelmo’s key
deﬁnitions and ideas are given by examples rather than by formal deﬁnitions. We restate
and clarify his argument in this paper.
To simplify the exposition, assume that all variables are discrete and let P denote their
probability measure. The factorization of the joint distribution of Y, U conditional on X
is given by P(Y, U |X = x) = P(Y |U, X = x) P(U |X = x). In contrast, in the abstract
operation of ﬁxing X is assumed not to aﬀect the marginal distribution of U. That is to say
that U (x) = U. Therefore the joint distribution of Y, U when X is ﬁxed at x is given by
P(Y (x), U (x)) = P(Y (x), U ) = P(Y |U, X = x) P(U ).
Fixing lies outside the scope of standard statistical theory and is often a source of confusion. Indeed, even though the probabilities P(Y |U, X = x) and P(U ) are well deﬁned,
neither the causal operation of ﬁxing nor the resulting joint distribution follow from standard
statistical arguments.6 Conditioning is equivalent to ﬁxing under independence of X and
6

See Pearl (2009) and Spirtes et al. (2000) for discussions.

9

U . In this case the conditional joint distribution of Y and U becomes P(Y, U |X = x) =
P(Y |U, X = x) P(U |X = x) = P(Y |U, X = x) P(U ).
To gain more intuition on the diﬀerence between ﬁxing and standard statistical theory
express the conditional expectation E(Y |X = x) as the integral across  over a restricted set
AC . By iterated substitution, we can write Y as Y = fY (fX (fU (U ), X ), fU (U ), Y ). Thus

E(Y |X = x) =

AC

fY (fX (fU (U ), X ), fU (U ), Y )dQ ()

dQ ()
AC

where AC = { = (U , X , Y ) ∈ supp() ; fX (fU (U ), X ) = x}.

(1)
(2)

Fixing, on the other hand, is written as the integral across  over its full support:

E(Y (x)) =

AF

fY (x, fU (U ), Y )dQ ()

dQ ()
AF

F

where A = { = (U , X , Y ) ∈ supp()} and

(3)

AF

dQ () = 1.

(4)

Fixing diﬀers from conditioning in terms of the diﬀerence in the integration sets AF and AC .
While conditional expectation (1) is a standard operation in statistics, the operation used to
deﬁne ﬁxing is not. Equation (1) is an expectation conditional on the event fX (fU (U ), X ) =
x, which aﬀects the integration set AC as given in (2). Fixing (3), on the other hand,
integrates the function fY (x, fU (U ), Y ) across the whole support of  as given in (4). The
inconsistency between ﬁxing and conditioning in the general case comes from the fact that
ﬁxing X is equivalent to setting the expression fX (fU (U ), X ) to x without changing the
probability measures of U , X associated with the operation of conditioning on the event
X = x.
This paper interprets Haavelmo’s approach by introducing a hypothetical model that
enables analysts to examine ﬁxing using standard tools of probability. The hypothetical
model departs from the data generating process by exploiting autonomy and creating a
hypothetical variable that has the desired property of independent variation with regard to

10

U . The hypothetical model is an idealization of the empirical model. Standard statistical
tools apply to both the data generating process and the hypothetical model.
To formalize Haavelmo’s notions of causality, let a hypothetical model with error terms
 and four variables including Y, X, U but also a new variable X̃ with the property that
X̃ ⊥⊥ (X, U, ).7 Invoking autonomy, the hypothetical model shares the same structural
equation as the empirical one but departs from it by replacing X with an X̃-input, namely
Y = fY (X̃, U, Y ). The hypothetical model is not a wildly speculative departure from the
empirical data generating process but an expanded version of it. Thus (Y |X = x, U = u) =
fY (x, u, Y ) in the empirical model and (Y |X̃ = x, U = u) = fY (x, u, Y ) in the hypothetical
model. The hypothetical model has the same marginal distribution of U as the empirical
model. The joint distributions of variables in the empirical model PE and the hypothetical
model PH may diﬀer.
The hypothetical model clariﬁes the notion of ﬁxing in the empirical model. Fixing in the
empirical model is based on non-standard statistical operations. However, the distribution
of the outcome Y when X is ﬁxed at x in the empirical model can be interpreted as standard
statistical conditioning in the hypothetical model, namely, PE (Y (x)) = PH (Y |X̃ = x). The
next section formalizes this notion using one modern language of causality.8

3

Haavelmo’s Framework Recast in a Modern Framework of Causality

We recast Haavelmo’s model in the framework of Directed Acyclic Graphs (DAGs). DAGs
are studied in Bayesian Networks (Howard and Matheson, 1981; Lauritzen, 1996) and are
often used to deﬁne and estimate causal relationships (Lauritzen, 2001). The literature on
7

We could express X̃ = fX̃ (X̃ ) to be notationally consistent.
Frisch’s (1938) notion of invariance used by Haavelmo is called SUTVA in one model of causality popular
in statistics. See Holland (1986) and Rubin (1986).
8

11

causality based on DAGs was advanced by Judea Pearl (2000, 2009).9
In this fundamentally recursive framework, a causal model consists of a set of variables
T = {V1 , . . . , Vn } associated with a set of mutually independent error terms  = {1 , . . . , n }
and a system of autonomous structural equations {f1 , . . . , fn }. Variable set T includes both
observed and unobserved variables. Variable set T also include both external and internal
variables. We clarify these concepts in the following way.
Causal relationships between a dependent variable Vi ∈ T and its arguments are deﬁned
by Vi = fi (P a(Vi ), i ), where P a(Vi ) ⊂ T and i ∈  are called parents of Vi and are said
to directly cause Vi . If P a(V ) = ∅ then variable V is not caused by any variable in T . In
this case, V is an external variable determined outside the system, otherwise the variable
is called an internal or endogenous variable. The error terms in  are not caused by any
variable and are introduced to avoid degenerate conditioning statements among variables in
T . For simplicity of notation, we keep the error terms  implicit, except when it clariﬁes
matters to do so. We assume that all random variables in this section and the next are
discrete valued although this requirement is easily relaxed.
Causal relationships are represented by a graph G where each node corresponds to a
variable V ∈ T . Nodes are connected by arrows from P a(V ) to V and represent causal
inﬂuences between variables. Descendants of a variable V , i.e. D(V ) ⊂ T , consist of all
variables connected to V by arrows of the same direction arising from V . Graph G is called
a DAG if no variable is a descendant of itself, i.e., V ∈
/ D(V ), ∀ V ∈ T . Observe that this
assumption rules out simultaneity—a central feature of Haavelmo’s approach. Children of a
variable V are the set of variables that have V as a parent, namely, Ch(V ) = {V  ∈ T ; V ∈
P a(V  )}.
Causal relationships are translated into statistical relationships in a DAG through a
property termed the Local Markov Condition (LMC) (Kiiveri et al., 1984; Lauritzen, 1996).
LMC states that a variable is independent of its non-descendants conditional on its parents.
9

Chalak and White (2012) present generalizations of this approach.

12

LMC (5) also holds among variables in T under the assumption that error terms {1 , . . . , n }
are mutually independent (Pearl, 1988; Pearl and Verma, 1994), namely:

LMC: for all V ∈ T , V ⊥⊥ (T \ D(V )) | P a(V ).

(5)

We use Dawid’s (1979) notation to denote conditional independence. If W, K, Z are subsets
of T , the expression W ⊥⊥ K|Z means that each variable in W is statistically independent
of each variable in K conditional on all variables in Z.
The conditional independence relationships generated by LMC (5) can be further manipulated using the Graphoid relations.10 A beneﬁt of LMC (5) is that we can factorize the
joint distribution of variables P(V1 , . . . , Vn ). Under a recursive model, we can assume without loss of generality that variables (V1 , . . . , Vn , . . . , VN ) are ordered so that (V1 , . . . , Vn−1 )
are non-descendants of Vn and thereby P a(Vn ) ⊂ (V1 , . . . , Vn−1 ). Thus:

P(V1 , . . . , Vn ) =



P(Vn |V1 , . . . , Vn−1 ) =

Vn ∈T



P(Vn |P a(Vn )),

(6)

Vn ∈T

where the last equality comes from applying LMC (5).
Table 1 uses the Haavelmo model described in Section 2 to illustrate the concepts discussed here. Table 1 presents two models and seven panels separated by a series of horizontal
lines. The ﬁrst panel names the models. The second panel presents the structural equations
generating the models. Columns 1 and 2 are based on structural equations that have the
10

The Graphoid relationships are a set of elementary conditional independence relationships presented by
Dawid (1979):
Symmetry: X ⊥⊥ Y |Z ⇒ Y ⊥⊥ X|Z.
Decomposition:
Weak Union:
Contraction:
Intersection:
Redundancy:

X
X
X
X
X

⊥⊥ (W, Y )|Z ⇒ X ⊥⊥ Y |Z.
⊥⊥ (W, Y )|Z ⇒ X ⊥⊥ W |(Y, Z).
⊥⊥ Y |Z and X ⊥⊥ W |(Y, Z) ⇒ X ⊥⊥ (W, Y )|Z.
⊥⊥ W |(Y, Z) and X ⊥⊥ Y |(W, Z) ⇒ X ⊥⊥ (W, Y )|Z.
⊥⊥ Y |X.

The intersection relation is only valid for variables with strictly positive probability distributions. See also
Dawid (2001).

13

same functional form, but diﬀerent inputs. The third panel represents the associated model
as a DAG. Squares represent observed variables, circles represent unobserved variables. (Except in the ﬁrst panel, the components of  are kept implicit in the table.) The fourth panel
displays the parents in T for each variable. The ﬁfth panel shows the conditional independence relationships generated by the application of LMC (5) and the sixth panel presents
the factorization of the joint distribution. The seventh and ﬁnal panel provides the joint
distribution of variables when X is ﬁxed at x and the corresponding joint distribution for
the hypothetical models. The content of the last panel is discussed further in this section.
Using this framework, we can discuss the concept of ﬁxing introduced in Section 2 in
greater generality. Following Section 2, we deﬁne the causal operation of ﬁxing a variable in
a model represented by a graph G by the intervention that sets a value to this variable in T in
a fashion that does not aﬀect the distribution of its non-descendants. In other words, ﬁxing
a random variable (or a set of random variables) X ∈ T to x translates to setting X = x for
all X-inputs in the structural equations associated with variables in Ch(X). Pearl (2009)
uses the term doing for what we call ﬁxing. We use his notation in writing Equation (7).
The post-intervention distribution of variables in T when X is ﬁxed at x is given by
P(T \ {X}|do(X) = x) =



P(V |P a(V ))

V ∈T \{{X}∪Ch(X)}



P(V |P a(V ) \ {X}, X = x).

V ∈Ch(X)

(7)
Versions of Equation (7) can be found in Pearl (2001); Robins (1986); Spirtes et al. (2000).
In this instance, do(X) = x is equivalent to conditioning X̃ at X̃ = x.
As noted in Section 2, standard arguments of statistical conditioning are unable to describe the probability laws governing the ﬁxing operation used in Equation (7). Our solution
to this problem draws on Haavelmo’s insight that causality is a property of hypothetical models in which causal eﬀects on output variables are generated through hypothetical independent variations of inputs. Speciﬁcally, we show that the ﬁxing operation is easily translated
into statistical conditioning under the Hypothetical model described in Section 3.1.

14

Table 1: Haavelmo Empirical and Hypothetical Models
1. Haavelmo
Empirical Model

2. Haavelmo
Hypothetical Model

T = {U, X, Y }
 = {U , X , Y }
Y = fY (X, U, Y )
X = fX (U, X )
U = fU (U )

T = {U, X, Y, X̃}
 = {U , X , Y }
Y = fY (X̃, U, Y )
X = fX (U, X )
U = fU (U )

U

U

X

Y

X

Y

X̃
P a(U ) = ∅,
P a(X) = {U }
P a(Y ) = {X, U }

P a(U ) = P a(X̃) = ∅,
P a(X) = {U }
P a(Y ) = {X̃, U }
Y ⊥⊥ X|(X̃, U )
X ⊥⊥ (X̃, Y )|U
X̃ ⊥⊥ U

PE (Y, X, U ) =

PH (Y, X, U, X̃) =

PE (Y |X, U ) PE (X|U ) PE (U )

PH (Y |X̃, U ) P(X|U ) PH (U ) PH (X̃)

PE (Y, U |X ﬁxed at x) =

PH (Y, U, X|X̃ = x) =

PE (Y |X = x, U ) PE (U )

PH (Y |X̃ = x, U ) P(X|U ) PH (U )

This table has two columns and seven panels separated by horizontal lines. Each column presents a causal
model. The ﬁrst panel names the models. The second panel presents the structural equations generating the
models. In this row alone we make  explicit. In the other rows it is kept implicit to avoid clutter. Columns
1 and 2 are based on structural equations that have the same functional form, but have diﬀerent inputs.
The third panel represents the model as a DAG. Squares represent observed variables, circles represent
unobserved variables. The fourth panel presents the parents in T of each variable. The ﬁfth panel shows the
conditional independence relationships generated by the application of the Local Markov Condition. The
sixth panel presents the factorization of the joint distribution of variables in the Bayesian Network. The last
panel of column 1 presents the joint distribution of variables when X is ﬁxed at x. This entails a thought
experiment implicit in Haavelmo (1943). The last panel of column 2 gives the joint distribution of variables
generated by the hypothetical model when X̃ is conditioned at value X̃ = x.

15

3.1

The Hypothetical Model

Our approach is based on a hypothetical model that is used to study causal eﬀects. To
recall, we use the term empirical model to designate the data generating process and the
term hypothetical model to designate the model used to characterize causal eﬀects.
The hypothetical model is based on the empirical model. It shares the same structural
equations and same distribution of error terms as the empirical model. The hypothetical
model diﬀers from the empirical model in two ways. First, it appends to the empirical
model an external variable (or a set of external variables) termed a hypothetical variable(s).
Second, it replaces the action of existing inputs. If X ∈ T is the target variable to be ﬁxed
in the empirical model, then the newly created hypothetical variable X̃ replaces the X-input
of one, some or all variables in Ch(X). In other words, children of X in the empirical model
will have their X-input replaced by a X̃-input in the hypothetical model. We assume that
X and X̃ have common supports.
Table 1 illustrates the concept of a hypothetical model using the Haavelmo model introduced in Section 2. Column 2 presents the hypothetical model associated with the Haavelmo
empirical model presented in the ﬁrst column.
For the sake of clarity, we use GE for the DAG representing the empirical model and
TE for its associated set of variables. We use P aE , DE , ChE for the parents, descendants,
and children with DAG GE . We use PE for the probability measure of variables in TE . For
the corresponding counterparts in the hypothetical model we use GH , TH , P aH , DH , ChH and
PH .
We now list some salient features of the hypothetical model. Let X̃ denote the hypothetical variable (or variables) associated with X ∈ TE . We expand the list of variables in the hypothetical model so that TH = TE ∪ {X̃}. The hypothetical variable can
replace some or all of the input X for variables in ChE (X), i.e., ChH (X̃) ⊆ ChE (X).
Children of X in the empirical model can be partitioned among X and X̃ in the hypo-

16

thetical model: ChE (X) = ChH (X) ∪ ChH (X̃).

11

As a consequence we also have that

DE (X) = DH (X) ∪ DH (X̃), that is, X-descendants of the empirical model constitute the X
and X̃ descendants in the hypothetical model. Parental sets of the hypothetical model are deﬁned by P aH (V ) = P aE (V ) ∀ V ∈ TE \ChH (X̃) and P aH (V ) = {P aE (V )\{X}}∪{X̃} ∀ V ∈
ChH (X̃). Moreover, X̃ is an external variable, that is, P aH (X̃) = ∅. The hypothetical model
is also a DAG. Thus LMC (5) holds and the joint distribution of the variables in TH can be
factorized using equation (6). By sharing the same structural equations and distribution of
error terms, the conditional probabilities of the hypothetical model can be written as:

PH (V |P aH (V )) = PE (V |P aE (V )) ∀ V ∈ TE \ ChH (X̃)

(8)

PH (V |P aH (V ) \ {X̃}, X̃ = x) = PE (V |P aE (V ) \ {X}, X = x) ∀ V ∈ ChH (X̃).

(9)

and

Equations (8)–(9) arise because the distribution of a variable V ∈ TE conditional on its
parents is determined by the distribution of its error terms, which is the same for hypothetical
and empirical models.
We now link the probability measures of the empirical and hypothetical models. Theorem T-1 uses LMC (5) and Equation (8) to show that the distribution of non-descendants
of X̃ are the same in both hypothetical and empirical models:
Theorem T-1. Let X̃ be the hypothetical variable in the hypothetical model represented
by GH associated with variable X in empirical model GE . Let W, Z be any disjoint set of
11

As an example, let a simple empirical model for mediation analysis consist of three variables: an input
variable X, a mediation variable M caused by X and an outcome of interest Y caused by X and M . This
model is represented as a DAG in Model 1 of Table 2 and ChE (X) = {M, Y }. Suppose we are interested in
the indirect eﬀect, that is the eﬀect of X on Y that operates exclusively by changes in M while holding the
distribution of X unaltered. The hypothetical model for the evaluation of the indirect causal eﬀect assigns
the causal link of X on M to the hypothetical variable X̃. Namely, X still causes Y , but X̃ causes M. This
hypothetical model is represented by Model 3 of Table 2. In this model ChH (X) = {Y }, ChH (X̃) = {M }
and ChE (X) = ChH (X) ∪ ChH (X̃).

17

variables in TE \ DH (X̃) then:
PH (W |Z) = PH (W |Z, X̃) = PE (W |Z) ∀ {W, Z} ⊂ TE \ DH (X̃).

Proof. See Appendix
Theorem T-1 also holds for the set of variables that are non-descendants of X according to
the empirical model, which are a subset of TE \ DH (X̃). Thus PH (W |Z) = PH (W |Z, X̃) =
PE (W |Z) for all {W, Z} ⊂ TE \ DE (X).
The following theorem uses Theorem T-1 and Equations (8)–(9) to show that the distribution of variables conditional on X and X̃ taking the same value x in the hypothetical
model is equal to the distribution of the variables conditional on X = x in the empirical
model:
Theorem T-2. Let X̃ be the hypothetical variable in the hypothetical model represented
by GH associated with variable X in empirical model GE and let W, Z be any disjoint12 set
of variables in TE then:
PH (W |Z, X = x, X̃ = x) = PE (W |Z, X = x) ∀ {W, Z} ⊂ TE .
Proof. See Appendix.13
The hypothetical variable is created to have the desired independent variation to generate
causal eﬀects. As a consequence, the operation of ﬁxing a variable in the empirical model is
translated into statistical conditioning in the hypothetical model. In particular, if we replace
12
13

Disjoint (i.e., distinct) from X
We also note the following result:

Corollary C-1. Let X̃ be uniformly distributed in the support of X and let W, Z be any disjoint set of
variables in TE then:
PH (W |Z, X = X̃) = PE (W |Z) ∀ {W, Z} ⊂ TE .
Proof. See Appendix. We thank an anonymous referee for suggesting this result and its proof.

18

the X-input by a X̃-input for all children of X, as suggested by the operation of ﬁxing or
“doing,” we have that the distribution of an outcome Y ∈ TE of the empirical model when
variable X is ﬁxed at x (for all its children) is equivalent to the distribution of Y conditioned
on the hypothetical variable X̃ being assigned to value x. This is captured by the following
theorem:
Theorem T-3. Let X̃ be the hypothetical variable in GH associated with variable X in the
empirical model GE , such that ChH (X̃) = ChE (X), then:
PH (TE \ {X}|X̃ = x) = PE (TE \ {X}|do(X) = x).

Proof. See Appendix
One beneﬁt of the hypothetical model is its greater ﬂexibility for the study of causal
eﬀects. While the do-operator targets all causal relationships involving a variable X, the
hypothetical variable allows analysts to target causal relationships of X separately. Indeed
we can choose which variable in Ch(X) will be caused by a hypothetical variable X̃, which
in turn replaces some of the X inputs. This ﬂexibility facilitates the investigation of causal
eﬀects in models that examine diﬀerent causal paths associated with a single input, such as
mediation analysis.14
To show this, consider a simple empirical model for mediation consisting of three variables, an input variable X, a mediation variable M caused by X and an outcome of interest Y caused by X and M . Its structural equations are given by Y = fY (X, M, Y ),
M = fM (X, M ), X = fX (X ) and its DAG is represented as Model 1 of Table 2 (with 
kept implicit). The total causal eﬀect of X on Y when X is ﬁxed at x compared to when
it is ﬁxed at x is given by T E(x, x ) = EE (Y (x) − Y (x )) where Y (x) = fY (x, M (x), Y ),
M (x) = fM (x, M ) and EE denotes the expectation over the probability measure of the empirical model. The hypothetical model for the evaluation of the total causal eﬀect considers all
14

Robins and Richardson (2011) analyze the mediation framework of Model 1 of Table 2 to examine four
broad classes of graphical causal models.

19

causal links of X in the hypothetical variable X̃. It is represented by Model 2 of Table 2. Using this model, the total causal eﬀect is given by T E(x, x ) = EH (Y |X̃ = x) − EH (Y |X̃ = x ),
where EH denotes the expectation over the probability measure of the hypothetical model in
Model 2 of Table 2. Suppose that we are interested in the indirect eﬀect, that is the eﬀect of
X on Y that operates exclusively by changing M while keeping the distribution of X unaltered from what it is in the empirical model, i.e., EE (fY (X, M (x), Y ) − fY (X, M (x ), Y )).
This eﬀect is EH (Y |X̃ = x) − EH (Y |X̃ = x ) derived from the hypothetical model presented
in model 3 of Table 2. Hypothetical model 4 of Table 2 gives the causal graph for the direct
eﬀect of X on Y that operates exclusively through changes in X conditioning on M in the
empirical model.15
15

See Heckman and Pinto (2013) for further discussion of mediation models.

20

Table 2: Models for Mediation Analysis

2. Hypothetical Model for Total Eﬀect of X on Y

1. Empirical Mediation Model

X

M

X

Y

M

Y
~
X

3. Hypothetical Model for Indirect Eﬀect of X on Y

X

M

Y

4. Hypothetical Model for Direct Eﬀect of X on Y

X

~
X

M

Y
~
X

This table shows four models represents by DAGs. To simplify the displays we keep the unobservables in  implicit. Model 1 represents the empirical model for mediation analysis. The remaining
three models are hypothetical models that target diﬀerent causal eﬀects of X on Y . Model 2 represents the hypothetical model for the analysis of total eﬀect of X on Y . Model 3 examines the
indirect eﬀect X on Y . Model 4 examines the direct eﬀect of X on Y .

The hypothetical model does not suppress the variable we seek to ﬁx, but rather creates a
new hypothetical variable that allows us to examine a variety of causal eﬀects. This approach
provides a natural framework within which to examine counterfactual outcomes that involve
both ﬁxing and conditioning. Speciﬁcally, the expected value of an outcome Y when an
input X is ﬁxed at x conditional on X = x is readily deﬁned by EH (Y |X̃ = x, X = x ) in
the hypothetical model. By characterizing causality through a hypothetical model we avoid
the necessity of deﬁning new mathematical tools outside standard statistical analysis. The

21

next section illustrates this point by identifying the causal eﬀects of the “Front-Door” model
of Pearl (2009) using his “do-calculus” and the standard statistical tools that can be used
to analyze the hypothetical model.
The hypothetical model allows analysts to clearly distinguish the deﬁnition of causal
eﬀects from their identiﬁcation in data. Causal eﬀects are translated into statistical conditioning in the hypothetical model. Identiﬁcation of causal eﬀects requires analysts to relate
the hypothetical and empirical distributions in a fashion that allows the evaluation of causal
eﬀects examined in the hypothetical model using data generated by the empirical model.
For example, a standard technique for doing so is matching:
Lemma L-1. Matching: Let Z, W be any disjoint set of variables in TE and let X̃ be a
hypothetical variable in model GH associated with X ∈ TE in model GE such that, in the
hypothetical model, X ⊥⊥ W |(Z, X̃), then
PH (W |Z, X̃ = x) = PE (W |Z, X = x).

Proof. See Mathematical Appendix.
Variables Z of Lemma L-1 are called matching variables. In statistical jargon, it is
said that matching variables solve the problem of confounding eﬀects between a treatment
indicator X and outcome W . Matching is commonly used to identify treatment eﬀects in
propensity score matching models.16 In these models, the conditional independence relation
of Lemma L-1 is assumed to be true. Pearl (1993) describes a graphical test called the
“Back-Door” criterion that can be applied to a DAG in order to check if a set of variables
satisfy the matching assumptions of Lemma L-1. The next section illustrates the use of
Lemma L-1.
16

See, e.g., Rosenbaum and Rubin (1983).

22

4

The Do Calculus and Haavelmo’s Notation of Causality

To illustrate the points made in the previous section, we give the rules of the “do-calculus”
and compare the identiﬁcation strategies associated with the do-calculus with an approach
using the hypothetical model. The do-calculus, developed by Pearl (1995), consists of three
graphical and statistical rules that operate on the empirical model PE and that supplement
standard statistics. In special cases they solve the problem of identifying causal eﬀects in
Bayesian Networks. The concept of a hypothetical model—central to the Frisch-Haavelmo
approach—is not used in the literature on DAGs. It is commonly used to process the
information of a causal model that can be represented by a DAG. Examples of this literature
are Huang and Valtorta (2006, 2008) and Tian and Pearl (2002, 2003).
To review these methods, we introduce the graphical and statistical notation used to
deﬁne the do-calculus. Let X, Y, Z be arbitrary disjoint sets of variables (nodes) in a causal
graph G. GX denotes a modiﬁcation of DAG G obtained by deleting the arrows pointing to
X, GX denotes the modiﬁed DAG obtained by deleting the arrows emerging from X and
GX,Z denotes the DAG obtained by deleting arrows pointing to X and emerging from Z.
Table 4 presents an example of the application of these rules for the Front-Door model,
which is described in Table 3. The Front-Door model is described in greater detail in the
next Section.
Let G be a DAG and let X, Y, Z, W be any disjoint sets of variables. The do-calculus
rules are:
• Rule 1: Insertion/deletion of observations:
Y ⊥⊥ Z|(X, W ) under GX ⇒ P(Y |do(X), Z, W ) = P(Y |do(X), W ).
• Rule 2: Action/observation exchange:
Y ⊥⊥ Z|(X, W ) under GX,Z ⇒ P(Y |do(X), do(Z), W ) = P(Y |do(X), Z, W ).

23

Table 3: “Front-Door” Empirical and Hypothetical Models
1. Pearl’s “Front-Door”
Empirical Model

2. Our Version of the “Front-Door”
Hypothetical Model

T = {U, X, M, Y }
 = {U , X , M , Y }
Y = fY (M, U, Y )
X = fX (U, X )
M = fM (X, M )
U = fU (U )

T = {U, X, M, Y, X̃}
 = {U , X , M , Y }
Y = fY (M, U, Y )
X = fX (U, X )
M = fM (X̃, M )
U = fU (U )

U

U

X

M

Y

X

M

Y

X̃
P a(U ) = ∅,
P a(X) = {U }
P a(M ) = {X}
P a(Y ) = {M, U }

P a(U ) = P a(X̃) = ∅,
P a(X) = {U }
P a(M ) = {X̃}
P a(Y ) = {M, U }

Y ⊥⊥ X|(M, U )
M ⊥⊥ U |X

Y ⊥⊥ (X̃, X)|(M, U )
M ⊥⊥ (U, X)|X̃
X ⊥⊥ (M, X̃, Y )|U
U ⊥⊥ (M, X̃)
X̃ ⊥⊥ (X, U )

PE (Y, M, X, U ) =

PH (Y, M, X, U, X̃) =

PE (Y |M, U ) PE (X|U ) PE (M |X) PE (U )

PH (Y |M, U ) P(X|U ) PH (M |X̃) PH (U ) PH (X̃)

PE (Y, M, U |do(X) = x) =

PH (Y, M, U, X|X̃ = x) =

PE (Y |M, U ) PE (M |X = x) PE (U )

PH (Y |M, U ) P(X|U ) PH (M |X̃ = x) PH (U )

This table has two columns and seven panels separated by horizontal lines. Each column presents a causal
model. The ﬁrst panel names the models. The second panel presents the structural equations generating the
model. In this row alone we make  explicit. In the other it is kept implicit to avoid clutter. Columns 1 and
2 are based on structural equations that have the same functional form, but have diﬀerent inputs. The third
panel represents the model as a DAG. Squares represent observed variables, circles represent unobserved
variables. The fourth panel presents the parents in T of each variable. The ﬁfth panel shows the conditional
independence relationships generated by the application of the Local Markov Condition. The sixth panel
presents the factorization of the joint distribution of variables in the Bayesian Network. The last panel of
column 1 presents the joint distribution of variables when X is ﬁxed at x using the “do operator.” The last
panel of column 2 gives the joint distribution of variables generated by the hypothetical models associated
with empirical model 1 when X̃ is conditioned at X̃ = x.

24

Table 4: Do-calculus and the Front-Door Model

1. Modiﬁed Front-Door Model GX = GM

2. Modiﬁed Front-Door Model GM

U

U

X

M

Y

X

M

Y

(Y, M ) ⊥⊥ X|U
(X, U ) ⊥⊥ M

(X, M ) ⊥⊥ Y |U
(Y, U ) ⊥⊥ M |X

3. Modiﬁed Front-Door Model GX,M

4. Modiﬁed Front-Door Model GX,M

U

U

X

M

Y

X

(X, M ) ⊥⊥ (Y, U )

M

Y

(Y, M, U ) ⊥⊥ X
U ⊥⊥ M

This table shows four models represented by DAGs ( are kept implicit to avoid notational clutter).
Squares represent observed variables, circles represent unobserved variables. Each DAG is generated
by the deletion of arrows of the original Front-Door model (ﬁrst column of Table 3) according to the
rules of the do-calculus. Below each model, we show conditional independent relations generated
by the application of the Local Markov Condition (5) to variables of the models.

• Rule 3: Insertion/deletion of actions:
Y ⊥⊥ Z|(X, W ) under GX,Z(W ) ⇒ P(Y |do(X), do(Z), W ) = P(Y |do(X), W ),
where Z(W ) is the set of Z-nodes that are not ancestors of any W -node in GX .
These rules are intended to supplement standard statistical tools with a new set of “do”
operations. We illustrate the use of the do-calculus in the next section.

25

4.1

Comparing Analyses Based on the Do-calculus with those
from the Hypothetical Model

We compare the do-calculus and an analysis based on our hypothetical model by identifying
the causal eﬀects of Pearl’s “Front-Door model”. That model consists of four variables: (1)
an external unobserved variable U ; (2) an observed variable X caused by U ; (3) an observed
variable M caused by X; and (4) an outcome Y caused by U and M . The Front-Door model
is presented in the ﬁrst column of Table 3.
We are interested in identifying the distribution of the outcome Y when X is ﬁxed at x.
By identiﬁcation we mean expressing the quantity P(Y |do(X)) in terms of the distribution
of observed variables.
The do-calculus identiﬁes P(Y |do(X)) through four steps which we now perform. Steps
1, 2 and 3 identify P(M |do(X)), P(Y |do(M )) and P(Y |M, do(X)) respectively. Step 4 uses
the ﬁrst three steps to identify P(Y |do(X)).
1. Invoking LMC (5) for variable M of DAG GX , (DAG 1 of Table 4) generates X ⊥⊥ M.
Thus, by Rule 2 of the do-calculus, we obtain P(M |do(X)) = P(M |X).
2. Invoking LMC (5) for variable M of DAG GM , (DAG 1 of Table 4) generates X ⊥⊥
M. Thus, by Rule 3 of the do-calculus, P(X|do(M )) = P(X). In addition, applying
LMC (5) for variable M of DAG GM , (DAG 2 of Table 4) generates M ⊥⊥ Y |X. Thus,
by Rule 2 of do-calculus, P(Y |X, do(M )) = P(Y |X, M ).
Therefore P(Y |do(M )) =



P(Y |X = x , do(M )) P(X = x |do(M ))

x ∈supp(X)

=



P(Y |X = x , M ) P(X = x ),

x ∈supp(X)

where “supp” means support.
3. Invoking LMC (5) for variable M of DAG GX,M , (DAG 3 of Table 4) generates
26

Y ⊥⊥ M |X. Thus, by Rule 2 of the do-calculus, P(Y |M, do(X)) = P(Y |do(M ), do(X)).
In addition, applyingLMC (5) for variable X of DAG GX,M , (DAG 4 of Table 4)
generates (Y, M, U ) ⊥⊥ X. By weak union and decomposition, we obtain Y ⊥⊥ X|M.
Thus by Rule 3 of the do-calculus, we obtain that P(Y |do(X), do(M )) = P(Y |do(M )).
Thus P(Y |M, do(X)) = P(Y |do(M ), do(X)) = P(Y |do(M )).
4. We collect the results from the three previous steps to identify P(Y |do(X)) from observed data:

P(Y |do(X) = x)

P(Y |M, do(X) = x) P(M |do(X) = x)
=
m∈supp(M )



=

m∈supp(M )



=

m∈supp(M )

=



P(Y |do(M ) = m, do(X) = x) P(M = m|do(X) = x)


Step 3

P(Y |do(M ) = m) P(M = m|do(X) = x)


Step 3



P(Y |X = x , M ) P(X = x ) P(M = m|X = x) .


m∈supp(M )
x ∈supp(X)
Step 1


Step 2

In this fashion, we use the do-calculus to identify the desired causal parameter. It is instructive to compare this proof of identiﬁcation with one based on the approach of Haavelmo.
We identify the causal eﬀects of X on Y for the Front-Door model using a hypothetical model.
We replace the relationship of X on M by a hypothetical variable X̃ that causes M. We
use PE to denote the probability of the Front-Door model that generates the data (Column
1 of Table 3) and PH for the hypothetical model (Column 2 of Table 3). As before, we
seek to identify PH (Y |X̃) (the equivalent of P(Y |do(X))) from observed distributions in the
empirical model.
We ﬁrst present a lemma that states three useful conditional independence relations of
the hypothetical model. The lemma is based on the application of LMC (5) and the Graphoid
27

relationships:
Lemma L-2. In the Front-Door hypothetical model, (1) Y ⊥⊥ X̃|M, (2) X ⊥⊥ M, and (3)
Y ⊥⊥ X̃|(M, X)
Proof. By LMC (5) for X, we obtain (Y, M, X̃) ⊥⊥ X|U. By LMC (5) for Y we obtain
Y ⊥⊥ (X, X̃)|(M, U ). By Contraction applied to (Y, M, X̃) ⊥⊥ X|U and Y ⊥⊥ (X, X̃)|(M, U )
we obtain (Y, X) ⊥⊥ X̃|(M, U ). By LMC (5) for U we obtain (M, X̃) ⊥⊥ U. By Contraction
applied to (M, X̃) ⊥⊥ U and(Y, M, X̃) ⊥⊥ X|U we obtain(X, U ) ⊥⊥ (M, X̃). The second
relationship in the Lemma is obtained by Decomposition. In addition, by Contraction on
(Y, X) ⊥⊥ X̃|(M, U ) and (M, X̃) ⊥⊥ U we obtain (Y, X, U ) ⊥⊥ X̃|M. The two remaining
conditional independence relationships of the Lemma are obtained by Weak Union and Decomposition.17
17

One can also prove Lemma L-2 using Pearl’s d-Separation criteria. According to Pearl (2009), a path p
connecting X and Y is said to be d-Separated (or blocked) by a set of nodes Z if and only if
1. a path p contains a chain i → m → j or a fork i ← m → j such that the middle node m is in Z, or
2. a path p contains an inverted fork (or collider) i → m ← j such that the middle node m is not in Z
and such that no descendant of m is in Z.
A set Z is said to d-separate X from Y if and only if Z blocks every path from a node in X to a node
in Y. If X and Y are d-Separated by Z according to a graph G, then Y ⊥⊥ Y |Z in G. We are examining
the Hypothetical Model described by second column of Table 2. Variables Y and X̃ are connected by a
single path X̃ → M → Y. Thus we have that Y ⊥⊥ X̃|M, according to part 1 of the d-Separation criteria.
Moreover, we can also state that Y ⊥⊥ X̃|(M, X) as X is not a collider nor a decendant of a collider (part
2 of the d-Separation criteria). Finally, there is no path that connects X and M of the form X → . . . → M
nor X ← . . . ← M. Thus we can state that X ⊥⊥ M according to part 1 of the d-Separation criteria.

28

Applying these results,

PH (Y |X̃ = x)

PH (Y |M = m, X̃ = x) PH (M = m|X̃ = x)
=
m∈supp(M )

=



m∈supp(M )

=
=





m∈supp(M )

x ∈supp(X)



m∈supp(M )

=



m∈supp(M )

=

PH (Y |M = m) PH (M = m|X̃ = x)



m∈supp(M )



PH (Y |X = x , M = m) PH (X = x |M = m) PH (M = m|X̃ = x)
PH (Y |X = x , M = m) PH (X = x ) PH (M = m|X̃ = x)

x ∈supp(X)



PH (Y |X = x , X̃ = x , M = m) PH (X = x ) PH (M = m|X̃ = x)

x ∈supp(X)



x ∈supp(X)

P (Y |M, X = x ) PE (X = x )



E
by Theorem T-2

by Theorem T-1

P (M = m|X = x) .
E

by Matching L-1

The second equality comes from relationship (1) Y ⊥⊥ X̃|M of Lemma L-2. The fourth
equality comes from relationship (2) X ⊥⊥ M of Lemma L-2. The ﬁfth equality comes from
relationship (3) Y ⊥⊥ X̃|(M, X) of Lemma L-2. The last equality links the distributions of
the hypothetical model with the ones of the empirical model. The ﬁrst term uses Theorem T2 to equate PH (Y |X = x , X̃ = x , M = m) = PE (Y |M, X = x ). The second term uses the
fact that X is not a child of X̃, thus by Theorem T-1, PH (X = x ) = PE (X = x ). Finally,
the last term uses Matching applied to M. Namely, LMC (5) for M generates M ⊥⊥ X|X̃ in
the hypothetical model. Then, by Matching L-1, PH (M |X̃ = x) = PE (M |X = x).
It is clear from this example that, even though both frameworks produce the same ﬁnal
identiﬁcation formula, the methods underlying them diﬀer greatly. A key concept in the
framework inspired by Haavelmo is the notion of a hypothetical model. Hypothetical models
are the essential ingredients of science. Using this speciﬁcation, identiﬁcation is secured using
the standard statistical tools involving the rules of conditional probability distributions.
LMC and the graphoid relations generate conditional independence relationships that arise
29

from the hypothetical model. Identiﬁcation using the hypothetical model is transparent, and
does not require additional causal rules.18

5

The Beneﬁts and Limitations of DAGs

A major beneﬁt of a DAG is its intuitive description of models as causal chains. DAG
assumptions list the variables in a model and their causal relationships. A DAG does not
generate or characterize any restrictions on functional forms or parametric speciﬁcations of
the structural equations. In this sense, if an identiﬁcation result is achieved, it is obtained
under very weak conditions.
The generality of a DAG is also the source of its limitation. Methods that focus on
identiﬁcation of models solely described by DAGs lack the tools for invoking additional
assumptions that would generate the identiﬁcation of an a priori non-identiﬁed model. We
clarify this point by considering the instrumental variable model.
The simplest instrumental variable model consists of four variables: (1) a confounding
variable U that is external and unobserved; (2) an external instrumental variable Z; (3)
an observed variable X caused by U and Z; and (4) an outcome Y caused by U and X.
The empirical instrumental variable model is described in the ﬁrst column of Table 5. Its
hypothetical counterpart is presented in the second column of Table 5.
The instrumental variable method is a fundamental ingredient of a huge literature on
econometric identiﬁcation (see, e.g., Matzkin, 2013). It is the basis for more sophisticated
models such as the Generalized Roy model, which is widely used in econometrics in the
analysis of selection bias and in evaluating social programs (Heckman, 1976, 1979, Heckman
and Robb, 1985, Powell, 1994, Heckman and Vytlacil, 2007a,b). Examples of this literature
are nonparametric control functions (see, e.g., Blundell and Powell, 2003) and identiﬁcation
through instrumental variables (Reiersöl, 1945).
18

Pearl (2009) also considers a “Back Door model” and applies do-calculus to identify a model that can
readily be deﬁned by the Haavelmo approach and identiﬁed using conventional matching methods.

30

Table 5: Instrumental Variable Empirical and Hypothetical Models
1. Instrumental Variable
Empirical Model

2. Instrumental Variable
Hypothetical Model

T = {U, X, Z, Y }
 = {U , X , Z , Y }
Y = gY (X, U, Y )
X = gX (U, Z, X )
Z = gZ (Z )
U = gU (U )

T = {U, X, Z, Y, X̃}
 = {U , X , Z , Y }
Y = gY (X̃, U, Y )
X = gX (U, Z, X )
Z = gZ (Z )
U = gU (U )

U
Z

X

U
Y

Z

X

Y

X̃

P a(U ) = P a(Z) = ∅,
P a(X) = {U, Z}
P a(Y ) = {U, X}

P a(U ) = P a(U ) = ∅,
P a(X) = {U, Z}
P a(Y ) = {U, X̃}

Y ⊥⊥ Z|(X, U )
Z ⊥⊥ U

Y ⊥⊥ (X, Z)|(X̃, U )
Z ⊥⊥ (U, Y, X̃)|(X̃, U )
X ⊥⊥ (Y, X̃)|(Z, U )
U ⊥⊥ (Z, X̃)
X̃ ⊥⊥ (U, X, Z)

PE (Y, Z, X, U ) =

PH (Y, Z, X, U, X̃) =

PE (Y |X, U ) PE (X|U, Z) PE (Z) PE (U )

PH (Y |X̃, U ) PH (X|U, Z) PH (Z) PH (U ) PH (X̃)

PE (Y, Z, U |do(X) = x) =

PH (Y, Z, X, U |X̃ = x) =

PE (Y |X = x, U ) PE (Z) PE (U )

PH (Y |X̃ = x, U ) PH (X|U, Z) PH (Z) PH (U )

This table has two columns and seven panels separated by horizontal lines. Each column presents a causal
model. The ﬁrst panel names the model. The second panel presents the structural equations generating the
model. In this row alone we make the  explicit. In the other rows it is kept implicit to avoid notational
clutter. Columns 1 and 2 are based on structural equations that have the same functional form, but have
diﬀerent inputs. The third panel represents the model as a DAG. Squares represent observed variables and
circles represent unobserved variables. The fourth panel presents the parents in T of each variable. The ﬁfth
panel shows the conditional independence relationships generated by the application of the Local Markov
Condition. The sixth panel presents the factorization of the joint distribution of variables in the Bayesian
Network. The last panel of column 1 presents the joint distribution of variables when X is ﬁxed at x.
(do(X) = x). The last panel of column 2 gives the joint distribution of variables generated by hypothetical
models associated with empirical model 1 when X̃ is conditioned on X̃ = x.

31

Chapters 3 and 5 of Pearl (2009) show that the instrumental variable model is not
identiﬁed applying the rules of the do-calculus. Indeed, it is impossible to identify the causal
eﬀect of X on Y without additional information.
The non-identiﬁcation of the instrumental variable model poses a major limitation for
the identiﬁcation literature that relies exclusively on DAGs. Identiﬁcation of the instrumental variable model relies on assumptions outside the scope of the DAG literature. For
example, we can use LMC (5) to obtain the following conditional independence relationships: Y ⊥⊥ Z|(U, X) and U ⊥⊥ Z. These relationships in addition to X ⊥

⊥ Z satisfy the
necessary criteria to apply the method of Two Stage Least Squares (TSLS). TSLS identiﬁes the instrumental variable model under a linearity assumption. As a consequence, if we
assume that the causal relationship of X and U on outcome Y are represented by a linear
equation, i.e., Y = Xβ + U , then it is well-known that parameters β can be identiﬁed using
cov(Z, Y )/ cov(Z, X) under standard rank conditions.
Linearity and homogeneity of the eﬀects of X on Y across agents (i.e., β is the same across
the values X, U take) are strong assumptions about the causal links that govern the relationship between Y and X. This assessment fostered a huge literature in economics devoted
to methods that relax linearity and homogeneity and that allow coeﬃcients to be correlated
with regressors. Examples of this literature are Imbens and Angrist (1994), Vytlacil (2002),
and Heckman and Vytlacil (2005, 2007a,b), who identify the instrumental variable model
under more general conditions by making assumptions on the causal relationship of Z with
X. Imbens and Angrist (1994) show that the instrumental variable model can be identiﬁed
under a “monotonicity” assumption (increasing the values of an instrument has the same
qualitative eﬀect on all agents). Vytlacil (2002) shows that this assumption is equivalent to
assuming an instrumental variable model in which the treatment assignment decision rule is
separable in terms of unobserved characteristics of the agents and the instrumental variable.
Heckman and Vytlacil (1999, 2005, 2007a,b) develop and apply this result.
Table 6 summarizes the common and distinct features of Pearl’s do-calculus and the

32

approach based on Haavelmo’s hypothetical model. Both approaches use structural equation
models in the sense of Koopmans and Reiersøl (1950). Both invoke autonomy and assume
mutually independent errors . In recursive models, both use the Local Markov Condition
and the Graphoid axioms. Both use “ﬁxing” or the “do operator” to deﬁne counterfactuals.
Table 6: Summarizing the Do-calculus of Pearl (2009) and Haavelmo’s Framework

Common Features of Haavelmo and Do-Calculus:
Autonomy (Frisch, 1938)
Errors Terms:  mutually independent
Statistical Tools: LMC and Graphoid Axioms apply
Counterfactuals: Fixing or Do-operator is a Causal, not statistical, operation
Distinctive Features of Haavelmo and Do-Calculus:

Approach:
Introduces:
Identiﬁcation:
Versatility:

Haavelmo
Thinks Outside the Box of the Empirical Model
Constructs a Hypothetical Model
Connects PH and PE
Basic Statistical Principles Apply

The approaches diverge in their analyses of identiﬁcation.

Do-calculus
Applies Complex Tools
Graphical Rules
Iteration of Do-Calculus Rules
Creates New Rules of Statistics

The approach based on

Haavelmo creates a hypothetical variable X̃ and an associated hypothetical model that
is “outside the box” of the empirical model. It applies standard probability calculus to
the hypothetical model to connect the hypothetical model to the empirical model. Pearl’s
do-calculus creates a new set of extra-statistical tools to identify the causal parameters created by ﬁxing or the “do operator.” Our analysis shows that in the hypothetical model of
Haavelmo, the special extra-statistical tools of the do-calculus are not required to identify
causal parameters. The econometric approach identiﬁes a broader range of models that
cannot be identiﬁed using the rules of the “do-calculus.”

33

6

Hypothetical Models and Simultaneous Equations

The literature on causality provides a framework for modeling causal processes that are based
on DAGs. Less is known about Directed Cyclic Graphs (DCGs) that are used to represent
Simultaneous Equations. Indeed, the fundamental Local Markov Condition no longer holds
for DCGs (Spirtes, 1995). Nevertheless, the notion of ﬁxing readily extends to a system of
simultaneous equations.
Consider a system of two equations:

Y1 = gY1 (Y2 , X1 , U1 ),

(7a)

Y2 = gY2 (Y1 , X2 , U2 ).

(7b)

TE = {Y1 , Y2 , X1 , X2 , U1 , U2 }. Our analysis can be readily generalized to systems with more
than two equations, but for the sake of brevity, we focus on the two-equation case. To
simplify notation, we keep  implicit.
The empirical Simultaneous Equations Model of (7a) and (7b) is represented as Model
1 of Table 7. Many diﬀerent versions of this model appear in the literature. For simplicity,
we assume U1 ⊥⊥ U2 and (U1 , U2 ) ⊥⊥ (X1 , X2 ).19
The hypothetical model associated with the causal operation of ﬁxing both Y2 and Y1 is
represented in Model 2 of Table 7. Under autonomy, the causal eﬀect of Y2 on Y1 when Y2
is ﬁxed at y2 is given by Y1 (y2 ) = gY1 (y2 , X, U1 ). Symmetrically, Y2 (y1 ) = gy2 (y1 , X, U2 ). We
deﬁne hypothetical random variables Ỹ1 , Ỹ2 . They replace the Y1 , Y2 inputs in Equations (7a)
and (7b) in the same fashion as discussed in previous sections. (Ỹ1 , Ỹ2 ) ⊥⊥ (X1 , X2 , U1 , U2 );
and Ỹ1 ⊥⊥ Ỹ2 . TH = {Ỹ1 , Ỹ2 , Y1 , Y2 , X1 , X2 , U1 , U2 }. We assume a common support for (Y1 , Y2 )
and (Ỹ1 , Ỹ2 ).
19

These assumptions are made to simplify the analysis. A large literature relaxes these assumptions

⊥ U2 and (U1 , U2 ) ⊥

⊥ (X1 , X2 ). The literature
and develops identiﬁcation criteria for cases where U1 ⊥
considers a variety of speciﬁcations (see Matzkin, 2008). We maintain the assumptions that U1 ⊥⊥ U2 and
(U1 , U2 ) ⊥⊥ (X1 , X2 ) for simplicity.

34

In the same fashion as in the model previously discussed, the distribution of Y1 when Y2
is ﬁxed at y2 is given by PH (Y1 |Ỹ2 = y2 ). The average causal eﬀect of Y2 on Y1 when Y2
is ﬁxed at the two values of y2 and y2 is given by EH (Y1 |Ỹ2 = y2 ) − EH (Y1 |Ỹ2 = y2 ), where
EH denotes expectation over the probability measure PH of the hypothetical model. The
hypothetical variation of Ỹ2 corresponds to the standard Marshallian and Walrasian thought
experiments in which quantities or prices are ﬁxed to trace out demand and supply curves
(see, e.g., Mas-Colell et al., 1995). A symmetric analysis produces the causal eﬀect of Y1 on
Y2 . Thus we obtain the counterpart to the counterfactuals deﬁned for the recursive models
earlier in this paper.
Table 7: Models for Simultaneous Equations

1. Empirical Model for Simultaneous Equations

X2

Y2

Y1

U2

U1

X1

2. Hypothetical Model for Simultaneous Equations

X2

Y2

Y1

X1

Y˜1

U2

U1

Y˜2

This table shows two models. (The  are kept implicit.) Model 1 represents the empirical model for
Simultaneous Equations where Y1 and Y2 cause each other. Model 1 is cyclic, and hence it is not
a DAG. Model 2 represents one possible hypothetical model associated with the empirical model
for Simultaneous Equations. In Model 2, the hypothetical variable Ỹ2 is associated with the causal
link of Y2 on Y1 of Model 1 and the hypothetical variable Ỹ1 is associated with the causal link of
Y1 on Y2 of Model 1.

Under simultaneity, the graph for Model 1 is cyclic and the relationships that hold for
35

DAGs, such as the LMC (5), break down (Lauritzen and Richardson, 2002; Spirtes, 1995).
Equations (7a) and (7b) cannot be represented as Directed Bayesian networks. The tools
developed for DAGs do not directly apply and require modiﬁcation. Equations (7a) and (7b)
are fundamentally non-recursive and observed variables emerge from a feedback process.
A traditional assumption in the simultaneous equations literature is “completeness”—the
existence of at least a local solution for Y1 and Y2 in terms of (X1 , X2 , U1 , U2 ):

Y1 = φ1 (X1 , X2 , U1 , U2 ),

(8a)

Y2 = φ2 (X1 , X2 , U1 , U2 ).20

(8b)

These are called “reduced form” equations (see, e.g., Matzkin, 2008, 2013). They inherit
the autonomy properties of the structural equations.
The assumption of the existence of a reduced form is not innocuous even in the linear cases
for continuous Y1 and Y2 analyzed by Haavelmo (1943, 1944) and the Cowles Foundation
pioneers (see Koopmans et al., 1950). Heckman (1978), Tamer (2003), and Chesher and
Rosen (2012) analyze the case in which Y1 and Y2 are discrete valued. Solutions (8a) and (8b)
may not exist except under conditions given in those papers.21 Alternatively, there may be
multiple solutions giving rise to reduced form correspondences. In the case where no solutions
exist, the model is incoherent as an equilibrium model unless additional assumptions are
invoked. However, one can construct hypothetical models using Haavelmo’s insights even in
incoherent cases.22
20
We use the term “completeness” in the sense of Koopmans et al. (1950); i.e., the existence of a local
solution of equations (7a) and (7b). This concept is to be distinguished from the notion of completeness in
the nonparametric IV literature (Newey and Powell, 2003) or in hypothesis testing (Lehmann and Romano,
2005).
21
Linear probability model approximations to Equations (7a) and (7b), as advocated by Angrist and
Pischke (2008), although widely used, are in general not autonomous. They can, however, be estimated
and identiﬁed for incoherent models, creating the illusion of coherency through approximation error. See
Heckman and MaCurdy (1985).
22
This might be a conceptually unsatisfactory enterprise unless the data intended to be described by the
model display disequilibrium cycling phenomena and a time sequence for the evolution of the system, e.g.,
(t)
(t+1)
, . . ., is postulated as functions of inputs where superscripts denote time-dated variables.
Y 1 , Y2

36

In addition, some frameworks for multivariate discrete data may not be suﬃciently rich
to distinguish correlation from causation. Heckman (1978) shows that log-linear models
for discrete data used in statistics (see, e.g., Bishop et al., 1975) have too few parameters
to make causal distinctions. He introduces a class of latent variable models in which such
distinctions are possible.
Note further that even in models in which the reduced form equations are well deﬁned, it is
not possible, in general, to simultaneously vary Ỹ1 and Ỹ2 so that they (i) solve Equations (7a)
and (7b) and (ii) also satisfy the requirement that (Ỹ1 , Ỹ2 ) ⊥⊥ (X1 , X2 , U1 , U2 ). This is apparent from the reduced form equations (8a) and (8b) that, under completeness, the proposed
variations must also satisfy. Nonetheless, Ỹ2 and Ỹ1 can be separately constructed to create
hypothetical models corresponding to Equations (7a) and (7b) respectively. These equations
exist as theoretical constructs independent of any particular equilibrium construct.23
Matzkin (2007, 2008, 2012, 2013) presents comprehensive and deﬁnitive treatments of
alternative approaches for identifying simultaneous equations. Our analysis readily extends
to systems with more than two equations, but for the sake of brevity we do not make the
extension here.
23

Under completeness, we can use a version of indirect least squares (ILS) to deﬁne causal parameters and
identify them where the induced variation in Ỹ1 and Ỹ2 satisfy equilibrium conditions. Thus if X1 and X2 are
disjoint, one can use ILS to identify from reduced form equations (8a) and (8b), assumed to be diﬀerentiable:
∂Y1
∂X2

=

(From 8a)
∂Y1
∂X2
∂Y2
∂X2

∂gY1 (Y2 , X1 , U1 )
∂Y2

=

∂φ1 (·)
∂X2
∂φ2 (·)
∂X2

=

∂Y2
∂X2

(From 8b)

∂gY1 (·)
∂Y2

If X1 and X2 contain common elements, the method can be modiﬁed to use only the distinct elements in
X1 and X2 in this analysis.

37

7

Summary and Conclusions

This paper examines Haavelmo’s fundamental contributions to the study of causal inference.
He produced the ﬁrst formal analysis of the distinction between causation and correlation.
He carefully distinguished the process of deﬁning causality—a mental act that assigns hypothetical variation to inputs—from the act of identifying causal models from data. Haavelmo
was remarkably clear about concepts that are still muddled in some quarters of statistics.24
Haavelmo shows us that causal eﬀects of inputs on outputs are deﬁned in abstract models
that assign independent variation to inputs. He formalized Frisch’s notion that causality
is in the mind. We formalize his insight extending his analysis for linear models to more
general models. This enables us to discuss causal concepts such as “ﬁxing” using an intuitive
approach that applies Haavelmo’s ideas.
Following Haavelmo, we distinguish the deﬁnition of causal parameters from their identiﬁcation. Our approach to deﬁning causality relies on the assumption of autonomy joined
with Haavelmo’s notion of hypothetical random variables. Together they enable us to express
the distribution of counterfactual outcomes using structural equations and the distributions
of the data by replacing the variables whose causal eﬀects we seek to establish with their
hypothetical counterparts. Causal models thus deﬁned apply standard statistical tools and
do not require new procedures like the do-calculus that lie outside the scope of the standard
tools of probability and statistics.
Identiﬁcation in Haavelmo’s model is achieved in recursive models by applying standard
statistical tools to Bayesian Networks. We link the distributions of empirical and hypothetical models by expressing the quantities of interest in the hypothetical model into observed
quantities in the empirical one.
We illustrate the beneﬁts of Haavelmo’s approach by comparing identiﬁcation of the
causal eﬀects of Pearl’s ﬂagship Front-Door model (Pearl, 2009) using a method based on
24
See, e.g., Holland (1986) and Sobel (2005) for examples of the confusion between models and identiﬁcation
strategies exempliﬁed by the claim that no causal statements are possible unless persons are randomly
assigned to treatment.

38

the Haavelmo approach and a method based on the do-calculus of Pearl (2009). While both
methods generate the same estimator, the identiﬁcation methods diﬀer on both conceptual
and methodological grounds. We discuss the limitations of methods of identiﬁcation that
rely on the fundamentally recursive approach of Directed Acyclic Graphs.
That framework cannot accommodate the fundamentally non-recursive framework of
the simultaneous equations model without violating autonomy. We consider causality in
the simultaneous equations model developed in the seminal research of Haavelmo (1943,
1944). The framework of simultaneous equations is fundamentally non-recursive and falls
outside of the framework of Bayesian causal nets and DAGs. The analysis of causality in
simultaneous equations models and the identiﬁcation of causal parameters are central and
enduring contributions of Haavelmo (1944).

39

References
Angrist, J. D. and J.-S. Pischke (2008). Mostly Harmless Econometrics: An Empiricist’s
Companion. Princeton: Princeton University Press.
Bishop, Y. M., S. E. Fienberg, and P. W. Holland (1975). Discrete Multivariate Analysis:
Theory and Practice. Cambridge, Massachusetts: The MIT Press.
Blundell, R. and J. Powell (2003). Endogeneity in nonparametric and semiparametric regression models. In L. P. H. M. Dewatripont and S. J. Turnovsky (Eds.), Advances in Economics and Econometrics: Theory and Applications, Eighth World Congress, Volume 2.
Cambridge, UK: Cambridge University Press.
Chalak, K. and H. White (2012). Causality, conditional independence, and graphical separation in settable systems. Neural Computation 24 (7), 1611–1668.
Chesher, A. and A. Rosen (2012). Simultaneous equations for discrete outcomes: Coherence,
completeness, and identiﬁcation. Working Papers CWP21/12, cemmap.
Dawid, A. (2001). Separoids: A mathematical framework for conditional independence and
irrelevance. Annals of Mathematics and Artiﬁcial Intelligence 32 (1–4), 335–372.
Dawid, A. P. (1979). Conditional independence in statistical theory (with discussion). Journal of the Royal Statistical Society. Series B (Statistical Methodological) 41 (1), 1–31.
Fechner, G. T. (1851). Outline of a new principle of mathematical psychology. Psychological
Research 49, 203–207.
Freedman, D., D. Collier, J. Sekhon, and P. Stark (2010). Statistical Models and Causal
Inference: A Dialogue with the Social Sciences. Cambridge, UK: Cambridge University
Press.

40

Frisch, R. (1930). A Dynamic Approach to Economic Theory: The Yale Lectures of Ragnar
Frisch, 1930. New York, New York: Routledge. Olav Bjerkholt and Duo Qin (eds).,
Published 2010.
Frisch, R. (1938). Autonomy of economic relations: Statistical versus theoretical relations in
economic macrodynamics. Paper given at League of Nations. Reprinted in D.F. Hendry
and M.S. Morgan (1995), The Foundations of Econometric Analysis, Cambridge University
Press.
Haavelmo, T. (1943, January). The statistical implications of a system of simultaneous
equations. Econometrica 11 (1), 1–12.
Haavelmo, T. (1944).

The probability approach in econometrics.

Economet-

rica 12 (Supplement), iii–vi and 1–115.
Hansen, L. P. and T. J. Sargent (1980, February). Formulating and estimating dynamic
linear rational expectations models. Journal of Economic Dynamics and Control 2 (1),
7–46.
Heckman, J. and R. Pinto (2013). Econometric mediation analyses: Identifying the sources of
treatment eﬀects from experimentally estimated production technologies with unmeasured
and mismeasured inputs. Forthcoming, Econometric Reviews.
Heckman, J. J. (1976, December). The common structure of statistical models of truncation,
sample selection and limited dependent variables and a simple estimator for such models.
Annals of Economic and Social Measurement 5 (4), 475–492.
Heckman, J. J. (1978, July). Dummy endogenous variables in a simultaneous equation
system. Econometrica 46 (4), 931–959.
Heckman, J. J. (1979, January). Sample selection bias as a speciﬁcation error. Econometrica 47 (1), 153–162.
41

Heckman, J. J. (2005, August). The scientiﬁc model of causality. Sociological Methodology 35 (1), 1–97.
Heckman, J. J. (2008, April). Econometric causality. International Statistical Review 76 (1),
1–27.
Heckman, J. J. and T. E. MaCurdy (1985, February). A simultaneous equations linear
probability model. Canadian Journal of Economics 18 (1), 28–37.
Heckman, J. J. and R. Robb (1985, October-November). Alternative methods for evaluating
the impact of interventions: An overview. Journal of Econometrics 30 (1–2), 239–267.
Heckman, J. J. and E. J. Vytlacil (1999, April). Local instrumental variables and latent
variable models for identifying and bounding treatment eﬀects. Proceedings of the National
Academy of Sciences 96 (8), 4730–4734.
Heckman, J. J. and E. J. Vytlacil (2005, May). Structural equations, treatment eﬀects and
econometric policy evaluation. Econometrica 73 (3), 669–738.
Heckman, J. J. and E. J. Vytlacil (2007a). Econometric evaluation of social programs, part I:
Causal models, structural models and econometric policy evaluation. In J. Heckman and
E. Leamer (Eds.), Handbook of Econometrics, Volume 6B, pp. 4779–4874. Amsterdam:
Elsevier.
Heckman, J. J. and E. J. Vytlacil (2007b). Econometric evaluation of social programs, part
II: Using the marginal treatment eﬀect to organize alternative economic estimators to
evaluate social programs and to forecast their eﬀects in new environments. In J. Heckman
and E. Leamer (Eds.), Handbook of Econometrics, Volume 6B, Chapter 71, pp. 4875–5143.
Amsterdam: Elsevier.
Heidelberger, M. (2004). Nature from within: Gustav Theodor Fechner and his psychophysical worldview. Pittsburgh, PA: University of Pittsburgh Press.
42

Holland, P. W. (1986, December). Statistics and causal inference. Journal of the American
Statistical Association 81 (396), 945–960.
Howard, R. A. and J. E. Matheson (1981). Principles and applications of decision analysis.
In Inﬂuence diagrams (1 ed.)., pp. 720–762. Menlo Park, CA: Stanford Research Institute.
Huang, Y. and M. Valtorta (2006). A study of identiﬁability in causal Bayesian network.
Technical report, University of South Carolina Department of Computer Science.
Huang, Y. and M. Valtorta (2008). On the completeness of an identiﬁability algorithm for
semi-markovian models. Annals of Mathematics and Artiﬁcial Intelligence 54 (4), 363–408.
Imbens, G. W. and J. D. Angrist (1994, March). Identiﬁcation and estimation of local
average treatment eﬀects. Econometrica 62 (2), 467–475.
Kiiveri, H., T. P. Speed, and J. B. Carlin (1984). Recursive causal models. Journal of the
Australian Mathematical Society (Series A) 36 (1), 30–52.
Koopmans, T. C. and O. Reiersøl (1950, June). The identiﬁcation of structural characteristics. The Annals of Mathematical Statistics XXI (2), 165–181.
Koopmans, T. C., H. Rubin, and R. B. Leipnik (1950). Measuring the equation systems of
dynamic economics. In T. C. Koopmans (Ed.), Statistical Inference in Dynamic Economic
Models, Number 10 in Cowles Commission Monograph, Chapter 2, pp. 53–237. New York:
John Wiley & Sons.
Lauritzen, S. L. (1996). Graphical Models. Oxford, UK: Clarendon Press.
Lauritzen, S. L. (2001). Causal inference from graphical models. In O. Barndorﬀ-Nielsen,
D. R. Cox, and C. Klüppelberg (Eds.), Complex Stochastic Systems, pp. 63–107. London:
Chapman and Hall.

43

Lauritzen, S. L. and T. S. Richardson (2002). Chain graph models and their causal interpretations. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 64 (3),
321–348.
Lehmann, E. L. and J. P. Romano (2005). Testing Statistical Hypotheses (Third ed.). New
York: Springer Science and Business Media.
Margolis, M., J. List, and D. Osgood (2012, April). Endangered options and endangered
species: what we can learn from a dubious design. Unpublished manuscript, Gettysburg
College, Department of Economics.
Marshall, A. (1890). Principles of Economics. New York: Macmillan and Company.
Mas-Colell, A., M. D. Whinston, and J. R. Green (1995). Microeconomic Theory. New York:
Oxford University Press.
Matzkin, R. L. (2007). Nonparametric identiﬁcation. In J. Heckman and E. Leamer (Eds.),
Handbook of Econometrics, Volume 6B. Amsterdam: Elsevier.
Matzkin, R. L. (2008). Identiﬁcation in nonparametric simultaneous equations models.
Econometrica 76 (5), 945–978.
Matzkin, R. L. (2012). Identiﬁcation in nonparametric limited dependent variable models
with simultaneity and unobserved heterogeneity. Journal of Econometrics 166 (1), 106–
115.
Matzkin, R. L. (2013). Nonparametric identiﬁcation of structural economic models. Annual
Review of Economics 5. Forthcoming.
Newey, W. K. and J. L. Powell (2003, September). Instrumental variable estimation of
nonparametric models. Econometrica 71 (5), 1565–1578.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. San Mateo, CA: Morgan Kaufmann Publishers Inc.
44

Pearl, J. (1993). [Bayesian analysis in expert systems]: Comment: Graphical models, causality and intervention. Statistical Science 8 (3), 266–269.
Pearl, J. (1995, December). Causal diagrams for empirical research. Biometrika 82 (4),
669–688.
Pearl, J. (2000). Causality. Cambridge, England: Cambridge University Press.
Pearl, J. (2001). Causality: Models, reasoning, and inference (Reprinted with corrections
ed.). New York: Cambridge University Press.
Pearl, J. (2009). Causality: Models, Reasoning, and Inference (2nd ed.). New York: Cambridge University Press.
Pearl, J. and T. S. Verma (1994). A theory of inferred causation. In D. Prawitz, B. Skyrms,
and D. Westerståhl (Eds.), Logic, Methodology, and Philosophy of Science, Volume IX, pp.
789–812. Amsterdam: Elsevier Science. Proceedings of the Ninth International Congress
of Logic, Methodology, and Philosophy of Science, Uppsala, Sweden, August 7–14, 1991.
Powell, J. L. (1994). Estimation of semiparametric models. In R. Engle and D. McFadden
(Eds.), Handbook of Econometrics, Volume 4, pp. 2443–2521. Amsterdam: Elsevier.
Reiersöl, O. (1945). Conﬂuence analysis by means of instrumental sets of variables. Arkiv
för Matematik, Astronomi och Fysik 32A(4), 1–119.
Robins, J. (1986). A new approach to causal inference in mortality studies with a sustained
exposure period: Application to control of the healthy worker survivor eﬀect. Mathematical
Modelling 7 (9–12), 1393–1512.
Robins, J. M. and T. S. Richardson (2011). Alternative graphical causal models and the
identiﬁcation of direct eﬀects. In P. E. Shrout, K. M. Keyes, and K. Ornstein (Eds.),
Causality and Psychopathology: Finding the Determinants of Disorders and their Cures,
Chapter 6, pp. 103–158. New York, NY: Oxford University Press.
45

Rosenbaum, P. R. and D. B. Rubin (1983, April). The central role of the propensity score
in observational studies for causal eﬀects. Biometrika 70 (1), 41–55.
Rubin, D. B. (1986). Statistics and causal inference: Comment: Which ifs have causal
answers. Journal of the American Statistical Association 81 (396), 961–962.
Simon, H. A. (1953). Causal ordering and identiﬁability. In W. C. Hood and T. C. Koopmans
(Eds.), Studies in Econometric Method, Chapter 3, pp. 49–74. New York, NY: John Wiley
& Sons, Inc.
Sobel, M. E. (2005). Discussion: ‘the scientiﬁc model of causality’. Sociological Methodology 35 (1), 99–133.
Spirtes, P. (1995). Directed cyclic graphical representations of feedback models. In Proceedings of the Eleventh Conference Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-95), pp. 491–498. San Francisco, CA: Morgan Kaufmann.
Spirtes, P., C. N. Glymour, and R. Scheines (2000). Causation, Prediction and Search (2
ed.). Cambridge, MA: MIT Press.
Tamer, E. (2003, January). Incomplete simultaneous discrete response model with multiple
equilibria. Review of Economic Studies 70 (1), 147–165.
Tian, J. and J. Pearl (2002). A general identiﬁcation condition for causal eﬀects. In Proceedings of the Eighteenth National Conference on Artiﬁcial Intelligence, pp. 567–573.
Cambridge, MA: AAAI Press.
Tian, J. and J. Pearl (2003). On the identiﬁcation of causal eﬀects. Technical report,
Cognitive Systems Laboratory, University of California at Los Angeles.
Vytlacil, E. J. (2002, January). Independence, monotonicity, and latent index models: An
equivalence result. Econometrica 70 (1), 331–341.

46

White, H. and K. Chalak (2009). Settable systems: An extension of Pearl’s causal model
with optimization, equilibrium, and learning. Journal of Machine Learning Research 10,
1759–1799.
Yule, G. U. (1895). On the correlation of total pauperism with proportion of out-relief. The
Economic Journal 5 (20), 603–611.

47

A

Mathematical Appendix

Theorem T-1:
Proof. If V is non-descendant of X̃ in the hypothetical model, i.e. V ∈ TE \ DH (X̃) then
V ∈ TE \ ChH (X̃) as ChH (X̃) ⊂ DH (X̃). Thus PH (V |P aH (V )) = PE (V |P aE (V )) from
Equation (8). Moreover, it must be the case that parents of V are also non-descendants
of X̃, i.e., P aH (V ) ⊂ TE \ DH (X̃) ⊂ TE \ ChH (X̃) ∴ P aH (V ) = P aE (V ) by Equation (8).
Another way of saying this is that the parents of V are not children of X̃. Thus we can use
factorization (6) to write:

PH (TE \DH (X̃)) =



PH (V |P aH (V )) =

V ∈TE \DH (X̃)



PE (V |P aE (V )) = PE (TE \DH (X̃)).

V ∈TE \DH (X̃)

As a consequence, PH (W ) = PE (W ) for all W ⊂ TE \ DH (X̃) and thereby

PH (W = w|Z = z) =

PE (W = w, Z = z)
PH (W = w, Z = z)
=
= PE (W = w|Z = z).
PH (Z = z)
PE (Z = z)

Conditioning on X̃ comes from that fact that X̃ ⊥⊥ (TE \ DH (X̃)), which is obtained by
applying LMC (5) to X̃ in GH .
Theorem T-2:
Proof. In order to prove the theorem, we ﬁrst partition the set of variables TE into four sets:
TE = {TE \ DE (X)} ∪ {DE (X) \ ChE (X)} ∪ {ChH (X)} ∪ {ChH (X̃)}.




 
 
Set 1

Set 2

Set 3

Set 4

Set 1 consists of all variables in TE that are non-descendants of X in the empirical model
and thereby nondescendants of X̃ in the hypothetical one. Set 2 consists of descendants of
X but not directly caused by X, i.e., except its Children. Sets 3 and 4 are the Children of
X and X̃ in the hypothetical model. Note that Sets 3 and 4 consist of all Children of X

48

in the empirical model as ChE (X) = ChH (X) ∪ ChH (X̃). We now examine the variables of
each set separately:
1. For all V ∈ TH \ DE (X) ⇒ {V, P aH (V )} ⊂ TH \ DE (X) ⊂ TE \ DH (X̃), as DH (X̃) ⊂
DE (X). Also X ∈ TE \ DH (X̃). Thus by Theorem T-1: PH (V |P aH , X̃ = x, X = x) =
PE (V |P aE (V ), X = x).
2. V ∈ DE (X) \ ChE (X) ⇒ X̃ ∈
/ P aH (V ), X ∈
/ P aH (V ), and P aH (V ) = P aE (V ). Moreover, X, X̃ must be non-descendants of V due to the acyclic property of the empirical model on X. Thus, by LMC (5), (X̃, X) ⊥⊥ V |P aH (V ). By Weak Union, X̃ ⊥⊥
V |(P aH (V ), X). Therefore PH (V |P aH (V ), X̃ = x, X = x) = PH (V |P aH (V ), X = x) =
PE (V |P aE (V ), X = x) by Equation (8).
3. V ∈ ChH (X) ⇒ X̃ ∈
/ P aH (V ) and X ∈ P aH (V ) = P aE (V ). Also, X̃ is external, thus
X̃ ⊥⊥ V |P aH (V ) by LMC (5) applied to V. Therefore PH (V |P aH (V ) \ X, X̃ = x, X =
x) = PH (V |P aH (V ) \ X, X = x) = PE (V |P aE (V ) \ X, X = x) by Equation (8) as
V ∈ ChH (X) ⊂ TE \ ChH (X̃).
4. V ∈ ChH (X̃) ⇒ X̃ ∈ P aH (V ). Moreover, X must be a non-descendant of V due to the
acyclic property of the empirical model on X. Thus, by LMC (5), X ⊥⊥ V |P aH (V ).
Therefore PH (V |P aH (V ) \ X̃, X̃ = x, X = x) = PH (V |P aH (V ) \ X̃, X̃ = x) =
PE (V |P aE (V ) \ X, X = x) by Equation (9).
Grouping items 1–4, we have that for all V ∈ TH , PH (V |P aH (V ), X̃ = x, X = x) =
PE (V |P aE (V ), X = x). Thus we can use the factorization (6) to obtain:
PH (TE |X = x, X̃ = x) =



PH (V |P aH (V ), X̃ = x, X = x)

V ∈TE

=



PE (V |P aE (V ), X = x)

V ∈TE

= PE (TE |X = x).

49

(12)

The claim of the theorem is a direct consequence of Equation (12).
Corollary C-1:
Proof.

PH (TE |X = X̃) =



PH (TE |X = x, X̃ = x)

PH (X = x, X̃ = x)
x∈supp(X) PH (X = x, X̃ = x)

PH (TE |X = x, X̃ = x)

PH (X = x)PH (X̃ = x)
x∈supp(X) PH (X = x)PH (X̃ = x)

PH (TE |X = x, X̃ = x)

PH (X = x)
x∈supp(X) PH (X = x)

x∈supp(X)

=


x∈supp(X)

=


x∈supp(X)

=



PH (TE |X = x, X̃ = x)PH (X = x)

x∈supp(X)

=



PE (TE |X = x)PE (X = x)

x∈supp(X)

= PE (TE ).
The second equality stems from P aH (X̃) = ∅ and X is not descendant of X̃, thus by
LMC (5), X ⊥⊥ X̃. Therefore PH (X = x, X̃ = x) = PH (X = x)PH (X̃ = x). The third
equality comes from the assumption that PH (X̃ = x) is constant due to uniformity. The
fourth equality comes from the fact that

x∈supp(X)

PH (X = x) = 1. The ﬁrst term of the

ﬁfth equality comes from an application of Theorem T-2. The second term of the ﬁfth
equality comes from Theorem T-1 and the fact that X ∈ TE \ DH (X̃).

50

Theorem T-3:
Proof.

PH (TE \ X|X̃ = x) =



PH (V |P a(V ))

V ∈TE \{X∪ChH (X)}

=



=

PH (V |P a(V ) \ X̃, X̃ = x)

V ∈ChH (X)

PH (V |P a(V ))

V ∈TE \{X∪ChE (X)}






PH (V |P a(V ) \ X̃, X̃ = x)

V ∈ChE (X)

PE (V |P a(V ))

V ∈TE \{X∪ChE (X)}



PE (V |P a(V ) \ X, X = x)

V ∈ChE (X)

= PE (TE \ X|do(X) = x).

The ﬁrst equality comes from the fact that the hypothetical model is a DAG, therefore we
apply factorization (6). The second equality comes from the characteristic of the do-operator,
which targets all causal links of a ﬁxed variable X. Thus the hypothetical variable X̃ must
replace all X inputs which is equivalent to ChH (X̃) = ChE (X). The ﬁrst and second terms
of the third equality come as a consequence of Equations (8) and (9) respectively. The last
equality comes from the deﬁnition of the do-operator.
Lemma L-1:
Proof.

PH (W |Z, X̃ = x) = PH (W |Z, X̃ = x, X = x)
= PE (W |Z, X = x)

by assumption X ⊥⊥ W |(Z, X̃) in GH
by Theorem T-2.

51

