NBER WORKING PAPER SERIES

NO CHILD LEFT BEHIND:
ESTIMATING THE IMPACT ON CHOICES AND STUDENT OUTCOMES
Justine S. Hastings
Jeffrey M. Weinstein
Working Paper 13009
http://www.nber.org/papers/w13009

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
April 2007

We would like to thank the Charlotte-Mecklenburg Public School District for making this project possible.
We thank Joseph Altonji, Douglas Staiger, Ebonya Washington, and participants at the Yale University
Labor-Public Finance Lunch and the Yale University Institution for Social and Policy Studies lunch
for valuable comments. We gratefully acknowledge financial support from the Smith Richardson Foundation
and the Yale University Institution for Social and Policy Studies. The views expressed herein are
those of the author(s) and do not necessarily reflect the views of the National Bureau of Economic
Research.
© 2007 by Justine S. Hastings and Jeffrey M. Weinstein. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.

No Child Left Behind: Estimating the Impact on Choices and Student Outcomes
Justine S. Hastings and Jeffrey M. Weinstein
NBER Working Paper No. 13009
April 2007
JEL No. D8,I2
ABSTRACT
Several recent education reform measures, including the federal No Child Left Behind Act (NCLB),
couple school choice with accountability measures to allow parents of children in under-performing
schools the opportunity to choose higher-performing schools. We use the introduction of NCLB in
the Charlotte-Mecklenburg School District to determine if the choice component had an impact on
the schools parents chose and if those changed choices led to academic gains. We find that 16% of
parents responded to NCLB notification by choosing schools that had on average 1 standard deviation
higher average test scores than their current NCLB school. We then use the lottery assignment of students
to chosen schools to test if changed choices led to improved academic outcomes. On average, lottery
winners experience a significant decline in suspension rates relative to lottery losers. We also find
that students winning lotteries to attend substantially better (above-median) schools experience significant
gains in test scores. Because proximity to high-scoring schools drives both the probability of choosing
an alternative school and the average test score at the school chosen, our results suggest that the availability
of proximate and high-scoring schools is an important factor in determining the degree to which school
choice and accountability programs can succeed at increasing choice and immediate academic outcomes
for students at under-performing schools.
Justine S. Hastings
Yale University
P.O. Box 208264
New Haven, CT 06520-8264
and NBER
justine.hastings@yale.edu
Jeffrey M. Weinstein
Yale University
P.O. Box 208268
New Haven, CT 06520-8268
jeffrey.weinstein@yale.edu

1. Introduction
The federal No Child Left Behind Act (NCLB) of 2001 set out to reform public education by
introducing accountability measures coupled with a public school choice requirement for all
schools receiving federal Title I funds (Title I schools).1 The public school choice component
requires that districts allow parents of children at persistently under-performing schools the
choice to send their child to a higher-performing school.2 Public school choice provisions are
included in several state-level accountability systems on which NCLB was in part based.3 Such
school choice and accountability programs are intended to provide all students the opportunity to
obtain a high-quality education, as stated in the Department of Education’s NCLB Public School
Choice Guide: “When all students … are provided high-quality educational options, and when
parents receive enough information to make intelligent choices among those options, public
school choice can increase both equity and quality in education.”4 Thus the purpose of public
school choice within an accountability program is two-fold. First, the choice provision offers
parents the immediate option to send their child to a higher-performing school. Second, the threat
of expanded parental choice may give schools a greater incentive to avoid regulation by
improving student learning to reach stipulated academic achievement goals.
This paper empirically examines how effective school choice and accountability systems might
be at accomplishing the first goal. We do so by estimating the impact of the NCLB school choice
provision in the Charlotte-Mecklenburg School District (CMS) on parents’ choices and
subsequent student outcomes. The primary purpose of the choice provision is to give students at
NCLB schools the opportunity to choose to attend schools with higher academic achievement
1

Title I schools receive federal funds provided to school districts for assistance in improving the academic
performance of students from low-income families. We will outline how Title 1 status is determined in the
Charlotte-Mecklenburg School District in detail in Section 3.
2
Title I Improving schools are Title I schools that have failed to make Adequate Yearly Progress (AYP) for two
consecutive years. We will outline how AYP is determined in the Charlotte-Mecklenburg School District in detail in
Section 3. Under NCLB, parents cannot choose another Title 1 Improving school for their child. The extent of
school choices can vary by district, as NCLB set out broad provisions but allowed states and local districts flexibility
in exactly how those mandates would be implemented.
3
See Figlio and Rouse (2006), West and Peterson (2006), and Hanushek and Raymond (2004, 2005) among others
for analyses of state-based accountability systems.
4
U.S. Department of Education. (2004). Public school choice.
http://www.ed.gov/policy/elsec/guid/schoolchoiceguid.pdf. (Page 9).

3

and to provide parents with clear information on academics to make an informed decision. The
intent is that expanded choice and simplified information on academic performance (rating
system) will allow these students to benefit from a higher-quality education. We find evidence
that the NCLB choice provision in CMS accomplished this goal for a significant fraction of
students. These families responded by choosing significantly better schools, and their children
experienced academic gains as a result of being admitted to those schools. However, we show
that that the proximity of schools with high test scores is an important factor determining the
probability of choosing an alternative school in response to NCLB notification, the average test
score at the alternative school chosen, and the subsequent impact of attending that school on
academic outcomes. Thus it is not clear how successful the school choice provision would be in
school districts with wide geographic densities of under-performing schools.
Because information on parental choice is often limited, and because NCLB just recently came
into effect, most researchers have focused on estimating the incentives for marginal schools to
improve to avoid regulation in states with accountability systems similar to those introduced
under NCLB (Figlio and Rouse (2006), West and Peterson (2006), Hanushek and Raymond
(2004, 2005)). There is some evidence that such accountability programs increase academic
achievement at marginal schools; however, there is question as to whether the findings are
generated by true gains in academic achievement or simply by gains in test score performance
(Figlio and Getzler (2002), Jacob (2005), Chakrabarti (2005), Cullen and Reback (2006)).
However, understanding parental choice response to regulation is a critical component in
understanding the effect of school choice and accountability measures on school incentives and
equilibrium outcomes. If regulation changes the population of families who choose to attend a
school, schools may face weaker or stronger incentives to avoid regulation and may take
different strategies to improve.
We use a unique policy experiment, the integration of NCLB into the newly-created school
choice plan in CMS, in order to understand how NCLB notification affected parental choice.
Starting in the 2002-2003 school year, CMS moved from a system of school assignment for
racial integration to a district-wide public school choice plan. Each subsequent spring the school
district elicited from parents the top three choices of where they wanted their child to go to

4

school and allocated slots at over-demanded schools by lottery. At the end of the 2003-2004
school year, CMS determined which schools failed to meet NCLB performance criteria. Students
slated to attend these schools in the fall were re-issued choice forms that summer, along with
notification that their school had failed to make AYP for two years in a row. They also received
the NCLB-required simplified information on academic performance at their school and at all
other schools in the district. This was a basic printout that listed every school in the district
(regardless of grade level and program) along with its proficiency score (the percent of students
who tested at or above grade level in reading and in math on the prior year’s standardized tests).
We are able to compare the choices parents made in the spring (prior to notification) with the
choices they made in that summer after receiving the NCLB notification letter. Because the two
rounds of choice fall so closely together, we can compare choices within student, before and
after the notification, thereby credibly holding all else equal (including grade level). In addition,
after receiving the summer NCLB choice forms, the district assigned students to schools based
on a lottery system. We can use these assignments to identify the effect of attending a chosen
school on academic outcomes. We find that NCLB notification led to a significant change in the
choice behavior of parents. Approximately 16% of parents who received notification responded
by choosing a school different from their NCLB school. These parents chose schools with
average test scores approximately 1 standard deviation higher than the schools they chose to
attend just a few months earlier.5 Thus, the NCLB notification with the information on academic
performance facilitated the choice of a higher-performing alternative school for a significant
fraction of parents.
We then use the lottery assignment of students to oversubscribed schools to determine if the
choices caused by NCLB notification resulted in higher academic achievement. We measure
academic achievement using observable student-level statistics such as absences, suspension
rates, and standardized test scores. We find that students who were randomly admitted to the
chosen school experienced a 9.9 percentage point decline in the probability of having a week or
5

This is 1 standard deviation in the distribution of scores across schools in the district, which translates into about
0.5 student-level standard deviations. We will show that this change in choice behavior is not generated by the
restriction that students exercising choice under NCLB cannot choose to attend other Title I Improving schools.
Rather, parents who responded to the NCLB notification picked higher-scoring schools controlling for this choice
set restriction.

5

more suspensions off of a mean of 25.8 percent. On average, students experience a positive but
statistically insignificant increase in standardized test scores. We do find evidence that students
who apply to schools that are substantially higher-scoring than their current school experience
significant improvements in test scores. Among students with above-median differences between
the chosen school’s average test score and the score at their current NCLB school, admission to
the chosen school increased test scores by 0.17-0.19 student-level standard deviations.
The remainder of this paper is organized as follows. Section 2 reviews the relevant NCLB
literature. Section 3 describes the school choice plan and implementation of NCLB in CMS.
Section 4 discusses the data and empirical results for the effect of NCLB notification on choices.
Section 5 discusses the empirical results for the effect of NCLB notification on student
outcomes. Section 6 concludes.

2. Literature Review
There is little empirical evidence to date on the impact of NCLB on parental school choices and
subsequent student outcomes. Because the regulation is so recent and information on student
choices is often not available, researchers have focused on the impacts of state accountability
systems on academic achievement. Greene (2001), Figlio and Rouse (2006), and West and
Peterson (2006) examine the effect of the Florida A+ Plan on school average test scores. They all
find a relative improvement in test scores at low-performing schools that are faced with the
threat of offering school vouchers (close to regulation, but not actually regulated). Figlio and
Rouse (2006), however, provide evidence that these gains are mainly due to the stigma of being
labeled a low-achieving school instead of voucher threats. Clark (2003) examines the 1990
Kentucky Education Reform Act (KERA) which consists of funding, academic, and
accountability reforms. She does not find an impact on academic performance measures for
white students but does find some evidence that KERA increased the performance of black
students. Hanushek and Raymond (2004, 2005) and Carnoy and Loeb (2002) construct statelevel panel data on accountability systems and find a positive relationship between average test
scores and accountability measures. However, Figlio and Getzler (2002), Jacob (2005),
Chakrabarti (2005), and Cullen and Reback (2006) all provide evidence that changes in school-

6

level average accountability measures may result from policy-induced behavior aimed at
maximizing test scores that count in regulation rather than improving academic achievement in
and of itself.
However, most of these studies do not examine how the choice component of accountability
systems such as NCLB affects parents’ choices and subsequent student achievement (West and
Peterson (2006)). Hastings, Kane, and Staiger (2006a) examine parental choices and preferences
using the introductory year of school choice in CMS. They find that low-income parents of
students with low academic performance tend to place a low weight on academics when
choosing schools. Hastings, Van Weelden, and Weinstein (2007) randomize simplified
information on academic quality of schools across non-NCLB schools serving middle- to lowincome students. They show that receiving simplified information with the school choice form
causes an increase in active choice participation, an increase in the average test score of the
chosen schools, and a doubling in the estimated preference for academic performance. They
conclude that a lower preference for academics for low-income students is consistent with higher
information and decision making costs. In a similar manner, NCLB requires that families be
provided with simplified information on the academic quality of their own school as well as on
other schools in the district. We find in CMS that a fraction of students respond by choosing
much higher academically-performing schools.
Another line of research estimates the academic gains for students who exercise choice. These
papers exploit random assignment of students to over-demanded schools in school choice or
voucher programs to estimate the average treatment effect of attending a first-choice school,
conditional on the school chosen (Witte, Sterr, and Thorn (1995); Greene, Peterson, and Du
(1997), Witte (1997), Rouse (1998), Peterson, Myers, and Howell (1998), Mayer et al. (2002),
Krueger and Zhu (2004), and Cullen, Jacob, and Levitt (2006)). These papers have been unable
to find robust or significant academic gains from attending a first-choice school; however,
Hastings, Kane, and Staiger (2006b) combine detailed information on student choices with
lottery randomization into first-choice schools and show that students placing a high weight on
academics when choosing schools experience significant academic gains when randomized into
their first-choice schools. We will show that NCLB led some parents to choose schools with

7

substantially higher scores and that their children experienced some gains in traditional measures
of academic outcomes as a result of gaining admission to those schools.

3. CMS School Choice Plan and No Child Left Behind

3.1. Overview of the CMS School Choice Plan
CMS introduced district-wide school choice in the fall of 2002. Prior to that, CMS operated
under a racial desegregation order for three decades, busing students from discontinuous
neighborhoods to achieve racial balance at schools across the district. In September 2001, the
U.S. Fourth Circuit Court of Appeals ordered the district to dismantle the race-based student
assignment plan by the beginning of the next school year. In December 2001, the school board
voted to approve a new district-wide public school choice plan.
In the spring of 2002, parents were asked to submit their top three choices of school programs for
each child. Each student was assigned a “home school” in her neighborhood, often the closest
school to her, and was guaranteed a seat at this school. Magnet students were similarly
guaranteed admission to continue in their current magnet programs. Admission for all other
students was limited by grade-specific capacity limits set by the district. Students could choose
any school in the district; however, busing transportation was only guaranteed to schools in a
student’s quadrant of the district (the district was split into 4 quadrants called “choice zones”).
The district allowed significant increases in school enrollment size in the first year of the school
choice program in an expressed effort to give each child one of her top three choices. In the
spring of 2002, the district received choice applications for approximately 105,000 of 110,000
students. Admission to over-subscribed schools was determined by a lottery system as described
in Hastings, Kane, and Staiger (2006b).
Once the first year of school choice was completed, students were required to submit choice
forms in subsequent years only if they were new to the district, rising graders (Kindergarten, 6th,
or 9th grade), affected by changes in home school boundaries due to new school openings, or
8

wanted to change schools from their current school assignment. In other words, once the first
year of school choice was completed, and students were for the most part attending chosen
schools, students had the guaranteed right to remain in that school until the terminal grade and
did not need to submit a choice form unless they wanted to change schools again. In the
following two years, CMS continued to experience near complete participation in the school
choice plan. In each year, admission to over-subscribed schools was determined by a lottery
system. However, after the first year of choice, CMS did not expand capacities at schools in an
attempt to accommodate demand, and hence, the number of oversubscribed schools increased
substantially.

3.2. Information on Schools
In order for parents to determine which schools to choose, CMS provided several resources.
First, each family received a choice book. The choice book was approximately 100 pages long. It
contained detailed instructions on how to complete the school choice form and how to submit it
along with a brief description of the lottery process.6 The bulk of the choice book was devoted to
written descriptions of each school and program, from preschool through high school. There are
approximately 120 elementary, 40 middle, and 30 high school choice options in the district. The
descriptions were written by the schools, describing the positive features each school offered to
students. Objective measures of school characteristics such as average test score performance,
suspension rates, or racial compositions were not provided.
In addition, CMS provided a family application center that parents could phone or visit in order
to ask questions about the school choice process. The staff members at the family planning
center were instructed to emphasize the positive aspects of each school during their discussions
with parents. In particular, staff members were supposed to respond to questions like “Which
school is the best school?” by advising parents to discuss with their children what their needs
were and then to visit the different school options in order to determine which school was the
best for their children, since what a “good” school is depends on each individual child.
6

Parents were not told how the lottery was run (e.g. first-choice maximizer) or how ‘priority boosts’ were
implemented.

9

CMS also offers an extensive website. On this website, parents can review statistics for each
school one-by-one. The school profiles provide statistics such as physical locations, standardized
test score performances, suspension rates, racial compositions, and attendance rates. However,
statistics for schools were reported as averages for the entire school even if different school
programs were housed on the same campus (e.g. magnet program and non-magnet program).
This aggregation may mask the true achievement rates of the separate school choice options on
each campus. In addition, parents would have to view all statistics for each school separately,
instead of viewing a statistic for all of their choice options on one simplified page. Hence,
obtaining objective information on schools would involve a significant web search and
comparison.

3.3. The Implementation of NCLB in CMS
NCLB legislation was introduced in January 2002. Beginning in the summer of 2003, CMS
implemented NCLB in accordance with North Carolina state regulation that in turn was based on
federal requirements. Each year, all schools are required to make Adequate Yearly Progress
(AYP). However, only Title I schools face sanctions under NCLB if they fail to do so. As
defined by CMS, a school is a Title I school (receives federal Title I funds) if 75% or more of its
students qualify for federal lunch subsidies. As defined by North Carolina under NCLB
compliance, a school needs to satisfy certain academic targets for 10 subgroups of students in
order to make AYP.7 Each subgroup needed to have forty or more students for it to be included
in the determination of school AYP. If just one target was missed for one subgroup, then the
school failed to make AYP. Targets include the percentage of students scoring proficient on
North Carolina standardized tests for math and reading for each subgroup (with the percentage
needed to make AYP gradually increasing over time in order to meet the federal requirement of
100% proficiency by the end of the 2013-14 school year)8, a minimum participation rate in each
of the exams for each subgroup (95% in each year or averaged over the prior two or three years),
7

For North Carolina, the subgroups are the entire school, Asian, American Indian, Black, Hispanic, Multi-racial,
White, economically disadvantaged, limited English proficiency, and students with disabilities.
8
This requirement can also be satisfied for a subgroup if its percent proficient falls within a 95% confidence interval
for the target percent proficient.

10

attendance rates for elementary and middle school students (an increase in 0.1% from the
previous year or anything over 90%), and graduation rates for high school students (an increase
in 0.1% from the previous year or anything over 90%).9
At the end of the 2003-2004 school year, CMS compiled the test score outcomes for schools in
the district and determined that sixteen schools, ten elementary and six middle schools, were
both Title I schools and had failed to make AYP for the past two years. These schools were
categorized as Title I Improving and entered regulation under NCLB. The regulation implied that
parents needed to be notified of the NCLB status of their school and offered the choice to attend
an alternative school. In addition, the district (as part of a federal requirement) was required to
supply with this notification information on the academic achievement of the schools that parents
could select.10 CMS provided information on the percent of students in the NCLB school who
made grade level in reading or math (percent proficient) for every school in the district (a three
page spread-sheet print-out), as well as list of Title I Improving schools since students exercising
choice under NCLB were not allowed to choose to attend another Title I Improving school. Thus
the NCLB legislation provided simplified information to parents on the academic achievement at
their school and at every other school in the district along with notification that their school had
failed to make AYP and that they therefore had a right to choose to send their child to another
non-NCLB school.
Parents who received the NCLB notification had submitted (along with all other parents) choice
forms in the spring of 2004 for the 2004-2005 school year. Because NCLB schools were
identified in June 2004, at the end of the school year, CMS re-sent choice forms in July along
with the NCLB notification to parents of students slated to attend NCLB schools in the fall.
These forms had a similar format to the typical CMS choice form and allowed parents to submit
three choices for what school they would like their child to attend in the 2004-2005 school year.
Because of the timing of the choice plan and determination of NCLB schools, we are able to
observe how choices change for parents receiving NCLB forms. We observe their choices
9

For information on other means by which subgroups can make AYP, please see the Consolidated State Application
for North Carolina (2005) which provides federal NCLB guidelines along with North Carolina’s implementation of
these guidelines.
10
U.S. Department of Education. (2004). Public school choice.
http://www.ed.gov/policy/elsec/guid/schoolchoiceguid.pdf. (Page 18).

11

submitted in the spring of 2004 for which school they would like their child to attend in the fall.
After parents received the NCLB notification letter, we observe their choices submitted in July
2004 for which school they would like their child to attend in the fall given this new information
on their school’s relative academic performance.
Once the NCLB choice forms were received, parents had approximately one month to submit
them. Students were then entered into a choice lottery. CMS made spaces available at previously
full schools (partly through normal summer attrition of current students) in order to
accommodate NCLB students. Students were sorted by priority group and a randomly assigned
lottery number.11 Admission to schools was thus determined first by priority group and then by
lottery number. Priority groups were based on academic achievement (above- or below-grade
level) and lunch-recipient status. This was done to satisfy the federal requirement that the poorest
and lowest-achieving students be allowed the first opportunity to attend an alternative school.
The lottery was not run as a first-choice maximizer as was typically done in the spring lotteries
for the district as a whole. Instead, all of a student’s choices were evaluated at once when her
lottery number came up. Parents, as usual, did not know the lottery process, and in this case they
were not informed of the NCLB-specific priority groupings either. If a student did not gain
admission to any of her choices, her parents were allowed to check a box that stated that they
would like the district to attempt to place their child administratively in a non-NCLB school with
an open slot instead of returning her to her NCLB school. The district would then supply an
assigned school in August, and parents could accept or reject that school over their current
NCLB school at that point. Of the students used in our analysis, 1,092 out of 6,695 submitted a
choice form in July and chose a school other than their current NCLB school. Of these, 615
checked the box saying that they did not wish to return to their NCLB school if they did not get
any of their choices. Of these, 208 did not receive one of their choices and were offered
administrative placement. Of these, only 50 ended up attending the administratively placed
school.

11

The random number was assigned by a computer using an algorithm that we verified with CMS computer
programmers.

12

4. Data and Regression Analysis
4.1. Data Description
We have secure access to administrative data from CMS including choice form information for
every student who submitted a form in the Spring 2004 school choice round and the July 2004
NCLB choice round, student-level lottery numbers for each choice round, school assignments,
attendance records, test score outcomes, and student demographics. We also have information on
student and school locations. We use the student-level data to construct school characteristics.
These characteristics include percent black, percent of students receiving lunch subsidies, and
average standardized test scores - each student’s test score is standardized by grade level, and the
average of these standardized scores over the students at a school choice is the school choice’s
test score measure. We focus on elementary and middle schools (students in K-8th grades), since
there were no Title 1 Improving high schools in CMS.
Table I describes the schools that were designated Title I Improving at the end of the 2003-2004
school year. The 16 Title I Improving schools had on average significantly lower test scores than
the district average. However, there were elementary and middle schools with average test scores
in this range that were not Title I Improving because they either failed to make AYP both years
but did not make Title I classification for at least one year (21 schools fall into this category) or
because they were Title I both years but made AYP in at least one of the two years (13 schools
fall into this category). Because Title I status in CMS is defined as 75% or above free- or
reduced-lunch concentration, Title I Improving schools have a substantially higher-than-average
lunch-recipient rate. In addition, they have a higher proportion of black students, a lower average
neighborhood income level, and higher-than-average suspension rates. In addition, according to
school-level capacity data from CMS, these schools are on-average under-demanded (below
capacity) and have a smaller student body population.

4.2. The Effect of NCLB on Student Choices
We began with the 8,284 students who received NCLB notification, of which 1,363 responded
by filling out a form in July. Parents were told, just like in the regular spring lottery, that if they
13

wanted to remain at their current school, they did not have to fill out a form. We exclude from
the analysis students who were not active in CMS at the time of the spring lottery (221 students),
students with special needs or those being retained (1,245 additional students), and students who
had missing demographic information (123 additional students). This left us with a sample of
6,695 students who received NCLB notification, of which 1,149 responded by submitting a form
in July. Of the parents who did fill out a form in July, 57 of them listed their current NCLB
school as their first choice which they did not need to do in order to remain at their NCLB
school. Thus, 1,092 students filled out a form in July and chose a school different than their
current NCLB school first, and 5,603 students either did not respond to NCLB notification or
chose their NCLB first in the July lottery.
Student were slated to attend a NCLB school in the fall of 2004 for one of two reasons: their
parents chose that school in the spring (either actively or through default), or their parents chose
a different school, did not win admission, and the student was assigned to the NCLB school.
Table II shows the cross tabulation of two indicators: if the parent chose the NCLB school in the
spring, and if they chose a school other than their NCLB school in July. Parents who did not
choose their NCLB school first in the spring but still got placed in the NCLB school were twice
as likely to respond by choosing a school different than their NCLB school first in July (30.31%
versus 14.55%). Approximately two thirds of parents who did not choose their NCLB school
first in the spring did not decide to select another school in July. This may be because parents
find it more difficult to change schools in the middle of the summer, not long before the new
school year. Therefore, while NCLB notification doubled the fraction of students choosing to
leave their NCLB school (865 versus 749), we may in fact be underestimating the effect given
the timing of the notification.
If we assume that parents who did not fill out a form in July wanted their child to attend the
NCLB school, then we can estimate the average impact that NCLB had on parents’ choices.
Table III presents a reduced-form regression of the effect that receiving a NCLB form had on the
characteristics of students’ first-choice schools. The data include each student’s first-choice
school in the spring and in July, where we assume that a student chose her current NCLB school
first in July if she did not fill out a form. The average effect is the average within-student change

14

in the characteristics of the first-choice school before (Spring) and after (July) receiving the
NCLB notification. Table III shows that receiving the NCLB form made parents less likely to
choose their NCLB school first, and their first-choice schools had higher average test scores,
lower percent black, and lower lunch subsidy rates. Although the coefficients are statistically
significant, they are small in magnitude. For example, NCLB form receipt caused an average
increase in the scores of schools chosen first of only 0.047 standard deviations – approximately
one to two percentile point ranks. However, 84% of parents in our sample did not return the form
or returned the form but chose their NCLB school first, therefore choosing a school with no
change in average test scores.12 The significant but small in absolute value increase suggests that
NCLB had a very large impact on the characteristics of the first-choice school for the 16% who
submitted forms and chose a school other than their NCLB school first in July.
Table IV shows the differences in the characteristics of first-choice schools between spring and
July choice rounds for students who responded in July by choosing a school other than their
NCLB school first. The first two rows show that students chose schools first that were on
average .5 student-level standard deviations higher in average test scores in July versus the
spring. This is more than a 15 percentile point rank increase in scores and is 1 standard deviation
higher based on the distribution of school average test scores. We break down these numbers by
race; however, there is not a substantial difference by race. In addition, the schools chosen first
in July had a substantially lower percent black and a substantially lower free- and reduced-lunch
concentration. In the spring, the students’ first-choice schools were in the 78th percentile of the
distribution of school percent black and the 82nd percentile of the distribution of school percent
free and reduced lunch of the schools they could choose. In July, the students’ first-choice
schools were in the 53rd percentile of the distribution of school percent black and the 47th
percentile of the distribution of school percent free and reduced lunch of the schools they could
choose. Because students were not allowed to choose other NCLB schools in the July lottery, we
may be concerned that the increase in the score of the first-choice school is partly generated by
the fact that students were prohibited from choosing some of the schools with low performance.
Rows 3 and 4 show the average test score of schools within five miles of the student (an
12

As shown in Table II, about 9% of the students who chose their NCLB school first in July (by virtue of not
submitting a form or by submitting a form and putting down their NCLB school first) chose a different school first
in the spring and did not get in. For these students, the difference in test scores would not be zero.

15

approximation for relevant schools in the choice set). The average score of available schools
increases from Row 3 to Row 4 due to the restriction that students could not select another
NCLB school. If we compare Rows 1 and 2 to Rows 3 and 4, we see that students’ first-choice
schools had lower-than-average test scores given their choice set in the spring and had higherthan-average test scores given their choice set in July. Hence, the increase in the scores at the
first-choice school was not mechanically generated by the choice set restriction placed by NCLB.
The statistics presented in Table IV showed that a fraction of families who received NCLB
notification responded by choosing substantially better schools. However, most families did not
respond. We can empirically examine what types of families were more likely to respond by
choosing a school other than their NCLB first in the July lottery; for example, did NCLB
notification affect the choices of higher-achieving students more than it did for lower-scoring
students? Tables V and VI both examine the decision to respond to the NCLB notification as a
function of student and school characteristics. Table V presents simple differences in mean
student attributes across these students who responded by choosing a school other than their
NCLB school first in July versus students who chose their NCLB school first in July (the latter
group includes students who did not fill out a form), controlling only for NCLB school fixed
effects. The first two columns of statistics in Table V give the straight means for the two groups
of students for each variable of interest. The final column reports the difference in means
between the students who responded by choosing a school different from their NCLB school first
in July versus students who chose their NCLB school first in July, adjusting for NCLB school
fixed effects.
The coefficients show that students who chose a school other than their NCLB school first in the
spring, but did not gain admission to that school, were more likely to choose a school different
from their NCLB school first again in July. This reflects the cross tabulations seen in Table II. In
addition, students in rising grades were more likely to choose to attend a different school in July.
This makes sense since many parents may find it easier to switch their child’s school at this time.
We also find that black students and students who are in magnet programs in their NCLB schools
are marginally statistically significantly more likely to choose a school other than their NCLB
school first in July. There are no significant differences for most of the other student-level

16

demographics such as gender, lunch-recipient status, baseline test score, and income. However,
parents of students who had more suspensions and who had fewer unexcused absences in the
baseline year were more likely to respond to the NCLB notification by choosing a different
school first in July.
There are two other characteristics that are on average significantly different across the two
groups of students. The distance from the family to the NCLB school and the average test score
of local schools all significantly differ across students who responded by choosing a school other
than their NCLB school first in July versus students who chose their NCLB school first in July.
These variables would increase or decrease the attractiveness of the NCLB school relative to
other potential schooling options. Students who responded by choosing a school other than their
NCLB school first in July lived on average further from their NCLB school and also had higher
average scoring schools to choose from within a five-mile driving distance. Hence alternatives to
the NCLB school may have been slightly more attractive for these students, increasing the
probability that they would respond to the NCLB notification by choosing a different school
first.
Table VI presents a conditional (fixed-effects) logit of the probability of responding to the NCLB
notification by choosing a school other than the NCLB school first as a function of the baseline
characteristics presented in Table V. Again, the fixed effects are at the NCLB school level. The
logit results are for the most part consistent with the mean differences in Table V. Students who
chose to attend a school other than their NCLB school in the spring, but were assigned back to
their NCLB school, were significantly more likely to respond to the NCLB notification. The
coefficient implies a 114% increase in the odds of responding (exp(.759) – 1), which translates
into an 11.0 percentage point increase in the probability of responding to the NCLB notification
by choosing a school other than the NCLB school first in July. In addition, an increase in
baseline test score significantly increases the odds of choosing a school different from the NCLB
school first in July. A one standard deviation increase in baseline test scores increases the odds
of choosing out of the NCLB school by 11%, which translates into a 1.5 percentage point
increase in the probability of responding by choosing out. Again we find that an increase in
baseline number of unexcused absences significantly decreases the probability of responding,

17

while an increase in suspensions significantly increases the probability of responding. For this
sample of students, the raw correlations between test scores, unexcused absences and
suspensions are small (between 0.18 and 0.22 in absolute value), implying that the effects do not
end up canceling each other out for a typical student. Taken together, there seem to be two types
of students who responded to NCLB notification by choosing a school other than their NCLB
school first in July: those who were doing on average better in school (higher test scores and
fewer unexcused absences) and those who were having problems (more suspensions).
The final significant determinant of responding to the NCLB notification is the average test score
of nearby schools. If the average test score at nearby schools increases by one student-level
standard deviation, there is an 84% increase in the odds of responding by choosing a school other
than the NCLB school first. This translates into an 8.8 percentage point increase in the
probability of responding. Thus, a key determinant of response is the availability of highachieving alternatives in the surrounding area. If there are not better alternatives available within
a reasonable distance, parents may not choose out of failing schools simply because there are no
better alternatives to choose from.
Table IV showed that students who responded to the NCLB notification chose on average
substantially better schools. There is, however, a large variation across responders in the average
test scores of their chosen schools. Figure 1 shows a kernel density estimate of the difference in
the test score of the first-choice school listed on the July choice form and the NCLB school for
students who chose a school other than their NCLB school first in July. On average, responders
selected first-choice schools with 0.62 student-level standard deviation higher test scores than
their NCLB school. However, a small fraction of students chose schools that were close to or
slightly worse performing than their NCLB school, while another minority of students chose
some of the highest-performing schools in the district; schools that outperformed their NCLB
school by over one student-level standard deviation in test scores.
Of students who chose a school other than their NCLB school first in July, what types of students
chose substantially better schools? Table VII presents regression results of the test score at the
July first-choice school on baseline student characteristics for those who chose a school different

18

from their NCLB school first in July, controlling for NCLB school fixed effects. Students who
did not choose their NCLB school first in the spring chose on average higher-scoring schools in
July. Black students chose schools with slightly lower test scores, as did students who were
entitled to receive federal lunch subsidies and students with unexcused absences. Students
enrolled in magnet programs at NCLB schools chose higher-scoring schools.
The biggest observable determinant of the average score of the first-choice school in July seems
to be proximity to higher-scoring schools. Increasing the average test score of schools in a fivemile radius by one student-level standard deviation increases the average score at the first-choice
school by 0.229. Hence the proximity and availability of much higher-scoring schools seem to
determine both the probability of choosing to leave the NCLB school as well as the test score at
the first-choice school in July. This is consistent with the strong weight parents place on
proximity when choosing schools as found in Hastings, Kane, and Staiger (2006a). Choice is
most effective when there are many options in close proximity, all else equal.

5. The Effect of Choices on Student Outcomes
NCLB notification succeeded in changing choices for some parents at Title I Improving schools.
The simplified information led a significant fraction of parents to choose to send their children to
higher-performing schools. However, it is not clear if changing the choices that parents make
improves their child’s academic outcomes. It is therefore important to estimate the impact that
these choices had on subsequent measures of academic achievement. We will use academic
outcomes at the end of the first year after NCLB notification to test for the presence of academic
gains as a result of attending a newly-chosen school.
Once choice forms were submitted, admissions were determined by lottery process. The lotteries
were run based on the number of seats made available for each grade and choice combination.
As described earlier, the lottery number was the concatenation of two priority numbers followed
by a random number. Priority was given to students performing below grade level and to
students who qualified for free or reduced lunch. This was done to satisfy the NCLB requirement

19

that the lowest-performing and poorest students be given the first right to attend a school other
than their “failing” school.
We present two different methods for estimating the effect of winning versus losing the lottery
on outcomes. Method 1 uses only the priority group (if any) in each grade and choice
combination for which some students won and some students lost that lottery; that is, we include
only students for whom lottery number alone determined admission to that grade and choice
combination. Method 2 uses all students, regardless of priority group, whose first-choice school
and grade is one in which some students won and some students lost the lottery for admission to
that school. Of the 1,092 students who filled out a form in July and chose a school different than
their current NCLB school first, 227 students are in the sample defined by Method 1, and 562
students are in the sample defined by Method 2.
For the sample defined by Method 1, admission is determined solely by lottery number, so the
impact of winning or losing the lottery is well identified. On the other hand, the sample size is
small, which may imply that the results do not hold more broadly. Hence, we also use the sample
defined by Method 2, more than doubling the sample size. However, here admission is
determined by priority group and lottery number; students with lower tests scores and/or who are
free- or reduced-lunch recipients have lower (better) priority numbers and hence a better chance
of admission. We will control for student baseline characteristics, as well as first-choice and
grade fixed effects, in the regression analyses that follow.
Table VIII reports mean baseline characteristics for lottery winners and losers, as well as
regression adjusted differences from an OLS regression including fixed effects for the school
choice program and grade for which the lottery is being conducted (lottery-block fixed effects).13
The first three columns use the sample defined by Method 1, and the second three columns use
the sample defined by Method 2. Coefficients in column 3, rows 1 through 7, show that lottery
winners and losers do not have significantly different baseline demographic or academic baseline
characteristics for students in the Method 1 sample. Hence lottery numbers do not predict
13

For the Method 1 sample, note that lottery-block fixed effects span priority-group fixed effects. We must control
for lottery-block fixed effects since the odds of admission change across each lottery.

20

baseline characteristics for students in the Method 1 sample, as we would expect if they were
indeed randomly assigned. Recall that the lottery was not run as a first-choice maximizer; rather,
it evaluated a student’s choices when the student’s lottery number came up. To construct the
Method 1 sample, we treat a student as choosing a school if she listed it as any of her choices. It
may be the case that students who listed a school second are less likely to be admitted since they
may have had a higher (worse) priority group and/or higher (worse) lottery number to have not
been admitted to their first-choice school. The third column of the final row shows that winners
were no more or less likely to have listed the choice as their first choice than losers were. We fail
to reject the null hypothesis that there is a differential impact of winning versus losing the lottery
on the probability of listing the choice first.14
The last column of Table VIII shows that baseline characteristics do vary significantly across
lottery winners and losers for students in the Method 2 sample. Students who won the lottery
were more likely to be black, were more likely to receive free or reduced lunch, had lower
incomes, had more suspensions, and had lower baseline test scores. These differences are
mechanically generated by the priority group definitions, and they are most significant for lunchrecipient status and baseline test scores (the two variables that define priority groups). We
control for these baseline characteristics when estimating the effect of winning versus losing the
lottery on student outcomes.
We now want to test if there are significant differences in end-of-year outcomes for students who
won the lottery to attend their chosen school versus those who lost the admission lottery. Since
we do not have data on outcome variables for students who were not enrolled in CMS for the
2004-2005 academic year, it is important to look at attrition. Table IX shows the effect of
winning versus losing the lottery on whether the student was not enrolled in any CMS school in
the 2004-2005 school year. This estimate gives the differential attrition between lottery winners
and lottery losers. The first two columns use the sample defined by Method 1, and the second
two columns use the sample defined by Method 2. The average attrition rate is 6.2% for students

14

In the Method 1 sample, 172 students listed the school as their first choice, and 55 listed the school as their second
choice. There were none who listed it as their third choice. However, given the many different choices and the
availability of those choices, this may not affect our sample considerably.

21

in the Method 1 sample, and 5.0% for students in the Method 2 sample.15 For the Method 1
sample, the estimated effect of winning versus losing the lottery on attrition is small (0.057) but
marginally significant (p-value = 0.081). However this has the opposite sign than what we would
expect. The positive value implies that, if anything, lottery winners were more likely to attrit than
lottery losers, instead of the other way around. For the Method 2 sample, the estimated effect of
winning the lottery on attrition is much smaller (0.012) and is not statistically significantly
different from zero.
Table X presents the reduced-form estimates of winning versus losing the lottery on measures of
academic achievement. The first four columns use the sample defined by Method 1, and the
second four columns use the sample defined by Method 2. The results are presented for all
students on average, as well as for those with above- and below-median expected “treatment”,
where treatment is defined as the difference between the test score at the chosen school and the
test score at the current NCLB school. For the Method 1 sample, the median difference in the test
score of the chosen school and the test score at the current NCLB school is 0.893 student-level
standard deviations. For the Method 2 sample, the median difference in the test score of the firstchoice school and the test score at the current NCLB school is 0.642 student-level standard
deviations. All regressions include fixed effects for each grade and choice combination, as well
as student baseline characteristics: black, lunch recipient, female, demeaned income, an indicator
for greater than or equal to five unexcused absences, an indicator for greater than or equal to one
unexcused absence, an indicator for greater than or equal to five suspensions, an indicator for
greater than or equal to one suspension, total number of unexcused absences, and total number of
suspensions. Results for student standardized test score outcomes also include baseline student
test scores, lowering the sample size to 161 students in the Method 1 sample and 451 in the
Method 2 sample, as only students in third through eighth grades take these exams. Standard
errors are clustered at the school-choice and grade level in all regressions.
The first row presents the effect of winning versus losing the lottery on the average test score of
the school attended. For students in the Method 1 sample, lottery winners attended schools with

15

These rates are lower than typical, but it is probably because students in our sample are observed half way through
the summer so we would expect them to be lower if most moving occurs in the first half of the summer.

22

on average 0.324 student-level standard deviation higher test scores than lottery losers. Students
who chose schools with substantially better (above-median) test scores than their current NCLB
schools attended schools with on average 0.375 student-level standard deviation higher test
scores if they won the lottery. Students who chose schools with below-median differences in test
scores attended schools that had 0.166 higher average test scores if they won the lottery;
however, the coefficient is not significant. For students in the Method 2 sample, lottery winners
attended schools with on average 0.146 student-level standard deviation higher test scores than
lottery losers, which is marginally significant (p-value = 0.070). Students who chose schools
with substantially better (above-median) test scores had a significant 0.256 increase in the score
of the school attended as a result of winning the lottery. This point estimate is -0.032 and
insignificant for students winning lotteries to attend schools that with moderately (belowmedian) higher scores than their NCLB school. Given that we find a small and insignificant
effect of winning versus losing the lottery on the test score of the school attended for students
with a below-median “treatment”, we may not expect to see significant academic gains for these
students as a result attending their first-choice school.
The second row of Table X shows that winning versus losing the lottery does not have a
statistically significant impact on the probability of having a week or more (“serious”) unexcused
absences. This result holds across both samples. However, we do find that, for the Method 1
sample, the probability of having at least one unexcused absence decreases by 21% (0.174 off of
a base of 0.845) when students choose, and gain admission to, schools that have above-median
increases in test scores over their current NCLB schools. Across both samples, the point
estimates imply that students who choose schools with below-median differences in test scores
have an increased likelihood of having at least one unexcused absence if they won, and the effect
is statistically significant for the Method 2 sample.
The fourth and fifth rows of Table X show the impact of winning versus losing the lottery on the
probability of having at least one week of suspensions (“serious”) or at least one suspension
(“mild”), respectively. Across both samples, we do not find any evidence that winning versus
losing the lottery affected the probability of having a “mild” suspension. For the Method 1
sample, we find, on average, that the likelihood of having “serious” suspensions decreases by a

23

statistically significant 38% (0.099 off of a mean of 0.258) when students win the lottery. While
the decrease is larger for students in the above-mean “treatment” group, it is not statistically
significant. The coefficient is closer to zero and insignificant for students in lotteries to attend
schools with below-median differences in test scores. For the Method 2 sample, the impact of
winning versus losing the lottery on “serious” suspensions is not statistically significant, both
overall and for the above- and below-median “treatment” students. Taken together, we do find
some evidence of an overall decrease in the probability of “serious”, but not “mild” suspensions,
for lottery winners versus lottery losers.
The final row presents results for standardized test scores. The students’ test scores are the
average of the math and reading scores for the North Carolina End of Grade tests, standardized
by grade level. For the students in the Method 1 sample, on average, winning versus losing the
NCLB lottery had a positive but insignificant effect on student test scores. For the students in the
Method 2 sample, the overall effect of winning versus losing the lottery is also insignificant, but
larger in magnitude than in the Method 1 sample. However, across both samples, for the students
who choose schools with substantially higher test scores than their current NCLB school, there is
a statistically significant and large effect of winning versus losing the lottery on test scores.
Across both samples, students who won versus lost these lotteries experienced a gain of
approximately 0.17 to 0.19 standard deviations on their exams. This is a substantial improvement
in cognitive outcomes, equivalent to approximately 5-6 percentile rank points on the End of
Grade exams. For both samples, students applying to schools that were only slightly better than
their current NCLB schools did not experience gains on test scores. The point estimate is
insignificant for the two samples, slightly negative in the case of the Method 1 sample and
slightly positive in the case of the Method 2 sample.

6. Conclusion
The school choice component of NCLB is aimed at allowing parents of children in underperforming schools the opportunity to choose to send their children to schools with higher
academic achievement. The immediate goal is to allow these students the opportunity to achieve

24

higher academic outcomes. This paper examined the introduction of NCLB in CMS to determine
if NCLB had an impact on the school choices parents made and if those choices led to gains in
academic outcomes.
Using detailed school choice data from CMS, we are able to compare the choices parents made
prior to NCLB notification with the choices they made after receiving notification. We find that
NCLB notification led to an overall small but significant change in the choice behavior of
parents. However, approximately 16% of parents who received notification responded by
choosing schools with test scores that were on average 1 standard deviation higher than the
schools they chose to attend just a few months earlier. Thus, the NCLB notification with the
required simplified information on academic performance of schools in the district facilitated the
choice of a higher-performing alternative school for a significant fraction of parents. The NCLB
notification increased the fraction of parents choosing to attend a school other than their NCLB
school by more than 100%.
We then use the lottery admissions of students to schools to determine if the choices caused by
NCLB notification resulted in higher academic achievement. We find some evidence that
winning admission to a chosen school leads to a significant decline in serious suspension rates
but no significant average gain in standardized test scores. We do find some evidence that
students who apply to schools that are substantially higher-scoring than their current NCLB
school do experience significant improvements in test scores. Among students with abovemedian differences between the chosen school’s average test score and the score at their current
NCLB school, admission to the chosen school increased test scores by 0.17 – 0.19 student-level
standard deviations.
However, our analysis shows that availability of and proximity to high-performing schools is a
key factor in determining the probability of responding to NCLB notification by choosing an
alternative school, the average test score at the school chosen, and hence the estimated test score
gain from attending the chosen school. Our results suggest that the availability of proximate and
high-scoring school alternatives is an important factor in determining the degree to which the
NCLB choice component can be successful at both allowing access through choice to alternative

25

schools and increasing academic outcomes for the students exercising choice. School districts
with large geographic densities of low-performing schools may therefore face more difficulty in
experiencing positive effects on student choice and subsequent outcomes as a result of the NCLB
school choice provision.

26

REFERENCES
Carnoy, Martin, and Susanna Loeb. (2002). Does external accountability affect student
outcomes? A cross-state analysis. Educational Evaluation and Policy Analysis, 24(4),
305-331.
Chakrabarti, Rajashri. (2005). Do public schools facing vouchers behave strategically? Evidence
from Florida. Manuscript, Harvard University.
Clark, Melissa A. (2003). Education reform, redistribution, and student achievement: Evidence
from the Kentucky Education Reform Act. Princeton, NJ: Mathematica Policy Research,
Inc.
Cullen, Julie Berry, Brian A. Jacob, and Steven Levitt. (2006). The effect of school choice on
participants: Evidence from randomized lotteries. Econometrica, 74(5), 1991-1230.
Cullen, Julie Berry, and Randall Reback. (2006). Tinkering toward accolades: School gaming
under a performance accountability system. NBER Working Paper No. 12286.
Figlio, David N., and Cecilia Elena Rouse. (2006). Do accountability and voucher threats
improve low-performing schools? Journal of Public Economics, 90(1-2), 239-255.
Figlio, David N., and Lawrence S. Getzler. (2002). Accountability, ability and disability:
Gaming the system. NBER Working Paper No. 9307.
Greene, Jay P. (2001). An evaluation of the Florida A-Plus accountability and school choice
program. New York: Manhattan Institute for Policy Research.
Greene, Jay P., Paul E. Peterson, and Jiangtao Du. (1997). The effectiveness of school choice:
The Milwaukee experiment. Cambridge, MA: Program on Education Policy and
Governance, Harvard University.
Hanushek, Eric A., and Margaret E. Raymond. (2004). The effect of school accountability
systems on the level and distribution of student achievement. Journal of the European
Economic Association, 2(2-3), 406-415.
Hanushek, Eric A., and Margaret E. Raymond. (2005). Does school accountability lead to
improved student performance? Journal of Policy Analysis and Management, 24(2), 297327.
Hastings, Justine S., Richard Van Weelden, and Jeffrey M. Weinstein. (2007). Preferences,
information, and parental choice behavior in public school choice. Manuscript, Yale
University.

27

Hastings, Justine S., Thomas J. Kane, and Douglas O. Staiger. (2006a). Parental preferences and
school competition: Evidence from a public school choice program. NBER Working
Paper No. 11805. May 2006 version available at:
www.econ.yale.edu/~jh529/Hastings_Kane_Staiger_QualityCompetition.pdf.
Hastings, Justine S., Thomas J. Kane, and Douglas O. Staiger. (2006b). Preferences and
heterogeneous treatment effects in a public school choice lottery. NBER Working Paper
No. 12145. June 2006 version available at:
www.econ.yale.edu/~jh529/Hastings_Kane_Staiger_Preferences&SchoolChoice.pdf.
Jacob, Brian A. (2005). Accountability, incentives and behavior: The impact of high-stakes
testing in the Chicago Public Schools. Journal of Public Economics, 89(5-6), 761-796.
Krueger, Alan B., and Pei Zhu. (2004). Another look at the New York City school voucher
experiment. American Behavioral Scientist, 47(5), 658-698.
Mayer, Daniel P., Paul E. Peterson, David E. Myers, Christina Clark Tuttle, and William G.
Howell. (2002). School choice in New York City after three years: An evaluation of the
School Choice Scholarships Program. Princeton, NJ: Mathematica Policy Research, Inc.
North Carolina State Board of Education. (2005). Consolidated state application accountability
workbook. Available at
http://www.ncpublicschools.org/docs/nclb/federal/consolidated/20050630workbook.pdf.
Peterson, Paul. E., David Myers, and William G. Howell. (1998). An evaluation of the New York
City School Choice Scholarship Program: The first year. Cambridge, MA: Mathematica
Policy Research and the Harvard Program on Education Policy and Governance.
Rouse, Cecilia Elena. (1998). Private school vouchers and student achievement: An evaluation of
the Milwaukee Parental Choice Program. Quarterly Journal of Economics, 113(2), 553602.
U.S. Census Bureau. Census 2000, Summary File 3; using American FactFinder. Retrieved
January 10, 2005 from http://factfinder.census.gov.
U.S. Department of Education. (2004). Public school choice. Available at
http://www.ed.gov/policy/elsec/guid/schoolchoiceguid.pdf.
West, Martin R., and Paul E. Peterson. (2006). The efficacy of choice threats within school
accountability systems: Results from legislatively induced experiments. Economic
Journal, 116(510), C46-C62.
Witte, John F. (1997). School choice and student performance in public schools. In Paul
Thurston (Ed.), Advances in Educational Administration. Raleigh, NC: JAI Press.

28

Witte, John F., Troy D. Sterr, and Christopher A. Thorn. (1995). Fifth-year report: Milwaukee
Parental Choice Program. Madison, Wisconsin: University of Wisconsin.

29

Table I: Summary Statistics for NCLB and Non-NCLB Schools
All
Schools
(N = 117)

Non-NCLB
Schools
(N = 101)

NCLB
Schools
(N = 16)

-0.069
(0.457)

0.026
(0.414)

-0.672
(0.163)

Has Academic Magnet

0.350
(0.479)

0.356
(0.481)

0.313
(0.479)

% Lunch Recipients

52.977
(28.089)

47.261
(25.853)

89.061
(6.060)

% Black

46.547
(24.666)

41.645
(22.367)

77.489
(13.670)

Average Income†

52,334
(17,330)

55,551
(16,361)

32,022
(5,307)

% Capacity

104.470
(15.612)

105.700
(15.078)

96.699
(17.164)

% Students With at Least One Suspension

13.772
(12.690)

11.443
(10.232)

28.476
(16.740)

% AYP Requirements Met

95.841
(7.888)

98.003
(3.798)

82.194
(12.444)

708.145
(294.770)

738.228
(299.504)

518.250
(171.373)

Variable
School characteristics
Average Test Score

Number of Students

Notes: Data are from the 2003-2004 school year. Standard deviations are in parentheses. All Schools and
Non-NCLB Schools do not include high schools or alternative schools since every NCLB school is an
elementary or middle school.
†
Income for a student is calculated as the median block-group household income level from the 2000 Census
for households of a student’s own race for the block group that the student lives in. We average this over
students at a school to get school average income.

30

Table II: Cross Tabulation of Parents Who Chose Non-NCLB Schools in Spring and July

Did Not Choose NCLB
School First in Spring

Did Not Choose NCLB
School First in July
227
(30.31%)

Chose NCLB School
First in July
522
(69.69%)

Total
749
(100%)

Chose NCLB School
First in Spring

865
(14.55%)

5,081
(85.45%)

5,946
(100%)

Total

1,092
(16.31%)

5,603
(83.69%)

6,695
(100%)

Note: Numbers in parentheses give the relative frequency of each cell in its row.

Table III: Regression Adjusted Treatment Effect
Variable
Chose NCLB School First

Spring Mean
0.888

July Mean
0.837

Treatment Effect
-0.051***
(0.006)

Average Test Score at First-Choice School

-0.580

-0.533

0.047***
(0.004)

% Black at First-Choice School

77.069

74.256

-2.814***
(0.204)

% Lunch Recipients at First-Choice School

83.863

80.279

-3.584***
(0.241)

Number of Observations

6,695

6,695

13,990

Notes: Data are from the 2003-2004 school year. Treatment Effect reports the coefficient on an indicator for July 2004 from
separate regressions with each variable in the first column as the dependent variable, controlling for student fixed effects.
Standard errors adjust for clustering at the student level. Asterisks indicate significance (*=.10, **=.05, ***=.01).

31

Table IV: Differences in Choice Characteristics for Students Who Chose Out in July
Variable
Average Test Score at First-Choice School
Spring 2004
July 2004
Average Test Score of Schools Within 5 Miles
Spring 2004
July 2004
% Black at First-Choice School
Spring 2004
July 2004
% Lunch Recipients at First-Choice School
Spring 2004
July 2004
Number of Students

All Students

Black

Not Black

-0.502
-0.017

-0.513
-0.034

-0.421
0.108

-0.322
-0.247

-0.328
-0.253

-0.277
-0.206

75.557
49.351

76.588
50.278

67.857
42.426

78.565
43.350

79.128
43.867

74.358
39.495

1,092

963

129

Note: Data are from the 2003-2004 school year.

32

Table V: Comparison of Student Characteristics: Chose Out Versus Chose NCLB School
Did Not Choose
NCLB School
First in July
0.208

Chose NCLB
School First in
July
0.093

0.882

0.832

Lunch Recipient

0.879

0.861

Female

0.493

0.498

Test Score

-0.554

-0.571

Income

32,134

32,043

Number of Unexcused Absences

5.168

5.760

Number of Suspensions

2.896

2.162

Rising Grader

0.316

0.228

In Magnet Program

0.154

0.115

Distance to NCLB School

3.497

2.859

Average Distance to Schools
Within 5 Miles in July Choice Set
Average Test Score at NCLB
School
Average Test Score of Schools
Within 5 Miles in July Choice Set

3.318

3.313

-0.632

-0.633

-0.247

-0.266

1,092

5,603

Variable
Did Not Choose NCLB School
First in Spring
Black

Number of Students†

Adjusted
Difference
0.105***
(0.018)
0.045*
(0.025)
0.019
(0.014)
-0.006
(0.012)
0.031
(0.021)
278.633
(426.203)
-0.620**
(0.240)
0.313**
(0.137)
0.064***
(0.019)
0.018*
(0.009)
0.405***
(0.119)
0.014
(0.014)
--0.017**
(0.008)
6,695

Notes: Data are from the 2003-2004 school year. Adjusted difference reports the coefficient on whether the
student did not choose her NCLB school first in July from separate regressions with each variable in the first
column as the dependent variable, controlling for NCLB school fixed effects. Standard errors adjust for clustering
at the level of the NCLB school. Asterisks indicate significance (*=.10, **=.05, ***=.01).
†
Student test score is the average of reading and math scores on the North Carolina End of Grade exams. Only
students in grades three through eight take these exams, so this variable is based on 818 students who did not
choose their NCLB school first in July and 3,828 who chose their NCLB school first in July.

33

Table VI: Conditional Logit of Probability a Family Chose Out in July
Coefficient

Standard
Error

P-value

0.759***

0.124

0.000

Black

0.448*

0.248

0.071

Lunch Recipient

0.131

0.162

0.419

Female

-0.031

0.071

0.660

Test Score

0.106**

0.049

0.030

Demeaned Income

-0.002

0.002

0.383

Number of Unexcused Absences

-0.019***

0.007

0.006

Number of Suspensions

0.014***

0.004

0.000

Rising Grader

0.203**

0.087

0.020

In Magnet Program

0.142**

0.062

0.022

Distance to NCLB School

0.019

0.012

0.120

Average Distance to Schools
Within 5 Miles in July Choice Set

0.009

0.029

0.748

Average Test Score of Schools
Within 5 Miles in July Choice Set

0.608***

0.208

0.003

Variable
Did Not Choose NCLB School First
in Spring

Mean of Dependent Variable
Number of Students
Log Pseudolikelihood

0.176
4,646
-2,029.688

Notes: Data are from the 2003-2004 school year. Conditional (fixed-effects) logit estimation with NCLB
school fixed effects. Standard errors adjust for clustering at the level of the NCLB school. Asterisks
indicate significance (*=.10, **=.05, ***=.01).

34

Table VII: Determinants of Test Score at Chosen School in July
Dependent Variable: Test Score
Standard
at July First-Choice School
Coefficient
P-value
Error
Did Not Choose NCLB School First
in Spring

0.103***

0.021

0.000

Black

-0.082**

0.028

0.011

Lunch Recipient

-0.069***

0.022

0.006

Female

-0.027

0.017

0.136

Test Score

0.025

0.019

0.208

Demeaned Income

-0.001

0.001

0.439

Number of Unexcused Absences

-0.003*

0.002

0.067

Number of Suspensions

0.001

0.001

0.376

Rising Grader

-0.052

0.038

0.189

In Magnet Program

0.077***

0.016

0.000

Distance to NCLB School

-0.015**

0.005

0.016

-0.010

0.014

0.468

Average Test Score of Schools
Within 5 Miles in July Choice Set

0.229***

0.052

0.001

Mean of Dependent Variable
Number of Students
Adjusted R-squared

0.019
818
0.1354

Average Distance to Schools Within
5 Miles in July Choice Set

Notes: Data are from the 2003-2004 school year. Linear regression estimation with NCLB school fixed
effects. Standard errors adjust for clustering at the level of the NCLB school. Asterisks indicate
significance (*=.10, **=.05, ***=.01).

35

Table VIII: Baseline Characteristics by Lottery Winners and Lottery Losers
Baseline Characteristic
In Priority Group for First
Choice

Won
Lottery
0.713

Method 1 Sample
Lost
Adjusted
Lottery
Difference
0.804
-0.079
(0.073)

Won
Lottery
--

Method 2 Sample
Lost
Adjusted
Lottery
Difference
----

Black

0.878

0.875

0.049
(0.065)

0.902

0.871

0.105**
(0.049)

Lunch Recipient

0.965

0.973

-0.013
(0.013)

0.925

0.853

0.181***
(0.041)

Female

0.461

0.518

0.015
(0.084)

0.481

0.526

-0.067
(0.078)

Income

30,207

28,658

1,892.770
(2,029.906)

30,846

31,351

-3,961.591*
(2,055.373)

Number of Unexcused
Absences

5.009

5.875

-1.634
(1.140)

5.762

4.920

0.371
(0.680)

Number of Suspensions

3.626

3.732

0.863
(1.302)

4.743

3.540

2.732**
(1.037)

Test Score

-0.796

-0.886

-0.066
(0.061)

-0.874

-0.366

-0.990***
(0.056)

115

112

227

214

348

562

Number of Students†

Notes: Adjusted Difference reports the coefficient on whether the student won the lottery from separate regressions with each variable in the
first column as the dependent variable. Lottery-block fixed effects are included for specifications using Method 1 Sample, and grade and firstchoice combination fixed effects are included for specifications using Method 2 Sample. Standard errors adjust for clustering at the level of the
fixed effects. Asterisks indicate significance (*=.10, **=.05, ***=.01).
†
Test score is the average of reading and math scores on the North Carolina End of Grade exams. Only students in grades three through eight
take these exams, so this variable is based on 89 students who won the lottery and 91 students who lost the lottery in the Method 1 Sample, and
182 students who won the lottery and 317 students who lost the lottery in the Method 2 Sample.

36

Table IX: The Impact of Winning Versus Losing the Lottery on Attrition

Not Present in CMS at Beginning
or End of 2004-2005 School Year
Number of Students

Method 1 Sample
Regression Adjusted
Difference:
Lottery Winners
Versus Lottery
Losers
Mean
0.062
0.057*
(0.032)
227

227

Method 2 Sample
Regression Adjusted
Difference:
Lottery Winners
Versus Lottery
Losers
Mean
0.050
0.012
(0.016)
562

562

Notes: Regression Adjusted Difference is from a regression of an indicator of attrition on whether the student won the lottery,
controlling for the following baseline covariates: black, lunch recipient, female, demeaned income, an indicator for greater
than or equal to five unexcused absences, an indicator for greater than or equal to one unexcused absence, an indicator for
greater than or equal to five suspensions, an indicator for greater than or equal to one suspension, total number of unexcused
absences, and total number of suspensions. Lottery-block fixed effects are included for specifications using Method 1 Sample,
and grade and first-choice combination fixed effects are included for specifications using Method 2 Sample. Standard errors
adjust for clustering at the level of the fixed effects. Asterisks indicate significance (*=.10, **=.05, ***=.01).

37

Table X: The Impact of Winning Versus Losing the Lottery on Outcomes
Method 1 Sample
Above-Median
All
Score
Students
Difference
0.324***
0.375***
(0.080)
(0.101)

Below-Median
Score
Difference
0.166
(0.100)

Mean
-0.284

Method 2 Sample
Above-Median
All
Score
Students
Difference
0.146*
0.256***
(0.079)
(0.088)

Below-Median
Score
Difference
-0.032
(0.056)

2004-2005 School Year Outcome
Average Test Score at School
Attended

Mean
-0.240

≥ 5 Unexcused Absences

0.427

-0.154
(0.108)

-0.205
(0.123)

-0.101
(0.179)

0.410

0.020
(0.057)

0.002
(0.076)

0.051
(0.054)

≥ 1 Unexcused Absence

0.845

-0.086
(0.077)

-0.174**
(0.072)

0.051
(0.102)

0.822

-0.001
(0.068)

-0.065
(0.088)

0.126**
(0.049)

≥ 5 Suspensions

0.258

-0.099*
(0.050)

-0.125
(0.097)

-0.060
(0.084)

0.288

-0.018
(0.040)

-0.023
(0.056)

-0.018
(0.046)

≥ 1 Suspension

0.451

-0.076
(0.076)

-0.063
(0.127)

-0.078
(0.073)

0.472

0.006
(0.048)

-0.026
(0.065)

0.095
(0.060)

Number of Students
Student’s Test Score

213
-0.805

213
0.046
(0.081)

107
0.169**
(0.076)

106
-0.070
(0.139)

534
-0.565

534
0.114
(0.076)

271
0.189*
(0.104)

263
0.047
(0.076)

Number of Students

161

161

82

79

451

451

213

238

Notes: Each column reports the coefficient on whether the student won the lottery from separate regressions with each variable in the first column as the dependent variable, controlling for the
following baseline covariates: black, lunch recipient, female, demeaned income, an indicator for greater than or equal to five unexcused absences, an indicator for greater than or equal to one
unexcused absence, an indicator for greater than or equal to five suspensions, an indicator for greater than or equal to one suspension, total number of unexcused absences, and total number of
suspensions. The Student’s Test Score specification includes baseline student test score. Lottery-block fixed effects are included for specifications using Method 1 Sample, and grade and firstchoice combination fixed effects are included for specifications using Method 2 Sample. Standard errors adjust for clustering at the level of the fixed effects. Asterisks indicate significance
(*=.10, **=.05, ***=.01).

38

0

.5

Density

1

1.5

Figure I: Kernel Density Estimate of Difference in Test Score Between July First-Choice
School and NCLB School for Families Who Did Not Choose NCLB School First in July
(N = 1,092)

-.5
0
.5
1
1.5
2
Difference in Test Score Between July First-Choice School and NCLB School
Kernel density estimate
Normal density
Note: We use the Epanechnikov kernel and the optimal width as computed by default in Stata.

39

