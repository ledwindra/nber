NBER WORKING PAPER SERIES

RACIAL DIFFERENCES IN INEQUALITY AVERSION:
EVIDENCE FROM REAL WORLD RESPONDENTS IN THE ULTIMATUM GAME
John D. Griffin
David Nickerson
Abigail K. Wozniak
Working Paper 17097
http://www.nber.org/papers/w17097

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
May 2011

We thank Bill Evans, Dan Hungerman, Lars Lefgren, Sandra Black, and seminar participants at Princeton
University, Vanderbilt University, and the University of Notre Dame for helpful comments. Wozniak
thanks the Industrial Relations Section at the Princeton Economics Department for financial support
during the course of this project. All errors are our own. The views expressed herein are those of the
author and do not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2011 by John D. Griffin, David Nickerson, and Abigail K. Wozniak. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.

Racial Differences in Inequality Aversion: Evidence from Real World Respondents in the
Ultimatum Game
John D. Griffin, David Nickerson, and Abigail K. Wozniak
NBER Working Paper No. 17097
May 2011
JEL No. J15
ABSTRACT
The distinct historical and cultural experiences of American blacks and whites may influence whether
members of those groups perceive a particular exchange as fair. We investigate racial differences
in fairness standards using preferences for equal treatment in the ultimatum game, where responders
choose to allow a proposed division of a monetary amount or to block it. Although previous research
has studied group differences in the ultimatum game, no study has been able to examine these across
races in America. We use a sample of over 1600 blacks and whites drawn from the universe of registered
voters in three states and merged with information on neighborhood income and racial composition.
We experimentally vary proposed divisions as well as the implied race of the ultimatum game proposer.
We find no overall racial differences in acceptance rates or aversion to unequal divisions. However,
we uncover racial differences in the response to pecuniary returns conditional on inequality of the
division. This is driven by the lowest income group in our sample, which represents the 10th percentile
of the black income distribution. The racial differences are robust across gender and age groups. We
also find that blacks are more sensitive to unfair proposals from other blacks.

John D. Griffin
Department of Political Science
University of Notre Dame
South Bend, IN 46556
John.Griffin@nd.edu
David Nickerson
Department of Economics
University of Notre Dame
441 Flanner Hall
Notre Dame, IN 46556

Abigail K. Wozniak
Department of Economics
University of Notre Dame
441 Flanner Hall
South Bend, IN 46556
and NBER
a_wozniak@nd.edu

I. Introduction
The relationship between whites and African Americans in the United States has included many
exchanges in which one group (virtually always African Americans) has been treated unfairly or even
unjustly by the other. It is therefore possible that African Americans’ historical and ongoing experiences
with discrimination may make them more alert to the possibility of exploitation and more sensitive to
deviations from fair treatment (Dawson 1995). In addition to this legacy of lopsided exposure to unfairness,
the distinct cultural norms of American blacks and whites may dictate different standards of fair treatment
across the two groups. In light of these differences, it is reasonable to ask whether blacks and whites
respond differently in settings where fairness is a consideration. We use a large sample of blacks and whites
drawn from a broad swath of the socio-economic distribution to examine this question. We focus on equal
treatment as our standard for fairness and examine preferences for equal treatment – also called inequality
aversion – in the classic setting of the ultimatum game.
The importance of fairness considerations in determining behavior in one-on-one and group
interactions is now widely recognized by economists. Inequality aversion specifically has been shown to
operate in a variety of important contexts, including bargaining in product and labor markets (Babcock and
Laschever, 2003), decisions over redistribution (Luttmer 2001), bequests and charitable giving (Fong and
Luttmer 2009), and political support for regulation (Lu, Scheve, and Slaughter, 2010). Examining racial
differences in inequality aversion is important because it can help us understand whether different
preferences for equal treatment might partially explain racial differences in behavior in these settings. For
example, Bartels (2008, 133) finds that blacks espouse more egalitarian attitudes on a number of dimensions.
Moreover, behavior in these settings may in turn influence relative outcomes. For example, fairness
concerns may affect the response to wage offers, which may in turn affect relative wages. Members of
groups with strong inequality aversion may be unwilling to accept very low relative wages in the labor
market (even if those wages reflect underlying human capital differences) leading to lower relative

3

employment rates. Similarly, group preferences for equality can influence the features of social
redistribution programs through the election process.
In this paper, we report the results of an experiment in which we use the ultimatum game (UG) to
test for racial differences in inequality aversion between whites and African Americans in the United States.
Preferences for inequality aversion are often studied using the UG. In the game, one participant divides a
monetary award into two parts and a second participant either accepts or rejects their offered share of the
division. Rejections of non-zero offers are thought to stem from preferences related to fairness, either from
a preference for equal splits, also called inequality aversion (Bolton and Ockenfels 2000; Oosterbeek et al.
2004), or from a preference for punishing proposers who act selfishly (Falk, Fehr, and Fischbacher 2005).
Our experimental approach enables us to isolate racial differences in responses to unequal treatment and
allows us to answer several questions. First, does a minority group that has historically experienced a high
level of discrimination and unfair treatment respond differently in the ultimatum game? Second, does this
depend on neighborhood income levels or racial integration that respondents experience? Finally, does an
unfair proposal elicit different responses among blacks when the proposer is black as opposed to white?
We conduct a survey experiment in which respondents are invited to play the UG by telephone.1
Participants are selected from the universe of registered voters in three states with large black populations.
The states we employ all record registrant race in the official state voter file, which also contains the
registrant’s home address and telephone number. Using the address data, we merge the voter file
information with neighborhood level (Census block group) characteristics from the 2000 U.S. Census. We
then draw a random sample stratified on race, block-level median income, and neighborhood racial diversity
from the merged data. Balancing our sample on these dimensions allows us to compare the game behavior
of comparable whites and African Americans, as well as to make comparisons along other socio-economic
lines. This is particularly important in our setting because preferences for equality have been shown to be

1

Others have studied inequality aversion using survey data (Atkinson 1970).

4

related to income, a characteristic which varies substantially across blacks and whites (Alesina and LaFerrara,
2005). Our final sample contains 1647 respondents.
In our study, all participants play the role of Responder. We focus on the Responder’s decision to
accept or reject the offer due to our interest in fairness preferences and inequality aversion in particular.2
Survey Respondents are told that they have been selected to play a game with a proposer (who is
hypothetical) who has made an offer to divide a given stakes amount. The stakes to be divided, the amount
of the offer, and the name of the “Proposer” are all randomly assigned to respondents. We use commonlyoccurring but racially distinct names among white and African American males to imply the race of the
Proposer. Our decision to use a research design with this type of deception may raise concerns among
experimental economists, and it was not taken lightly. We discuss and defend this decision when we explain
the treatment in detail below.
We find that African Americans and whites have similar levels of aversion to unequal treatment
defined as their response to a given share allocated by our hypothetical proposers. Two differences in the
groups are apparent, however. First, African Americans but not whites are more likely to accept larger
offers, after controlling for the share of the total sum being offered. This difference is confined to those
with very low incomes. Second, the implied race of the proposer matters. African Americans facing
proposers who are implied to be African American increase their acceptance rates to higher offered shares
more rapidly than do African Americans facing proposers who are implied to be white. Conversely, blacks
facing black proposers decrease their acceptance rates faster as the share falls when proposers are implied to
be black.
Our paper makes three contributions. First, we add an important new dimension to the literature on
group differences in economic decision-making. We provide the first analysis of racial differences in fairness
preferences in a representative U.S. population. Over the last decade economists have used laboratory

Non-trivial offers by the first player have been attributed either to strategic or altruistic behavior (Kravitz and Gunto 1992).
Altruism is somewhat different from fairness, and we believe the behavior of the second player more directly reflects fairness
preferences (Oosterbeek et al 2004.)
2

5

experiments to uncover striking and potentially important differences across groups in how they behave in a
variety of experimental settings. Differences along lines of gender and ethnicity (or nationality) in particular
have received considerable attention. Examples from this literature are many (On gender: Bolton and
Katok, 1995; Eckel and Grossman 1998; Solnick 2001; Andreoni and Vesterlund, 2001; Gneezy, Niederle,
and Rustichini 2003; Niederle and Vesterlund, 2007. On nationality/ethnicity: Roth et al 1991; Henrich
2000; Chuah et al 2007; Ferraro and Cummings 2007; Ferhstman and Gneezy 2001; Chen and Tang 2009). 3
However, the study of racial differences in economic decision-making has so far been quite limited. Two
important exceptions are Benjamin, Choi and Strickland (2009), who examine the effects of race priming on
subjects’ expressed preferences for risk aversion in a university-based sample, and Fong and Luttmer (2009)
who use a representative sample to analyze racial differences in charitable giving. Ayers and Siegelman
(1995) find that the racial composition of bargaining pairs affects bargaining behavior. The only racial
comparison of UG behavior we have been able to locate is ancillary to what is otherwise a comparison of
gender in UG play (Eckel and Grossman 2001)4.
Our second contribution is to implement a sampling methodology that allows us to answer these
questions outside the laboratory and in the field. We introduce a new (to economics) technique for
obtaining representative samples of experimental subjects—the use of voter file demographic and contact
information combined with Census block group data on neighborhood characteristics. Using voter
registration files as a sampling frame has been used outside economics to construct representative samples,
most widely in health surveys (Adimora et al 2001 for example) but also by political scientists (Gerber and
Green 2000; Nickerson 2008; Gerber, Karlan and Bergan 2009) and pollsters.5
Our sampling methodology has several advantages over others in the experimental economics
literature. First, we are able to construct a data set that is much more representative of the general
population than is typically found in laboratory-based experimental studies. Studies of social preferences
There are also some studies of age differences (Murnighan and Saxon 2001).
Eckel and Grossman (1998) briefly note that large differences were detected in how students at a predominantly black college
played the UG compared to the students from a predominantly white college.
5 See the discussion on the Field Poll’s website: http://www.field.com/fieldpoll/methods.html.
3
4

6

(albeit not the UG) that have compared the behavior of students and non-students have identified
significant differences between the groups (Gordon, Slade, and Schmidt 1986; Fehr and List 2004;
Carpenter, Connolly, and Myers 2008). As one study put it, “problems exist in replicating with nonstudent
subjects behavioral phenomena observed in student samples” (Gordon et al., 1986). Importantly, our
sample contains large numbers of respondents from different parts of the black income distribution. Onethird of our respondents come from Census block groups where the median household income is at the
tenth percentile of the black income distribution. To our knowledge, no other experimental study in a
developed country has such a substantial representation of the poor. Second, respondents to our survey did
not volunteer in response to an advertisement for experiment participants, a form of sample self-selection
that can also detract from representativeness (Doty and Silverthorne, 1975).6 Finally, our approach allows us
to collect detailed information on a respondent’s neighborhood, something that is typically missing from
databases of voluntary, non-student participants and which cannot be reliably collected in a survey format.7
The neighborhood level information also allows us to balance our sample according to the types of
environments respondents live in, something that is not possible with other data sets. If neighborhoods are
important determinants or correlates of preferences, then this type of neighborhood-level information can
provide both a crucial set of controls and a useful dimension for sampling.
A final contribution is that our experimental design allows us to consider different notions of
fairness. Specifically, we expand the model developed in Fehr and Schmidt (1999) to incorporate two types
of inequality, which we call relative and absolute. The Fehr and Schmidt model concerns what we call
absolute inequality, where disparities in the amounts paid to the two players generate disutility. In our
model, relative inequality arises when the share of the division deviates from a 50-50 split. Our experiment
varies offer size and stakes randomly across players, so we can determine whether relative and absolute

Two recent papers also examine the issue of selection via volunteering to enter a survey pool or experiment. Abraham et al
(2009) studies selection into the American Time Use Survey from the Current Population Survey sample, and Falk et al (2011)
study selection into lab experiments from college student populations. Interestingly, they come to opposing conclusions about the
severity of selection.
7 Examples of these databases include Knowledge Networks and Time Sharing Experiments in the Social Sciences (TESS).
6

7

inequality play separate roles in the acceptance decision. This is not an entirely new question, but to our
knowledge this is the first time competing measures of inequality have been tested outside the laboratory
using techniques that allow us to experimentally vary both the amount to be divided and the offer size.
Oosterbeek et al (2004) consider a similar question using meta-analysis, and a seminal paper on the
ultimatum game, Guth et al. (1982) examines behavior across a range of stakes sizes.
A shortcoming of our study is that we can say little about the relationship between our subjects’
behavior in the UG and in real world settings. The potential for this connection is a major motivation for
our study. However, because the feasibility of conducting an experiment by phone with our sample was
previously untested, we elected to keep the experiment extremely simple. We therefore omit more
complicated features that would allow us to directly link our findings to racial differences in behavior in
other settings.

II. Choice Model
The participants in our study are faced with a choice problem. Specifically, they are offered the
possibility of absolute financial gain accompanied by possibly unequal gains accruing to the (hypothetical)
proposer. Fehr and Schmidt (1999; hereafter FS) model the utility maximization problem faced by
individuals in two-person decision games where player i must decide whether to accept player j’s offer as
follows:
(1)

,0

,0

If i accepts j’s offer, she receives xi but incurs a utility penalty if xi  x j . FS call this inequality aversion,
and their model allows for the utility penalties to differ according to whether i receives more or less than j in
the proposed split. The FS formulation also assumes that the utility penalty is a linear function of the
absolute difference between what i and j receive.

8

The choice model we estimate differs from FS in two key ways. First, since there are no instances of
experimental splits where the respondent is offered more than an even split, we do not model asymmetric
utility losses to inequality that favors the respondent. We do not see this as a limitation of our study, since
splits favoring the respondent rarely occur in experimental UG play.8
Second, since we experimentally vary both the stakes and the offer among our respondents, we can
examine the utility penalty to relative inequality in addition to absolute inequality. We embed straightforward
concepts of relative and absolute inequality into a simple utility function that is a direct extension of FS.
We assume that an accepted offer in game
2

(2)

generates the following utility for our respondents:

0.5

The vector g contains two elements with information on the game variables:
offer;
stakes

, the stakes or total to be split between i and j with i receiving
represents.

and

are subject to the constraint that

the size of the proposed

. Define

to be the share of the
. Following FS, the first

term in (2) is simply the utility gained from the offer, while the second term represents the penalty from
receiving an unequal distribution of funds. The third term reflects a utility penalty due to relative inequality in
the split. We express this as deviation from a focal point of 0.5, which represents an even split.9
The individual’s choice problem is then to accept or reject in order to maximize utility. 10 As
modeled, respondents accept an offer if the utility of the financial gain exceeds the penalties associated with
the two inequality dimensions.

8 UG offers are usually center around 40% of the stakes, and on average 16% of non-zero offers are rejected, typically when they
fall below 20% of the total sum. See Roth (1995) and Camerer (2003) for reviews of the literature on the ultimatum game. For a
meta-analysis of 75 UG experiments in 37 studies see Oosterbeek, Sloof, and Kuilen (2004).
9 The terms in this function can be collected and rearranged. (2) is therefore not a unique way of writing the sum. However, we
postulate that the included quantities are the relevant determinants of utility. Written in this way, there is a direct path to
multinomial logit estimation of the parameters in (2) given the exogenous variation induced by our experiment.
10 Alternatively, we could have outlined a model following Bolton and Ockenfels (2000). Their ERC model postulates two
elements in the utility function: pecuniary return (equivalent to our offer) and the respondent’s relative share of the payoff. We
could construct the latter from our offer and stakes variables and substitute those for the second and third terms in (2). While this
model would be equally interesting, it would not likely alter the substance of our results, since the most stable racial differences we
estimate pertain to how changes in the offer size affect behavior, not offer relative to other variables. The individual’s choice

9

Individuals choose
(3)

0,1 to maximize u subject to some error:

;

We assume individuals choose optimally given
distributed error, so that Pr

1|

, their individual characteristics X, and an extreme-value
1;

0;

|

.

We can therefore estimate the following in a standard logit framework:
2

(4)
where

is the unobserved latent variable driving the observed choice

, and

is extreme value

distributed. Control terms for individual characteristics are omitted for clarity. If we also assume that
, , , then we can estimate the above equation using standard logit.
One of our main questions of interest is whether blacks and whites respond differently when faced with
various combinations of offer, absolute inequality, and relative inequality. Therefore we would also like to
estimate the following:
(5)

The estimate of

will tell us whether blacks are more likely than whites generally to accept an offer in the

ultimatum game. The interaction coefficients will tell us whether specific game components affect black
decisions differently from whites.
The standard logit approach has several advantages. First, we can readily test whether the game
components, on average, affect the choices of blacks and whites statistically differently. Second, the
likelihood function is computationally simple to optimize.

problem remains the same, and estimation of the utility function parameters via multinomial logit would still go through in this
case.

10

A drawback to the standard logit approach is that it assumes that the error term is uncorrelated with
the control variables listed above, at least conditional on the inclusion of other individual covariates. This is
, , in (5). This assumption can fail if there is individual

equivalent to the assumption that

level heterogeneity in the utility function parameters. In that case, we would prefer to estimate the following:
(6)
This is possible using the mixed logit approach developed in Revelt and Train (1998) and implemented in
Cappelen et al (2007), Barros et al (2005), and Detang-Dessendre et al (2008). The mixed logit approach
allows us to estimate the distribution of

in our experimental sample after assuming that the underlying

individual parameters are drawn from a specific family of distributions.
Specifically, we can estimate the following:
(7)
where the

term represents unobserved heterogeneity that depends on included covariates.

The mixed logit approach is appealing in the choice setting because it relaxes the assumption that the
relationship between the game components and acceptance choices is identical across individuals up to a
location parameter. It also relaxes the common Independence of Irrelevant Alternatives (IIA) assumption,
but that is less of a concern here since our participants only face two alternatives.
The main drawback to the mixed logit approach is that the likelihood function is computationally
difficult to optimize. Solutions require use of simulation methods to approximate the integrals over the
individual utility functions. Also, statistical testing for differences between estimated parameter distributions
in the mixed logit framework is not well-developed. Therefore it is not an ideal model for testing one of our
central questions of interest – whether blacks and whites exhibit different degrees of inequality aversion.
Finally, it is not clear that unobserved heterogeneity is even a major concern in our setting, where
the treatment is randomly assigned to participants instead of respondents selecting into the choice setting
11

endogenously. For all these reasons, we rely on the standard logit as our main empirical specification. We
present a limited set of mixed logit estimates as a robustness check. Ultimately, we view our empirical
analysis as fitting best in the category of experiments that test a single model, in the framework of Card,
Della Vigna, and Malmendier (forthcoming). We test whether the nominal amount of an offer, as well as its
relative inequality features, have differential impacts across blacks and whites. We do not test the Bolton and
Ockenfels (2000) framework against Fehr and Schmidt (1999) – which would be equivalent to Card et al’s
competing models experimental analyses, nor do we directly estimate structural parameters in our main
specifications.

III. Experimental Design
The experiment consisted of calling subjects on the telephone and inviting them to play an
ultimatum game. The rules were explained and the name and home city of a hypothetical proposer was
provided to the subject. The offer was presented to the subject, who could then accept or reject the offer.
The subject was then asked to identify the smallest offer the subject would have accepted. The call ended by
confirming the contact information of the subject.
The treatment was administered by Eastern Research Services, a professional survey firm. Calls were
completed between December 10, 2008, and January 26, 2009, and monitored remotely (via telephone) by
the authors and directly by supervisors in the call centers.11 The callers were carefully trained regarding the
rules of the game and the nature of the research.12 In particular, the callers were trained to read the rules of
the game slowly and answer questions about the game.13 The script used during the course of calling is
Nearly all of the calls took place in the evening. Calls were placed during working hours on weekends and a handful of other
days in order to increase response rates. Numbers with no answer initially were also retried, and in some specifications we control
for the number of times a number was called (we find this makes no difference to our results).
12 Given the generally positive response from subjects contacted by Eastern Research Services, the callers reported very high job
satisfaction and found the study a welcome change from consumer research polling. We therefore believe that although this firm
primarily conducts consumer research polling, their skills translated well to academic polling.
13 Two of the authors personally monitored several calls. Our impression was that most subjects understood the rules of the
game, although this may not have always been the case, as we discuss below.. On the one hand, this naïve first encounter with the
ultimatum game is a quantity of interest. On the other hand it would have been interesting to see how respondents played after a
few trial runs.
11

12

included in Appendix A.14 The script asks the subject if she would like to play a game for research
purposes, describes the financial incentives, explains the rules of the game, confirms that the subject
understands the rules before proceeding, introduces the fictitious proposer, informs her of the proposer’s
offer, and records whether she accepts or rejects the offer. The caller then asks the subject for her minimally
acceptable offer and verifies the subject’s contact information. A debriefing letter describing the purpose of
the study and the nature of the deception was mailed to participating subjects the day after the call took
place, along with any winnings from accepting an offer.
The call script varied two factors in the treatment. The first factor varied in the script was the terms
of the ultimatum. It is important to our study that both the stakes of the ultimatum game played and the
offer made by the opponent varied. Most prior work on inequality aversion keeps the stakes of the game
constant and varies the offers, assuming that the absolute differences in the stakes will not materially affect a
subject’s utility so inequality aversion must be driving the results. Our goal was to randomly vary the stakes
of the game from which particular offers were made to directly manipulate the inequality of the offer. For
instance, an offer of $2 could be made in a $5, $10, $20, $50, or $100 game. To ensure that the results were
not idiosyncratic to the amount of the offer, several different dollar values of offers were tested ($1, $2, $5,
and $10). The frequency distribution of the offer and stake combinations used in the game is presented in
Appendix 2. This distribution concentrated subjects into offer/stakes cells where high variance in
acceptance rates was anticipated and away from cells where it was anticipated that acceptance rates would be
very low or high. Since subjects in each stratum were randomly assigned to each cell, the different
probabilities of receiving a particular offer or stakes does not bias the results in the slightest because subjects
were equally likely to be assigned to each of the conditions.
The second factor varied in the script was the implied race of the proposer. To the extent that the
names are associated with a particular race, we can manipulate a subject’s perception of their opponent’s

To facilitate ease of contact the sample was limited to households containing three or fewer registered voters (excluding roughly
1% of the sample).

14

13

race. As explained above, differences between the races in the probability of accepting a given offer may
depend on the race of the proposer. In the laboratory, race can be manipulated by purposefully pairing
subjects or using confederates. This visual manipulation is not possible in a telephone-based study, so we
instead rely on racially polarized names. Appendix 3 provides details of the name construction as well as
evidence that the selected names indeed connoted different races. After agreeing to play the game, the
subject was told the full name of the fictitious proposer. The proposer’s first name was then repeated four
more times over the course of the script (see Appendix 1). Upon the final statement of the proposer’s name,
the amount each player would receive from the proposed split was explicitly stated in order to make the
connection between the proposer’s name and the inequality of the split very direct.
We want to give particular attention to one aspect of our experimental design because it often raises
questions among experimental economists. Our design deceives respondents in telling them that they are
paired with a proposer who in fact is hypothetical. While we go to no particular lengths to convince
respondents that the proposer is a real person, this is a likely inference on their part.15 In fact, the design
relies on respondents making inferences about the race and gender of the proposer. This type of deception
is typically eschewed by experimental economists, largely although not exclusively on the grounds that it can
contaminate subject pools by making potential participants in future studies question the truthfulness of
experimental instructions.16 Nevertheless, a number of studies have deceived subjects to some degree
(Bertrand and Mullainathan, 2004; audit studies, as summarized in Fix and Turner 1998; Karlan and Zinman
2009). Not coincidentally, these studies all take place in the field, where it sometimes becomes prohibitive to
introduce the kind of control that makes it easier to avoid deception in the laboratory.
The objectives of our experiment posed a considerable challenge to developing a non- deceptive
design. Because we wanted to reach respondents who rarely appear in laboratory experiments, we needed to
15 Blount (1995) points out that behavior in the ultimatum game is strongly influenced by the respondent’s beliefs about whether
(a) the proposer is human and therefore making offers intentionally and (b) whether the proposer benefits from her decision.
16Deception of the form in which the experimental instructions contain a non-truth or in which the likely inference from the
instructions is untrue is typically avoided. Deception in the form of omission is common. For example, experimenters typically
provide very limited information about the purpose of an experiment, often to the point that inferences subjects are expected to
make about the nature of the experiment are very different from the true objectives. Jamison et al (2008) discuss the origins of the
deception prohibition and provide evidence of its effects on the public good of undergraduate subject pools.

14

take the experiment into the field. While several papers investigate ultimatum game behavior in the field,
these studies have taken place in developing countries where lower costs and more common community
gathering places dramatically lower the barriers to bringing subjects together in the field to play the game
(Cameron 1999; Henrich 2000; Henrich et al 2001), and as this implies they have used convenience sampling
procedures . The logistical challenges to bringing together the subjects in our sample are formidable but
might have been overcome. One might recruit a group of subjects as proposers and go door-to-door in
sampled neighborhoods or use a design like that in Berger and Pope (2010), where proposals are made in a
first round and then matched by experimenters to responders in a second round. The former design would
either sacrifice the geographic representativeness of our sample or be prohibitively expensive. The latter
design generates a real set of proposals that is manipulated by experimenters to randomize the offers along
certain dimensions across respondents; experimental instructions reflect this. Yet our objectives pose
problems for even this design. First, we wanted to randomize the racial connotations of proposer names in
order to signal differences in race without overtly mentioning race as a dimension of interest. Benjamin et al
(2010) show that race-related framing can have important effects on respondent behavior, and we felt that it
was important to keep explicit mention of race out of the script. If we developed a pool of proposals from
real participants, we would have to disclose a participant’s name to his responder in the second stage.17 Even
assuming that disclosure of participant identity was approved by our IRB, we find this an unappealing
design element. It would certainly select a certain type of subject who is unconcerned about disclosing his
participation and behavior. Second, we wanted to explore the effects of offer size independently of the share
represented by the offer. This requires a pool of offers that are outside the typical range, since most
ultimatum game proposers offer roughly one-third to one-half of the stakes (Oosterbeek et al. 2004).

17

This is also true of the method that takes proposers door-to-door.

15

Moreover, these would of course have to come from proposers with racially distinct names who were
willing to have their behavior disclosed to their respondents.18
Harrison and List (2004) describe the ideal experiment as featuring no deception.19 While we agree
that this is ideal, our objectives posed formidable obstacles to meeting this ideal. Moreover, we executed our
study in a manner that mitigates the concerns experimental economists have with deception. First, it is not
unreasonable to think that the public scrutinizes “cold calls”, which are often used for sales pitches, political
campaigning, and other solicitations. Since subjects are conditioned to consider a caller’s motivations, a
norm of non-deception within economics is unlikely to alter the behavior of subjects within a game. Second,
while 1600 subjects is large by standards of experimental papers, it is a very small proportion of the
universe. It is therefore unlikely that contamination is a major concern for future researchers in this pool.
Finally, we debriefed subjects about the deception in a letter following the experiment. A copy of the
debriefing letter appears in Appendix 4.

IV. Sample
Experimental games frequently take place in laboratories and rely on undergraduate subject
populations.20 Undergraduate samples pose a particular problem for studying racial differences in inequality
aversion because blacks are underrepresented in college populations. College samples also draw from a
higher stratum of the socio-economic spectrum where individuals may experience inequality differently
from older and less educated persons. Volunteer samples from the wider population suffer from similar
problems. As Doty and Silverthorne (1975) note, volunteers in human research “typically have more
education, higher occupational status, earlier birth position, lower chronological age, higher need for

18 We also note that our technique of constructing a full name from racially polarized first and last names (as described in
Appendix 3) means that actually very few individuals have the names we used in our survey. Power calculations under various
assumptions of racial name polarization, response rates, and income stratification are available from the authors upon request.
19 They write that an experiment is ideal “…in the sense that one is able to observe a subject in a controlled setting but where the
subject does not perceive any of the controls as being unnatural and there is no deception being practiced.”
20 See Levitt and List (2009) for a discussion of issues related to typical experimental samples.

16

approval and lower authoritarianism than non-volunteers” To address these concerns, we sought to
randomly sample a broad a spectrum of subjects from the general population.
We use a sampling frame new to the economics literature: voter registration files. Eligible citizens
must be registered to vote in 49 states21, a population that consists of 70% of citizens over the age of 18
(Census Bureau 2009, Table 2-1). According to the Current Population Survey November Supplement
(2008), on average, registered voters tend to be slightly older (45 versus 42 years of age), more likely to be
married (46% versus 42%) and female (51% versus 48%), and less likely to be non-white (14% versus 17%)
and Hispanic (10% versus 13%) than unregistered citizens.22 Voter files typically include identifying
information such as full name, address, gender, date of birth, and whether the person voted in recent
elections. Voter files have been used as the sampling frame for studies of voter mobilization (e.g., Gerber
and Green 2000; Nickerson 2008) and where voter turnout is a dependent variable of interest (e.g., Gerber,
Karlan and Bergan 2009). Sampling registered voters has been found to be slightly superior to random-digit
dialing with regards to election forecasting (Green and Gerber 2006). Registered voters are representative of
people who participate in politics, and similar to the population as a whole. In the U.S. as a whole, 64.9% of
all individuals 18 and over and 71% of citizens 18 and over are registered to vote. Registration rates are
slightly higher for whites (73.5% for white, non-Hispanics) than for blacks (69.7%).23 The registration rates
are even closer in states in our sample: in two of the three states, black registration slightly exceeds white
registration, and in the third the gap is smaller than the national average (U.S. Census Bureau, 2008).
Our sample comes from randomly selected black and white registered voters with telephone
numbers in three states: Georgia, North Carolina, and South Carolina. To be included in the study a state
needed to meet four criteria. First, the state’s voter file needed to include race as one of the fields

North Dakota does not require citizens to register to vote.
Registered and unregistered persons also share the same median categories of education and income in the CPS November
Supplement. That said, the overall distribution of education and income tends to be shifted higher for registered voters compared
to unregistered citizens.
23 All figures from U.S. Census Bureau tabulations of 2008 Current Population Survey data.
21
22

17

consistently collected (excluding Virginia and all non-southern states)24. Second, the state’s voter file needed
to be relatively easy to obtain (excluding Alabama, Florida, and Mississippi). Third, the population could not
have been subject to a recent diaspora (excluding Louisiana, due to Hurricane Katrina). Fourth, variation in
electoral support for President Obama was desired as a proxy for progressive political culture (i.e., Obama
won North Carolina in 2008, narrowly lost Georgia, and lost South Carolina by a wide margin).25 Finally,
geographically proximate states were selected in order to keep long run determinants of state development
such as weather and immigration patterns as constant as possible. Lists of registered voters were used as the
sampling frame because the process of assembling the sample was less expensive than random digit dialing
with screening questions for race and income. The statewide voter files for all three states were obtained in
early October 2008. The voter files were matched against consumer data files maintained by InfoUSA, one
of the largest consumer data firms in the world, to append reliable telephone numbers and update address
information.26 After the address information was cleaned, we geocoded the observations and appended
data on Census block group characteristics. Thus, the data used in this experiment come from the voter file,
census block group data, and answers provided during the experiment itself.
Our use of Census block group information makes our sampling approach different from that of
using a representative population sample from a maintained database, like those available through
KnowledgeNetworks and TESS. It allows us to sample respondents based on their neighborhood characteristics.
This in turn allows us to stratify our sampling to achieve sample balance along several important
dimensions. This feature of our sampling approach is an important contribution of our study.
Neighborhood level information is not part of the basic demographic variables collected in maintained
databases. Indeed, detailed geographic information is relatively uncommon in microdata. It is also unlikely
that survey respondents can be relied upon to provide this information themselves, since individuals are
24 The Voting Rights Act required some states with a history of disenfranchising black voters to collect this information. For a
history, see Canon (1999).
25 This left us with five potential states. We limited ourselves to three in order to have sufficient sample size within each state to
allow for potential analysis at the sub-state level. This excluded Tennessee and Arkansas.
26 The particular fuzzy matching algorithm used by InfoUSA is proprietary. InfoUSA’s data comes from a combination of public
sources, shared client data, purchased data, and independently collected data.

18

unlikely to know such information as their Census block group’s median household income. Moreover, we
estimate that our sample is considerably larger than the sample that could be drawn using similar restrictions
from the KnowledgeNetworks panel.27
In order to focus on racial differences in inequality aversion, we selected a sample stratified on
several characteristics that differ across blacks and whites but that may also condition behavior in the
ultimatum game and reactions to unequal treatment in general. The first of these is income. Given persistent
differences in income between blacks and whites, it is important to keep the income of the participants in
the experiment as consistent as possible.28 Thus, our first strata dimension is Census block group median
income. To balance income across blacks and whites while providing variance in income, black and white
subjects were drawn only from census block groups falling within the following three bands of income
based on median household income among blacks for each state: a) 10th percentile to 10th percentile plus
$2,000; b) the median plus or minus $1,000; and c) $2,000 below the 90th percentile to the 90th percentile.
For example, whites and blacks sampled in South Carolina resided in census block groups with median
household incomes between $21 – 23,000, $30 – 32,000, and $48 – 50,000. Despite economic segregation in
housing and the Census Bureau’s efforts to draw boundaries to account for well-defined neighborhoods and
increased homogeneity, there will be variance in income within neighborhoods so the average household
income may still differ across blacks and whites in our sample. However, we can be certain that blacks and
whites were selected from neighborhoods with similar average socio-economic statuses.
The racial diversity of a subject’s neighborhood may also moderate her response to the treatment
stimulus, so we also stratify on the racial diversity of a respondent’s neighborhood. The racial mix of each
neighborhood was determined by looking at the percentage of registered voters in each street name – voting
precinct group in the major racial categories. We used the street-voting precinct as the neighborhood in this
instance since it allowed us to more precisely characterize a subject’s true neighborhood. Census block
Conversations with KnowledgeNetworks indicated that KN could recruit roughly 400 African-American participants from our
three target states. However, once we limited that sample to our neighborhood characteristic strata, the yield falls to less than 100.
28 In 2008, the median household income for non-Hispanic whites was $55,530 compared to $34,218 for non-Hispanic blacks
(DeNavas-Walt, Proctor, and Smith 2009).
27

19

groups are larger and may contain distinct neighborhoods or sub-neighborhoods. Drawing loosely on the
neighborhood tipping point literature (Card, Mas, and Rothstein 2008), we then categorized neighborhoods
as being relatively homogeneous (0-20% and 80-100% black), predominantly white (20-40% black), evenly
balanced (40-60% black), or predominantly black (60-80% black). The goal of the categories was to keep the
neighborhood context as constant as possible across black and white respondents.
The subject pool was then stratified based on state of residence, the race of the subject (white or
black), the three income categories (low, middle, and high for blacks in the state), and the levels of
neighborhood diversity (homogeneous, predominantly white, evenly divided, and predominantly black). In
all, there are 24 strata in the experiment, with balanced representation from each of the three states within
each strata. Subjects within each strata were then randomly assigned to the three facets of the treatment.
Table 1 shows descriptive statistics for our final sample of survey respondents. The sample differs
substantially from typical undergraduate populations. The top panel of Table 1 shows that not only is the
sample racially diverse, but our respondents are also considerably older. The subjects also come from
diverse neighborhoods socio-economically, as the neighborhood levels of education (17% college degree
and a median of 11.7 years of education) and income (median household income $35,500 with 16% poverty)
from the appended Census block group data suggest. Most importantly for our purposes, the blacks and
whites drawn for the experiment are comparable in nearly every measurable capacity. The only statistically
significant difference between the black and white variable means in Table 1 is in the percentage of blacks
an individual’s neighborhood – a difference due to our strategy of stratifying on neighborhood racial
composition and using racially homogenous neighborhoods as one strata.
In Tables 2a and 2b we examine how our final sample of respondents compares to the sample of
potential respondents the survey firm attempted to contact. Although blacks and whites who participated in
the survey are balanced on observable characteristics, participants may differ from potential subjects due to
non-random non-response. We have information on the pool of potential subjects the survey firm
attempted to contact, so we can compare characteristics of our final sample to those of the wider potential
20

subject population. Non-response can occur at two points: first, respondents may refuse to answer the
phone when contacted, or second, they may refuse to participate in the survey after answering the
telephone. We have information on characteristics of all three groups: all potential subjects, those who
answered the telephone when contacted by our survey firm but refused to participate, and participants.
The survey firm attempted to dial 5397 valid telephone numbers during the calling period and
completed experiments with 1650 subjects for a total response rate of 31%. We later dropped three
respondents with invalid phone numbers. Of those subjects who answered the telephone, the refusal rate
was only 19% (i.e., 81% of the people took the survey once on the telephone). These completion and refusal
rates are very good compared to other public opinion surveys and yielded a broad cross section of the
populace.
Unsurprisingly, individuals who chose to answer the phone differ somewhat from the broader pool
of potential subjects. This is shown in Table 2, panel A. People answering the telephone were more likely to
be white, female, and older, and live in neighborhoods with less income and education. That said, the
differences between the ideal sample and the people contacted are not large (e.g., 2 percentage point
difference in gender, $2000 in income). Since treatment conditions were randomly assigned, the offers,
stakes, and opponent names provided to subjects did not differ between contacted and uncontacted
individuals.
Conditional on answering the telephone, there were very few differences between the people
participating in the experiment and those refusing to participate, as shown in Table 2, panel B. Compliers
and non-compliers were similar with regards to race, age, education, and income. The only statistically
significant difference between compliers and non-compliers was that women were more likely to participate
in the experiment. Thus, the population of subjects participating in the experiment is fairly representative of
black and white registered voters in the three states for the income categories and neighborhood types
selected.

21

V. Results
A. Randomization Checks
Before proceeding to the analysis, it is important to verify that the treatments (names, stakes, and
offers) were balanced across subjects—in other words, that our randomization worked properly. We verify
this in Table 3, in which we present the estimated mean and standard error of each treatment variable by the
24 strata defined by income, race, and neighborhood diversity. Looking down the columns of means, there
is a high degree of consistency in a treatment variable’s mean across strata groups. Importantly, the withinstratum mean is typically within one standard error of the total sample mean and in all cases within two. No
systematic differences are observed for any of the treatments. We conclude that the randomization
procedure was successful, and therefore differences in acceptance rates can be attributed to the treatments
rather than the characteristics of the subjects. However, for completeness we present results with controls
for the strata and for observable characteristics.
B. Graphical analysis
We begin by presenting a graphical version of our basic analysis in Figures 1(a)-(f). The figures
reflect acceptance rates conditional on respondent race and offer size both for the experiment overall and
separately by assigned stakes size. The vertical bars represent 95% confidence intervals around the mean
acceptance rates.
Figure 1a reports acceptance rates by race and offer averaged over all stakes levels. Acceptance rates
are non-trivial but less than one at all offer levels in our experiment. This is reassuring, since it suggests that
our stakes-offer combinations span a range over which there is potential for changes in stakes or offer size
to lead to changes in respondent behavior. Figure 1a also shows that whites are somewhat more likely than
blacks to accept small offers (although the difference is not significant). This relationship is reversed for
larger offers (and the difference becomes significant). We seek to better understand this reversal below and
in our conclusion. For now, we simply point out that this reversal implies neither race is likely to have a
general propensity toward higher acceptance rates. We demonstrate this more formally in our logit analysis.
22

Figures 1(b) through 1(f) show acceptance rates disaggregated across the five stakes levels in our
experiment. Within these sub-experiments, consistent with prior studies acceptance rates generally rise with
offer size, and again are non-trivial but less than one.29 In instances where respondents are offered an even
split, roughly 50% accept. This is true across all three stakes levels (5, 10, and 20) in which this split was
offered. The figures also show that the black-white reversal in relative acceptance rates generally occurs
between $2 and $5 offers, regardless of stakes size.30
In Figure 2(a), we plot acceptance rates by race over the share that the proposed split represents.
Figures 2(b) through 2(e) plot these separately by offer amount, allowing us to observe how varying the
stakes affects the acceptance rate for a given offer. It is clear from the figures that acceptance rates increase
with offered share for both races. At higher offers - $5 and $10 - this increase is monotonic and blacks have
uniformly higher acceptance rates than whites. For small offers, blacks and whites exhibit similar acceptance
rates but there is some non-monotonicity in responses to these offers when they represent very small shares.
These figures foreshadow another of our results, namely that blacks and whites differ in their responses to
offer size, conditional on the share that the offer represents.

C. Choice Model Estimation
i. Analysis of Choice Model Results
We use standard logit models, as discussed in Section IV, to investigate the determinants of
acceptance more formally. We begin by estimating baseline models on our complete sample of respondents.
Specifically, we estimate Equations (4) and (5) where black is an indicator variable for whether the
respondent indicated that she is African American on the voter registration form.31 Recall that we obtain

29 Interestingly, there are several instances in which acceptance rates decrease when the offer rises from $1 to $2, although the
change is not significant in our sample. This is reminiscent of results of studies on the non-linear effects of monetary incentives
on performance, as discussed in Gneezy and Rustichini (2000).
30 The $50 stakes sub-experiment is the only exception to this.
31 Racial coding was also confirmed by data collected by the consumer data firm where available. Unsurprisingly, people who
check “Black” on voter registration forms are also likely to select “Black” in other outlets.

23

this information from the voter file data, rather than from the respondent directly, so the respondent’s race
is not being explicitly cued in the experiment.
The relative identity introduces some subtlety into the analysis. An artifact of the construction of
relative is that offer can be interpreted as a function of absolute and relative. This makes interpreting the
coefficient on offer in model 1(b) somewhat awkward. In light of this, we estimate two specifications for each
of our models of interest. Model 1(a) includes only offer and relative from among the assigned game variables,
so no game variable in this specification is a function of any other two variables. The coefficients on offer
and relative estimated by (1a) are cumulative of effects due to their covariance with absolute, but their
interpretation is more transparent. We then show that these effects are robust to controlling for absolute by
estimating (1b). Omitting absolute is not the only way to address this problem. However, theory and previous
experimental work predict an important role for relative, and Figures 1(a) to (f) suggest racial differences in
the response to offer. In order of interest then, absolute seems the natural candidate for exclusion.
We estimate versions of the equation (5) specification that exclude interactions of the assigned game
variables with black. By omitting the race interactions, we can more formally assess whether blacks are more
or less likely to accept an ultimatum offer on average across our games. We also document the average
effects of changes in our game variables on acceptance decisions. We then address questions about whether
blacks and whites respond differently to changes in specific ultimatum game variables.
We also run our models with additional control variables. In principle, our randomization strategy
should balance respondents on observed and unobserved characteristics, so no additional covariates need be
included for consistent estimation of the parameters in equation (5). Nevertheless, we verify that the
estimates obtained from the “no controls” specifications are robust to the inclusion of controls for observed
characteristics. Our second specification adds controls for a respondent’s Census block group
characteristics. These are listed in Table 1. Their inclusion will control for differences across our strata in

24

tract-level aggregate characteristics other than median household income and racial diversity.32 A third
specification adds controls for “game conditions,” which includes the length of the interview in minutes, the
time of day the interview was conducted (which embeds the date and is not listed in the table), and the
number of times the survey firm attempted to reach the respondent. If for some reason respondents across
race-income-neighborhood racial diversity strata differ in the times they were called or their interaction with
the survey firm in a way that affects their responses, these variables should account for that.33 In our fourth
specification, we estimate a specification with a full set of fixed effects for our 24 strata. This flexibly
captures any differences in game responses that occur systematically across strata.
The results of estimating these 16 specifications are presented in Table 4. Panels A and B present
estimates from the permutations of model (5). The sample is not stable across the four specifications, as
shown in the bottom rows of the panels. A large number of respondents are missing data for the game
conditions variables and, to a lesser extent, for the tract-level demographic controls.34 These respondents
are therefore omitted from the estimation when these variables are included. For this reason, our preferred
specification includes the fixed effects strata. The strata fixed effects specification includes all respondents
yet controls for group differences across our race-income-neighborhood diversity cells. Results from this
specification are presented in bold in Table 4.
There is a high degree of consistency across results from the eight specifications in Panel A. Black is
an insignificant predictor of acceptance. In all specifications, relative has a large, strongly significant and
negative impact on acceptance – the smaller the relative share of the stakes offered, the less likely it is
accepted. The implied marginal effect of a 10 percentage point decrease in relative is a decrease in the
likelihood of acceptance of a little more than four percentage points. Moreover, the coefficient on relative is
unaffected by the addition of the relative*black interaction term. This means that blacks and whites respond
32 See McClellan and Skinner (1999) and Geronimus et al. (1996) for an example and discussion of using neighborhood level
aggregates to proxy for individual characteristics.
33 Some of the data appended by the survey firm, like time of interview, is missing for respondents who did not progress
sufficiently far in the interview. Some of these respondents did manage to answer the ultimatum game question, so they are part
of our sample.
34 Based on conversations with the survey firm, it appears that respondents who ended the call before the very end of the script
was reached (but after answering the ultimatum questions) were sometimes not assigned values for the game variables.

25

similarly to changes in the relative share that a proposed offer represents, holding offer size constant. We
interpret this as evidence that blacks and whites have similar levels of aversion to inequality on average.
Changes in offer size, however, tell a different story. The estimated impact of offer is positive, and
marginally significant in four cases (although not in our preferred specification) when the interactions with
race are excluded. Once race interactions are included, the sign on offer changes (in three of the four control
specifications). This is because blacks and whites differ significantly in their response to changes in the offer
amount, holding relative constant. Estimates in the last four columns of Panel A show that an extra dollar in
the offer increases the likelihood of a black respondent accepting by about two percentage points, an effect
that is roughly half that of a ten percentage point increase in relative.
These results are robust to the inclusion of the absolute variable in Panel B. If anything, the patterns
identified in Panel A are stronger and more consistent in Panel B. This is because coefficients on offer and
relative no longer reflect the combined effects of those variables and absolute. Black is again insignificant in all
specifications. Offer has a positive sign when the race interactions are excluded and a negative sign when they
are included but is insignificant in all cases. The impact of relative is even more strongly negative than in
Panel A. Again there is no significant difference between blacks and whites in the effect of relative. The offer
effect identified for blacks in Panel A is larger in the Panel B estimates. Absolute does not significantly
predict acceptance, although it consistently has a positive, small point estimate. In specifications with the
race interactions, black*absolute has a similarly sized negative point estimate.
In Table 5, we estimate a mixed logit choice model that allows us to relax some of the assumptions
behind the standard logit model, as discussed in Section IV. Specifically, we estimate a mixed logit model in
which the coefficients on offer, relative, and absolute are allowed to differ across individuals. In other
words, these are the “random” coefficients. We omit other controls from the model since the mixed logit is
computationally intensive and the Table 4 analysis showed little impact of including additional controls on
the standard logit estimates, consistent with our experimental approach. To compare the behavior across

26

racial groups, we estimate the mixed logit model on the full sample and then on subsamples of blacks and
whites.
The mixed logit model generates estimates of the mean and standard deviation of (assumed)
normally distributed coefficient vectors. Table 5 shows that the mean of the coefficients on offer and
absolute inequality are insignificantly different from zero in the full sample, while the negative coefficient on
relative inequality is highly significant, indicating that higher relative inequality reduces the likelihood of
accepting an offer. The estimated standard deviations reported in the bottom half of Table 5 shows that in
all three cases, the data fail to identify a confidence interval for the estimated standard deviation that
excludes zero. This means that although the mean coefficient on relative inequality is different from zero,
we have insufficient power to identify to what degree the coefficient differs across individuals in the
population. We obtain very similar results to those for the full sample when we estimate the model
separately on blacks and whites. The mean of the relative inequality coefficient is very similar across the two
groups, and other coefficient means and all standard deviations are insignificant. The exception is the mean
coefficient on offer in the black sample. As in the standard logit estimates, an increase in offer increases the
probability of acceptance for blacks but not for whites. But as with the coefficients on relative inequality,
there is little evidence that this parameter differs importantly across individuals within the black subsample.
The results are qualitatively similar in the mixed and standard logit models. Moreover, the offer
effect identified for blacks in the standard logit model is robust to the more flexible mixed logit
specification. Since the qualitative results for the (mean) coefficients are similar, we conclude that the
standard logit estimates provide an adequate approximation for identifying differences across groups in the
means of the underlying parameters. Unfortunately we simply lack the power to precisely estimate a
distribution of heterogeneous coefficients.

ii. Understanding the Offer Effect and Robustness Checks

27

We know of no previous research that identifies an independent effect of offer size on acceptance
behavior in the ultimatum game, so we are unable to compare this feature of our results to the work of
others. In this section, we investigate the robustness of this finding in order to better understand it. One
possible source of this result is that respondents may not have had a good understanding of the game. If
blacks overlooked or misunderstood the fairness aspects of the game more frequently than whites, then we
might observe an independent effect of offer size for blacks but not for whites.
We can investigate this possibility using answers to Question 3 (Q3) of our survey, which asked
respondents to name the smallest offer that they would have found acceptable. As Table 1 documented,
nearly 40% of respondents who answered Q3 provided an answer that was incompatible with their behavior
in the game moments before. We separate our sample, conditional on answering Q3, into those who gave a
response consistent with their behavior and those whose response was inconsistent.35 We then re-estimate
our preferred specifications from Table 4 on these subsamples. It is important to note that we consider the
compatibility of Q3 to be only a suggestive proxy for understanding the game. It is possible that
respondents who answered Q3 inconsistently nevertheless understood and answered Q2 (the accept/reject
item) in a valid way. Nevertheless, we believe that the large number of incompatible responses justifies
investigating whether the offer effect is driven by this particular subgroup.
We also investigated the sensitivity of our results to the stakes in the game. Some of our treatments
use offer and stakes amounts that are small relative to findings in the literature on the importance of larger
amounts for achieving salience with subjects (e.g. Slonim and Roth 1998; Cameron 1999). We therefore
repeat our specifications restricting the sample to those who received treatments where the stakes were $20
or more.
The results of these checks are presented in Table 6. Several important points emerge from this
analysis. First, respondents who gave inconsistent answers to Q3 exhibit a response to changes in relative

Specifically, a consistent answer was defined as a minimum amount less than or equal to an accepted offer OR a minimum
amount greater than a rejected offer. Inconsistent responses gave minimum amounts less than or equal to a rejected offer OR
greater than an accepted offer.
35

28

inequality that is wrong-signed and sometimes statistically significant. Given that their response to increases
in relative inequality differs so dramatically from that of respondents who provided consistent answers, we
surmise that an inconsistent response to Q3 strongly suggests that the respondent did not understand the
game.36 Also, the coefficient on relative inequality among those with consistent responses is roughly double
that in the full sample. This means that among respondents who appear to have understood the game,
responses to even splits and near-zero share offers are closer to those found elsewhere in the literature. Very
small shares are accepted with very low probability while even splits are accepted with very high probability.
Second, the offer effect identified for blacks in Table 4 is not driven by individuals with poor game
comprehension. If anything, the opposite appears to be true. This is apparent from the second two columns
in Panel B of Table 6. This model includes all three game variables and their interactions with black. Blacks
who gave consistent responses to Q3 exhibit a larger, statistically significant response to increases in offer
size than do whites with a point estimate that exceeds the Table 4 estimate, while blacks and whites who
provided inconsistent responses to the two questions are insignificantly affected by offer.
Interestingly, a probit analysis of the determinants of a valid Q3 response turned up no significant
predictors.37 Although it is somewhat puzzling that variables like neighborhood income and education level
do not predict game comprehension, there are reasons why this might be the case. Perhaps game
comprehension requires catching respondents at a moment when they can briefly pay attention to the caller.
If the likelihood of getting a respondent at a “good time” is unrelated to respondent characteristics – i.e. it is
just surveyor luck—then again we would find no significant predictors of Q3 response.38
While we cannot know the exact reason for poor game comprehension in our experiment, it is
helpful for our analysis that comprehension is unrelated to observable respondent characteristics. Given this

The implied marginal effect of share is greater than 1 for respondents who gave consistent answers. This simply indicates that
the slope of share is steeper than one over the range observed in the data.
37 A probit model predicting inconsistent Q3 responses included the following as possible determinants: black dummy, female
dummy, household size, age and age squared, state indicators, indicators for neighborhood diversity categories, median household
income, and mean years of education at the Census tract level.
38 In principle it is possible that there was a subset of interviewers at the survey firm who were difficult for respondents to
understand. In this case, random allocation of phone numbers to surveyors would mean that respondent demographics are
orthogonal to game comprehension. Based on our observation of the callers, we consider this possibility unlikely.
36

29

orthogonality, we continue our analysis using the complete sample of Q2 respondents and retaining the
experimental data in the form in which it was collected, rather than dropping those who appear not to have
understood the game. We assume this is a conservative strategy. Because game understanding is unrelated to
observable characteristics, and in particular to race, poor game comprehension on the part of a large
number of respondents likely attenuates our reported results.
The third point to take from Table 6 concerns the importance of salience (large stakes) for our
findings. The rightmost two columns in the Table 6 panels show that our main results hold when our
sample is restricted to those with consistent Q3 response and regardless of the stakes in the assigned
treatment. Specifically, the final two columns show that (i) blacks and whites respond similarly to changes in
relative inequality and (ii) there is a small but robust effect of offer size conditional on relative inequality
among blacks. Sample size becomes an issue here as our standard errors increase as we cut the data. But the
robustness of the point estimates on relative inequality and the interaction of black*offer leads us to conclude
that our main findings are not driven by problematic levels of salience nor by poor game comprehension
among some subjects.
The final two columns of Table 6 present our main specifications estimated separately on those
respondents who received offers representing different shares. Specifically, we were interested in trying to
understand what role the very low shares in some of our treatment profiles might have played in our results.
In experiments with subjects divided in both proposers and responders, proposers typically offer one-third
to one-half of the stakes (Forsythe et al. 1994). Offers representing shares much below that range are
uncommon in laboratory studies, although we implement them here to provide a broader survey of ranges
in which black and white responses might differ. We divided our sample into those who received offers
representing common, real-world shares (in the range of 0.25 to 0.5) versus those whose offers represented
lower shares.39 Our standard errors increase considerably after splitting the sample in this way, but the point

39

No treatments offered shares above 0.5.

30

estimate on the interaction of black and offer is similar in both subsamples, and achieves acceptable
significance levels in Panel A.40
To further investigate the source of the offer effect for, we re-estimated our baseline models on
subsets of the data defined by demographic characteristics, to determine if the offer effect might be stronger
in some groups than others. The results of interest from this exercise are presented in Tables 7 and 8. 41
Table 7 presents our Table 4 specifications estimated separately on subsamples of our data divided
by sex and by age. Our parameter estimates vary little across any of these subgroups. In particular, the point
estimates for both relative and black*offer are very similar across all four subgroups. The effects are sometimes
insignificant, as we’ve reduced our sample sizes in these estimates, but the point estimates are stable and
very similar to those for analogous specifications in Table 4.
Table 8 shows the results of estimating our models separately on subsamples defined by income (the
same three income categories used to define our sample strata). Note that we omit strata fixed effects from
the models in Table 8 because (i) their inclusion does not substantively impact the results but does increase
imprecision, and (ii) the relevant strata are not constant across income categories, meaning that the
estimating equation would necessarily change across subsamples.
Panel A reports estimates from the full game variable specification. We first report estimates from
the model without race interactions, primarily for comparison. The results of interest are in the rightmost
three columns of Panel A. These estimates show that the effects of both offer and relative on acceptance differ
across income groups. The main effect of relative is largest for the lowest income group, and is significantly
larger (at the 10% level) than the coefficient on relative for the highest income group.
Although the main effect of relative is insignificant for the highest income group, this is a result of
retaining respondents with poor game comprehension. When the “inconsistent” group defined in Table 6 is
excluded, the patterns in Panel A of Table 8 are retained but the coefficients increase in magnitude and gain
The p-value on black*offer in the second column from the right is 0.11.
We performed the same exercise using subsamples defined by neighborhood racial diversity. The offer effect was again limited
to blacks and was similar in size across all subsamples (and therefore similar in size to the estimates in Table 4). We found no
differences of note in the other parameter estimates.

40
41

31

significance in some cases, consistent with the attenuation bias that error-prone data would introduce. In
particular, the lowest income blacks in this exercise were significantly less influenced by relative than whites
in that group. In the highest income group, the main effect of relative was positive and significant, and did
not differ significantly across races.
Returning to the results in Table 8, the main effect of offer in the lowest income group has a negative
coefficient and is weakly significant, but blacks in this group are significantly more likely than whites to
accept after a dollar is added to offer. The net offer effect for blacks is roughly 0.031 and is significant at the
5% level – very similar to the result we reported for the complete sample in Table 4. The main effect and
black*offer are both insignificant in estimates using the two higher income groups.
The negative and significant main effect of offer in the lowest income group is counterintuitive since,
conditional on relative, we would expect higher offers to be more likely to be accepted. It is reasonable to
worry that the inter-relationship between the three game variables is contributing to this. To assess this
possibility, we present versions of the model with either absolute or relative omitted in Panel B. The main
effect of offer is either insignificant or positive across all income groups in these models. Black*offer is still
large and positive for the lowest income group, and the pattern of results for relative is also retained in the
first three columns in Panel B. Results in Panel B were qualitatively the same when we omitted respondents
with poor game comprehension from the estimation. In light of the Panel B results, it seems likely that the
negative sign on offer size among whites in the lowest income group in Panel A is explained by the fact that
offer is a function of absolute and relative.
We conclude that the offer effect for blacks identified in Table 4 is driven by blacks in the lowest
income group.42 Moreover, blacks and whites in the lowest part of the black income distribution respond
differently to the relative and offer variables, with offer size having relatively more influence on blacks’
acceptance decisions and relative having more influence for whites. Interestingly, lower income whites and
It is possible that blacks in the lowest neighborhood income group are still poorer than whites in the lowest income group.
Individual income differences may therefore contribute to this effect. We cannot rule this out with publicly available data, but we
are skeptical that this is the entire explanation, since blacks in the other neighborhood income groups may also be poorer than
whites in those groups yet we observe no black-white differences in these groups.
42

32

blacks both differ from their higher income racial counterparts, but they do so in different ways. For those
in the highest income group, relative is less important than it is for low income whites and offer is less
important than it is for low income blacks. Futhermore, we find that the offer effect among blacks in Table
4 is highly robust across male and female subsamples and across subsamples of working age versus older
respondents. Our data do not allow us to analyze differences across multi-level demographic groups (e.g.
neighborhood income and age), but the fact that we find that neighborhood income subsamples are the only
instance in which our parameter estimates deviate noticeably from those in the overall sample strongly
suggests to us that the most striking black-white differences in ultimatum game behavior occur at the lowest
income levels.

iii. The Effect of Proposer Race
The final dimension to our experiment was the random assignment of the implied race of our
hypothetical ultimatum proposer. As described in the data section, this was done through the use of racially
distinct male names.43 Tables 9 and 10 explore variation in acceptance rates across proposer-respondent
race combinations. Table 9 presents raw acceptance rates for the four proposer-respondent race cells, and
acceptance rates across the cells are all very close to the sample average. This is reflected in Table 10, which
presents results from a logit model of the respondent’s acceptance decision with controls for the four
mutually exclusive race combinations. White respondent-white proposer is the omitted category. There is
evidence that blacks facing a black proposer are more likely to accept than respondents in the other three
scenarios. This is consistent with this cell having the highest observed acceptance rate in Table 9.
We explore the effect of proposer race further by adding interactions with proposer race to the
offer-relative model discussed above. We use the offer-relative model, rather than the full game variable
model, because (i) the previous analysis showed that adding absolute to the model had little impact on

Note that callers for the survey firm were predominantly white. While this is probably helpful from the standpoint of
preventing additional variation in the treatment, it is possible that the race of the caller is more salient than the implied race of the
proposer. As a result, our variation on this dimension may suffer from reduced impact.
43

33

coefficients obtained from the offer-relative model, and (ii) the interactions with proposer race place
additional strain on the data, so we try to eliminate unnecessary covariates from the model. We create a
dummy variable equal to one if the name of the hypothetical proposer was a high frequency black name.44
As before, we present results from specifications both with and without respondent race interactions with
the randomly assigned variables.
The results are presented in Table 11. A number of findings from previous specifications carry over
to the proposer race models. There is no average difference in acceptance rates across blacks and whites;
relative is an important predictor of acceptance; and the offer effect for blacks is largely robust to the new
specifications. With the caveat in mind that we may be asking a lot of the data to estimate these models –
particularly the one which includes race interactions – the results in Table 11 reveal some interesting points
about proposer race. First, the main effect of proposer race is insignificant. This means that, at least as we
have conveyed it through our experiment, proposer race has no impact on overall acceptance rates.
Proposer race also does not significantly alter the effects of the game variables offer and relative, or of
respondent race. Finally, the fact that proposer race * black is small and insignificant appears at odds with the
results in Table 10. However, the model with race interactions shows that blacks respond more strongly to
increases in relative when the proposer is black. This likely explains why blacks facing black proposers exhibit
somewhat higher acceptance rates in the more parsimonious model of Table 10. Therefore the Table 10
result turns out to be something more subtle than a level effect of black proposers when the respondent is
black. Rather, blacks facing black proposers increase their acceptance rates more rapidly in response to
higher offered shares than do blacks facing white proposers.

V. Conclusion

An alternative is to define a proposer race dummy equal to one if the proposer’s implied race was the same as the respondent’s.
We estimated this alternative model. The results are substantively similar to those presented in Table 10, but their interpretation is
less transparent. We therefore omit them from the paper. They are available upon request.
44

34

We report results from an ultimatum game experiment played with a large sample of black and white
respondents in the United States. Our respondents came from a stratified random sample drawn from
registered voters in three Southern states. Our sample was representative of blacks and whites from low,
middle, and high deciles of the black neighborhood income distribution and balanced on neighborhood
diversity in those states. Respondents played the game over the telephone against a (hypothetical) proposer.
We varied implied race of the proposer by using distinctively black and white names, and we varied
dimensions of inequality by randomizing both the stakes and offer amount across games.
We examine aversion to two types of inequality—relative and absolute—characterized by rejection
of an offer of a given level of inequality. We find that both blacks and whites are much more responsive to
relative inequality than absolute inequality. We find no differences across races in inequality aversion as
measured by rejection of offers with identical inequality characteristics. However, we find that blacks are
more likely to accept larger offers conditional on relative inequality, and that this difference is driven in the
behavior of blacks and whites in the lowest income category. In the framework of Bolton and Ockenfels
(2000), this suggests that the lowest income blacks differ from the lowest income whites in terms of the
importance of pecuniary returns for their decisions in this game. We also find an unequal effect of implied
proposer race. Blacks increase their acceptance of the offered relative share more rapidly when the perceived
race of the Proposer is black.
Our finding that black acceptance of offers is more responsive to shares offered by implied blacks
than whites relates to prior work on discrimination (Fershtman and Gneezy 2001). That is, the behavior we
observe is consistent with blacks being more sensitive to being treated unfairly by whites than by blacks.
At this point we can only speculate about the source of the offer effect among poorer blacks. One
possible explanation relates to concerns about status. Prior work has shown that individuals will attempt to
distinguish themselves from groups to which they belong demographically if the reference group is of low
status. For instance, Charles, Hurst, and Roussanov (2009) show that blacks and Hispanics spend more than
whites on conspicuous goods conditional on income. They argue that this behavior stems from concerns
35

about reference group status. In our study, poorer blacks showed a tendency to reject small offers ($1 or $2)
regardless of the stakes; this tendency might be attributed to the stigma associated with welfare (Gilens
1999). When larger sums are offered, very low income blacks tend more than whites to accept the offer,
perhaps because the value of the offer has exceeded the stigma associated with accepting it.
However, there are at least two alternative explanations for the offer effect among blacks. One
possibility is that “gifts” might be associated with stronger expectations of future reciprocity among low
income blacks as compared to low income whites. This could make accepting small gifts relatively less
desirable. Like the stigma explanation, this explanation assumes that utility maximizing behavior in response
to social concerns drives racial differences in ultimatum game behavior. A final possibility is that different
underlying preferences drive racial differences in behavior. In our case, blacks and whites are both more
likely to accept offers that represent larger shares of the stakes (lower relative inequality), suggesting a
common preference for more equal splits, but blacks are more likely to impose a nominal lower bar below
which any share of a split is unacceptable. While it is theoretically possible that a “reservation” level is part
of preferences for blacks more often than for whites, the fact that only low income blacks respond to the
game in this way suggests to us that social and status concerns are more likely explanations for the
differences we observe.
Our findings suggest many interesting avenues for future work. We find that American blacks and
whites differ in the importance they assign to pecuniary returns in the economic exchange of the ultimatum
game, conditional on the inequality embodied in the exchange, with pecuniary returns playing a larger role
for blacks. Understanding whether the differences detected in this artefactual field experiment have bearing
on more substantive decision-making – such as preferences for redistribution, support for social programs,
and labor market behavior – is certainly an important area to explore. A second interesting area to explore is
the potential for racial differences in other notions of fairness, particularly procedural fairness versus
distributional fairness, as examined in Bolton, Brandts, and Ockenfels (2005). Finally, our work suggests that

36

a much richer understanding of group differences in experimental behavior is possible through the creative
and widespread deployment of experiments in a representative population.

Acknowledgements: We thank Bill Evans, Dan Hungerman, Lars Lefgren, Sandra Black, and seminar
participants at Princeton University, Vanderbilt University, and the University of Notre Dame for helpful
comments. Wozniak thanks the Industrial Relations Section at the Princeton Economics Department for
financial support during the course of this project. All errors are our own.

37

Works Cited:
Abraham, Katharine; Helms, Sara; and Presser, Stanley. 2009. “How Social Processes Distort Measurement:
The Impact of Survey Non-response on Estimates of Volunteer Work in the United States.” American
Journal of Sociology. 114(4): 1129-1165.
Adimora, A. A.; Schoenbach, V. J; Martinson, F. E. A.; Stancil, T. R.; Donaldson, K. H. 2001. “Driver’s
License and Voter Registration Lists as Population-Based Sampling Frames for Rural African Americans.”
Annals of Epidemiology. 11(6): 385-88.
Alesina, A. and Ferrara, E. 2005. “Preferences for Redistribution in the Land of Opportunities.” Journal of
Public Economics. 89(2005): 897-931.
Andreoni, James, and Lise Vesterlund. 2001. “Which Is the Fair Sex? Gender Differences
in Altruism,” Quarterly Journal of Economics CXVI 293–312.
Atkinson, A.B. 1970. “On the Measurement of Inequality.” Journal of Economic Theory 2: 244-63.
Ayres, I., and Siegelman, P. 1995. “Race and Gender Discrimination in Bargaining For a New Car.”
American Economic Review 85: 304-21.
Babcock, L, and Laschever, S. 2003. Women Don’t Ask: Negotiation and the Gender Divide. Princeton University.
Barros, C. P.; Proenca, I.; and Vieira, C. J. 2005. “Low-Wage Employment in Portugal: A Mixed-Logit
Approach.” IZA Discussion Paper No. 1667.
Bartels, Larry M. Unequal Democracy: The Political Economy of the New Gilded Age. Princeton: Princeton
University Press (2008).
Benjamin, D. J., Choi, J. J., and Strickland, A. 2009. “Social Identity and Preferences.” NBER Working Paper
W13309.
Berger, Jonah and Pope, Devin. 2010. “Can Losing Lead to Winning?” Management Science.
Bertrand, M. and Mullainathan, S. 2004. “Are Emily and Brendan More Employable that Latoya and
Tyrone? Evidence on Racial Discrimination in the Labor Market from a Large Randomized Experiment.”
American Economic Review.
Blount, Sally. “When Social Outcomes Aren’t Fair: The Effect of Causal Attributions on Preferences.”
Organizational Behavior and Human Decision Processes. 63(2): 131-144.
Bolton, Gary; Jordi Brandts; and Axel Ockenfels. 2005. “Fair Procedures: Evidence from Games Involving
Lotteries.” The Economic Journal. 115(October): 1054-1076.
Bolton, Gary, and Elena Katok. 1995. “An Experimental Test for Gender Differences in
Beneficent Behavior,” Economics Letters, XLVIII 287–92.
Bolton, Gary, and Axel Ockenfels. 2000. “ERC: A Theory of Equity, Reciprocity, and Competition.”
American Economic Review 90(1): 166-193.
38

Camerer, C. 2003. “Behavioral Studies of Strategic Thinking in Games. Trends in Cognitive Sciences 7(5): 225231.
Cameron, Lisa A. 1999. "Raising the Stakes in the Ultimatum Game: Experimental Evidence from
Indonesia." Economic Inquiry 37(1): 47-59.
Cappelen, A. W.; Hole, A. D.; Sorensen, E. O.; Tungodden, B. 2007. “The Pluralism of Fairness Ideals: An
Experimental Approach.” American Economic Review, 97(3): 818-827.
Canon, David. 1999. Race, Redistricting, and Representation: The Unintended Consequences of Majority Black Districts.
Chicago: University of Chicago Press.
Card, David, Alexandre Mas, and Jesse Rothstein. 2008. "Tipping and the Dynamics of Segregation."
Quarterly Journal of Economics 123(1):177-218.
Card, David; Stefano Della Vigna; and Ulrike Malmendier. Forthcoming. “The Role of Theory in Field
Experiments.” Journal of Economic Perspectives.
Carpenter, Jeffrey, Cristina Connolly, and Caitlin Knowles Myers. 2008. “Altruistic Behavior in a
Representative Dictator Experiment.” Experimental Economics 11: 282-98.
Chen, Kang and Fang-Fang Tang. 2009. “Cultural Differences between Tibetans and Ethnic Han Chinese in
Ultimatum Bargaining Experiments.” European Journal of Political Economy 25: 78-84.
Chuah, Swee-Hoon, Robert Hoffman, Martin Jones, and Geoffrey Williams. 2007. “Do Cultures Clash?
Evidence from Cross-National Ultimatum Game Experiments.” Journal of Economic Behavior & Organization
64: 35-48.
Dawson, Michael. 1995. Behind the Mule: Race and Class in African-American Politics. Princeton: Princeton
University Press.
DeNavas-Walt, C., Proctor, B. D., and Smith, J. C. 2008. “Income, Poverty, and Health Insurance Coverage
in the United States: 2007.” US Census Bureau, US Department of Commerce.
Detang-Dessendre, C.; Goffette-Nagot, F.; Piguet, V. 2008. “Life Cycle and Migration to Rural and Urban
Areas: Estimation of a Mixed Logit Model on French Data.” Journal of Regional Science, 48(4): 789-824.
Doty, R. L., and Silverthorne, C. 1975. “Influence of Menstrual Cycle on Volunteering Behavior.” Nature
254: 138–40.
Eckel, C. C., and Grossman, P. J. 1998. “Are Women Less Selfish Than Men?: Evidence from Dictator
Experiments.” The Economic Journal 108(448): 726-35.
Eckel, Catherine C. and Philip J. Grossman. 2001. “Chivalry and Solidarity in Ultimatum Games.” Economic
Inquiry 39(2): 171-88.
Falk, Armin; Fehr, Ernst; and Fischbacher. 2005. “Driving Forces behind Informal Sanctions.” Econometrica,
73(6): 2017-30.
Falk, Armin; Meier, Stephan; Zehnder, Christian. 2011. “Did We Overestimate the Role of Social
Preferences? The Case of Self-Selected Student Samples.” IZA Discussion Paper #5475.
39

Fehr, E., and List, J. A. 2004. “The Hidden Costs and Returns of Incentives – Trust and Trustworthiness
Among CEOs.” IEW Working Papers 134.
Fehr, E. and Schmidt, K. M. 1999. “A Theory of Fairness, Competition, and Cooperation.” Quarterly Journal
of Economics, 119(3): 817-68.
Ferraro, Paul J. and Ronald G. Cummings. 2007. “Cultural Diversity, Discrimination, and Economic
Outcomes: An Experimental Analysis.” Economic Inquiry 45(2) 217-32.
Fershtman, C. and Gneezy, U. 2001. “Discrimination in A Segmented Society: An Experimental Approach.”
The Quarterly Journal of Economics 116(1): 351-77.
Fix, Michael and Turner, Margery, eds. A National Report Card on Discrimination in America: The Role of Testing.
Washington, DC: Urban Institute Press, 1998.
Fong, C. M., Luttmer E. F. P. 2009. “Do Race and Fairness Matter in Generosity? Evidence from a
Nationally Representative Charity Experiment.” Working Paper Series rwp09-014, Harvard University, John
F. Kennedy School of Government.
Forsythe, Robert; Joel Horowitz; N.E. Savin; and Martin Sefton. 1994. “Fairness in Simple Bargaining
Experiments.” Games and Economic Behavior. 6(3): 347-369.
Gerber, Alan S., Dean Karlan, and Daniel Bergan. 2009. "Does the Media Matter? A Field Experiment
Measuring the Effect of Newspapers on Voting Behavior and Political Opinions." American Economic Journal:
Applied Economics 1(2): 35–52.
Gerber, Alan S., and Donald P. Green. 2000. "The Effects of Canvassing, Direct Mail, and Telephone
Contact on Voter Turnout: A Field Experiment." American Political Science Review 94(3):653–63.
Geronimus, A. T., Bound, J., and Neidert, L. 1996. “On the Validity of Using Census Geocode
Characteristics to Proxy Individual Socioeconomic Characteristics.” Journal of the American Statistical
Association 91(434): 529-37.
Gilens, Martin. 1999. Why Americans Hate Welfare. Chicago: University of Chicago Press.
Gneezy, U., and Rustichini, A. 2000. “Pay Enough or Don’t Pay at All.” The Quarterly Journal of Economics
115(3): 791-810.
Gneezy, U., Niederle, M., and Rustichini, A. 2003. “Performance in Competitive Environments: Gender
Differences.” The Quarterly Journal of Economics 118(3): 1049-74.
Gordon, M.E., L.A. Slade, and N. Schmitt. 1986. “The ‘Science of the Sophomore’ Revisited: From
Conjecture to Empiricism.” Academy of Management Review 11: 191-207.
Green, Donald P., and Alan S. Gerber. 2006. Can Registration-Based Sampling Improve the Accuracy of
Midterm Election Forecasts? Public Opinion Quarterly. 70(2): 197-223.
Guth, W., R. Schmittberger, and B. Schwarze. 1982. “An Experimental Study of Ultimatum Bargaining.”
Journal of Economic Behavior and Organization 3: 367-88.
40

Harrison, Glenn and List, John. 2004. “Field Experiments.” The Journal of Economic Literature. 42(4): 10091055.
Henrich, Joseph. 2000. “Does Culture Matter in Economic Behavior? Ultimatum Game Bargaining Among
the Machiguenga of the Peruvian Amazon.” The American Economic Review 90(4): 973-79.
Henrich, Joseph; Boyd, R.; Bowles, S.; Camerer, C.; Fehr, E.; Gintis, H.; and McElreath, R. “In Search of
Homo Economicus: Behavioral Experiments in 15 Small Scale Societies.” The American Economic Review.
91(2): 73-78.
Hole, A. R. 2007. “Fitting Mixed Logit Models by Using Maximum Simulated Likelihood.” Stata Journal.
7(3):388-401. RePec download of 2008 version:
http://econpapers.repec.org/software/bocbocode/s456883.htm.
Jamison, Julian; Karlan, Dean; and Schechter, Laura. 2008. “To Deceive or Not to Deceive: The Effect of
Deception on Behavior in Future Laboratory Experiments.” Journal of Economic Behavior and Organization. 68:
477-488.
Karlan, Dean and Zinman, Jonathan. 2009. “Observing Unobservables: Asymmetries with a Consumer
Credit Field Experiment.” Econometrica 77(6): 1993-2008.
Kerwin K. C.; E. Hurst; and N. Roussanov. 2009. “Conspicuous Consumption and Race.” The Quarterly
Journal of Economics 124(2): 425-67.
Kravitz, D. A. and Gunto, S. 1992. “Decisions and Perceptions of Recipients in Ultimatum Bargaining
Games.” Journal of Socio-Economics 21(1): 65-84.
Levitt, S. and List, J. “Field Experiments in Economics: The Past, the Present, and the Future.” European
Economic Review 53(January): 1-18.
Luttmer, Erzo F. P. 2001. “Group Loyalty and the Taste for Redistribution.” Journal of Political Economy,
109(3):500-28.
Lu, X.; Scheve, K. F.; and Slaughter, M. J. 2010. “Envy, Altruism, and the International Distribution of
Trade Protection.” NBER Working Paper #15700.
McClellan, M., and Skinner, J. 1999. “Medicare Reform: Who Pays, and Who Benefits?” Health Affairs
18(1):48-62.
Murnighan, J. Keith and Michael Scott Saxon. 1998. “Ultimatum Bargaining by Children and Adults.”
Journal of Economic Psychology 19: 415-45.
Nickerson, David W. 2008. "Is Voting Contagious? Evidence from Two Field Experiments," American
Political Science Review 102(Feb):49-57.
Niederle, M., and Vesterlund, L. 2007. “Do Women Shy Away From Competition? Do Men Compete Too
Much?” The Quarterly Journal of Economics 122(3): 1067-1101.
Oosterbeek, Hessel, Randolph Sloof, and Gus van de Kuilen. 2004. “Cultural Differences in Ultimatum
Game Experiments: Evidence from a Meta-Analysis.” Experimental Economics 7: 171-88.
Revelt, D. and Train, K. 1998. “Mixed Logit with Repeated Choices: Households Choices of Appliance
Efficiency Level.” The Review of Economics and Statistics, 80(4): 647-57.
41

Roth, A.E., V. Prasnikar, M. Okuno-Fujiware, and S. Zamir. 1991. “Bargaining and Market Behavior in
Jerusalem, Ljubljana, Pittsburgh, and Tokyo: An Experimental Study.” American Economic Review 81: 106895.
Roth, A. 1995. “Bargaining Experiments”. The Handbook of Experimental Economics. Princeton University.
Slonim, Robert and Alvin E. Roth. 1998. “Learning in High Stakes Ultimatum Games: An Experiment in
the Slovak Republic.” Econometrica 66(3): 569-96.
Solnick, S. 2001. “Gender Differences in the Ultimatum Game.” Economic Inquiry 39(2): 189-200.

42

Appendix 1: Survey Script
Hello, may I speak with [subject first name] [subject last name]?
I’m calling on behalf of researchers at the University of Notre Dame. You have been randomly selected to
participate in a one question study. If you agree to answer the question, you will be entered in a drawing to
win a $500 gift card from Amazon.com. You will also have an opportunity to make some money today.
Great! Just so you know, your personal information and answers will be kept confidential, used only to mail
your winnings, and will be discarded when the project is completed. Would you like to participate in the
study? [If yes, proceed, if no, thank them for their time].
Q1. You have been selected at random to play a game with [opponent first name] [opponent last name]
from [opponent city], who was also selected at random from a statewide sample.
The rules of the game are simple. [Opponent first name] was asked to propose a split of [stakes] with you. If
you accept the proposal, you will be paid that amount and [Opponent first name] will keep the rest. If you
reject the proposal, neither of you will be paid anything. Are the rules clear? [If yes, proceed, if no, repeat
the rules].
Q2. [Opponent first name] has proposed a split in which you receive [offer] of the [stakes] and [Opponent
first name] receives [Amount – Offer]. Do you accept or reject the offer?
[Indicate acceptance or rejection].
1 Accept
2 Reject
*BOTH Accept and Reject count as a COMPLETE
Thank you.
ASK EVERYONE REGARDLESS OF ACCEPT OR REJECT:
Q3. By the way, what is the smallest amount you would have been willing to accept as a division of [stakes]?
[Enter amount]
Thank you for your participation. I’d like to remind you that you have also been entered in a drawing for a
$500 Amazon gift card.
[IF OFFER WAS ACCEPTED, Q2=1] Can I confirm your name and the address where your check should
be sent? 45
[IF OFFER WAS REJECTED, Q2=2] Can I confirm your name and the address where the gift card will be
sent if your name is drawn?

Authors’ note: This was the script as it was read to participants. However, the Iowa floods of summer 2008 forced us to change
survey vendors at the last minute, as the vendor we initially contracted with was flooded and unable to do the survey as planned.
Eastern Research Services mailed cash exclusively but the script was not updated to reflect this. As this information was only
revealed at the end of the phone call, it has no bearing on selection into the sample or game behavior.
45

43

Appendix 2: Distribution of treatment profiles

Stakes
100
100
100
100
50
50
50
50
20
20
20
20
10
10
10
5
5

Offer
1
2
5
10
1
2
5
10
1
2
5
10
1
2
5
1
2

44

Number
50
100
125
100
50
100
125
100
50
125
125
50
75
125
125
75
125

Appendix 3: Construction of Racially Polarized Names
The state voter files used to draw the subject pool provided a natural population by which to
calculate racially polarized names. Using this database was consistent with our back story (i.e., both players
were randomly selected among registered voters) and also accurately reflected the distribution of names in
the subject’s social milieu (as opposed to using a national database). To calculate racial polarization of
names, we first calculated the probability a person of a give race had a particular name (i.e., what percentage
of blacks are named “John” and what percentage of whites are named “Sam”). The racial polarization of a
name is then calculated by dividing the likelihood of a person of a black person having the name divided by
the probability a white person has the same name (see Bertrand and Mullainathan, 2004).
Pr
Pr
A name was deemed racially polarized if the polarization score was greater than 10 or less than 0.10.46 In
order to select relatively common names, only names with at least 300 occurrences in each of the three state
voter files were considered. We also limited the names to be male so as to keep gender constant. Applying
these filters, there were 2 white first names, 10 black first names, 22 white first names, and 10 black last
names remained candidates. After eliminating names deemed difficult to pronounce (e.g., Hensley), shared
by prominent public figures (e.g., Helms), or possibly overly stereotypical (e.g., Tyrone), the authors selected
two first names and two last names for each race. We ultimately selected Scott (polarization = 0.072) and
Dustin (polarization = 0.097) as the “white” first names, Cedric (32.3) and Andre (21.2) as the “black” first
names, Walsh (0.058) and Snyder (0.069) as the “white” first names, and Washington (42.3) and Booker
(10.3) as “black” last names. Thus, we have four possible combinations of names for each race.47 The
polarization scores for each first and last name are shown in Tables A3.1 and A3.2, below. These tables also
report the results of a survey of 200 individuals via Amazon’s Mechanical Turk labor marketplace in which
A “black” name was deemed polarized if the polarization score was greater than 10. Similarly, a “white” name was deemed
polarized if the polarization score was less than 0.10.
47 White: Scott Walsh; Scott Snyder; Dustin Walsh; Dustin Snyder. Black: Cedric Washington; Cedric Booker; Andre Washington;
Andre Booker.
46

45

we asked respondents to rate an individual with each of our eight names as very likely black, more likely
black, equally likely black/white, more likely white, or very likely white. The survey responses show strong
polarization in the racial connotations of the names used in our survey. We suspect that the use of both a
first and last name is critical to conveying this impression. “Cedric” alone may not clearly connote race, but
“Cedric Washington” sends a clearer signal.
To give the treatment a little more detail and seem more real, we said the fictitious opponent lived in
the state’s biggest city.48 To ensure that subjects were unlikely to know anyone with our created names, we
check that no one shared those names in those cities. Across the three states, only six people shared any of
the treatment names. Thus, subjects should have viewed the names as believable but would not know
anyone sharing the same name.
Subjects of both races were randomly assigned to have a black or white opponent. One of the four
created names was then randomly assigned for each race. Thus, each subject was equally likely to have one
of the eight possibilities named as an opponent. Multiple names were created for each race to minimize the
risk that the results of the experiment were dependent on a particular name. None of the results from any
analysis conducted depends on the particular racialized name presented, so the analysis presented in the
paper will only pay attention to the race of the opponent provided rather than the particular name used.

48

Charlotte, North Carolina; Atlanta, Georgia; and, Columbia, South Carolina.

46

Table A3.1 Racial Polarization of Selected Black Names
Last
Names

First
Names

Booker

Washington

Polarization Score
[Population Frequency]

0.04
[2460]

0.07
[430]

Andre

0.08
[599]

11% likely white
74% likely black

6% likely white
87% likely black

Cedric

0.05
[386]

9% likely white
83% likely black

2% likely white
87% likely black

Table A3.2 Racial Polarization of Selected White Names
Last
Names

First
Names

Snyder

Walsh

Polarization Score
[Population Frequency]

25.6
[505]

30.3
[344]

Dustin

18.2
[365]

80% likely white
1% likely black

90% likely white
2% likely black

Scott

24.6
[2555]

85% likely white
1% likely black

89% likely white
0% likely black

Notes: Polarization score equals the percentage of whites with the name divided by the percentage of blacks with the name in the universe of
registered voters in GA, NC, and SC. Number in brackets reports the total number of registered voters in GA, SC, and NC with the name. In
the 4 x 4 subgrid, the top percentage is the share of 200 respondents to our survey on Amazon’s Mechanical Turk who thought a person with
the first/last name combination was very likely or more likely white on a five point scale. The bottom percentage is the share who thought a
person with the first/last name combination was very likely or more likely black. The difference between the sum of these shares and one is due
to respondents who thought a person with the name was equally likely to be black or white.

47

Appendix 4: Debriefing Letter to Participants
“Racial Differences in Inequality Aversion”
John D. Griffin, David Nickerson, and Abigail Wozniak, Principal Investigators
University of Notre Dame
Recently, you received a telephone call on behalf of researchers at the University of Notre Dame. Thank you for
agreeing to participate in our study. Based on how you played this game, you may have earned a payment. If
you did, it is enclosed.
In addition, we would like to briefly explain the purpose and procedure of the game you played. The purpose of
the study in which you participated was to examine racial differences in aversion to inequality. In order to
accomplish the objectives of the research and to do so at a reasonable cost, it was necessary for us to make a
misrepresentation in the game you played. Specifically, the individual you played against was actually a fictional
person and the amount you were offered was drawn from a distribution of offers from prior games.
The responses you provided will be used to examine differences in the sensitivity of racial groups in the United
States to inequality and how this sensitivity is affected by the race and gender of the individuals with which they
interact. Thank you once again for your participation.
If you would like to receive the results of this study, please contact the Principal Investigators at:
John D. Griffin
Department of Political Science
217 O’Shaughnessy
University of Notre Dame
Notre Dame, IN 46556
574‐631‐7659
John.Griffin@nd.edu

David Nickerson
Department of Political Science
217 O’Shaughnessy
University of Notre Dame
Notre Dame, IN 46556
574‐631‐7016
dnickers@nd.edu

Abigail Wozniak
Department of Economics and Econometrics
434 Flanner Hall
University of Notre Dame
Notre Dame, IN 46556
574‐631‐6208
awozniak@nd.edu

To read more about citizens’ attitudes concerning inequality, see:
Eckel, Catherine C. and Philip J.Grossman. 2001. “Chivalry and Solidarity in Ultimatum Games.” Economic
Inquiry 39(2) 171‐88.

48

Table 1: Descriptive Statistics within Black and White Subsamples
Variable

Black

Min

Max

Mean

Std dev

Min

Max

19
0
1
0.2

99
1
3
1

59.47
0.57
1.38
0.36

16.68
0.50
0.55
0.25

19
0
1
0

95
1
3
0.80

Characteristics from Merged Census Tract Data
% Single parents
11.44
5.28
0
% In poverty
16.25
10.40
0
% Single unit
67.64
19.66
5
% College grads
16.65
12.44
0
% Homeowners
68.77
19.32
2
% Urban
62.75
42.84
0
% Blue collar
49.81
15.99
10
% Professionals
24.89
12.13
0
% White collar
33.07
11.04
4
% Unemployed
3.69
2.83
0
% Hispanic
2.14
4.19
0
% Asian
0.45
1.26
0
% 65 and over
22.21
8.82
3
Med HH income
35.5
13.9
20.0
Mean years educ
11.67
1.18
8

34
54
100
77
97
100
88
76
63
24
44
11
48
65.0
16

10.47
15.43
66.29
16.51
69.95
54.02
49.73
25.26
32.95
3.18
1.86
0.52
22.95
35.6
11.65

5.60
10.40
18.77
12.77
18.82
44.86
15.60
11.80
10.77
2.72
3.21
1.57
8.35
13.9
1.14

1
0
3
0
2
0
6
0
5
0
0
0
3
20.0
8

35
62
100
81
95
100
86
63
63
16
44
21
56
65.0
16

Game and Interview Variables
Interview length
1.47
Times tried
4.77
Stakes
41.75
Offer
4.00
Share
0.18
Accept
0.37
Acceptable min
10.26
Acceptable share
0.27
Invalid min flag
0.40

5
20
100
10
0.5
1
100
1
1

1.36
4.35
41.56
3.97
0.18
0.34
8.27
0.21
0.42

0.92
3.98
35.78
2.98
0.15
0.47
15.60
0.26
0.49

0
0
5
1
0.01
0
0
0
0

6
17
100
10
0.5
1
100
1
1

Mean

Std Dev

White

Characteristics from Voter File Record
Age
57.11
15.25
Female
0.64
0.48
Household size
1.42
0.61
Neighb’d % Black
0.62
0.25

N

0.84
4.46
35.65
2.99
0.15
0.48
16.72
0.28
0.49

0
0
5
1
0.01
0
0
0
0

818

829

Notes: Data collected by Eastern Research Services via phone interviews for the authors, December 2008-January 2009. Median
household income in $1000s. Share = offer / stakes. Acceptable share = acceptable min / stakes.

49

Table 2: Comparison of selected characteristics across subsamples
A. Contacted versus non-contacted subsamples
Contacted
Black
0.49
Female
0.59
Age
58.68
Median hh income
35536
Mean years of educ
11.68
Stakes
42.0
Offer
4.01
Accept
0.35
N

Not Contacted
0.55*
0.57
51.86*
37733*
11.80*
41.8
4.00
-

2003

5666

B. Participants versus non-participants in the contacted subsample
Participated
Did not participate
Black
0.50
0.48
Female
0.60
0.55
Age
58.3*
60.44*
Median hh income
35526
35581
Mean years of educ
11.66
11.72
Stakes
41.7
43.58
Offer
4.0
4.02
Accept
0.35
N

1647

356

Notes: Contact defined as having day of interview recorded by survey firm. Participation defined as
answering accept/reject ultimatum offer (Q2). * indicates different from the mean in the contacted
(participated) subsample at the 5% level.

50

Table 3: Distribution of treatment variables across strata
Strata
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

Stakes
Mean
SE
39.04
4.22
40.25
4.05
42.43
4.23
46.75
4.03
43.49
4.24
37.21
4.16
44.36
4.33
37.66
4.14
39.43
4.81
36.17
4.09
45.58
5.01
46.02
4.58
43.44
4.13
47.63
4.23
43.24
4.18
40.71
4.09
40.66
3.85
46.08
4.23
46.16
4.58
42.19
4.48
32.24
4.53
43.77
5.11
32.27
3.98
36.10
4.13

Total

41.66

0.88

Offer
Mean
SE
4.01
0.35
3.92
0.34
4.24
0.34
4.09
0.31
3.86
0.34
3.89
0.38
4.30
0.35
3.95
0.38
3.32
0.38
3.80
0.41
3.87
0.42
4.45
0.41
4.09
0.33
4.93
0.37
4.03
0.34
4.27
0.36
3.28
0.31
4.01
0.33
4.45
0.39
3.47
0.36
3.57
0.35
3.91
0.44
3.55
0.35
3.85
0.37
3.99

0.07

Share
Mean
0.19
0.19
0.18
0.18
0.16
0.19
0.18
0.19
0.15
0.18
0.16
0.17
0.18
0.19
0.18
0.19
0.15
0.16
0.18
0.15
0.22
0.16
0.20
0.18

SE
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02
0.02

0.18

0.004

Race
Treatment
Mean
SE
0.55
0.06
0.51
0.06
0.53
0.06
0.52
0.05
0.59
0.06
0.48
0.06
0.54
0.06
0.58
0.06
0.49
0.07
0.60
0.06
0.52
0.07
0.48
0.06
0.51
0.06
0.46
0.06
0.54
0.06
0.40
0.06
0.38
0.06
0.40
0.05
0.52
0.06
0.47
0.06
0.41
0.07
0.45
0.07
0.41
0.06
0.42
0.06
0.49

0.01

N
73
79
70
91
73
61
70
64
53
60
60
64
77
80
74
70
76
83
69
64
58
53
66
59
1647

Notes: Strata defined by race of respondent (two categories), three block level income categories, and four
neighborhood racial diversity categories. Stakes and offer are randomly assigned within strata. Share equals
offer/stakes. Race treatment randomly assigned within strata-stakes-offer cells.

51

Table 4: Standard probit models of ultimatum choice
A. Offer-Relative Specification
Respondent race interactions
omitted

Respondent race interactions
included

Black

0.035
0.023
-0.008
0.14
-0.037
-0.04
-0.03
0.066
(1.50)
(0.91)
(0.26)
(1.59)
(0.53)
(0.55)
(0.39)
(0.61)
Offer
0.007
0.008
0.009
0.007
-0.002
-0.002
0.001
-0.002
(1.71)† (1.93)† (1.87)† (1.70)†
(0.69)
(0.32)
(0.09)
(0.40)
Relative
-0.44
-0.45
-0.60
-0.43
-0.44
-0.43
-0.50
-0.43
(5.60)** (5.70)** (6.20)** (5.45)** (3.92)** (3.81)** (4.15)** (3.83)**
Black*Offer
0.018
0.02
0.017
0.018
(2.21)* (2.34)* (1.78)† (2.20)*
Black*Relative
-0.005
-0.05
-0.13
0.001
(0.03)
(0.29)
(0.72)
(0.01)
Additional controls…
Tract controls
X
X
X
X
Game conditions
X
X
Strata dummies
X
X
Observations
1647
1598
1500
1647
1647
1598
1500
1647
B. Full Specification
Respondent race interactions
Respondent race interactions
omitted
included
Black
0.035
0.023
-0.008
0.14
-0.074
-0.084
-0.08
0.027
(1.50)
(0.90)
(0.27)
(1.60)
(0.94)
(1.06)
(0.88)
(0.23)
Absolute
0.001
0.001
0.001
0.001
0.001
0.001
0.002
0.001
(1.08)
(1.29)
(1.30)
(1.17)
(1.44)
(1.67)† (-1.70)
(1.54)
Offer
0.005
0.005
0.006
0.005
-0.007
-0.007
-0.005
-0.007
(1.03)
(1.16)
(1.08)
(1.00)
(1.00)
(1.02)
(0.68)
(1.04)
Relative
-0.54
-0.58
-0.74
-0.54
-0.63
-0.66
-0.80
-0.64
(4.40)** (4.61)** (5.08)** (4.38)** (3.63)** (3.73)** (4.04)** (3.66)**
Black*Absolute
-0.001
-0.001
-0.001
-0.001
(0.95)
(1.07)
(-1.02)
(1.01)
Black*Offer
0.022
0.024
0.022
0.022
(2.42)* (2.59)** (2.06)* (2.43)*
Black*Relative
0.17
0.16
0.10
0.19
(0.71)
(0.66)
(0.34)
(0.78)
Additional controls…
Tract controls
X
X
X
X
Game conditions
X
X
Strata dummies
X
X
Observations

1647

1598

1500

1647

1647

1598

1500

1647

Notes: Dependent variable is indicator for acceptance of proposed split. All equations estimated via probit; implied marginal
effects calculated at the means reported. Absolute value of robust z statistics in parentheses. † significant at 10%; * significant at
5%; ** significant at 1%.

52

Table 5: Mixed logit estimates of random coefficient choice model

Coefficient Mean
Offer
Absolute
Relative
Coefficient SD
Offer
Absolute
Relative

Full Sample

Whites

Blacks

0.033
(1.54)
-0.005
(-0.39)
-2.81
(-2.69)**

-0.003
(-0.09)
-0.016
(-0.45)
-2.26
(-1.66)†

0.064
(2.16)*
0.001
(0.28)
-2.94
(-2.48)*

0.003
(0.99)
0.028
(0.86)
3.13
(0.94)

0.001
(0.01)
0.048
(0.69)
2.58
(0.42)

0.003
(0.01)
0.00
(0.02)
2.73
(0.76)

Notes: Dependent variable is indicator of respondent’s choice between accepting and rejecting ultimatum.
Model was estimated using mixed logit routine in Stata developed by A. R. Hole (2008) with nreps set to
500. No other covariates included. Z-statistics in parentheses.

53

Table 6: Standard probit models separately by Q3 response, stakes size, and share size
A. Offer-Relative Specification
Q3 response
relative to Q2:
Additional
restrictions:
Black
Offer
Relative

Not
Not
Consistent Consistent
Consistent Consistent Consistent consistent
-

-

-

-

0.04
(0.28)
0.01
(1.58)
-1.18
(9.48)**

0.30
(1.99)*
0.002
(0.42)
0.29
(2.97)**

951

663

-0.15
(0.89)
-0.002
(0.24)
-1.26
(6.87)**
0.02
(1.89)†
0.12
(0.48)
951

0.19
(1.07)
-0.011
(1.36)
0.22
(1.37)
0.03
(1.96)*
0.11
(0.54)
663

Black*Offer
Black*Relative
Observations

All

All

Small
Stakes

Large
Stakes

Share
in 0.250.50

Share <
0.25

-0.35
(0.95)
-0.002
(0.03)
-1.13
(2.07)*
0.04
(0.55)
0.25

0.07
(0.25)
0.008
(0.67)
-0.88
(2.84)**
0.01
(0.71)
-0.26

-0.10
(0.44)
-0.004
(0.28)
-0.62
(1.80)†
0.04
(1.57)
0.20

-0.31
(0.16)
-0.001
(0.22)
-0.11
(0.36)
0.02
(1.85)†
0.02

(0.34)

(0.58)

(0.42)

(0.05)

293

658

438

1209

All

All

B. Full Game Variable Specification
Q3 response
relative to Q2:
Additional
restrictions:
Black
Absolute
Offer
Relative

Not
Not
Consistent Consistent
Consistent Consistent Consistent consistent
-

-

-

-

Small
Stakes

Large
Stakes

Share
in 0.250.50

Share
< 0.25

0.04
(0.31)
0.002
(2.71)**
0.002
(-0.24)
-1.60
(7.83)**

0.13
(1.40)
-0.001
(0.93)
0.004
(0.85)
0.40
(2.57)*

951

663

-0.20
(1.08)
0.003
(2.89)**
-0.013
(1.43)
-1.92
(6.14)**
-0.002
(-1.43)
0.03
(2.29)*
0.62
(1.48)
951

0.21
(1.12)
-0.001
(0.75)
-0.007
(0.72)
0.36
(1.39)
0.00
(0.36)
0.016
(1.43)
0.03
(0.10)
663

-0.09
(0.2)
0.05
(0.91)
-0.04
(0.6)
-2.37
(1.58)
0.01
(0.16)
0.04
(0.4)
-0.03
(0.02)
293

-0.16
(0.49)
0.004
(3.23)**
-0.01
(0.94)
-2.18
(4.57)**
-0.003
(1.5)
0.02
(1.32)
0.66
(0.93)
658

-0.07
(0.24)
-0.04
(0.96)
0.01
(0.55)
1.02
(0.58)
0.008
(0.14)
0.03
(0.89)
-0.14
(0.06)
438

-0.28
(0.96)
0.001
(1.31)
-0.009
(1.04)
-0.71
(1.31)
-0.002
(1.33)
0.027
(2.26)*
0.89
(1.15)
1209

Black*Absolute
Black*Offer
Black*Relative
Observations

Notes: Q2 is the respondent’s choice of whether to accept or reject the ultimatum offer. Q3 is the respondent’s statement about the
minimum amount s/he would have accepted. Roughly 40% of respondents who answered Q3 give minimum amounts inconsistent with
their choices in Q2. All specifications contain a full set of strata fixed effects. 36 respondents did not answer Q3. Dependent variable is
indicator for acceptance of proposed split. All equations estimated via probit; implied marginal effects calculated at the means reported.
Absolute value of robust z statistics in parentheses. † significant at 10%; * significant at 5%; ** significant at 1%.

54

Table 7: Standard probit models separately by age and sex
A. Offer-Relative Specification
Subsample:

Women

Men

Ages
18-55

Ages
55+

Black

0.067
(0.48)
-0.01
(1.31)
-0.38
(2.58)**
0.016
(1.55)
-0.014
(0.07)
992

-0.008
(0.04)
0.007
(0.78)
-0.519
(2.88)**
0.025
(1.81)+
-0.001
(0.00)
655

0.153
(0.83)
0.003
(0.32)
-0.534
(2.80)**
0.019
(1.38)
-0.064
(0.24)
715

-0.137
(1.08)
-0.003
(0.42)
-0.335
(2.40)*
0.02
(1.90)+
0.044
(0.22)
932

Offer
Relative
Black*Offer
Black*Relative
Observations

B. Full Game Variable Specification
Subsample:

Women

Men

Ages
18-55

Ages
55+

Black

-0.02
(0.13)
0.001
(1.28)
-0.015
(1.79)+
-0.605
(2.67)**
-0.002
(1.66)+
0.025
(2.18)*
0.375
(1.2)
992

-0.139
(0.73)
0.001
(0.97)
0.003
(0.33)
-0.726
(2.60)**
0.001
(0.43)
0.023
(1.45)
-0.139
(0.34)
655

0.095
(0.48)
0.002
(1.47)
-0.003
(0.28)
-0.859
(2.92)**
-0.002
(0.91)
0.024
(1.58)
0.221
(0.55)
715

0.031
(0.22)
0.001
(0.87)
-0.006
(0.80)
-0.48
(2.25)*
-0.001
(0.74)
0.024
(2.08)*
0.22
(0.71)
932

Absolute
Offer
Relative
Black*Absolute
Black*Offer
Black*Relative
Observations

Notes: Q2 is the respondent’s choice of whether to accept or reject the ultimatum offer. Q3 is the respondent’s
statement about the minimum amount s/he would have accepted. Roughly 40% of respondents who answered Q3 give
minimum amounts inconsistent with their choices in Q2. All specifications contain a full set of strata fixed effects. 36
respondents did not answer Q3. Dependent variable is indicator for acceptance of proposed split. All equations
estimated via probit; implied marginal effects calculated at the means reported. Absolute value of robust z statistics in
parentheses. † significant at 10%; * significant at 5%; ** significant at 1%.

55

Table 8: Standard probit models by income category
A. Full Specification
Black 10th
Percentile
Black
0.054
(1.33)
Absolute
0.002
(1.68)†
Offer
0.006
(0.91)
Relative
-0.71
(3.55)**
Black*Absolute

Black
Median
0.03
(0.74)
-0.001
(0.96)
0.008
(1.00)
-0.27
(1.30)

Black 90th
Percentile
0.017
(0.37)
0.001
(1.22)
0.00
(0.05)
-0.63
(2.67)**

Black*Offer
Black*Relative
Observations

614

560

473

B. Offer-Relative and Absolute-Offer Models
Offer-Relative Model
Black 10th
Black
Black 90th
Percentile Median
Percentile
Black
-0.15
-0.05
0.15
(1.38)
(0.45)
(1.14)
Absolute
Offer
Relative

-0.011
(1.13)
-0.49
(2.67)**

0.005
(0.57)
-0.587
(3.07)**

0.00
(0.04)
-0.17
(0.80)

Black*Absolute
Black*Offer
Black*Relative
Observations

0.04
(3.22)**
0.06
(0.22)
614

-0.002
(0.16)
0.29
(1.09)
560

0.009
(0.60)
-0.53
(1.71)†
473

Black 10th
Percentile
-0.20
(1.59)
0.002
(1.72)†
-0.018
(1.76)†
-0.87
(3.03)**
-0.001
(0.73)
0.049
(3.28)**
0.284
(0.71)
614

Black
Median
-0.129
(0.90)
0.00
(0.01)
0.005
(0.49)
-0.58
(1.93)*
-0.002
(1.00)
0.006
(0.35)
0.61
(1.46)
560

Black 90th
Percentile
0.16
(1.03)
0.001
(0.87)
-0.006
(0.43)
-0.383
(1.18)
0.00
(0.07)
0.009
(0.50)
-0.57
(1.19)
473

Absolute-Offer Model
Black 10th
Black
Black 90th
Percentile Median
Percentile
-0.12
0.05
-0.007
(1.61)
(0.72)
(0.09)
-0.001
-0.002
0.00
(0.98)
(2.10)*
(0.02)
-0.004
0.016
0.002
(0.41)
(1.75)†
(0.17)
0.00
(0.26)
0.04
(3.20)**

0.00
(0.07)
-0.006
(0.45)

-0.002
(1.17)
0.017
(1.14)

614

560

473

Notes: Dependent variable is indicator for acceptance of proposed split. All equations estimated via logit; implied
marginal effects calculated at the means reported. Models include only listed covariates. Absolute value of robust z
statistics in parentheses. † significant at 10%; * significant at 5%; ** significant at 1%.

56

Table 9: Raw acceptance rates by race of respondent and implied race of proposer
Proposer:
Respondent:
Black
White

Black

White

0.39
(0.02)
0.35
(0.02)

0.35
(0.02)
0.33
(0.02)

Notes: Data collected by Eastern Research Services for the authors over December 2008-January 2009. Standard errors
of mean acceptance rates within the cells are in parentheses.

Table 10: Probability of acceptance across proposer-respondent race combinations
Respondent-Proposer Race:
Black-Black

Marginal Effect
0.184
(2.00)*
0.14
(1.5)
0.026
(0.77)

0.065
(2.02)*
0.022
(0.66)
0.022
(0.64)

Strata Fixed Effects

Yes

No

Observations

1647

1647

Black-White
White-Black

Notes: Dependent variable is indicator for acceptance of proposed split. All equations estimated via probit; implied
marginal effects reported and robust z-statistics in parentheses. White-White is the omitted category. P-value of F-test
that the three reported coefficients are jointly different from zero is 0.33 in fixed effects column; 0.24 in no fixed effects
column. * significant at 5%.

57

Table 11: Standard probit model with proposer race interactions
Proposer Race
(PR) Definition:
Black
Offer
Relative
PR*Black
PR*Offer
PR*Relative
PR
Black*Offer
Black*Relative
PR*Black*Offer
PR*Black*Relative

Proposer Race=1 if
Proposer Black
0.022
(0.65)
0.008
(1.33)
-0.37
(3.37)**
0.022
(0.45)
-0.002
(0.19)
-0.13
(0.81)
0.065
(0.88)

-0.14
(1.49)
0.00
(0.02)
-0.52
(3.53)**
0.21
(1.39)
-0.005
(0.46)
0.20
(0.87)
-0.02
(0.18)
0.015
(1.33)
0.33
(1.50)
0.006
(-0.36)
-0.65
(2.03)*

Notes: Dependent variable is indicator for acceptance of proposed split. All equations estimated via probit; implied
marginal effects calculated at the means reported and absolute value of robust z-statistics in parentheses. N equals 1647
in all specifications. † significant at 10%; * significant at 5%; ** significant at 1%.

58

FIGURES 1a-1e. Figures show mean acceptance rates by offer amount separately for blacks (squares) and whites (diamonds). Vertical lines show
95% confidence intervals around mean estimates.
Stakes of 5

.2

.1

.2

.3

Acceptance rate
.3
.4

Acceptance rate
.4
.5

.5

.6

.6

Acceptance Rate over All Games

1

2

5

10

1

2

5

Offer
Black Mean

10
Offer

White Mean

Black Mean

1a.

White Mean

1b.
Stakes of 20

.2

.2

.3

Acceptance rate
.4
.5

Acceptance rate
.4
.6
.8

.6

1

.7

Stakes of 10

1

2

5

10

1

Offer
Black Mean

1c.

2

5

10
Offer

White Mean

Black Mean

1d.
59

White Mean

0

0

Acceptance rate
.2
.4

Acceptance rate
.2
.4

.6

Stakes of 100

.6

Stakes of 50

1

2

5

10

1

Offer
Black Mean

1e.

2

5

10
Offer

White Mean

Black Mean

1f.

60

White Mean

FIGURES 2a-2e. Figures show mean acceptance rates by share separately for blacks (squares) and whites (diamonds). Vertical lines show 95%
confidence intervals around mean estimates.
Offer of 1

0

0

.2

Acceptance rate
.2
.4

Acceptance rate
.4
.6

.6

.8

Acceptance Rate over All Games

0

.1

.2

.3

.4

.5

0

.05

.1
Share

Share
Black Mean

White Mean

Black Mean

2a.

.15

.2

White Mean

2b.
Offer of 5

.1

.1

.2

.2

Acceptance rate
.3
.4

Acceptance rate
.3
.4
.5

.5

.6

.6

Offer of 2

0

.1

.2
Share
Black Mean

2c.

.3

.4

0

.1

.2

.3
Share

White Mean

Black Mean

2d.
61

White Mean

.4

.5

.2

Acceptance rate
.4
.6
.8

1

Offer of 10

.1

.2

.3
Share
Black Mean

.4

.5

White Mean

2e.

62

