                               NBER WORKING PAPER SERIES




 DYNAMICS OF INFORMATION EXCHANGE IN ENDOGENOUS SOCIAL NETWORKS

                                        Daron Acemoglu
                                        Kostas Bimpikis
                                        Asuman Ozdaglar

                                       Working Paper 16410
                               http://www.nber.org/papers/w16410


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                   September 2010




We thank seminar participants at Columbia, Cornell, Microsoft Research, MIT, Stanford, University
of Chicago, University of Michigan, the SED Meeting at Montreal and the Quebec Workshop on the
Economics of Social Networks for useful comments and suggestions. The views expressed herein
are those of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

Â© 2010 by Daron Acemoglu, Kostas Bimpikis, and Asuman Ozdaglar. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including Â© notice, is given to the source.
Dynamics of Information Exchange in Endogenous Social Networks
Daron Acemoglu, Kostas Bimpikis, and Asuman Ozdaglar
NBER Working Paper No. 16410
September 2010
JEL No. D83,D85

                                              ABSTRACT

We develop a model of information exchange through communication and investigate its implications
for information aggregation in large societies. An underlying state determines payoffs from different
actions. Agents decide which others to form a costly communication link with incurring the associated
cost. After receiving a private signal correlated with the underlying state, they exchange information
over the induced communication network until taking an (irreversible) action. We define asymptotic
learning as the fraction of agents taking the correct action converging to one in probability as a society
grows large. Under truthful communication, we show that asymptotic learning occurs if (and under
some additional conditions, also only if) in the induced communication network most agents are a
short distance away from "information hubs", which receive and distribute a large amount of information.
Asymptotic learning therefore requires information to be aggregated in the hands of a few agents.
We also show that while truthful communication may not always be a best response, it is an equilibrium
when the communication network induces asymptotic learning. Moreover, we contrast equilibrium
behavior with a socially optimal strategy profile, i.e., a profile that maximizes aggregate welfare. We
show that when the network induces asymptotic learning, equilibrium behavior leads to maximum
aggregate welfare, but this may not be the case when asymptotic learning does not occur. We then
provide a systematic investigation of what types of cost structures and associated social cliques (consisting
of groups of individuals linked to each other at zero cost, such as friendship networks) ensure the emergence
of communication networks that lead to asymptotic learning. Our result shows that societies with too
many and sufficiently large social cliques do not induce asymptotic learning, because each social clique
would have sufficient information by itself, making communication with others relatively unattractive.
Asymptotic learning results if social cliques are neither too numerous nor too large, in which case
communication across cliques is encouraged.


Daron Acemoglu                                        Asuman Ozdaglar
Department of Economics                               Department of Electrical Engineering
MIT, E52-380B                                         and Computer Science
50 Memorial Drive                                     Massachusetts Institute of Technology
Cambridge, MA 02142-1347                              77 Massachusetts Ave, E40-130
and CIFAR                                             Cambridge, MA 02139
and also NBER                                         asuman@mit.edu
daron@mit.edu

Kostas Bimpikis
MIT - Sloan School of Management
50 Memorial Drive
Cambridge, MA 02142
kostasb@MIT.EDU
1    Introduction
Most social decisions, ranging from product and occupational choices to voting and political behavior,
rely on information agents gather through communication with friends, neighbors and co-workers as
well as information obtained from news sources and prominent webpages. A central question in social
science concerns the dynamics of communication and information exchange and whether such dynam-
ics lead to the effective aggregation of dispersed information. Our objective in this paper is to develop
a tractable benchmark model to study the dynamics of belief formation and information aggregation
through communication and the choices that individuals make concerning whom to communicate with.
A framework for the study of these questions requires communication to be strategic, time-consuming
and/or costly, since otherwise all information could be aggregated immediately by simultaneous com-
munication among the agents. Our approach focuses on dynamic and costly communication (and we
also allow strategic communication, though this turns out to be less important in the present context).
    An underlying state of the world determines which action has higher payoff (which is assumed to
be the same for all agents). Because of discounting, earlier actions are preferred to later ones. Each
agent receives a private signal correlated with the underlying state. In addition, she can communicate
with others, but such communication first requires the formation of a communication link, which
may be costly. Therefore, our framework combines elements from models of social learning and
network formation. The network formation decisions of agents induce a communication graph for
the society. Thereafter, agents communicate with those whom they are connected to until they take
an irreversible action. Crucially, information acquisition takes time because the â€œneighborsâ€ of an
agent with whom she communicates acquire more information from their own neighbors over time.
Information exchange will thus be endogenously limited by two features: the communication network
formed at the beginning of the game, which allows communication only between connected pairs, and
discounting, which encourages agents to take actions before they accumulate sufficient information.
    We characterize the equilibria of this network formation and communication game and then inves-
tigate the structure of these equilibria as the society becomes large (i.e., for a sequence of games). Our
main focus is on how well information is aggregated, which we capture with the notion of asymptotic
learning. We say that there is asymptotic learning if the fraction of agents taking the correct action
converges to one (in probability) as the society becomes large.
    Our analysis proceeds in several stages. First, we take the communication graph as given and
assume that agents are non-strategic in their communication, i.e., they disclose truthfully all the
information they possess when communicating. Under these assumptions, we provide a condition that
is sufficient and (under an additional mild assumption) necessary for asymptotic learning. Intuitively,
this condition requires that most agents are a short distance away from information hubs, which are




                                                    1
agents that have a very large (in the limit, infinite) number of connections.1 Two different types
of information hubs can be conduits of asymptotic learning in our benchmark model. The first are
information mavens who receive communication from many other agents, enabling them to aggregate
information. If most agents are close to an information maven, asymptotic learning is guaranteed.
The second type of hubs are social connectors who communicate to many agents, enabling them to
spread their information widely.2 Social connectors are only useful for asymptotic learning, if they
are close to mavens so that they can distribute their information. Thus, asymptotic learning is also
obtained if most agents are close to a social connector, who is in turn a short distance away from a
maven. The intuition for why such information hubs and almost all agents being close to information
hubs are necessary for asymptotic learning is instructive: were it not so, a large fraction of agents
would prefer to act before waiting for sufficient information to arrive. But then a nontrivial fraction
of those would take the incorrect action, and moreover, they would also disrupt the information flow
for the agents to whom they are connected. The advantage of the first part of our analysis is that it
enables a relatively simple characterization of equilibria and the derivation of intuitive conditions for
asymptotic learning.
       Second, we show that even if individuals misreport their information (which they may want to do
in order to delay the action of their neighbors and obtain more information from them in future com-
munication), it is an equilibrium of the strategic communication game to report truthfully whenever
truthful communication leads to asymptotic learning. Interestingly, the converse is not necessarily
true: strategic communication may lead to asymptotic learning in some special cases in which truthful
communication precludes learning. From a welfare perspective, we show a direct connection between
asymptotic learning and the maximum aggregate welfare that can be achieved by any strategy profile:
when asymptotic learning occurs, all equilibria are (asymptotically) socially efficient, i.e., they achieve
the maximum welfare. However, when asymptotic learning does not occur, equilibrium behavior can
lead to inefficiencies that arise from the fact that agents do not internalize the positive effect of delay-
ing their action and continuing information exchange. Thus, our analysis identifies a novel information
externality that is a direct product of the agents being embedded in a network: the value of an agent
to her peers does not only originate from her initial information but also from the paths she creates
between different parts of the network through her social connections. It is precisely the destruction
of these paths when the agent takes an action that may lead to a welfare loss in equilibrium.
       Our characterization results on asymptotic learning can be seen both as â€œpositiveâ€ and â€œnegativeâ€.
On the one hand, to the extent that most individuals obtain key information from either individuals or
news sources (websites) approximating such hubs, efficient aggregation of information may be possible
   1
      We also derive conditions under which , Î´-asymptotic learning occurs at an equilibrium strategy profile. We say that
, Î´-asymptotic learning occurs when at least 1 âˆ’  fraction of the population takes an -optimal action with probability
at least 1 âˆ’ Î´.
    2
      Both of these terms are inspired by Gladwell (2000).



                                                            2
in some settings. We show in particular that hierarchical graph structures where agents in the higher
layers of the hierarchy can communicate information to many agents at lower layers lead to asymptotic
learning.3 On the other hand, communication structures that do not feature such hubs appear more
realistic in most contexts, including communication between friends, neighbors and co-workers.4 Our
model thus emphasizes how each agentâ€™s incentive to act sooner rather than later makes information
aggregation significantly more difficult.
       Third, armed with the analysis of information exchange over a given communication network, we
turn to the study of the endogenous formation of this network. We assume that the formation of
communication links is costly, though there also exist social cliques, groups of individuals that are
linked to each other at zero cost. These can be thought of as â€œfriendship networksâ€ that are linked
for reasons unrelated to information exchange and thus act as conduits of such exchange at low cost.
Agents have to pay a cost at the beginning in order to communicate (receive information) from those
who are not in their social clique. Even though network formation games have several equilibria, the
structure of our network formation and information exchange game enables us to obtain relatively
sharp results on what types of societies lead to endogenous communication networks that ensure
asymptotic learning. In particular, we show that societies with too many (disjoint) and sufficiently
large social cliques induce behavior inconsistent with asymptotic learning. The reason why relatively
large social cliques may discourage efficient aggregation of information is that because they have
enough information, communication with others (from other social cliques) becomes unattractive, and
as a consequence, the society gets segregated into a large number of disjoint social cliques that do not
share information. In contrast, asymptotic learning obtains in equilibrium if social cliques are neither
too numerous nor too large so that it is worthwhile for at least some members of these cliques to
communicate with members of other cliques, forming a structure in which information is shared across
(almost) all members of the society.
       These results also illustrate an interesting feature of the information exchange process: an agentâ€™s
willingness to perform costly search (which here corresponds to forming a link with another social
clique) is decreasing with the precision of the information that is readily accessible to her. This gives a
natural explanation for informational segregation: agents do not internalize the benefits for the group
of forming an additional link, leading to a socially inefficient information exchange structure. It further
suggests a form of informational Braessâ€™ paradox,5 whereby the introduction of additional information
may have adverse effects for the welfare of a society by discouraging the formation of additional links
for information sharing (see also Morris and Shin (2002) and Duffie, Malamud, and Manso (2009) for
   3
     An additional challenge when significant information is concentrated in the hands of a few hubs may arise because
of misalignment of interests, which our approach ignores.
   4
     In particular, the popular (though not always empirically plausible) random graph models such as preferential
attachment and Poisson (ErdoÌ‹s-Renyi) graphs do not lead to asymptotic learning.
   5
     In the original Braessâ€™ paradox, the addition of a new road may increase the delays faced by all motorists in a Nash
equilibrium.


                                                           3
a related result). Consider, for example, the website of a film critic that can be viewed as a good
but still imprecise information source (similar to a reasonable-sized social clique in our model). Other
agents can access the criticâ€™s information and form an opinion about a movie quickly. However, this
precludes information sharing among the agents and may lead to a decrease in the aggregate welfare.
   Our paper is related to several strands of the literature on social and economic networks. First,
it is related to the large and growing literature on social learning. Much of this literature focuses
on Bayesian models of observational learning, where each individual learns from the actions of others
taken in the past. A key impediment to information aggregation in these models is the fact that actions
do not reflect all of the information that an individual has and this can induce a pattern reminiscent
to a â€œherd,â€ where individuals ignore their own information and copy the behavior of others (see,
for example, Bikhchandani, Hirshleifer, and Welch (1992), Banerjee (1992), and Smith and SÃ¸rensen
(2000), as well as Bala and Goyal (1998), for early contributions, and Smith and SÃ¸rensen (2010),
Banerjee and Fudenberg (2004) and Acemoglu, Dahleh, Lobel, and Ozdaglar (2010) for models of
Bayesian learning with richer observational structures). While observational learning is important in
many situations, a large part of information exchange in practice is through communication.
   Several papers in the literature study communication, though typically using non-Bayesian or â€œmy-
opicâ€ rules (for example, Ellison and Fudenberg (1995), DeMarzo, Vayanos, and Zwiebel (2003) and
Golub and Jackson (2010)). A major difficulty faced by these approaches, often precluding Bayesian
and dynamic game theoretic analysis of learning in communication networks, is the complexity of
updating when individuals share their ex-post beliefs (because of the difficulty of filtering out com-
mon sources of information). We overcome this difficulty by adopting a different approach, whereby
individuals can directly communicate their signals and there is no restriction on the total â€œbitsâ€ of
communication. This leads to a tractable structure for updating of beliefs and enables us to study per-
fect Bayesian equilibria of a dynamic game of network formation, communication and decision-making.
It also reverses one of the main insights of these papers, also shared by the pioneering social learning
work by Bala and Goyal (1998), that the presence of â€œhighly connectedâ€ or â€œinfluentialâ€ agents, or
what Bala and Goyal (1998) call a â€œroyal family,â€ acts as a significant impediment to the efficient
aggregation of information. On the contrary, in our model the existence of such highly connected
agents (information hubs, mavens or connectors) is crucial for the efficient aggregation of information.
Moreover, the existence of such â€œhighly connectedâ€ also reduces incentives for non-truthful communi-
cation, and is the key input into our result that truthful communication can be an equilibrium. The
recent paper by Duffie, Malamud, and Manso (2009) is also noteworthy: in their model agents are
randomly matched according to endogenously determined search intensities, and because they focus
on an environment with a continuum of agents, communication of beliefs in their setup is equivalent
to exchanging signals, and thus enables them to avoid the issues arising in the previous literature.
Their main focus is on characterizing equilibrium search intensities as a function of the information


                                                   4
that an agent already has access to. In contrast to our work, there is no explicit network structure.
Finally, Mobius, Phan, and Szeidl (2010) empirically compare a non-Baysian model of communication
(similar to the one adopted by Golub and Jackson (2010)) with a model in which, similar to ours,
signals are communicated and agents are Bayesian. Although their study is not entirely conclusive
on whether agents behave according to one or the other model, their evidence broadly supports the
Bayesian alternative.
   Our work is also related to the growing literature on network formation, since communication takes
place over endogenously formed networks. Bala and Goyal (2000) model strategic network formation as
a non-cooperative game and study its equilibria under various assumptions on the benefits of forming
a link. In particular, they distinguish between one-way and two-way flow of benefits, depending
on whether a link benefits only the agent that decides to form it or both participating agents. They
identify a number of simple structures that arise at equilibrium: the empty network, the wheel, the star
and the complete network. More recently, Galeotti, Goyal, and Kamphorst (2006) and Galeotti (2006)
study the role of heterogeneity among agents in the network structures that arise at equilibrium.
Closer to our work is Hojman and Szeidl (2008) who study a network formation model where the
benefits from connecting to other agents have decreasing returns to scale (which is also the case in
our model of information exchange because of endogenous reasons). The main focus of the network
formation literature is on characterizing equilibrium structures and comparing them with patterns
observed in real world networks (e.g., small distances between agents, high centrality etc.). However,
in most of the literature the benefits and costs associated with forming a link are exogenous. A novelty
in our work is that the benefits of forming links are endogenously determined through the subsequent
information exchange. Our focus is also different: although we also obtain characterization results
on the shape of the network structures that arise in equilibrium (which are similar to those in the
literature), our focus is on whether these structure lead to asymptotic learning. Interestingly, while
network formation games have a large number of equilibria, the simple structure of our model enables
us to derive relatively sharp results about environments in which the equilibrium networks lead to
asymptotic learning.
   Finally, our paper is related to the literature on strategic communication, pioneered by the cheap
talk framework of Crawford and Sobel (1982). While cheap talk models have been used for the study
of information aggregation with one receiver and multiple senders (e.g. Morgan and Stocken (2008))
and multiple receivers and single sender (e.g. Farrell and Gibbons (1989)), most relevant to our paper
are two recent papers that consider strategic communication over general networks, Galeotti, Ghiglino,
and Squintani (2010) and Hagenbach and Koessler (2010). A major difference between these works
and ours is that we consider a model where communication is allowed for more than one time period,
thus enabling agents to receive information outside their immediate neighborhood (at the cost of a
delayed decision) and we also endogenize the network over which communication takes place. On the


                                                   5
other hand, our framework assumes that an agentâ€™s action does not directly influence othersâ€™ payoffs,
while such payoff interactions are the central focus of Galeotti, Ghiglino, and Squintani (2010) and
Hagenbach and Koessler (2010). Our paper is also related to the existing work by Ambrus, Azevedo,
and Kamada (2010), where the sender and the receiver communicate strategically through a chain of
intermediators. Their primary focus is information intermediation, thus communication takes place
over multiple rounds but it is restricted on a ordered line from the sender to the receiver, where each
agent only sends information once.
    The rest of the paper is organized as follows. Section 2 develops a general model of information
exchange among rational agents, that are embedded in a communication network. Also, it introduces
the two main environments we study. Section 3 contains our main results on social learning given
a communication graph. It also includes a welfare discussion that draws the connection between
learning and efficient communication. Finally, it illustrates how our results can be applied to a number
of random graph models. Section 4 incorporates endogenous network formation to the information
exchange model. Our main result in this section shows the connection between incentives to form
communication links and asymptotic learning. Section 5 concludes. All proofs are presented in the
Appendix.

2     A Model of Information Exchange in Social Networks
In the first part of the paper, we focus on modelling information exchange among agents over a given
communication network. In the second part (Section 4), we investigate the question of endogenous
formation of this network. We start by presenting the information exchange model for a finite set
N n = {1, 2, Â· Â· Â· , n} of agents. We also describe the limit economy as n â†’ âˆž.
2.1   Actions, Payoffs and Information

Each agent i âˆˆ N n chooses an irreversible action xi âˆˆ R. Her payoff depends on her action and an
underlying state of the world Î¸ âˆˆ R, which is an exogenous random variable. In particular, agent iâ€™s
payoff when she takes action xi and the state of the world is Î¸ is given by f (xi , Î¸) = Ï€ âˆ’ (x âˆ’ Î¸)2 ,
where Ï€ is a constant.
    The state of the world Î¸ is unknown and agents observe noisy signals about it. In particular, we
assume that Î¸ is drawn from a Normal distribution with known mean Âµ and precision Ï. Each agent
receives a private signal si = Î¸ + zi , where the zi â€™s are idiosyncratic and independent from one another
and Î¸, with common mean ÂµÌ„ (normalized to 0) and precision ÏÌ„.
2.2   Communication

Our focus is on information aggregation, when agents are embedded in a network that imposes com-
munication constraints. In particular, agent i forms beliefs about the state of the world from her
private signal si , as well as information she obtains from other agents through a given communication



                                                    6
network Gn , which, as will be described shortly, represents the set of communication constraints im-
posed on them. We assume that time t âˆˆ [0, âˆž) is continuous and there is a common discount rate
r > 0. Communication times are stochastic. In particular, communication times are exponentially
distributed with parameter Î» > 0.6 At a given time instant t, agent i decides whether to take action
xi (and receive payoff f (xi , Î¸) discounted by eâˆ’rt ) or â€œwaitâ€ to obtain more information in subsequent
communication rounds from her peers. Throughout the rest of the paper, we say that the agent â€œexitsâ€
at time t, if she chooses to take the irreversible action at time t. Discounting implies that an earlier
exit is preferred to a later one. We define Uin as the discounted payoff of agent i (from the viewpoint
of time t = 0) when the size of the society is n. For example, when the underlying state is Î¸ and the
agent takes action xi at time t, we would have that

                                            Uin = eâˆ’rt Ï€ âˆ’ (xi âˆ’ Î¸)2 .
                                                                    


       As mentioned above, each agent obtains information from other agents through a communication
network represented by a directed graph Gn = (N n , E n ), where E n is the set of directed edges with
which agents are linked. We say that agent j can obtain information from i or that agent i can send
information to j if there is an edge from i to j in graph Gn , i.e., (i, j) âˆˆ E n . Let Ii,t
                                                                                         n denote the

                                          n denote the set of all possible information sets. Then, for
information set of agent i at time t and Ii,t
every pair of agents i, j, such that (i, j) âˆˆ E n , we say that agent j communicates with agent i or that
agent i sends a message to agent j, and define the following mapping

                                        mnij,t : Ii,t
                                                  n
                                                      â†’ Mnij,t for (i, j) âˆˆ E n ,

where Mnij,t âŠ† Rn denotes the set of messages that agent i can send to agent j at time t. Note that
without loss of generality the k-th component of mnij,t represents the information that agent i sends to
agent j at time t regarding the signal of agent k 7 . Moreover, the definition of mnij,t captures the fact that
communication is directed and is only allowed between agents that are linked in the communication
network, i.e., j communicates with i if and only if (i, j) âˆˆ E n . The direction of communication should
be clear: when agent j communicates with agent i, then agent i sends a message to agent j, that could
in principle depend on the information set of agent i as well as the identity of agent j.
       Importantly, we assume that the cardinality (â€œdimensionalityâ€) of Mnij,t is such that communi-
cation can take the form of agent i sharing all her information with agent j. This has two key
implications. First, an agent can communicate (indirectly) with a much larger set of agents than just
her immediate neighbors, albeit with a time delay. For example, the second time agent j communicates
   6
      Equivalently, agents â€œwakeâ€ up and communicate simultaneously with their neighbors, when a Poisson clock with
rate Î» ticks.
    7
      As will become evident in subsequent discussion, we assume that communication involves exchange of signals and not
posterior beliefs. Moreover, information is tagged, i.e., the receiver of the message understands that its k-th component
is associated with agent k.



                                                           7
with agent i, then j can send information not just about her direct neighbors, but also their neighbors
(since presumably she obtained such information during the first communication). Second, mechanical
duplication of information can be avoided. In particular, the second time agent j communicates with
agent i, she can repeat her original signal, but this is not recorded as an additional piece of information
by agent j, since given the size of the message space Mnij,t , each piece of information is â€œtaggedâ€.
This ensures that there need be no confounding of new information and previously communicated
information.
   Let Tt denote the set of times that agents communicated with their neighbors before time t. That
defines the information set of agent i at time t > 0 as:

                        n
                       Ii,t = {si , mnji,Ï„ , for all Ï„ âˆˆ Tt and j such that (j, i) âˆˆ E n }

     n = {s }. In particular, the information set of agent i at time t > 0 consists of her private signal
and Ii,0   i

and all the messages her neighbors sent to i in previous communication times. Agent iâ€™s action at
time t is a mapping from her information set to the set of actions, i.e.,

                                         Ïƒ ni,t : Ii,t
                                                   n
                                                       â†’ {â€œwaitâ€} âˆª R.

   The tradeoff between taking an irreversible action and waiting, should be clear at this point. An
agent would wait, in order to communicate indirectly with a larger set of agents and choose a better
action. On the other hand, future is discounted, therefore, delaying is costly.
   We close the section with a number of definitions. We define a path between agents i and j in
network Gn as a sequence i1 , Â· Â· Â· , iK of distinct nodes such that i1 = i, iK = j and (ik , ik+1 ) âˆˆ E n for
k âˆˆ {1, Â· Â· Â· , K âˆ’ 1}. The length of the path is defined as K âˆ’ 1. Moreover, we define the distance of
agent i to agent j as the length of the shortest path from i to j in network Gn , i.e.,

                     distn (i, j) = min{length of P     P is a path from i to j in Gn }.

Finally, the k-step neighborhood of agent i is defined as

                                         n
                                        Bi,k = {j     distn (j, i) â‰¤ k},

       n = {i}, i.e., B n consists of all agents that are at most k links away from agent i in graph
where Bi,0             i,k
Gn . Intuitively, if agent i waits for k communication steps and all of the intermediate agents receive
                                                                                                   n .
and communicate information truthfully, i will have access to all of the signals of the agents in Bi,k
2.3   Equilibria of the Information Exchange Game

We refer to the game defined above as the Information Exchange Game. We next define the equilibria
of the information exchange game Î“inf o (Gn ) for a given communication network Gn . We use the



                                                       8
standard notation Ïƒ âˆ’i to denote the strategies of agents other than i and we let Ïƒ i,âˆ’t denote the vector
of actions of agent i at all times except t. Also, let PÏƒ and EÏƒ denote the conditional probability,
conditional expectation respectively when agents behave according to profile Ïƒ.

Definition 1. An action strategy profile Ïƒ n,âˆ— is a pure-strategy perfect Bayesian Equilibrium of the
information exchange game Î“inf o (Gn ) if for every i âˆˆ N n and time t, Ïƒ n,âˆ—
                                                                          i,t maximizes the expected
payoff of agent i given the strategies of other agents Ïƒ n,âˆ—
                                                         âˆ’i , i.e.,


                             Ïƒ n,âˆ—
                               i,t âˆˆ arg      max         E((y,Ïƒn,âˆ— ),Ïƒn,âˆ— ) (Uin Ii,t
                                                                                   n
                                                                                       ).
                                           yâˆˆ{â€œwaitâ€}âˆªR         i,âˆ’t   âˆ’i



We denote the set of equilibria of this game by IN F O(Gn ).

   For the remainder, we refer to a pure-strategy perfect Bayesian Equilibrium simply as an equilib-
rium (we do not study mixed strategy equilibria). It is important to note here that although equilibria
depend on the discount rate r, we do not explicitly condition on r (through the use of a subscript) for
convenience.
   If agent i decides to exit and take an action at time t, then the optimal action would be:

                                 xn,âˆ—                       n            n
                                  i,t = arg max E[f (x, Î¸) Ii,t ] = E[Î¸ Ii,t ],
                                                x

where the second equality holds as f (x, Î¸) = Ï€ âˆ’ (x âˆ’ Î¸)2 . Since actions are irreversible, the agentâ€™s
decision problem reduces to determining the timing of her action. It is straightforward to see that
at equilibrium an agent takes the irreversible action immediately after some communication step
concludes. Thus, an equilibrium strategy profile Ïƒ induces an equilibrium timing profile Ï„ n,Ïƒ , where
Ï„ n,Ïƒ
  i   designates the communication step after which agent i exits by taking an irreversible action. The
Ï„ notation is convenient to use for the statement of some of our results below. Finally, similar to
 n , we define the k-step neighborhood of agent i under equilibrium Ïƒ as follows: a path P Ïƒ between
Bi,k
agents i and j in Gn under Ïƒ is a sequence i1 , Â· Â· Â· , iK of distinct nodes such that i1 = i, iK = j,
(ik , ik+1 âˆˆ E n ) and Ï„ n,Ïƒ
                         ik â‰¥ k âˆ’ 1, which ensures that the information from j will reach agent i before
any of the agents in the path take an irreversible action. Then, we can define

       distn,Ïƒ (i, j) = min{length of P Ïƒ     P Ïƒ is a path from i to j in Gn under equilibrium Ïƒ}

and
                                       n,Ïƒ
                                      Bi,k = {j      distn,Ïƒ (j, i) â‰¤ k}.

2.4   Assumptions on the Information Exchange Process

The communication model described in Section 2.2 is fairly general. In particular, the model does not
restrict the set of messages that an agent can send. Throughout, we maintain the assumption that the
communication network Gn is common knowledge. Also, we focus on the following two environments

                                                          9
              4                                              4                                             4
                                                                  s4
                                                                 s2                                             s3
              2        1                                     2         1                                   2           1
                                                        s3        s5                                                       (s6 , s7 )
              3        5                                     3         5                                   3           5
                                                                              s7
                                                                  s6
                       6         7                                     6           7                                   6           7
                  I1,0 = (s1 )                           I1,1 = (s1 , s2 , s4 , s5 )               I1,2 = (s1 , s2 , s4 , s5 , s3 , s6 , s7 )

          (a) Time t = 0.               (b) First communication step.                        (c) Second communication step.

                  Figure 1: The information set of agent 1 under truthful communication.


defined by Assumptions 1 and 2 respectively.

Assumption 1 (Truthful Communication). Communication between agents is truthful, i.e.,

                                                  if |Tt | â‰¤ Ï„ n,Ïƒ
                                     n
                                     mÌ‚ ij,t
                           mnij,t =                            i
                                     mÌ‚n ij,Ï„ n,Ïƒ
                                              i
                                                  otherwise.

and
                                                                            if distn,Ïƒ
                                                             
                                       (mÌ‚n
                                                                 s`                i,` â‰¤ |Tt |
                                              ij,t )`   =
                                                                 âˆˆR         otherwise



    Intuitively, this assumption compactly imposes three crucial features: (1) Communication takes
place by sharing signals, so that when agent j communicates with agent i at time t, then agent i
sends to j all the information agent i has obtained thus far (refer to Figure 1 for an illustration of the
communication process centered at a particular agent); (2) Agents cannot strategically manipulate the
messages they sent, i.e., an agentâ€™s private signal is hard information. Moreover, they cannot refuse to
disclose the information they possess; (3) When an agent takes an irreversible action, then she no longer
obtains new information and, thus, can only communicate the information she has obtained until the
time of her decision. The latter feature captures the fact that an agent, who engages in information
exchange to make a decision, would have weaker incentives to collect new information after reaching
that decision. Nevertheless, she can still communicate the information she had previously obtained to
other agents. An interesting consequence of this feature is that it imposes dynamically new constraints
to communication: agent i can communicate with agent j only if there is a directed path between
them in the original communication network Gn and the agents in the path do not exit early. We call
this type of communication truthful to stress the fact that the agents cannot strategically manipulate
the information they communicate.8 We discuss the implications of relaxing Assumption 1 by allowing
  8
    Yet another variant of this assumption would be that agents exit the social network after taking an action and stop
communicating entirely. In this case, the results are essentially identical when their action is observed by their neighbors.


                                                                       10
strategic communication in Subsection 3.4.
2.5     Learning in Large Societies

We are interested in whether equilibrium behavior leads to information aggregation. This is captured
by the notion of â€œasymptotic learningâ€, which characterizes the behavior of agents over communication
networks with growing size.
    We consider a sequence of communication networks {Gn }âˆž            n    n   n
                                                          n=1 , where G = {N , E }, and refer
to this sequence of communication networks as a society. A sequence of communication networks
induces a sequence of information exchange games, and with a slight abuse of notation we use the
term equilibrium to denote a sequence of equilibria of the information exchange games, or of the society
{Gn }âˆž                                            n âˆž                            n          n
     n=1 . We denote such an equilibrium by Ïƒ = {Ïƒ }n=1 , which designates that Ïƒ âˆˆ IN F O(G ) for
all n. For any fixed n â‰¥ 1 and any equilibrium of the information exchange game Ïƒ n âˆˆ IN F O(Gn ),
we introduce the indicator variable:
                       
                         1 if agent i takes an action that is -close to the optimal,
              Min, =                                                                                                     (1)
                         0 otherwise.

In other words, Min, = 1 (for some ) if and only if agent i chooses irreversible action xi , such that
|xi âˆ’ Î¸| â‰¤ .
Next definition introduces , Î´-asymptotic learning for a given society.9

Definition 2. We say that , Î´-asymptotic learning occurs in society {Gn }âˆž
                                                                          n=1 along equilibrium Ïƒ if
we have:
                                                         n
                                                   "                   #   !
                                                       1X
                                      lim PÏƒ               (1 âˆ’ Min, ) >  < Î´.
                                     nâ†’âˆž               n
                                                        i=1
This definition states that , Î´-asymptotic learning occurs when the probability that at least (1 âˆ’ )-
fraction of the agents take an action that is -close to the optimal action (as the society grows infinitely
large) is at least 1 âˆ’ Î´.

Definition 3. We say that perfect asymptotic learning occurs in society {Gn }âˆž
                                                                             n=1 along equilibrium
Ïƒ if we have:
                                                         n
                                                   "                 #  !
                                                       1X        n,
                                      lim PÏƒ               (1 âˆ’ Mi ) >  = 0.
                                     nâ†’âˆž               n
                                                        i=1

for any  > 0.

    Perfect asymptotic learning is naturally a stronger definition (corresponding to  and Î´ being
arbitrarily small in the definition of , Î´-asymptotic learning) and requires all but a negligible fraction
of the agents taking the optimal action in the limit as n â†’ âˆž.
However, if their action is not observable, then the analysis needs to be modified in particular, there exist other equilibria
where several agents might exit together expecting others to exit. We do not analyze these variants in the current version
to save space.
   9
     Note that we could generalize Definition 2 by introducing yet another parameter  and study , Î´, Î¶-asymptotic learning,
in which case we would require that limnâ†’âˆž PÏƒ n1 n                    n,
                                                      P                       
                                                          i=1 (1 âˆ’ Mi ) > Î¶ < Î´.



                                                              11
3     Learning and Efficient Communication
In this section, we present our main results on learning and discuss their implications for the aggregate
welfare. Before doing so, we discuss the decision problem of a single agent, i.e., determining the best
time to take an irreversible action given that the rest of the agents behave according to strategy profile
Ïƒ. Later, we contrast the single agent problem with that of a social planner, whose objective is to
maximize the expected aggregate welfare. The analysis in the next three subsections assumes that
communication is truthful (cf. Assumption 1).
3.1   Agent iâ€™s problem

The (non-discounted) expected payoff of agent i taking an action after observing k truthful private
signals (including her own) is given by:
                                                         1
                                                 Ï€âˆ’           ,
                                                      Ï + ÏÌ„k
where recall that Ï, ÏÌ„ are the precisions of the state Î¸ and the idiosyncratic noise respectively. To
see this, note that if agent i takes her irreversible action, then the optimal such action would be
          n ] and the associated non-discounted payoff would be equal to:
Î¸Ì‚ = E[Î¸ Ii,t

                                                                    k
                                                                                         !
                                                                    X s(i)                            1
         E[Ï€ âˆ’ (Î¸Ì‚ âˆ’ Î¸)2 Ii,t
                          n                       n
                              ] = Ï€ âˆ’ var(Î¸Ì‚ âˆ’ Î¸ Ii,t ) = Ï€ âˆ’ var                  n
                                                                              âˆ’ Î¸ Ii,t       =Ï€âˆ’           ,
                                                                          k                        Ï + ÏÌ„k
                                                                    i=1

where s(i) denotes the i-th signal observed by the agent and Î¸Ì‚ is equal to the sum of k private signals
normalized by k.
                                                                                       n and assuming
    By the principle of optimality, the value function for agent i at information set Ii,t
that the rest of the agents behave according to profile Ïƒ is given by:
                               1
                     (
      n n
                       Ï€ âˆ’ Ï+ÏÌ„k n,Ïƒ                   (when she takes the optimal irreversible action),
 EÏƒ (Ui Ii,t ) = max    âˆ’rdt
                                 i,t
                                     n n         n
                       e     E[EÏƒ (Ui Ii,t+dt ) Ii,t ] (when she decides to wait, i.e., x = â€œwaitâ€),
       n,Ïƒ
where ki,t denotes the number of distinct private signals agent i has observed up to time t. The first
line is equal to the expected payoff for the agent when she chooses the optimal irreversible action
                       n , i.e., E[Î¸|I n ], and she has observed k n,Ïƒ private signals, while the second
under information set Ii,t            i,t                         i,t
line is equal to the discounted expected continuation payoff.
    The following lemma states that an agentâ€™s optimal action takes the form of a threshold rule: there
                     n,Ïƒ âˆ—
exists a threshold (ki,T |t|
                             ) , such that an agent decides to take an irreversible action at time t as long
                                n,Ïƒ âˆ—
as she has observed more that (ki,T |t|
                                        ) private signals. Like all other results in the paper, the proof
of this lemma is provided in the Appendix.

Lemma 1. Suppose Assumption 1 holds. Given communication network Gn and equilibrium Ïƒ âˆˆ
                                                                               n,Ïƒ âˆ— âˆž
IN F O(Gn ), there exists a sequence of signal thresholds for each agent i, {(ki,Ï„ ) }Ï„ =0 , that depend on
the current communication round, the identity of the agent i, the communication network Gn and Ïƒ

                                                      12
                                                                     n by taking action xn (I n ) defined
such that agent i maximizes her expected utility at information set Ii,t                 i,t i,t
as                                                                     n,Ïƒ     n,Ïƒ âˆ—
                                                           n ],
                                                       E[Î¸ Ii,t    if ki,t â‰¥ (ki,|T    ) ,
                               xni,t (Ii,t
                                       n
                                           )   =                                    t|
                                                       â€œwaitâ€,     otherwise,

     A consequence of Lemma 1 is that an equilibrium strategy profile Ïƒ defines both a time in which
agent i acts (immediately after communication step Ï„ n,Ïƒ
                                                     i ), but also the number of signals that agent i
has access to when she acts.
3.2    Asymptotic Learning

We begin the discussion by introducing the concepts that are instrumental for asymptotic learning:
the observation radius and k-radius sets. Recall that an equilibrium of the information exchange game
on communication network Gn , Ïƒ n âˆˆ IN F O(Gn ), induces a timing profile Ï„ n,Ïƒ , such that agent i
takes an irreversible action after Ï„ n,Ïƒ
                                     i   communication steps. We call Ï„ n,Ïƒ
                                                                        i   the observation radius of
agent i under equilibrium profile Ïƒ n . We also define agent iâ€™s perfect observation radius, Ï„ ni , as the
communication round that agent i would exit assuming that all other agents never exit. Note that
an agentâ€™s perfect observation radius is equilibrium independent and depends only on the network
structure. On the other hand, Ï„ n,Ïƒ
                                i   is an endogenous object and depends on both the network as well
as the specific equilibrium profile Ïƒ. Given the notion of an observation radius, we define k-radius sets
(and similarly perfect k-radius sets) as follows.

Definition 4. Let Vkn,Ïƒ be defined as

                                         Vkn,Ïƒ = {i âˆˆ N             n,Ïƒ
                                                                   Bi,Ï„ n,Ïƒ â‰¤ k}.
                                                                       i


We refer to Vkn,Ïƒ as the k-radius set (along equilibrium Ïƒ). Similarly, we refer to

                                               Vkn = {i âˆˆ N         n
                                                                   Bi,Ï„ n, â‰¤ k}
                                                                       i


as the perfect k-radius set.

     Intuitively, Vkn,Ïƒ includes all agents that take an action before they receive signals from more than
k other individuals at equilibrium Ïƒ. Equivalently, the size of their (indirect) neighborhood by the
time they take an irreversible action is no greater than k. From Definition 4 it follows immediately
that
                                       i âˆˆ Vkn,Ïƒ â‡’ i âˆˆ Vkn,Ïƒ
                                                         0   for all k 0 > k.                         (2)

    The following proposition provides a necessary and a sufficient condition for , Î´-asymptotic learning
                                                                             Rx       2
to occur in a society under equilibrium profile Ïƒ. Recall that erf (x) = âˆš2Ï€ 0 eâˆ’t dt denotes the error
function of the normal distribution.

Proposition 1. Suppose Assumption 1 holds. Then,

                                                              13
 (a) , Î´-asymptotic learning does not occur in society {Gn }âˆž
                                                             n=1 under equilibrium profile Ïƒ if there
      exists k > 0 such that
                                                                   r         !
                                   1                                   kÏÌ„
                        Î· = lim sup Â· Vkn,Ïƒ >  and erf                         < (1 âˆ’ Î´)(1 âˆ’ /Î·).     (3)
                               nâ†’âˆž n                                    2

 (b) , Î´-asymptotic learning occurs in society {Gn }âˆž
                                                     n=1 under equilibrium profile Ïƒ if there exists
      k > 0 such that
                                                                       r         !
                                      1                                    kÏÌ„             Î´( âˆ’ Î¶)
                          Î¶ = lim inf   Â· Vkn,Ïƒ <  and erf                         >1âˆ’            .    (4)
                                 nâ†’âˆž n                                      2               1âˆ’Î¶

   This proposition provides conditions such that , Î´-asymptotic learning takes place (or does not take
place). Intuitively, asymptotic learning is precluded if there exists a significant fraction of the society
that takes an action before seeing a large set of signals, since in this case there is a large enough
probability that these agents will take an action far away from the optimal one. The proposition
quantifies the relationship between the fraction of agents taking actions before seeing a large set of
signals and the quantities  and Î´. Because agents are estimating a normal random variable from noisy
observations (where the noise is also normally distributed), their probability of error is captured by
the error function erf (x), which is naturally decreasing in the number of observations. In particular,
the probability thatan
                      qagent with k signals takes an action at least  away from the optimal action
                        kÏÌ„
is no less than erf  2 (see Lemma 2 in the Appendix), and this enables us to characterize the
fraction of agents that will take an action at least  away from the optimal one in terms of the set
Vkn,Ïƒ as well as  and Î´. We thus obtain sufficient conditions for both , Î´-learning to take place and
for it to be incomplete. Finally, recall that equilibria and subsequently k-radius sets depend on the
discount rate (thus, different discount rates result in different answers for , Î´-learning).
   Proposition 1 is stated in terms of the sets Vkn,Ïƒ , which depend on the equilibrium (as the condition-
ing on Ïƒ makes clear). Our next proposition provides a necessary and sufficient condition for perfect
asymptotic learning to occur in any equilibrium profile as a function of only exogenous objects, i.e.,
the perfect k-radius sets, that depend exclusively on the original network structure. Before stating the
proposition, we define the notion of leading agents. Intuitively, a society contains a set of leading agents
if there is a negligible fraction of the agents (the leading agents) whose actions affect the equilibrium
behavior of a much larger set of agents (the followers). Let indegin = |Bi,1
                                                                         n |, outdeg n = {j i âˆˆ B n }
                                                                                    i            j,1
denote the in-degree, out-degree of agent i in communication network Gn respectively.

Definition 5. A collection {S n }âˆž
                                 n=1 of sets of agents is called a set of leading agents if

                                                n
  (i) There exists k > 0, such that S nj âŠ† Vk j for all j âˆˆ J, where J is an infinite index set.

 (ii) limnâ†’âˆž   1
               n   Â· S n = 0, i.e., the collection {S}âˆž
                                                      n=1 contains a negligible fraction of the agents as the


                                                      14
                                        Â·Â·Â·


                                              A1
                                                        1         A


                                                        ..
                                                         .


                                        Â·Â·Â·            n          B


                                              An

                            Figure 2: Leading agents and asymptotic learning.


     society grows.

               1
(iii) limnâ†’âˆž   n   Â· Sfnollow > , for some  > 0, where Sfnollow denotes the set of followers of S n . In
     particular,
                              Sfnollow = {i there exists j âˆˆ S n such that j âˆˆ Bi,1
                                                                                n
                                                                                    }.

Proposition 2. Suppose Assumption 1 holds. Then,

  (i) Perfect asymptotic learning occurs in society {Gn }âˆž
                                                         n=1 in any equilibrium Ïƒ if

                                                             1
                                              lim lim          Â· Vkn = 0.                              (5)
                                              kâ†’âˆž nâ†’âˆž        n

 (ii) Conversely, if condition (5) does not hold for society {Gn }âˆž
                                                                  n=1 and the society does not contain
     a set of leading agents, then perfect asymptotic learning does not occur in any equilibrium Ïƒ.

   Proposition 2 is not stated as an if and only if result because the fact that condition (5) does not
hold in a society does not necessarily preclude perfect asymptotic learning in the presence of leading
agents. In particular, depending on their actions, a large set of agents may exit early before obtaining
enough information to learn, or delay their actions and learn. Figure 2 clarifies this point: if the
leading agents (agents A and B) delay their irreversible decision for one communication round, then
a large fraction of the rest of the agents (agents 1 to n) may take (depending on the discount rate)
an irreversible action as soon as they communicate with the leading agents and their neighbors (i.e.,
after the second communication round concludes), thus, perfect asymptotic learning fails. However, if
the leading agents do not â€œcoordinate,â€ then they exit early and this may lead the rest of the agents
to take a delayed (after the third communication round), but more informed action. Generally, in the
presence of leading agents, asymptotic learning may occur in all or some of the induced equilibria,

                                                     15
even when condition (5) does not hold.
       In the rest of this section, we present two corollaries that help clarify the intuition of the asymptotic
learning result and identify the role of certain types of agents on information spread in a given society.
We focus on perfect asymptotic learning, since we can obtain sharper results, though we can state
similar corollaries for , Î´-asymptotic learning for any  and Î´. All corollaries are again expressed in
terms of the original network topology.10
       In particular, Corollary 1 identifies a group of agents, that is crucial for a society to permit
asymptotic learning: information mavens, who have high in-degrees and can thus act as effective
aggregators of information (a term inspired by Gladwell (2000)). Information mavens are one type
of hubs the importance of which is clearly illustrated by our learning results. Our next definition
formalizes this notion.

Definition 6. Agent i is called an information maven of society {Gn }âˆž
                                                                     n=1 if i has an infinite in-degree,
i.e., if
                                                  lim indegin = âˆž.
                                                 nâ†’âˆž

Let MAVEN ({Gn }âˆž                                           n âˆž
                n=1 ) denote the set of mavens of society {G }n=1 .


       For any agent j, let dMAVEN
                             j
                                   ,n
                                      denote the shortest distance defined in communication network Gn
between j and a maven k âˆˆ MAVEN ({Gn }âˆž                    n
                                      n=1 ). Finally, let W denote the set of agents that are at
distance at most equal to their perfect observation radius from a maven in communication network
Gn , i.e., W n = {j     dMAVEN
                         j
                               ,n
                                  â‰¤ Ï„ nj }.
       The following corollary highlights the importance of information mavens for asymptotic learning.
Informally, it states that if almost all agents have a short path to a maven, then asymptotic learning
occurs.

Corollary 1. Suppose Assumption 1 holds. Then, asymptotic learning occurs in society {Gn }âˆž
                                                                                          n=1 if

                                                       1
                                                 lim     Â· W n = 1.
                                                nâ†’âˆž    n

       Corollary 1 thus clarifies that asymptotic learning is obtained when there are information mavens
and almost all agents are at a â€œshort distanceâ€ away from one (less than their observation radius).
       As mentioned in the Introduction, a second type of information hub also plays an important role
in asymptotic learning. While mavens have high in-degree and are thus able to effectively aggregate
dispersed information, they may not be in the right position to distribute this aggregated information.
If so, even in a society that has several information mavens, a large fraction of the agents may not
benefit from their information. Social connectors, on the other hand, are defined as agents with a high
  10
    The corollaries are stated under the additional assumption, that the in-degree of an agent is non-decreasing with n.
This is simply a technicality that allows us to simplify the statement of the corollaries.



                                                          16
out-degree, and thus play the role of spreading the information aggregated by the mavens. Before
stating the proposition, we define social connectors.

Definition 7. Agent i is called a social connector of society {Gn }âˆž
                                                                   n=1 if i has an infinite out-degree,
i.e., if
                                           lim outdegin = âˆž.
                                          nâ†’âˆž

The following corollary illustrates the role of social connectors for asymptotic learning.

Corollary 2. Suppose Assumption 1 holds. Consider a society {Gn }âˆž
                                                                 n=1 , such that the set of infor-
mation mavens does not grow at the same rate as the society itself, i.e.,

                                          MAVEN ({Gn }âˆž
                                                      n=1 )
                                    lim                     = 0.
                                   nâ†’âˆž         n

Then, for asymptotic learning to occur, the society should contain a social connector within a short
distance to a maven, i.e.,

                             dMAVEN
                              i
                                    ,n
                                       â‰¤ Ï„ ni , for some social connector i.




    Corollary 2 thus states that unless a large fraction of the agents belongs to the set of mavens and,
subsequently, the rest can obtain information directly from a maven, then, information aggregated
at the mavens is spread through the out-links of a connector (note that an agent can be both a
maven and a connector). These two corollaries highlight two ways in which society can achieve perfect
asymptotic learning. First, it may contain several information mavens who not only collect and
aggregate information but also distribute it to almost all the agents in the society. Second, it may
contain a sufficient number of information mavens, who pass their information to social connectors,
and almost all the agents in the society are a short distance away from social connectors and thus
obtain accurate information from them. This latter pattern has a greater plausibility in practice than
one in which the same agents collect and distribute dispersed information. For example, if a website or
a news source can rely on information mavens (journalists, researchers or analysts) to collect sufficient
information and then reach a large number of individuals, then information can be aggregated.
    The results summarized in Propositions 1 and 2 as well as in Corollaries 1 and 2 can be seen
both as positive and negative, as already noted in the Introduction. On the one hand, communication
structures that do not feature information mavens (or connectors) do not lead to perfect asymptotic
learning, and information mavens may be viewed as unrealistic or extreme. On the other hand, as
already noted above, much communication in modern societies happens through agents that play the
role of mavens and connectors (see again Gladwell (2000)). These are highly connected agents that are
able to collect and distribute crucial information. Perhaps more importantly, most individuals obtain

                                                   17
                                                                   Layer 1


                                                                          Layer 2




                                                                       Layer 3


                                              Figure 3: Hierarchical Society.


some of their information from news sources, media, and websites, which exist partly or primarily for
the purpose of acting as information mavens and connectors.11
3.3    Asymptotic Learning in Random Graphs

As an illustration of the results we outlined in Subsection 3.2, we apply them to hierarchical graphs, a
class of random graphs defined below. Note that in the present section we assume that communication
                                                               n then j âˆˆ B n .
networks are bidirectional, or equivalently that if agent i âˆˆ Bj,1         i,1


Definition 8 (Hierarchical graphs). A sequence of communication networks {Gn }âˆž            n
                                                                              n=1 , where G =
{N n , E n }, is called Î¶-hierarchical (or simply hierarchical) if it was generated by the following process:

  (i) Agents are born and placed into layers. In particular, at each step n = 1, Â· Â· Â· , a new agent is
       born and placed in layer `.

 (ii) Layer index ` is initialized to 1 (i.e., the first node belongs to layer 1). A new layer is created
                                                                                                                        1
       (and subsequently the layer index increases by one) at time period n â‰¥ 2 with probability                      n1+Î¶
                                                                                                                           ,
       where Î¶ > 0.

(iii) Finally, for every n we have

                                    p
            P ((i, j) âˆˆ E n ) =   |N n
                                        ,   independently for all i, j âˆˆ N n that belong to the same layer `,
                                     `|



       where N`n denotes the set of agents that belong to layer ` at step n and p scalar, such that
       0 < p < 1. Moreover,

                                      1             X
              P ((i, k) âˆˆ E n ) =          and             P ((i, k) âˆˆ E n ) = 1 for all i âˆˆ N`n , k âˆˆ N<`
                                                                                                        n
                                                                                                           , ` > 1,
                                    |N<` |
                                                   kâˆˆN<`

              n denotes the set of agents that belong to a layer with index lower than ` at step n.
       where N<`
  11
    For example, a news website such as cnn.com acts as a connector that spreads the information aggregated by
the journalists-mavens to interested readers. Similarly, a movie review website, e.g., imdb.com, spreads the aggregate
knowledge of movie reviewers to interested movie aficionados.


                                                              18
    Intuitively, a hierarchical sequence of communication networks resembles a pyramid, where the
top contains only a few agents and as we move towards the base, the number of agents grows. The
following argument provides an interpretation of the model. Agents on top layers can be thought of as
â€œspecialâ€ nodes, that the rest of the nodes have a high incentive to connect to. Moreover, agents tend
to connect to other agents in the same layer, as they share common features with them (homophily).
As a concrete example, academia can be thought of as such a pyramid, where the top layer includes
the few institutions, then next layer includes academic departments, research labs and finally at the
lower levels reside the home pages of professors and students.

Proposition 3. Suppose Assumption 1 holds and consider society {Gn }âˆž
                                                                    n=1 . There exist rÌ„ > 0 and a
function Î¶(Î·) such that perfect asymptotic learning occurs in society {Gn }âˆž
                                                                           n=1 with probability at least
1 âˆ’ Î·, if the sequence of communication networks {Gn }âˆž
                                                      n=1 is Î¶(Î·)âˆ’hierarchical and the discount rate
r < rÌ„.

    The probability Î· that perfect asymptotic learning fails is related here to the stochastic process
that generated the graph. The results presented provide additional insights on the conditions under
which asymptotic learning takes place. It can also be proved that the popular preferential attachment
and ErdoÌ‹s-Renyi graphs do not lead to asymptotic learning (we omit these results to save space). This
can be interpreted as implying that asymptotic learning is unlikely in several important networks.
Nevertheless, these network structures, though often used in practice, do not provide a good description
of the structure of many real life networks. In contrast, our results show that asymptotic learning takes
place in hierarchical graphs, where â€œspecialâ€ agents are likely to receive and distribute information to
lower layers of the hierarchy. Although this result is useful in pointing out certain structures where
information can be aggregated efficiently, our analysis on the whole suggests that the conditions for
both perfect asymptotic learning and for , Î´-learning are somewhat stringent.
3.4       Strategic Communication

Next we explore the implications of relaxing the assumption that agents cannot manipulate the mes-
sages they send. In particular, we replace Assumption 1 with the following:

Assumption 2 (Strategic Communication). Communication between agents is strategic if

                                              mnij,t âˆˆ Rn ,

for all agents i, j and time t.

    This assumption makes it clear that in this case the messages need not be truthful. Allowing
strategic communication adds an extra dimension in an agentâ€™s strategy, since the agent can choose
to â€œlieâ€ about (part) of her information set in the hope that this increases her expected payoff. Note
that, in contrast with â€œcheap talkâ€ models, externalities in our framework are purely informational

                                                   19
                                9                                                   6     3

                               10       8       A          B         1          2         4

                               11                                                   7     5

          Figure 4: Agents may have an incentive to misreport/not disclose their information.


as opposed to payoff relevant. Thus, an agent may have an incentive to â€œlieâ€ as a means to obtain
more information from the information exchange process (by inducing a later exit decision from her
neighbors).
    Figure 4 illustrates how incentives for non-truthful communication may arise. Here, agent B may
have an incentive not to disclose her information to agent A. In particular, for a set of parameter
values we have that if agent B is truthful to A, then A takes an action after the first communication
round. On the other hand, if B does not disclose her information to A, then A waits for an additional
time period and B obtains access to the information of agents 9, 10 and 11.
    Let (Ïƒ n , mn ) denote an action-message strategy profile, where mn = {mn1 , Â· Â· Â· , mnn } and mni =
[mnij,Ï„ ]t=0,1,Â·Â·Â· , for j such that i âˆˆ Bj,1
                                          n . Also let P n n refer to the conditional probability when agents
                                                        Ïƒ ,m

behave according to the action-message strategy profile (Ïƒ n , mn ).

Definition 9. An action-message strategy profile (Ïƒ n,âˆ— , mn,âˆ— ) is a pure-strategy perfect Bayesian Equi-
librium of the information exchange game Î“inf o (Gn ) if for every i âˆˆ N n and communication round Ï„ ,
we have
                     E(Ïƒn,âˆ— ,mn,âˆ— ) (Uin Ii,t
                                          n)â‰¥E
                                              ((Ïƒ n        n       n,âˆ—    n     n     n,âˆ—     (Uin Ii,t
                                                                                                    n ),
                                                    i,Ï„ ,Ïƒ i,âˆ’Ï„ ,Ïƒ âˆ’i ),(mi,Ï„ ,mi,âˆ’Ï„ mâˆ’i ))


for all mni,Ï„ , mni,âˆ’Ï„ , and Ïƒ ni,Ï„ , Ïƒ ni,âˆ’Ï„ . We denote the set of equilibria of this game by IN F O(Gn ).

    Similarly we extend the definitions of asymptotic learning [cf. Definitions 2 and 3]. We show that
strategic communication does not harm perfect asymptotic learning. The main intuition behind this
result is that it is weakly dominant for an agent to report her private signal truthfully to a neighbor
with a high in-degree (maven), as long as others are truthful to the maven.

Proposition 4. If perfect asymptotic learning occurs in society {Gn }âˆž
                                                                     n=1 under Assumption 1, then
there exists an equilibrium (Ïƒ, m), such that perfect asymptotic learning occurs in society {Gn }âˆž
                                                                                                 n=1
along equilibrium (Ïƒ, m) when we allow strategic communication (cf. under Assumption 2).

    This proposition therefore implies that the focus on truthful reporting was without much loss of
generality as far as perfect asymptotic learning is concerned. In any communication network in which
there is perfect asymptotic learning, even if agents can strategically manipulate information, there is
arbitrarily little benefit in doing so. Thus, the main lessons about asymptotic learning derived above
apply regardless of whether communication is strategic or not.

                                                          20
                                       Â·Â·Â·


                                             A1
                                                         1        A


                                                          ..
                                                           .


                                       Â·Â·Â·               n        B


                                             An

                    Figure 5: Strategic communication may lead to better actions.


   However, this proposition does not imply that all learning outcomes are identical under truth-
ful and strategic communication. In particular, interestingly, as illustrated in Figure 5, strategic
communication may lead agents to take a better action with higher probability than under non-
strategic communication (cf. Assumption 1). The main reason for this (counterintuitive) fact is that
under strategic communication an agent may delay taking an action compared to the non-strategic
environment. Therefore, the agent obtains more information from the communication network and,
consequently, chooses an action, that is closer to optimal. In particular, in the example illustrated in
Figure 5, if agents A, B decide not to disclose their information, then agents 1, Â· Â· Â· , n may delay their
action so as to communicate with the neighbors of A1 , Â· Â· Â· , An and thus take an action based on more
information.
3.5   Welfare

In this subsection, we turn to the question of efficient communication and compare equilibrium al-
locations (communication and action profiles in equilibrium) with the timing of agentsâ€™ actions and
communications that would be dictated by the welfare-maximizing social planner. We identify con-
ditions under which a social planner can improve over an equilibrium strategy profile. In doing so,
we illustrate that communication over social networks might be inefficient because agents do not
internalize the positive externality that delaying their action generates for their peers.
   A social planner whose objective is to maximize the aggregate expected welfare of the population
of n agents can implement the timing profile that is a solution to the optimization:
                                                   n
                                                   X
                                             max
                                              n
                                                         Espn [Uin ]                                     (6)
                                             sp
                                                   i=1

   We call the resulting timing profile as the optimal allocation and we denote it by spn = (Ï„ n,sp          n,sp
                                                                                               1 , Â· Â· Â· , Ï„ n ).



                                                     21
    Similarly with the asymptotic analysis for equilibria, we define a sequence of optimal allocations for
societies of growing size, sp = {spn }âˆž
                                      n=1 . We are interested in identifying conditions under which the
social planner can / cannot achieve an asymptotically better allocation than an equilibrium (sequence
of equilibria) Ïƒ, i.e., we are looking at the expression:
                                                       n                 n
                                       P                     P
                                          iâˆˆN n Espn [Ui ] âˆ’  iâˆˆN n EÏƒ [Ui ]
                                   lim                                       .
                                  nâ†’âˆž                     n

The next proposition shows a direct connection between learning and efficient communication.

Proposition 5. Consider society {Gn }âˆž
                                     n=1 . If perfect asymptotic learning occurs at the optimal allo-
cation sp = {spn }âˆž
                  n=1 , then all equilibria are asymptotically efficient, i.e.,

                                                    n                 n
                                    P                     P
                                       iâˆˆN n Espn [Ui ] âˆ’  iâˆˆN n EÏƒ [Ui ]
                               lim                                        = 0,
                              nâ†’âˆž                      n

for all equilibria Ïƒ.

Therefore if perfect learning occurs at the optimal allocation, then perfect learning occurs in all
equilibria Ïƒ.
    We next provide a partial converse to Proposition 5. Before stating this result, we contrast the
decision problem an individual agent i with that of the social planner. With a slight abuse of notation,
Uin (k, Ïƒ) denotes the expected payoff of agent i when agents behave according to profile Ïƒ and the
agent has observed k signals. Agent i decides to take an irreversible action at time t and not to wait
for an additional dt, when other agents behave according to Ïƒ, if (cf. Appendix)
                                            !
                      r+Î»            1                n,Ïƒ     n,Ïƒ              n,Ïƒ
                             Ï€âˆ’         n,Ïƒ   â‰¥ Uin (ki,t + |Bi,|T   |+1 | âˆ’ |Bi,|T t|
                                                                                       |, Ïƒ)                       (7)
                       Î»         Ï + ÏÌ„ki,t                        t



Similarly, in the corresponding optimal allocation agent i exits at time t and does not wait if:
                             !
  r+Î»                1
           Ï€âˆ’           n,sp
    Î»           Ï + ÏÌ„ki,t
                  n,sp       n,sp            n,sp
                                                              X
         â‰¥ Uin (ki,t   + |Bi,|T t |+1
                                      | âˆ’ |B i,|Tt | |, sp) +   Esp [Ujn i â€œwaitsâ€ at t] âˆ’ Esp [Ujn i â€œexitsâ€ at t],
                                                         j6=i
                                                                                                                   (8)

The comparison of (7) to (8) shows the reason for why equilibria may be inefficient in this setting:
when determining when to act, agent i does not take into account the positive externality that a later
action exerts on others. This externality is expressed by the summation on the right hand side of (8).
We next derive sufficient conditions under which a social planner outperforms an equilibrium allocation
                                          n and Ï„ n,Ïƒ > Ï„ n,Ïƒ + 1, which implies that B n
Ïƒ. Consider agents i and j such that i âˆˆ Bj,1                                                     n
                                                                                               âŠƒ Bi,Ï„
                                                  j       i                            j,Ï„ n,Ïƒ        n,Ïƒ
                                                                                                        j          i
(i.e., agent j communicates with a superset of the agents that i communicates with before taking an
                    n,Ïƒ
action). Also, let kij,Ï„ n,Ïƒ denote the additional agents that j would observe if i delayed her irreversible
                         i



                                                          22
action by dt and communication took place. Then, the aggregate welfare of the two agents increases
if the following condition holds:

                    n,Ïƒ         n,Ïƒ                n,Ïƒ         n,Ïƒ               n,Ïƒ           r + Î» n n,Ïƒ
              Ujn (kj,Ï„ n,Ïƒ + k
                                ij,Ï„ n,Ïƒ
                                         ) + Uin (ki,Ï„ n,Ïƒ + k
                                                              ij,Ï„ n,Ïƒ
                                                                       ) > Ujn (kj,Ï„ n,Ïƒ ) +        Ui (ki,Ï„ n,Ïƒ ),        (9)
                       j           i                 i              i               j            Î»           i


            n,Ïƒ                                          n,Ïƒ
   Let set Dk,` denote the following set of agents: j âˆˆ Dk,` , if

       n,Ïƒ
  (i) kj,Ï„ n,Ïƒ â‰¤ k.
          j

                                 n,Ïƒ
 (ii) There exists an agent i âˆˆ Bj,1 such that

       (i) Ï„ n,Ïƒ
             j   > Ï„ n,Ïƒ
                     i   + 1.

       (ii) If i exits at Ï„ n,Ïƒ
                            i   + 1, then j gains access to at least an additional ` signals.

                  n,Ïƒ
Intuitively, set Dk,` contains agents that would obtain higher payoff in expectation if one of their
neighbors delayed taking her irreversible action. In particular, under equilibrium profile Ïƒ, agent
     n,Ïƒ
j âˆˆ Dk,` takes an action after observing at most k signals. If her neighbor i delayed her action by one
communication round, then she would have access to at least k + ` signals by the time of her action.
   The following proposition provides a sufficient condition for an equilibrium to be inefficient.
                                                                                                                        n,Ïƒ
                                                                                                                      |Dk,` |
Proposition 6. Consider society {Gn }âˆž                         n âˆž
                                     n=1 and equilibrium Ïƒ = {Ïƒ }n=1 . Assume that limnâ†’âˆž                               n       >
Î¾ > 0, for k, ` that satisfy the following:

                                       r          2           r 1
                                         Ï€+               < 2+           .
                                       Î»    Ï + ÏÌ„(k + `)      Î» Ï + ÏÌ„k

Then, there exists an Î¶ > 0, such that
                                                  n                 n
                                  P                     P
                                     iâˆˆN n Espn [Ui ] âˆ’  iâˆˆN n EÏƒ [Ui ]
                              lim                                       > Î¶,
                             nâ†’âˆž                     n

i.e., equilibrium Ïƒ is asymptotically inefficient. Moreover, there exist , Î´ such that , Î´-asymptotic
learning fails at equilibrium Ïƒ.

   We close this section with a discussion on the implications of increasing the information that agents
have access to at the beginning of the information exchange process. Consider the following setting:
agents at time t = 0 have access to k public signals in addition to their private signal. This results
in the following tradeoff: on the one hand, agents are better informed about the underlying state,
but then, on the other hand, they will have less incentive to delay taking an action and thus obtain
and share information with others. In particular, one can show that when all agents have access to
the same k public signals, then information sharing will be reduced compared to a setting without
public signals, in the sense that agents take an irreversible action earlier. Moreover, in some cases
the presence of public signals leads to a strictly smaller aggregate welfare. Thus, more information


                                                               23
is not necessarily better for the aggregate welfare of the agents. This result is similar to those in
Duffie, Malamud, and Manso (2009) and in Morris and Shin (2002), both of which show how greater
availability of public information may reduce welfare.

4    Network Formation
We have so far studied information exchange among agents over a given communication network
Gn = (N n , E n ). We now analyse how this communication network emerges. We assume that link
formation is costly. In particular, communication costs are captured by an n Ã— n nonnegative matrix
C n , where Cij
             n denotes the cost that agent i has to incur in order to form the directed link (j, i) with

agent j. As noted previously, a linkâ€™s direction coincides with the direction of the flow of messages.
In particular, agent i incurs a cost to form in-links. We refer to C n as the communication cost
matrix. We assume that Ciin = 0 for all i âˆˆ N n . Our goal in this section is to provide conditions
under which the network structures that emerge as equilibria of the network formation game defined
below guarantee asymptotic learning. Our results indicate that easy access to information (i.e., low
cost to form links with some information sources) may preclude asymptotic learning, as it reduces
the incentives for further information sharing. Moreover, asymptotic learning may depend on how
well agents coordinate at equilibrium: we show that there may be multiple equilibria that induce
sparser/denser network structures and lead to different answers for asymptotic learning.
    We define agent iâ€™s link formation strategy, gin , as an n-tuple such that gin âˆˆ {0, 1}n and gij
                                                                                                  n = 1

implies that agent i forms a link with agent j. The cost agent i has to incur if she implements strategy
gin is given by
                                                              X
                                           Cost(gin ) =              n
                                                                    Cij    n
                                                                        Â· gij .
                                                              jâˆˆN

The link formation strategy profile       gn   =   (g1n , Â· Â· Â·   , gnn ) induces the communication network Gn =
(N n , E n ), where (j, i) âˆˆ E n if and only if gij
                                                 n = 1.

    We extend our environment to the two-stage Network Learning Game Î“(C n ), where C n denotes
the communication cost matrix. The two stages of the network learning game can be described as
follows:
Stage 1 [Network Formation Game]: Agents choose their link formation strategies. The link
formation strategy profile g n induces the communication network Gn = (N n , E n ).
We refer to stage 1 of the network learning game, when the communication cost matrix is C n as the
network formation game and we denote it by Î“net (C n ).
Stage 2 [Information Exchange Game]: Agents communicate over the induced network Gn as
studied in previous sections.
    We next define the equilibria of the network learning game Î“(C n ). Note that we use the standard
notation gâˆ’i and Ïƒ âˆ’i to denote the strategies of agents other than i. Also, we let Ïƒ i,âˆ’t denote the



                                                            24
vector of actions of agent i at all times except t.

Definition 10. A pair (g n,âˆ— , Ïƒ n,âˆ— ) is a pure-strategy perfect Bayesian Equilibrium of the network
learning game Î“(C n ) if

 (a) Ïƒ n,âˆ— âˆˆ IN F O(Gn ), where Gn is induced by the link formation strategy g n,âˆ— .

 (b) For all i âˆˆ N n , gin,âˆ— maximizes the expected payoff of agent i given the strategies of other agents
       n,âˆ—
      gâˆ’i  , i.e.,
                           gin,âˆ— âˆˆ arg     max                        n,âˆ—
                                                       EÏƒ [Î i (gin , gâˆ’i  )] â‰¡ EÏƒ (Uin Ii,0
                                                                                        n
                                                                                            ) âˆ’ Cost(gin ).
                                         gin âˆˆ{0,1}n

                                                                                        n,âˆ—
      for all Ïƒ âˆˆ IN F O(GÌƒn ), where GÌƒn is induced by link formation strategy (gin , gâˆ’i  ).

We denote the set of equilibria of this game by N ET (C n ).

   Similar to the analysis of the information exchange game, we consider a sequence of communication
cost matrices {C n }âˆž
                    n=1 , where for fixed n,

                                                      n+1
                      C n : N n Ã— N n â†’ R+ and Cij
                                                n
                                                   = Cij  for all i, j âˆˆ N n .                                           (10)

   For the remainder of the section, we focus our attention to the social cliques communication cost
structure. The properties of this communication structure are stated in the next assumption.

                                                                                               1
Assumption 3. Let cnij âˆˆ {0, c} for all pairs (i, j) âˆˆ N n Ã— N n , where c <                  Ï+ÏÌ„ .   Moreover, let cij = cji
for all i, j âˆˆ N n (symmetry), and cij + cjk â‰¥ cik for all i, j, k âˆˆ N n (triangular inequality).

                                    1
   The assumption that c <         Ï+ÏÌ„    rules out the degenerate case where no agent forms a costly link.
The symmetry and triangular inequality assumptions are imposed to simplify the definition of a social
clique, which is introduced next. Suppose Assumption 3 holds. We define a social clique (cf. Figure
6) H n âŠ‚ N n as a set of agents such that

                                   i, j âˆˆ H n          if and only if cij = cji = 0.

Note that this set is well-defined since, by the triangular inequality and symmetry assumptions, if an
agent i does not belong to social clique H n , then cij = c for all j âˆˆ H n . Hence, we can uniquely
partition the set of nodes N n into a set of K n pairwise disjoint social cliques Hn = {H1n , Â· Â· Â· , HK
                                                                                                       n }.
                                                                                                         n

We use the notation Hkn to denote the set of pairwise disjoint social cliques that have cardinality
greater than or equal to k, i.e., Hkn = {Hin , i = 1, . . . , K n | |Hin | â‰¥ k}. We also use SC n (i) to denote
the social clique that agent i belongs to.
   We consider a sequence of communication cost matrices {C n }âˆž
                                                               n=1 satisfying condition (10) and
Assumption 3, and we refer to this sequence as a communication cost structure. As shown above,
the communication cost structure {C n }âˆž                                               n âˆž
                                       n=1 uniquely defines the following sequences, {H }n=1 and


                                                                25
                                 Social clique 1                Social clique 2
                                                         c
                                    0          0                   0          0
                                          0              c                0


                                                         c


                                              Figure 6: Social cliques.


{Hkn }âˆž
      n=1 for k > 0, of sets of pairwise disjoint social cliques. Moreover, it induces network equilibria
(g, Ïƒ) = (g n , Ïƒ n )âˆž               n   n           n
                     n=1 such that (g , Ïƒ ) âˆˆ N ET (C ) for all n. Our goal is to identify conditions on
the communication cost structure that lead to the emergence of networks which guarantee asymptotic
learning. We focus entirely on perfect asymptotic learning, as this enables us to obtain sharp results.
Similar results can be obtained for , Î´-asymptotic learning.

Proposition 7. Let {C n }âˆž
                         n=1 be a communication cost structure and let Assumptions 1 and 3 hold.
Then, there exists a constant kÌ„ = kÌ„(c) such that the following hold:

 (a) Suppose that

                                                   HkÌ„n
                                         lim sup        â‰¥  for some  > 0.                         (11)
                                          nâ†’âˆž      n

     Then, perfect asymptotic learning does not occur in any network equilibrium (g, Ïƒ).

 (b) Suppose that

                                        HkÌ„n
                              lim            = 0 and lim H`n = âˆž for some `.                        (12)
                             nâ†’âˆž        n            nâ†’âˆž

     Then, perfect asymptotic learning occurs in all network equilibria (g, Ïƒ) when the discount rate
     r satisfies 0 < r < rÌ„, where rÌ„ > 0 is a constant.

 (c) Suppose that there exists M > 0 such that

                                    HkÌ„n
                             lim         = 0 and lim sup H`n < M for all `,                         (13)
                            nâ†’âˆž     n               nâ†’âˆž

     and let agents be patient, i.e., consider the case, when the discount rate r â†’ 0. Then, there
     exists a cÌ„ > 0 such that

       (i) If c â‰¤ cÌ„, perfect asymptotic learning occurs in all network equilibria (g, Ïƒ).

      (ii) If c > cÌ„, there exists at least one network equilibrium (g, Ïƒ), where there is no perfect


                                                         26
                 : Clique with size > kÌ„                                 : Individual Agent
                  : Clique with infinite size                            : Small Clique
                                                                                   ...




                                                                   ...                              ...


                                                                          sender         receiver



                                                                                   ...

      (a) Equilibrium network, when (12) holds.        (b) Equilibrium network, when (13) holds.

                            Figure 7: Network formation among social cliques.

           asymptotic learning and there exists at least one network equilibrium (g, Ïƒ) where perfect
           asymptotic learning occurs.

   The results in this proposition provide a fairly complete characterization of what types of environ-
ments lead to the formation of networks that subsequently induce perfect asymptotic learning. The
key concept is that of a social clique, which represents groups of individuals that are linked to each
other at zero cost. These can be thought of as â€œfriendship networks,â€ which are linked for reasons
unrelated to information exchange and thus can act as conduits of such exchange at low cost. Agents
can exchange information without incurring any costs (beyond the delay necessary for obtaining infor-
mation) within their social cliques. However, if they wish to obtain further information, from outside
their social cliques, they have to pay a cost at the beginning in order to form a link. Even though
network formation games have several equilibria, the structure of our network formation and infor-
mation exchange game enables us to obtain relatively sharp results on what types of societies lead to
endogenously formed communication networks that ensure perfect asymptotic learning. In particular,
the first part of Proposition 7 shows that perfect asymptotic learning cannot occur in any equilibrium
if the number of sufficiently large social cliques increases at the same rate as the size of the society.
This is intuitive; when this is the case, there are many social cliques of sufficiently large size that none
of their members wish to engage in further costly communication with members of other social cliques.
But since several of these do not contain an information hub social learning is precluded.
   In contrast, the second part of the proposition shows that if the number of disjoint and sufficiently
large social cliques is limited (grows less rapidly than the size of the society) and some of them are
large enough to contain information hubs, then perfect asymptotic learning takes place (provided
that future is not heavily discounted). In this case, as shown by Figure 7(a), sufficiently many social
cliques connect to the larger social cliques acting as information hubs, ensuring effective aggregation
of information for the great majority of the agents in the society. It is important that the discount

                                                    27
factor is not too small, otherwise smaller cliques do not find it beneficial to form links with the larger
cliques.
    Finally, the third part of the proposition outlines a more interesting configuration, potentially
leading to perfect asymptotic learning. In this case, many small social cliques form an â€œinformational
ringâ€(Figure 7(b)). Each is small enough that it finds it beneficial to connect to another social clique,
provided that this other clique also connects to others and obtain further information. This intuition
also clarifies why such information aggregation takes place only in some equilibria. The expectation
that others do not form the requisite links leads to a coordination failure. Interestingly, however, if
agents are sufficiently patient and the cost of link formation is not too large, the coordination failure
equilibrium disappears, because it becomes beneficial for each clique to form links with another one,
even if further links are not forthcoming. Finally, the ring structure is a direct consequence of the
fact that agents are patient (and has been shown to emerge as an equilibrium configuration in other
models of network formation, e.g., Bala and Goyal (2000))

5    Conclusion
We have developed a framework for the analysis of information exchange through communication
and investigated its implications for information aggregation in large societies. An underlying state
determines the payoffs from different actions. Agents decide which agents to form a communication
link with incurring the associated cost. After receiving a private signal correlated with the underlying
state, they exchange information over the induced communication network until taking an (irreversible)
action.
    Our focus has been on asymptotic learning, defined as the fraction of agents taking the correct
action converging to one in probability as a society grows large. We showed that asymptotic learning
occurs if and, under some additional mild assumptions, only if the induced communication network
includes information hubs and most agents are at a short distance from a hub. Thus asymptotic
learning requires information to be aggregated in the hands of a few agents. This kind of aggregation
also requires truthful communication, which we show is an equilibrium of the strategic communication
in large societies (partly as a consequence of the fact there is no conflict among the agents concerning
which action is best).
    Our analysis also provides a systematic investigation of what types of cost structures, and asso-
ciated social cliques which consist of groups of individuals linked to each other at zero cost (such
as friendship networks), ensure the emergence of communication networks that lead to asymptotic
learning. Our main result on network formation shows that societies with too many (disjoint) and
sufficiently large social cliques do not form communication networks that lead to asymptotic learning,
because each social clique would have sufficient information to make communication with others not
sufficiently attractive. Asymptotic learning results if social cliques are neither too numerous nor too


                                                   28
large so as to encourage communication across cliques. Our analysis was conducted under a simplifying
assumption that all agents have the same preferences. Interesting avenues for research include inves-
tigation of similar dynamic models of information exchange and network formation in the presence
of ex ante or ex post heterogeneity of preferences as well as differences in the quality of information
available to different agents, which may naturally lead to the emergence of hubs.




                                                  29
Appendix
Proofs from Section 3

Proof of Lemma 1.
    Recall that, by the principle of optimality, agent iâ€™s optimal continuation payoff at information set
 n , when the rest of the agents behave according to strategy profile Ïƒ, is given by:
Ii,t

                                       1
                           (
                               Ï€âˆ’        n,Ïƒ
                                    Ï+ÏÌ„ki,t
                                                                     (when she takes the optimal irreversible action),
EÏƒ (Uin    n
          Ii,t )   = max         âˆ’rdt
                               e      E[EÏƒ (Uin |Ii,t+dt
                                                  n      )    n]
                                                             Ii,t    (when she decides to wait, i.e., x = â€œwaitâ€),
       n,Ïƒ
where ki,t denotes the number of distinct private signals agent i has observed up to time t. The first
line is equal to the expected payoff for the agent when she chooses the optimal irreversible action
                       n , i.e., E[Î¸|I n ], and she has observed k n,Ïƒ private signals, while the second
under information set Ii,t            i,t                         i,t
line is equal to the discounted expected continuation payoff.
    For the latter, we have that with probability Î»dt, communication takes place in time interval [t, t +
dt], thus the information set of agent i expands; with probability (1 âˆ’ Î»dt), there is no communication
and the value function for agent i remains unchanged. If communication takes place in interval [t, t+dt],
                        n,Ïƒ              n,Ïƒ
then agent i observes |Bi,|T t |+1
                                   | âˆ’ |Bi,|T t|
                                                 | additional signals.
    Note that since we assume that signals are identically distributed and independent, the value
                                                                                     n , k n,Ïƒ and profile
function can simply be expressed as a function of the number of distinct signals in Ii,t  i,t
Ïƒ. The agent chooses to take an irreversible action and not to wait if

                       1          âˆ’rdt
          Ï€âˆ’              n,Ïƒ â‰¥ e      E[EÏƒ (Uin |Ii,t+dt
                                                   n         n
                                                          ) Ii,t ]
                   Ï + ÏÌ„ki,t
                                                                                                                       
                                                  n,Ïƒ        n,Ïƒ               n,Ïƒ                        n n,Ïƒ
                              â‰¥ eâˆ’rdt Î»dtUin (ki,t    + |Bi,|T    t |+1
                                                                        | âˆ’ |B i,|Tt | |, Ïƒ) + (1 âˆ’ Î»dt)U i (k i,t , Ïƒ)   .

Thus, we obtain that the agent chooses not to wait if:
                                                                                                       !
                                   n,Ïƒ     n,Ïƒ              n,Ïƒ             r+Î»             1
                             Uin (ki,t + |Bi,|T t |+1
                                                      | âˆ’ |Bi,|T t|
                                                                    |, Ïƒ) â‰¤          Ï€âˆ’        n,Ïƒ         .                  (14)
                                                                             Î»          Ï + ÏÌ„ki,t

The left hand side of (14) is upper bounded by Ï€, whereas the right hand side is increasing in the
                           n,Ïƒ                                                r+Î»
number of private signals ki,t and in the limit is equal to                    Î» Ï€   > Ï€. This establishes the lemma.
    The next lemma will be used in the rest of the Appendix. It shows that the probability of choosing
an action that is more than  away from the optimal for agent i âˆˆ Vkn,Ïƒ , i.e., PÏƒ (Min, = 0), is
uniformly bounded away from 0 in terms of the error function.

Lemma 2. Let k > 0 be a constant, such that the k-radius set Vkn,Ïƒ is non-empty. Then,
                                              r !
                             n,                 kÏÌ„                 n
                        P(Mi = 0) â‰¥ erf               for all i âˆˆ Vk,Ïƒ ,
                                                  2


                                                                    30
                        Rx      2
where erf (x) =   âˆš2
                    Ï€   0    eâˆ’t dt is the error function.

Proof. Note that because of our normality assumption the empirical mean Î¸Ì‚ after observing ` private
signals is normally distributed around Î¸ with precision ÏÎ¸Ì‚ = `ÏÌ„. Then, the probability that Min, = 0
is simply equal to the probability that the error does not belong to the interval [âˆ’, ], i.e.,
                                                           r !
                                         n,                 `ÏÌ„
                                     P(Mi = 0) = erf             .
                                                              2

The lemma follows since agent i âˆˆ Vkn,Ïƒ , thus she takes an irreversible action after observing at most
k private signals.
Proof of Proposition 1. First, we show that learning fails if condition (3) holds, i.e., there exists
a k > 0, such that
                                                                                   r         !
                                1                                                      kÏÌ„
                     Î· = lim sup Â· Vkn,Ïƒ >  and erf                                            < (1 âˆ’ Î´)(1 âˆ’ /Î·).       (15)
                            nâ†’âˆž n                                                       2

From condition (15) we obtain that there exists an infinite index set J such that

                                                        n
                                                      Vk j â‰¥ Î· Â· nj for j âˆˆ J.

Now restrict attention to index set J, i.e., consider n = nj for some j âˆˆ J. Then,
                                               hP                                     i          
          PÏƒ n1 ni=1 Min, > 1 âˆ’  = PÏƒ n1                    n,                  n,
                P                                                  P
                                                        n,Ïƒ M     +     iâˆˆV n,Ïƒ M        > 1 âˆ’  
                                               hPiâˆˆVk        i          / k
                                                                                  ii          
                                                1             n,             n,Ïƒ
                                       â‰¤ PÏƒ n       iâˆˆVkn,Ïƒ M
                                                              i   + n âˆ’    V k       >  1 âˆ’                           ,
                                                                     n,Ïƒ
                                                                                 
                                                1 P          n,    V k
                                       = PÏƒ n iâˆˆV n,Ïƒ Mi > n âˆ’ 
                                                                          k



where the inequality follows since we let Min, = 1 for all i âˆˆ
                                                              / Vkn,Ïƒ . Next we use Markovâ€™s inequality
                          ï£«                             ï£¶         hP                i
                                                n,Ïƒ                             n,
                                                              E           n,Ïƒ M
                            1                V                  Ïƒ     iâˆˆVk      i
                                     Min, > k
                               X
                      PÏƒ ï£­                          âˆ’ ï£¸ â‰¤             n,Ïƒ        .
                            n    n,Ïƒ            n              n Â· Vk /n âˆ’ 
                                     iâˆˆVk

We can view each summand
                     q   above as an independent Bernoulli variable with success probability
                       kÏÌ„
bounded above by erf  2 from Lemma 2. Thus,

                                                                              q 
                                                       n,
                                                                     Vkn,Ïƒ erf  kÏÌ„
                                          P
                                    EÏƒ            n,Ïƒ Mi
                                              iâˆˆV                                  2
                                                  k
                                                               â‰¤                  
                                     nÂ·       Vkn,Ïƒ /nâˆ’             nÂ· Vk  n,Ïƒ
                                                                                /nâˆ’
                                                                                q 
                                                                      Î·
                                                                 â‰¤   Î·âˆ’ erf     kÏÌ„
                                                                                   2  < 1 âˆ’ Î´,

where the second inequality follows from the fact that n was chosen such that Vkn,Ïƒ â‰¥ Î· Â· n. Finally,
the last expression follows from the choice of k (cf. Condition (3)). We obtain that for all j âˆˆ J it



                                                                     31
holds that                                     "      nj              #    !
                                                   1 X        nj , 
                                        PÏƒ               1 âˆ’ Mi         >  â‰¥ Î´.
                                                   nj
                                                        i=1

Since J is an infinite index set we conclude that
                                           " n              #  !
                                             1X         n,
                              lim inf PÏƒ          (1 âˆ’ Mi ) >  â‰¥ Î´,
                                  nâ†’âˆž        n
                                                               i=1

thus , Î´-asymptotic learning is incomplete when (3) holds.
Next, we prove that Condition (4) is sufficient for , Î´-asymptotic learning. As mentioned above, if
agent i takes an irreversible action after observing ` signals, then the probability that Min, = 1 is
equal to                                                                               r         !
                                                                                           `ÏÌ„
                                             PÏƒ (Min,      = 1) = erf                              .   (16)
                                                                                            2
Similarly with above, we have


                          n
                    "                 #  !                                    "                 #    !
                        1X        n,                                        1X            n,     V
               PÏƒ           (1 âˆ’ Mi ) >   â‰¤ PÏƒ                                    (1 âˆ’ Mi ) >  âˆ’
                        n                                                    n                     n
                         i=1                                                   iâˆˆV
                                                                                /
                                                                          P             n, 
                                                                       EÏƒ     / (1 âˆ’ Mi )
                                                                             iâˆˆV
                                                               â‰¤                              ,         (17)
                                                                           n  âˆ’ V /n
             n                        o
                     n
where V =     i     Bi,Ï„ n,Ïƒ â‰¤ k           and the second inequality follows from Markovâ€™s inequality. By
                          i
combining Eqs. (16) and (17) and letting kin,Ïƒ denote the number of private signals that agent i
observed before taking an action,
                                                                                          q n,Ïƒ 
                                                                       P                     ki ÏÌ„
                                   P           n,                        iâˆˆV
                                                                             /    1 âˆ’ erf     2
                              EÏƒ       / (1 âˆ’ Mi )
                                      iâˆˆV
                                                     â‰¤                                           .     (18)
                                    n  âˆ’ V /n                                    n  âˆ’ V /n

We have                                                 r              !
                                                            kin,Ïƒ ÏÌ„               Î´( âˆ’ Î¶)
                                           erf                            >1âˆ’              ,            (19)
                                                               2                    1âˆ’Î¶
for all i âˆˆ
          / V from the definition of k (cf. Condition (4)). Thus, combining Eqs. (17),(18) and (19), we
obtain
                                            n
                                      "                   #   !
                                          1X
                               PÏƒ             (1 âˆ’ Min, ) >  < Î´ for all n > N,
                                          n
                                             i=1

where N is a sufficiently large constant, which implies that condition (4) is sufficient for asymptotic
learning.
Similar to perfect k-radius sets, we define sets Xkn for scalar k as

                         Xkn = {i âˆˆ N n there exists ` âˆˆ Bi,Ï„
                                                          n               n
                                                              n with indeg` > k},
                                                              i




                                                                       32
                                                                                          n
                                                               Agent iâ€™s observation set Bi,Ï„ n
                                                                                                                    i

                                                     Agent jâ€™s observation set                      n
                                                                                                   Bj,Ï„ n
                                                                                                        j




                                                                                                            Node `
           Node i                                        Node j

                                           Ï„ nj                                Ï„ nj
                                                                                      dist(`, j)
                                                    dist(`, i) â‰¤ Ï„ ni


                                        Figure 8: Proof of Proposition 8.

i.e., the set Xkn consists of all agents, which have an agent with in-degree at least k within their perfect
observation radius.

Proposition 8. Suppose Assumption 1 holds. Then, perfect asymptotic learning occurs in society
{Gn }âˆž
     n=1 in any equilibrium Ïƒ if
                                                              1 n
                                                  lim lim      |X | = 1.
                                                  kâ†’âˆž nâ†’âˆž     n k
Proof. Consider equilibrium profile Ïƒ and society {Gn }âˆž                           1   n
                                                       n=1 such that limkâ†’âˆž limnâ†’âˆž n |Xk | = 1.

   Define Zkn,Ïƒ as the following set of agents

                                                          n,Ïƒ
                       Zkn,Ïƒ = {i âˆˆ N n there exists ` âˆˆ Bi,Ï„               n
                                                              n,Ïƒ with indeg` > k},
                                                                          i


i.e., the agents that at equilibrium Ïƒ, communicate with an agent with in-degree at least k. Next, we
show that for k large enough (and consequently n large enough), Xkn = Zkn,Ïƒ .
   Consider i âˆˆ Xkn and let P n = {`, i1 , Â· Â· Â· , iK , i} denote the shortest path in communication network
Gn between i and any agent `, with indeg`n â‰¥ k. First we show the following (refer to Figure 8)

                                        i âˆˆ Xkn â‡’ j âˆˆ Xkn for all j âˆˆ P n .                                                 (20)

Assume for the sake of contradiction that condition (20) does not hold. Then, let

                         j = arg min
                                  0
                                     {distn (`, j 0 ) j 0 âˆˆ P n and distn (`, j 0 ) > Ï„ nj0 },
                                    j

where recall that Ï„ ni denotes the perfect observation radius of agent i. For agents i, j we have Ï„ ni > Ï„ nj
and dist(j, i) + dnj < dist(`, i) â‰¤ Ï„ ni , since otherwise j âˆˆ Xkn . This implies that Bj,Ï„
                                                                                        n        n
                                                                                            n âŠ‚ Bi,Ï„ n .
                                                                                                                        j     i

Furthermore,
                                                       dist(`,j)âˆ’Ï„ nj                                     !
                            1                      Î»                                      1
                  Ï€âˆ’          n |) >                                          Ï€âˆ’          n | + k)              ,           (21)
                     Ï + ÏÌ„(|Bj,Ï„ n               Î»+r                            Ï + ÏÌ„(|Bj,Ï„ n
                                    j                                                              j


In particular, the left hand side is equal to the expected payoff of agent j if she takes an irreversible

                                                             33
action at time Ï„ nj after receiving |Bj,Ï„
                                      n | observations, whereas the right hand side is a lower bound on
                                          n
                                               j

the expected payoff if agent j delays taking an action until after she communicates with agent `. The
inequality follows, from the definition of the observation radius for agent j. On the other hand, since
                  n , we have
for agent i, ` âˆˆ Bi,Ï„ n
                     i

                                   dist(`,i)âˆ’dist(j,i)âˆ’Ï„ nj                                           !
           1                   Î»                                                             1
 Ï€âˆ’          n |) <                                                      Ï€âˆ’            n | + k + k0 )       , for some k 0 > 0. (22)
    Ï + ÏÌ„(|Bj,Ï„ n            Î»+r                                             Ï + ÏÌ„(|Bj,Ï„ n
                j                                                                            j


For k large enough we conclude that dist(`, j) < dist(`, i)âˆ’dist(j, i), which is obviously a contradiction.
This implies that (20) holds.
   Next we show, by induction on the distance from agent ` with in-degree â‰¥ k that Xkn = Zkn,Ïƒ for
equilibrium Ïƒ. The claim is obviously true for all agents with distance equal to 0 (agent `) and 1 (her
neighbors). Assume that the claim holds for all agents with distance at most t from agent `, i.e., if
i âˆˆ Xkn and dist(`, i) â‰¤ t then i âˆˆ Zkn,Ïƒ . Finally, we show the claim for an agent i such that i âˆˆ Xkn
and dist(`, i) = t + 1. Consider a shortest path P n from i to `. Condition (20) implies that all agents
j in the shortest path are such that j âˆˆ Xkn , thus from the induction hypothesis we obtain j âˆˆ Zkn,Ïƒ .
Thus, for k sufficiently large we obtain that i âˆˆ Zkn,Ïƒ , for any equilibrium Ïƒ.
   Finally, by the hypothesis of the proposition, i.e., limkâ†’âˆž limnâ†’âˆž n1 |Xkn | = 1, we conclude that
limkâ†’âˆž limnâ†’âˆž n1 |Zkn,Ïƒ | = 1, for any equilibrium Ïƒ. The latter implies that limkâ†’âˆž limnâ†’âˆž n1 |Vkn,Ïƒ | =
0, thus asymptotic learning occurs along equilibrium Ïƒ from Proposition 1.
Proof of Proposition 2.
   The first part of Proposition 2 follows directly from Proposition 8, since

                                                   1 n                1
                                    lim lim         |V | = 0 â‡’ lim lim |Xkn | = 1.
                                kâ†’âˆž nâ†’âˆž            n k        kâ†’âˆž nâ†’âˆž n

To conclude the proof we need to show that if asymptotic learning occurs along some equilibrium Ïƒ
when condition (4) does not hold, then the society contains a set of leading agents. In particular,
consider a society {Gn }âˆž                                                              n âˆž
                        n=1 in which condition (4) does not hold and equilibrium Ïƒ = {Ïƒ }n=1 along
which asymptotic learning occurs in the society. This implies that there should exist a subset {Rn,Ïƒ }âˆž
                                                                                                      n=1
of agents such that limnâ†’âˆž n1 |Rn,Ïƒ | >  and there is an infinite index set J for which

                                                                     n        n ,Ïƒ
                                         i âˆˆ Rnj ,Ïƒ and Ï„ i j < Ï„ i j , for j âˆˆ J.                                             (23)

This further implies that
                                                           nj                 nj ,Ïƒ
                                                      |B        n    | > |B      n      |.                                     (24)
                                                           i,Ï„ i j            i,Ï„ i j

From equations (23) and (24) we obtain that there should exist a collection of agents {S n }âˆž
                                                                                            n=1 such
that (we restrict attention to index set J):

  (i) Rn,Ïƒ âŠ† Sfnollow .

                                                                         34
 (ii) There exists a k > 0 such that S n âŠ† Vkn,Ïƒ .

 (ii) limnâ†’âˆž n1 |S n | = 0, since otherwise asymptotic learning would not occur under equilibrium Ïƒ.

   Note that collection {S n }âˆž
                              n=1 satisfies the definition of a set of leading agents [cf. Definition 5] and
Proposition 2 (ii) follows.


Proof of Proposition 3. Consider the following two events A and B.
Event A: Layer 1 (the top layer) has more than k agents, where k > 0 is a scalar.
Event B: The total number of layers is more than k.
From the definition of a hierarchical sequence of communication networks, we have
                                      k                      k
                                                                     !
                                     Y          1            X     1
                             P(A) =       1 âˆ’ 1+Î¶ < exp âˆ’               .                              (25)
                                              i                  i1+Î¶
                                      i=2                                     i=2

Also,
                                                         âˆž
                                                E(L)   1X 1
                                         P(B) â‰¤      =          ,                                      (26)
                                                 k     k   i1+Î¶
                                                                  i=2

from Markovâ€™s inequality, where L is a random variable that denotes the number of layers in the
hierarchical society. Let Î¶(Î·) be small enough and k (and consequently n) large enough such that
Pk      1         4     Pâˆž 1          kÂ·Î·
  i=2 i1+Î¶ > log Î· and     i=2 i1+Î¶ < 4 . For those values of Î¶ and k we obtain P(A) < Î·/4 and
P(B) < Î·/4. Next, consider the event C = Ac âˆ© B c , which from Eqs. (25) and (26) has probability
P(C) > 1 âˆ’ Î·/2 for the values of Î¶ and k chosen above. Moreover, we consider
Event D: The agents on the top layer are information hubs, i.e.,

                                            n
                                      lim |Bi,1 | = âˆž, for all i âˆˆ N1n .
                                     nâ†’âˆž

We claim that event D occurs with high probability if C occurs, i.e., P(D C) > 1 âˆ’ Î·/2, which implies

                              P(C âˆ© D) = P(D C)P(C) > (1 âˆ’ Î·/2)2 > 1 âˆ’ Î·.                              (27)

In particular, note that conditional on event C occurring, the total number of layers and the total
number of agents in the top layer is at most k. From the definition of a hierarchical society, agents
in layers with index ` > 1 have an edge to a uniform agent that belongs to a layer with lower index,
with probability one. Therefore, if we denote the degree of an agent in a top layer by D1n we have
                                              n                         n
                                                                       TL
                                             T2
                                             X                         X
                                   D1n   =          level2
                                                   Ii,1      + Â·Â·Â· +          levelL
                                                                             Ii,1    ,                 (28)
                                             i=1                       i=1

                                                              levelj
where Tin denotes the random number of agents in layer i and Ii,1    is an indicator variable that
takes value one if there is an edge from agent i to agent 1 (here levelj simply denotes that agent i


                                                             35
                                                           levelj                                         1
belongs to level j). Again from the definition, we have P(Ii1     = 1) =                               Pjâˆ’1          , where the sum in
                                                                                                         `=1   T`n
the denominator is simply the total number of agents that lie in layers with lower index, and finally,
T1n + Â· Â· Â· TLn = n.
    We can obtain a lower bound on the expected degree of an agent in the top layer conditional on
event C by viewing (28) as the following optimization problem:
                                                       x2                      xk
                                               min        + Â·Â·Â· +
                                                       x1              x1 + Â· Â· Â· + xkâˆ’1
                                                       P k
                                               s.t.      j=1 x j   =   n,
                                                       0 â‰¤ x1 â‰¤ k,
                                                       0 â‰¤ x2 , Â· Â· Â· , xkâˆ’1 ,

where we make use of the fact that the total number of layers is bounded by k, since we condition
on event C. By solving the problem we obtain that the objective function is lower bounded by Ï†(n),
where Ï†(n) = O(n1/k ) for every n. Then,

     E[D1n C] =
           k
           X          X
       =                        P(L = `, T1n = k1 , Â· Â· Â· , T`n = k` |C) Â· E[D1n C, L = `, T1n = k1 , Â· Â· Â· , T`n = k` ]
            `=2 k1 â‰¤k,Â·Â·Â· ,k`
               k1 +Â·Â·Â·+k` =n
           k
           X          X
       â‰¥                        P(L = `, T1n = k1 , Â· Â· Â· , T`n = k` |C) Â· Ï†(n) = Ï†(n),                                            (29)
            `=2 k1 â‰¤k,Â·Â·Â· ,k`
               k1 +Â·Â·Â·+k` =n

where Eq. (29) follows since E[D1n C, L = `, T1n = k1 , Â· Â· Â· , T`n = k` ] â‰¥ Ï†(n) for all values of ` (2 â‰¤ ` â‰¤ k)
and k1 , Â· Â· Â· , k` (k1 â‰¤ k, k1 + Â· Â· Â· + k` = n) from the optimal solution of the optimization problem. The
same lower bound applies for all agents in the top layer. Similarly we have for the variance of the
degree of an agent in the top layer (we use `, k1 , Â· Â· Â· , k` as a shorthand for L = `, T1n = k1 , Â· Â· Â· , T`n = k` )

                          k
                          X        X
      V   ar[D1n   C] =                        P(`, k1 , Â· Â· Â· , k` |C) Â· V ar[D1n C, `, k1 , Â· Â· Â· , k` ]
                          `=2 k1 â‰¤k,Â·Â·Â· ,k`
                             k1 +Â·Â·Â·+k` =n
                 k
                 X         X                                                                                  
                                                                           level2                      level`
             =                         P(`, k1 , Â· Â· Â· , k` |C) Â· k2 V ar(Ii,1    ) + Â· Â· Â· + k` V ar(Ii,1    )                    (30)
                   `=1 k1 â‰¤k,Â·Â·Â· ,k`
                      k1 +Â·Â·Â·+k` =n
                 k
                 X         X                                                                            
                                                                        level2                   level`
             â‰¤                         P(`, k1 , Â· Â· Â· , k` |C) Â· k2 E(Ii,1    ) + Â· Â· Â· + k` E(Ii,1    ) = E[D1n C],              (31)
                   `=1 k1 â‰¤k,Â·Â·Â· ,k`
                      k1 +Â·Â·Â·+k` =n

where Eq. (30) follows by noting that conditional on event C and the number of layers and the agents
in each layer being fixed, the indicator variables (defined above) are independent and Eq. (31) follows
since the variance of an indicator variable is smaller that its expectation. We conclude that the variance



                                                                    36
of the degree is smaller than the expected value and from Chebyschevâ€™s inequality we conclude that
                                                \      Din
                                   P(D) â‰¥ P(               > Î¶) > 1 âˆ’ Î·/2,
                                                      Ï†(n)
                                              iâˆˆN1n

where Î¶ > 0, i.e., with high probability all agents in the top layer are information hubs (recall that
limnâ†’âˆž Ï†(n) = âˆž).
   We have shown that when event C âˆ©D occurs, there is a path of length at most k (the total number
of layers) from each agent to an agent at the top layer, i.e., an information hub with high probability.
Therefore, if the discount rate r is smaller than some bound (r < rÌ„), then perfect asymptotic learning
occurs. Finally, we complete the proof by noting that P(C âˆ© D) > (1 âˆ’ Î·/2)2 > 1 âˆ’ Î·.
Proof of Proposition 4.

Proposition 4 is a direct consequence of the next lemma, which intuitively states that there is no
incentive to lie to an agent with a large number of neighbors, assuming that everybody else is truthful.

Lemma 3 (Truthful Communication to a High Degree Agent). There exists a scalar k > 0, such
that truth-telling to agent i, with indegin â‰¥ k, in the first time period is an equilibrium of IN F O(Gn ).
Formally,
                                     (Ïƒ n,truth , mn,truth ) âˆˆ IN F O(Gn ),

where mn,truth
       ji,0
                             n .
               = sj for j âˆˆ Bi,1

                                                                                 n except j report
Proof. The proof is based on the following argument. Suppose that all agents in Bi,1
                                               n | â‰¥ k, where k is a large constant. Then, it is an weakly
their signals truthfully to i. Moreover, let |Bi,1
dominant strategy for j to report her signal truthfully to i, since jâ€™s message is not pivotal for agent i,
i.e., i will take an irreversible action after the first communication step, no matter what j reports.
Proof (sketch) of Proposition 5. Assume that asymptotic learning occurs at the optimal allocation
sp = {spn }âˆž
           n=1 . Then,
                                                        1 n
                                            lim lim      |V | = 0.                                    (32)
                                            kâ†’âˆž nâ†’âˆž     n k
This follows since if Equation (32) were not true, then a social planner could replicate the allocation
induced by the perfect observation radius and achieve a higher aggregate welfare. This is possible
       n âŠ‡ B n,sp for every Ï„ , where sp denotes the socially optimal strategy profile. From Equation
since Bi,Ï„  i,Ï„
(32) and Proposition 1 we obtain that asymptotic learning occurs in all equilibria Ïƒ. Finally, the
proposition follows using similar arguments as those used in the proof of Proposition 8.
Proof of Proposition 6.          The claim follows by noting that the social planner could choose the
                                          n,Ïƒ
following strategy profile: for each j âˆˆ Dk,` delay iâ€™s irreversible action by at least one time period,
where i is an agent such that if i delays then j gains access to a least ` additional signals. Moreover,
it is straightforward to see that there exist , Î´ for which , Î´-learning fails.


                                                       37
Proofs from Section 4
Proof of Proposition 7

First we make an observation which will be used frequently in the subsequent analysis. Consider an
                   n
agent i such that HSC(i) âˆˆ HkÌ„n , where kÌ„ is an integer appropriately chosen (see below), i.e., the size of
                                                               n
the social clique of agent i is greater than or equal to kÌ„, |HSC(i) | â‰¥ kÌ„. Suppose agent i does not form a
link with cost c with any agents outside her social clique. If she makes a decision at time t = 0 based
                                                       1
on her signal only, her expected payoff is Ï€ âˆ’        Ï+ÏÌ„ .     If she waits for one period, she has access to the
signals of all the agents in her social clique (i.e., she has access to at least kÌ„ signals), implying that
                                                                             
                                                             Î»          1
her expected payoff would be bounded from below by r+Î»            Ï€ âˆ’ Ï+ÏÌ„ kÌ„
                                                                                . Hence, her expected payoff
E[Î i (g n )] satisfies                                                                         
                                  n                       1      Î»                       1
                           E[Î i (g )] â‰¥ max Ï€ âˆ’               ,             Ï€âˆ’                         ,
                                                        Ï + ÏÌ„ r + Î»                  Ï + ÏÌ„kÌ„
for any link formation strategy g n and along any Ïƒ âˆˆ IN F O(Gn ) (where Gn is the communication
network induced by g n ). Suppose now that agent i forms a link with cost c with an agent outside her
social clique. Then, her expected payoff is bounded from above by
                                              (                2    )
                                                     1       Î»
                            E[Î i (g n )] < max Ï€ âˆ’       ,         Ï€âˆ’c ,
                                                   Ï + ÏÌ„ Î» + r

where the second term in the maximum is an upper bound on the payoff she could get by having access
to the signals of all agents she is connected to in two time steps (i.e., signals of the agents in her social
clique and in the social clique that she is connected to). Combining the preceding two relations, we
                          n
see that an agent i with HSC(i) âˆˆ HkÌ„n will not form any costly links in any network equilibrium, i.e.,

                    n                                                         n
                   gij = 1 if and only if SC(j) = SC(i) for all i such that |HSC(i) | â‰¥ kÌ„.                               (33)

for kÌ„ such that                                                            2
                                   Î»                1                     Î»
                                         Ï€âˆ’                      â‰¥                  Ï€ âˆ’ c.
                                  r+Î»            Ï + ÏÌ„kÌ„                Î»+r
(a)   Condition (11) implies that for all sufficiently large n, we have

                                                     HkÌ„n â‰¥ Î¾n,                                                           (34)

where Î¾ > 0 is a constant. For any  with 0 <  < Î¾, we have
                                       ï£«ï£®                                                                        ï£¹    ï£¶
            n
                            !
                      n,
           X1âˆ’M
                     i
                                              X       1 âˆ’ Min,                           X            1âˆ’   Min, ï£º
                          >    =                               +                                                ï£» > ï£¸
                                       ï£¬ï£¯                                                                             ï£·
       P                             P ï£­ï£°
                   n                                      n                                                 n
             i=1                              n  i| |HSC(i) |<kÌ„                          n
                                                                                     i| |HSC(i) |â‰¥kÌ„
                                           ï£«                                        ï£¶
                                                    X            1 âˆ’ Min,
                                   â‰¥                                       > ï£¸ .                                         (35)
                                          ï£¬                                   ï£·
                                         Pï£­
                                                    n
                                                                     n
                                               i| |HSC(i) |â‰¥kÌ„



                                                            38
   The right-hand side of the preceding inequality can be re-written as
                ï£«                          ï£¶             ï£«                          ï£¶
                                    n,                                      n,
                     X       1 âˆ’ Mi                            X       1 âˆ’ Mi
                                        > ï£¸ = 1 âˆ’ P ï£­                           â‰¤ ï£¸
                ï£¬                          ï£·             ï£¬                          ï£·
              Pï£­
                     n
                                 n                             n
                                                                           n
                       i| |HSC(i) |â‰¥kÌ„                                             i| |HSC(i) |â‰¥kÌ„
                                                                               ï£«                                    ï£¶
                                                                                        X            Min,
                                                                = 1 âˆ’ Pï£­                                     â‰¥ r âˆ’ ï£¸ ,
                                                                       ï£¬                                            ï£·
                                                                                        n
                                                                                                      n
                                                                                   i| |HSC(i) |â‰¥kÌ„

            P                   1
where r =            n
                i| |HSC(i) |â‰¥kÌ„ n .   By Eq. (34), it follows that for n sufficiently large, we have r â‰¥ Î¾. Using
Markovâ€™s inequality, the preceding relation implies
                    ï£«                         ï£¶       P                   n,
                          X      1 âˆ’ Min,                  n
                                                       i| |HSC(i) |â‰¥kÌ„ E[Mi ]    1
                                           > ï£¸ â‰¥ 1 âˆ’                         Â·     .                                     (36)
                    ï£¬                         ï£·
                  Pï£­
                          n
                                     n                           n              râˆ’
                            i| |HSC(i) |â‰¥kÌ„


By Lemma 2 and observation (33), E[Min, ] for an agent i with |HSC(i)
                                                                 n     | â‰¥ kÌ„ is upper bounded by
                                                                         ï£« s                 ï£¶
                                                                                 n
                                                                               |HSC(i) |ÏÌ„
                                              P(Min, = 0) â‰¥ erf ï£­                          ï£¸,
                                                                                    2

and therefore                                                        ï£« s                  ï£¶
                                                                             n
                                                                           |HSC(i) |ÏÌ„
                                          E[Min, ] â‰¤ 1 âˆ’ erf ï£­                          ï£¸.
                                                                                   2

Now assuming that social cliques are ordered by size (H1n is the biggest), we can re-write Eq. (36) as
                              ï£«                          ï£¶
                                                  n,
                                    X       1 âˆ’ Mi
                                                      > ï£¸ â‰¥
                              ï£¬                          ï£·
                             Pï£­
                                     n
                                                n
                                              i| |HSC(i) |â‰¥kÌ„
                                                                            q n 
                                                    P|HkÌ„n |  n                |Hj |ÏÌ„
                                                        j=1 |Hj |    1 âˆ’ erf    2
                                          â‰¥1âˆ’
                                                           (r âˆ’ ) Â· n
                                              r Â· (1 âˆ’ Î¶)       Î¾ Â· (1 âˆ’ Î¶)
                                          â‰¥1âˆ’             â‰¥1âˆ’               >Î´                                            (37)
                                                 râˆ’               Î¾âˆ’

Here, the second inequality is obtained
                                 q since the largest value for the sum is achieved when all sum-
mands are equal and Î¶ = erf  kÌ„ÏÌ„   2 . The third inequality holds using the relation r â‰¥ Î¾ and
choosing appropriate values for , Î´.
   This establishes that for all sufficiently large n, we have
                                           n
                                                           !
                                          X   1 âˆ’ Min,
                                      P                 >  > Î´ > 0,
                                                  n
                                                      i=1




                                                                    39
which implies
                                                     n
                                                                                 !
                                                     X 1 âˆ’ M n,       i
                                    lim sup P                              >        > Î´,
                                      nâ†’âˆž                          n
                                                     i=1

and shows that perfect asymptotic learning does not occur in any network equilibrium.

(b)    We show that if the communication cost structure satisfies condition (12), then asymptotic learn-
ing occurs in all network equilibria (g, Ïƒ) = ({g n , Ïƒ n })âˆž
                                                            n=1 . For an illustration of the resulting commu-
nication networks, when condition (13) holds, refer to Figure 7(a). Let Bin (Gn ) be the neighborhood
of agent i in communication network Gn (induced by the link formation strategy g n ),

                         Bin (Gn ) = {j    there exists a path P in Gn from j to i},

i.e., Bin (Gn ) is the set of agents in Gn whose information agent i can acquire over a sufficiently large
(but finite) period of time.
                                                               n
      We first show that for any agent i such that lim supnâ†’âˆž HSC(i) < kÌ„, her neighborhood in any
network equilibrium satisfies limnâ†’âˆž Bin = âˆž. We use the notion of an isolated social clique to show
this. For a given n, we say that a social clique H`n is isolated (at a network equilibrium (g, Ïƒ)) if no
agent in H`n forms a costly link with an agent outside H`n in (g, Ïƒ). Equivalently, a social clique H`n is
not isolated if there exists at least one agent j âˆˆ H`n , such that j incurs cost c and forms a link with
an agent outside H`n .
                                                   n
      We show that for an agent i with lim supnâ†’âˆž HSC(i)                          n
                                                         < kÌ„, the social clique HSC(i) is not isolated
in any network equilibrium for all sufficiently large n. Using condition (12), we can assume without loss
of generality that social cliques are ordered by size from largest to smallest and that limnâ†’âˆž |H1n | = âˆž.
              n
Suppose that HSC(i) is isolated in a network equilibrium (g, Ïƒ). Then the expected payoff of agent i
is upper bounded (similarly with above)
                                                                                                     
                            n                          1      Î»                         1
                     E[Î i (g )] â‰¤ max Ï€ âˆ’                  ,                   Ï€âˆ’
                                                     Ï + ÏÌ„ r + Î»                 Ï + ÏÌ„(kÌ„ âˆ’ 1)

Using the definition of kÌ„, it follows that for some  > 0,
                                              (                 2       )
                                                      1       Î»
                            E[Î i (g n )] â‰¤ max Ï€ âˆ’        ,         Ï€âˆ’câˆ’                                   (38)
                                                    Ï + ÏÌ„ r + Î»

      Suppose next that agent i forms a link with an agent j âˆˆ H1n . Her expected payoff E[Î i (g n )]
satisfies                                                2                                !
                                                     Î»                         1
                               E[Î i (g n )] â‰¥                  Â·       Ï€âˆ’                       âˆ’ c,
                                                    r+Î»                   Ï + ÏÌ„ H1n
since in two time steps, she has access to the signals of all agents in the social clique H1n . Since




                                                           40
limnâ†’âˆž |H1n | = âˆž, there exists some N1 such that
                                                         2
                                     n            Î»
                              E[Î i (g )] >                     Ï€âˆ’câˆ’           for all n > N1 .
                                                 Î»+r

Comparing this relation with Eq. (38), we conclude that under the assumption that r < rÌ„ (for appro-
                               n
priate rÌ„), the social clique HSC(i) is not isolated in any network equilibrium for all n > N1 .
       Next, we show that limnâ†’âˆž |Bin | = âˆž in any network equilibrium. Assume to arrive at a contra-
diction that lim supnâ†’âˆž |Bin | < âˆž in some network equilibrium. This implies that lim supnâ†’âˆž |Bin | <
|H1n | for all n > N2 > N1 . Consider some n > N2 . Since HSC(i)
                                                           n     is not isolated, there exists some
     n
j âˆˆ HSC(i)                                                   n
           such that j forms a link with an agent h outside HSC(i) . Since lim supnâ†’âˆž |Bin | < |H1n |,
                                                            n = 0 and g n = 1 for h0 âˆˆ H n , i.e., j is
agent j can improve her payoff by changing her strategy to gjh         jh0              1
better off deleting her existing costly link and forming one with an agent in social clique H1n . Hence,
for any network equilibrium, we have

                               lim |Bin | = âˆž                                 n
                                                     for all i with lim sup |HSC(i) | < kÌ„                     (39)
                              nâ†’âˆž                                       nâ†’âˆž

       We next consider the probability that a non-negligible fraction (-fraction) of agents takes an action
that is at least -away from optimal with probability at least Î´ along a network equilibrium (g, Ïƒ). For
any n, we have from Markovâ€™s inequality
                                     n                                  n
                                                               !
                                     X 1 âˆ’ M n,     i               1 X E[1 âˆ’ Min, ]
                                 P                        >        â‰¤ Â·                                        (40)
                                                 n                           n
                                     i=1                                 i=1

We next provide upper bounds on the individual terms in the sum on the right-hand side. We have
                                                     r          !
                                        n,            ÏÌ„|Bin |
                               E[1 âˆ’ Mi ] â‰¤ erf                  .                          (41)
                                                          2

                                             n
       Consider an agent i with lim supnâ†’âˆž |HSC(i)                 n
                                                   | < kÌ„ (i.e., |HSC(i) | < kÌ„ for all n large). By Eq. (39),
we have limnâ†’âˆž |Bin | = âˆž. Together with Eq. (41), this implies that for some Î¶ > 0, there exists
some N such that for all n > N , we have

                                            Î¶
                          E[1 âˆ’ Min, ] <                                         n
                                                         for all i with lim sup |HSC(i) | < kÌ„.                (42)
                                             2                            nâ†’âˆž

                                                  n
       Consider next an agent i with lim supnâ†’âˆž |HSC(i) | â‰¥ kÌ„, and for simplicity, let us assume that the
                             n
limit exists, i.e., limnâ†’âˆž |HSC(i) | â‰¥ kÌ„.12
  12
    The case when the limit does not exist can be proven by focusing on different subsequences. In particular, along
                                                  n
any subsequence Ni such that limnâ†’âˆž,nâˆˆNi |HSC(i)      | â‰¥ kÌ„, the same argument holds. Along any subsequence Ni with
limnâ†’âˆž,nâˆˆNi |HSC(i) | < kÌ„, we can use an argument similar to the previous case to show that limnâ†’âˆž,nâˆˆNi |Bin | = âˆž,
                n

and therefore E[1 âˆ’ Min, ] < 2Î¶ for n large and n âˆˆ Ni .




                                                               41
                    n
This implies that |HSC(i) | â‰¥ kÌ„ for all large n, and therefore,

                                                                |Hkn |
                                                                                       ï£« s              ï£¶
                         X                  E[1 âˆ’ Min, ]       X                            ÏÌ„|Hjn |        |HkÌ„n |
                                                            â‰¤            |Hjn |   Â· erf ï£­              ï£¸â‰¤           Â· kÌ„,
                                                 n                                               2            n
                               n
               i| lim supnâ†’âˆž |HSC(i) |â‰¥kÌ„                       j=1


                                                                                                                             HkÌ„n
where the first inequality follows from Eq. (41). Using condition (12), i.e., limnâ†’âˆž                                         n      = 0, this
relation implies that there exists some NÌƒ such that for all n > NÌƒ , we have
                                                     X                    E[1 âˆ’ Min, ]   Î¶
                                                                                        <    .                                           (43)
                                                         n
                                                                               n           2
                                         i| lim supnâ†’âˆž |HSC(i) |â‰¥kÌ„


Combining Eqs. (42) and (43) with Eq. (40), we obtain for all n > max {N, NÌƒ },
                                        n
                                                         !
                                       X    1 âˆ’ Min,
                                   P                  >  < Î¶,
                                                n
                                                      i=1

where Î¶ > 0 is an arbitrary scalar. This implies that
                                                         n
                                                                                      !
                                                         X 1 âˆ’ M n,         i
                                              lim P                               >      = 0,
                                             nâ†’âˆž                         n
                                                         i=1

for all , showing that perfect asymptotic learning occurs along every network equilibrium.

(c)   The proof proceeds in two parts. First, we show that if condition (13) is satisfied, learning occurs
in at least one network equilibrium (g, Ïƒ). Then, we show that there exists a cÌ„ > 0, such that if c < cÌ„,
then learning occurs in all network equilibria. We complete the proof by showing that if c > cÌ„, then
there exist network equilibria, in which asymptotic learning fails, even when condition (13) holds. We
consider the case when agents are patient, i.e., the discount rate r â†’ 0. We consider kÌ„, such that
c>      1
      Ï+ÏÌ„kÌ„
               and c <       1
                         Ï+ÏÌ„(kÌ„âˆ’1)
                                      âˆ’ 0 , for some 0 > 0 (such a kÌ„ exists). Finally, we assume that c <                              1
                                                                                                                                         Ï+ÏÌ„ ,
since otherwise no agent would have an incentive to form a costly link.
Part 1: We assume, without loss of generality, that social cliques are ordered by size (H1n is the small-
           n denote the set of social cliques of size less than kÌ„, i.e., Hn = {H n , i = 1, . . . , K n | |H n | <
est). Let H< kÌ„                                                            <kÌ„   i                           i
kÌ„}. Finally, let rec(j) and send(j) denote two special nodes for social clique Hjn , the receiver and the
sender (they might be the same node). We claim that (g n , Ïƒ n ) described below and depicted in Figure
7(b) is an equilibrium of the network learning game Î“(C n ) for n large enough and Î´ sufficiently close
to one.                 ï£±
                        ï£´ 1 if SC(i) = SC(j), i.e., i, j belong to the same social clique,
                        ï£² 1 if i = rec(` âˆ’ 1) and j = send(`) for 1 < ` â‰¤ |Hn |,
                        ï£´
                   n                                                         <kÌ„
                  gij =                   n |) and j = send(1),
                        ï£´
                        ï£´ 1 if i = rec(|H< kÌ„
                        ï£³
                          0 otherwise
and Ïƒ n âˆˆ IN F O(Gn ), where Gn is the communication network induced by g n . In this communication
network, social cliques with size less than kÌ„ are organized in a directed ring, and all agents i, such

                                                                   42
       n
that |HSC(i) | < kÌ„ have the same neighborhood, i.e., Bin = B n for all such agents.
Next, we show that the strategy profile (g n , Ïƒ n ) described above is indeed an equilibrium of the network
learning game Î“(C n ). We restrict attention to large enough nâ€™s. In particular, let N be such that
P|H< N |
      kÌ„  N
   i=1 |Hi | > kÌ„ and consider any n > N (such N exists from condition (13)). Moreover, we assume
that the discount rate is sufficiently close to zero. We consider the following two cases.
                                           n = 1 if and only if SC(j) = SC(i). Agent iâ€™s neighborhood
Case 1: Agent i is not a connector. Then, gij
                                                      1
as noted above is set B n , which is such that Ï€ âˆ’ Ï+ÏÌ„|B n | > Ï€ âˆ’ c from the assumption on n, i.e., n > N ,
                    P|H< N |
where N such that i=1 kÌ„ |HiN | > kÌ„. Agent i can communicate with all agents in B n in at most |H<kÌ„ |
communication steps. Therefore, her expected payoff is lower-bounded by
                                                       n
                                                        H<                       
                                  n            Î»           kÌ„             1
                           E[Î i (g )] â‰¥                         Â· Ï€âˆ’                  > Ï€ âˆ’ c,
                                              Î»+r                      Ï + ÏÌ„kÌ„
under any equilibrium Ïƒ n for r sufficiently close to zero. Agent i can deviate by forming a costly
link with agent m, such that SC(m) 6= SC(i). However, this is not profitable since from above her
expected payoff under (g n , Ïƒ n ) is at least Ï€ âˆ’ c (which is the maximum possible payoff if an agent
chooses to form a costly link).
                                                                                                n = 1.
Case 2: Agent i is a connector, i.e., there exists exactly one j, such that SC(j) 6= SC(i) and gij
Using a similar argument as above we can show that it is not profitable for agent i to form an additional
costly link with an agent m, such that SC(m) 6= SC(i). On the other hand, agent i could deviate by
         n = 0. However, then her expected payoff would be
setting gij
                                                                     
                       n                1      Î»                1
                E[Î i (g )] = max Ï€ âˆ’        ,       Ï€âˆ’                                                  (44)
                                      Ï + ÏÌ„ r + Î»        Ï + ÏÌ„|Hin |
                                                                       
                                        1      Î»                  1
                           â‰¤ max Ï€ âˆ’        ,       Ï€âˆ’                      < Ï€ âˆ’ c âˆ’ 0
                                      Ï + ÏÌ„ r + Î»        Ï + ÏÌ„(kÌ„ âˆ’ 1)
                                   Hn                      
                                Î»     <kÌ„            1
                           <                  Ï€âˆ’                âˆ’ c âˆ’ ,
                               r+Î»               Ï + ÏÌ„|B n |

for discount rate sufficiently close to zero. Therefore deleting the costly link is not a profitable devia-
tion. Similarly we can show that it a (weakly) dominant strategy for the connector not to replace her
costly link with another costly link.
   We showed that (g n , Ïƒ n ) is an equilibrium of the network learning game. Note that we described a
link formation strategy, in which social cliques connect to each other in a specific order (in increasing
                                                                                       n | cliques is an
size). There is nothing special about this ordering and any permutation of the first |H< kÌ„
equilibrium as long as they form a directed ring. Finally, any node in a social clique can be a receiver
or a sender.
   Next, we argue that asymptotic learning occurs in network equilibria (g, Ïƒ) = {(g n , Ïƒ n )}âˆž
                                                                                               n=1 ,
where for all n > N , N is a large constant, g n has the form described above. As shown above,


                                                           43
                     H`n1       X
                                                                   H`n
                     H`n2                                                           X
                 (a) Deviation for i âˆˆ H`n1 - property (i).    (b) Deviation for i âˆˆ H`n - property (ii).


                            Figure 9: Communication networks under condition (13).

                        n
all agents i for which HSC(i) < kÌ„ have the same neighborhood, which we denoted by B n . Moreover,
limnâ†’âˆž |B n | = âˆž, since social cliques with size less than kÌ„ are connected to the ring and, by condition
(13), limnâ†’âˆž i| |H n |<kÌ„ |Hin | = âˆž. For discount rate r sufficiently close to zero and from arguments
              P
                      i

similar to those in the proof of part (b), we conclude that asymptotic learning occurs in network
equilibria (g, Ïƒ).
Part 2: We have shown a particular form of network equilibria, in which asymptotic learning occurs.
The following proposition states that for discount rate sufficiently close to zero network equilibria fall
in one of two forms.

Proposition 9. Suppose Assumptions 1, 3 and condition (13) hold. Then, an equilibrium (g n , Ïƒ n ) of
the network learning game Î“(C n ) can be in one of the following two forms.
                                                                                             n |,
  (i) (Incomplete) Ring Equilibrium: Social cliques with indices {1, Â· Â· Â· , j}, where j â‰¤ |H<kÌ„
      form a directed ring as described in Part 1 and the rest of the social cliques are isolated. We
      call those equilibria ring equilibria and, in particular, a ring equilibrium is called complete if
            n |, i.e., if all social cliques with size less than kÌ„ are not isolated.
      j = |H< kÌ„

                                                                                         n |, and clique
 (ii) Directed Line Equilibrium: Social cliques with indices {1, Â· Â· Â· , j}, where j â‰¤ |H< kÌ„
                   n | (the largest clique) form a directed line with the latter being the endpoint. The
      with index |HK n

      rest of the social cliques are isolated.

Proof. Let (g n , Ïƒ n ) be an equilibrium of the network learning game Î“(C n ). Monotonicity of the
expected payoff as a function of the number of signals observed implies that if clique H`n is not
isolated, then no clique with index less than ` is isolated in the communication network induced by g n .
In particular, let conn(`) be the connector of social clique H`n and E[Î conn(`) (g n )] be her expected
payoff. Consider an agent i such that SC(i) = `0 < ` and, for the sake of contradiction, H`n0 is
isolated in the communication network induced by g n . Social cliques are ordered by size, therefore,
|H`n0 | â‰¤ |H`n |. Now, we use the monotonicity mentioned above. Consider the expected payoff of i:
                                                                     
                         n               1      Î»             1
                  E[Î i (g )] = max Ï€ âˆ’       ,        Ï€âˆ’
                                       Ï + ÏÌ„ Î» + r      Ï + ÏÌ„|H`n0 |
                                                                     
                                         1      Î»             1
                             â‰¤ max Ï€ âˆ’       ,        Ï€âˆ’                  < E[Î conn(`) (g n )],    (45)
                                       Ï + ÏÌ„ Î» + r      Ï + ÏÌ„|H`n |


                                                              44
where the last inequality follows from the fact that agent conn(`) formed a costly link. Consider a
deviation, gin,deviation for agent i, in which gi,conn(`)
                                                n,deviation          n,deviation
                                                            = 1 and gij             n , i.e., agent i forms a
                                                                                 = gij
costly link with agent conn(`). Then,

                                                     Î»
                         E[Î i (g n,deviation )] â‰¥       E[Î conn(`) (g n )] > E[Î i (g n )],
                                                    Î»+r

from (45) and for discount rate sufficiently close to zero. Therefore, social clique H`n0 will not be
isolated in any network equilibrium (g n , Ïƒ n ).
   Next, we show two structural properties that all network equilibria (g n , Ïƒ n ) should satisfy, when
the discount rate r is sufficiently close to one. We say that there exists a path P between social cliques
H`n1 and H`n2 , if there exists a path between some i âˆˆ H`n1 and j âˆˆ H`n2 . Also, we say that the in-degree
(out-degree) of social clique H`n1 is k, if the sum of in-links (out-links) of the nodes in H`n1 is k, i.e.,
H`n1 has in-degree k if                    n
                        P        P
                           iâˆˆH n
                               `1    / n gij = k.
                                   j âˆˆH `1


  (i) Let H`n1 , H`n2 be two social cliques that are not isolated. Then, there should exist a directed path
      P in Gn induced by g n between the two social cliques.

 (ii) The in-degree and out-degree of each social clique is at most one.

Figure 9 provides an illustration of why the properties hold for patient agents. In particular, for
property (i), let i = conn(H`n1 ) and j = conn(H`n2 ) and assume, without loss of generality, that
|Bin | â‰¤ |Bjn |. Then, for discount rate sufficiently close to zero and from monotonicity of the expected
payoff, we conclude that i has an incentive to deviate, delete her costly and form a costly link with
agent j. Property (ii) follows due to similar arguments. From the above, we conclude that the only
two potential equilibrium topologies are the (incomplete) ring and the directed line with the largest
clique being the endpoint under the assumptions of the proposition.
   So far we have shown a particular form of network equilibria that arise under condition (13), in
which asymptotic learning occurs. We also argued that under condition (13) only (incomplete) ring
or directed line equilibria can arise for network learning game Î“(C n ). In the remainder we show that
there exists a bound cÌ„ > 0 on the common cost c for forming a link between two social cliques, such
that if c < cÌ„ all network equilibria (g, Ïƒ) that arise satisfy that g n is a complete ring equilibrium for
all n > N , where N is a constant. In those network equilibria asymptotic learning occurs as argued
in Part 1. On the other hand, if c > cÌ„ coordination among the social cliques may fail and additional
equilibria arise in which asymptotic learning does not occur. Let
                                (                                               )
                                                 1                       1
                       cÌ„n = min âˆ’                              +          n |)                         (46)
                                   Ï + ÏÌ„( kj=1 |Hjn | + |Hk+1
                                                           n |)   Ï + ÏÌ„|Hk+1
                                          P
                              k




                                                          45
                 n | and
                                 Pk1          n         n | (size of the largest social clique). Moreover, let
where k1 â‰¤ k < |H<kÌ„                    j=1 |Hj |   â‰¥ |HK n



                                                         cÌ„ = lim inf cÌ„n .
                                                                 nâ†’âˆž

The following proposition concludes the proof.

Proposition 10. Suppose Assumptions 1, 3 and condition (13) hold. If c < cÌ„ asymptotic learning
occurs in all network equilibria (g, Ïƒ). Otherwise, there exist equilibria in which asymptotic learning
does not occur.

Proof. Let the common cost c be such that c < cÌ„, where cÌ„ is defined as above, and consider a network
equilibrium (g, Ïƒ). Let N be a large enough constant and consider the corresponding g n for n > N .
We claim that g n is a complete ring equilibrium for all such n. Assume for the sake of contradiction
that the claim is not true. Then, from Proposition 9, g n is either an incomplete ring equilibrium or
a directed line equilibrium. We consider the former case (the latter case can be shown with similar
arguments). There exists an isolated social clique H`n , such that |H`n | < kÌ„ and all cliques with index
less than ` are not isolated and belong to the incomplete ring. However, from the definition of cÌ„ we
obtain that an agent i âˆˆ H`n would have an incentive to connect to the incomplete ring, thus we reach a
                                                                                           n,deviation
contradiction. In particular, consider the following link formation strategy for agent i: gim          =1
               n        n,deviation    n for j 6= m. Then,
for agent m âˆˆ H`âˆ’1 and gij          = gij
                                               |H n |                                    !
                                             Î»     <kÌ„                   1
              E[Î ni (g n,deviation )]   â‰¥                Ï€âˆ’                                  âˆ’c
                                           Î»+r               Ï + ÏÌ„( `âˆ’1     n | + |H n |)
                                                                    P
                                                                     j=1 |H  j       `
                                                                                 
                                                      1     Î»              1
                                        > max Ï€ âˆ’        ,        Ï€âˆ’                   = E[Î ni (g n )],
                                                   Ï + ÏÌ„ Î» + r       Ï + ÏÌ„|H`n |

where the strict inequality follows from the definition of cÌ„ for r sufficiently close to zero. Thus, we
conclude that if c < cÌ„, g n is a complete ring for all n > N , where N is a large constant, and from
Part 1 asymptotic learning occurs in all network equilibria (g, Ïƒ). On the contrary, if c > cÌ„, then there
exists an infinite index set W , such that for all n in the (infinite) subsequence, {nw }wâˆˆW , there exists
a k, such that
                                                     1                              1
                                            Pk                         âˆ’c<            n |.           (47)
                           Ï + ÏÌ„(            +      n
                                               j=1 |Hj |
                                                               n |)
                                                             |Hk+1            Ï + ÏÌ„|Hk+1
                 n | < kÌ„ and
                              Pk       n        n
     Moreover, |Hk+1             j=1 |Hj | â‰¥ |HK n |. We conclude that for (47) to hold it has to be that
Pk      n
  j=1 |Hj |   < R, where R is a uniform constant for all n in the subsequence. Consider (g, Ïƒ)âˆž
                                                                                              n=1 , such
that for every n in the subsequence, g n is such that social cliques with index greater than k (as described
above) are isolated and the rest form an incomplete ring or a directed line and Ïƒ n = IN F O(Gn ), where
Gn is the communication network induced by g n . From above, we obtain that for c > cÌ„, (g n , Ïƒ n ) is an
equilibrium of the network learning game Î“(C n ). perfect assymptotic learning, however, fails in such
an equilibrium, since for every i âˆˆ N n , |Bin | â‰¤ R, where Bin denotes the neighborhood of agent i.


                                                                46
References
Acemoglu, D., M. Dahleh, I. Lobel, and A. Ozdaglar (2010): â€œBayesian learning in social
networks,â€ Working paper.
Ambrus, A., E. Azevedo, and Y. Kamada (2010): â€œHierarchical Cheap Talk,â€ Working paper.
Angeletos, G., and A. Pavan (2007): â€œEfficient use of information and social value of informa-
tion,â€ Econometrica, 75(4), 1103â€“1142.
Austen-Smith, D. (1990): â€œInformation transmission in debate,â€ American Journal of Political
Science, 34(1), 124â€“152.
Austen-Smith, D., and J. Banks (1996): â€œInformation aggregation, rationality, and the Condorcet
jury theorem,â€ American Political Science Review, 90(1), 34â€“45.
Bala, V., and S. Goyal (1998): â€œLearning from neighbours,â€ Review of Economic Studies, 65(3),
595â€“621.
           (2000): â€œA noncooperative model of network formation,â€ Econometrica, 68(5), 1181â€“1229.
Banerjee, A. (1992): â€œA simple model of herd behavior,â€ Quarterly Journal of Economics, 107(3),
797â€“817.
Banerjee, A., and D. Fudenberg (2004): â€œWord-of-mouth learning,â€ Games and Economic
Behavior, 46, 1â€“22.
BarabaÌsi, A., and R. Albert (1999): â€œEmergence of scaling in random networks,â€ Science,
286(5439), 509â€“512.
Bikhchandani, S., D. Hirshleifer, and I. Welch (1992): â€œA theory of fads, fashion, custom,
and cultural change as informational cascades,â€ Journal of Political Economy, 100(5), 992â€“1026.
Crawford, V., and J. Sobel (1982): â€œStrategic information transmission,â€ Econometrica, 50(6),
1431â€“1451.
DeMarzo, P., D. Vayanos, and J. Zwiebel (2003): â€œPersuasion Bias, Social Influence, and
Unidimensional Opinions,â€ Quarterly Journal of Economics, 118(3), 909â€“968.
Duffie, D., S. Malamud, and G. Manso (2009): â€œInformation Percolation with Equilibrium
Search Dynamics,â€ Econometrica, 77(5), 1513â€“1574.
Ellison, G., and D. Fudenberg (1995): â€œWord-of-mouth learning,â€ The Quarterly Journal of
Economics, 110, 93â€“126.
Farrell, J., and R. Gibbons (1989): â€œCheap talk with two audiences,â€ American Economic
Review, 79(5), 1214â€“1223.
Gale, D., and S. Kariv (2003): â€œBayesian learning in social networks,â€ Games and Economic
Behavior, 45(2), 329â€“346.
Galeotti, A. (2006): â€œOne-way flow networks: the role of heterogeneity,â€ Economic Theory, 29(1),
163â€“179.



                                                47
Galeotti, A., C. Ghiglino, and F. Squintani (2010): â€œStrategic information transmission in
networks,â€ Working paper.
Galeotti, A., and S. Goyal (2010): â€œThe law of the few,â€ forthcoming in the American Economic
Review.
Galeotti, A., S. Goyal, and J. Kamphorst (2006): â€œNetwork formation with heterogeneous
players,â€ Games and Economic Behavior, 54(2), 353â€“372.
Gladwell, M. (2000): The Tipping Point: How little things can make a big difference. Little Brown.
Golub, B., and M. Jackson (2010): â€œNaÄ±Ìˆve Learning in Social Networks and the Wisdom of
Crowds,â€ American Economic Journal: Microeconomics, 2(1), 112â€“149.
Goyal, S. (2007): Connections: an introduction to the economics of networks. Princeton University
Press, Princeton, New Jersey.
Hagenbach, J., and F. Koessler (2010): â€œStrategic communication networks,â€ Review of Eco-
nomic Studies, 77(3), 1072â€“1099.
Hojman, D., and A. Szeidl (2008): â€œCore and periphery in networks,â€ Journal of Economic
Theory, 139(1), 295â€“309.
Jackson, M. (2004): A survey of models of network formation: stability and efficiency. In group
formation in Economics; Networks, clubs and coalitions, edited by Gabrielle Demange and Myrna
Wooders. Cambridge University Press.
          (2008): Social and economic networks. Princeton University Press, Princeton, New Jersey.
Jackson, M., and B. Rogers (2006): â€œSearch in the formation of large networks: How random
are social networks?,â€ American Economic Review, 97(3), 890â€“915.
Mobius, M., T. Phan, and A. Szeidl (2010): â€œTreasure Hunt: Social Learning in Real-World
Social Networks,â€ Working paper.
Morgan, J., and P. Stocken (2008): â€œInformation aggregation in polls,â€ American Economic
Review, 98(3), 864â€“896.
Morris, S., and H. S. Shin (2002): â€œSocial value of public information,â€ American Economic
Review, 92(5), 1521â€“1534.
Smith, L., and P. SÃ¸rensen (2000): â€œPathological outcomes of observational learning,â€ Econo-
metrica, 68(2), 371â€“398.
          (2010): â€œRational social learning with random sampling,â€ Working paper.




                                                48
