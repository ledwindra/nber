NBER WORKING PAPER SERIES

MACROECONOMICS WITH HETEROGENEITY:
A PRACTICAL GUIDE
Fatih Guvenen
Working Paper 17622
http://www.nber.org/papers/w17622
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
November 2011

This article was prepared for a special issue of the Federal Reserve Bank of Richmond’s Economic
Quarterly. For helpful discussions, I thank Dean Corbae, Cristina De Nardi, Per Krusell, Serdar Ozkan,
and Tony Smith. Special thanks to Andreas Hornstein and Kartik Athreya for detailed comments on
the draft. David Wiczer and Cloe Ortiz de Mendivil provided excellent research assistance. The views
expressed herein are those of the author and not necessarily those of the Federal Reserve Bank of Chicago,
the Federal Reserve System, or the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2011 by Fatih Guvenen. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.

Macroeconomics With Heterogeneity: A Practical Guide
Fatih Guvenen
NBER Working Paper No. 17622
November 2011, Revised January 2012
JEL No. E1,E13,E21,E24,E32,E6
ABSTRACT
This article reviews macroeconomic models with heterogeneous households. A key question for the
relevance of these models concerns the degree to which markets are complete. This is because the
existence of complete markets imposes restrictions on (i) how much heterogeneity matters for aggregate
phenomena and (ii) the types of cross-sectional distributions that can be obtained. The degree of market
incompleteness, in turn, depends on two factors: (i) the richness of insurance opportunities provided
by the economic environment and (ii) the nature and magnitude of idiosyncratic risks to be insured.
First, I review a broad collection of empirical evidence—from econometric tests of “full insurance,”
to quantitative and empirical analyses of the permanent income (“self-insurance”) model that examine
how it fits the facts about life cycle allocations, to studies that try to directly measure where economies
place between these two benchmarks (“partial insurance”). The empirical evidence I survey reveals
significant uncertainty in the profession regarding the magnitudes of idiosyncratic risks as well as
whether or not these risks have increased since the 1970s. An important difficulty stems from the fact
that inequality often arises from a mixture of idiosyncratic risk and fixed (or predictable) heterogeneity,
making the two challenging to disentangle. I also discuss applications of incomplete markets models
to trends in wealth, consumption, and earnings inequality both over the life cycle and over time, where
this challenge is evident. Third, I discuss “approximate” aggregation—the finding that some incomplete
markets models generate aggregate implications very similar to representative-agent models. What
approximate aggregation does and does not imply is illustrated through several examples. Finally,
I discuss some computational issues relevant for solving and calibrating such models and I provide
a simple yet fully parallelizable global optimization algorithm that can be used to calibrate heterogeneous
agent models.
Fatih Guvenen
Department of Economics
University of Minnesota
4-151 Hanson Hall
1925 Fourth Street South
Minneapolis, MN, 55455
and NBER
guvenen@umn.edu

What is the origin of inequality among men and is it authorized by natural law?
—Academy of Dijon, 1754 (Theme for essay competition)

1

Introduction

The quest for the origins of inequality has kept philosophers and scientists occupied for
centuries. A central question of interest—also highlighted in Academy of Dijon’s solicitation
for its essay competition1 —is whether inequality is determined solely through a natural
process or through the interaction of innate diﬀerences with man-made institutions and
policies. And, if it is the latter, what is the precise relationship between these origins and
socio-economic policies?
While many interesting ideas and hypotheses have been put forward over time, the main
impediment to progress came from the diﬃculty of scientifically testing these hypotheses,
which would allow researchers to refine ideas that were deemed promising and discard those
that were not. Economists, who grapple with the same questions today, have three important advantages that can allow us to make progress. First, modern quantitative economics
provides a wide set of powerful tools, which allow researchers to build “laboratories” in which
various hypotheses regarding the origins and consequences of inequality can be studied. Second, the widespread availability of rich micro data sources—from cross-sectional surveys to
panel datasets from administrative records that contain millions of observations—provides
fresh input into these laboratories. Third, thanks to Moore’s law, the cost of computation
has fallen radically in the past decades, making it feasible to numerically solve, simulate, and
estimate complex models with rich heterogeneity on a typical desktop workstation available
to most economists.
There are two broad sets of economic questions for which economists might want to
model heterogeneity. First, and most obviously, these models allow us to study crosssectional, or distributional, phenomena. The US economy today provides ample motivation
for studying distributional issues, with the top 1% of households owning almost half of
all stocks and 1/3 of all net worth in the United States, and wage inequality having risen
virtually without interruption for the last forty years. Not surprisingly, many questions of
current policy debate are inherently about their distributional consequences. For example,
heated disagreements about major budget issues—such as reforming Medicare, Medicaid,
and the Social Security system—often revolve around the redistributional eﬀects of such
changes. Similarly, a crucial aspect of the current debate on taxation is about “who should
pay what?”. Answering these questions would begin with a sound understanding of the
fundamental determinants of diﬀerent types of inequality.
1
The competition generated broad interest from scholars of the time, including Jean-Jacques Rousseau,
who wrote his famous Discourse on the Origins of Inequality in response, but failed to win the top prize.

3

A second set of questions for which heterogeneity could matter involve aggregate phenomena. This second use of heterogeneous-agent models is less obvious than the first,
because various aggregation theorems as well as numerical results (e.g., Rios-Rull (1996)
and Krusell and Smith (1998)) have established that certain types of heterogeneity do not
change (many) implications relative to a representative-agent model.2
To understand this result and its ramifications, in Section 2, I start by reviewing some
key theoretical results on aggregation (Rubinstein (1974) and Constantinides (1982)). Our
interest in these theorems comes from a practical concern: basically, a subset of the conditions required by these theorems are often satisfied in heterogeneous-agent models, making
their aggregate implications closely mimic those from a representative-agent economy. For
example, an important theorem proved by Constantinides (1982) establishes the existence
of a representative agent if markets are complete.3 This central role of complete markets
turned the spotlight since the late 1980s onto its testable implications for perfect risk sharing
(or “full insurance”). As I review in Section 3, these implications have been tested by an extensive literature using datasets from all around the world—from developed countries such
as the United States to village economies in India, Thailand, Uganda, and so on. While
this literature delivered a clear statistical rejection, it also revealed a surprising amount
of “partial” insurance, in the sense that individual consumption growth (or, more generally, marginal utility growth) does not seem to respond to many seemingly large shocks,
such as long spells of unemployment, strikes, and involuntary moves (Cochrane (1991) and
Townsend (1994), among others).
This raises the more practical question of “how far are we from the complete markets
benchmark?”. To answer this question, researchers have recently turned to directly measuring the degree of partial insurance, defined for our purposes as the degree of consumption
smoothing over and above what an individual can achieve on her own via “self-insurance”
in a permanent income model (i.e., using a single risk-free asset for borrowing and saving).
Although this literature is quite new—and so a definitive answer is still not on hand—it is
likely to remain an active area of research in the coming years.
The empirical rejection of the complete markets hypothesis launched an enormous literature on incomplete markets models starting in the early 1990s, which I discuss in Section
4. Starting with Imrohoroglu (1989), Huggett (1993), and Aiyagari (1994), this literature
has been addressing issues from a very broad spectrum, covering diverse topics such as the
equity premium and other puzzles in finance; important life cycle choices, such as education,
marriage/divorce, housing purchases, fertility choice, etc.; aggregate and distributional effects of a variety of policies ranging from capital and labor income taxation to the overhaul
2

These aggregation results do not imply that all aspects of a representative-agent model will be the same
as those of the underlying individual problem. I discuss important examples to the contrary in Section 7.2.
3
(Financial) markets are “complete” when agents have access to a suﬃciently rich set of assets that allows
them to transfer their wealth/resources across any two dates and/or states of the world.

4

of Social Security, reforming the health care system, among many others. An especially important set of applications concerns trends in wealth, consumption, and earnings inequality.
These are discussed in Section 5.
A critical pre-requisite for these analyses is the disentangling of “ex ante heterogeneity”
from “risk/uncertainty” (also called ex post heterogeneity)—two sides of the same coin,
with potentially very diﬀerent implications for policy and welfare. But this is a challenging
task, because inequality often arises from a mixture of heterogeneity and idiosyncratic
risk, making the two diﬃcult to disentangle. It requires researchers to carefully combine
cross-sectional information with suﬃciently long time-series data for analysis. The stateof-the-art methods used in this field increasingly blend the set of tools developed and used
by quantitative macroeconomists with those used by structural econometricians. Despite
the application of these sophisticated tools, there remains significant uncertainty in the
profession regarding the magnitudes of idiosyncratic risks as well as whether or not these
risks have increased since the 1970s.
The Imrohoroglu-Huggett-Aiyagari framework sidestepped a diﬃcult issue raised by the
lack of aggregation—that aggregates, including prices, depend on the entire wealth distribution. This was accomplished by abstracting from aggregate shocks, which allowed them to
focus on stationary equilibria in which prices (the interest rate and the average wage) were
simply some constants to be solved for in equilibrium. A far more challenging problem with
incomplete markets arises in the presence of aggregate shocks, in which case equilibrium
prices become functions of the entire wealth distribution, which varies with the aggregate
state. Individuals need to know these equilibrium functions, so that they can forecast how
prices will evolve in the future as the aggregate state evolves in a stochastic manner. Because the wealth distribution is an infinite-dimensional object, an exact solution is typically
not feasible. Krusell and Smith (1998) proposed a solution whereby one approximates the
wealth distribution with a finite number of its moments (inspired by the idea that a given
probability distribution can be represented by its moment generating function). In a remarkable finding, they showed that the first moment (the mean) of the wealth distribution
was all individuals needed to track in this economy for predicting all future prices. This
result—generally known as “approximate aggregation”—is a double-edged sword. On the
one hand, it makes feasible the solution of a wide range of interesting models with incomplete
markets and aggregate shocks. On the other hand, it suggests that ex post heterogeneity
does not often generate aggregate implications much diﬀerent from a representative-agent
model. So, the hope that some aggregate phenomena that were puzzling in representativeagent models could be explained in an incomplete markets framework is weakened with this
result. While this is an important finding, there are many examples where heterogeneity
does aﬀect aggregates in a significant way. I discuss a variety of such examples.
Finally, I turn to computation and calibration. First, in Section 6, I discuss some details
of the Krusell-Smith method. A number of potential pitfalls are discussed and alternative
5

checks of accuracy are studied. Second, an important practical issue that arises with calibrating/estimating large and complex quantitative models is the following. The objective
function that we minimize often has lots of jaggedness, small jumps, and/or deep ridges
due to a variety of reasons that have to do with approximations, interpolations, binding
constraints, etc. Thus, local optimization methods are typically of little help on their own,
because they very often get stuck in some local minima. In Section 8, I describe a global
optimization algorithm that is simple yet powerful and is fully parallelizable without requiring any knowledge of MPI, OpenMP, and so on. It works on any number of computers
that are connected to the Internet and have access to a synchronization service like DropBox. I provide a discussion of ways to customize this algorithm with diﬀerent options to
experiment.

2

Aggregation

Even in a simple static model with no uncertainty we need a way to deal with consumer
heterogeneity. Adding dynamics and risk into this environment make things more complex
and require a diﬀerent set of conditions to be imposed. In this section, I will review some key
theoretical results on various forms of aggregation. I begin with a very simple framework
and build up to a fully dynamic model with idiosyncratic (i.e., individual-specific) risk and
discuss what types of aggregation results one can hope to get and under what conditions.
Our interest in aggregation is not mainly for theoretical reasons. As we shall see,
some of the conditions required for aggregation are satisfied (sometimes inadvertently!)
by commonly used heterogeneous-agent frameworks, making them behave very much like
a representative-agent model. Although this often makes the model easier to solve numerically, at the same time it can make its implications “boring”—i.e., too similar to a
representative-agent model. Thus, learning about the assumptions underlying the aggregation theorems can allow model builders to choose the features of their models carefully so
as to avoid such outcomes.

2.1

A Static Economy

Consider a finite set I (with cardinality I) of consumers who diﬀer in their preferences (over
l types of goods) and wealth in a static environment. Consider a particular good and let
xi (p, wi ) denote the demand function of consumer i for this good, given prices p 2 Rl and
wealth wi . Let (w1 , w2 , ..., wI ) be the vector of wealth levels for all I consumers. “Aggregate
demand” in this economy can be written as
x (p, w1 , w2 , ..., wI ) =

I
X
i=1

6

xi (p, wi ) .

As seen here, the aggregate demand function x depends on the entire wealth distribution,
which is a formidable object to deal with. The key question then is, when can we write
P
x(p, w1 , w2 , · · · , wn ) ⌘ x(p, wi )? For the wealth distribution to not matter, we need
aggregate demand to not change for any redistribution of wealth that keeps aggregate
P
wealth constant ( dwi = 0). Taking the total derivative of x, and setting it to zero yields
P
n
X
@xi (p, wi )
@x(p, wi )
=0)
dwi = 0
@wi
@wi
i=1

for all possible redistributions. This will only be true if
@xj (p, wj )
@xi (p, wi )
=
@wi
@wj

8i, j 2 I.

Thus, the key condition for aggregation is that individuals have the same marginal
propensity to consume (MPC) out of wealth (or linear Engel curves). In one of the earliest
works on aggregation, Gorman (1961) formalized this idea via restrictions on consumers’
indirect utility function, which delivers the required linearity in Engel curves.
Theorem 1 (Gorman (1961))
Consider an economy with N < 1 commodities and a set I of consumers. Suppose that
the preferences of each consumer i 2 I can be represented by an indirect utility function4 of
the form
vi (p, yi ) = ai (p) + b(p)wi ,
and that each household i 2 I has a positive demand for each commodity, then these preferences can be aggregated and represented by those of a representative household, with indirect
utility
v(p, y) = a(p) + b(p)w,
P
P
where a(p) = i ai (p) and w = i wi is aggregate income.

As we shall see later, the importance of linear Engel curves (or constant MPCs) for
aggregation is a key insight that carries over to much more general models, all the way up
to the infinite-horizon incomplete markets model with aggregate shocks studied in Krusell
and Smith (1998).
4

Denoting the consumer’s utility function over goods with U , the indirect utility function is simply
vi (p, wi ) ⌘ U (xi (p, wi ))—that is, the maximum utility of a consumer who has wealth wi and faces price
vector p.

7

2.2

A Dynamic Economy (No Idiosyncratic Risk)

Rubinstein (1974) extends Gorman’s result to a dynamic economy where individuals consume out of wealth (no income stream). Linear Engel curves are again central in this
context.
Consider a frictionless economy in which each individual solves an intertemporal consumptionsavings/portfolio allocation problem. That is, every period current wealth wt is apportioned
between current consumption ct and a portfolio of a risk-free and a risky security with respective (gross) returns Rtf and Rts .5 Let ↵t denote the portfolio share of the risk-free asset
at time t, and denote the subjective time discount factor. Individuals solve:
!
T
X
t
max E
U (ct )
{ct ,↵t }

t=1

s.t. wt+1 = (wt

ct )(↵t Rtf + (1

↵t )Rts ).

Furthermore, assume that the period utility function, U, belongs to the Hyperbolic Absolute Risk Aversion (HARA) class, which is defined as utility functions that have linear
00
risk tolerance: T (c) ⌘ U (c)0 /U (c) = ⇢ + c and < 1.6 This class encompasses three
1
utility functions that are well-known in economics: U (c) = (
1) 1 (⇢ + c)1
(Generalized power utility; Standard CRRA form when ⇢ ⌘ 0); U (c) = ⇢ ⇥ exp( c/⇢) if ⌘ 0
(exponential utility); and U (c) = 0.5(⇢ c)2 defined for values c < ⇢ (Quadratic utility).
The following theorem gives six sets of conditions under which aggregation obtains.
Theorem 2 (Rubinstein (1974))7
Consider the following homogeneity conditions:
1. All individuals have the same resources w0 , and tastes
2. All individuals have the same

and taste parameters

3. All individuals have the same taste parameters

and U .
6= 0.

= 0.

4. All individuals have the same resources w0 and taste parameters ⇢ = 0 and

= 1.

5. A complete market exists and all individuals have the same taste parameter

= 0.

5

We can easily allow for multiple risky securities at the expense of complicating the notation.
“Risk tolerance” is the reciprocal of the Arrow-Pratt measure of “absolute risk aversion,” which measures
consumers’ willingness to bear a fixed amount of consumption risk. See, e.g., Pratt (1964).
7
The language of the theorem below diﬀers from Rubinstein’s original statement by assuming rational
expectations and combines results with the extension to a multiperiod setting in his footnote 5.
6

8

6. A complete market exists and all individuals have the same resources w0 and taste ,
⇢ = 0, and = 1.
Then, all equilibrium rates of return are determined in case (1) as if there exists only
composite individuals each with resources w0 and tastes and U ; and equilibrium rates
of return are determined in cases (2) to (6) as if there exists only composite individuals
P i
each with the following
economic characteristics: (i) Resources: w0 =
w0 /I; (ii) Tastes:
P
P i
i (⇢i / ⇢i )
=⇧( )
(where
⌘ 1/
1) or =
/I; and (iv) preference parameters
P
⇢=
⇢i /I, and .
Several remarks are in order.

Demand Aggregation. An important corollary to this theorem is that whenever a composite consumer can be constructed, in equilibrium, rates of return are insensitive to the
distribution of resources among individuals. This is because the aggregate demand functions
(for both consumption and assets) depend only on total wealth and not on its distribution.
Thus, we have “demand aggregation.”
Aggregation and Heterogeneity in Relative Risk Aversion. Notice that all six cases
that give rise to demand aggregation in the theorem require individuals to have the same
curvature parameter, . To see why this is important, note that (with HARA preferences)
the optimal holdings of the risky asset is a linear function of the consumer’s wealth: 1 +
2 wt / , where 1 and 2 are some constants that depend on the properties of returns. It
is easy to see that with identical slopes, 2 / , it does not matter who holds the wealth.
In other words, redistributing wealth between any two agents would cause changes in total
demand for assets that will cancel out each other, due to linearity and same slopes. Notice
also that while identical curvature is a necessary condition, it is not suﬃcient for demand
aggregation: each of the six cases adds more conditions on top of this identical curvature
requirement.8

2.3

A Dynamic Economy (With Idiosyncratic Risk)

While Rubinstein (1974)’s theorem delivers a strong aggregation result, it achieves this by
abstracting from a key aspect of dynamic economies: uncertainty that evolves over time.
Almost every interesting economy that we discuss in the coming sections will feature some
kind of idiosyncratic risk that individuals face (coming from labor income fluctuations,
8
Notice also that, because in some cases (such as (2)) heterogeneity in ⇢ is allowed, individuals will
exhibit diﬀerent relative risk aversions (if they have diﬀerent wt ), for example in the generalized CRRA
case, and still allow aggregation.

9

shocks to health, shocks to housing prices and asset returns, among others). Rubinstein
(1974)’s theorem is silent about how the aggregate economy behaves under these scenarios.
This is where Constantinides (1982) comes into play: He shows that if markets are
complete, under much weaker conditions (on preferences, beliefs, discount rates, etc.) one
can replace heterogeneous consumers with a planner who maximizes a weighted sum of
consumers’ utilities. In turn, the central planner can be replaced by a composite consumer
who maximizes a utility function of aggregate consumption.
To show this, consider a private ownership economy with production as in Debreu (1959),
with m consumers, n firms, and l commodities. As in Debreu (1959), these commodities
can be thought of a date-event labelled goods (and concave utility functions, Ui , as being
defined over these goods) allowing us to map these results into an intertemporal economy
with uncertainty. Consumer i is endowed with wealth (wi1 , wi2 , ..., wil ) and shares of firms
P
(✓i1 , ✓i2 , ..., ✓in ) with ✓ij 0 and m ✓ij = 1. Let the vectors Ci and Yj denote, respectively,
individual i’s consumption set and firm j’s production set.

⇤ n
⇤
An equilibrium is an (m + n + 1)-tuple ((c⇤i )m
i=1 , (yj )j=1 , p ) such that, as usual, consumers maximize utility, firms maximize their profits, and markets clear. Under standard
assumptions an equilibrium exists and is Pareto optimal.

Optimality implies that there exist positive numbers
solution to the following problem (P1):
max
c,y

m
X

i Ui (ci )

i,

i = 1, ..., m, such that the

(P 1)

i=1

s.t. yj 2 Yj ,

j = 1, 2, ..., n;

ci 2 Ci ,
i = 1, 2, ..., m;
m
n
m
X
X
X
cih =
yjh +
wih ,
h = 1, 2, ..., l,
i=1

j=1

i=1

(where h indexes commodities) is given by (ci ) = (c⇤i ) and (yj ) = (yj⇤ ). Let aggregate
m
X
consumption be z ⌘ (z1 , · · · , zl ), zh ⌘
cih . Now, for a given z, consider the problem
i=1

(P2) of eﬃciently allocating it across consumers:
U (z) ⌘ max
c

m
X

i Ui (ci )

i=1

s.t. ci 2 Ci ,
i = 1, 2, ...m,
m
X
cih = zh ,
h = 1, 2, ...l.
i=1

10

Now, given the production sets of each firm and the aggregate endowments of each
commodity, consider the optimal production decision (P3):
max U (z)
y,z

s.t. yj 2 Yj , 8j;

zh =

X
j

yjh + wh , 8h.

Theorem 3 (Constantinides (1982, Lemma 1))
P
⇤
+ wh , 8h.
(a) The solution to (P3) is (yj ) = (yj⇤ ) and zh = nj=1 yjh
(b) U (z) is increasing and concave in z.
P ⇤
+ wh , 8h, then the solution to (P2) is (xi ) = (x⇤i ).
(c) If zh = yjh

(d) Given i , i = 1, 2, · · · , m, if the consumers are replaced by one composite consumer
with utility U (z), with endowment equal to the sum of m consumers’ endowments and shares
P
⇤
⇤ n
⇤
the sum of their shares, then the (1 + n + 1)-tuple ( m
i=1 ci , (yj )j=1 , p ) is an equilibrium.

Constantinides versus Rubinstein. Constantinides allows for much more generality
than Rubinstein by relaxing two important restrictions. First, no conditions are imposed
on the homogeneity of preferences, which was a crucial element in every version of Rubinstein’s theorem. Second, Constantinides allows for both exogenous endowment as well as
production at every date and state. In contrast, recall that, in Rubinstein’s environment,
individuals start life with a wealth stock and receive no further income or endowment during life. In exchange, Constantinides requires complete markets and does not get demand
aggregation. Notice that the existence of a composite consumer does not imply demand
aggregation, for at least two reasons. First, composite demand depends on the weights
in the planner’s problem and, thus, depends on the distribution of endowments. Second,
the composite is defined at equilibrium prices and no claim is made that its demand curve
coincides with the aggregate demand curve.
Thus, the usefulness of Constantinides’ result hinges on (i) the degree to which markets
are complete, (ii) whether we want to allow for idiosyncratic risk and heterogeneity in
preferences (which are both restricted in Rubinstein’s theorem), and (iii) whether or not we
need demand aggregation. Below I will address these issues in more detail. We will see that,
interestingly, even when markets are not complete, in certain cases, we will not only get
close to a composite consumer representation, but we can also get quite close to the much
stronger result of demand aggregation! An important reason for this outcome is that many
heterogeneous-agent models assume identical preferences, which eliminates an important
source of heterogeneity, satisfying Rubinstein’s conditions for preferences. However, these
11

models do feature idiosyncratic risk, but as we shall we, when the planning horizon is long,
such shocks can often be smoothed eﬀectively using even a simple risk free asset. More on
this in the coming sections.
Completing Markets by Adding Financial Assets. It is useful to distinguish between
“physical” assets—those in positive net supply (e.g. equity shares, capital, housing, etc)—
and “financial” assets—those in zero net supply (bonds, insurance contracts, etc.). The
latter are simply some contracts written on a piece of paper that specifies the conditions
under which one agent transfers resources to another one. In principle, it can be created
with little cost. Now suppose that we live in a world with J physical assets and that there
are S(> J) states of the world. In this general setting, markets are incomplete. However, if
consumers have homogenous tastes, endowments, and beliefs, then markets are (eﬀectively)
complete by simply adding enough financial assets (in zero net supply). There is no loss of
optimality and nothing will change by this action, because in equilibrium identical agents
will not trade with each other. The bottom line is that the more “homogeneity” we are
willing to assume among consumers, the less demanding the complete markets assumption
becomes. This point should be kept in mind as we will return to it later.

3

Empirical Evidence on Insurance

Dynamic economic models with heterogeneity typically feature individual-specific uncertainty that evolves over time—coming from fluctuations in labor earnings, health status,
portfolio returns, among others. Although this structure does not fit into Rubinstein’s
environment, it is covered by Constantinides’ theorem, which requires complete markets.
Thus, a key empirical question is the extent to which complete markets can serve as a useful
benchmark and a good approximation to the world we live in. As we shall see in this section,
the answer turns out to be more nuanced than a simple yes or no.
To explain the broad variety of evidence that has been brought to bear on this question,
this section is structured in the following way. First, in Section 3.1, I begin by discussing
a large empirical literature that has tested a key prediction of complete markets—that
marginal utility growth is equated across individuals. This is often called “perfect” or
“full” insurance, and it is soundly rejected in the data. Section 3.2 discusses an alternative
benchmark, inspired by this rejection. This is the permanent income model, in which
individuals have access to only borrowing and saving—or “self insurance.” In a way, this is
the other extreme end of the insurance spectrum. Finally, Section 3.3 discusses studies that
take an intermediate view—“partial insurance”—and provide some evidence to support it.
We now begin with the tests of full insurance.

12

3.1

Benchmark 1: Full Insurance

To develop the theoretical framework underlying the empirical analyses, start with an economy populated by agents who derive utility from consumption ct as well as some other
good(s) dt : U i cit+1 , dit+1 , where i indexes individuals. These other goods can include
leisure time (of husband and wife if the unit of analysis is a household), children, lagged
consumption (as in habit formation models), so on and so forth.
The key implication of perfect insurance can be derived by following two distinct approaches. The first environment assumes a social planner who pools all individuals’ resources
and maximizes a social welfare function that assigns a positive weight to every individual.
In the second environment, allocations are determined in a competitive equilibrium of a
frictionless economy where individuals are able to trade in a complete set of financial securities. Both of these frameworks make the following strong prediction for the growth rate
of individuals’ marginal utilities:
i
i Uc

cit+1 , dit+1
⇤t+1
=
,
i
i
i
Uc (ct , dt )
⇤t

(1)

where Uc denotes the marginal utility of consumption and ⇤t is the aggregate shock.9 This
condition thus says that every individual’s marginal utility must grow in locksteps with the
aggregate and, hence, with each other. No individual-specific term appears on the right
hand side, such as idiosyncratic income shocks, unemployment, sickness, and so on. All
these idiosyncratic events are perfectly insured in this world. From here one can introduce
a number of additional assumptions for empirical tractability.
Complete Markets and Cross-Sectional Heterogeneity: A Digression. So far we
have focused on what market completeness implies for the study of aggregate phenomena
in light of Constantinides’ theorem. However, complete markets also imposes restrictions
on the evolution of the cross-sectional distribution, which can be seen in condition (1).
For a given specification of U , condition (1) translates into restrictions on the evolutions
of ct and dt (possibly a vector). Although, it is possible to choose U to be suﬃciently
general and flexible (e.g., include preference shifters, assume non-separability) to generate
rich dynamics in cross-sectional distributions, this strategy would attribute all the action
to preferences, which are essentially unobservable. Even in that case, models that are not
bound by (1)—and therefore have idiosyncratic shocks aﬀect individual allocations—can
generate a much richer set of cross-sectional distributions. Whether that extra richness is
necessary for explaining salient features of the data is another matter and is not always
obvious (see, e.g., Caselli and Ventura (2000), Badel and Huggett (2007), and Guvenen and
9

Alternatively stated, ⇤t is the Lagrange multiplier on the aggregate resource constraint at time t in the
planner’s problem or the state price density in the competitive equilibrium interpretation.

13

Kuruscu (2009)).10
Now I return back to the empirical tests of condition (1).
In a pioneering paper, Altug and Miller (1990) were the first to formally test the implications of (1). They considered households as their unit of analysis and specified a rich
Beckerian utility function that included husband’s and wife’s leisure times as well as consumption (food expenditures), and adjusted for demographics (children, age, etc.). Using
data from the Panel Study of Income Dynamics (PSID) they could not reject full insurance. Hayashi et al. (1996) revisited this topic a few years later and using the same data
set, they rejected perfect risk sharing.11 Given this rejection in the whole population, they
investigated if there might be better insurance within families, who presumably have closer
ties with each other than the population at large and could therefore provide insurance to
the members in need. They found that this hypothesis too was statistically rejected.12
In a similar vein, Guvenen (2007a) investigated how the extent of risk sharing varies
across diﬀerent wealth groups, such as stockholders and non-stockholders. This question is
motivated by the observation that stockholders (who made up less than 20% of the population for much of the 20th century) own about 80% of net worth and 90% of financial
wealth in the US economy, and therefore play a disproportionately large role in the determination of macroeconomic aggregates. On the one hand, these wealthy individuals have
access to a wide range of financial securities that can presumably allow better risk insurance;
on the other hand, they are exposed to diﬀerent risks not faced by the less-wealthy nonstockholders. Using data from the PSID, he strongly rejected perfect risk-sharing among
stockholders, but perhaps surprisingly, did not find evidence against it among nonstockholders. This finding suggests further focus on risk factors that primarily aﬀect the wealthy, such
as entrepreneurial income risk that is concentrated at the top of the wealth distribution.
A number of other papers imposed further assumptions before testing for risk sharing.
A very common assumption is the separability between ct and dt , (for example, leisure),
which leads to an equation that only involves consumption (Cochrane (1991), Attanasio
and Davis (1996), Nelson (1994)).13 Assuming power utility in addition to separability, we
10

Caselli and Ventura (2000) show that a wide range of distributional dynamics and income mobility
patterns can arise in the Cass-Koopmans optimal savings model and in the Arrow-Romer model of productivity spillovers. Badel and Huggett (2007) show that lifecycle inequality patterns (discussed in Section
3.2) that have been viewed as evidence of incomplete markets can in fact be generated using a complete
markets model. Guvenen and Kuruscu (2009) show that a human capital model with heterogeneity in
learning ability and skill-biased technical change generates rich non-monotonic dynamics consistent with
the US data since the 1970s, despite featuring no idiosyncratic shocks (and thus has complete markets).
11
Datasets such as the PSID are known to go through regular revisions, which might be able to account
for the discrepancy between the two papers’ results.
12
This finding has implications for the modeling of the household decision-making process as a unitary
model as opposed to one in which there is bargaining between spouses.
13
Non-separability, for example between consumption and leisure, can be allowed for if the planner is
assumed to be able to transfer leisure freely across individuals. While transfers of consumption are easier to

14

can take the logs of both sides of equation (1) and then time-diﬀerence to obtain:
Ci,t =
where Ct ⌘ log(ct ) and Ct ⌘ Ct
running a regression of the form:

⇤t ,

(2)

Ct 1 . Several papers have tested this prediction by

Ci,t =

⇤t +

0

Zit + ✏i,t ,

(3)

where the vector Zit contains factors that are idiosyncratic to individual/household/group
i. Perfect insurance implies that all the elements of the vector
are equal to zero.
Cochrane (1991), Mace (1991), and Nelson (1994) are the early studies that exploit this
simple regression structure. Mace (1991) focused on whether or not consumption responded
to idiosyncratic wage shocks: i.e., Zit = Wti .14 While Mace failed to reject full insurance,
Nelson (1994) later pointed out several issues with the treatment of data (and measurement
error in particular) that aﬀects Mace’s results. Nelson showed that a more careful treatment
of these issues results in strong rejection.
Cochrane (1991) raised a diﬀerent point. He argued that studies, such as Mace’s, that
test risk sharing by examining the response of consumption growth to income may have
low power if income changes are (at least partly) anticipated by individuals. He instead
proposed to use idiosyncratic events that are arguably harder to predict, such as plant
closures, long strikes, long illnesses, and so on. Cochrane rejected full insurance for illness
or involuntary job loss but not for long spells of unemployment, strikes or involuntary moves.
Notice that a crucial assumption in all of the work of this kind is that none of these shocks
can be correlated with unmeasured factors that determine marginal utility growth.
Townsend (1994) tested for risk sharing in village economies of India and concluded that
although the model was statistically rejected, full insurance provided a surprisingly good
benchmark. Specifically, he found that individual consumption comoves with village-level
consumption and is not influenced much by own income, sickness, and unemployment.
Attanasio and Davis (1996) observe that equation (2) must also hold for multi-year
changes in consumption and when aggregated across groups of individuals.15 This implies,
for example, that even if one group of individuals experience faster income growth relative
to another group during a ten year period, their consumption growth must be the same.
implement (through taxes and transfers), the transfer of leisure is harder to defend on empirical grounds.
14
Because individual wages are measured with (often substantial) error in micro survey datasets, an OLS
estimation of this regression would suﬀer from attenuation bias, which may lead to a failure to reject full
insurance even when it is false. The papers discussed here employ diﬀerent approaches to deal with this issue
(such as using an Instrumental Variables regression or averaging across groups to average out measurement
error).
15
Hayashi et al. (1996) also use multi-year changes to test for risk sharing.

15

The substantial rise in the education premium in the United States (i.e., the wages of
college graduates relative to high school graduates) throughout the 1980’s provided a key
test of perfect risk sharing. Contrary to this hypothesis, they found that the consumption
of college graduates grew much faster than that of high school graduates during the same
period, violating the premise of perfect risk sharing.
Finally, Schulhofer-Wohl (2011) sheds new light on this question. He argues that if more
risk-tolerant individuals self-select into occupations with more (aggregate) income risk, then
the regressions in (3) used by Cochrane (1991), Nelson (1994), and others (which incorrectly
assumes away such correlation) will be biased towards rejecting perfect risk sharing. By
using self-reported measures of risk attitudes from the Health and Retirement Survey, he
establishes such a correlation. Then he develops a method to deal with this bias and,
applying the corrected regression, he finds that consumption growth responds very weakly
to idiosyncratic shocks, implying much larger risk sharing than found in these previous
papers. He also shows that the coeﬃcients estimated from this regression can be mapped
into a measure of “partial insurance.”
Taking Stock. As the preceding discussion makes clear, with few exceptions, all empirical studies agree that perfect insurance in the whole population is strongly rejected in
a statistical sense. However, this statistical rejection per se is not suﬃcient to conclude
that complete markets is a poor benchmark for economic analysis for two reasons. First,
there seem to be a fair deal of insurance against certain types of shocks, as documented by
Cochrane (1991) and Townsend (1994), and among certain groups of households, such as
in some villages in less developed countries (Townsend (1994)), or among non-stockholders
in the United States (Guvenen (2007a)). Second, the reviewed empirical evidence arguably
documents statistical tests of an extreme benchmark (equation (1)) that we should not
expect to hold precisely—for every household, against every shock. Thus, with a large
enough sample, statistical rejection should not be surprising.16 What these tests do not do
is to tell us how “far” the economy is from the perfect insurance benchmark. In this sense,
analyses such as in Townsend (1994)—that identify the types of shocks that are and are not
insured—are somewhat more informative than those in Altug and Miller (1990), Hayashi
et al. (1996), and Guvenen (2007a) that rely on model misspecification type tests of risk
sharing.

3.2

Benchmark 2: Self-Insurance

The rejection of full consumption insurance led economists to search for other benchmark
frameworks for studying individual choices under uncertainty. One of the most influential
16

One view is that hypothesis tests without an explicit alternative (such as the ones discussed here) often
“degenerate into elaborate rituals designed to measure the sample size. (Leamer (1983, p. 39))”

16

Figure 1: Within-Cohort Inequality Over Life Cycle
Cross Sectional Variance of Log Income

0.7

Cross Sectional Variance of Log Consumption
0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

30

40

Age

50

60

0.1

30

40

50

60

Age

studies of this kind has been Deaton and Paxson (1994), who brought a diﬀerent kind
of evidence to bear. They began by documenting two empirical facts. Using micro data
from the United States, United Kingdom, and Taiwan, they first documented that withincohort inequality of labor income (as measured by the variance of log income) increases
substantially and almost linearly over the life cycle. Second, they documented that withincohort consumption inequality showed a very similar pattern and also rose substantially as
individuals age. The two empirical facts are replicated in figure 1 from data in Guvenen
(2007b, 2009a).
To understand what these patterns imply for the market structure, first consider a complete markets economy. As we saw in the previous section, if consumption is separable from
leisure and other potential determinants of marginal utility, consumption growth will be
equalized across individuals, independent of any idiosyncratic shock (equation (2)). Therefore, while consumption level may diﬀer across individuals due to diﬀerences in permanent
lifetime resources, this dispersion should not change as the cohort ages.17 Therefore, Deaton
and Paxson (1994)’s evidence has typically been interpreted as contradicting the complete
markets framework. I now turn to the details.
17
There are two obvious modifications that preserve complete markets and would be consistent with rising
consumption inequality. The first one is to introduce heterogeneity in time discounting. This is not very
appealing because it “explains” by entirely relying on unobservable preference heterogeneity. Second, one
could question the assumption of separability: if leisure is non-separable and wage inequality is rising over
the life cycle—which it does—then consumption inequality would also rise to keep marginal utility growth
constant (even under complete markets). But this explanation also predicts that hours inequality should
also rise over the life cycle, a prediction that does not seem to be borne out in the data—although see Badel
and Huggett (2007) for an interesting dissenting take on this point.

17

3.2.1

The Permanent Income Model

The canonical framework for self-insurance is provided by the permanent income lifecycle
model, in which individuals have only access to a risk-free asset for borrowing and saving.
Therefore, as opposed to full insurance, there is only “self-insurance” in this framework.
Whereas the complete markets framework represents the maximum amount of insurance,
the permanent income model arguably provides the lower bound on insurance (to the extent
that we believe individuals have access to a savings technology, and borrowing is possible
subject to some constraints).
It is instructive to develop this framework in some detail as the resulting equations
will come in handy in the subsequent exposition. The framework here closely follows Hall
and Mishkin (1982) and Deaton and Paxson (1994). Start with an income process with
permanent and transitory shocks:
(4)

yt = ytP + "t ,
ytP

=

ytP 1

+ ⌘t .

Suppose that individuals discount the future at the rate of interest and define:
1/(1 + r). Preferences are of quadratic utility form:
max E0
s.t.

T
X

t

"

(yt

T
1X

2

t

(c⇤

t=1

ct ) + A0 = 0,

ct ) 2

=

#
(5)

t=1

where c⇤ is the bliss level and A0 is the initial wealth level (which may be zero). This
problem can be solved in closed form to obtain a consumption function. First-diﬀerencing
this consumption rule yields
c t = ⌘ t + t "t ,
(6)
⌘
⇣P
T t ⌧
where t ⌘ 1/
is the annuitization factor.18 This term is close to zero when
⌧ =0
the horizon is long and the interest rate is not too high, the well-understood implication
being that the response of consumption to transitory shocks is very weak given their low
annuitized value. More importantly: consumption responds to permanent shocks one for
one. Thus, consumption changes reflect permanent income changes.
For the sake of this discussion, assume that the horizon is long enough so that t ⇡ 0
and thus ct ⇠
= ⌘t . If we further assume that covi cit 1 , ⌘ti = 0 (where i indexes individuals
18
Notice that the derivation of (6) requires two more pieces in addition to the Euler equation: it requires
us to explicitly specify the budget constraint (5) as well as the stochastic process for income (4).

18

and the covariance is taken cross-sectionally), we get:
vari (cit ) ⇠
= vari (cit 1 ) + var(⌘t ).
So the rise in consumption inequality from age t 1 to t is a measure of the variance
of the permanent shock between those two ages. Since, as seen in figure 1, consumption
inequality rises significantly and almost linearly, this figure is consistent with permanent
shocks to income that are fully accommodated as predicted by the permanent income model.

Deaton and Paxson’s Striking Conclusion. Based on this evidence, Deaton and Paxson (1994) argued that the permanent income model is a better benchmark for studying
individual allocations than is complete markets. Storesletten et al. (2004b) went one step
further and showed that a calibrated life cycle model with incomplete markets can be quantitatively consistent with the rise in consumption inequality as long as income shocks are
suﬃciently persistent (⇢ & 0.90). In his presidential address to the American Economic
Association, Robert Lucas (2003, p. 10) succinctly summarized this view:
The fanning out over time of the earnings and consumption distributions
within a cohort that Angus Deaton and Paxson (1994) document is striking
evidence of a sizable, uninsurable random walk component in earnings.
This conclusion was shared by the bulk of the profession in the 1990’s and 2000’s, giving
a strong impetus to the development of incomplete markets models featuring large and
persistent shocks that are uninsurable. I review many of these models in Sections 4 and 5.
However, a number of recent papers have revisited the original Deaton-Paxson finding and
have reached a diﬀerent conclusion.
Reassessing the Facts: An Opposite Conclusion. Four of these papers, by and large,
follow the same methodology as described and implemented by Deaton and Paxson (1994),
but each uses a data set that extends the original CE sample used by these authors (that
covered 1980 to 1990) and diﬀer somewhat in their sample selection strategy. Specifically,
Primiceri and van Rens (2009, figure 2) use data from 1980 to 2000; Heathcote et al. (2010,
figure 14) use the 1980 to 1998 sample; Guvenen and Smith (2009, figure 11) use the 1980 to
1992 sample and augment it with the 1972-73 sample; and Kaplan (2010, figure 2) uses data
from 1980 to 2003. Whereas Deaton and Paxson (1994, figures 4 and 8) and Storesletten
et al. (2004b, figure 1) documented a rise in consumption inequality of about 30 log points
(between ages 25 and 65), these four papers find a much smaller rise of about 5–7 log points.

19

Taking Stock.
Taken together, these reanalyses of CE data reveal that Deaton and
Paxson (1994)’s earlier conclusion is not robust to small changes in the sample period
studied. Although more work on this topic certainly seems warranted,19 these recent studies
raise substantial concerns on one of the key pieces of empirical evidence on the extent of
market incompleteness. A small rise in consumption inequality is hard to reconcile with
the combination of large permanent shocks and self-insurance. Hence, if this latter view
is correct, either income shocks are not as permanent as we thought or there is insurance
above and beyond self-insurance. Both of these possibilities are discussed next.

3.3

An Intermediate Case: Partial Insurance

A natural intermediate case to consider is an environment between the two extremes of full
insurance and self insurance. That is, perhaps individuals have access to various sources
of insurance (e.g., through charities, help from family and relatives, etc.) in addition to
borrowing and saving, but these forms of insurance still fall short of full insurance. If this
is the case, is there a way to properly measure the degree of this “partial insurance”?
To address this question, Blundell et al. (2008) examine the response of consumption to
innovations in income. They start with equation (6) derived by Hall and Mishkin (1982)
that links consumption change to income innovations, and modify it by introducing two
parameters—✓ and —to encompass a variety of diﬀerent scenarios:
ct = ✓⌘t +

t "t .

(7)

Now, at one extreme is the self-insurance model (i.e., the permanent income model):
✓ = = 1; at the other extreme is a model with full insurance: ✓ = = 0. Values of
✓ and between zero and one can be interpreted as the degree of partial insurance—the
lower the value, the more insurance there is. In their baseline analysis, Blundell et al. (2008)
estimate ✓ ⇡ 2/3 and find that it does not vary significantly over the sample period.20 They
interpret the estimate of ✓ to imply that about 1/3 of permanent shocks are insured above
and beyond what can be achieved through self-insurance.21
A couple of remarks are in order. First, the derivation of equation (6) that forms
the basis of the empirical analysis here requires quadratic preferences. Indeed, this was
19
For example, as Attanasio et al. (2007) show that the facts regarding the rise in consumption inequality
over time are sensitive to whether one uses the “recall survey” or the “diary survey” in the CE dataset.
All the papers discussed in this section (on consumption inequality over the life cycle, including Deaton
and Paxson (1994)) use the recall survey data. It would be interesting to see if the diary survey alters the
conclusions regarding consumption inequality over the life cycle.
20
They also find
t = 0.0533 (0.0435) indicating very small transmission of transitory shocks to consumption. This is less surprising since it would also be implied by the permanent income model.
21
The parameter is of lesser interest given that transitory shocks are well-known to be smoothed quite
well even in the permanent income model and the value of one estimates depends on what one assumes
about t —hence the interest rates.

20

the maintained assumption in Hall and Mishkin (1982) and Deaton and Paxson (1994).
Blundell et al. (2008) show that one can derive, as an approximation, an analogous equation
(7) with CRRA utility and self-insurance but now ✓ = ⇡ ⇡i,t , where ⇡i,t is the ratio of
human wealth to total wealth. In other words, the coeﬃcients ✓ and are both equal
to 1 under self-insurance only if preferences are of quadratic form; generalizing to CRRA
predicts that even with self insurance the response to permanent shocks, given by ⇡i,t , will
be less than one for one if non-human wealth is positive. Thus, accumulation of wealth
due to precautionary savings or retirement can dampen the response of consumption to
permanent shocks and give the appearance of partial insurance. Blundell et al. (2008)
examine if younger individuals (who have less non-human wealth and thus have a higher
⇡i,t than older individuals) have a higher response coeﬃcient to permanent shocks. They
do find this to be the case.
Insurance or Advance Information? Primiceri and van Rens (2009) conduct an analysis similar to Blundell et al. (2008) and also find a small response of consumption to
permanent income movements. However, they adopt a diﬀerent interpretation for this
finding—that income movements are largely “anticipated” by the individuals as opposed to
being genuine permanent “shocks.” As has been observed as far back as Hall and Mishkin
(1982), this alternative interpretation illustrates a fundamental challenge with this kind of
analysis: advance information and partial insurance are diﬃcult to disentangle by simply
examining the response of consumption to income.
Insurance or Less Persistent Shocks? Kaplan and Violante (2010) raise two more
issues regarding the interpretation of ✓. First, they ask, what if income shocks are persistent but not permanent? This is a relevant question because, as I discuss in the next
section, nearly all empirical studies that estimate the persistence coeﬃcient (of an AR(1)
or ARMA(1,1)) find it to be 0.95 or lower—sometimes as low as 0.7. To explore this issue, they simulate data from a life cycle model with self insurance only, in which income
shocks follow an AR(1) process with a first order autocorrelation of 0.95. They show that
when they estimate ✓ as in Blundell et al. (2008), they find it to be close to the 2/3 figure
reported by these authors.22 Second, they add a retirement period to the lifecycle model,
which has the eﬀect that now even a unit root shock is not permanent, given that its eﬀect
does not translate one for one into the retirement period. Thus, individuals have even more
reason not to respond to permanent shocks, especially when they are closer to retirement.
Overall, their findings suggest that the response coeﬃcient of consumption to income can
be generated in a model of pure self-insurance to the extent that income shocks are allowed
22

The reason is simple. Because the AR(1) shock decays exponentially, this shock loses 5% of its value
in one year, but 1 0.9510 ⇡ 40% in 10 years and 65% in 20 years. Thus, the discounted lifetime value of
such a shock is significantly lower than a permanent shock which retains 100% of its value at all horizons.

21

to be slightly less than permanent.23 One feature this model misses, however, is the age
profile of response coeﬃcients, which shows no clear trend in the data according to Blundell
et al. (2008), but is upward sloping in Kaplan and Violante (2010)’s model.
Taking Stock. Before the early 1990s, economists typically appealed to aggregation theorems to justify the use of representative agent models. Starting in the 1990s, the widespread
rejections of the full insurance hypothesis (necessary for Constantinides (1982)’s theorem)
combined with the findings of Deaton and Paxson (1994) led economists to adopt versions
of the permanent income model as a benchmark to study individual’s choices under uncertainty (Hubbard et al. (1995), Carroll (1997), Carroll and Samwick (1997), Blundell and
Preston (1998), Attanasio et al. (1999), Gourinchas and Parker (2002), among many others). The permanent income model has two key assumptions: a single risk-free asset for
self-insurance and permanent—or very persistent—shocks, typically implying substantial
idiosyncratic risk. The more recent evidence, discussed in this subsection, however, suggests that a more appropriate benchmark needs to incorporate either more opportunities
for partial insurance or idiosyncratic risk that is smaller than once assumed.

4

Incomplete Markets in General Equilibrium

This section and the next discuss incomplete markets models in general equilibrium without
aggregate shocks. Bringing in a general equilibrium structure allows researchers to jointly
analyze aggregate and distributional issues. As we shall see, the two are often intertwined,
making such models very useful. The present section discusses the key ingredients that
go into building a general equilibrium incomplete markets model (e.g., types of risks to
consider, borrowing limits, modeling individuals versus households, among others). The
next section presents three broad questions that these models have been used to address: the
cross-sectional distributions of consumption, earnings, and wealth. These are substantively
important questions and constitute an entry point into broader literatures. I now begin
with a description of the basic framework.

4.1

The Aiyagari (1994) Model

In one of the first quantitative models with heterogeneity, Imrohoroglu (1989) constructed a
model with liquidity constraints and unemployment risk that varied over the business cycle.
She assumed that interest rates were constant to avoid the diﬃculties with aggregate shocks,
which were subsequently solved by Krusell and Smith (1998). She used this framework to
re-assess Lucas (1987)’s earlier calculation of the welfare cost of business cycles. She found
23

Another situation in which ✓ < 1 with self-insurance alone is if permanent and transitory shocks are
not separately observable and there is estimation risk.

22

only a slightly higher figure than Lucas’, mainly because of her focus unemployment risk,
which typically has a short duration in the United States.24 Regardless of its empirical
conclusions, this paper represents an important early eﬀort in this literature.
In what has become an important benchmark model, Aiyagari (1994) studied a version
of the deterministic growth framework, with a Neoclassical production function and a large
number of infinitely-lived consumers (dynasties). Consumers are ex ante identical, but
there is ex post heterogeneity due to idiosyncratic shocks to labor productivity which are
not directly insurable (via insurance contracts). However, consumers can accumulate a
(conditionally) risk-free asset for self insurance. They can also borrow in this asset, subject
to a limit determined in various ways. At each point in time, consumers may diﬀer in the
history of productivities experienced, and hence in accumulated wealth.
More concretely, an individual solves the following problem:
#
"1
X
t
max E0
U (ct )
{ct }

s.t.

t=0

ct + at+1 = wlt + (1 + r) at ,

at

Bmin ,

(8)

and lt follows a finite-state first-order Markov process.25
There are (at least) two ways to embed this problem in general equilibrium. Aiyagari
(1994) considered a production economy and viewed the single asset as the capital in the
firm, which obviously has a positive net supply. In this case, aggregate production is determined by the savings of individuals, and both r and the wage rate w, must be determined in
general equilibrium. Huggett (1993) instead assumed that the single asset was a household
bond in zero net supply. In this case, the aggregate amount of goods in the economy is
exogenous (exchange economy), and the only aggregate variable to be determined is r.
The borrowing limit Bmin can be set to the “natural” limit, which is defined as the loosest
possible constraint consistent with certain repayment of debt: Bmin = wlmin /r. Note that
if lmin is zero, this natural limit will be zero. Some authors have used this feature to rule
out borrowing (e.g., Carroll (1997) and Gourinchas and Parker (2002)). Alternatively, it
can be set to some ad hoc limit stricter than the natural one. More on this later.
24
There is a large literature on the costs of business cycles following Lucas’ original calculation. I do
not discuss these papers here for brevity. Lucas (2003)’s presidential address to the American Economic
Association is an extensive survey of this literature that also discusses how Lucas’ views on this issue evolved
since the original 1987 paper.
25
Prior to Aiyagari, the decision problem described here was studied in various forms by, among others,
Schechtman and Escudero (1977), Bewley (undated), Flavin (1981), Hall and Mishkin (1982), Clarida (1987,
1990), Deaton (1991), and Carroll (1991). With the exceptions of Bewley (undated) and Clarida (1987,
1990), however, most of these earlier papers did not consider general equilibrium, which is the main focus
here.

23

The main substantive finding in Aiyagari (1994) is that with incomplete markets, the
aggregate capital stock is higher than it is with complete markets, although the diﬀerence
is not quantitatively very large. Consequently, the interest rate is lower (than the time
preference rate), which is also true in Huggett (1993)’s exchange economy version. This
latter finding initially led economists to conjecture that these models could help explain the
equity premium puzzle,26 which is also generated by a low interest rate. It turned out that
while this environment helps, it is neither necessary nor suﬃcient to generate a low interest
rate. I return to this issue later. Aiyagari (1994) also shows that the model generates the
right ranking between diﬀerent types of inequality: wealth is more dispersed than income,
which is more dispersed than consumption.
The frameworks analyzed by Huggett (1993) and Aiyagari (1994) contain the bare
bones of a canonical general equilibrium incomplete markets model. As such, they abstract from many ingredients that would be essential today for conducting serious empirical/quantitative work, especially given that almost two decades have passed since their
publication. In the next three subsections, I review three main directions the framework
can be extended. First, the nature of idiosyncratic risk is often crucial for the implications
generated by the model. There is a fair bit of controversy about the precise nature and
magnitude of such risks, which I discuss in some detail. Second, and as I alluded to above,
the treatment of borrowing constraints is very reduced form here. The recent literature has
made significant progress in providing useful micro-foundations for a richer specification of
borrowing limits. Third, the Huggett-Aiyagari model considers an economy populated by
bachelor(ette)s as opposed to families—this distinction clearly can have a big impact on
economic decisions, which is also discussed.

4.2

Nature of Idiosyncratic Income Risk27

The rejection of perfect insurance brought to the fore idiosyncratic shocks as important
determinants of economic choices. However, after three decades of empirical research (since
Lillard and Willis (1978)), a consensus among researchers on the nature of labor income risk
still remains elusive. In particular, the literature in the 1980s and 1990s produced two—
quite opposite—views on the subject. To provide context, consider this general specification
26

The equity premium puzzle of Mehra and Prescott (1985) is the observation that in the historical data
stocks yield a much higher return than bonds over long horizons, which has turned out to be very diﬃcult
to explain by a wide range of economic models.
27
The exposition here draws heavily on Guvenen (2009a).

24

for the wage process:
yti = g(t, observables,...)
{z
}
|

common systematic component

zti

=

⇢zti 1

+

⌘ti ,

+

⇥

⇤
↵i + i t
| {z }

profile heterogeneity

+

⇤
zti + "it ,
| {z }

⇥

(9)

stochastic component

(10)

where ⌘ti and "it are zero mean innovations that are i.i.d. over time and across individuals.
The early papers on income dynamics estimated versions of the process given in (9) from
0 (cf., Lillard and Weiss (1979);
labor income data and found: 0.5 < ⇢ < 0.7, and 2
Hause (1980)). Thus according to this first view, which I shall call the “Heterogeneous
Income Profiles” (HIP) model, individuals are subject to shocks with modest persistence,
while facing life-cycle profiles that are individual-specific (and hence vary significantly across
the population). As we will see in the next section, one theoretical motivation for this
specification is the human capital model, which implies diﬀerences in income profiles if, for
example, individuals diﬀer in their ability level.
In an important paper, MaCurdy (1982) cast doubt on these findings. He tested the
null hypothesis of 2 = 0 and failed to reject it. He then proceeded by imposing 2 ⌘ 0
before estimating the process in (9), and found ⇢ ⇡ 1 (see also Abowd and Card (1989),
Topel (1990), Hubbard et al. (1995), and Storesletten et al. (2004a)). Therefore, according
to this alternative view, which I shall call the “Restricted Income Profiles” (RIP) model,
individuals are subject to extremely persistent—nearly random walk—shocks, while facing
similar life-cycle income profiles.
MaCurdy (1982)’s Test. More recently, two papers have revived this debate. Baker
(1997) and Guvenen (2009a) have shown that MaCurdy’s test has low power and therefore
the lack of rejection does not contain much information about whether or not there is growth
rate heterogeneity. MaCurdy’s test was generally regarded as the strongest evidence against
the HIP specification, and it was repeated in diﬀerent forms by several subsequent papers
(Abowd and Card (1989); Topel (1990); and Topel and Ward (1992)), so it is useful to
discuss in some detail.
To understand its logic, notice that, using the specification in (9), the nth auto-covariance
of income growth can be shown to be
✓
◆
1 ⇢ 2
i
i
2
n 1
,
(11)
⇢
cov( yt , yt+n ) =
1+⇢ ⌘
for n 2. The idea of the test is that for suﬃciently large n, the second term will vanish
(due to exponential decay in ⇢n 1 ), leaving behind a positive autocovariance equal to 2 .
Thus, if HIP is indeed important— 2 is positive—then higher order autocovariances must
be positive.
25

Table I: How Informative is MaCurdy’s (1982) Test?

Data
Lag
#

0
1

2
3
4
5
10
15
18

N!

27,681

Autocovariances
HIP Process
27,681

500,000

Autocorrelations
Data
HIP Process
27,681

27,681

.1215

.1136

.1153

1.00

1.00

(.0023)

(.00088)

(.00016)

(0.000)

(.000)

.0385

.04459

.04826

.3174

.3914

(.0011)

(.00077)

(.00017)

(0.010)

(.0082)

.0031

.00179

.00195

.0261

.0151

(.0010)

(.00075)

(.00018)

(0.008)

(.0084)

.0023

.00146

.00154

.0192

.0128

(.0008)

(.00079)

(.00020)

(0.009)

(.0087)

.0025

.00093

.00120

.0213

.0080

(.0007)

(.00074)

(.00019)

(0.010)

(.0083)

.0001

.00080

.00093

.0012

.0071

(.0008)

(.00081)

(.00020)

(0.007)

(.0090)

.0017

.00003

.00010

.0143

.0003

(.0006)

(.00072)

(.00019)

(0.009)

(.0081)

.0053

.00017

.00021

.0438

.0015

(.0007)

(.00076)

(.00020)

(0.008)

(.0086)

.0012

.00036

.00030

.0094

.0032

(.0009)

(.00076)

(.00018)

(0.011)

(.0087)

Notes: The table is reproduced from Guvenen (2009a, Table 3). N denotes the sample size (number of
individual-years) used to compute the statistics. Standard errors are in parenthesis. The statistics in the
“data” columns are calculated from a sample of 27681 males from the PSID as described in that paper. The
counterparts from simulated data are calculated using the same number of individuals and a HIP process fitted
to the covariance matrix of income residuals.

Guvenen (2009a) raises two points. First, he asks how large n must be for the second
term to be negligible. He shows that for the value of persistence he estimates with the HIP
process (⇢ ⇠
= 0.82), the autocovariances in (11) do not even turn positive before the 13th
lag (because the second term dominates), whereas MaCurdy only studied the first 5 lags.
Second, he conducts a Monte Carlo analysis in which he simulates data using equation (9)
with substantial heterogeneity in growth rates.28 The results of this analysis are reproduced
here in Table I. MaCurdy’s test does not reject the false null hypothesis of 2 = 0 for any
sample size smaller than 500,000 observations (column 3)! Even in that case, only the 18th
autocovariance is barely significant (with a t-statistic of 1.67). For comparison, MaCurdy
28
More concretely, the estimated value of 2 used in his Monte Carlo analysis implies that at age 55 more
than 70% of wage inequality is due to profile heterogeneity.

26

(1982)’s dataset included around 5,000 observations. Even the more recent PSID datasets
typically contain fewer than 40,000 observations.
In light of these results, imposing the a priori restriction of 2 = 0 on the estimation
exercise seems a risky route to follow. Baker (1997), Haider (2001), Haider and Solon
(2006), and Guvenen (2009a) estimate the process in (9) without imposing this restriction
and find substantial heterogeneity in i and a low persistence, confirming the earlier results
of Lillard and Weiss (1979) and Hause (1980). Baker and Solon (2003) use a large panel
dataset drawn from Canadian tax records and allow for both permanent shocks and profile
heterogeneity. They find statistically significant evidence of both components.
In an interesting recent paper, Browning et al. (2010) estimate an income process that
allows for “lots of” heterogeneity. The authors use a simulated method of moments estimator
and match a number of moments whose economic significance is more immediate than the
covariance matrix of earnings residuals, which has typically been used as the basis of a
GMM estimation in the bulk of the extant literature. They uncover a lot of interesting
heterogeneity, for example, in the innovation variance as well as in the persistence of AR(1)
shocks. Moreover, they “find strong evidence against the hypothesis that any worker has a
unit root.” Gustavsson and Osterholm (2010) use a long panel data set (1968-2005) from
administrative wage records on Swedish individuals. They employ local-to-unity techniques
on individual-specific time series and reject the unit root assumption.
Inferring Risk vs Heterogeneity from Economic Choices. Finally, a number of
recent papers examine the response of consumption to income shocks to infer the nature of
income risk. In an important paper, Cunha et al. (2005) measure the fraction of individualspecific returns to education that are predictable by individuals by the time they make their
college decision versus the part that represents uncertainty. Assuming a complete markets
structure, they find that slightly more than half of the returns to education represents
known heterogeneity from the perspective of individuals.
Guvenen and Smith (2009) study the joint dynamics of consumption and labor income
(using PSID data) in order to disentangle “known heterogeneity” from income risk (coming
from shocks as well from uncertainty regarding one’s own income growth rate). They
conclude that a moderately persistent income process (⇢ ⇡ 0.7 0.8) is consistent with
the joint dynamics of income and consumption. Furthermore, they find that individuals
have significant information about their own i at the time they enter the labor market
and hence face little uncertainty coming from this component. Overall, they conclude that
with income shocks of modest persistence and largely predictable income growth rates,
the income risk perceived by individuals is substantially smaller than what is typically
assumed in calibrating incomplete markets models (many of which borrow their parameter
values from MaCurdy (1982), Abowd and Card (1989), and Meghir and Pistaferri (2004),
among others). Along the same lines, Krueger and Perri (2009) use rich panel data on
27

Italian households and conclude that the response of consumption to income suggests low
persistence for income shocks (or a high degree of partial insurance).29
Studying economic choices to disentangle risk from heterogeneity has many advantages.
Perhaps most importantly, it allows researchers to bring a much broader set of data to
bear on the question. For example, many dynamic choices require individuals to carefully
weigh the diﬀerent future risks they perceive against predictable changes before making a
commitment. Decisions on home purchases, fertility, college attendance, retirement savings,
and so on are all of this sort. At the same time, this line of research also faces important
challenges: these analyses need to rely on a fully-specified economic model, so the results
can be sensitive to assumptions regarding the market structure, specification of preferences,
and so on. Therefore, experimenting with diﬀerent assumptions is essential before a definitive conclusion can be reached with this approach. Overall, this represents a diﬃcult but
potentially very fruitful area for future research.

4.3

Wealth, Health, and Other Shocks

One source of idiosyncratic risk that has received relatively little attention until recently
come from shocks to wealth holdings, resulting for example from fluctuations in housing
prices and stock returns, among others. A large fraction of the fluctuations in housing prices
are due to local or regional factors and are substantial (as the latest housing market crash
showed once again). So these fluctuations can have profound eﬀects on individuals’ economic choices. In one recent example, Krueger and Perri (2009) use panel data on Italian
households’ income, consumption, and wealth. They study the response of consumption
to income and wealth shocks and find the latter to be very important. Similarly, Mian
and Sufi (forthcoming) use individual-level data from 1997 to 2008 and show that housing
price boom lead to significant equity extraction—about 25 cents for every dollar increase
in prices—which in turn led to higher leverage and personal default during this time. Their
“conservative” estimates is that home equity-based borrowing added $1.25 trillion in household debt and accounted for about 40% of new defaults from 2006 to 2008.
Another source of idiosyncratic shocks is out-of-pocket medical expenditures (hospital bills, nursing home expenses, medications, etc), which can potentially have significant
eﬀects on household decisions. French and Jones (2004) estimate a stochastic process for
health expenditures, modeled as a normal distribution adjusted to capture the risk of catastrophic health care costs. Simulating this process, they show that 0.1% of households every
year receive a health cost shock with a present value exceeding $125,000. Hubbard et al.
29

A number of important papers have also studied the response of consumption to income, such as
Blundell and Preston (1998) and Blundell et al. (2008). These studies however assumed the persistence of
income shocks to be constant and instead focused on what can be learned about the sizes of income shocks
over time.

28

(1994, 1995) represent the earliest eﬀorts to introduce such shocks into quantitative incomplete markets models. The 1995 paper shows that the interaction of such shocks with
means-tested social insurance programs are especially important to account for in order to
understand the very low savings rate of low-income individuals.
De Nardi et al. (2010) ask if the risk of large out-of-pocket medical expenditures late in
life can explain the savings behavior of the elderly. They examine a new and rich dataset
called AHEAD, which is part of the Health and Retirement Survey (HRS) conducted by the
University of Michigan, which allows them to characterize medical expenditure risk for the
elderly (even for those in their 90’s) more precisely than previous studies, such as Hubbard
et al. (1995) and Palumbo (1999).30 De Nardi et al. find out-of-pocket expenditures to rise
dramatically at very old ages, which (in their estimated model) provides an explanation for
the lack of significant dissaving by the elderly. Ozkan (2010) shows that the lifecycle profile
of medical costs (inclusive of the costs paid by private and public insurers to providers)
diﬀer significantly between rich and poor households. In particular, on average, the medical
expenses of the rich are higher than that of the poor until mid-life, after which the expenses
of the poor exceed those of the rich—by 25% in absolute terms. Further, the expenses of the
poor have thick tails—lots of individuals with zero expenses and many with catastrophically
high costs. He builds a model in which individuals can invest in their health (i.e., preventive
care), which aﬀects the future distribution of health shocks and, consequently, the expected
life time. High-income individuals do precisely this, which explains their higher spending
early on. Low-income individuals do the opposite, which ends up costing more later in life.
He concludes that a reform of the health care system that encourages use of health care
for low-income individuals have positive welfare gains, even when fully accounting for the
increase in taxes required to pay for them.

4.4

Endogenizing Credit Constraints

The basic Aiyagari model features a reduced-form specification for borrowing constraints
(8), and does not model the lenders’ problem that gives rise to such constraint. As such, it
is silent about potentially interesting variation in borrowing limits across individuals and
states of the economy. A number of recent papers attempt to close this gap.
In one of the earliest studies of this kind, Athreya (2002) constructed a general equilibrium model of unsecured household borrowing to quantify the welfare eﬀects of the
30

Palumbo’s estimates of medical expenditures is quite a bit smaller than that in De Nardi et al. (2010),
which is largely responsible for the smaller eﬀects he quantifies. De Nardi et al. (2010) argue that one reason
for the discrepancy could be the fact that Palumbo used data from the National Medical Care Expenditure
Survey, which, unlike the AHEAD dataset, does not contain direct measures of nursing home expenses.
He did imputations from a variety of sources, which may be missing the large actual magnitude of such
expenses found in the AHEAD dataset.

29

Bankruptcy Reform Act of 1999 in the United States. In the pooling equilibrium of this
model (which is what Athreya focused on), the competitive lending sector charges a higher
borrowing rate than the market lending rate to break even (i.e., zero profit condition), accounting for the fraction of households who will default. This framework allows him to study
diﬀerent policies, such as changing the stringency of means testing as well as eliminating
bankruptcy altogether.
In an important paper, Chatterjee et al. (2007) build a model of personal default behavior and endogenous borrowing limits. The model features (i) several types of shocks—to
earnings, preferences, and liabilities (e.g., hospital and lawsuit bills, which precede a large
fraction of defaults in the United States), (ii) a competitive banking sector, and (iii) postbankruptcy legal treatment of defaulters that mimics the US Chapter 7 bankruptcy code.
The main contribution of Chatterjee et al. (2007) is to show that a separating equilibrium
exist in which banks oﬀer a menu of debt contracts to households whose interest rate varies
optimally with the level of borrowing to account for the changing default probability. Using a calibrated version of the model, they quantify the separate contributions of earnings,
preferences, and liability shocks to debt and default. Chatterjee and Eyigungor (2011) introduce collateralized debt (i.e., mortgage debt) into this framework to examine the causes
of the run-up in foreclosures and crash in housing prices after 2007.
Livshits et al. (2007) study a model similar to Chatterjee et al. (2007) in order to
quantify the advantages to a “fresh start” bankruptcy system (e.g., US Chapter 7) against
a European style system in which debtors cannot fully discharge their debt in bankruptcy.
The key trade-oﬀ is that dischargeable debts adds insurance against bad shocks, helping to
smooth across states, but the inability to commit to future repayment increase interest rates
and limits the ability to smooth across time. Their model is quite similar to Chatterjee et al.
(2007), except that they model an explicit OLG structure. They calibrate the model to the
age-specific bankruptcy rate and debt-to-earnings ratio. For their baseline parameterization,
they find that fresh start bankruptcy is welfare improving, but that result is sensitive to the
process for expenditure and income shocks, the shape of the earnings profile and household
size. Livshits et al. (2010) build on this framework to evaluate several theories for the rise
in personal bankruptcies since the 1970s. Finally, Glover and Short (2010) use the model
of personal bankruptcy to understand the incorporation of entrepreneurs. Incorporation
protects the owners’ personal assets and their access to credit markets in case of default,
but by increasing their likelihood of default, incorporation also implies a risk premium is
built into their borrowing rate.

4.5

From Bachelor(ette)s to Families

While the framework described above can shed light on some interesting distributional
issues (e.g., inequality in consumption, earnings, and wealth), it is completely silent on
30

a crucial source of heterogeneity—the household structure. In reality, individuals marry,
divorce, have kids and make their decisions regarding consumption, savings, labor supply
and so on, jointly with these other life choices. For many economic and policy questions, the
interaction between these domestic decisions and economic choices in an incomplete markets
world can have a first order eﬀect on the answers we get. Just to give a few example, consider
these facts: men and women are well-known to exhibit diﬀerent labor supply elasticities;
the tax treatment of income varies depending on whether an individual is single, married,
and whether he/she have kids, etc.; the trends in labor market participation rate in the
United States since the 1960s have been markedly diﬀerent for single and married women;
the fractions of individuals who are married and divorced have changed significantly, again
since the 1960s, and so on.
A burgeoning literature works to bring a richer household structure into macroeconomics. For example, in an influential paper Greenwood et al. (2005) study the role of
household technologies (the widespread availability of washing machines, vacuum cleaners,
refrigerators, etc.) in leading women into the labor market. Greenwood and Guner (2008)
extend the analysis to study the marriage and divorce patterns since World War II. Jones et
al. (2003) explore the role of the closing gender wage gap for married women’s rising labor
supply. Knowles (2007) argues that the working hours of men are too long when viewed
through the lens of a unitary model of the household in which the average wage of females
rise as in the data. He shows that introducing bargaining between spouses into the model
reconciles it with the data. Guner et al. (2010) study the eﬀects of potential reforms in
the US tax system in a model of families with children and an extensive margin for female
labor supply. Guvenen and Rendall (2011) study the insurance role of education for women
against divorce risk and the joint evolution of education trends with those in marriage and
divorce.

5

Inequality in Consumption, Wealth, and Earnings

A major use of heterogeneous-agent models is to study inequality or dispersion in key economic outcomes, most notably in consumption, earnings, and wealth. The Aiyagari model—
as well as its aggregate-shock augmented version, the Krusell-Smith model presented in the
next section—takes earnings dispersion to be exogenous and makes predictions about inequality in consumption and wealth. The bulk of the incomplete markets literature follows
this lead in their analysis. Some studies introduce an endogenous labor supply choice and
instead specify the wage process to be exogenous, delivering earnings dispersion as an endogenous outcome (Pijoan-Mas (2006), Domeij and Floden (2006), Heathcote et al. (2008),
among others). While this is a useful step forward, a lot of the dispersion in earnings before
age 55 is due to wages and not hours, so the assumption of an exogenous wage process
still leaves quite a bit to be understood. Other strands of the literature attempt to close
31

this gap by writing models that also generate wage dispersion as an endogenous outcome in
the model—for example, due to human capital accumulation (e.g., Guvenen and Kuruscu
(2009, forthcoming), and Huggett et al. (forthcoming)) or due to search frictions.31

5.1

Consumption Inequality

Two diﬀerent dimensions of consumption inequality has received attention in the literature.
The first one concerns how much within-cohort consumption inequality increases over the
lifecycle. The diﬀerent views on this question has been summarized in Section 3.2.32 The
second one concerns whether, and by how much, (overall) consumption inequality has risen
in the United States since the 1970s, a question whose urgency was raised by the substantial
rise in wage inequality during the same time. In one of the earliest papers on this topic,
Cutler and Katz (1992) used data from the 1980s on US households from the Consumer
Expenditure Survey (CE) and found that the evolution of consumption inequality closely
tracked the rise in wage inequality during the same time. This finding served as a rejection
of earlier claims in the literature (e.g., Jencks (1984)) that the rise of means tested inkind transfers starting in the 1970s had improved the material well-being of low income
households relative to what would be judged by their income statistics.
Interest in this question was reignited more recently by a thought-provoking paper by
Krueger and Perri (2006), who concluded from an analysis of CE data that, from 1980 to
2003, within-group income inequality increased substantially more than within-group consumption inequality (in contrast, they found that between-group income and consumption
inequality tracked each other).33 They then proposed an explanation based on the premise
that the development of financial services in the US economy has helped households smooth
consumption fluctuations relative to income variation.
To investigate this story, they applied a model of endogenous debt constraints as in
Kehoe and Levine (1993). In this class of models, what is central is not the ability of
households to pay back their debt, but rather it is their incentives or willingness to pay
31
The search literature is very large with many interesting models to cover. I do not discuss these models
here because I cannot do justice to this extensive body of work in this limited space. For an excellent survey,
see Rogerson et al. (2005). Note, however, that as Hornstein et al. (forthcoming) show, search models have
trouble generating the magnitudes of wage dispersion we observe in the data.
32
Another recent paper is of interest is Aguiar and Hurst (2008), who examine the lifecycle mean and
variance profiles of the subcomponents of consumption—housing, utility bills, clothing, food at home, food
away from home, etc. They show rich patterns that vary across categories, whereby the variance rises
monotonically for some categories, while being hump-shaped for others, and yet declining monotonically
for some others. The same patterns are observed for the mean profile. These disaggregated facts provide
more food for thought to researchers.
33
Attanasio et al. (2007) have questioned the use of the CE Interview survey and argued that some
expenditure items are poorly measured in the survey relative to another component of CE, called the
diary survey. They propose an optimal way of combining the two survey data and find that consumption
inequality, especially in the 1990s has increased more than what is revealed by the Interview survey alone.

32

back. To give the right incentives, lenders can punish a borrower that defaults, for example,
by banning her from financial markets forever (autarky). However, if the individual borrows
too much or if autarky is not suﬃciently costly, it may still make sense to default in certain
states of the world. Thus, given the parameters of the economic environment, lenders will
compute the optimal state-contingent debt limit, which will ensure that the borrower never
defaults in equilibrium. Krueger and Perri (2006) notice that if income shocks are really
volatile, then autarky is a really bad outcome, giving borrowers less incentive to default.
Lenders who know this, in turn, are more willing to lend, which endogenously loosens the
borrowing constraints. This view of the last 30 years thus holds that the rise in the volatility
of income shocks gave rise to the development of financial markets (more generous lending),
which in turn led to a smaller rise in consumption inequality.34
Heathcote et al. (2007) argue that the small rise in consumption inequality can be
explained simply if the rise in income shocks have been of more transitory nature, since such
shocks are easier to smooth through self insurance. Indeed, Blundell and Preston (1998)
earlier made the same observation to conclude that in the 1980s the rise in income shock
variance was mostly permanent in nature (evidenced by the observation that income and
consumption inequality grew together) whereas in the 1990s, it was mostly transitory given
the opposite was true. Heathcote et al. (2007) calibrate a fully specified model and show
that it can go a long way towards explaining the observed trends in consumption inequality.
One point to keep in mind is that that these papers take as given that the volatility of
income shocks rose during this period, a conclusion that is subject to uncertainty in light
of the new evidence discussed above.
Before concluding, a word of caution about measurement. The appropriate price deflator
for consumption may have trended diﬀerently for households in diﬀerent parts of the income
distribution (i.e., the “Walmart eﬀect” at the lower end). To the extent that this eﬀect is
real, the measured trend in consumption inequality could be overstating the actual rise in
the dispersion of material well-being. This issue still deserves a fuller exploration.

5.2

Wealth Inequality

The main question about wealth inequality is a cross-sectional one: Why do we observe
such enormous disparities in wealth, with a Gini coeﬃcient of about 0.8 for net worth and
a Gini exceeding 0.90 for financial wealth?
Economists have developed several models that can generate highly skewed wealth distributions (see, for example, Huggett (1996), Krusell and Smith (1998), Quadrini (2000),
34

Very recently, Aguiar and Bils (2011) have taken a diﬀerent approach and constructed a measure of
CE consumption by using data on income and (self-reported) savings rate by households. They argue
that consumption inequality tracked income inequality closely in the past 30 years. Although this is still
preliminary work, the paper raises some interesting challenges.

33

Castañeda et al. (2003), Guvenen (2006), and Cagetti and De Nardi (2006). These models
typically use one (or more) of three mechanisms to produce this inequality: (1) dispersion
in luck in the form of large and persistent shocks to labor productivity: the rich are luckier
than the poor; (2) dispersion in patience or thriftiness: the rich save more than the poor;
and (3) dispersion in rates of return: the rich face higher asset returns than the poor. This
section describes a baseline model and variations of it that incorporate various combinations
of the three main mechanisms that economists have used to generate substantial inequality
in general equilibrium models.35
Dispersion in Luck. Huggett (1996) asks how much progress can be made towards
understanding wealth inequality using (i) a standard lifecycle model with (ii) Markovian
idiosyncratic shocks, (iii) uncertain lifetimes and (iv) a Social Security system. He finds
that although the model can match the Gini coeﬃcient for wealth in the United States,
this comes from low income households holding too little wealth, rather than the extreme
concentration of wealth at the top in the US economy. Moreover, whereas in the US data the
dispersion of wealth within each cohort is nearly as large as the dispersion across cohorts,
the model understates the former significantly.
Castañeda et al. (2003) study an enriched model that combines elements of Aiyagari
(1994) and Huggett (1996). Specifically, the model (i) has a Social Security system, (ii)
has perfectly altruistic bequests, (iii) allows for intergenerational correlation of earnings
ability, (iv) has a progressive labor and estate tax system as in the US, and (iv) allows a
labor supply decision. As for the stochastic process for earnings, they do not calibrate its
properties based on micro-econometric evidence on income dynamics as is commonly done,
but rather they choose its features (the 4 ⇥ 4 transition matrix and 4 states of a Markov
process) so that the model matches the cross-sectional distribution of earnings and wealth.
To match the extreme concentration of wealth at the upper tail, this calibration procedure
implies that individuals must receive a large positive shock (about 1060 times the median
income level) with a small probability. This high income level is also very fleeting—it lasts
for about 5 years—which leads these high income individuals to save substantially (for
consumption smoothing) and results in high wealth inequality.
Dispersion in Patience. Laitner (1992, 2002)’s original insight was that wealth inequality could result from a combination of: (1) random heterogeneity in lifetime incomes across
generations, and (2) altruistic bequests, which are constrained to be non-negative. Each
newly born consumer in Laitner’s model receives a permanent shock to his lifetime income
and, unlike in the Aiyagari model, faces no further shocks to income during his lifetime. In
essence, in Laitner’s model only households who earn higher than average lifetime income
35
Some of the models discussed in this section contain aggregate shocks in addition to idiosyncratic ones.
While aggregate shocks raise some technical issues that will be addressed in the next section, they pose no
problems for the exposition in this section.

34

Figure 2: Determination of Wealth Inequality in Various Models
0.06
Long−Run Asset Demand Schedule
5%
Long−Run Asset Demand (Impatient Agent)

0.05

4%

Asset Return

Asset Return

R

0.03
Long−Run Asset Demand (Patient Agent)

Rs
Rf

3%

2%

0.02

0.01

0
−5 −Bmin

η: Time preference rate

η

Impatient Agent’s
Wealth

0

5 Asset Demand 10

1%

Nonstockholders’
wealth

Patient Agent’s
wealth

15

0
−5 −B
min

20

(a) Laitner (1992) and Krusell and Smith (1998)

0

5 Asset Demand 10

Stockholders’
wealth

15

20

(b) Guvenen (2006)

want to transfer some amount to their oﬀsprings who are not likely to be as fortunate. This
altruistic motive makes these households eﬀectively more thrifty (compared to those who
earn below average income) since they also care about future utility. Thus even small differences in lifetime income can result in large diﬀerences in savings rates—a fact empirically
documented by Carroll (2000)—and hence in wealth accumulation.
The stochastic-beta model of Krusell and Smith (1998) is a variation on this idea in a
dynastic framework, where heterogeneity in thrift (i.e., in the time-discount rate) is imposed
exogenously.36 Being more parsimonious, the stochastic-beta model also allows for the
introduction of aggregate shocks. Krusell and Smith show that even small diﬀerences in
the time discount factor that are suﬃciently persistent is suﬃcient to generate the extreme
skewness of the US wealth distribution. The intuition for this result will be discussed in a
moment.
Dispersion in Rates of Return. Guvenen (2006) introduces return diﬀerentials into a
standard stochastic growth model (i.e., in which consumers have identical, time-invariant
discount factors and idiosyncratic shocks do not exist). He allows all households to trade
in a risk-free bond, but restricts one group of agents from accumulating capital. Quadrini
(2000) and Cagetti and De Nardi (2006) study models of inequality with entrepreneurs and
workers, which can also generate skewed wealth distributions. The mechanisms have similar
flavors: agents who face higher returns end up accumulating substantial amount of wealth.
The basic mechanism in Guvenen (2006) can be described as follows. Non-stockholders
have a precautionary demand for wealth (bonds), but the only way they can save is if stock36

Notice that in this paper I use to denote the time discount factor and was used to denote the income
growth rate. I will continue with this convention, except when I specifically refer to the Krusell-Smith model
which has come to be known as a stochastic-beta model.

35

holders are willing to borrow. In contrast, stockholders have access to capital accumulation,
so they could smooth consumption even if the bond market was completely shut down. Furthermore, non-stockholders’ asset demand is even more inelastic because they are assumed
to have a lower elasticity of intertemporal substitution (consistent with empirical evidence)
and therefore have a strong desire for consumption smoothing. Therefore, trading bonds
for consumption smoothing is more important for non-stockholders than it is for stockholders. As a result, stockholders will only trade in the bond market if they can borrow at
a low interest rate. This low interest rate in turn dampens non-stockholders’ demand for
savings further, and they end up with little wealth in equilibrium (and stockholders end
up borrowing very little). Guvenen (2009b) shows that a calibrated version of this model
easily generates the extremely skewed distribution of the relative wealth of stockholders to
non-stockholders in the US data.
Can we Tell them Apart? The determination of wealth inequality in the three models
discussed so far can be explained using variations of a diagram used by Aiyagari (1994).
The left panel of figure 2 shows how wealth inequality is determined in Laitner’s model and,
given their close relationship, in the Krusell-Smith model. The top solid curve originating
from “ Bmin ” plot the long-run asset demand schedule for the impatient agent; the bottom
curve is for the patient agent. A well-known feature of incomplete markets models is that the
asset demand schedule is very flat for values of returns that are close to the time preference
rate, ⌘ (so ⌘ 1/(1 + ⌘)). Thus, both types of individuals’ demand schedules asymptote
to their respective time preference rates (with ⌘patient < ⌘impatient ).37 If the equilibrium
return (which must be lower than ⌘patient for an equilibrium to exist) is suﬃciently close
to ⌘patient , the high sensitivity of asset demands to interest rates will generate substantial
wealth inequality between the two types of agents.
Similarly, the right panel shows the mechanism in the limited participation model, which
has a similar flavor. For simplicity, let us focus on the case where stockholders and nonstockholders have the same preferences and face the same portfolio constraints. We have
⌘ > RS > Rf . Again, given the sensitivity of asset demand to returns near ⌘, even a small
equity premium generates substantial wealth inequality. It should be stressed, however,
that a large wealth inequality is not a foregone conclusion in any of these models. If returns
were too low relative to ⌘, individuals would be on the steeper part of their demand curves,
which could result in smaller diﬀerences in wealth holdings.
While the mechanics described here may appear quite similar for the three models,
their substantive implications diﬀer in crucial ways. For example, consider the eﬀect of
eliminating aggregate shocks from all three models. In Guvenen (2006), there will be
no equity premium without aggregate shocks and, consequently, no wealth inequality. In
37

See, for example, Aiyagari (1994) and references therein. This result also holds when asset returns are
stochastic (Chamberlain and Wilson (2000)).

36

Krusell and Smith (1998), wealth inequality will increase as the patient agent holds more of
the aggregate wealth (and would own all the wealth if there were no idiosyncratic shocks). In
Laitner (1992), wealth inequality will remain unchanged, since it is created by idiosyncratic
lifetime income risk. These dramatically diﬀerent implications suggest that one can devise
methods to bring empirical evidence to bear on the relevance of these diﬀerent mechanisms.
Cagetti and De Nardi (2006). Cagetti and De Nardi (2006) introduce heterogeneity
across individuals in both work and entrepreneurial ability. Entrepreneurial firms operate
decreasing returns to scale production functions and higher entrepreneurial ability implies
a higher optimal scale. Because debt contracts are not perfectly enforceable due to limited
commitment, business owners need to put some of their assets as collateral, a portion of
which would be confiscated in case of default. Entrepreneurs with very promising projects
thus have more to lose from default, which induces them to save more for collateral, borrow
more against it, and reach their larger optimal scale. The model is able to generate the
extreme concentration of wealth at the top of the distribution (among households many of
whom are entrepreneurs).
Although this model diﬀers from the limited participation framework in many important
ways, the diﬀerential returns to saving is a critical element for generating wealth inequality in
both models. This link could be important because many individuals in the top 1% and 5%
of the US wealth distribution hold significant amounts of stocks but are not entrepreneurs
(hold no managerial roles), which the Cagetti/De Nardi model misses. The opposite is also
true: many very rich entrepreneurs are not stockholders (outside of their own company)
which does not fit well with Guvenen’s model (see Heaton and Lucas (2000) on the empirical
facts about wealthy entrepreneurs and stockholders). The view that perhaps the very high
wealth holdings of these individuals is driven by the higher returns that they enjoy—either
as a stockholder or as an entrepreneur—can oﬀer a unified theory of savings rate diﬀerences.

5.3

Wage and Earnings Inequality

Because the consumption-savings decision is the cornerstone of the incomplete markets
literature, virtually every model has implications for consumption and wealth inequality.
The same is not true for earnings inequality. Many models assume that labor supply is
inelastic and the stochastic process for wages is exogenous, making the implications for wage
and earnings inequality to be mechanical reflections of the assumptions of the model. Even
if labor supply is assumed to be endogenous, many properties of the earnings distribution
(exceptions noted below) mimic those of the wage distribution. For things to get more
interesting, it is the latter which needs to be endogenized as well.
In this section, I first review the empirical facts regarding wage inequality—both over
the life cycle and over time. These facts are useful to know for practitioners since they are
37

commonly used as exogenous inputs into incomplete markets models. Unless specifically
mentioned, all the facts discussed here pertain to male workers, because the bulk of the
existing work is available consistently for this group.38 Second, I discuss models that attempt
to endogenize wages and explain the reported facts and trends.
5.3.1

Inequality Over the Lifecycle

The main facts about the evolution of (within-cohort) earnings inequality over the lifecycle
have first been documented by Deaton and Paxson (1994) and shown in the left panel of
figure 1. The same exercise has been repeated by numerous authors using diﬀerent datasets
or time periods (among others, Storesletten et al. (2004b), Guvenen (2009a), Heathcote et
al. (2010), and Kaplan (2010)). While the magnitudes diﬀer somewhat, the basic fact that
wage and earnings inequality rise substantially over the life cycle is well-established.
One view is that this fact does not require an elaborate explanation, because wages
follow a very persistent, perhaps permanent, stochastic process as implied by the RIP
model. Thus, the rising lifecycle inequality is simply a reflection of the accumulation of
such shocks, which drives up the variance of log wages in a linear fashion (in the case of
permanent shocks). I will continue to refer to this view as the RIP model because of its
emphasis on persistent “shocks.” 39
An alternative perspective, which has received attention more recently, emphasizes systematic factors—heterogeneity as opposed to random shocks. This view is essentially in
the same spirit as the HIP model of the previous section. But it goes one step further by
endogenizing the wage distribution based on the human capital framework of Becker (1964)
and Ben-Porath (1967) among others. In an influential paper, Huggett et al. (2006) have
studied the distributional implications of the standard Ben-Porath (1967) model by asking
the types of heterogeneity that one needs to introduce to generate patterns consistent with
the US data. They find that too much heterogeneity in initial human capital levels results
in the counterfactual implication that wage inequality should fall over the lifecycle. In
contrast, heterogeneity in the learning ability generates rise in wage inequality consistent
with the data. A key implication of this finding is that the rise in wage inequality can be
generated without appealing to idiosyncratic shocks of any kind. Instead, it is the systematic fanning out of wage profiles, resulting from diﬀerent investment rates, that generates
rising inequality over the life cycle. Guvenen et al. (2009) and Huggett et al. (forthcoming)
introduce idiosyncratic shocks into the Ben-Porath framework. Both papers find that heterogeneous growth rates continue to play the dominant role for the rise in wage inequality.
38

Some of the empirical trends discussed below also apply to women, while others do not. See Autor et
al. (2008) for a comparative look at wage trends for males and females during the period.
39
Of course, one can write deeper economic models which generates the the observation that wages
follow a random walk process, such as the learning model of Jovanovic (1979) in a search and matching
environment, or the optimal contracts in the limited commitment model of Harris and Holmstrom (1982).

38

The Ben-Porath formulation is also central for wage determination in Heckman et al. (1998)
and Kitao et al. (2008).
5.3.2

Inequality Trends Over Time

A well-documented empirical trend since the 1970s is the rise in wage inequality among
male workers in the United States. This trend has been especially prominent above the
median of the wage distribution: for example, the log wage diﬀerential between the 90th
and the 50th percentiles has been expanding in a secular fashion for the past four decades.
The changes at the bottom have been more episodic, with the log 50-10 wage diﬀerential
strongly expanding until late 1980s and then closing subsequently (see Autor et al. (2008)
for a detailed review of the evidence). Acemoglu (2002) contains an extensive summary
of several related wage trends as well as a review of proposed explanations. Here I only
discuss the subset of papers that are more closely relevant for the incomplete markets macro
literature.
Larger Shocks or Increasing Heterogeneity? Economists’ interpretations of the rise
in wage inequality over the life cycle and over time are intimately related. The RIP view that
was motivated by analyses of life-cycle wages was dominant in the 1980s and 1990s, so it was
natural for economists to interpret the rise in wage inequality over time, through the same
lens. Starting with Gottschalk and Moﬃtt (1994) and Moﬃtt and Gottschalk (1995), this
trend has been broadly interpreted as reflecting a rise in the variances of idiosyncratic shocks,
either permanent or transitory (Meghir and Pistaferri (2004), Heathcote et al. (2008), etc.).
This approach remains the dominant way to calibrate economic models that investigate
changes in economic outcomes from the 1970s to date.
However, some recent papers have documented new evidence that seems hard to reconcile
with the RIP view. The first group of papers revisit the econometric analyses of wage and
earnings data. Among these, Sabelhaus and Song (2009, 2010) have used panel data from
Social Security records covering millions of American workers, in contrast to the long list of
previous studies that use survey data (e.g., the PSID).40 While this dataset has the potential
drawback of under-reporting (because it is based on income reported to the IRS), it has
three important advantages: (i) a much larger sample size (on the order of 50+ million
observations, compared to at most 50,000 in the PSID), (ii) no survey response error, and
(iii) no attrition. Sabelhaus and Song found that the volatility of annual earnings growth
increased during the 1970s, but that it declined monotonically during the 1980s and 1990s.
Furthermore, applying the standard permanent-transitory decomposition as in Moﬃtt and
Gottschalk (1995) and Meghir and Pistaferri (2004) reveals that permanent shock variances
40

These include, among others, Meghir and Pistaferri (2004), Dynan et al. (2007), Heathcote et al. (2008),
and Solon and Shin (2011).

39

were stable and transitory shocks became smaller from 1980 into the 2000s. A separate
study conducted by the Congressional Budget Oﬃce (2008), also using wage earnings from
Social Security records from 1980 to 2003, reached the same conclusion.41 Finally, Kopczuk
et al. (2010) document (also using Social Security data) that both long-run and short-run
mobility have stayed remarkably stable from 1960s into the 2000s. But this finding seems
diﬃcult to reconcile with Moﬃtt and Gottschalk (1995) and the subsequent literature that
found permanent and transitory shock variances to have risen in diﬀerent sub-periods from
1970 to 2000s. If true, the latter would result in fluctuations in mobility patterns over these
sub-periods, which is not borne out in Kopczuk et al. (2010)’s analysis.
Another piece of evidence from income data is oﬀered by Haider (2001), who estimated a
stochastic process for wages similar to the one in Moﬃtt and Gottschalk (1995) and others,
but with one key diﬀerence. He allowed for individual-specific wage growth rates (HIP)
and he also allowed for the dispersion of growth rates to vary over time. The stochastic
component is specified as an ARMA(1,1). With this more flexible specification, he found
no evidence of a rise in the variance of income shocks after 1970s, but instead found a large
increase in the dispersion of systematic wage growth rates.
A second strand of the literature have studied the trends in labor market flows in the
United States. These studies do not find any evidence of rising job instability or churning,
which one might expect to see in conjunction with larger idiosyncratic shocks. In contrast,
these studies document an across-the-board moderation in labor market flows. For example,
Gottschalk and Moﬃtt (1999) focus on male workers between ages 20 and 62 and conclude
their analysis as follows:
[W]e believe that a consistent picture is emerging on changes in job stability
and job security in the 1980s and 1990s. Job instability does not seem to have
increased, and the consequences of separating from an employer do not seem to
have worsened.42
Shimer (2005, 2007) and Davis et al. (2010) extend this analysis to cover the 2000s and use a
variety of datasets to reach the same conclusion. Further, both papers show that expanding
the sample of individuals to include women and younger workers shows a declining trend
in labor market flows and increase in job security.
41
Sabelhaus-Song attribute the reason why some earlier studies found rising variances of wage shocks
(e.g., Moﬃtt and Gottschalk (2008)) to the inclusion of individuals with self-employment income and those
who earn less than the Social Security minimum. Even though there are few of these households, they show
that they make a big diﬀerence in the computed statistics. Similarly, Solon and Shin (2011, p. 978-980)
use PSID data and also do not find a trend in the volatility of wage earnings changes during the 1980s and
1990s. They argue that the increasing volatility found in earlier studies, such as Dynan et al. (2007) seems
to be coming from the inclusion of some auxiliary types of income (from business, farming, etc) whose
treatment has been inconsistent in the PSID over the years.
42
They also say, “Almost all studies based on the various Current Population Surveys (CPS) supplements
...show little change in the overall separation rates through the early 1990s.”

40

Taking Stock. To summarize, the seeming consensus of the 1990s—that rising wage
inequality was driven by an increase in idiosyncratic shock variances—is being challenged
by a variety of new evidence, some of which from data sets many orders of magnitude
larger than the surveys used in previous analyses. In addition, the evidence from labor
market flows described above—while perhaps more indirect—raises questions about the
sources of larger idiosyncratic shocks in a period where labor market transitions seem to
have moderated. Although, it would be premature to conclude that the alternative view
is the correct one—more evidence is needed to reach a definitive conclusion. Having said
that, if this alternative view is true, and income shock variances have not increased, this
new “fact” would require economists to rethink a variety of explanations put forward for
various trends, which took as given a rise in shock variances during this period.

6

Heterogeneity with Aggregate Shocks

Krusell and Smith (1998) added two elements to the basic Aiyagari framework. First they
introduced aggregate technology shocks. Second, they assumed that the cumulative discount
factor at time t (which was assumed to be t before), now follows the stochastic process
˜ t 1 , where ˜ is a finite-state Markov chain. The stochastic evolution of the discount
t =
factors within a dynasty captures some elements of an explicit overlapping-generations
structure with altruism and less than perfect correlation in genes between parents and
children, as in Laitner (1992). With this interpretation in mind, the evolution of ˜ is
calibrated so that the average duration of any particular value of the discount factor is
equal to the lifetime of a generation. (Krusell and Smith (1997) study a version of this
model where consumers are allowed to hold a risk-free bond in addition to capital.)
The specifics of the model are as follows. There are two types of shocks: (i) idiosyncratic employment status: (✏e , ✏u ) ⌘ (employed, unemployed); (ii) aggregate productivity:
(zg , zb ) ⌘ (expansion, recession). Employment status and aggregate productivity jointly
evolve as a first order Markov process. Assume that ✏ is i.i.d. conditional on z, so the
fraction of employed workers (and hence l) only depends on z. Competitive markets imply
w(K, L, z) = (1

↵)z(K/L)

↵

,

and

r(K, L, z) = ↵z(K/L)↵ .

(12)

Finally, the entire wealth distribution, which I denote with is a state variable for this
0
model and let = H( , z, z 0 ) denote its endogenous transition function (or law of motion).

6.1

Krusell-Smith Algorithm

A key equilibrium object in this class of models is the law of motion, H. In principle,
computing this object is a formidable task since the distribution of wealth is infinite41

dimensional. Krusell and Smith (1997, 1998) show, however, that this class of models,
when reasonably parameterized, exhibits “approximate aggregation”: loosely speaking, to
predict future prices consumers need to forecast only a small set of statistics of the wealth
distribution rather than the entire distribution itself. This result makes it possible to use
numerical methods to analyze this class of models. Another key feature of the Krusell-Smith
algorithm is that it solves the model by simulating it. Specifically, the basic version of the
algorithm works as follows:
1. Approximate with a finite number (I) of moments. (H reduces to a function mapping the I moments today into the I moments tomorrow depending on z today.)
• We will start by selecting one moment—the mean—so I = 1.43
2. Select a family of functions for H. I will choose a log-linear function following Krusell
and Smith.
i
h
0
0
0
0
V (k,✏ ; , z) = max
U (c) + E[V (k , ✏ ; , z )| z,✏ ]
0
c,k

0

c + k = w(K, L, z) ⇥ l ⇥ ✏ + r(K, L, z) ⇥ k,
0

for z = zb

0

for z = zg

log K = a0 + a1 log K
log K = b0 + b1 log K

k0

0

3. Make an (educated) initial guess about (a0 , a1 , b0 , b1 ) ) yields initial guess for H0 .
Make also an initial guess for 0 .
4. Solve the consumer’s dynamic program. Using only the resulting decision rules, simulate {kn,t }N,T
n=1,t=1 for (N, T ) large.
e=
5. Update H by estimating (where K

1
N

PN

n=1

e 0 = a0 + a1 log K
e
log K
e 0 = b0 + b1 log K
e
log K

kn ) :
for z = zb
for z = zg

6. Iterate on 4-5, until the R2 of this regression is “suﬃciently high” and the forecast
variance is “small.”
• If accuracy remains insuﬃcient, go back to step 1 and increase I.
43

When we add more moments, we do not have to proceed as mean, variance, skewness, and so on. We
can include, say, the wealth holdings of the top 10 percent of population, mean- to-median wealth ratio,
etc.

42

6.2

Details

Educated Initial Guess. As with many numerical methods, a good initial guess is critical. More often than not, the success or failure of a given algorithm will depend on the initial
guess. One idea (used by Krusell and Smith) is to first solve a standard representative-agent
RBC model with the same parameterization. Then estimate the coeﬃcients (a0 , a1 , b0 , b1 )
using capital series simulated from this model to obtain an initial guess for step 1 above.44
More generally, a good initial guess can often be obtained by solving a simplified version
of the full model. Sometimes this simplification involves ignoring certain constraints, sometimes by shutting down certain shocks, and so on.
Discretizing an AR(1) Process. Oftentimes, the exogenous driving force in incomplete
markets models is assumed to be generated by an AR(1) process, which is discretized and
converted into a Markov chain. One popular method for discretization is described in
Aiyagari (1993) and has been used extensively in the literature. However, an alternative
method due to Rouwenhorst (1995) (and which received far less attention until recently) is
far superior in the quality of the approximation that it provides, especially when the process
is very persistent, which is often the case. Moreover, it is very easy to implement. Kopecky
and Suen (2010) and Lkhagvasuren and Galindev (2010) provide comprehensive comparisons
of diﬀerent discretization methods, which reveal the general superiority of Rouwenhorst
(1995)’s method. They also show how this method can be extended to discretize more
general processes.
Non-Trivial Equilibrium Pricing Function. One simplifying feature of Krusell and
Smith (1998) is that equilibrium prices (wages and interest rates) are determined trivially
by the marginal product conditions (12). Thus they depend only on the aggregate capital
stock and not on its distribution. Some models do not have this structure—instead pricing
functions must be determined by equilibrium conditions—such as market-clearing or zeroprofit conditions—that explicitly depend on the wealth distribution. This would be the
case, for example, if a household bond is traded in the economy. Its price must be solved
for using market clearing condition, which is a challenging task. Moreover, if there is
an additional asset, such as a stock, two prices must be determined simultaneously, and
this must be done in such a way that avoids providing arbitrage opportunities—along the
iterations of the solution algorithm. Otherwise, individuals’ portfolio choices will go haywire
(in an attempt to take advantage of perceived arbitrage), wreaking havoc with the solution
algorithm. Krusell and Smith (1997) solved such a model and proposed an algorithm to
tackle these issues. I refer the interested reader to that paper for details.
44
Can’t we update H without simulating? Yes we can. den Haan and Rendahl (2009) propose a method
where they use the policy functions for capital holdings and integrate
R 0 them over distribution ⇤(k,✏ ) of
0
households across capital and employment status: K = Hj (K, z) = kj (k,✏ ; , z)d⇤(k,✏ ). This works well
when the decision rules are parameterized in a particular way. See den Haan and Rendahl (2009).

43

Checking for Accuracy of Solution. Many studies with aggregate fluctuations and
heterogeneity use two simple criteria to asses the accuracy of the law of motion in their
limited information approximation. If agents are using the law of motion
log K 0 = ↵1 + ↵2 z + ↵3 log K + u

(13)

a perfectly solved model should find u = 0. Thus, practitioners will continue to solve the
model until either the R2 of this regression is larger than some minimum or u falls below
some threshold (step 6 in the algorithm above).
However, one should not rely solely on R2 and u . There are at least three reasons for
this (den Haan (2010)). First, both measures average over all periods of the simulation.
Thus, infrequent but large deviations from the forecast rule can be hidden in the average.
These errors may be very important to agents and their decision rule. For example, the
threat of a very large recession may increase buﬀer stock saving, but the approximation
may understate the movement of capital in such a case. Second, and more importantly,
these statistics only measure one-step ahead forecasts. The regression only considers the
dynamics from one period to the next, so errors are only the deviations between the actual
next-period capital and the expected amount in the next period. This misses potentially
large deviations between the long-term forecast for capital and its actual level. Aware of
this possibility, Krusell and Smith (1998) also checked the R2 for forecasting prices 25 years
ahead (100 model periods) and found it to be extremely high as well! (They also check the
maximum error in long-term forecasts, which was very small.) Third, R2 is scaled by the
LHS of the regression. An alternative is to check the R2 of:
log K 0

logK = ↵1 + ↵2 z + (↵3

1) log K

As a particularly dramatic demonstration, den Haan (2010) uses a savings decision rule
that solves the Krusell and Smith (1998) model, simulates it for T periods and estimates a
P
law of motion in the form of equation (13). He then manipulates ↵1 , ↵3 such that T 1 ut =
0 but the R2 falls from 0.9999 to 0.999 and then 0.99. This has economically meaningful
consequences: the time series standard deviation of the capital stock simulated from the
perturbed version of equation (13) falls to 70% and then 46% of the true figure.
Finally, den Haan (2010) proposes a useful test that begins with the approximated law
of motion
log K 0 = ↵
ˆ1 + ↵
ˆ2z + ↵
ˆ 3 log K + u
(14)
e t+1 }T and then compare these to the sequence
to generate a sequence of realizations of {K
t=0
e t+1 }T
generated by aggregating from decision rules, the true law of motion. Because {K
t=0
e 0 , errors can accumulate.
is obtained by repeatedly applying Equation (14) starting from K
This is important because in the true model, today’s choices depend upon expectations
44

about the future state, which in turn depends upon the future’s future expectations and
e t to Kt , den Haan proposes an “essential
so errors cascade. To systematically compare K
accuracy plot.” For a sequence of shocks (not those originally used when estimating ↵’s to
e t and Kt . One can then compare moments of the
solve the model) generate a sequence of K

two simulated sequences. The “main focus” of the accuracy test are the errors calculated
e t log Kt |, whose maximum should be made close to zero.
by u
et = | log K

Prices Are More Sensitive Than Quantities. The accuracy of the numerical solution
becomes an even more critical issue if the main focus of analysis is (asset) prices rather
than quantities. This is because prices are much more sensitive to approximation errors
(see, e.g., Christiano and Fisher (2000), and Judd and Guu (2001)). The results in Christiano and Fisher (2000) are especially striking. These authors compare a variety of diﬀerent
implementations of the “parameterized expectations” method and report the approximation
errors resulting from each. For the standard deviation of output, consumption, and investment (i.e., “quantities”) the approximation errors range from less than 0.1% of the true value
to 1-2% in some cases. For the stock and bond return and the equity premium, the errors
regularly exceed 50% and are greater than 100% in several cases. The bottom-line is that
the computation of asset prices require extra care.
Pros and Cons. An important feature of the Krusell-Smith method is that it is a “local”
solution around the stationary recursive equilibrium. In other words, this method relies
on simulating a very long time series of data (e.g., capital series) and making sure that
after this path has converged to the ergodic set, the predictions of agents are accurate for
behavior inside that set. This has some advantages and some disadvantages. On advantage
is the eﬃciency gain compared to solving a full recursive equilibrium, which enforces the
equilibrium conditions at every point of a somewhat arbitrary grid, regardless of whether
or not a given state is ever visited in the stationary equilibrium.
One disadvantage is that if you take a larger deviation—say by setting K0 to a value
well below the steady state value—your “equilibrium functions” are likely to be inaccurate,
and the behavior of the solution may diﬀer significantly from the true solution. Why should
we care about this? Suppose you solve your model then want to study a policy experiment
where you eliminate taxes on savings. You would need to write a separate program from
the “transition” between the two stationary equilibria. Instead, if you solve for the full
recursive equilibrium over a grid that contains both the initial and final steady states, you
would not need to do this. However, solving for the full equilibrium is often much harder
and, therefore, is often “over-kill.”
An Alternative to Krusell-Smith: Tracking History of Shocks. Some models have
few exogenous state variables, but a large number of endogenous states. In such cases, using
a formulation that keeps track of all these state variables can make the numerical solution
45

extremely costly or even infeasible. An alternative method begins with the straightforward
observation that all current endogenous state variables are nothing more than functions of
the infinite history of exogenous shocks. So, one could replace these endogenous states with
the infinite history of exogenous states. Moreover, many models turn out to have “limited
memory” in the sense that only the recent history of shocks matters in a quantitatively
significant way, allowing us to only track a truncated history of exogenous states. The first
implementation of this idea I have been able to find is in Veracierto (1997), who studied a
model with plant-level investment irreversibilities, which give rise to S-s type policies. He
showed that it is more practical to track a short history of the S-s thresholds instead of the
current-period endogenous state variables.
As another example, consider an equilibrium model of the housing market where the
only exogenous state is the interest rate, which evolves as a Markov process. Depending
on the precise model structure, the individual endogenous state variables can include the
mortgage debt outstanding, the time-to-maturity of the mortgage contract, etc., and the
aggregate endogenous state can include the entire distribution of agents over the individual
states. This is potentially an enormously large state space! Arslan (2011) successfully solves
a model of this sort with realistic fixed-rate mortgage contracts, a lifecycle structure, and
stochastic interest rates, using 4 lags of interest rates. Other recent examples that employ
this basic approach include Lustig and Chien (2010) who solve an asset pricing model with
aggregate and idiosyncratic risk in which agents are subject to collateral constraints arising
from limited commitment, and Lorenzoni (2009) who solves a business cycle model with
shocks to individuals’ expectations. In all these cases, tracking a truncated history turns
out to provide computational advantages over choosing endogenous state variables that
evolve in a Markovian fashion. One advantage of this approach is that it is often less costly
to add an extra endogenous state variable relative to the standard approach, because the
same number of lags may still be suﬃcient. One drawback is that if the Markov process
has many states or the model has long memory, the method may not work as well.

7

When Does Heterogeneity Matter for Aggregates?

As noted earlier, Krusell and Smith (1998) report that the time series behavior of the
aggregated incomplete markets model, by and large, looks very similar to the corresponding
representative-agent model. A similar result was reported in Rios-Rull (1996). However,
it is important to interpret these findings correctly and to not overgeneralize them. For
example, even if a model aggregates exactly, modeling heterogeneity can be very important
for aggregate problems. This is because the problem solved by the representative agent
can look dramatically diﬀerent from the problem solved by individuals (for example, have
very diﬀerent preferences). Here, I discuss some important problems in macroeconomics
46

where introducing heterogeneity yields conclusions quite diﬀerent from a representativeagent model.

7.1

The Curse of Long Horizon

It is useful to start by discussing why incomplete markets do not matter in many models.
Loosely speaking, this outcome follows from the fact that a long horizon makes individuals’
savings function approximately linear in wealth (i.e., constant MPC out of wealth). As we
saw in Section 2, the exact linearity of savings rule delivered exact demand aggregation
in both Gorman (1961) and Rubinstein (1974)’s theorems. As it turns out, even with
idiosyncratic shocks, this near-linearity holds for wealth levels that are not immediately near
borrowing constraints. Thus, even though markets are incomplete, redistributing wealth
would matter little, and we have something that looks like demand aggregation!
Is there a way to get around this result? It is instructive to look at a concrete example.
Mankiw (1986) was one of the first papers in the literature on the equity premium puzzle
and one that gave prominence to the role of heterogeneity. Mankiw (1986) showed that, in a
two-period model with incomplete markets and idiosyncratic risk of the right form, one can
generate an equity premium as large as desired. However, researchers who followed up on
this promising lead (Telmer (1993), Heaton and Lucas (1996), and Krusell and Smith (1997))
quickly came to a disappointing conclusion: once agents in these models are allowed to live
for multiple periods, trading a single risk-free asset yields suﬃcient consumption insurance,
which in turn results in a tiny equity premium.
This result—that a suﬃciently long horizon can dramatically weaken the eﬀects of incomplete markets—is quite general. In fact, Levine and Zame (2002) proved that in a
single good economy with no aggregate shocks if: (i) idiosyncratic income shocks follow a
Markov process, (ii) marginal utility is convex, and (iii) all agents have access to a single
risk-free asset, then as individual’s subjective time discount factor ( ) approaches unity,
then incomplete markets allocations (and utilities) converge to those from a complete markets economy with the same aggregate resources. Although Levine and Zame’s result is
theoretical for the limit of such economies (as ! 1), it still sounds a cautionary note
to researchers building incomplete markets models: unless shocks are extremely persistent
and/or individuals are very impatient, these models are unlikely to generate results much
diﬀerent from a representative-agent model.
Constantinides and Duﬃe (1996) have shown one way to get around the problem of a long
horizon, which is also consistent with the message of Levine-Zame’s theorem. Essentially
they assume that individuals face permanent shocks, which eliminates the incentives to
smooth such shocks. Therefore, they behave as if they live in a static world and choose not
to trade. Constantinides and Duﬃe also revive another feature of Mankiw (1986)’s model:
idiosyncratic shocks must have larger variance in recessions (i.e., countercyclical variances)
47

to generate a large equity premium. With these two features, they show that Mankiw’s
original insight can be made to work once again in an infinite horizon model. Storesletten
et al. (2007) found that a calibrated model along the lines suggested by Constantinides and
Duﬃe can generate about 1/4 of the equity premium observed in the US data.
The bottom line is that, loosely speaking, if incomplete markets matter in a model
mainly through its eﬀect on the consumption-saving decision, a long horizon can significantly weaken the bite that incomplete markets has. With a long enough horizon, agents
accumulate suﬃcient wealth and end up on the nearly linear portion of their savings function, delivering results not far from a complete markets model. This is also the upshot of
Krusell and Smith (1998)’s analysis.

7.2

Examples Where Heterogeneity Does Matter

There are many examples in which heterogeneity does matter for aggregate phenomena.
Here, I review some examples.
First, aggregating heterogeneous-agent models can give rise to preferences for the representative agent that may have nothing to do with the preferences in the underlying model.
A well-known example of such a transformation was present in the early works of Hansen
(1985) and Rogerson (1988), who showed that in a model in which individuals have no intensive margin of labor supply (i.e., zero Frisch labor supply elasticity), one can aggregate
the model to find that the representative agent has linear preferences in leisure (i.e., infinite
Frisch elasticity!). This conclusion challenges one of the early justifications for building
models with micro-foundations, which was to bring evidence from micro data to bear on
the calibration of macro models. In an excellent survey paper, Browning et al. (1999)
sounded an early cautionary note, giving several examples where this approach is fraught
with danger.
Building on the earlier insights of Hansen and Rogerson, Chang and Kim (2006) construct a model in which aggregate labor-supply elasticity depends on the reservation-wage
distribution in the population. The economy is populated by households (husband and
wife) who each supply labor only along the extensive margin: they either work full time or
stay home. Workers are hit by idiosyncratic productivity shocks, causing them to move in
and out of the labor market. The aggregate labor-supply elasticity of such an economy is
around 1, greater than a typical micro estimate and much greater than the Frisch elasticity
one would measure at the intensive margin (which is zero) in this model. The model thus
provides a reconciliation between the micro and macro labor-supply elasticities. In a similar vein, Chang et al. (2010) show that preference shifters that play an important role in
discussions of aggregate policy are not invariant to policies if they are generated from the
aggregation of a heterogeneous-agent model. Such a model also generates “wedges” at the
aggregate level that do not translate into any well-defined notion of preference shifters at
48

the micro level.45 Finally, Erosa et al. (2009) build a lifecycle model of labor supply, by
combining and extending the ideas introduced in earlier papers, such as Chang and Kim
(2006) and Rogerson and Wallenius (2009). Their goal is to build a model with empirically
plausible patterns of hours over the life cycle and examine the response elasticities of labor
supply to various policy experiments.
In another context, Guvenen (2006) asks why macroeconomic models in the RBC tradition typically need to assume a high elasticity of intertemporal substitution (EIS) to
explain output and investment fluctuations, whereas Euler equation regressions (such as
in Hall (1988) and Campbell and Mankiw (1990)) that use aggregate consumption data
estimate a much smaller EIS (close to zero) to fit the data. He builds a model with two
types of agents who diﬀer in their EIS. The model generates substantial wealth inequality
and much smaller consumption inequality, both in line with the US data. Consequently,
capital and investment fluctuations are mainly driven by the rich (who hold almost all the
wealth in the economy) and thus reflect the high EIS of this group. Consumption fluctuations, on the other hand, reflect an average that puts much more weight on the poor’s EIS,
who contribute significantly to aggregate consumption. Thus, a heterogeneous agent model
is able to explain aggregate evidence that a single representative-agent model has trouble
fitting.
In an asset pricing context, Constantinides and Duﬃe (1996), Chan and Kogan (2002)
and Guvenen (2011) show a similar result for risk aversion. Constantinides and Duﬃe (1996)
show theoretically how the cross-sectional distribution of consumption in a heterogeneousagent model gets translated into a higher risk aversion for the representative agent. Guvenen (2011) shows that in a calibrated model with limited stock market participation that
matches several asset pricing facts, the aggregate risk aversion is measured to be as high
as 80, when the individuals’ risk aversion is only 2. These results as well as the papers
discussed above confirm and amplify the concerns originally highlighted by Browning et al.
(1999). The conclusion is that researchers must be very careful when using micro evidence
to calibrate representative-agent models.

8

Computation and Calibration

Because of their complexity, the overwhelming majority of models in this literature are
solved on a computer using numerical methods.46 Thus, I now turn to a discussion of
computational issues that researchers often have to confront when solving models with
heterogeneity.
45

See Chari et al. (2007) for the definition of business cycle wedges.
There are few examples of analytical solutions and theoretical results established with heterogeneousagent models. See, e.g., Constantinides and Duﬃe (1996), Heathcote et al. (2007), Wang (2009), Guvenen
and Kuruscu (forthcoming), and Rossi-Hansberg and Wright (2007).
46

49

8.1

Calibration and Estimation: Avoid Local Search

Economists often need to minimize an objective function of multiple variables that has
lots of kinks, jaggedness, and deep ridges. Consequently, the global minimum is often
surrounded by a large number of local minima. A typical example of such a problem arises
when a researcher tries to calibrate several structural parameters of an economic model
by matching some data moments. Algorithms based on local optimization methods (e.g.,
Newton-Raphson style derivative-based methods or Nelder-Mead simplex style routines)
very often get stuck in local minima, because the objective surface is typically very rough
(non-smooth).
It is useful to understand some of the sources of this roughness. For example, linear interpolation that is often used in approximating value functions or decision rules generates an
interpolated function that is non-diﬀerentiable (i.e., have kinks) at every knot point. Similarly, problems with (borrowing, portfolio, etc.) constraints can create significant kinks.
Because researchers use a finite number of individuals to simulate data from the model
(to compute moments) a small change in the parameter value (during the minimization
of the objective) can move some individuals across the threshold—from being constrained
to unconstrained or vice versa—which can cause small jumps in the objective value. And
sometimes, the moments that the researcher decides to match would be inherently discontinuous in the underlying parameters (with a finite number of individuals), such as the
median of a distribution (e.g., wealth holdings). Further compounding the problems, if
the moments are not jointly suﬃciently informative about the parameters to be calibrated,
the objective function would be flat in certain directions. As can be expected, trying to
minimize a relatively flat function with lots of kinks, jaggedness, and even small jumps, can
be a very diﬃcult task indeed.47
While the algorithm described here can be applied to the calibration of any model, it is
especially useful in models with heterogeneous agents—since such models are time consuming to solve even once, an exhaustive search of the parameter space becomes prohibitively
costly (which could be feasible in simpler models).

8.2

A Simple Fully Parallelizable Global Optimization Algorithm

Here, I describe a global optimization algorithm that I regularly use for calibrating
models and I have found it to be very practical and powerful. It is relatively straightforward
47

One simple, but sometimes overlooked, point is that when minimizing an objective function of moments
to calibrate a model, one should use the same “seeds” for the random elements of the model that are used
to simulate the model in successive evaluations of the objective function. Otherwise, some of the change
in objective value will be due to the inherent randomness in diﬀerent draws of random variables. This can
create significant problems with the minimization procedure.

50

to implement, yet allows full parallelization across any number of CPU cores as well as across
any number of computers that are connected to the Internet. It requires no knowledge of
MPI, OpenMP, or related tools, and no knowledge of computer networking other than using
some commercially available synchronization tools (such as DropBox, SugarSync, etc.).
A broad outline of the algorithm is as follows. As with many global algorithms, this
procedure combines a global stage with a local search stage that is restarted at various
locations in the parameter space. First, we would like to search the parameter space as
thoroughly as possible, but do so in as eﬃcient a way as possible. Thoroughness is essential
because we want to be sure that we found the true global minimum, so we are willing to
sacrifice some speed to ensure this. The algorithm proceeds by taking an initial starting
point (chosen in a manner described momentarily) and conducting a local search from that
point on until the minimization routine converges as specified by some tolerance. For local
search, I typically rely on the Nelder-Mead’s downhill simplex algorithm because it does
not require derivative information (that may be inaccurate given the approximation errors
in the model’s solution algorithm).48 The minimum function value as well as the parameter
combination that attained that minimum are recorded in a file saved on the computer’s hard
disk. The algorithm then picks the next “random” starting point and repeats the previous
step of local minimization. The results are then added to the previous file, which records
all the local minima found up to that point.
Of course, the most obvious algorithm would be to keep doing a very large number of
restarts of this sort and take the minimum of all the minima found in the process. But
this would be very time consuming and would not be particularly eﬃcient. Moreover, in
many cases, the neighborhood of the global minimum can feature many deep ridges and
kinks nearby, which requires more extensive searching near the global minimum, whereas
the proposed approach would devote as much time to points far away from the true global
minimum as to the points near it. Further, if the starting points are chosen literally randomly, this would also create potentially large eﬃciency losses, because these points have a
non-negligible chance of falling near points previously tried. Because those areas have been
previously searched, devoting more time is not optimal.
A better approach is to use “quasi-random” numbers to generate the starting points.
Quasi-random numbers (also called Low-discrepancy sequences) are sequences of deterministic numbers that spread to any space in the maximally separated way. They avoid the
pitfall of random draws that may end up being too close to each other. Each draw in the
sequence “knows” the location of previous points drawn and attempts to fill the gaps as
evenly as possible.49 Among a variety of sequences proposed in the literature, the Sobol’
48

An alternative that can be much faster but requires a bit more tweaking for best performance is the
trust region method of Zhang et al. (2010) that builds on Powell (2009)’s BOBYQA algorithm.
49
Another common application of low-discrepancy sequences is in quasi-Monte Carlo integration, where
they have been found to improve time-to-accuracy by several orders of magnitude.

51

sequence is generally viewed to be superior in most practical applications, having a very
uniform filling of the space (i.e., maximally separated) even for a small number of points
drawn as well as a very fast algorithm that generates the sequence.50
Next, how do we use the accumulated information from previous restarts? As suggested
by genetic algorithm heuristics, I combine information from previous best runs to adaptively
direct the new restarts to areas that appear more promising. This is explained further below.
Now for the specifics of the algorithm.
Let p be a J-dimensional parameter vector with generic element pj , j = 1, .., J.
• Step 0. Initialization:
1. Determine bounds for each parameter, outside of which the objective function
should be set to a high value.
2. Generate a sequence of Sobol’ numbers with a sequence length of Imax (the maximum anticipated number of restarts in the global stage). Set the global iteration
number i = 1.
• Step 1. Global stage:
1. Draw the ith (vector) value in the Sobol’ sequence: si .
2. If i > 1, open and read from the text file “saved_parameters.dat” the function
values (and corresponding parameter vectors) of previously found local minima.
Denote the lowest function value found as of iteration i 1 as filow1 and the
corresponding parameter vector as plow
i 1.
3. Generate a starting point for the local stage as follows:
– If i < Imin (< Imax ) then use si as the initial guess: Si = si . Here, Imin is the
threshold below which we use fully quasi-random starting points in the global
stage.
– If i
Imin , take the initial guess to be a convex combination of si and the
parameter value that generated the best local minima so far: Si = (1 ✓i )si +
✓plow
i 1 . The parameter ✓i 2 [0, ✓] with ✓ < 1, and increases with i. For example,
I found that a convex increasing function, such as ✓i = min[✓, (i/Imax )2 ], works
well in some applications. An alternative heuristic is given later.
– As ✓i is increased local searches are restarted from a narrower part of the parameter space that yielded the lowest local minima before.
50
In a wide range of optimization problems, Liberti and Kucherenko (2005) and Kucherenko and Sytsko
(2005) find that Sobol’ sequences outperforms Holton sequences, both in terms of computation time and
probability of finding the global optimum. The Holton sequence is particularly weak in high dimensional
applications.

52

• Step 2: Local stage:
– Using Si as a starting point, use the downhill simplex algorithm to search for a
local minimum. (For the other vertices of the simplex, randomly draw starting
points within the bounds of the parameter space).
– Stop when either (i) a certain tolerance has been achieved, (ii) function values do
not improve more than a certain amount, or (iii) the maximum iteration number
is reached.
– Open saved_parameters.dat and record the local minimum found (function
value and parameters).
• Step 3. Stopping Rule:
– Stop if the termination criterion described below is satisfied. If not go to step 1.
Termination Criterion. One useful heuristic criterion relies on a Bayesian procedure
that estimates the probability that the next local search will find a new local minimum
based on the rate at which new local minima have been located in past searches. More
concretely, if W diﬀerent local minima have been found after K local searches started from
a set of uniformly distributed points, then the expectation of the number of local minima is
Wexp = W (K−1)/(K−W −2)
provided that K > W + 2. The searching procedure is terminated if Wexp < W + 0.5. The
idea is that, after a while of searching, if subsequent restarts keep finding one of the the
same local minima found before, the chances of improvement in subsequent searches is not
worth the additional time cost. Although this is generally viewed as one of the most reliable
heuristics, care must be applied as with any heuristic.
Notice also that Wexp can be used to adaptively increase the value of ✓i in the global
stage (Step 1(3) above). The idea is that as subsequent global re-starts do not yield a
new local minimum with a high enough probability, it is time to narrow down the search
and further explore areas of promising local minima. Because jaggedness and deep ridges
cause local search methods to often get stuck, we want to explore promising areas more
thoroughly.
One can improve upon this basic algorithm in various ways. I am going to mention a
few that seem worth exploring.

53

Refinements: Clustering and Pre-testing. First, suppose that in iteration k, the
proposed starting point Sk ends up being “close” to one of the previous minima, say plow
n ,
low
for n < k. Then it is likely that the search starting from Sk will end up converging to pn .
But then we have wasted an entire cycle of local search without gaining anything. To prevent
this, one heuristic (called “clustering methods”) proceeds by defining a “region of attraction”
(which is essentially a J-dimensional ball centered) around each one of the local minima
found so far.51 Then the algorithm would discard a proposed re-starting point if it falls into
the region attraction of any previous local minima. Because the local minimization stage is
the most computationally intensive step, this refinement of restarting the local search only
once in a given region of attraction can result in significant computational gains. Extensive
surveys of clustering methods can be found in Rinnooy Kan and Timmer (1987a,b).
Second, one can add a “pre-test” stage where N points from the Sobol’ sequence are
evaluated before any local search (i.e., in Step 0 above), and only a subset of N ⇤ < N
points with lowest objective values are used as seeds in the local search. The remaining
points as well as regions of attraction around it are ignored as not promising. Notice that
while this stage can improve speed, it trades oﬀ reliability in the process.
Narrowing Down the Search Area. The file saved_parameters.dat contains a lot of
useful information gathered in each iteration to the global stage, which can be used more
eﬃciently as follows. As noted, the Nelder-Mead algorithm requires J + 1 candidate points
as inputs (the vertices of the J-dimensional simplex). One of these points is given by Si ,
chosen as described above; the other vertices were drawn randomly. But as we accumulate
more information with every iteration on the global stage, if we keep finding local minima
that seem to concentrate in certain regions, it makes sense to narrow the range of values
from which we pick the vertices. One way to do this is as follows: After a suﬃciently large
number of re-starts have been completed, rank all the function values and take the lowest
x% of values (e.g., 10% or 20%). Then for each dimension, pick the minimum (call pjmin ) and
maximum parameter value (call pjmax ) within this set of minima. Then to generate vertices,
take randomly sampled points between pjmin and pjmax in each dimension j. This allows the
simplex algorithm to search more intensively in a narrower area, which can improve results
very quickly when there are ridges or jaggedness in the objective function that makes the
algorithm to get stuck.
51

While diﬀerent formulas have been proposed for determining the optimal radius, these formulas contain
some undetermined coeﬃcients which makes them less than useful in real life applications.

54

8.3

Parallelizing the Algorithm

The algorithm can be parallelized in a relatively straightforward manner.52 The basic idea
is to let each CPU core perform a separate local search in a diﬀerent part of the parameter
space, which is a time-consuming process. If we can do many such searches simultaneously,
we can speed up the solution dramatically. One factor that makes parallelization simple is
that fact that the CPU cores do not need to communicate with each other during the local
search stage. In-between the local stages, each CPU core will contribute its findings (the
last local minimum it found along with the corresponding parameter vector) to the collective
wisdom recorded in saved_parameters.dat and also get the latest updated information
about the best local minimum found so far from the same file. Thus, as long as all CPU cores
have access to the same copy of the file saved_parameters.dat, parallelization requires no
more than a few lines for housekeeping across CPUs. Here are some more specifics.
Suppose that we have a workstation with N CPU cores (for example, N = 4, 6, or 12).
The first modification we need to make is to change the program to distinguish between
the diﬀerent “copies” of the code, running on diﬀerent CPU cores. This can be done by
simply having the program ask the user (only once, upon starting the code) to input an
integer value, n, between 1 and N , which uniquely identifies the “sequence number” of the
particular instance of the program running. Then open N terminal windows and launch a
copy of the program in each window. Then for each one, enter a unique sequence number
n = 1, 2..., N .
Upon starting, each program will first simulate the same quasi-random sequence regardless of n, but each run will pick a diﬀerent element of this sequence as its own seed.
For simplicity, suppose run n chooses the nth element of the sequence as its seed and
launches a local search from that point. After completion, each run will open the same
file saved_parameters.dat and record the local minimum and parameter value it finds.53
Now suppose that all copies of the program complete their respective first local searches, so
there are N lines, each written by a diﬀerent CPU core, in the file saved_parameters.dat.
Then each run will start its second iteration and pick as its next seed the (N + n)th element
of the quasi-random sequence. When the total number of iterations across all CPUs exceed
some threshold Imin , then we would like to combine the quasi-random draw with the previous best local minima as described in Step 1.3 above. This is simple since all runs have
access to the same copy of saved_parameters.dat.54
52

I am assuming here that a compiled language, such as Fortran or C, is used to write the program. So
multiple parallel copies of the same code can be run in diﬀerent terminal windows.
53
Because this opening and writing stage takes a fraction of a second, the likelihood that two or more
programs access the file simultaneously and create a run-time error is negligible.
54
It is often useful for each run to keep track of the total number of local searches completed by all
CPUs—call this NLast . For example, sometimes the increase in ✓i can be linked to NLast . This number
can be read oﬀ as the total number of lines recorded up to that point in saved_parameters.dat. Another

55

Notice that this parallelization method is completely agnostic about whether the CPU
cores are on the same PC or distributed across many PCs as long as all computers keep
synchronized copies of saved_parameters.dat. This can be achieved by using a synchronization service like DropBox. This feature easily allows one to harness the computational
power of many idle PCs distributed geographically with varying speeds and CPU cores.

9

Future Directions and Further Reading

This article has surveyed the current state of the heterogeneous-agent models literature and
drew several conclusions. First, two key ingredients in such models are (i) the magnitudes
and types of risk that the model-builder feeds into the model and (ii) the insurance opportunities allowed in the economy. In many cases, it is diﬃcult, if not impossible, to measure
each component separately. In other words, the assumptions a researcher makes regarding
insurance opportunities will typically aﬀect the inference drawn about the magnitudes of
risks and vice versa. Further complicating the problem is the measurement of risk: individuals often have more information than the econometrician about future changes in their life.
So, for example, a rise or fall in income that the econometrician may view as a “shock” may
in fact be partially or completely anticipated by the individual. This suggests that equating
income movements observed in the data with risk (as is often done in the literature) is likely
to overstate the true magnitude. This entanglement of “risk,” “anticipated changes,” and
“insurance” presents a diﬃcult challenge to researchers in this area. Although some recent
progress has been made, more work remains.
A number of surveys contain very valuable material that are complementary to this
paper. First, Heathcote et al. (2009) is another recent survey of quantitative macroeconomics with heterogeneous households that is complementary to this paper. Second,
Browning et al. (1999) contains an extensive review of micro models that are often used
as the foundations of heterogeneous-agent models. It highlights several pitfalls in trying to
calibrate macro models using micro evidence. Third, Meghir and Pistaferri (2011) provides
a comprehensive treatment of how earnings dynamics aﬀect life cycle consumption choice,
which is closely related to the issues discussed in Section 4 of this survey. Finally, because
heterogeneous-agent models use micro survey data in increasingly sophisticated ways, a
solid understanding of issues related to measurement error (which is pervasive in micro
data) is essential. Failure to understand such problems can wreak havoc with the empirical
analysis. Bound et al. (2001) is an extensive and authoritative survey of the subject.
use of this index is for determining which point in the sequence to select as the next seed point. So as
opposed to run n selecting the (kN + n)th point in the sequence where k is the number of local searches
completed by run n, it could just pick the (NLast + 1)th number in the sequence. This avoids leaving gaps
in the sequence for seeds, in case some CPUs are much faster than others and hence finish many more local
searches than others.

56

The introduction of families into incomplete markets models represents an exciting area
of current research. For many questions of empirical relevance, the interactions taking place
within a household (implicit insurance, bargaining, etc.) can have first-order eﬀects on how
individuals respond to idiosyncratic changes. To give a few examples, Gallipoli and Turner
(2011) document that the labor supply responses to disability shocks of single workers are
larger and more persistent than those of married workers. They argue that an important
part of this diﬀerence has to do with the fact that couples are able to optimally change
their time (and task) allocation within household in response to disability, an option not
available to singles. This finding suggests that modeling households would be important for
understanding the design of disability insurance policies. Similarly, Guner et al. (2010) show
that to quantify the eﬀects of alternative tax reforms, it is important to take into account
the joint nature of household labor supply. In fact, it is hard to imagine any distributional
issue for which the household structure does not figure in an important way.
Another promising area is the richer modeling of household finances in an era of everincreasing sophistication in financial services. The Great Recession, which was accompanied
by a housing market crash and soaring personal bankruptcies, home foreclosures, and so
on, has created a renewed sense of urgency for understanding household balance sheets.
Developments on two fronts—advances in theoretical modeling as discussed in Section 4
combined with richer data sources on credit histories and mortgages that are increasingly
becoming available to researchers—will make faster progress feasible in this area.

References
Abowd, John M and David Card, “On the Covariance Structure of Earnings and Hours
Changes,” Econometrica, March 1989, 57 (2), 411–45. 25, 27
Acemoglu, Daron, “Technical Change, Inequality, and the Labor Market,” Journal of
Economic Literature, March 2002, 40 (1), 7–72. 39
Aguiar, Mark and Erik Hurst, “Deconstructing Lifecycle Expenditures,” Technical Report, University of Rochester 2008. 32
and Mark Bils, “Has Consumption Inequality Mirrored Income Inequality,” Technical
Report, University of Rochester 2011. 33
Aiyagari, S Rao, “Uninsured Idiosyncratic Risk and Aggregate Saving,” Working Paper
502, Federal Reserve Bank of Minneapolis 1993. 43
, “Uninsured Idiosyncratic Risk and Aggregate Saving,” The Quarterly Journal of Economics, August 1994, 109 (3), 659–84. 4, 23, 24, 34, 36
57

Altug, Sumru and Robert A Miller, “Household Choices in Equilibrium,” Econometrica, May 1990, 58 (3), 543–70. 14, 16
Arslan, Yavuz, “Interest Rate Fluctuations and Equilibrium in the Housing Market,”
Technical Report, Central Bank of the Republic of Turkey 2011. 46
Athreya, Kartik B., “Welfare implications of the Bankruptcy Reform Act of 1999,” Journal of Monetary Economics, 2002, 49 (8), 1567 – 1595. 29
Attanasio, Orazio and Steven J Davis, “Relative Wage Movements and the Distribution
of Consumption,” Journal of Political Economy, December 1996, 104 (6), 1227–62. 14,
15
, Erich Battistin, and Hidehiko Ichimura, “What Really Happened to Consumption Inequality in the US?,” in E. Berndt and C. Hulten, eds., Measurement Issues in
Economics - Paths Ahead: Essays in Honour of Zvi Griliches, Chicago: University of
Chicago Press, 2007. 20, 32
Attanasio, Orazio P, James Banks, Costas Meghir, and Guglielmo Weber,
“Humps and Bumps in Lifetime Consumption,” Journal of Business & Economic Statistics, January 1999, 17 (1), 22–35. 22
Autor, David H., Lawrence F. Katz, and Melissa S. Kearney, “Trends in U.S. Wage
Inequality: Revising the Revisionists,” The Review of Economics and Statistics, 2008, 90
(2), 300–323. 38, 39
Badel, Alejandro and Mark Huggett, “Interpreting Life-Cycle Inequality Patterns as
an Eﬃcient Allocation: Mission Impossible?,” Technical Report, Georgetown University
2007. 13, 14, 17
Baker, Michael, “Growth-Rate Heterogeneity and the Covariance Structure of Life-Cycle
Earnings,” Journal of Labor Economics, 1997, 15 (2), 338–375. 25, 27
and Gary Solon, “Earnings Dynamics and Inequality among Canadian Men, 1976–
1992: Evidence from Longitudinal Income Tax Records,” Journal of Labor Economics,
2003, 21 (3), 289–321. 27
Becker, Gary S., Human Capital: A Theoretical and Empirical Analysis, with Special
Reference to Education, University of Chicago Press, 1964. 38
Ben-Porath, Yoram, “The Production of Human Capital and the Life Cycle of Earnings,”
Journal of Political Economy, 1967, 75 (4), 352–365. 38
Bewley, Truman F., “Interest Bearing Money and the Equilibrium Stock of Capital,”
undated. 23
58

Blundell, Richard and Ian Preston, “Consumption Inequality And Income Uncertainty,” The Quarterly Journal of Economics, May 1998, 113 (2), 603–640. 22, 28, 33
, Luigi Pistaferri, and Ian Preston, “Consumption Inequality and Partial Insurance,”
American Economic Review, December 2008, 98 (5), 1887–1921. 20, 21, 22, 28
Bound, John, Charles Brown, and Nancy Mathiowetz, “Measurement error in survey
data,” in J.J. Heckman and E.E. Leamer, eds., Handbook of Econometrics, Elsevier, 2001,
chapter 59, pp. 3705–3843. 56
Browning, Martin, Lars Peter Hansen, and James J. Heckman, “Micro Data
and General Equilibrium Models,” in J. B. Taylor and M. Woodford, eds., Handbook
of Macroeconomics, 1999. 48, 49, 56
, Mette Ejrnaes, and Javaier Alvarez, “Modelling Income Processes with Lots of
Heterogeneity,” Review of Economic Studies, 2010, 77, 1353–1381. 27
Cagetti, Marco and Mariacristina De Nardi, “Entrepreneurship, Frictions, and
Wealth,” Journal of Political Economy, October 2006, 114 (5), 835–870. 34, 35, 37
Campbell, John Y. and N. Gregory Mankiw, “Consumption, Income, and Interest
Rates: Reinterpreting the Time Series Evidence,” NBER Working Papers 2924, National
Bureau of Economic Research, Inc May 1990. 49
Carroll, Christopher, “Why Do the Rich Save So Much?,” in Joel Slemrod, ed., Does
Atlas Shrug? The Economic Consequences of Taxing the Rich, Harvard University Press,
2000, pp. 466–484. 35
Carroll, Christopher D., “Buﬀer-Stock Saving and the Permanent Income Hypothesis,”
Working Paper Series/Economic Activity Section 114, Board of Governors of the Federal
Reserve System 1991. 23
Carroll, Christopher D, “Buﬀer-Stock Saving and the Life Cycle/Permanent Income
Hypothesis,” The Quarterly Journal of Economics, February 1997, 112 (1), 1–55. 22, 23
Carroll, Christopher D. and Andrew A. Samwick, “The nature of precautionary
wealth,” Journal of Monetary Economics, September 1997, 40 (1), 41–71. 22
Caselli, Francesco and Jaume Ventura, “A Representatitve Consumer Theory of Distribution,” American Economic Review, 2000, 90 (4), 909–926. 13, 14
Castañeda, Ana, Javier Díaz-Giménez, and José-Víctor Ríos-Rull, “Accounting
for the U.S. Earnings and Wealth Inequality,” The Journal of Political Economy, 2003,
111 (4), 818–857. 34
59

Chamberlain, Gary and Charles A. Wilson, “Optimal Intertemporal Consumption
Under Uncertainty,” Review of Economic Dynamics, July 2000, 3 (3), 365–395. 36
Chan, Yeung Lewis and Leonid Kogan, “Catching up with the Joneses: Heterogeneous
Preferences and the Dynamics of Asset Prices,” The Journal of Political Economy, 2002,
110 (6), 1255–1285. 49
Chang, Yongsung and Sun-Bin Kim, “From Individual to Aggregate Labor Supply:
A Quantitative Analysis based on a Heterogeneous-Agent Macroeconomy,” International
Economic Review, 2006, 47 (1), 1–27. 48, 49
,
, and Frank Schorfheide, “Labor Market Heterogeneity, Aggregation, and the
Lucas Critique,” Technical Report 16401, National Bureau of Economic Research 2010.
48
Chari, V. V., Patrick J. Kehoe, and Ellen R. McGrattan, “Business Cycle Accounting,” Econometrica, 2007, 75 (3), 781–836. 49
Chatterjee, Satyajit and Burcu Eyigungor, “A Quantitative Analysis of the US Housing and Mortgage Markets and the Foreclosure Crisis,” Technical Report, Federal Reserve
Bank of Philadelphia 2011. 30
, Dean Corbae, Makoto Nakajima, and José-Víctor Ríos-Rull, “A Quantitative
Theory of Unsecured Consumer Credit with Risk of Default,” Econometrica, November
2007, 75 (6), 1525–1589. 30
Christiano, Lawrence and Jonas D. M. Fisher, “Algorithms for Solving Dynamic Models with Occasionally Binding Constraints,” Journal of Economic Dynamics and Control,
2000, 24 (8), 1179–1232. 45
Clarida, Richard, “Consumption, Liquidity Constraints, and Asset Accumulation in the
Presence of Random Income Fluctuations,” International Economic Review, 1987, 38,
339–351. 23
, “International Lending and Borrowing in a Stochastic, Stationary Equilibrium,” International Economic Review, 1990, 31, 543–558. 23
Cochrane, John H, “A Simple Test of Consumption Insurance,” Journal of Political Economy, October 1991, 99 (5), 957–76. 4, 14, 15, 16
Congressional Budget Oﬃce, “Recent Trends in the Variability of Individual Earnings
and Family Income,” Washington, DC 2008. 40

60

Constantinides, George M., “Intertemporal Asset Pricing with Heterogeneous Consumers and Without Demand Aggregation,” Journal of Business, 1982, 55, 253–267.
4, 10, 22
and Darrell Duﬃe, “Asset Pricing with Heterogeneous Consumers,” The Journal of
Political Economy, 1996, 104 (2), 219–240. 47, 49
Cunha, Flavio, James Heckman, and Salvador Navarro, “Separating Uncertainty
from Heterogeneity in Life Cycle Earnings,” Oxford Economic Papers, 2005, 57 (2), 191–
261. 27
Cutler, David M and Lawrence F Katz, “Rising Inequality? Changes in the Distribution of Income and Consumption in the 1980’s,” American Economic Review, May 1992,
82 (2), 546–51. 32
Davis, Steven, Jason Faberman, John Haltiwanger, Ron Jarmin, and Javier
Miranda, “Business Volatility, Job Destruction, and Unemployment,” Technical Report
No. 14300, National Bureau of Economic Research 2010. 40
De Nardi, Mariacristina, Eric French, and John Bailey Jones, “Why Do the Elderly
Save? The Role of Medical Expenses,” Journal of Political Economy, 2010, 118 (1), 39–75.
29
Deaton, Angus, “Saving and Liquidity Constraints,” Econometrica, September 1991, 59
(5), 1221–48. 23
and Christina Paxson, “Intertemporal Choice and Inequality,” Journal of Political
Economy, June 1994, 102 (3), 437–67. 17, 18, 19, 20, 21, 22, 38
Debreu, Gerard, Theory of Value, New York: John Wiley and Sons, 1959. 10
den Haan, Wouter, “Assessing the Accuracy of the Aggregate Law of Motion in Models
with Heterogeneous Agents,” Journal of Economic Dynamics and Control, 2010, 34 (1),
79–99. 44
and Pontus Rendahl, “Solving the Incomplete Markets Model with Aggregate Uncertainty Using Explicit Aggregation,” Technical Report, University of Amsterdam 2009.
43
Domeij, David and Martin Floden, “The Labor-Supply Elasticity and Borrowing Constraints: Why Estimates are Biased,” Review of Economic Dynamics, April 2006, 9 (2),
242–262. 31
Dynan, Karen, Douglas W. Elmendorf, and Daniel E. Sichel, “The Evolution of
Household Income Volatility,” Technical Report, Federal Reserve Board 2007. 39, 40
61

Erosa, A., L. Fuster, and G. Kambourov, “The Heterogeneity and Dynamics of Individual Labor Supply over the Life Cycle: Facts and Theory,” Working Paper, University
of Toronto 2009. 49
Flavin, Marjorie A., “The Adjustment of Consumption to Changing Expectations About
Future Income,” Journal of Political Economy, 1981, 89 (5), 974–1009. 23
French, Eric and John Bailey Jones, “On the Distribution and Dynamics of Health
Care Costs,” Journal of Applied Econometrics, 2004, 19 (6), 705–721. 28
Gallipoli, Giovanni and Laura Turner, “Household Responses to Individual Shocks:
Disability and Labour Supply,” Technical Report, University of British Columbia 2011.
57
Glover, Andrew and Jacob Short, “Bankruptcy, Incorporation, and the Nature of
Entrepreneurial Risk,” Working Papers, University of Western Ontario 2010. 30
Gorman, William M, “On a Class of Preference Fields,” Metroeconomica, 1961, 13, 53–56.
7, 47
Gottschalk, Peter and Robert Moﬃtt, “The Growth of Earnings Instability in the U.S.
Labor Market,” Brookings Papers on Economic Activity, 1994, 25 (1994-2), 217–272. 39
and , “Changes in Job Instability and Insecurity Using Monthly Survey Data,” Journal
of Labor Economics, 1999, 17 (4), S91–S126. 40
Gourinchas, Pierre-Olivier and Jonathan A. Parker, “Consumption over the Life
Cycle,” Econometrica, 2002, 70 (1), 47–89. 22, 23
Greenwood, Jeremy, Ananth Seshadri, and Mehmet Yorukoglu, “Engines of Liberation,” Review of Economic Studies, 2005, 72 (1), 109–133. 31
and Nezih Guner, “Marriage and Divorce since World War II: Analyzing the Role
of Technological Progress on the Formation of Households,” in “NBER Macroeconomics
Annual,” Vol. 23, National Bureau of Economic Research, 2008, pp. 231–276. 31
Guner, Nezih, Remzi Kaygusuz, and Gustavo Ventura, “Taxation and Household
Labor Supply,” Technical Report, Arizona State University 2010. 31, 57
Gustavsson, Magnus and Par Osterholm, “Does the Labor-Income Process Contain a
Unit Root? Evidence from Individual-Specific Time Series,” Technical Report, Uppsala
University 2010. 27

62

Guvenen, Fatih, “Reconciling conflicting evidence on the elasticity of intertemporal substitution: A macroeconomic perspective,” Journal of Monetary Economics, 2006, 53 (7),
1451 – 1472. 34, 35, 36, 49
, “Do Stockholders Share Risk More Eﬀectively than Nonstockholders?,” The Review of
Economics and Statistics, 03 2007, 89 (2), 275–288. 14, 16
, “Learning Your Earning: Are Labor Income Shocks Really Very Persistent?,” American
Economic Review, June 2007, 97 (3), 687–712. 17
, “An Empirical Investigation of Labor Income Processes,” Review of Economic Dynamics,
January 2009, 12 (1), 58–79. 17, 24, 25, 26, 27, 38
, “A Parsimonious Macroeconomic Model for Asset Pricing,” Econometrica, 2009, 77 (6),
1711–1750. 36
, “Limited Stock Market Participation Versus External Habit: An Intimate Link,” Technical Report, University of Minnesota 2011. 49
and Anthony A Smith, “Inferring Labor Income Risk from Economic Choices: An
Indirect Inference Approach,” Working Paper, University of Minnesota 2009. 19, 27
and Burhanettin Kuruscu, “A Quantitative Analysis of the Evolution of the U.S.
Wage Distribution, 1970-2000,” NBER Macroeconomics Annual, 2009, 24 (1), 227–276.
13, 14, 32
and , “Understanding the Evolution of the U.S. Wage Distribution: A Theoretical
Analysis,” Journal of the European Economic Association, forthcoming. 32, 49
and Michelle Rendall, “Emancipation Through Education,” Technical Report, University of Minnesota 2011. 31
, Burhanettin Kuruscu, and Serdar Ozkan, “Taxation of Human Capital and Wage
Inequality: A Cross-Country Analysis,” NBER Working Papers 15526 2009. 38
Haider, Steven J., “Earnings Instability and Earnings Inequality of Males in the United
States: 1967-1991,” Journal of Labor Economics, 2001, 19 (4), 799–836. 27, 40
and Gary Solon, “"Life-Cycle Variation in the Association between Current and Lifetime Earnings.,” American Economic Review, 2006, 96 (4), 1308–1320. 27
Hall, Robert E, “Intertemporal Substitution in Consumption,” Journal of Political Economy, April 1988, 96 (2), 339–57. 49

63

and Frederic S Mishkin, “The Sensitivity of Consumption to Transitory Income:
Estimates from Panel Data on Households,” Econometrica, March 1982, 50 (2), 461–81.
18, 20, 21, 23
Hansen, Gary, “Indivisible Labor and the Business Cycle,” Journal of Monetary Economics, 1985, 16 (3), 309–327. 48
Harris, Milton and Bengt Holmstrom, “A Theory of Wage Dynamics,” Review of
Economic Studies, 1982, 49 (315-333.). 38
Hause, John C., “The Fine Structure of Earnings and the On-the-Job Training Hypothesis,” Econometrica, 1980, 48 (4), 1013–1029. 25, 27
Hayashi, Fumio, Joseph Altonji, and Laurence Kotlikoﬀ, “Risk-Sharing between
and within Families,” Econometrica, March 1996, 64 (2), 261–94. 14, 15, 16
Heathcote, Jonathan, Fabrizio Perri, and Giovanni L. Violante, “Unequal We
Stand: An Empirical Analysis of Economic Inequality in the United States, 1967-2006,”
Review of Economic Dynamics, 2010, 13 (1), 15–51. 19, 38
, Kjetil Storesletten, and Giovanni L Violante, “Consumption and Labour Supply
with Partial Insurance: An Analytical Framework,” C.E.P.R. Discussion Papers 6280
2007. 33, 49
, , and Giovanni L. Violante, “The Macroeconomic Implications of Rising Wage
Inequality in the United States,” NBER Working Papers 14052 June 2008. 31, 39
, , and , “Quantitative Macroeconomics with Heterogeneous Households,” Annual
Review of Economics, 2009, 1, 319–354. 56
Heaton, John and Deborah J Lucas, “Evaluating the Eﬀects of Incomplete Markets
on Risk Sharing and Asset Pricing,” Journal of Political Economy, June 1996, 104 (3),
443–87. 47
and Deborah Lucas, “Portfolio Choice and Asset Prices: The Importance of Entrepreneurial Risk,” Journal of Finance, 06 2000, 55 (3), 1163–1198. 37
Heckman, James, Lance Lochner, and Christopher Taber, “Explaining Rising Wage
Inequality: Explanations With A Dynamic General Equilibrium Model of Labor Earnings
With Heterogeneous Agents,” Review of Economic Dynamics, January 1998, 1 (1), 1–58.
39
Hornstein, Andreas, Per Krusell, and Giovanni L. Violante, “Frictional Wage Dispersion in Search Models: A Quantitative Assessment,” American Economic Review,
forthcoming. 32
64

Hubbard, R. Glenn, Jonathan Skinner, and Stephen P. Zeldes, “The Importance
of Precautionary Motives in Explaining Individual and Aggregate Saving,” CarnegieRochester Conference Series on Public Policy, 1994, 40, 59–125. 28
,
, and
, “Precautionary Saving and Social Insurance,” The Journal of Political
Economy, 1995, 103 (2), 360–399. 22, 25, 29
Huggett, Mark, “The Risk-Free Rate in Heterogeneous-Agent Incomplete-Insurance
Economies,” Journal of Economic Dynamics and Control, 1993, 17, 953–969. 4, 23,
24
, “Wealth distribution in life-cycle economies,” Journal of Monetary Economics, December
1996, 38 (3), 469–494. 33, 34
, Gustavo Ventura, and Amir Yaron, “Human Capital and Earnings Distribution
Dynamics,” Journal of Monetary Economics, 2006, 53, 265–290. 38
, , and
32, 38

, “Sources of Lifetime Inequality,” American Economic Review, forthcoming.

Imrohoroglu, Ayse, “Cost of Business Cycles with Indivisibilities and Liquidity Constraints,” Journal of Political Economy, 1989, 97 (6), 1364–1383. 4, 22
Jencks, Christopher, “The Hidden Prosperity of the 1970s,” Public Interest, 1984, 77,
37–61. 32
Jones, Larry E., Rodolfo E. Manuelli, and Ellen R. McGrattan, “Why are Married
Women Working So Much?,” Staﬀ Report 317, Federal Reserve Bank of Minneapolis
2003. 31
Jovanovic, Boyan, “Job Matching and the Theory of Turnover,” Journal of Political
Economy, October 1979, 87 (5), 972–90. 38
Judd, Kenneth L. and Sy-Ming Guu, “Asymptotic Methods for Asset Market Equilibrium Analysis,” Economic Theory, 2001, 18 (127-157.). 45
Kaplan, Greg, “Inequality and the Life Cycle,” Technical Report, University of Pennsylvania 2010. 19, 38
and Giovanni L. Violante, “How Much Consumption Insurance Beyond SelfInsurance?,” American Economic Journal: Macroeconomics, 2010, 2 (4), 53–87. 21,
22
Kehoe, Tim J and David K Levine, “Debt-Constrained Asset Markets,” Review of
Economic Studies, 1993, 60, 865–888. 32
65

Kitao, S., L. Ljungqvist, and T. Sargent, “A Life Cycle Model of Trans-Atlantic
Employment Experiences,” Working Paper, USC and NYU 2008. 39
Knowles, John, “Why Are Married Men Working So Much? The Macroeconomics of
Bargaining Between Spouses,” Technical Report, University of Pennsylvania 2007. 31
Kopczuk, Wojciech, Emmanuel Saez, and Jae Song, “Earnings Inequality and Mobility in the United States: Evidence from Social Security Data Since 1937,” Quarterly
Journal of Economics, 2010, 125 (1). 40
Kopecky, Karen A. and Richard M. H. Suen, “Finite State Markov-chain Approximations to Highly Persistent Processes,” Review of Economic Dynamics, 2010, 13, 701–714.
43
Krueger, Dirk and Fabrizio Perri, “Does Income Inequality Lead to Consumption
Inequality? Evidence and Theory,” Review of Economic Studies, 01 2006, 73 (1), 163–
193. 32, 33
and , “How Do Households Respond to Income Shocks?,” Technical Report, University
of Minnesota 2009. 27, 28
Krusell, Per and Anthony A Smith, “Income and Wealth Heterogeneity, Portfolio
Choice, and Equilibrium Asset Returns,” Macroeconomic Dynamics, 1997, 1 (1), 387–
422. 41, 42, 43, 47
and , “Income and Wealth Heterogeneity in the Macroeconomy,” Journal of Political
Economy, 1998, 106 (5), 867–896. 4, 5, 7, 22, 33, 35, 37, 41, 42, 43, 44, 46, 48
Kucherenko, Sergei and Yury Sytsko, “Application of Deterministic Low-Discrepancy
Sequences in Global Optimization,” Computational Optimization and Applications, 2005,
30, 297–318. 52
Laitner, John, “Random Earnings Diﬀerences, Lifetime Liquidity Constraints, and Altruistic Intergenerational Transfers,” Journal of Economic Theory, 1992, 58 (2), 135–170.
34, 35, 37, 41
, “Wealth Inequality and Altruistic Bequests,” American Economic Review P&P, 2002,
92 (2), 270–273. 34
Leamer, Edward, “Let’s Take the Con out of Econometrics,” American Economic Review,
1983, 73 (1), 31–43. 16
Levine, David K. and William R. Zame, “Does Market Incompleteness Matter?,”
Econometrica, 2002, 70 (5), 1805–1839. 47
66

Liberti, Leo and Sergei Kucherenko, “Comparison of Deterministic and Stochastic
Approaches to Global Optimization,” International Transactions in Operations Research,
2005, 12, 263–285. 52
Lillard, Lee A. and Robert J. Willis, “Dynamic Aspetcs of Earnings Mobility,” Econometrica, 1978, 46 (5), 985–1012. 24
and Yoram Weiss, “Components of Variation in Panel Earnings Data: American
Scientists 1960-70,” Econometrica, 1979, 47 (2), 437–454. 25, 27
Livshits, Igor, James MacGee, and Michele Tertilt, “Accounting for the Rise in
Consumer Bankruptcies,” American Economic Journal: Macroeconomics, April 2010, 2
(2), 165–93. 30
, Jim MacGee, and Michele Tertilt, “Consumer Bankruptcy – A Fresh Start,” American Economic Review, 2007, 97 (1), 402–418. 30
Lkhagvasuren, Damba and Ragchaasuren Galindev, “Discretization of Highly Persistent Correlated AR(1) Shocks,” Journal of Economic Dynamics and Control, 2010, 34,
1260–1276. 43
Lorenzoni, Guido, “A Theory of Demand Shocks,” American Economic Review, 2009, 99
(5), 2050–2084. 46
Lucas, Robert E., Models of Business Cycles, New York: Basil Blackwell, 1987. 22
, “Macroeconomic Priorities,” American Economic Review, March 2003, 93 (1), 1–14. 19,
23
Lustig, Hanno and Yi-li Chien, “The Market Price of Aggregate Risk and the Wealth
Distribution,” Review of Financial Studies, 2010, 23 (4), 1596–1650. 46
Mace, Barbara J., “Full Insurance in the Presence of Aggregate Uncertainty,” Journal of
Political Economy, 1991, 99 (5), 928–956. 15
MaCurdy, Thomas E., “The Use of Time Series Processes to Model the Error Structure
of Earnings in a Longitudinal Data Analysis,” Journal of Econometrics, January 1982,
18 (1), 83–114. 25, 26, 27
Mankiw, N. Gregory, “The equity premium and the concentration of aggregate shocks,”
Journal of Financial Economics, September 1986, 17 (1), 211–219. 47
Meghir, Costas and Luigi Pistaferri, “Income Variance Dynamics and Heterogeneity,”
Econometrica, 2004, 72 (1), 1–32. 27, 39
67

and , “Earnings, Consumption, and Life Cycle Choices,” in Orley Ashenfelter and
David Card, eds., Handbook of Labor Economics, Vol 4B, Elsevier, 2011, pp. 773 – 854.
56
Mehra, Rajnish and Edward C. Prescott, “The equity premium: A puzzle,” Journal
of Monetary Economics, March 1985, 15 (2), 145–161. 24
Mian, Atif and Amir Sufi, “House Prices, Home Equity-Based Borrowing, and the U.S.
Household Leverage Crisis,” American Economic Review, forthcoming. 28
Moﬃtt, R. and P. Gottschalk, “Trends in the covariance structure of earnings in the
United States: 1969-1987,” Institute for Research on Poverty Discussion Papers 1001-93,
University of Brown 1995. 39, 40
Moﬃtt, Robert and Peter Gottschalk, “Trends in the Transitory Variance of Male
Earnings in the U.S., 1970-2004,” Working Paper, Johns Hopkins University 2008. 40
Nelson, Julie A, “On Testing for Full Insurance Using Consumer Expenditure Survey
Data: Comment,” Journal of Political Economy, April 1994, 102 (2), 384–94. 14, 15, 16
Ozkan, Serdar, “Income Diﬀerences and Health Care Expenditures over the Life Cycle,”
Technical Report, University of Pennsylvania 2010. 29
Palumbo, Michael G., “Uncertain Medical Expenses and Precautionary Saving Near the
End of the Life Cycle,” Review of Economic Studies, 1999, 66, 395–421. 29
Pijoan-Mas, Josep, “Precautionary Savings or Working Longer Hours?,” Review of Economic Dynamics, 2006, 9 (2), 326–352. 31
Powell, Michael J.D., “The BOBYQA algorithm for bound constrained optimization
without derivatives,” Numerical Analysis Papers NA06, Department of Applied Mathematics and Theoretical Physics, Cambridge August 2009. 51
Pratt, John W., “Risk Aversion in the Small and in the Large,” Econometrica, 1964, 32
(1/2), 122–136. 8
Primiceri, Giorgio E. and Thijs van Rens, “Heterogeneous life-cycle profiles, income
risk and consumption inequality,” Journal of Monetary Economics, 2009, 56 (1), 20 – 39.
Carnegie-Rochester Conference Series on Public Policy: The Causes and Consequences
of Rising Economic Inequality April 25-26, 2008. 19, 21
Quadrini, Vincenzo, “Entrepreneurship, Saving, and Social Mobility,” Review of Economic Dynamics, 2000, 3 (1), 1–40. 33, 35

68

Rinnooy Kan, Alexander and G. T. Timmer, “Stochastic Global Optimization Methods, Part I, Clustering Methods,” Mathematic Programming, 1987, 39 (27-56). 54
and , “Stochastic Global Optimization Methods, Part II, Multilevel Methods,” Mathematic Programming, 1987, 39 (57-78). 54
Rios-Rull, Jose Victor, “Life-Cycle Economies and Aggregate Fluctuations,” Review of
Economic Studies, 1996, 63, 465–490. 4, 46
Rogerson, Richard, “Indivisible Labor, Lotteries and Equilibrium,” Journal of Monetary
Economics, January 1988, 21 (1), 3–16. 48
and Johanna Wallenius, “Micro and Macro Elasticities in a Life Cycle Model With
Taxes,” Journal of Economic Theory, 2009, 144 (6), 2277–2292. 49
, Robert Shimer, and Randall Wright, “Search-Theoretic Models of the Labor Market: A Survey,” Journal of Economic Literature, December 2005, 43 (4), 959–988. 32
Rossi-Hansberg, Esteban and Mark L. J. Wright, “Establishment Size Dynamics in
the Aggregate Economy,” American Economic Review, 2007, pp. 1639–1666. 49
Rouwenhorst, K. Geert, “Asset Pricing Implications of Equilibrium Business Cycle Models,” in “Frontier of Business Cycle Research,” Princeton University Press, 1995, chapter 10. 43
Rubinstein, Mark, “An Aggregation Theorem for Securities Markets,” Journal of Financial Economics, 1974, 1, 225–244. 4, 8, 9, 10, 47
Sabelhaus, John and Jae Song, “Earnings Volatility Across Groups and Time,” National
Tax Journal, 2009, 62 (2), 347–364. 39
and , “The Great Moderation in Micro Labor Earnings,” Journal of Monetary Economics, 2010, 57, 391–403. 39
Schechtman, Jack and Vera L. Escudero, “Some Results on an ’Income Fluctuation
Problem’,” Journal of Economic Theory, 1977, 41, 151–166. 23
Schulhofer-Wohl, Sam, “Heterogeneity and Tests of Risk Sharing,” Technical Report,
Federal Reserve Bank of Minneapolis 2011. 16
Shimer, Robert, “The Cyclicality of Hires, Separations, and Job-to-Job Transitions,”
Federal Reserve Bank of St. Louis Review, 2005, 87 (4), 493–507. 40
, “Reassessing the Ins and Outs of Unemployment,” NBER Working Papers 13421, National Bureau of Economic Research 2007. 40
69

Solon, Gary and Donggyun Shin, “Trends in Men’s Earnings Volatility: What Does
the Panel Study of Income Dynamics Show?,” Journal of Public Economics, 2011, 95,
973–982. 39, 40
Storesletten, Kjetil, Chris I. Telmer, and Amir Yaron, “Cyclical Dynamics in Idiosyncratic Labor Market Risk,” Journal of Political Economy, June 2004, 112 (3), 695–
717. 25
, Chris Telmer, and Amir Yaron, “Asset Pricing with Idiosyncratic Risk and Overlapping Generations,” Review of Economic Dynamics, October 2007, 10 (4), 519–548.
48
, Christopher I. Telmer, and Amir Yaron, “Consumption and risk sharing over the
life cycle,” Journal of Monetary Economics, April 2004, 51 (3), 609–633. 19, 38
Telmer, Chris I., “Asset Pricing Puzzles and Incomplete Markets,” Journal of Finance,
1993, 48, 1803–1832. 47
Topel, Robert H., “Specific Capital, Mobility, and Wages: Wages Rise with Job Seniority,”
March 1990, (3294). 25
Topel, Robert H and Michael P Ward, “Job Mobility and the Careers of Young Men,”
The Quarterly Journal of Economics, May 1992, 107 (2), 439–79. 25
Townsend, Robert M, “Risk and Insurance in Village India,” Econometrica, May 1994,
62 (3), 539–91. 4, 15, 16
Veracierto, Marcelo, “Plant-Level Irreversible Investment and Equilibrium Business Cycles,” Discussion Paper 115, Federal Reserve Bank of Minneapolis 1997. 46
Wang, Neng, “Optimal Consumption and Asset Allocation with Unknown Income
Growth,” Journal of Monetary Economics, 2009, 56, 524–534. 49
Zhang, Hongchao, Andrew R. Conn, and Katya Scheinberg, “A Derivative-Free
Algorithm for Least-Squares Minimization,” SIAM Journal on Optimization, 2010, 20
(6), 3555–3576. 51

70

