NBER WORKING PAPER SERIES

BAYESIAN AVERAGING, PREDICTION AND NONNESTED MODEL SELECTION
Han Hong
Bruce Preston
Working Paper 14284
http://www.nber.org/papers/w14284

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
August 2008

We thank Raffaella Giacomini, Jin Hahn, Bernard Salanie, Barbara Rossi, Frank Schorfheide and Chris
Sims for comments. We also thank the NSF and the Sloan Foundation for generous research support.
The views expressed herein are those of the author(s) and do not necessarily reflect the views of the
National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
¬© 2008 by Han Hong and Bruce Preston. All rights reserved. Short sections of text, not to exceed two
paragraphs, may be quoted without explicit permission provided that full credit, including ¬© notice,
is given to the source.

Bayesian Averaging, Prediction and Nonnested Model Selection
Han Hong and Bruce Preston
NBER Working Paper No. 14284
August 2008
JEL No. C14,C52
ABSTRACT
This paper studies the asymptotic relationship between Bayesian model averaging and post-selection
frequentist predictors in both nested and nonnested models. We derive conditions under which their
difference is of a smaller order of magnitude than the inverse of the square root of the sample size
in large samples. This result depends crucially on the relation between posterior odds and frequentist
model selection criteria. Weak conditions are given under which consistent model selection is feasible,
regardless of whether models are nested or nonnested and regardless of whether models are correctly
specified or not, in the sense that they select the best model with the least number of parameters with
probability converging to 1. Under these conditions, Bayesian posterior odds and BICs are consistent
for selecting among nested models, but are not consistent for selecting among nonnested models.

Han Hong
Stanford University
Landau Economics Building
579 Serra Mall
Stanford, CA 94305
doubleh@stanford.edu
Bruce Preston
Department of Economics
Columbia University
420 West 118th Street
New York, NY 10027
and NBER
bp2121@columbia.edu

Bayesian Averaging, Prediction and Nonnested Model Selection
Han Hong and Bruce Preston1
Previous version: January 2006
This version: August 2008

Abstract
This paper studies the asymptotic relationship between Bayesian model averaging and postselection frequentist predictors in both nested and nonnested models. We derive conditions
under which their difference is of a smaller order of magnitude than the inverse of the square
root of the sample size in large samples. This result depends crucially on the relation between
posterior odds and frequentist model selection criteria. Weak conditions are given under which
consistent model selection is feasible, regardless of whether models are nested or nonnested and
regardless of whether models are correctly specified or not, in the sense that they select the
best model with the least number of parameters with probability converging to 1. Under these
conditions, Bayesian posterior odds and BICs are consistent for selecting among nested models,
but are not consistent for selecting among nonnested models.
JEL Classification: C14; C52
Keywords: Model selection criteria, Nonnested, Posterior odds, BIC

1

Introduction

Bayesian methods are becoming increasingly popular, both as a framework of model selection and
also as a tool of forecasting ‚Äî see, among others, Fernandez-Villaverde and Rubio-Ramirez (2004a),
Schorfheide (2000), Stock and Watson (2001), Timmermann (2005), Clark and McCracken (2006)
and Wright (2003). These methods are often used to summarize statistical properties of data,
identify parameters of interest, and conduct policy evaluation. While empirical applications of
these methods are abundant, less is understood about their theoretical sampling properties. This
paper provides a starting point for understanding the relation between Bayesian forecast averaging
and frequentist model selection and prediction in a general framework that admits the possibility
of model misspecification.
1

Department of Economics, Stanford University and Department of Economics, Columbia University. We thank
Raffaella Giacomini, Jin Hahn, Bernard SalanieÃÅ, Barbara Rossi, Frank Schorfheide and Chris Sims for comments.
We also thank the NSF and the Sloan Foundation for generous research support. The usual caveat applies.

2
We study the large sample properties of Bayesian prediction and model averaging for both nested
and nonnested models. We first show that, for a single model, the difference between Bayesian and
frequentist predictors are of smaller order of magnitude than the inverse of the square root of the
sample size in large samples, regardless of the expected loss function used in forming the Bayesian
predictors. This contrasts with the difference between
and Bayesian estimators, formed using
 MLE
‚àö 
a variety of loss functions, which is of the order Op 1/ T .
For multiple models, we derive general conditions under which the Bayesian posterior odds place
asymptotically unit weight on the best model with the most parsimonious parameterization. Under
these conditions, the Bayesian average model forecast is equivalent to the frequentist post-selection
forecast up to a term that is of smaller order of magnitude than the inverse of the square root of
the sample size in large samples. These findings essentially combine Schwarz‚Äô original contribution
regarding BIC ‚Äî that it is an asymptotic approximation to posterior odds ‚Äî with the insights by
Sin and White (1996) who demonstrate the inconsistency of BIC for selecting among nonnested
models. The conditions we derive are weaker, more general, and allow for a much wider class of
models.
An immediate consequence of multiple model comparison is that Bayesian posterior odds comparison is inconsistent for selecting between nonnested models. While this procedure will select one of
the best fitting models, it does not necessarily choose the most parsimonious model with probability
converging to 1 in large samples. Consistent selection among possibly nonnested models is feasible
using nonnested model selection criteria in the spirit of Sin and White (1996).
Empirical analyses frequently find that forecasts generated from averages of a number of models
typically perform better than forecasts of any one of the underlying models ‚Äî see, for instance,
Stock and Watson (2001). Our theoretical findings suggest that this can be because the models
under consideration are close to each other and are all misspecified; so that the posterior weights are
non-degenerate among the set of models under comparison. Indeed, it is shown that for nonnested
models, posterior weights will be non-degenerate, as long as the models are sufficiently close to
each other. The case of nested models is more interesting. It turns out that when the two models
are sufficiently far from each other or sufficiently close from each other, the posterior weights will
be degenerate. However, when the two models are ‚Äújust close enough‚Äù but ‚Äúnot too close‚Äù, the
posterior weights can be non-degenerate, and, as a consequence, model averaging can outperform
each individual model.
Our results are of interest given the burgeoning use of Bayesian methods in the estimation of dynamic stochastic general equilibrium models in modern macroeconomics. See, inter alia, Fernandez-

3
Villaverde and Rubio-Ramirez (2004a), Schorfheide (2000), Smets and Wouters (2002), Lubik and
Schorfheide (2003) and Justiniano and Preston (2004) for examples of estimation in both closed and
open economy settings. These papers all appeal to posterior odds ratios as a criterion for model
selection. By giving a classical interpretation to the posterior odds ratio, the present paper intends
to provide useful information regarding the conditions under which such selection procedures ensure
consistency. The analysis contributes to understanding the practical limitations of standard model
selection procedures given a finite amount of data.
The paper proceeds as follows. Section 2 describes model assumptions and derives their implications
on the large sample behavior of the likelihood function. Section 3 demonstrates the asymptotic
equivalence between Bayesian and frequentist predictors for a single model under weak conditions.
The rest of the paper generalizes this result to multiple models. Section 4 first derives weak
conditions under which the generalized posterior odds ratio is equivalent to BIC up to a term that is
asymptotically negligible, and under which alternative model selection criteria are feasible to select
consistently between both nested and nonnested models. Section 5 makes use of the asymptotic
equivalence between posterior odds ratio and BIC to derive the relation between Bayesian model
averaging and frequentist post-selection prediction. Finally, section 6 discusses the implications for
our results for non-likelihood-based models, and section 7 concludes.

2

Model assumptions and implications

For clarity of exposition, we identify a model with the likelihood function that is being used to
estimate model parameters. All results extend to general random distance functions that satisfy
the stochastic equicontinuity assumptions stated below.
A parameter Œ≤ is often estimated by maximizing a random log-likelihood function QÃÇ (Œ≤) associated
with some model f (yt , Œ≤) that depends on observed data yt and parameterized by the vector Œ≤:
QÃÇ (Œ≤) ‚â° Q (yt , t = 1 . . . , T ; Œ≤) .
For example, under i.i.d sampling of the data, as in Vuong (1989) and Sin and White (1996), the
log-likelihood function takes the form of
QÃÇ (Œ≤) =

T
X

log f (yt ; Œ≤) ,

t=1

which minimizes the Kullback-Leibler distance between the parametric model and the data.
Under standard assumptions, the random objective function converges to a population limit when
the sample size increases without bound. It is assumed that there exists a function Q (Œ≤), uniquely

4
maximized at Œ≤0 , which is the uniform limit of the random sample analog
sup
Œ≤‚ààB

1
p
QÃÇ (Œ≤) ‚àí Q (Œ≤) ‚àí‚Üí 0.
T

Typically, the following decomposition holds for QÃÇ (Œ≤):
 
 
QÃÇ Œ≤ÃÇ = QÃÇ Œ≤ÃÇ ‚àí QÃÇ (Œ≤0 ) + QÃÇ (Œ≤0 ) ‚àí T Q (Œ≤0 ) + T Q (Œ≤0 ) .
{z
} | {z }
|
{z
} |
(Qb)

(Qa)

Under suitable regularity conditions, the following are true:
‚àö 
(Qa) = Op (1) ,
(Qb) = Op
T ,

(Qc)

(Qc) = O (T ) .

The regularity conditions under which the first equality holds are formally given below. They are
the same as those in Chernozhukov and Hong (2003). They do not require the objective function to
be smoothly differentiable, and permit complex nonlinear or simulation-based estimation methods.
In particular, conditions that require smoothness of the objective function are typically violated
in simulation-based estimation methods and in percentile-based non-smooth moment conditions.
Even for simulation-based estimation methods, it can be difficult for researchers to insure that the
simulated objective functions are smooth in model parameters.
ASSUMPTION 1 The true parameter vector Œ≤0 belongs to the interior of a compact convex
subset B of Rdim(Œ≤) .
ASSUMPTION 2 For any Œ¥ > 0, there exists  > 0, such that



1
QÃÇ (Œ≤) ‚àí QÃÇ (Œ≤0 ) ‚â§ ‚àí = 1.
lim inf P
sup
T ‚Üí‚àû
|Œ≤‚àíŒ≤0 |‚â•Œ¥ T
p

ASSUMPTION 3 There exist quantities ‚àÜT , JT , ‚Ñ¶T , where JT ‚Üí ‚àíAŒ≤ , ‚Ñ¶T = O (1),
1
d
‚àö ‚Ñ¶‚àí1/2
‚àÜT ‚àí‚Üí N (0, I) ,
T
T
such that if we write
RT (Œ≤) = QÃÇ (Œ≤) ‚àí QÃÇ (Œ≤0 ) ‚àí (Œ≤ ‚àí Œ≤0 )0 ‚àÜT +

1
(Œ≤ ‚àí Œ≤0 )0 (T JT ) (Œ≤ ‚àí Œ≤0 )
2

then it holds that for any sequence of Œ¥T ‚Üí 0
sup
|Œ≤‚àíŒ≤0 |‚â§Œ¥T

RT (Œ≤)
= op (1) .
1 + T |Œ≤ ‚àí Œ≤0 |2

5
 
THEOREM 1 Under assumptions (1), (2) and (3), QÃÇ Œ≤ÃÇ ‚àí QÃÇ (Œ≤0 ) = Op (1).
 
The asymptotic distribution of QÃÇ Œ≤ÃÇ ‚àí QÃÇ (Œ≤0 ) is also easy to derive in many situations. This
distribution is useful for model selection tests but is not directly used in model selection criteria
developed in section 4. In particular, satisfaction of the information matrix equality, ‚Ñ¶T = ‚àíAŒ≤ +
op (1), is not necessary for our discussion of model selection criteria. This is especially relevant
under potential model misspecification.
‚àö 
ASSUMPTION 4 QÃÇ (Œ≤0 ) ‚àí T Q (Œ≤0 ) = Op
T .
Assumption 4 typically follows from an application of the central limit theorem

1 
d
‚àö
QÃÇ (Œ≤0 ) ‚àí T Q (Œ≤0 ) ‚àí‚Üí N (0, Œ£Q ) ,
T

(2.1)

where

Œ£Q = lim V ar


1 
‚àö
QÃÇ (Œ≤0 ) ‚àí T Q (Œ≤0 ) .
T

For example, in the case of the log-likelihood function for i.i.d. observations, where
QÃÇ (Œ≤) =

T
X

log f (yt ; Œ≤)

and Q (Œ≤) = E log f (y; Œ≤) ,

t=1

such convergence follows immediately from the central limit theorem: Œ£ = V ar (log f (yt ; Œ≤)).
Beyond the likelihood setting, assumption 4 can in general be easily verified for extreme estimators
based on optimizating random objective functions. These include M estimator, generalized method
of moment estimators and their recent information-theoretic variants. It can be shown to hold for
generalized method of moment estimators regardless of whether the model is correctly specified or
misspecified.
The property of the final term (Qc) is immediate.

3

Bayesian predictive analysis

Consider the Bayesian inference problem of predicting yT +1 given a data set YT with observations
up to T . Typically we need to calculate the predictive density of yT +1 given YT , and for this

6
purpose need to average over the posterior distribution of the model parameters Œ≤ given the data
YT :
Z
f (yT +1 |YT ) = f (yT +1 |YT , Œ≤) f (Œ≤|YT ) dŒ≤
Z
=

f (yT +1 |YT , Œ≤) R

eQÃÇ(Œ≤) œÄ (Œ≤)
eQÃÇ(Œ≤) œÄ (Œ≤) dŒ≤

dŒ≤.


We will assume that f (yT +1 |YT ; Œ≤) = f yT +1 |YÃÑT ; Œ≤ , where YÃÑT is a finite dimensional subcomponent of Y . For example, YÃÑT can be yT , the most recent observation in YT . It is well understood

that the first-order randomness in the prediction is driven by f yT +1 |YÃÑT , Œ≤ . The length of the

prediction interval comprises two parts: the first is due to f yT +1 |YÃÑT , Œ≤ and the second is due to
the uncertainty from estimating Œ≤. While the second part will decrease to zero as the sample size
T increases, the first part remains constant.
We are interested in the second-order uncertainty in the prediction that is due to the estimation
of the parameter Œ≤, and therefore will consider a fixed value yÃÑ of the random component YÃÑT and
density
Z
f (yT +1 |yÃÑ, YT ) = f (yT +1 |yÃÑ, Œ≤) f (Œ≤|YT ) dŒ≤,
(3.2)
where yÃÑ can potentially differ from the observed realization of YÃÑT in the sample. For example, one
might consider out of sample predictions where yÃÑ does not take the realized value of YÃÑT .
Point predictions can be constructed as functionals of the posterior predictive density f (yT +1 |yÃÑ, YT ).
For example, a mean prediction can be obtained by
Z
E (yT +1 |yÃÑ, YT ) = E (yT +1 |yÃÑ, YT ; Œ≤) f (Œ≤|YT ) dŒ≤.
A median prediction is given by
 Z

med fyT +1 (¬∑|yÃÑ, Y ) = inf x :

x

f (yT +1 |yÃÑ, Y ) dyT +1


1
.
‚â•
2

Under suitable regularity conditions, Œ≤ÃÇ is an asymptotic sufficient statistic for the random posterior
distribution and f (Œ≤|YT ) is approximately normal with mean Œ≤ÃÇ and variance ‚àí T1 (AŒ≤ )‚àí1 :


1
A
f (Œ≤|YT ) ‚àº N Œ≤ÃÇ, ‚àí (AŒ≤ )‚àí1 .
T
This asymptotic approximation of the posterior distribution of Œ≤ by a normal distribution is the
key element of the results formally developed in section 4, and forms the basis of the following
asymptotic approximation of the predictive distribution.

7
 
The density f (yT +1 |yÃÑ, YT ) satisfies the approximation, up to order op ‚àö1T ,
Z
0
dim(Œ≤)
1
1
f (yT +1 |yÃÑ, Œ≤) (2œÄ)‚àí 2 det (‚àíT AŒ≤ ) 2 e‚àí 2 (Œ≤‚àíŒ≤ÃÇ ) (‚àíT AŒ≤ )(Œ≤‚àíŒ≤ÃÇ ) dŒ≤

Z 
h0 (‚àíAŒ≤ )h
dim(Œ≤)
1
h
2
= f yT +1 |yÃÑ, Œ≤ÃÇ + ‚àö
(2œÄ)‚àí 2 det (‚àíAŒ≤ ) 2 e‚àí
dh.
T
Researchers are most often interested in mean predictions and predictive intervals. Mean prediction
is convenient to analyze because of its linearity property. For instance,
Z
E (yT +1 |yÃÑ, YT ) = E (yT +1 |yÃÑ, Œ≤) f (Œ≤|YT ) dŒ≤


Z
h
= E yT +1 |yÃÑ, Œ≤ÃÇ + ‚àö
f (h|Y ) dh.
T
If E (yT +1 |yÃÑ; Œ≤) is linear in Œ≤, then it is easy to see from the fact that f (h|Y ) is approximately
normal with mean 0 and variance ‚àíAŒ≤‚àí1 that




1
.
E (yT +1 |yÃÑ, YT ) = E yT +1 |yÃÑ; Œ≤ÃÇ + op ‚àö
T
Hence Bayesian mean prediction is asymptotically equivalent up to order

‚àö1
T

with frequentist


prediction, where the predictive density is formed using the extremum estimate Œ≤ÃÇ, f yT +1 |yÃÑ, Œ≤ÃÇ .
And even if E (yT +1 |yÃÑ; Œ≤) is not linear in Œ≤, as long as it is sufficiently
smooth in Œ≤, it is still possible

h
‚àö
to use a first-order Taylor expansion of E yT +1 |yÃÑ; Œ≤ÃÇ + T around Œ≤ÃÇ to prove that the same result
holds. Given the generic notation of yT +1 , these arguments also apply without change to more
general functions of yT +1 , in the sense that for a general function t (¬∑) of yT +1 ,




1
E (t (yT +1 ) |yÃÑ, YT ) = E t (yT +1 ) |yÃÑ, Œ≤ÃÇ + op ‚àö
.
T
These results can be generalized to nonlinear predictions that can be expressed as nonlinear functions of the predictive density f (yT +1 |yÃÑ, YT ). For example, median prediction and predictive intervals can readily be constructed. In the following, we will formulate a general prediction as one
that is defined through minimizing posterior expected nonlinear and non-smooth loss functions.
A general class of nonlinear predictors ŒªÃÇ (yÃÑ, YT ) can be defined as the solution to minimizing an
expected loss function œÅ (¬∑), which is assumed to be convex:
ŒªÃÇ (yÃÑ, YT ) = arg min E (œÅ (yT +1 , Œª) |yÃÑ, YT )
Œª
Z
= arg min œÅ (yT +1 , Œª) f (yT +1 |yÃÑ, YT ) dyT +1 ,
Œª

8
where the predictive density f (yT +1 |yÃÑ, YT ) is defined in equation (3.2). For example, to construct
a one-sided œÑ th predictive interval, we can take
œÅ (yT +1 , Œª) ‚â° œÅœÑ (yT +1 ‚àí Œª) = (œÑ ‚àí 1 (yT +1 ‚â§ Œª)) (yT +1 ‚àí Œª) .
The following focuses on this loss function.
We are interested in comparing ŒªÃÇ (yÃÑ, YT ) to the nonlinear frequentist prediction, defined as
Z


ŒªÃÉ (yÃÑ, YT ) = arg min œÅ (yT +1 , Œª) f yT +1 |yÃÑ, Œ≤ÃÇ dyT +1 .
Œª

Also define the infeasible loss function, where the uncertainty from estimation of the unknown
parameters is absent, as
Z
œÅÃÑ (yÃÑ, Œª; Œ≤) = œÅ (yT +1 , Œª) f (yT +1 |yÃÑ, Œ≤) dyT +1
=E (œÅ (yT +1 , Œª) |yÃÑ, Œ≤) .
Then the Bayesian predictor and the frequentist predictor can be written as
Z
ŒªÃÇ (yÃÑ, YT ) = arg min œÅÃÑ (yÃÑ, Œª; Œ≤) f (Œ≤|YT ) dŒ≤,
Œª

and


ŒªÃÉ (yÃÑ, YT ) = arg min œÅÃÑ yÃÑ, Œª; Œ≤ÃÇ .
Œª

The next theorem establishes the asymptotic relation between ŒªÃÇ (yÃÑ, YT ) and ŒªÃÉ (yÃÑ, YT ).
THEOREM 2 Under assumptions (1), (2) and (3), and assuming that for each yÃÑ, œÅÃÑ (yÃÑ, Œª; Œ≤)
is three times continuously differentiable in Œ≤ and Œª in a small neighborhood of Œ≤0 and Œª0 ‚â°
arg minŒª œÅÃÑ (yÃÑ, Œª, Œ≤0 ), with uniformly integrable derivatives, then
 p
‚àö 
T ŒªÃÇ (yÃÑ, YT ) ‚àí ŒªÃÉ (yÃÑ, YT ) ‚àí‚Üí 0.
Note that the condition of this theorem only requires the integrated loss function œÅÃÑ (yÃÑ, Œª; Œ≤) to
be smoothly differentiable in Œª and Œ≤, and does not impose smoothness conditions directly on
œÅ (yT +1 , Œª). The results cover predictive intervals, as long as the predictive density given Œ≤ is
smoothly differentiable around the percentiles to be predicted.
Different loss functions can be used to construct a variety of Bayesian point estimators from the
posterior density. Unless the loss function is convex and symmetric around 0, the corresponding

9
Bayes estimator
 ‚àöis typically different from the frequentist maximum likelihood estimator at the
order of Op 1/ T . In contrast, we found that when different loss functions are used to define
different
Bayesian and frequentist predictors coincide with each other up to the order

 ‚àö predictors,
op 1/ T . This is probably a more relevant result concerning loss functions because researchers
are typically more interested in using loss functions to define the properties of predictions, rather
than to define the estimator itself.

4

Posterior odds and consistent model selection

Bayesian averaging is a popular method for making predictions in the context of multiple models.
The weights used in Bayesian averaging are calculated through posterior odds ratios. To generalize
the results in the previous section to multiple models, it is important to understand first the relation
between posterior odds ratio calculation and consistent frequentist model selection criteria. This is
of independent interest given the increasing use of Bayesian methods in economics; particularly the
recent macroeconomics literature on estimation of dynamic stochastic general equilibrium models
which makes use of the posterior odds ratio for model selection and prediction.
4.1

Large sample properties of Bayes factors

The posterior distribution is an integral transformation of the model (f, Q), defined as
Z

0
QÃÇ(Œ≤)
e
œÄ (Œ≤) / eQÃÇ(Œ≤ ) œÄ Œ≤ 0 dŒ≤ 0 .
Associated with the posterior distribution is the Bayes factor
Z
PQ √ó eQÃÇ(Œ≤) œÄ (Œ≤) dŒ≤,
where PQ is a prior probability weight of model (f, Q).
The following theorem connects the properties of the Bayes posterior distribution to the extremum
estimator and establishes the relation between the Bayes factor and the extremum estimator analog
of BICs. Under the regularity conditions stated in assumptions 1 to 3, the Bayes factor is asymptotically equivalent to using BIC as a model selection criterion. While the relation between Bayes
factor and BIC comes from the original contribution by Schwartz, the conditions in the following
theorem are considerably weaker and more general.
THEOREM 3 Under assumptions (1), (2) and (3), the Bayes factor satisfies the following rela-

10
tion
T

dim(Œ≤)
2

Z

p

eQÃÇ(Œ≤)‚àíQÃÇ(Œ≤ÃÇ ) œÄ (Œ≤) dŒ≤ ‚àí‚Üí œÄ (Œ≤0 ) (2œÄ)

dim(Œ≤)
2

det (‚àíAŒ≤ )‚àí1/2 .

The formal details of the proof are relegated to an appendix. When QÃÇ (Œ≤) is smoothly differentiable,
intuition can be gleaned by considering the expression:
Z
Z
 
QÃÇ(Œ≤)
log e
œÄ (Œ≤) dŒ≤ ‚àí QÃÇ Œ≤ÃÇ = log eQÃÇ(Œ≤)‚àíQÃÇ(Œ≤ÃÇ ) œÄ (Œ≤) dŒ≤.
 
It can be approximated up to an op (1) term as follows. First, QÃÇ (Œ≤) ‚àí QÃÇ Œ≤ÃÇ can be approximated
‚àö
by a quadratic function centered at Œ≤ÃÇ, in a size 1/ T neighborhood around Œ≤ÃÇ:
 
0 ‚àÇ 2 QÃÇT Œ≤ÃÇ 

  1
Œ≤ ‚àí Œ≤ÃÇ
Œ≤
‚àí
Œ≤ÃÇ
.
QÃÇ (Œ≤) ‚àí QÃÇ Œ≤ÃÇ ‚âà
2
‚àÇŒ≤‚àÇŒ≤ 0
‚àö
Second,
  pin this 1/ T size neighborhood, the prior density is approximately constant around Œ≤0 as
œÄ Œ≤ÃÇ ‚àí‚Üí œÄ (Œ≤0 ). The impact of the prior density is negligible except for the value at œÄ (Œ≤0 ). Out‚àö
side the 1/ T size neighborhood around Œ≤ÃÇ, the difference between eQÃÇ(Œ≤) and eQÃÇ(Œ≤ÃÇ ) is exponentially
small, and makes only asymptotically negligible construction to the overall integral.
The appendix proves formally the approximation:
Z
Z
 
0 ‚àÇ 2 QÃÇT (Œ≤ÃÇ )
1
Œ≤‚àíŒ≤ÃÇ ) ‚àÇŒ≤‚àÇŒ≤
QÃÇ(Œ≤)
0 (Œ≤‚àíŒ≤ÃÇ )
log e
œÄ (Œ≤) dŒ≤ ‚àí QÃÇ Œ≤ÃÇ = log œÄ (Œ≤0 ) e 2 (
dŒ≤ + op (1)
Z
0
1
= log œÄ (Œ≤0 ) e 2 (Œ≤‚àíŒ≤ÃÇ ) T AŒ≤ (Œ≤‚àíŒ≤ÃÇ ) dŒ≤ + op (1)


dim(Œ≤)
1
= log œÄ (Œ≤0 ) (2œÄ) 2 det (‚àíT AŒ≤ )‚àí 2 + op (1)
= log œÄ (Œ≤0 ) +
|

dim (Œ≤)
1
1
log (2œÄ) ‚àí det (‚àíAŒ≤ ) ‚àí dim (Œ≤) log T + op (1) ,
2
2
{z
} 2
C (AŒ≤ ,Œ≤ )

where C (AŒ≤ , Œ≤) can also depend on the prior weight on the model itself.
4.2

Consistent model selection

Now introduce a competing model, g (Y ; Œ±), Œ± ‚àà Œõ (generalizing the notations to multiple models
is immediate), where Œ± is estimated by the random log-likelihood function:
LÃÇ (Œ±) ‚â° L (yt , t = 1, . . . , T ; Œ±) .

11
Similarly to model (f, Q), we assume that the decomposition of
LÃÇ (Œ±ÃÇ) = LÃÇ (Œ±ÃÇ) ‚àí LÃÇ (Œ±0 ) + LÃÇ (Œ±0 ) ‚àí T L (Œ±0 ) + T L (Œ±0 )
{z
} |
{z
} | {z }
|
(La)

(Lb)

satisfies the stochastic order properties
(La) = Op (1) ,

(Lb) = Op

‚àö 
T ,

(Lc) = O (T ) .

We also assume that assumptions (1), (2), (3) and (4), and subsequently theorems 1, 2 and 3 hold
for model (g, L).
A conventional consistent model selection criterion takes the form of a comparison between
 
QÃÇ Œ≤ÃÇ ‚àí dim (Œ≤) ‚àó CT
and
LÃÇ (Œ±ÃÇ) ‚àí dim (Œ±) ‚àó CT ,
where CT is a sequence of constants that tends to ‚àû as T goes to ‚àû, at a rate to be prescribed
below, and dim () indexes the parametric dimension of the model. Œ≤ÃÇ and Œ±ÃÇ are maximands of the
corresponding objective functions. The second term in the model selection criteria penalizes the
dimension of the parametric model. For instance, BIC takes CT = log T and AIC adopts CT = 2.
While the discussion so far has focused on parametric likelihood models, all results can be applied
without modification to other non-likelihood-based models, including method of moment models.
They are also applicable to non-log-likelihood random criterion functions that measure other notions of distance between the model and the data generating process, such as the Cressie-Read
discrepancy statistic in generalized empirical likelihood methods.
For non-likelihood-based models, in addition to penalizing the parametric dimension of the model,
the dimension of the estimation procedure, such as the number of moment conditions could also be
penalized as in Andrews and Lu (2001). The goal of the latter penalization term is to preserve the
parsimony of the model and the informativeness of the estimation procedure, thereby improving
the precision of the model with a finite amount of data. To emphasize this possibility, we will adopt
a general notation dim (f, Q) and dim (g, L), rather than dim (Œ≤) and dim (Œ±). The first argument
refers to the parametric dimension of the model, while the second argument refers to the dimension
of the information used in the inference procedure regarding model parameters.
For the specific case of the Bayesian (Schwartz) information criteria,
 
BIC = QÃÇ Œ≤ÃÇ ‚àí LÃÇ (Œ±ÃÇ) ‚àí (dim (f, Q) ‚àí dim (g, L)) √ó log T
implicitly defining CT = log T . It now will be demonstrated that whether models
  are nested
or nonnested has important consequences for the asymptotic properties of QÃÇ Œ≤ÃÇ ‚àí LÃÇ (Œ±ÃÇ) and

12
consistency of the BIC as model selection criterion. The two cases of nested and nonnested models
are treated in turn.
4.2.1

Nested Models

There are two cases of interest: one model is better in the sense that Q (Œ≤0 ) 6= L (Œ±0 ) and both
models are equally good in the sense that Q (Œ≤0 ) = L (Œ±0 ). Consider the former. Without loss of
generality, let Q (Œ≤) be the larger nesting model and L (Œ±) be the smaller model nested in Q (Œ≤).
When Œ≤0 6= Œ±0 , it is typically the case that Q (Œ≤0 ) > L (Œ±0 ). The goal of BIC is to select the correct
model, Q (Œ≤), with probability converging to 1. In this case, this will be true because
BIC = (Qa) ‚àí (La) + (Qb) ‚àí (Lb) + T (Q (Œ≤0 ) ‚àí L (Œ±0 )) ‚àí (dim (f, Q) ‚àí dim (g, L)) √ó log T,
{z
} |
{z
} |
{z
}
|
‚àö
Op (1)
O(T )
Op ( T )
will be dominated by +T (Q (Œ≤0 ) ‚àí L (Œ±0 )), which increases to +‚àû with probability converging to
1 as T ‚Üí ‚àû. In other words, for any M > 0,
P (BIC > M ) ‚àí‚Üí 1.
Hence, BIC selects the correct model with probability converging to 1 if there is one correct model.
More generally, BIC selects one of the correct models with probability converging to 1.
Suppose now that both models are equally good, so that Q (Œ≤0 ) = L (Œ±0 ). Because we are discussing
nested models, this also means that Œ≤0 = Œ±0 (with the obvious abuse of the notation of equality
with different dimensions), and that QÃÇ (Œ≤0 ) = LÃÇ (Œ±0 ) almost surely. In the case of likelihood models,
this means that f (Zt ; Œ≤0 ) = g (Zt ; Œ±0 ) almost surely, since g (¬∑) is just a subset of f (¬∑). The true
model lies in the common subset of (Œ≤0 , Œ±0 ) and has to be the same model.
In this case, the second term is identically equal to 0:
(Qb) ‚àí (Lb) = QÃÇ (Œ≤0 ) ‚àí LÃÇ (Œ±0 ) ‚àí (T Q (Œ≤0 ) ‚àí T L (Œ±0 )) ‚â° 0.
Given that the last terms (Qc) and (Lc) disappear as a consequence of the equality Q (Œ≤0 ) = L (Œ±0 ),
the BIC comparison is reduced to
BIC = (Qa) ‚àí (La) ‚àí (dim (f, Q) ‚àí dim (g, L)) √ó log T.
{z
}
|
Op (1)

The second term, which is of order O (log T ), will dominate. So if dim (f, Q) > dim (g, L), BIC will
converge to ‚àí‚àû with probability converging to 1. In other words, for any M > 0,
P (BIC < ‚àíM ) ‚àí‚Üí 1.

13
Hence, given two equivalent models, in the sense of Q (Œ≤0 ) = L (Œ±0 ), the BIC will choose the most
parsimonious model (namely the one with the smallest dimension, either dim (f, Q) or (g, L)) with
probability converging to 1.
It is clear from the above arguments that, instead of using CT = log T , we can choose any sequence
of CT such that CT ‚àí‚Üí ‚àû and CT = o (T ).
4.2.2

Nonnested Models

Many model comparisons are performed among models that are not nested inside each other. A
leading example is the choice between a nonlinear model and its linearized version. For instance,
in an extensive literature on estimating consumption Euler equations, there has been debate on
the appropriateness of using log-linear versus non-linear Euler equations to estimate household
preference parameters. See, for instance, Carroll (2001), Paxson and Ludvigson (1999), and Attanasio and Low (forthcoming). More recently, Fernandez-Villaverde and Rubio-Ramirez (2003)
and Fernandez-Villaverde and Rubio-Ramirez (2004b) show that nonlinear filtering methods render
feasible the estimation of some classes of nonlinear dynamic stochastic general equilibrium models.
Being equipped with model selection criteria to compare multiple non-linear non-nested models is
clearly desirable.
In contrast to the case of nested models, the comparison of nonnested models imposes more stringent
requirements on CT for consistent model selection. Indeed, the further condition on CT that
‚àö
‚àö
CT / T ‚àí‚Üí ‚àû is required in addition to CT = o (T ). As an example, CT = T log T will satisfy
both requirements, but CT = log T will not satisfy the second requirement. Let‚Äôs call the model
‚àö
selection criteria N IC when we choose CT = log T T . To our knowledge these rate conditions are
first due to Sin and White (1996) in the context of smooth likelihood models.
Suppose that Q (Œ≤0 ) is greater than L (Œ±0 ), implying model (f, Œ≤) is better than model (g, Œ±).
Then, as before, N IC is dominated by T (Q (Œ≤0 ) ‚àí L (Œ±0 )), which increases to +‚àû with probability
‚àö
converging to 1. This is true regardless of whether we choose CT = log T or CT = T log T , since
both are of smaller order of magnitude than T . Hence the behavior of N IC when one model is
better than the other is essentially the same for both nested models and nonnested models.
However, when both models are equally good, so that Q (Œ≤0 ) = L (Œ±0 ), NIC comprises the nonvanishing term
(Qb) ‚àí (Lb) = QÃÇ (Œ≤0 ) ‚àí LÃÇ (Œ±0 ) ‚àí (T Q (Œ≤0 ) ‚àí T L (Œ±0 ))
‚àö
which is of order O( T ). In contrast to the nested case, it is no longer true that QÃÇ (Œ≤0 ) ‚â° LÃÇ (Œ±0 )

14
with probability one when the two models are equally good. Hence the model selection criterion
takes the form
N IC = (Qa) ‚àí (La) + (Qb) ‚àí (Lb) ‚àí (dim (f, Q) ‚àí dim (g, L)) √ó CT .
|
{z
} |
{z
}
‚àö
Op (1)
Op ( T )
‚àö
Hence, for choice of penalty function CT = log T T , or any other sequence that increases to ‚àû
‚àö
faster than T , the last term dominates. So if dim (f, Q) > dim (g, L), NIC converges to ‚àí‚àû with
probability converging to 1, or
P (N IC < ‚àíM ) ‚àí‚Üí 1

for any

M > 0.

Thus, N IC will choose the most parsimonious model among the two models with probability
converging to 1.
In contrast, if the BIC had been used, where CT = log T , then the final term fails to dominate.
The second term, which is random, might instead dominate. It is immediate that model (f, Q)
and model (g, L) will both be selected with strictly positive probabilities. Such model selection
behavior does not have a clear interpretation when a unique ranking of models is desired. While an
analyst may be content with identifying just one of the best fitting models, regardless of whether
it is the most parsimonious or not, the definition of consistency we adopt from the literature seeks
to uniquely rank models given a model selection criterion and a particular set of assumptions. As
such, positive probability weights on multiple models fails our requirements.
While the properties of AIC and BIC for selecting among nested parametric models are well understood (e.g. see Sims (2001) and Gourieroux and Monfort (1995)), the following theorem serves
to summarize the above discussion about the NIC.
THEOREM 4 Suppose that assumptions (1), (2), (3) and (4) hold for models (f, Q) and (g, L).
Then
 
‚àö
N IC = QÃÇ Œ≤ÃÇ ‚àí LÃÇ (Œ±ÃÇ) ‚àí (dim (f, Q) ‚àí dim (g, L)) √ó log T T
has the following properties for (f, Q) and (g, L) either nested or nonnested:
1. If Q (Œ≤0 ) > L (Œ±0 ) then P (N IC > M ) ‚àí‚Üí 1 for all M > 0;
2. If Q (Œ≤0 ) = L (Œ±0 ) and dim (f, Q) ‚àí dim (g, L) > 0 then P (N IC > M ) ‚àí‚Üí 1 for all M > 0.

15
4.3

Posterior odds and BIC

The posterior odds ratio between the two models is defined as
R QÃÇ(Œ≤)
œÄ (Œ≤) dŒ≤
e
PQ
√óR
log
PL
eLÃÇ(Œ±) Œ≥ (Œ±) dŒ±
when the two models have prior probability weights PQ and PL = 1 ‚àí PQ . Exploiting the approximation in section 4.1, up to a term of order op (1), the log posterior odds ratio can be written
as


 
PQ
1
1
dim (Œ≤) ‚àí dim (Œ±) log T + C (AŒ≤ , Œ≤) ‚àí C (AŒ± , Œ±) + log
QÃÇ Œ≤ÃÇ ‚àí LÃÇ (Œ±ÃÇ) ‚àí
2
2
PL
implicitly defining a penalization term CT = log T . It is immediately clear that this expression is
asymptotically equivalent, up to a constant that is asymptotically negligible, to BIC. As discussed
previously, this choice of CT gives a consistent model selection criterion only when comparing
nested models. In the nonnested case, when the null hypothesis contends that two models are
asymptotically equivalent in fit and therefore misspecified, it fails to select the most parsimonious
model with probability converging to 1.
Fernandez-Villaverde and Rubio-Ramirez (2004a) also explore the large sample properties of the
posterior odds ratio in the case of parametric likelihood estimation. They demonstrate, assuming
there exists a unique asymptotically superior model in the sense of Q (Œ≤) > L (Œ±), that the posterior odds ratio will select model (f, Q) with probability converging to 1 as T goes to infinity.
Theorem 3 serves to generalize this finding. First, the results presented here are established under
very weak regularity conditions. Second, only by considering a null hypothesis that admits the
possible equivalence of the two models, in the sense that Q (Œ≤) = L (Œ±), can a complete classical
interpretation properly be given to the posterior odds ratio as a model selection criterion. Statistically distinguishing models is fundamental to classical hypothesis testing. Given the absence of
prior information in classical estimation it is necessary to entertain the possibility that two or more
models are equally good in a testing framework. The results of this paper are therefore seen to be
couched naturally in the classical paradigm.
4.4

Multiple Models

The previous theorem extends directly to the case of multiple models. Suppose there are a total
of M models, of which k of them are equally good, in the sense of having the same limit objective
function with magnitude Q (Œ≤0 ), but the other M ‚àí k models have lower valued limit objective
functions. Then with probability converging to 1, both the BICs and the NICs for the k good

16
models will be infinitely larger than those for the M ‚àí k inferior models. In other words, with
probability converging to 1, none of the M ‚àí k inferior models will ever be selected by either BIC
or NIC comparison.
However, the behavior of BIC and NIC can be very different among the k best models depending
on whether they are nested or nonnested. If the k best models are all nested inside each other,
both BICs and NICs will select the most parsimonious model with probability converging to 1. In
contrast, if there exist two models that are not nested inside each other, then BIC will put random
weights on at least two models. But NIC will still choose the most parsimonious model among all
the k best models.

5

Bayesian model averaging and post-selection prediction

The results in the previous section can now be applied to study prediction using the average of
two or more models. It is well known that the frequentist asymptotic distribution properties of
post-selection estimation and prediction are not affected by the model selection step, as long as the
model selection criterion is consistent. Here we demonstrate conditions under which the property
of consistent model selection is possessed by the Bayesian prediction procedure (in the sense of
asymptotically placing unitary probability weight on a single model).
With two models (f, Q) and (g, L), we can write the predictive density as
f (yT +1 |yÃÑ, YT ) =

BFQ
BFL
f (yT +1 |yÃÑ, YT , Q) +
f (yT +1 |yÃÑ, YT , L)
BFQ + BFL
BFQ + BFL

where the posterior probability weights on each model are defined as
Z
Z
QÃÇ(Œ≤)
BFQ = PQ e
œÄ (Œ≤) dŒ≤ and BFL = PL eLÃÇ(Œ±) Œ≥ (Œ±) dŒ±.
In the case of likelihood models eQÃÇ(Œ≤) = f (YT |Œ≤) and eLÃÇ(Œ±) = f (YT |Œ±). In particular, note
f (YT ) = BFQ + BFL
is the marginal density of the data YT . The model specific predictive densities are respectively,
R
f (yT +1 |yÃÑ, YT , Q) =

eQÃÇ(Œ≤) œÄ (Œ≤) f (yT +1 |yÃÑ, Œ≤) dŒ≤
R
eQÃÇ(Œ≤) œÄ (Œ≤) dŒ≤

Z
=

f (yT +1 |yÃÑ, Œ≤) f (Œ≤|YT , Q) dŒ≤

17
and f (yT +1 |yÃÑ, YT , L) =
be defined by

R

f (yT +1 |yÃÑ, Œ±) f (Œ±|YT , L) dŒ±. As before, a general class of predictions can
Z

ŒªÃÇ (yÃÑ, YT ) = arg min
Œª

œÅ (yT +1 , Œª) f (yT +1 |yÃÑ, YT ) dyT +1 .

The following theorem establishes the asymptotic properties of the Bayesian predictor formed by
averaging the two models.
THEOREM 5 Suppose the assumptions stated in Theorem 2 hold for both models (f, Q) and
(g, L). Also assume that one of the following two conditions holds:
1. Q (Œ≤0 ) > L (Œ±0 ), (f, Q) and (g, L) can be either nested or nonnested.
2. Q (Œ≤0 ) = L (Œ±0 ) but (f, Q) is nested inside (L, g). In addition, dim (Œ±) ‚àí dim (Œ≤) > 1.
 p


‚àö 
Then T ŒªÃÇ (yÃÑ, YT ) ‚àí ŒªÃÉ (yÃÑ, YT ) ‚àí‚Üí 0, where ŒªÃÉ (yÃÑ, YT ) = arg minŒª œÅÃÑ yÃÑ, Œª; Œ≤ÃÇ is formed from
(f, Q).
It is clear from previous discussion that if Q (Œ≤0 ) = L (Œ±0 ) and (f, Q) and (g, L) are not nested,
then the weight on neither BFQ nor BFL will converge to 1, and the behavior of f (yT +1 |yÃÑ, YT ) will
be a random average between the two models.
These theoretical results contrast with the conventional wisdom regarding forecasting: that averages (or some combination) of available forecasting models perform better than any one model
taken in isolation. Recent studies by Stock and Watson (2001), Stock and Watson (2003) and
Wright (2003) adduce evidence consistent with this view. The former papers show that a range
of combination forecasts outperform benchmark autoregressive models when forecasting output
growth of seven industrialized economies and similarly that the predictive content of asset prices
in forecasting inflation and output growth when models are averaged improves upon univariate
benchmark models. The latter paper shows that Bayesian model averaging can improve upon the
random walk model of exchange rate forecasts. Understanding the fundamental cause of this disjunction between theoretical and empirical findings is left for future work. However, we offer the
following remarks based on our findings
5.1

Models that are locally close

For forecasts generated from averages of a number of models to perform better than forecasts of
any one of the underlying models, the Bayesian posterior weights have to be asymptotically nondegenerate among at least two of the models that are being averaged. From the previous analysis,

18
we have shown that this is possible among nonnested models when two of the models are ‚Äúequally
good‚Äù (Q (Œ≤0 ) = L (Œ±0 ).
Our analysis also suggests that among nested models, unless their dimensions are essentially the
same (dim (f, Q) = dim (g, L)), it is most likely that posterior weights will concentrate on one of
the models, either the ‚Äúbest‚Äù model or the most parsimonious model. It turns out that it is still
possible for the posterior weights to be non-degenerate among nested models when the two models
are ‚Äújust close enough‚Äù but ‚Äúnot too close‚Äù. To see this, suppose that Q (Œ≤0 ) ‚àí L (Œ±0 ) = h (T ),
where h (T ) is a function of the sample size T . Recall the decomposition for nested models:
(Qa) ‚àí (La) + T (Q (Œ≤0 ) ‚àí L (Œ±0 )) ‚àí (dim (f, Q) ‚àí dim (g, L)) √ó CT .
{z
} |
{z
}
|
Op (1)

T √óO(h(T ))

As long as T √óh (T ) is approximately the same order as CT , the penalization term will not dominate
the second term, and it is then possible for posterior weights to be non-degenerate if the last two
terms happen to approximately offset each other.

6

Generalized nonnested model selection criteria

While results in the previous sections are stated with the parametric log-likelihood function in
mind, they apply without modification to a wide class of random non-likelihood-based objective
functions, assuming direct comparison between Q (Œ≤) and L (Œ±) is interpretable. Such interpretation
is possible, for example, when both are constructed as information-theoretic empirical likelihoods
corresponding to a different set of model and moment conditions.
In the case where QÃÇ (Œ≤) takes the form of a quadratic norm, such as the GMM estimator, it can be
similarly shown that when Q (Œ≤0 ) 6= 0, or when the GMM model is misspecified, (2.1) continues to
hold. When Q (Œ≤0 ) = 0, or when the GMM model is correctly specified, QÃÇ (Œ≤0 ) ‚àí T Q (Œ≤0 ) = QÃÇ (Œ≤0 )
typically converges in distribution to the negative of the quadratic norm of a normal distribution,
a special case of which is the œá2 distribution when an optimal weighting
‚àö  matrix is being used.
T , so that the statement
Regardless, QÃÇ (Œ≤0 ) ‚àí T Q (Œ≤0 ) = Op (1) implies QÃÇ (Œ≤0 ) ‚àí T Q (Œ≤0 ) = Op
‚àö 
(Qb) = Op
T is valid in both cases.
For non-likelihood Q and L, the integral transformations
Z
Z
 0

0
QÃÇ(Œ≤)
QÃÇ(Œ≤ 0 )
0
LÃÇ(Œ±)
Œ≥ (Œ±) / eLÃÇ(Œ± ) Œ≥ Œ±0 dŒ±0 ,
e
œÄ (Œ≤) / e
œÄ Œ≤ dŒ≤ and e
can be properly interpreted as distributions. Chernozhukov and Hong (2003) show that estimation
can proceed using simulation methods from Bayesian statistics, such as Markov Chain Monte
Carlo methods, using various location measures to identify the parameters of interest. These

19
so-called Laplace-type estimators are defined analogously to Bayesian estimators but use general
statistical criterion functions in place of the parametric likelihood function. By considering integral
transformations of these statistical functions to give quasi-posterior distributions, this approach
provides a useful alternative consistent estimation method to handle intensive computation of
many classical extremum estimators such as the GMM estimators of Hansen (1982), Powell (1986)‚Äôs
censored median regression, nonlinear IV regression such as Berry, Levinsohn, and Pakes (1995)
and instrumental quantile regression as in Chernozhukov and Hansen (2005).
The following provides two specific examples of generalized posterior odds ratios constructed from
non-likelihood random distance functions. They implicitly deliver a penalization term for the
dimension of the information used in the estimation procedure, though all penalize the parametric
dimension of the model.
It is worth underscoring that while penalization for parameterization is a natural choice for parametric likelihood models, in general it is not obvious this is necessarily the most desirable form of
penalty function outside the likelihood framework. In the context of generalized Bayesian inference,
there may be grounds to consider alternative penalty functions that also penalize the dimension
of the estimation procedure. For instance, Andrews and Lu (2001) and Hong, Preston, and Shum
(2003) consider such penalty functions that involve both the number of parameters and the number
of moment conditions. The use of additional moments in GMM and GEL contexts is desirable on
efficiency grounds.
Andrews and Lu (2001) proposed consistent model and moment selection criteria for GMM estimation. Interestingly, such selection criteria, which award the addition of moment conditions
and penalize the addition of parameters, can not be achieved using a generalized Bayes factor
constructed from the GMM objective function:
QÃÇ (Œ≤) = ‚àíT gT (Œ≤)0 WT gT (Œ≤) ,

where

gT (Œ≤) =

T
1X
m (yt , Œ≤) ,
T
t=1

p

for m (yt , Œ≤) a vector of moment conditions and WT ‚àí‚Üí W positive definite.
With the objective function QÃÇ (Œ≤) and associated prior PQ , the volume
Z
1
PQ eQÃÇ(Œ≤) œÄ (Œ≤) dŒ≤ = eQÃÇ(Œ≤ÃÇ ) eC (AŒ≤ ,Œ≤ ) T ‚àí 2 dim(Œ≤) √ó eop (1) ,
only shrinks at a rate related to the number of parameters and not the number of moment conditions. Generalized Bayes factors using the quadratic GMM objective functions do not encompass
the model selection criteria proposed by Andrews and Lu (2001). This is not surprising given the

20
lack of likelihood interpretation of conventional two-step GMM estimators based on a quadratic
norm of the moments.
The recent literature on generalized empirical likelihood (GEL) estimators proposes an informationtheoretic alternative to efficient two-step method of moment estimators that minimizes a likelihood
distance between the data and the model moments. A GEL estimator is defined as the saddle point
of a GEL function,


Œ≤ÃÇ, ŒªÃÇ = arg max arg min QÃÇ (Œ≤, Œª) .
Œ≤‚ààB

Œª‚ààŒõ

For example, in the case of exponential tilting
QÃÇ (Œ≤, Œª) =

T
X


œÅ Œª0 m (yt , Œ≤)

where

œÅ (x) = ex .

t=1

Given the connection between GEL and parametric likelihood models, it is interesting to ask
whether one can define a generalized Bayes factor based on the GELs that mimics the model
and moment selection criteria of Andrews and Lu (2001) and others. We define such a GEL Bayes
factor as
Z
1
œÄ (Œ≤) dŒ≤
GELBF =
R
e‚àíQÃÇ(Œ≤,Œª) œÜ (Œª) dŒª
where œÜ (Œª) is a prior density on the lagrange multiplier Œª. Intuitively, a large volume of the integral
Z
e‚àíQÃÇ(Œ≤,Œª) œÜ (Œª) dŒª
indicates that Œª tends to be large, and therefore that the GMM model (f, Q) is more likely to
be incorrect, or misspecified. Hence, we use its inverse to indicate the strength of the moments
involved in the GMM model. The asymptotic equivalence between this GEL Bayes factor and a
model and moment selection criteria is given in the following proposition.
Proposition 1 Suppose assumptions (1), (2) and (3) hold,
 assume also that there are interior
points in the parameter space Œª0 and Œ≤0 such that T QÃÇ Œ≤ÃÇ, ŒªÃÇ ‚àí Q (Œ≤0 , Œª0 ) = Op (1), where Q (Œ≤, Œª)
is the uniform probably limit of QÃÇ (Œ≤, Œª) /T . Then

 1
log GELBF = QÃÇ Œ≤ÃÇ, ŒªÃÇ + (dim (Œª) ‚àí dim (Œ≤)) log T + Op (1) .
2
As a consequence of this proposition, asymptotically the GEL Bayes factor puts all weight on models
with the best fit Q (Œ≤0 , Œª0 ), and among models with equal best fit, the one with the largest number of
over-identifying moment conditions dim (Œª) ‚àí dim (Œ≤). Model selection behaviors are undetermined
among models with equal fit and equal number of overidentifying moment conditions.

21

7

Conclusion

This paper exploits the connections between Bayesian and classical predictions. For predictions
based on minimization of a general class of nonlinear loss functions, we demonstrated conditions
under which the asymptotic distribution properties of prediction intervals are not affected by model
averaging and the posterior odds place asymptotically unitary probability weight on a single model.
This establishes an analogue to the well-known classical result: that asymptotical distribution
properties of post-selection estimation and prediction are not affected by first-stage model selection
so long as the model selection criterion is consistent.
Of course, when confronted with multiple misspecified models, it is clear that there is no unique
approach to model selection. While the number of parameters might appropriately capture model
parsimony in the nested case, this may not necessarily be true in the nonnested case. There may
be better ways to capture model complexity in this instance. Exploring possible criteria is left for
future work.
This paper should not be interpreted as claiming that model selection criteria can be used to the
exclusion of standard measures of model fit. For example, there are a number of single model
specification tests available ‚Äî see, among others, Newey (1985), Fan (1994) and Hansen (1982) ‚Äî
which should be applied in assessing model fit. Even the best model chosen by model comparison
methods may be rejected by such specification tests indicating that all models are far from the
true data generating process (see Sims (2003)). Indeed, as emphasized by Gourieroux and Monfort
(1995), hypothesis testing procedures may lead to the simultaneous acceptance or rejection of two
nonnested hypothesis. The former may reflect a lack a data while the latter may suggest the testing
framework is misspecified.
It is also worth noting Bayesian inference often advocates the use of more general classes of loss
function for model evaluation ‚Äì see Schorfheide (2000) for a recent discussion and promotion of such
an approach. For instance, one potential criticism of ranking models based only on statistical fit and
parameter parsimony, is that such criterion could well give rise to perverse policy recommendations
if the selected model fails to capture important components for the transmission mechanism in the
case of monetary policy. However, this will in general be true for any proposed criterion for ranking
models, whether decision theoretic or purely statistical, and only serves to emphasize that analysis
of this kind is never meant to supersede sound economic reasoning.
What this paper has attempted to treat carefully is whether models can be statistically distinguished, given a plausible set of models, finite sample and a particular set of assumptions. It
remains as further work to analyze the statistical properties of general classes of decision theoretic

22
approaches to model selection and whether said approaches resolve the inconsistency of posterior
odds ratios under our assumptions.

References
Andrews, D. (1994): ‚ÄúEmpirical Process Methods in Econometrics,‚Äù in Handbook of Econometrics, Vol.
4, ed. by R. Engle, and D. McFadden, pp. 2248‚Äì2292. North Holland.
Andrews, D., and B. Lu (2001): ‚ÄúConsistent model and moment selection procedures for GMM estimation
with application to dynamic panel data models,‚Äù Journal of Econometrics, 101, 123‚Äì164.
Attanasio, O., and H. Low (forthcoming): ‚ÄúEstimating Euler Equations,‚Äù Review of Economic Dynamics.
Berry, S., J. Levinsohn, and A. Pakes (1995): ‚ÄúAutomobile Prices in Market Equilibrium,‚Äù Econometrica, 63, 841‚Äì890.
Carroll, C. D. (2001): ‚ÄúDeath to the Log-Linearized Consumption Euler Equation! (And Very Poor
Health to the Second-Order Approximation),‚Äù Advances in Macroeconomics.
Chernozhukov, V., and C. Hansen (2005): ‚ÄúAn IV Model of Quantile Treatment Effects,‚Äù Econometrica,
73(1), 245‚Äì261.
Chernozhukov, V., and H. Hong (2003): ‚ÄúA MCMC Approach to Classical Estimation,‚Äù Journal of
Econometrics, 115(2), 293‚Äì346.
Clark, T. E., and M. W. McCracken (2006): ‚ÄúCombining Forecasts from Nested Models,‚Äù working
paper, Federal Reserve Bank of Kansas.
Fan, Y. (1994): ‚ÄúTesting the Goodness of Fit of a Parametric Density Function by Kernel Me thod,‚Äù
Econometric Theory, 10, 316‚Äì356.
Fernandez-Villaverde, J., and J. F. Rubio-Ramirez (2003): ‚ÄúEstimating Nonlinear Dynamic Equilibrium Economies: Linear versus Nonlinear Likelihood,‚Äù unpublished, University of Pennsylvannia.
(2004a): ‚ÄúComparing Dynamic Equilibrium Models to Data: A Bayesian Approach,‚Äù Journal of
Econometrics, 123, 153‚Äì187.
(2004b): ‚ÄúEstimating Nonlinear Dynamic Equilibrium Economies: A Likelihood Approach,‚Äù University of Pennsylvannia, PIER Working Paper 04-001.
Gourieroux, C., and A. Monfort (1995): Statistics and Econometric Models. Cambridge University
Press, Cambridge, UK.
Hansen, L. (1982): ‚ÄúLarge Sample Properties of Generalized Method of Moments Estimators,‚Äù Econometrica, 50(4), 1029‚Äì1054.

23
Hong, H., B. Preston, and M. Shum (2003): ‚ÄúGeneralized Empirical Likelihood-Based Model Selection
Criteria for Moment Condition Models,‚Äù Econometric Theory, 19, 923‚Äì943.
Justiniano, A., and B. Preston (2004): ‚ÄúNew Open Economy Macroeconomics amd Imperfect Passthrough: An Emprical Analysis,‚Äù unpublished, Columbia University and International Monetary Fund.
Lubik, T. A., and F. Schorfheide (2003): ‚ÄúDo Central Banks Respond to Exchange Rate Movements?
A Structural Investigation,‚Äù unpublished, Johns Hopkins University and University of Pennsylvania.
Newey, W. (1985): ‚ÄúMaximum Likelihood Specification Testing and Conditional Moment Tests,‚Äù Econometrica, pp. 1047‚Äì1070.
Newey, W., and D. McFadden (1994): ‚ÄúLarge Sample Estimation and Hypothesis Testing,‚Äù in Handbook
of Econometrics, Vol. 4, ed. by R. Engle, and D. McFadden, pp. 2113‚Äì2241. North Holland.
Pakes, A., and D. Pollard (1989): ‚ÄúSimulation and the Asymptotics of Optimization Estimators,‚Äù
Econometrica, 57(5), 1027‚Äì1057.
Paxson, C. H., and S. Ludvigson (1999): ‚ÄúApproximation Bias in Euler Equation Estimation,‚Äù NBER
Working Paper No. T0236.
Pollard, D. (1991): ‚ÄúAsymptotics for Least Absolute Deviation Regression Estimator,‚Äù Econometric
Theory, 7, 186‚Äì199.
Powell, J. L. (1986): ‚ÄúCensored Regression Quantiles,‚Äù Journal of Econometrics, 32, 143‚Äì155.
Schorfheide, F. (2000): ‚ÄúLoss Function-Based Evaluation of DSGE Models,‚Äù Journal of Applied Econometrics, 15, 645‚Äì670.
Sims, C. (2001): ‚ÄúTime Series Regression, Schwartz Criterion,‚Äù Lecture Note, Princeton University.
(2003): ‚ÄúProbability Models for Monetary Policy Decisions,‚Äù unpublished, Princeton University.
Sin, C. Y., and H. White (1996): ‚ÄúInformation Criteria for Selecting possibly misspecified parametric
models,‚Äù Journal of Econometrics, 71, 207‚Äì225.
Smets, F., and R. Wouters (2002): ‚ÄúAn Estimated Dynamics Stochastic General Equilibrium Model of
the Economy,‚Äù National Bank of Belgium, Working Paper No. 35.
Stock, J. H., and M. W. Watson (2001): ‚ÄúForecasting Output and Inflation: The Role of Asset Prices,‚Äù
NBER Working Paper 8180.
(2003): ‚ÄúCombination Forecasts of Output Growth in a Seven-country Data Set,‚Äù unpublished,
Princeton University.
Timmermann, A. (2005): ‚ÄúForecast Combinations,‚Äù in Handbook of Economic Forecasting, ed. by C. W. G.
Graham Elliott, and A. Timmermann. North Holland.

24
Vuong, Q. (1989): ‚ÄúLikelihood-ratio tests for model selection and non-nested hypotheses,‚Äù Econometrica,
pp. 307‚Äì333.
Wright, J. H. (2003): ‚ÄúBayesian Model Averaging and Exchange Rate Forecasts,‚Äù Board of Governers of
the Federal Reserve System, International Finance Discussion Papers, No. 779.

A

Proof of Theorem 1

It has been shown (e.g. Pakes and Pollard (1989), Newey and McFadden (1994) and Andrews
(1994)) that under assumptions 1, 2 and 3,

‚àö 
‚àÜT
T Œ≤ÃÇ ‚àí Œ≤0 = ‚àíA‚àí1
Œ≤ ‚àö + op (1) .
T
Combined with assumption 3, this implies that
 
0 ‚àÜ
0

‚àö 
‚àö 
1‚àö 
T
QÃÇ Œ≤ÃÇ ‚àí QÃÇ (Œ≤0 ) = T Œ≤ÃÇ ‚àí Œ≤0 ‚àö ‚àí
T Œ≤ÃÇ ‚àí Œ≤0 (JT ) T Œ≤ÃÇ ‚àí Œ≤0 + op (1) ,
2
T
 
because of RT Œ≤ÃÇ = op (1), we can write
 
‚àÜ
1 ‚àÜT 0
‚àö T + op (1) .
QÃÇ Œ≤ÃÇ ‚àí QÃÇ (Œ≤0 ) = ‚àí ‚àö A‚àí1
Œ≤
2 T
T
The conclusion follows from the assumptions that


B

‚àÜT
‚àö
T

= Op (1) and that ‚àíAŒ≤ is positive definite.

Proof of Theorem 3


‚àö 
‚àÜ
,
and
define
h
=
T
Define Œ≤ÃÉT = Œ≤0 ‚àí T1 A‚àí1
Œ≤
‚àí
Œ≤ÃÉ
. Then through a change of variables, we
T
T
Œ≤
can write

‚Äú
‚Äù 
Z
Z
dim(Œ≤)
h
QÃÇ ‚àöh +Œ≤ÃÉT
QÃÇ(Œ≤)
T
2
T
e
œÄ (Œ≤) dŒ≤ = e
œÄ ‚àö + Œ≤ÃÉT dh.
T
Chernozhukov and Hong (2003) has shown that (equation A5 of p326) under the same set of
assumptions:


‚Äú
‚Äù
Z
1
h
QÃÇ ‚àöh +Œ≤ÃÉT ‚àíQÃÇ(Œ≤0 )+ 2T
‚àÜ0T A‚àí1
Œ≤ ‚àÜT
T
e
œÄ ‚àö + Œ≤ÃÉT dh
T
p

‚àí‚Üí œÄ (Œ≤0 ) (2œÄ)

dim(Œ≤)
2

det (‚àíAŒ≤ )‚àí1/2 .

25
Therefore the proof for theorem 3 will be completed if one can show that

  
1 0 ‚àí1
p
QÃÇ Œ≤ÃÇ ‚àí QÃÇ (Œ≤0 ) ‚àí
‚àÜT AŒ≤ ‚àÜT ‚àí‚Üí 0,
2T
where Œ≤ÃÇ is the conventional M estimator, defined as (see Pakes and Pollard (1989)),
 


QÃÇ Œ≤ÃÇ = inf QÃÇ (Œ≤) + op T ‚àí1/2 .
Œ≤

This is indeed the conclusion of Theorem 3.

C



Proof of Theorem 2

Define œàÃÇ (yÃÑ, YT ) =


‚àö
‚àö 
T ŒªÃÇ (yÃÑ, YT ) ‚àí ŒªÃÉ (yÃÑ, YT ) so that ŒªÃÇ (yÃÑ, YT ) = ŒªÃÉ (yÃÑ, YT ) + œàÃÇ (yÃÑ, YT ) / T . By

definition, œàÃÇ (yÃÑ, YT ) minimizes the following loss function with respect to œà,

Z 
œà
œÅÃÑ yÃÑ, ŒªÃÉ (yÃÑ, YT ) + ‚àö ; Œ≤ f (Œ≤|YT ) dŒ≤.
T

‚àö 
Also define h ‚â° T Œ≤ ‚àí Œ≤ÃÇ as the localized parameter space around Œ≤ÃÇ. The implied density for
localized parameter h is given by




1 dim(Œ≤)
h
Œæ (h) = ‚àö
f Œ≤ÃÇ + ‚àö |YT .
T
T
Then œàÃÇ (yÃÑ, YT ) also minimizes the equivalent loss function of

Z 
œà
h
QT (œà) = œÅÃÑ yÃÑ, ŒªÃÉ + ‚àö ; Œ≤ÃÇ + ‚àö
Œæ (h) dh
T
T
where we are using the shorthand notations ŒªÃÇ = ŒªÃÇ (yÃÑ, YT ) and ŒªÃÉ = ŒªÃÉ (yÃÑ, YT ). For a given œà, we are
interested in the asymptotic behavior of QT (œà) as T ‚Üí ‚àû. Define

 Z 



œà
h
QÃÑT (œà) = œÅÃÑ yÃÑ, ŒªÃÉ + ‚àö ; Œ≤ÃÇ + œÅÃÑ yÃÑ, ŒªÃÉ; Œ≤ÃÇ + ‚àö
Œæ (h) dh ‚àí œÅÃÑ yÃÑ, ŒªÃÉ; Œ≤ÃÇ .
(C.3)
T
T
Essentially, QÃÑT (œà) is a first order approximation to QT (œà). Under the assumptions stated in
Theorem 2, it can be shown that for each œà, 2

 p
T QT (œà) ‚àí QÃÑT (œà) ‚àí‚Üí 0.
2

‚Äú
‚Äù
As T ‚Üí ‚àû, Œæ (h) converges in a strong total variation norm in probability to œÜ h; ‚àíA‚àí1
, the multivariate
Œ≤

normal density with mean 0 and variance ‚àíA‚àí1
Œ≤ . In fact, the proof of Theorem 3 shows that
Z
hŒ± Œæ (h) dh = Op (1) ,
`
¬¥
for all Œ± ‚â• 0. This, combined with the stated assumption that the differentiability of œÅÃÑ YÃÑ , Œª; Œ≤ with respect to Œª
and Œ≤, implies the stated convergence in probability.

26
Because QT (œà) and QÃÑT (œà) are both convex in œà, and since QÃÑT (œà) is uniquely minimized at œà ‚â° 0,
the convexity lemma (e.g. Pollard (1991)) is used to deliver the desired result that
 p
‚àö 
œàÃÇ = T ŒªÃÇ (yÃÑ, YT ) ‚àí ŒªÃÉ (yÃÑ, YT ) ‚àí‚Üí 0.
An alternative proof can be based on a standard Taylor expansion of the first order conditions that
define ŒªÃÇ (yÃÑ, YT ) and ŒªÃÉ (yÃÑ, YT ). This is straightforward but the notations will be more complicated.
End of proof of theorem 2.


D

Proof of Theorem 5

It is clear from Theorem 3 and its following discussions that under either one of the stated conditions,
wQ ‚â°

BFQ
p
‚Üí1
BFQ + BFL

as

T ‚Üí ‚àû.

It is because from Theorem 3, for constants
CQ = PQ œÄ (Œ≤0 ) (2œÄ)dim(Œ≤)/2 det (‚àíAŒ≤ )‚àí1/2
and
CL = PL œÄ (Œ±0 ) (2œÄ)dim(Œ±)/2 det (‚àíAŒ± )‚àí1/2
we can write
1 ‚àí wQ =

CL eLÃÇ(Œ±ÃÇ) T ‚àí dim(Œ±)/2 (1 + op (1))
CQ eQÃÇ(Œ≤ÃÇ ) T ‚àí dim(Œ≤)/2 (1 + op (1)) + CL eLÃÇ(Œ±ÃÇ) T ‚àí dim(Œ±)/2 (1 + op (1))
dŒ≤ ‚àídŒ±

CL LÃÇ(Œ±ÃÇ)‚àíQÃÇ(Œ≤ÃÇ )
(1 + op (1)) C
e
T 2
Q

.
=
dŒ≤ ‚àídŒ±
CL LÃÇ(Œ±ÃÇ)‚àíQÃÇ(Œ≤ÃÇ )
(1 + op (1)) 1 + CQ e
T 2
p

While wQ ‚Üí 1 under the stated conditions, the specific rate of convergence depends on the specific
condition stated in Theorem 3. Under condition 1, It is clear that ‚àÉ Œ¥ > 0 such that with probability
converging to 1, for all T large enough,
1 ‚àí wQ < e‚àíT Œ¥ .

(D.4)

27
 
On the other hand, when condition 2 holds, we know that LÃÇ (Œ±ÃÇ) ‚àí QÃÇ Œ≤ÃÇ converges to a quadratic
norm of a normal distribution. It can then be shown that
T

dŒ± ‚àídŒ≤
2

d

(1 ‚àí wQ ) ‚àí‚Üí


CL
exp œáÃÑ2 ,
CQ

(D.5)

where œáÃÑ2 is typically distributed as the quadratic form of a random vector such as described in
Vuong (1989).
Using the definition of œÅÃÑ (yÃÑ, Œª; Œ≤), and define œÅÃÑ (yÃÑ, Œª; Œ±) similarly, we can write ŒªÃÇ (yÃÑ, YT ) as the
minimizer with respect to Œª of
Z
Z
wQ œÅÃÑ (yÃÑ, Œª; Œ≤) f (Œ≤|YT , Q) dŒ≤ + (1 ‚àí wQ ) œÅÃÑ (yÃÑ, Œª; Œ±) f (Œ±|YT , L) dŒ±.

As before, define œàÃÇ (yÃÑ, YT ) =



‚àö 
‚àö 
T ŒªÃÇ (yÃÑ, YT ) ‚àí ŒªÃÉ (yÃÑ, YT ) and define h ‚â° T Œ≤ ‚àí Œ≤ÃÇ .

Then

œàÃÇ (yÃÑ, YT ) equivalently minimizes, with respect to œà,
QT (œà) =wQ Q1T (œà) + (1 ‚àí wQ ) Q2T (œà)
where, with ŒªÃÉ ‚â° ŒªÃÉ (yÃÑ, YT ),
Q1T (œà) =

Z



œà
h
œÅÃÑ yÃÑ, ŒªÃÉ + ‚àö ; Œ≤ÃÇ + ‚àö
Œæ (h) dh,
T
T

Q2T

Z


œà
œÅÃÑ yÃÑ, ŒªÃÉ + ‚àö ; Œ± f (Œ±|YT , L) dŒ±.
T

and
(œà) =



Now recall the definition of QÃÑT (œà) in equation (C.3) in the proof of theorem 2. Also define


QÃÉT (œà) = wQ QÃÑT (œà) + (1 ‚àí wQ ) œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ .
We are going to show that with this definition

 p
T QT (œà) ‚àí QÃÉT (œà) ‚àí‚Üí 0.

(D.6)

If (D.6) holds, it then follows again from the convexity lemma of Pollard (1991) and the fact that
QÃÉT (œà) is uniquely optimized at œà = 0 that
 p
‚àö 
œàÃÇ (yÃÑ, YT ) = T ŒªÃÇ (yÃÑ, YT ) ‚àí ŒªÃÉ (yÃÑ, YT ) ‚àí‚Üí 0.

28
Finally, we will verify (D.6). With the definition of QÃÉT (œà), we can write






T QT (œà) ‚àí QÃÉT (œà) = T wQ Q1T (œà) ‚àí QÃÑT (œà) + T (1 ‚àí wQ ) Q2T (œà) ‚àí œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ .
 p
p
Because wQ ‚Üí 1, it follows from the proof of Theorem 2 that wQ T Q1T (œà) ‚àí QÃÑT (œà) ‚Üí 0. As the
sample size increases, f (Œ±|YT , L) tends to concentrate on Œ±ÃÇ, therefore it can also be shown that

 p
Q2T (œà) ‚àí œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ ‚Üí 0.
Now if either condition 1 holds or if condition 2 holds and dŒ± ‚àí dŒ≤ > 2, then because of (D.4) and
p
(D.5), T (1 ‚àí wQ ) ‚àí‚Üí 0 and the second term vanishes in probability. Finally, in the last case where
dŒ± = dŒ≤ + 2 under condition 2, we know from equation (D.5) that

CL
exp œáÃÑ2 = Op (1) .
(D.7)
CQ



Hence it is also true that T (1 ‚àí wQ ) Q2T (œà) ‚àí œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ = op (1). Therefore (D.6) holds.

d

T (1 ‚àí wQ ) ‚àí‚Üí

Remark: It also follows from the same arguments as above that the results of Theorem 5 does not
hold when dŒ± = dŒ≤ + 1. In fact, in this case, we can redefine
 

 œà 
‚àÇ 
QÃÉT (œà) = wQ QÃÑT (œà) + (1 ‚àí wQ ) œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ +
œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ ‚àö
.
‚àÇŒª
T
We can then follow the same logic as before to show that

 p
T QT (œà) ‚àí QÃÉT (œà) ‚àí‚Üí 0.
Note that in the definition of
(C.3) of Theorem 2, using a second order Taylor

 QÃÑT (œà) in equation
œà
expansion we can replace œÅÃÑ yÃÑ, ŒªÃÉ + ‚àöT ; Œ≤ÃÇ by


1 01
œà œÅÃÑŒªŒª yÃÑ, ŒªÃÉ; Œ≤ÃÇ œà + op
T 2

 
1
.
T

As such we can write

Z 





h
‚àö
œÅÃÑ yÃÑ, ŒªÃÉ; Œ≤ÃÇ +
T QÃÉT (œà) ‚àí wQ
Œæ (h) dh ‚àí œÅÃÑ yÃÑ, ŒªÃÉ; Œ≤ÃÇ
‚àí (1 ‚àí wQ ) œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ
T



‚àö
1
‚àÇ 
= œà 0 œÅÃÑŒªŒª ŒªÃÉ; Œ≤ÃÇ œà + T (1 ‚àí wQ )
œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ œà + op (1) .
2
‚àÇŒª

 p
‚àÇ
‚àÇ
It follows from both (D.7) and the convergence of ‚àÇŒª
œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ ‚àí‚Üí ‚àÇŒª
œÅÃÑ (yÃÑ, Œª0 ; Œ±0 ) that
‚àö

T (1 ‚àí wQ )


 ‚àÇ 0
‚àÇ 
d CL
œÅÃÑ yÃÑ, ŒªÃÉ; Œ±ÃÇ ‚àí‚Üí
exp œáÃÑ2
œÅÃÑ (yÃÑ, Œª0 ; Œ±0 )0
‚àÇŒª
CQ
‚àÇŒª

29
where Œª0 = arg minŒª œÅÃÑ (yÃÑ, Œª; Œ≤0 ). Hence again with convexity arguments for uniform convergence
we can show that
 ‚àÇ
CL
1
d
exp œáÃÑ2
œÅÃÑ (yÃÑ, Œª0 ; Œ±0 )0 œà
œàÃÇ (yÃÑ, YT ) ‚àí‚Üí arg min œà 0 œÅÃÑŒªŒª (Œª0 ; Œ≤0 ) œà +
œà 2
CQ
‚àÇŒª
 ‚àÇ
CL
= ‚àí œÅÃÑŒªŒª (Œª0 ; Œ≤0 )‚àí1
exp œáÃÑ2
œÅÃÑ (yÃÑ, Œª0 ; Œ±0 )0 .
CQ
‚àÇŒª

‚àö 
In other words, if dŒ≤ = dŒ± ‚àí 1, T ŒªÃÇ (yÃÑ, YT ) ‚àí ŒªÃÉ (yÃÑ, YT ) converges in distribution to a non
degenerate random variable.

