                                NBER WORKING PAPER SERIES




      IDENTIFICATION AND INFERENCE WITH MANY INVALID INSTRUMENTS

                                           Michal KolesÃ¡r
                                             Raj Chetty
                                          John N. Friedman
                                          Edward L. Glaeser
                                          Guido W. Imbens

                                        Working Paper 17519
                                http://www.nber.org/papers/w17519


                      NATIONAL BUREAU OF ECONOMIC RESEARCH
                               1050 Massachusetts Avenue
                                 Cambridge, MA 02138
                                     October 2011




We thank the National Science Foundation for financial support. The views expressed herein are those
of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.

NBER working papers are circulated for discussion and comment purposes. They have not been peer-
reviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.

Â© 2011 by Michal KolesÃ¡r, Raj Chetty, John N. Friedman, Edward L. Glaeser, and Guido W. Imbens.
All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit
permission provided that full credit, including Â© notice, is given to the source.
Identification and Inference with Many Invalid Instruments
Michal KolesÃ¡r, Raj Chetty, John N. Friedman, Edward L. Glaeser, and Guido W. Imbens
NBER Working Paper No. 17519
October 2011
JEL No. C01,C2,C26,C36

                                              ABSTRACT

We analyze linear models with a single endogenous regressor in the presence of many instrumental
variables. We weaken a key assumption typically made in this literature by allowing all the instruments
to have direct effects on the outcome. We consider restrictions on these direct effects that allow for
point identification of the effect of interest. The setup leads to new insights concerning the properties
of conventional estimators, novel identification strategies, and new estimators to exploit those strategies.
A key assumption underlying the main identification strategy is that the product of the direct effects
of the instruments on the outcome and the effects of the instruments on the endogenous regressor has
expectation zero. We argue in the context of two specific examples with a group structure that this
assumption has substantive content.


Michal KolesÃ¡r                                       Edward L. Glaeser
Department of Economics                              Department of Economics
Harvard University                                   315A Littauer Center
1805 Cambridge St.                                   Harvard University
Cambridge, MA 02138                                  Cambridge, MA 02138
kolesarmi@gmail.com                                  and NBER
                                                     eglaeser@harvard.edu
Raj Chetty
Department of Economics                              Guido W. Imbens
Harvard University                                   Department of Economics
1805 Cambridge St.                                   Littauer Center
Cambridge, MA 02138                                  Harvard University
and NBER                                             1805 Cambridge Street
chetty@fas.harvard.edu                               Cambridge, MA 02138
                                                     and NBER
John N. Friedman                                     imbens@fas.harvard.edu
Harvard Kennedy School
Taubman 356
79 JFK St.
Cambridge, MA 02138
and NBER
john_friedman@harvard.edu
1    Introduction
A key condition underlying identification of the causal effect in instrumental variable models
is the assumption that the instruments only affect the outcome of interest through their
effect on the endogenous variable. However, in many empirical applications, there is a
concern that instruments may also affect the outcome directly. To address this concern, this
paper establishes conditions under which the effects of interest are identified in settings with
direct effects of instruments on the outcome. Following Kunitomo (1980), Morimune (1983),
Bekker (1994), Hahn (2002), Chamberlain and Imbens (2004), Chao and Swanson (2005),
Hansen, Hausman and Newey (2008), Chioda and Jansson (2009), Anderson, Kunitomo and
Matsushita (2010), and others, we focus on the case with many instruments where each
individual instrument is weak in the Staiger and Stock (1997) sense but collectively the
instruments have substantial predictive power.
    In the absence of direct effects of the instruments the limited-information-maximum-
likelihood (liml) estimator is consistent (Bekker, 1994) and efficient (Chioda and Jansson,
2009; Anderson et al., 2010) under the Bekker many-instrument asymptotic sequence given
homoscedasticity. The two-stage-least-squares (tsls) estimator is inconsistent (Kunitomo,
1980; Bekker, 1994), but a bias-corrected version, known as the bias-corrected-two-stage-
least-squares (btsls) (Donald and Newey, 2001), estimator remains consistent. Another
consistent estimator in this setting is the jackknife-instrumental-variables-estimator (jive)
(Phillips and Hale, 1977; Angrist, Imbens and Krueger, 1999). Motivated by our leading
examples, and as in Anatolyev (2011), we also allow the number of exogenous covariates to
increase in proportion with the sample size. This requires some minor modification of the
btsls and jive estimators (denoted by mbtsls and mjive), but does not affect the consistency
of liml.
    We examine the robustness of these five estimators (liml, btsls, jive, mbtsls, and mjive)
to the presence of direct effects in this many-instrument setting. We show that liml loses
consistency if direct effects are present. The intuition is that the liml estimator attempts to
impose proportionality of all the reduced form coefficients. This explains the efficiency of
liml in the absence of direct effects, but because the reduced form coefficients are no longer
proportional when direct effects are present, it makes liml sensitive to their presence. On the
other hand, under the assumption that the product of the direct effects of the instruments
on the outcome and the direct effects on the endogenous regressor has expectation zero,
the btsls and jive estimators (in the case with a fixed number of exogenous variables) or
their many-exogenous-variables modifications mbtsls and mjive (in general) remain consis-

                                              [1]
tent. We argue through some examples and a link with the clustering literature that this
identifying assumption, although not innocuous, substantively weakens existing identifica-
tion conditions. The intuition for the robustness compared to liml is that the btsls, jive,
mbtsls, and mjive estimators, like the tsls estimator, can be thought of as two-stage estima-
tors. In the first stage a single instrument is constructed as a function of only instruments
and endogenous regressors, not involving the outcome variable. This constructed instrument
is then used in the second stage to estimate the parameter of interest using methods for
just-identified settings. Identification only requires validity of the constructed instrument,
not of all the individual instruments.
    We then study in detail two leading cases that motivate the set up and illustrate the
range and applicability of our new identifying assumptions. Both cases have a clustering
structure where the instruments are related to the cluster indicators. Such settings are
often the reason for the presence of many instruments. The many-instrument asymptotic
approximation implies that, as is common in clustering settings, large sample approximations
are based on the number of clusters growing with the sample size while the number of sampled
units from each cluster remains fixed.
    The first of the two special cases arises when the instruments are cluster indicators. For
example, Fryer (2011) and Levitt, List, Neckermann and Sadoff (2011) conducted a series
of experiments where students in randomly selected schools were given varying financial
incentives to improve achievement on test scores. Suppose we are interested in the effect
of test score achievement on outcomes later in life as in Chetty, Friedman, Hilger, Saez,
Schanzenbach and Yagan (2011). One could use the school indicators study as instruments
to capture the fact that the incentives varied between schools. However, one might be
concerned that schools also affect outcomes directly, not just through test scores. Our results
suggest that a sufficient, and, because of the randomization, substantially weaker condition
for identification is that the direct effects of the school on the outcomes are uncorrelated
with the effects of the school on test scores.
    In another example within this class, Aizer and Doyle, Jr. (2011) and Nagin and Snod-
grass (2011) study the effect of incarceration on subsequent outcomes. Defendants are ran-
domly assigned to one of a relatively large number of judges. Judges vary in their propensity
to sentence individuals to jail terms. The judge assignment is used as an instrument. One
might be concerned that judges have direct effects on outcomes beyond those mediated
through the effect on incarceration. Our critical identification assumption is that these di-
rect effects of the judges are uncorrelated with the judgesâ€™ propensity to incarcerate. This is


                                              [2]
a substantive assumption that may or may not hold in practice, but shifts the discussion of
the validity of inference away from the substantially stronger assumption that judges have
no direct effect on outcomes whatsoever.
    In the second case we have a small number of basic instruments. These basic instruments
are interacted with cluster indicators to generate a large number of instruments. Here the
number of exogenous regressors (which includes the cluster indicators) increases proportional
to the number of instruments. This case is motivated by the Angrist and Krueger (1991,
AK from hereon) study where the basic instruments, four quarter of birth indicators, are
interacted with year and state of birth indicators to generate additional instruments. In the
context of this set up our approach suggests new identification strategies that allow for direct
effects of the instruments on the outcome. In the first of these identification strategies, the
average direct effect of the instruments on the outcome is zero. In the second identification
strategy, the average direct effect (equal to the direct effect of the basic instrument) is
unrestricted, but the direct effects are uncorrelated with the effect of the instruments on the
endogenous regressor. Again these are not innocuous assumptions, but they substantively
weaken the assumption that all instruments are valid.
    The results in this paper contribute to two strands of literature. First, we contribute
to the recent many-instrument literature that has extended the earlier work by Kunitomo
(1980), Morimune (1983), Bekker (1994), and Chao and Swanson (2005). In recent work
Anatolyev (2011) also relaxes the assumption of fixed number of exogenous regressors. Haus-
man, Newey, Woutersen, Chao and Swanson (2009); Chao, Swanson, Hausman, Newey and
Woutersen (2010) and Ackerberg and Devereux (2009) relax the assumption of homoscedas-
ticity. Hansen et al. (2008), Belloni, Chen, Chernozhukov and Hansen (2011) and Gautier
and Tsybakov (2011) allow the first stage to be estimated non-parametrically. This paper
takes a complementary approach: we relax the assumption of no direct effects, but keep the
rest of the model simple to maintain tractability.
    Second, we contribute to the literature studying properties of instrumental variables
methods allowing for direct effects in settings with a fixed number of instruments. The
focus of this literature has been on correcting size distortions of tests, biases of estimators,
sensitivity analyses, and bounds in the presence of direct effects. Fisher (1961, 1966, 1967),
Caner (2007); Berkowitz, Caner and Fang (2008) and Guggenberger (2010) analyze the
implications of local (small) violations of exogeneity assumption. Hahn and Hausman (2005)
compare biases for different estimators in the presence of direct effects. Conley, Hansen and
Rossi (2007); Ashley (2009) and Kraay (2008) propose sensitivity analyses in the presence of


                                              [3]
possibly invalid instruments. Nevo and Rosen (2010) consider assumptions about the sign of
the direct effects of the instruments on the outcome to derive bounds on the parameters of
interest. Reinhold and Woutersen (2011) and Flores and Flores-Lagunes (2010) also derive
bounds allowing for direct effects of the instruments on the outcome. The current paper is
the first to derive (point) identification results in the presence of non-local departures from
the no-direct-effects assumption or exclusion restriction.
    The rest of the paper is organized as follows. In Section 2 we introduce the set up and
the notation. In Section 3 we introduce the estimators. In Section 4 we present the main
formal results allowing for direct effects of the instruments. In Section 5 we discuss in detail
two leading cases with a clustering structure. We apply the methods developed in this paper
to the data analyzed by AK in Section 6. Section 7 concludes.


2     Set Up
We consider the following instrumental variables model:

       Yi = Xi Î² + Wi0 Î´ + Zi0 Î³ + i .
                                                                                            (2.1)
      Xi = Zi0 Ï€12 + Wi0 Ï€22 + Î½i .

The first equation relates a scalar outcome Yi , i = 1, . . . , N , to a potentially endogenous
scalar regressor Xi . Wi is a vector of exogenous regressors with dimension LN , and Zi is
a vector of instruments with dimension KN . The second equation relates the endogenous
regressor Xi to the exogenous regressors Wi the instruments Zi . The object of interest is the
coefficient Î² on the endogenous regressor in the outcome equation.
    The model (2.1) modifies the conventional many-instruments model (e.g., Bekker (1994))
in two ways. First, we allow Î³ to be non-zero, thus allowing for direct effects of the instrument
on the outcome. If we restrict Î³ = 0, then the exclusion restriction holds, and the instruments
are valid. If we leave Î³ unrestricted, then Î², the coefficient of interest, is not identified. In
this paper, we will be concerned with determining assumptions on Î³ that are weaker than
Î³ = 0, but that still allow us to identify Î². Second, like Anatolyev (2011), we allow the
number of exogenous regressors, LN , to change with the sample size. The main motivation
for this extension is that often the presence of a large number of instruments is the result
of interacting a few basic instruments with many exogenous covariates. We discuss such an
example in detail in Section 5.2.
    Because the number of instruments and the number of exogenous variables changes with

                                               [4]
the sample size, the distribution of some of the random variable also changes with the sample
size. To be precise, we should therefore index the random variables and parameters by the
sample size N . For ease of notation we drop this index.
    We assume that the pairs of structural errors (i , Î½i ) are mutually independent, and
conditionally homoscedastic:
         "        !         !0                                     #
             i        i
     E                           Z1 , . . . , ZN , W1 , . . . , WN = Î£
             Î½i        Î½i

Recent papers by Chao et al. (2010) and Hausman et al. (2009) investigate the implications
of heteroscedasticity in the setting with many valid instruments, and show that liml loses
some of its attractive properties in that case. Our results complement theirs in the sense
that our results highlight a different potential concern with liml. To simplify the derivation
of distributional results, we will assume in addition that the structural errors Normally
distributed. We do not require Normality for consistency arguments.
     In the remainder of this section we introduce some additional notation. Let Y be the
N -component vector with ith element Yi , X the N -component vector with ith element Xi ,
 the N -component vector with ith element i , Î½ the N -component vector with ith element
Î½i , W the N Ã— LN matrix with ith row equal to Wi0 , and Z the N Ã— KN matrix with ith row
equal to Zi0 . Let X = (X, W) be the full matrix of endogenous and exogenous regressors,
let Y = (Y, X) be the full matrix of endogenous variables, and let Z = (Z, W) be the full
matrix of exogenous variables. Define for an arbitrary N Ã— J matrix S the following four
N Ã— N matrices, the projection matrix PS , the matrix MS that projects on the orthogonal
complement of S, the diagonal matrix DS with diagonal elements equal to those of the
projection matrix, and the product of MS and (1 âˆ’ DS )âˆ’1 :
                  âˆ’1                                   âˆ’1
PS = (S (S0 S)         S0 ,      MS = I âˆ’ (S (S0 S)         S0 ,       DS = Diag(PS ),       and HS = MS (1 âˆ’ DS )âˆ’1 .

We use the subscript âŠ¥ as shorthand for taking residuals after regression on the exogenous
regressors W, so ZâŠ¥ = MW Z, XâŠ¥ = MW X, YâŠ¥ = MW Y, and YâŠ¥ = MW Y. We also
denote by Î¹N and N -dimensional vector of ones.
   Define the augmented concentration parameter, the two by two matrix Î›N :
                                       !
                      Î›N,11 Î›N,12                         0                       
     Î›N =                                  =       Î³ Ï€12        Z0âŠ¥ ZâŠ¥       Î³ Ï€12       .                    (2.2)
                      Î›N,12 Î›N,22

The (2, 2) element of Î›N , denoted by Î›N,22 is a key measure of the strength of the instru-

                                                                [5]
ments. The (1,1) element, Î›N,11 , measures the degree of misspecification. In the case with
valid instruments, Î³ = 0, Î›N,11 = Î›N,12 = 0 and the only non-zero element of Î›N is Î›N,22 .
The (2, 2) element Î›N,22 is closely related to the conventional concentration parameter (Mar-
iano, 1973; Rothenberg, 1984), defined as Î›N,22 /Î£22 . Here, following Andrews, Moreira and
Stock (2006), we use the version without dividing by the structural variance Î£22 because
that will simplify the discussion later.


3    Estimators
In this section we introduce the five estimators for Î² whose properties we shall study. All
five have asymptotically equivalent in the setting with a fixed number of valid instruments
and a fixed number of exogenous regressors. Four of these estimators have been introduced
previously, and the fifth is a minor modification of a previously proposed estimator. The
first three estimators fit into the k-class (Nagar, 1959; Theil, 1961, 1971; Davidson and
MacKinnon, 1993). Given a scalar k, a k-class estimator for (Î², Î´) is given by:
             !
        Î²Ì‚k      0             âˆ’1  0              
               = X (I âˆ’ kMZ )X       X (I âˆ’ kMZ )Y .
         Î´Ì‚k

We are primarily interested in the estimator for Î², which can be written using the âŠ¥ notation
as

                                      âˆ’1
     Î²Ì‚k = (X0âŠ¥ (I âˆ’ kMZâŠ¥ )XâŠ¥ )            (X0âŠ¥ (I âˆ’ kMZâŠ¥ )YâŠ¥ ) .                       (3.1)

A prominent member of the k-class is the two-stage-least-squares (tsls Basmann, 1957; Theil,
1961) estimator, with kÌ‚tsls = 1. This estimator has been shown to be inconsistent under
many-instrument asymptotics, see Kunitomo (1980) and Bekker (1994). We therefore do not
further investigate its properties under the various generalizations of the many-instrument
setup here. Instead we consider a bias-corrected version of the tsls estimator. Nagar (1959)
suggested the correction kÌ‚nagar = 1 + (KN âˆ’ 2)/N, but the first of the five estimators we
focus on is a slightly different version suggested by Donald and Newey (2001), with

                       1
     kÌ‚btsls =                    .
                 1 âˆ’ (KN âˆ’ 2)/N

Although in samples with a moderate number of instruments the difference between the
Nagar and Donald-Newey estimators is small, this difference does not go away under many-

                                                     [6]
instruments asymptotics with KN /N â†’ Î±K > 0, and only the Donald-Newey version is
consistent. As we will show in the next section, once we allow LN to increase with sample
size, btsls also loses consistency. To address this issue, the second estimator we consider is a
further modification of the Donald-Newey bias-corrected estimator that achieves consistency
even when LN /N â†’ Î±L > 0:
                       1 âˆ’ LN /N
      kÌ‚mbtsls =                     .
                   1 âˆ’ KN /N âˆ’ LN /N
This estimator is also considered in Anatolyev (2011).
    The third estimator we consider is the limited-information-maximum-likelihood estimator
(liml, Anderson and Rubin, 1949), with

                       (Y âˆ’ XÎ²)0 MW (Y âˆ’ XÎ²)
      kÌ‚liml = min                           .
                   Î²   (Y âˆ’ XÎ²)0 MZ (Y âˆ’ XÎ²)
This estimator has been shown to be asymptotically efficient under many-instrument asymp-
totics (Chioda and Jansson, 2009; Anderson et al., 2010).
    The fourth estimator we study in the current paper is the jackknife-instrumental-variables
estimator (jive Phillips and Hale, 1977; Angrist et al., 1999):

                                        âˆ’1
      Î²Ì‚jive = (X0âŠ¥ (MW âˆ’ HZ ) XâŠ¥ )          (X0âŠ¥ (MW âˆ’ HZ ) YâŠ¥ ) .                                  (3.2)

Ackerberg and Devereux (2009) present simulation evidence that this estimator is biased
when the number of exogenous regressors is large, and suggest a bias-corrected version. We
study a new version of the jackknife estimator, closely related to the Ackerberg-Devereux
estimator, which we refer to as the modified jive estimator, or mjive:

                                                          âˆ’1
      Î²Ì‚mjive = (X0âŠ¥ (MW âˆ’ (1 âˆ’ LN /N )HZ ) XâŠ¥ )               (X0âŠ¥ (MW âˆ’ (1 âˆ’ LN /N )HZ ) YâŠ¥ ) . (3.3)

We will show that unlike the original jive estimator, this estimator remains consistent even
if LN /N â†’ Î±L > 0.
     The focus of the current paper is on the properties of these five estimators, that is,
Î²Ì‚btsls , Î²Ì‚mbtsls , Î²Ì‚liml , Î²Ì‚jive , and Î²Ì‚mjive , under various assumptions about the rates at which the
number of instruments and exogenous regressors increase with the sample size, KN , LN , and
the assumptions about the parameters governing the misspecification, Î³.




                                                    [7]
4     Many Invalid Instruments
In this section we look at the properties of the five estimators allowing for many exogenous
covariates (LN /N â†’ Î±L > 0), and allowing for direct effects of the instruments (Î³ 6= 0). If
we fix Î±L = 0 and Î³ = 0, we are in the many instrument case studied in the literature (e.g.
Bekker, 1994; Morimune, 1983; Hahn, 2002; Chao and Swanson, 2005). If we also restrict
Î±K = 0, we are back in the case with conventional instrumental variables asymptotics
discussed in most textbooks (e.g. Wooldridge, 2002; Angrist and Pischke, 2009).
    We make the following assumptions.
Assumption 1.(Instruments and exogenous variables)
  (i) Zi âˆˆ RKN , Wi âˆˆ RLN , i âˆˆ R, Î½i âˆˆ R, for i = 1, . . . , N , N = 1, . . . are triangular arrays
      of random variables with (Zi , Wi , i , Î½i ), i = 1, . . . , N exchangeable.
 (ii) Z is full column rank with probability one.
(iii) (PZ )ii < c for some c < 1 for all i = 1, . . . , N with probability one.
                           âˆš
(iv) maxiâ‰¤N |(ZâŠ¥ )0i Ï€12 |/ N â†’ 0 and;
                   P                                        P
 (v) supN supiâ‰¥1 j |(PZâŠ¥ )ij | < C and supN supiâ‰¥1 j |(PW )ij | < C for some C < âˆž with
      probability one

The first two parts of this assumption are standard, with a minor adaption to allow for many
exogenous variables. The remaining three parts are technical assumptions we use to deal
with the jive and mjive estimators.
Assumption 2.(Model)
 (i) (i , Î½i )0 | Z, W are iid with mean zero, positive definite covariance matrix Î£, and finite
     fourth moments;
(ii) The distribution of (i , Î½i )0 | Z, W is Normal.

For consistency we only use the first part of this assumption. For the distributional results we
use Normality to highlight the specific modifications to the asymptotic distributions coming
from the direct effects of the instruments.
Assumption 3.(Number of instruments and exogenous regressors)
For some 0 â‰¤ Î±K < 1 and 0 â‰¤ Î±L < 1,

      KN /N = Î±K + o(N âˆ’1/2 ),         and      LN /N = Î±L + o(N âˆ’1/2 ).

The first part of this assumption is standard in the many-instrument literature. The second
part is identical to the corresponding assumption in Anatolyev (2011).


                                                 [8]
Assumption 4.(Concentration parameter)
For some positive semi-definite Î› with Î›22 > 0,
                p
      Î›N /N â†’ Î›,            and E [Î›N /N ] â†’ Î›.

The first part of assumption 4 is a natural extension of the assumption underlying the
Bekker many-instrument asymptotics. The second part of the assumption strengthens this
slightly by also requiring the expectation of the concentration parameter to converge to its
probability limit.
    The first main result establishes the probability limit of the estimators.
Theorem 1.(Consistency with Many Invalid Instruments)
Suppose Assumptions 1(i)â€“(iii), 2(i), 3 and 4 hold. Then:
                      p              1âˆ’Î±L            Î›22
 (i) (k-class) if kÌ‚ âˆ’â†’ k with k < 1âˆ’Î± K âˆ’Î±L
                                             + Î£22 (1âˆ’Î± K âˆ’Î±L )
                                                                , then:

                        p              Î›12 + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)Î£12
                    Î²Ì‚kÌ‚ âˆ’â†’ Î²k = Î² +                                       ,
                                       Î›22 + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)Î£22

(ii) (liml) Suppose min eig(Î£âˆ’1 Î›) < Î›22 /Î£22 . Then:

                               Î›12 âˆ’ min eig(Î£âˆ’1 Î›)Î£12                    1 âˆ’ Î±L     min eig(Î£âˆ’1 Î›)
                Î²liml = Î² +                            ,     kliml =               +                ,
                               Î›22 âˆ’ min eig(Î£âˆ’1 Î›)Î£22                 1 âˆ’ Î±K âˆ’ Î±L   1 âˆ’ Î±K âˆ’ Î±L

(iii) (btsls)

                               Î›12 + {Î±K Î±L /(1 âˆ’ Î±K )} Î£12                      1
                Î²btsls = Î² +                                ,      kbtsls =          ,
                               Î›22 + {Î±K Î±L /(1 âˆ’ Î±K )} Î£22                   1 âˆ’ Î±K

(iv) (mbtsls)

                               Î›12                    1 âˆ’ Î±L
            Î²mbtsls = Î² +          ,   kmbtsls =               ,
                               Î›22                 1 âˆ’ Î±K âˆ’ Î±L

 (v) (jive) Suppose Î±L < Î›22 /Î£22 . Then:

                               Î›12 âˆ’ Î±L Î£21
                Î²jive = Î² +                 ,
                               Î›22 âˆ’ Î±L Î£22




                                                      [9]
(vi) (mjive)

                            Î›12
             Î²mjive = Î² +       ,
                            Î›22

If we impose Î›11 = 0 (implying Î›12 = 0) and Î±L = 0, the condition for consistency of Î²Ì‚kÌ‚
is the same as in Chao and Swanson (2005), namely that kÌ‚ â†’ 1/(1 âˆ’ Î±K ). Having many
exogenous regressors changes the condition on kÌ‚ to kÌ‚ â†’ (1 âˆ’ Î±L )/(1 âˆ’ Î±K âˆ’ Î±L ).
    A key finding is the robustness of the mbtsls and mjive estimators relative to the liml
estimator. Specifically, if Î›12 is equal to zero, then mbtsls and mjive are consistent even if
Î›11 differs from zero. If the number of exogenous variables is fixed, then btsls and jive are
also consistent if Î›12 = 0. In order for liml to be consistent for all values of Î£, then it has
to be the case that Î›11 is equal to zero (and that immediately implies that Î›12 = 0). To
provide some intuition, consider the reduced-form based on the model (2.1):

      Yi = Zi0 (Ï€12 Î² + Î³) + Wi0 (Î´ + Ï€22 Î²) + (Î½i Î² + i ),
      Xi = Zi0 Ï€12 + Wi0 Ï€22 + Î½i .

If the instruments are valid, so that Î³ = 0, then the vector of reduced-form coefficients
on Zi in the first equation is proportional to Ï€12 , the vector of reduced-form coefficients in
the second equation. The liml estimator tries to impose this proportionality. This leads
to efficiency if proportionality holds (Chioda and Jansson, 2009; Anderson et al., 2010).
However, if Î³ 6= 0, then the proportionality does not hold in the population, and liml
loses consistency. On the other hand, mbtsls and mjive, like tsls, can be thought of as
two stage estimators. In the first stage composite instruments are constructed, one for
each regressor (endogenous or exogenous) based on the data on the endogenous regressor,
the exogenous variables, and the instruments alone. These instruments are then used to
estimate the parameters of interest using a method for just-identified settings, possibly with
some adjustment. In this procedure proportionality of the reduced forms is never exploited.
This explains why Î›12 = 0 is a sufficient condition for consistency, although it results in
efficiency loss relative to liml when proportionality does hold.
    Next we consider large sample approximations to the distribution of the estimators.
We make use of Assumption 2(ii), which puts Normality on the error terms. If instead of
Normality we only assumed finite fourth moments (Assumption 2 (i)), then the asymptotic
variance terms would depend on the third and fourth moments of the error terms (Hansen
et al., 2008; van Hasselt, 2010). Assuming Normality leads to simpler asymptotic formulae

                                                 [10]
that will allow us to better focus on the effect of relaxing the standard assumptions that
Î³ = 0 and Î±L = 0 and highlight the substantive differences. To put the main results for the
case with direct effects in perspective we first present the distributional results for the case
with Î³ = 0, but Î±L possibly positive. See Anatolyev (2011) for asymptotic variances for liml
and mbtsls without normality.
Theorem 2.(Asymptotic Normality with Many Exogenous Regressors)
Suppose Assumptions 1â€“4 hold. Suppose in addition that Î³ = 0. Then:
  (i) (liml)
             âˆš 
                                                                                      
                          
                                d         âˆ’2              Î±K (1 âˆ’ Î±L )             2
                                                                                      
              N Î²Ì‚liml âˆ’ Î² | Z â†’ N 0, Î›22 Â· Î£11 Î›22 +                   Î£11 Î£22 âˆ’ Î£12
                                                         1 âˆ’ Î±K âˆ’ Î±L
 (ii) (mbtsls)
             âˆš 
                                                                                       
                            
                                  d         âˆ’2             Î±K (1 âˆ’ Î±L )             2
                                                                                       
              N Î²Ì‚mbtsls âˆ’ Î² | Z â†’ N 0, Î›22 Î£11 Î›22 +                   Î£11 Î£22 + Î£12
                                                          1 âˆ’ Î± K âˆ’ Î± L
(iii) (mjive) Suppose in addition that N âˆ’1 i 1âˆ’(D1 )ii â†’ Ï„
                                           P
                                                      Z


            âˆš              
                                   d
                N Î²Ì‚mjive âˆ’ Î² | Z â†’

                      N 0, Î›âˆ’2                                                 2
                                                                                     
                            22 Î£11 Î›22 + (1 âˆ’ Î±L ) ((1 âˆ’ Î±L )Ï„ âˆ’ 1) Î£11 Î£22 + Î£12          . (4.1)

The presence of many exogenous variables increases the asymptotic variance of liml and
mbtsls since (1 âˆ’ Î±K )Î±K /(1 âˆ’ Î±L âˆ’ Î±K ) > Î±K /(1 âˆ’ Î±K ) if Î±L > 0, but the conclusion that
liml is more efficient than mbtsls does not change. Also, by Jensenâ€™s inequality Ï„ â‰¥ 1âˆ’Î±K1 âˆ’Î±L ,
so that mbtsls has smaller asymptotic variance than mjive.
    If we want to determine the asymptotic distribution when Î³ is allowed to differ from
zero, it no longer suffices to simply condition on Z and treat the sequence of parameters Î³
as constant. The reason is because the stochastic behaviour of the estimators now depends
on Î›N,12 . Even if the limit Î›12 = 0, if Î³ differs from zero (and thus Î›11 > 0) it will generally
be the case that Î›N,12 differs from zero for finite N . The stochastic behavior of Î›N,12 affects
the large sample distribution of the estimators, and we need to put sufficient structure on it
to be able to determine this distribution.
    The assumption below puts a random effects structure on the direct effects of the in-
strument on the outcome and the endogenous regressor similar to that in Chamberlain and
Imbens (2004). This provides a natural way of determining the stochastic behaviour of Î›N,12 ,
although it is not necessarily the only way of doing so.
    First we redefine the parameters by orthogonalizing them with respect to ZâŠ¥ as
                                            
                            0    1/2
         Î³Ìƒ Ï€Ìƒ12 = (Î±K ZâŠ¥ ZâŠ¥ )         Î³ Ï€12 .

                                              [11]
Then we consider the following assumption
Assumption 5.(Incidental parameters)
The pairs (Î³Ìƒk , Ï€Ìƒ12,k ), for k = 1, 2, . . . , KN , are iid with distribution
                !                              ! !
          Î³Ìƒk                           ÂµÎ³
                     Z, W âˆ¼ N                     ,Îž .
        Ï€Ìƒ12,k                          ÂµÏ€
A key implication of Assumption 5 is that
                                              !                 !
                                           Î³0
                          
                      Î›N             1                       
       Î› = plim           = plim            0
                                                Z0âŠ¥ ZâŠ¥ Î³ Ï€12
                      N             N Ï€12
                         KN
                                    !                !        !       !0
                       1 X     Î³Ìƒk
                                                        Âµ Î³     Âµ Î³
         = plim                         Î³Ìƒk Ï€Ìƒ12,k     =                 + Îž.
                      KN k=1 Ï€Ìƒ12,k                       ÂµÏ€      ÂµÏ€

Hence, if we rule out the knife-edge case Îž12 = âˆ’ÂµÎ³ ÂµÏ€ , then under Assumption 5, the
identification condition Î›12 = 0 is equivalent to ÂµÎ³ = 0 and Îž12 = 0. This equivalence
will be useful in determining more primitive conditions that imply the condition Î›12 = 0.
We defer further discussion of this assumption, and in particular the motivation for making
the independent random effects assumption in terms of (Î³Ìƒk , Ï€Ìƒ12,k ) (instead of in terms of
(Î³k , Ï€12,k )) to the next section where we consider two special cases. Next we present the
large sample distribution theory for the case with Î³ 6= 0.
Theorem 3.(Asymptotic Normality with Many Invalid Instruments)
Suppose that Assumptions 1(i)â€“(iii), 2â€“5 hold. Suppose in addition that ÂµÎ³ = Îž12 = 0. Then:
 (i) (mbtsls)

             âˆš                
                                 d
                 N Î²Ì‚mbtsls âˆ’ Î² â†’
                                                                             
                          âˆ’2         Î±K (1 âˆ’ Î±L )            2
                                                                           Î›22
                  N 0, Î›22 Î£11 Î›22 +              Î£11 Î£22 + Î£12 + Î›11 Î£22 +         ,
                                     1 âˆ’ Î±K âˆ’ Î±L                            Î±K

(ii) (mjive) Suppose that in addition N âˆ’1                 1
                                                   P
                                                      i 1âˆ’(DZ )ii   â†’ Ï„ . Then:

             âˆš            
                             d
              N Î²Ì‚mjive âˆ’ Î² â†’
                                                                                     
                   âˆ’2                                                2
                                                                                   Î›22
             N 0, Î›22 Î£11 Î›22 + (1 âˆ’ Î±L )((1 âˆ’ Î±L )Ï„ âˆ’ 1) Î£11 Î£22 + Î£12 + Î›11 Î£22 +         ,
                                                                                    Î±K

Compared to Theorem 2 (ii)â€“(iii), allowing for direct effects leads to an additional term in
the asymptotic variance which is proportional to Î›11 , which measures the extent of mis-

                                                    [12]
specification. If Î›11 = 0, then the asymptotic variance of mbtsls and mjive reduces to that
in Theorem 2(ii)â€“(iii). Note that the extra term decreases in the number of instruments.
The intuition is that as the number of instruments increases, we are better able to deal
with the presence of direct effects, as the product of the direct effects and the effects of the
instruments on the endogenous variable gets averaged out to identify Î².


5     Two Special Cases
In this section we consider two special cases with additional structure on the data generating
process. In both cases each unit i belongs to a subpopulation or cluster, with cluster indicator
Gi âˆˆ {1, 2, . . . , GN }. These clusters are closely related to the instruments. We are interested
in large sample approximations where the number of units sample from each subpopulation is
finite, and the number of subpopulations increases proportional to the sample size, leading to
the many-instruments setting. Let the number of units in group g be Ng , with N = G
                                                                                         P N
                                                                                           g=1 Ng .
For convenience, let us assume that the number of unit sampled from each subpopulation is
the same for all subpopulations, Ng = N/GN for all g.


5.1    Special Case I: Clustering
To focus on the conceptual issues, let us assume there are no exogenous regressors beyond
the intercept, LN = 1. In the first special case the instruments are the cluster indicators,
Zik = 1Gi =k , for k = 1, . . . , GN âˆ’ 1, so that the number of instruments is the number of
clusters minus one, KN = GN âˆ’ 1. The general model in (2.1) can now be written as

                            N âˆ’1
                           GX
      Yi = Î´ + Î²Xi +               Î³k 1Gi =k + i ,                                          (5.1)
                           k=1
                    N âˆ’1
                   GX
      Xi = Ï€22 +           Ï€12,k 1Gi =k + Î½i .                                               (5.2)
                   k=1


Exploiting the special structure here, in combination with the equal cluster size, the aug-
mented concentration parameter can be written as the sample covariance matrix of (Î³k , Ï€12,k ):

              GN âˆ’1
                                                                   !
           N X          (Î³k âˆ’ Î³)2          (Î³k âˆ’ Î³) (Ï€12,k âˆ’ Ï€ 12 )
      Î›N =                                                           ,
           GN k=1 (Î³k âˆ’ Î³) (Ï€12,k âˆ’ Ï€ 12 )     (Ï€12,k âˆ’ Ï€ 12 )2



                                                      [13]
where
             GN âˆ’1                                   GN âˆ’1
           1 X                                     1 X
       Î³=          Î³k ,        and       Ï€ 12   =          Ï€12,k .
          GN k=1                                  GN k=1

Now let us consider Assumption 5 and interpret it in this context. Suppose we have a
large population of clusters. Let ÂµY,g and ÂµX,g be the population means of Yi âˆ’ Î²Xi and
Xi in cluster g, and let ÂµY and ÂµX be the overall population means. In terms of the
original parametrization, we have: Ï€22 = ÂµX,GN , Ï€12,k = ÂµX,k âˆ’ ÂµX,GN , Î´ = ÂµY,GN and
Î³k = ÂµY,k âˆ’ ÂµY,GN .
    The natural way to impose a random effects structure on the parameters would be to
assume that the cluster means (ÂµY,k , ÂµX,k ) are independent and
                 !                !     !
          ÂµY,k               ÂµY
                     âˆ¼N               ,Î¦ .                                                                            (5.3)
          ÂµX,k               ÂµX

This implies
                           ï£«          ï£¶
                r           ÂµY,1 ÂµX,1
                    GN âˆ’ 1 ï£¬                                                      âˆš
                             ..   .. ï£·
                                                                                                                               
                                                                              1âˆ’1/ GN
    Î³Ìƒ Ï€Ìƒ12 =             Bï£¬  .    . ï£·ï£¸,              B=         IGN âˆ’1 âˆ’       GN âˆ’1
                                                                                      Î¹GN âˆ’1 Î¹0GN âˆ’1             âˆ’ âˆšG1 N Î¹GN âˆ’1
                     GN    ï£­
                            ÂµY,1 ÂµX,1


where the (GN âˆ’ 1) Ã— GN matrix B satisfies BÎ¹GN = 0, and BB 0 = IGN âˆ’1 . Thus, a ran-
dom effects specification on (ÂµY,k , ÂµX,k ) as in (5.3) implies a random effects specification on
(Î³Ìƒ, Ï€Ìƒ12 ), namely
                    !                 ! !
               Î³Ìƒk                 0                             GN âˆ’ 1
                      Z, W âˆ¼ N          ,Îž ,         with Îž =           Â· Î¦.
             Ï€Ìƒ12,k                0                               GN

On the other hand, because (Î³k , Ï€12,k ) measure the effect relative to the last group, GN ,
assuming independence of (Î³k , Ï€12,k ) of (Î³l , Ï€12,l ) is not attractive. The random effects as-
sumption on (Î³Ìƒk , Ï€Ìƒ12,k ) is therefore more reasonable than a random effects assumption on
(Î³k , Ï€12,k ) would be. Moreover, the augmented concentration parameter can be expressed as
a sample covariance matrix of (Î³Ìƒk , Ï€Ìƒ12,k ):

               GN                          2           1
                                                             PGN                                       !
            N X                   Î³Ìƒk âˆ’ Î³Ìƒ             GN        k=1    Î³Ìƒk âˆ’ Î³Ìƒ       Ï€Ìƒ12,k âˆ’ Ï€Ìƒ 12
       Î›N =                                                                           2                   ,
            GN k=1        Î³Ìƒk âˆ’ Î³Ìƒ Ï€Ìƒ12,k âˆ’ Ï€Ìƒ 12                      Ï€Ìƒ12,k âˆ’ Ï€Ìƒ 12

                                                      [14]
where Î³Ìƒ = G1N G
                P N                    1
                                         PGN
                  g=1 Î³Ìƒk and Ï€Ìƒ 12 = GN   g=1 Ï€Ìƒ12,k .
    There is an alternative representation to the set up in (5.1)â€“(5.2) that ties it in more
closely to the clustering literature. In this alternative representation the demeaned direct
effects ÂµY,g âˆ’ ÂµY are viewed as random effects reflecting clustering. Let us write the outcome
equation (5.1) as
                                                         
      Yi = ÂµY + Î²Xi + Î·i ,          where Î·i = ÂµY,Gi âˆ’ ÂµY + i ,

is the composite residual. The cluster-specific component is equal to the direct effect of the
instrument. Hence, we can think of the residuals Î·i having a clustering structure associated
with the instruments
                                                ï£±
                                                ï£´ Î£ + Î¦11 if i = j,
                                                ï£² 11
                                                ï£´
                                                ï£´
      E [Î·i | Z] = 0             E [Î·i Î·j | Z] = Î¦11        if Gi = Gj , i 6= j,
                                                ï£´
                                                ï£´
                                                ï£´
                                                ï£³0          otherwise.

Analogously we can write the second equation with a clustering structure:
                                             
     Xi = ÂµX + Î¶i      where Î¶i = ÂµX,Gi âˆ’ ÂµX + Î½i ,

and
                                                      ï£±
                                                      ï£´ Î£ + Î¦22   if i = j,
                                                      ï£² 22
                                                      ï£´
                                                      ï£´
      E [Î¶i | Z] = 0                   E [Î¶i Î¶j | Z] = Î¦22        if Gi = Gj , i 6= j,
                                                      ï£´
                                                      ï£´
                                                      ï£´
                                                      ï£³0          otherwise.

In addition, let Î¦12 = E [Î¶i Î·j |Gi = Gj ]. The critical assumption that Î›12 is equal to zero
(equivalent to Î¦12 = 0) in this representation amounts to assuming that the cluster compo-
nent in the outcome equation is uncorrelated with the cluster component in the first stage.
This assumption is not innocuous, but assumptions about zero correlations for cluster com-
ponents are often made in clustering settings. It is obviously substantively weaker than
assuming the absence of clustering effects in the outcome equation, or Î¦11 = 0.
   In this case with the instruments equal to the group dummies the original jive estimator
has an interesting form. The predicted value for Xi underlying the tsls estimator is the
average value of Xj for all units in the cluster,

      b tsls = 1
                        X
      X i                         Xj
              NGi      j:Gj =Gi


                                                  [15]
The jive estimator modifies that to the average over all units in the cluster, excluding unit
i itself:

        b jive =   1         X
       X  i                           Xj .
                 NGi âˆ’ 1 j:G =G ,j6=i
                                j     i


With a finite number of units per cluster omitting unit i can make a substantial difference,
and this is reflected in the inconsistency of tsls in this setting.
    The properties of the previously discussed estimators liml, btsls, mbtsls, jive, and mjive
follow as a special case of Theorems 1-3, specializing it to the case with LN = 1 so that
Î±L = 0. In this case there is no difference asymptotically between jive and mjive and
between btsls and mbtsls because the number of exogenous variables is fixed.


5.2    Special Case II: Clusters with Interactions
In the second case we maintain the cluster structure with cluster indicator Gi âˆˆ {1, 2, . . . , GN }.
For each unit there is a binary indicator Qi that serves as the basic instrument. More
generally we could have a number of basic instruments, and allow these to be discrete or
continuous. This special case is motivated by the Angrist-Krueger analysis where the basic
instruments are quarter of birth indicators. We generate additional instruments by inter-
acting the cluster indicator with this binary instrument. We include the cluster indicators
as exogenous covariates, Wi,k = 1Gi =k , so that again KN = LN = GN . Again for ease of
exposition let us assume that the clusters are all equal size, Ng = N/GN for all g, and that
                                                              P
the fraction of Qi = 1 units in each cluster is equal to q = i Qi Â· 1Gi =g /Ng for all g. The
model can now be written as
                     KN
                     X                    KN
                                          X
      Yi = Î²Xi +           Î´k Wik +             Î³k Zik + i ,                                 (5.4)
                     k=1                  k=1




             KN
             X                      KN
                                    X
      Xi =         Ï€12,k Zik +            Ï€22,k Wik + Î½i .                                    (5.5)
             k=1                    k=1


In this case the limit of the augmented concentration parameter is
                    !            !0
              ÂµÎ³           ÂµÎ³
      Î›=                              + Îž.                                                    (5.6)
              ÂµÏ€           ÂµÏ€


                                                             [16]
We can directly apply the results from Section 4, which imply that mjive and mbtsls are
consistent and asymptotically normally distributed if Î›12 is equal to zero. In this case
Î›12 = 0 is not necessarily an attractive assumption. It would require that ÂµÎ³ Â· ÂµÏ€ + Îž12 = 0,
which essentially requires that both ÂµÎ³ and Îž12 are zero. We can in fact relax the sufficient
conditions for identification in this special setting. We consider two specific alternatives.
First, we assume that ÂµÎ³ = 0, allowing Îž12 to be different from zero. Second, we consider
the assumption that Îž12 = 0, allowing ÂµÎ³ to be different from zero. In both cases Î›12 6= 0,
yet the parameter of interest is identified.
   Under the first assumption, ÂµÎ³ = 0, we can simply use the Wald estimator with Qi as
the single instrument and Wi = 1 as a single exogenous regressor:
                 1                          1
                    P                              P
                Nq   i : Qi =1 Yi    âˆ’   N (1âˆ’q)       i : Qi =0   Yi
     Î²Ì‚wald =   1
                   P                        1
                                                   P
                Nq  i : Qi =1 Xi     âˆ’   N (1âˆ’q)       i : Qi =0   Xi

   Adding the interactions of the type Qi Â· 1Gi =k as additional instruments would lead to
inconsistency if we use the liml, btsls, mbtsls, jive or mjive estimators.
Theorem 4.(Zero Mean)
Suppose the model in (5.4)-(5.5) holds. Suppose also that Assumptions 1â€“3 and 5 hold.
Suppose that in addition ÂµÎ³ = 0 and ÂµÏ€ 6= 0. Then Î²Ì‚wald is consistent for Î² and satisfies
     âˆš              
                       d
         N Î²Ì‚wald âˆ’ Î² â†’ N (0, (Îž11 /Î±K + Î£11 )/Âµ2Ï€ )

In the second case with ÂµÎ³ 6= 0 and Îž12 = 0, again using all interactions as instruments does
not lead to consistency whether we use liml, btsls, mbtsls, jive or mjive. However, in this
case we can base a consistent estimator on a strategy where we treat Qi as an exogenous
regressor instead of an instrument, and only use the remaining KN âˆ’ 1 interactions of the
type Qi Â· 1Gi =k as instruments with the mbtsls or mjive estimators:

                               KN
                               X
      Y i = X i Î² + Q i Î´0 +         Wik Î´k + i                                       (5.7a)
                               i=1
                        KN
                        X                      N âˆ’1
                                              KX
     Xi = Qi Ï€22,0 +          Wik Ï€22,k +               Ï€12,k Qi Wik + Î½i             (5.7b)
                        k=1                    k=1


This allows for a direct (common) effect of the original basic instrument, but rules out
interaction effects.


                                                             [17]
Theorem 5.(Interactions)
Suppose that the model (5.4)â€“(5.5) holds. Suppose also that Assumptions 1â€“5 hold and that
Îž12 = 0. Then the mbtsls and mjive estimators based on the model (5.7) are consistent for
Î². Moreover, under those assumptions:

      âˆš                      d
          N (Î²Ì‚mbtsls âˆ’ Î²) â†’
                                                                                     
                         âˆ’2                                (1 âˆ’ Î±K )Î±K            2
                                                                                     
                N 0, Îž22 Îž11 Îž22 /Î±K + Îž11 Î£22 + Îž22 Î£11 +             Î£11 Î£22 + Î£12
                                                            (1 âˆ’ 2Î±K )

and

      âˆš                      d
          N (Î²Ì‚mjive âˆ’ Î²) â†’
      N 0, Îžâˆ’2                                                                        2
                                                                                               
            22 Îž11 Îž22 /Î±K + Îž11 Î£22 + Îž22 Î£11 + (1 âˆ’ Î±K )((1 âˆ’ Î±K )Ï„ âˆ’ 1) Î£11 Î£22 + Î£12


               q2         (1âˆ’q)2
where Ï„ =    qâˆ’Î±K
                     +   1âˆ’qâˆ’Î±K
                                   is the probability limit of tr((I âˆ’ DZ )âˆ’1 /N ).


6     An Application
We apply some of the methods to a subset of the Angrist and Krueger (1991) data. We
use individuals born in the first and fourth quarter (so we have a single binary basic in-
strument, although this is not essential), dropping observations from Alaska because there
are some years birth quarters with no observations, leaving us with observations on 162,487
individuals.
    Let Wik , for k = 1, . . . , GN be the cluster indicators, corresponding to year of birth times
state of birth interactions, so that GN = 500, and let Qi be the binary quarter of birth
indicator. The general model we consider is

                      KN
                      X                KN
                                       X
      Yi = Î²Xi +            Î´k Wik +         Î³k Qi Wik + i ,                                (6.1)
                      k=1              k=1




             KN
             X                       KN
                                     X
      Xi =          Ï€12,k Qi Wik +         Ï€22,k Wik + Î½i .                                  (6.2)
              k=1                    k=1




                                                         [18]
                 !                     !    !
          Î³k                      ÂµÎ³
                     Z, W âˆ¼ N              ,Îž .
         Ï€12,k                    ÂµÏ€

    We look at six estimators, the five studied in this paper and the two-stage-least-squares
(tsls) estimator. We consider three sets of instruments and exogenous variables.
    In the first setting, we use a single binary instrument, an indicator for being born in
the fourth quarter, Zi = Qi . There are no exogenous covariates beyond the intercept. The
properties of this estimator are captured by Theorem 4. In particular, in this just-identified
case the iv estimator is valid here if the average direct effect of the instruments is zero,
ÂµÎ³ = 0.
    In the second case we interact the qob dummy with state of year times year of birth
dummies, for a total of 500 instruments, and 500 exogenous regressors. Here Theorems 1
and 3 contain the relevant results. In this case liml is not consistent unless Îž11 , Îž12 and ÂµÎ³
are zero. The mjive and mbtsls estimators are consistent under the weaker condition that
the linear combination Î›12 = Îž12 + ÂµÎ³ Â· ÂµÏ€ is equal to zero. Within the context of the model
this setting requires the strongest conditions.
    In the third case we only use the interactions as instruments and treat the basic quarter
of birth dummy as an exogenous variable rather than as an excluded instrument. We also
include the year of birth times quarter of birth dummies as exogenous covariates. For this
case Theorem 5 has the appropriate results. Here liml is not consistent unless both Îž11 and
Îž12 are equal to zero. The mbtsls and mjive estimators are consistent under the weaker
condition that Îž12 = 0.
    Table 1 presents the estimates and standard errors under various assumptions. Liml,
mbtsls, and mjive yield similar point estimates, irrespective of the set of instruments. Jive
yields smaller point estimates under the designs which include many exogenous regressors,
which is consistent with Theorem 1. On the other hand, the bias of btsls under these designs
appears small.
    The standard errors are quite different though for the different estimators when we use a
large number of instruments. Taking into account the large number of exogenous variables
does not appear to matter very much. Neither does taking into account non-zero values for
ÂµÎ³ , Îž12 or Îž11 . In this specific case this appears to be due to the fact that point estimates
for Î›11 conditional on Î›12 = 0 are close to zero: for this data set there is little evidence for
direct effects of the instruments, consistent with the validity of the instruments.




                                                [19]
7     Conclusion
In this paper we analyze linear models with a single endogenous and many instruments.
Departing from the current literature we allow for direct effects of the instruments on the
outcome. Such direct effects have very different impacts on standard estimators. The liml
estimator, efficient in the many-valid-instrument case, is inconsistent in the presence of such
effects. The btsls and jive estimators are consistent if the direct effects are uncorrelated with
the effects of the instruments on the endogenous regressor. This condition is not innocuous.
In many cases direct effects of the instruments on the outcome may well be correlated with
effects on the endogenous regressor. However, it does shift the discussion of identification
issues in instrumental variables away from the focus on the requirement that none of the
instruments have any direct effects whatsoever, which in cases with many instruments may be
unrealistic, and as this paper shows, unnecessarily restrictive. The results in the paper also
suggest a re-assesment of the merits of liml versus other estimators in the many-instrument
setting.




                                              [20]
Appendices
We first define some additional notation. Write the reduced-form based on Equations (2.1)
as:
                                   
                          Ï€11 Ï€12
                                       + Vi0
              
       Yi Xi = Zi Wi
                            Ï€21 Ï€22

where Ï€11 = Î³ + Ï€12 Î² and Ï€21 = Î´ + Ï€12 Î², and Vi = (i + Î½i Î², Î½i )0 , and let V be the N by
2 matrix with ith row equal to Vi0 . Denote the upper KN Ã— 2 submatrix of the matrix of
reduced-form coefficients by Î 1 = (Ï€11 , Ï€12 ). Let:
                 
             1 0
     Î“=
            âˆ’Î² 1

Let â„¦ = E[Vi Vi0 ] denote the reduced-form covariance matrix. Then:
                                                             
           âˆ’1 0     âˆ’1     Î£11 + 2Î£12 Î² + Î£22 Î² 2 Î£12 + Î£22 Î²
     â„¦ = Î“ Î£Î“ =
                                Î£21 + Î£22 Î²           Î£22

Let Wd (f, V, V âˆ’1 M ) denote a d-dimensional non-central Wishart distribution with f de-
grees of freedom, scale parameter V , and non-centrality parameter M . Let S1/2 denote the
symmetric square root of a symmetric positive semi-definite matrix S.


Appendix A                Auxilliary Lemmata
Lemma A.1.
Consider the quadratic form Q = (M + U )0 C(M + U ), where M âˆˆ RN Ã—S , C âˆˆ RN Ã—N are
non-stochastic, C is symmetric, and U = (u1 , . . . , uN )0 , with ui âˆ¼ [0, â„¦] iid. Let a âˆˆ RS be a
non-stochastic vector. Assume ui has finite fourth moments. Denote dC = diag(C). Then:
 (i) (Lemma 1, Bekker and van der Ploeg, 2005)

               E[Q | C] = M 0 CM + tr(C)â„¦
            var(Qa | C) = a0 â„¦aM 0 C 2 M + a0 M 0 C 2 M aâ„¦ + â„¦aa0 M 0 C 2 M + M C 2 M aa0 â„¦
                             + tr(C 2 )(a0 â„¦aâ„¦ + â„¦aa0 â„¦)
                             + d0C dC [E(a0 u)2 uu0 âˆ’ a0 â„¦aa0 â„¦ âˆ’ a0 â„¦aâ„¦] + 2d0C CM aE[(a0 u)uu0 ]
                             + M 0 CdC E[(a0 u)2 u0 ] + E[(a0 u)2 u]d0C CM

      If the distribution of ui is Normal, the last two lines of the variance expression equals
      zero.




                                               [21]
(ii) Suppose that the distribution of ui is Normal, and that, as N â†’ âˆž:

                 M 0 C 2 M/N â†’ QCM                                tr(C 2 )/N â†’ Ï„C 2
                                                                                     âˆš
                           is of C may depend on N . Suppose also that maxiâ‰¤N kmis k/ N â†’
       where the elements cP
       0 and supN maxiâ‰¤N N    j=1 |cij | = DC < âˆž. Then:

                 âˆš                             d
                      N (Qa/N âˆ’ EQa/N ) â†’ N (0, V ) ,

       where

                 V = a0 â„¦aQCM + a0 QCM aâ„¦ + â„¦aa0 QCM + QCM aa0 â„¦ + Ï„C 2 (a0 â„¦aâ„¦ + â„¦aa0 â„¦).

Proof. We only prove Part (ii). We follow the arguments in van Hasselt (2010), who proves
asymptotic Normality of Qa/N when ui are non-normal, but imposes slightly stronger regularity
conditions. By the CrameÌr-Wold device, it suffices to prove that for any vector b âˆˆ RS :
                              d
       N âˆ’1/2 b0 Qa âˆ’ E b0 Qa â†’ N 0, b0 V b .
                                          
                                                                                                            (A.1)

Let mb = M b be an N -vector with the ith element equal to Ss=1 mis bs , and similarly for ma , ub
                                                                      P
and ua . Let also â„¦p,r = p0 â„¦r, for p, r âˆˆ {a, b}. Then the left-hand side of (A.1) can be written as:
                               X X                                             X                   X a,b
     N âˆ’1/2 b0 Qa âˆ’ E b0 Qa =               cij (uai mbi + mai ubi + uai ubi ) âˆ’   cii â„¦a,b = N âˆ’1/2
                        
                                                                                                      Di ,
                                         i     j                                 i                      i

where, using the fact that cij = cji :
        a,b
                                          X               X               X               X
     DN,i   = cii (uai ubi âˆ’ â„¦a,b ) + ubi   cij uaj + uai   cij ubj + ubi   cij maj + uai   cij mbj .       (A.2)
                                         j<i            j<i              j                 j

              a,b
{N âˆ’1/2 DN,i      , 1 â‰¤ i â‰¤ N } is a martingale-difference sequence with respect to the filtration FN,i =
Ïƒ(u1 , . . . , ui ). To apply a martingale central limit theorem, we need to verify that:
                N      h                i p
                          a,b 2
                X
           âˆ’1
       N              E (DN,i ) | FN,iâˆ’1 â†’ b0 V b                                                           (A.3)
                i=1




                                                      [22]
Expanding the expression yields:
         X h a,b             i       X                                       XXX
 N âˆ’1     E (DN,i )2 | Fn,iâˆ’1 = N âˆ’1   c2ii (â„¦a,a â„¦b,b + â„¦2a,b ) + â„¦b,b N âˆ’1     cij cik uaj uak
          i                                                i                                                       i   j<i k<i
                                                                   XXX                                                 XXX
                                            + â„¦a,a N âˆ’1                             cij cik ubj ubk + 2â„¦a,b N âˆ’1                          cij cik uaj ubk
                                                                   i j<i k<i                                            i       j<i k<i
                                                       0       0   2                       0        2                            0    2
                                            + â„¦b,b a M C M a/N + â„¦a,a b M C M b/N + 2â„¦a,b b M C M a/N
                                                         XXX                                  XXX
                                            + 2â„¦b,b N âˆ’1         cij cik mak uaj + 2â„¦a,b N âˆ’1     cij cik mbk uaj
                                                                   i   j<i     k                                            i    j<i   k
                                                                   XXX                                                  XXX
                                            + 2â„¦a,b N âˆ’1                             cij cik mak ubj + 2â„¦a,a N âˆ’1                           cij cik mbk ubj
                                                                   i   j<i     k                                            i    j<i   k
                                                                                                                                                    (A.4)

The last four terms are op (1) since their variance converges to zero. This follows from writing them
as:
                                          ï£«                         ï£¶

                      cij cik mpk urj =                 cij cjk mpk ï£¸ uri
           XXX                          X        X   X
      N âˆ’1                                ï£­N âˆ’1                           p, r âˆˆ {a, b}
               i    j<i      k                     i                     j>i    k

and noting that
     ï£«                                 ï£¶2                                                 ï£«                    ï£¶2
                                                                   âˆš                                                           âˆš
                          cij cjk mpk ï£¸ â‰¤ (max mpi / N )2 N âˆ’1                                               cik ï£¸ â‰¤ (max mpi / N )2 CM
X             XX                                                                    X         X          X
     ï£­N âˆ’1                                                                                ï£­        cij                                4
                                                                                                                                        â†’0
                                                 iâ‰¤N                                                                            iâ‰¤N
 i            j>i    k                                                                i        j         k



Now consider the terms of the form:

                    cij cik upj urk = N âˆ’1    c2ij upj urj + N âˆ’1     cij cik (upj urk + cij cik urj upk )
         XXX                               XX                     XXX
    N âˆ’1
               i    j<i k<i                                    i   j<i                             i     j<i k<j
                    ï£«          ï£¶
                1 X ï£­X 2   1 2ï£¸ p r                                              1 X 2 p r
                                                 cij cik (upj urk + urj upk ) âˆ’
                                             XXX
              =       cij + cii uj uj + N âˆ’1                                        cii ui ui
                N          2                                                    2N
                         j       i>j                                           i     j<i k<j                                                 i
               1              1 X 2 p r
              = Ï„C 2 p0 â„¦r âˆ’     cii ui ui + op (1)
               2             2N
                                             i




                                                                         [23]
The last line follows from applying Chebyshev inequality to the first two terms, and noting that:
                                                                                             ï£«              ï£¶2
                  P P                                                                             1
                                           + 12 c2ii upj urj = var(upj urj ) Â· N âˆ’2
                                                                                         X    X
     var       1
               N        j
                                      2
                                 i>j cij
                                                                                             ï£­  c2ij + c2ii ï£¸
                                                                                                      2
                                                                                         j     i>j

                                                               â‰¤   var(upj urj )N âˆ’2 tC 2 DC
                                                                    â†’0                     2
                                                                    ï£«          ï£¶2
                                              
                                           p r
                                                                X X  X
                                                    âˆ’2 0
                1
                                                      p â„¦pr0 â„¦p        cij cik ï£¸ â‰¤ O(N âˆ’2 DC
                                                                                           4
                       P P P
      var       N       i j<i k<j cij cik uj uk = N
                                                                    ï£­                        )â†’0
                                                                                     j   k<j    i>j

Pulling together the results yields:
                            h                      i
                                  a,b 2
                X
           âˆ’1
      N                 E       (DN,i )    | Fn,iâˆ’1 = b0 V b+
                   i
                                                  X
                                           N âˆ’1        c2ii (â„¦a,a b0 â„¦b + (â„¦a,b )2 âˆ’ â„¦a,a ubi ubi /2 âˆ’ b0 â„¦buai uai /2 âˆ’ â„¦a,b uai ubi )
                                                   i

This establishes (A.3), since the second term is op (1) as maxi c2ii /N â†’ 0.
                                                        a,b 4
   Secondly, it is possible to show that N âˆ’2 i E(DN,i
                                               P
                                                           ) â†’ 0, so that the Lindeberg condition
holds. Hence, a martingale central limit theorem applies, which yields the result.              
Lemma A.2.
Consider a sequence of random matrices {XN }âˆž                                âˆ’1
                                            N =1 such that XN âˆ¼ WS (JN , â„¦, â„¦ ÎžN ).
Suppose that ÎžN /N â†’ Îž, and that JN /N = Î± + o(N âˆ’1/2 ), Î± > 0. Then, for any vector
a âˆˆ RS

      N âˆ’1/2 (XN a/N âˆ’ (ÎžN /N + Î±â„¦)a)
                                           d
                                           â†’ N (0, (a0 â„¦aÎž + a0 Îžaâ„¦ + â„¦aa0 Îž + Îžaa0 â„¦) + Î±(a0 â„¦aâ„¦ + â„¦aa0 â„¦))
Proof. By definition of a non-central Wishart distribution, we can decompose XN = (U +M )0 (U +
M ), where U = (u1 , . . . , uJN )0 , uj âˆ¼ N (0, â„¦) iid, M 0 M = ÎžN , and ÎžN /JN â†’ Îž/Î±. Hence, we can
apply Lemma A.1 (ii) with C = IJN to get:

       âˆ’1/2
      JN           (XN a âˆ’ (ÎžN + JN â„¦)a)
                                               d
                                               â†’ N 0, Î±âˆ’1 (a0 â„¦aÎž + a0 Îžaâ„¦ + â„¦aa0 Îž + Îžaa0 â„¦) + a0 â„¦aâ„¦ + â„¦aa0 â„¦
                                                                                                               


which yields the result.                                                                                                                  
Lemma A.3.
Suppose Assumptions 1, 2(i), 3 and 4 hold. Then:
                    0                p
                YâŠ¥ YâŠ¥ /N â†’ Î¨ + (1 âˆ’ Î±L )â„¦                                                                                       (A.5a)
       0                             p
      YâŠ¥ PZâŠ¥ YâŠ¥ /N                  â†’ Î¨ + Î±K â„¦                                                                                  (A.5b)
         0                           p
      YâŠ¥ HZ YâŠ¥ /N                   â†’â„¦                                                                                          (A.5c)

                                                                        [24]
where
                                              
            Î›11 + 2Î›12 Î² + Î›22 Î² 2 Î›12 + Î›22 Î²
        Î¨=                                                                                             (A.6)
                 Î›12 + Î›22 Î²           Î›22

These probability limits also hold conditional on Z.

Proof. First we establish the probability limit of V0 PZâŠ¥ V/N . By Lemma A.1 (i):

        E[V0 PZâŠ¥ V/N | ZâŠ¥ ] = (KN /N )â„¦                                                                (A.7)

Fix a âˆˆ R2 . Since PZâŠ¥ is a projection matrix, 0 â‰¤ (PZâŠ¥ )ii â‰¤ 1. Hence,             2
                                                                          P                  P
                                                                            i (PZâŠ¥ )ii   â‰¤       i (PZâŠ¥ )ii   â‰¤
KN . Therefore:

        var(V0 PZâŠ¥ Va/N ) = E var(V0 PZâŠ¥ Va/N | PZâŠ¥ )
                          = E tr(PZâŠ¥ /N 2 ) (a0 â„¦aâ„¦ + â„¦aa0 â„¦)
                                          

                              + E N âˆ’2 i (PZâŠ¥ )2ii [E(a0 Vi )2 Vi Vi0 âˆ’ a0 â„¦aa0 â„¦ âˆ’ a0 â„¦aâ„¦]
                                       P          
                                                                                                       (A.8)
                            KN                       KN
                          â‰¤ 2 (a0 â„¦aâ„¦ + â„¦aa0 â„¦) + 2 [E(a0 vi )2 vi vi0 âˆ’ a0 â„¦aa0 â„¦ âˆ’ a0 â„¦aâ„¦]
                            N                        N
                                      2
                          = O(KN /N )

Combining Equations (A.7) and (A.8) with Assumption 3 yields :
                        p
        V0 PZâŠ¥ V/N â†’ Î±K â„¦                                                                              (A.9)

By similar arguments:
                        p
        V0 MW V/N â†’ (1 âˆ’ Î±L )â„¦                                                                        (A.10)

Next, by Assumption 2 (i), E[Î 01 Z0âŠ¥ V/N | ZâŠ¥ ] = 0, so that:

      var Î 01 Z0âŠ¥ Va/N = E var Î 01 Z0âŠ¥ Va/N | ZâŠ¥ = (a0 â„¦a)E Î 01 Z0âŠ¥ ZâŠ¥ Î 1 /N 2
                                                                          
                                      0 
                        = (a0 â„¦a)Î“âˆ’1 E Î›N /N 2 Î“âˆ’1 = O(1/N )
                                                 

where the last equality follows by Assumption 4. Consequently:
                    p
        Î 1 Z0âŠ¥ V/N â†’ 0                                                                                (A.11)

Combining the representation YâŠ¥ = ZâŠ¥ Î 1 + VâŠ¥ with the limits in Equations (A.10) and (A.11),
and Assumption 4 establishes (A.5a):
          0
        YâŠ¥ YâŠ¥ /N = Î 01 Z0âŠ¥ ZâŠ¥ Î 1 /N + Î 01 ZâŠ¥ V/N + V0 ZâŠ¥ Î 1 /N + V0 MW V/N
                   = Î“âˆ’1 Î›N Î“âˆ’1 /N + (1 âˆ’ Î±L )â„¦ + op (1)
                   = Î¨ + (1 âˆ’ Î±L )â„¦




                                                 [25]
Claim (A.5b) follows by similar arguments from Equations (A.9) and (A.11):
          0
      YâŠ¥ PZâŠ¥ YâŠ¥ /N = Î 01 Z0âŠ¥ ZâŠ¥ Î 1 /N + Î 01 ZâŠ¥ V/N + V0 ZâŠ¥ Î 1 /N + V0 PZâŠ¥ V/N
                          p
                          â†’ Î¨ + Î±K â„¦

Next we prove (A.5c). As an intermediate step, we need to find the probability limit of V0 HZ V.
Because HZ is symmetric, we can apply Lemma A.1 (i), so that:

      E[V0 HZ V/N ] = E tr(HZ /N )â„¦ = â„¦
                                     2 ), we have t = tr(M (I âˆ’ D )âˆ’2 ) = tr((I âˆ’ D )âˆ’1 ) â‰¤
since tr(HZ ) = N . Denoting t = tr(HZ                                                                                N
                                                          Z      Z                 Z                                 1âˆ’c
by Assumption 1. Moreover, i (HZ )2ii = i 12 = N . Hence, for any a âˆˆ RG+1 :
                             P            P

      var(V0 HZ Va/N ) = E var(V0 HZ Va/N | Z)
                                                                   "                #
                                                                    X
                                           0        0
                              = E[t] Â· (a â„¦aâ„¦ + â„¦aa â„¦)/N + E   2
                                                                         (HZ )2ii       Â· [E(a0 vi )2 vi vi0 âˆ’ a0 â„¦aa0 â„¦ âˆ’ a0 â„¦aâ„¦]/N 2
                                                                     i
                                 1
                              â‰¤     (a0 â„¦aâ„¦ + â„¦aa0 â„¦)/N + [E(a0 vi )2 vi vi0 âˆ’ a0 â„¦aa0 â„¦ âˆ’ a0 â„¦aâ„¦]/N
                                1âˆ’c
                              = O(N âˆ’1 )

Therefore, by Chebyshevâ€™s inequality:
          0                            p
      Y HZ Y/N = V0 HZ V/N â†’ â„¦                                                                                   (A.12)

Finally, the same calculations go through even if we condition on Z, so that the probability limits
hold also conditional on Z.                                                                      
Lemma A.4.
                                     p                                     1âˆ’Î±L               Î›22 /Î£22
Consider a k-class estimator with kÌ‚ â†’ k subject to k <                  1âˆ’Î±L âˆ’Î±K
                                                                                         +   1âˆ’Î±L âˆ’Î±K
                                                                                                       .   Then under
Assumptions 1, 2 (i), 3 and 4:

              p       Î›12 + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)Î£12
      Î²Ì‚kÌ‚ â†’ Î² +
                      Î›22 + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)Î£22

Proof. Combining Lemma A.3 with the condition kÌ‚ = k + op (1) yields:
                  0               0
     (1 âˆ’ kÌ‚)YâŠ¥ YâŠ¥ /N + kÌ‚YâŠ¥ PZâŠ¥ YâŠ¥ /N = (1 âˆ’ k)(Î¨ + (1 âˆ’ Î±L )â„¦) + k(Î¨ + Î±K â„¦) + op (1)
                                                                                                                 (A.13)
                                                = Î¨ + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)â„¦ + op (1)

The (2,2) element of (A.13) is given by:

      (1 âˆ’ kÌ‚)X0âŠ¥ XâŠ¥ /N + kÌ‚X0âŠ¥ PZâŠ¥ XâŠ¥ /N = Î›22 + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)Î£22 + op (1)

becausee Î£22 = â„¦22 . By the condition on k, Î›22 + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)Î£22 > 0, so that:
                                                âˆ’1
         (1 âˆ’ kÌ‚)X0âŠ¥ XâŠ¥ /N + kÌ‚X0âŠ¥ PZâŠ¥ XâŠ¥ /N            = (Î›22 + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)Î£22 )âˆ’1 + op (1)

                                                        [26]
                                                                                                        (A.14)

The (1,2) element in Equation (A.13) is given by:

       (1 âˆ’ kÌ‚)X0âŠ¥ YâŠ¥ /N + kÌ‚X0âŠ¥ PZâŠ¥ YâŠ¥ /N = Î›12 + Î›22 Î² + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)â„¦12 + op (1)
       = Î›12 + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)Î£12 + (1 âˆ’ Î±L âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )k)Î£22 Î² + Î›22 Î² + op (1)
                                                                                          (A.15)

Applying Equations (A.14) and (A.15) to Î²Ì‚kÌ‚ :

               (1 âˆ’ kÌ‚)X0âŠ¥ YâŠ¥ /N + kÌ‚x0âŠ¥ PZâŠ¥ YâŠ¥                  Î›12 + ((1 âˆ’ k)(1 âˆ’ Î±L ) + Î±K k)Î£12
   Î²Ì‚kÌ‚ =                                                = Î²+                                       + op (1). 
            (1 âˆ’   kÌ‚)X0âŠ¥ XâŠ¥ /N    +   kÌ‚X0âŠ¥ PZâŠ¥ XâŠ¥ /N           Î›22 + ((1 âˆ’ k)(1 âˆ’ Î±L ) + Î±K k)Î£22



Appendix B                         Proofs of Theorems
Proof of Theorem 1. The results for a general k-class estimator, btsls and mbtsls follows di-
rectly from Lemma A.4. We therefore just need to derive the results for liml, jive and mjive.
    First, we establish the result for liml. Define
                               0
                          Ï†0 YâŠ¥ YâŠ¥ /N Ï†
       QÌ‚N (Ï†) =           0                .
                        Ï†0 YâŠ¥ MZâŠ¥ YâŠ¥ /N Ï†
Then
                                       0
                           (1, âˆ’Î²Ìƒ)YâŠ¥ YâŠ¥ /N (1, âˆ’Î²Ìƒ)0
       kÌ‚liml = min                0                      = min QÌ‚N (Ï†)
                   Î²Ìƒ    (1, âˆ’Î²Ìƒ)YâŠ¥ MZâŠ¥ YâŠ¥ /N (1, âˆ’Î²Ìƒ)0         Ï†âˆˆS 1


where S 1 denotes the unit circle in R2 . Applying Lemma A.3 yields:

                   p    Ï†0 (Î¨ + (1 âˆ’ Î±L )â„¦)Ï†   Ï†0 T Ï†
       QÌ‚N (Ï†) â†’                             â‰¡         â‰¡ Q(Ï†)
                        (1 âˆ’ Î±L âˆ’ Î±K )Ï†0 â„¦Ï†    Ï†0 TâŠ¥ Ï†

where we define T = Î¨ + (1 âˆ’ Î±L )â„¦ and TâŠ¥ = (1 âˆ’ Î±L âˆ’ Î±K )â„¦. Assumption 2 (i) guarantees that
the denominator is non-zero for any value of Ï†. The minimum of Q(Ï†) is achieved at:

                             1 âˆ’ Î±L           1          Ï†0 Î¨Ï†
       min Q(Ï†) =                     +              min 0
       Ï†âˆˆS 1              1 âˆ’ Î±K âˆ’ Î±L 1 âˆ’ Î±L âˆ’ Î±K Ï†âˆˆS 1 Ï† â„¦Ï†
                             1 âˆ’ Î±L     min eig(Î£âˆ’1 Î›)
                        =             +                = kliml
                          1 âˆ’ Î±K âˆ’ Î±L   1 âˆ’ Î±K âˆ’ Î±L

where the last line follows since the eigenvalues of â„¦âˆ’1 Î¨ correspond to the eigenvalues of Î£âˆ’1 Î›.
The minimand Ï†liml is given by the eigenvector corresponding to the smallest eigenvalue of the
matrix:
               1
                           â„¦âˆ’1 (Î¨ + (1 âˆ’ Î±L )â„¦)
       1 âˆ’ Î±K âˆ’ Î±L

                                                         [27]
We now need to show that:
                                                   p
      kÌ‚liml âˆ’ kliml = min QÌ‚N (Ï†) âˆ’ Q(Ï†liml ) â†’ 0                                                (A.1)
                       Ï†âˆˆS 1

To this end, we first show that the convergence of the objective function is uniform:
                               p
      sup |QÌ‚N (Ï†) âˆ’ Q(Ï†)| â†’ 0                                                                    (A.2)
      Ï†âˆˆS 1

Fix Ï† âˆˆ S 1 . By triangle inequality:

                                   1                0                      0
|QÌ‚N (Ï†) âˆ’ Q(Ï†)| â‰¤         0                 Ï†0 YâŠ¥ YâŠ¥ Ï†/N âˆ’ Q(Ï†)Ï†0 YâŠ¥ MZâŠ¥ YâŠ¥ Ï†/N
                      |Ï†0 YâŠ¥ MZâŠ¥ YâŠ¥ Ï†/N |
                                   1              0
                                                                         
                                                                           0
                                                                                           
                  =        0                 Ï†0 (YâŠ¥ YâŠ¥ /N âˆ’ T )Ï† âˆ’ Q(Ï†)Ï†0 YâŠ¥ MZâŠ¥ YâŠ¥ /N âˆ’ TâŠ¥ Ï†
                      |Ï†0 YâŠ¥ MZâŠ¥ YâŠ¥ Ï†/N |
                               1          
                                                       0
                                                                               
                                                                                 0
                                                                                                  
                  â‰¤        0                      Ï†0 (YâŠ¥ YâŠ¥ /N âˆ’ T )Ï† + Q(Ï†) Ï†0 YâŠ¥ MZâŠ¥ YâŠ¥ /N âˆ’ TâŠ¥ Ï†
                      |Ï†0 YâŠ¥ MZâŠ¥ YâŠ¥ Ï†/N |
                                                                                                  (A.3)

We now need to bound all three terms in the expression uniformly in Ï†. Because the trace operator
is the inner product under Frobenius norm, by Cauchy-Schwarz inequality:
                                                                 
             0                             0
       |Ï†0 (YâŠ¥ MZâŠ¥ YâŠ¥ /N âˆ’ TâŠ¥ )Ï†| = tr (YâŠ¥ MZâŠ¥ YâŠ¥ /N âˆ’ TâŠ¥ )Ï†Ï†0
                                    p               0
                                  â‰¤ tr((Ï†Ï†0 )2 )k(YâŠ¥ MZâŠ¥ YâŠ¥ /N âˆ’ TâŠ¥ )kF
                                             0
                                       = k(YâŠ¥ MZâŠ¥ YâŠ¥ /N âˆ’ TâŠ¥ kF
                                       = op (1)
                                                                                    0         p
where the third line follows since kÏ†k2 = 1, and the last line follows since YâŠ¥ MZâŠ¥ YâŠ¥ /N â†’ TâŠ¥ by
Lemma A.3. By similar argument
              0
      |Ï†0 (YâŠ¥ YâŠ¥ /N âˆ’ T )Ï†| = op (1)
                                                         0             p                  0
Finally, we bound the denominator. Because YâŠ¥ MZâŠ¥ YâŠ¥ /N â†’ TâŠ¥ > 0, Ï†0 YâŠ¥ MZâŠ¥ YâŠ¥ Ï†/N > 0
                          0
wpa1, so that wpa1 |Ï†0 YâŠ¥ MZâŠ¥ YâŠ¥ Ï†/N | < C for some C < âˆž. Applying these bounds and the
fact that Q(Ï†) is bounded implies that the right-hand side in (A.3) is op (1), which implies (A.2).
    Next, denote the argmin of QÌ‚N (Ï†) by Ï†Ì‚. Note that kÌ‚liml and hence Ï†Ì‚ exists wpa1. We can now
establish (A.1), using the uniform convergence result (A.2):

      Q(Ï†liml ) â‰¤ Q(Ï†Ì‚) = QÌ‚N (Ï†Ì‚) + (Q(Ï†Ì‚) âˆ’ QÌ‚N (Ï†Ì‚)) â‰¤ QÌ‚(Ï†liml ) + (Q(Ï†Ì‚) âˆ’ QÌ‚n (Ï†Ì‚))
                          = Q(Ï†liml ) + (QÌ‚N (Ï†liml ) âˆ’ Q(Ï†liml )) + (Q(Ï†Ì‚) âˆ’ QÌ‚N (Ï†Ì‚))
                          = Q(Ï†liml ) + op (1)

The probability limit for liml then follows by Lemma A.4.



                                                        [28]
   It remains to establish the results for jive and mjive. Applying Lemma A.3, we get:
                           0                  p
                         Y (MW âˆ’ HZ )Y/N â†’ Î¨ âˆ’ Î±L â„¦                                                (A.4)
        0                                     p
      Y (MW âˆ’ (1 âˆ’ LN /N )HZ )Y/N â†’ Î¨                                                              (A.5)

Because Î›22 > Î±L Î£22 , it follows from the (2,2) element of (A.4) that:

                         (X0 (MW âˆ’ HZ )X)âˆ’1 = (Î›22 âˆ’ Î±L Î£22 ) + op (1)
      (X0 (MW âˆ’ (1 âˆ’ LN /N )HZ )X)âˆ’1 = Î›22 + op (1)

Combining these with an expansion of the (2,1) element in (A.4) and (A.5) yields the results for
jive and mjive.                                                                               

Proof of Theorem 2. All probability statements are conditional on Z. We omit the condition-
ing for ease of notation.
    Proof of part (i) The liml estimator is given by the minimand of the objective function:

                          (YâŠ¥ âˆ’ XâŠ¥ Î²Ìƒ)0 (YâŠ¥ âˆ’ XâŠ¥ Î²Ìƒ)
      QÌ‚N (Î²Ìƒ) =
                        (YâŠ¥ âˆ’ XâŠ¥ Î²Ìƒ)0 MZâŠ¥ (YâŠ¥ âˆ’ XâŠ¥ Î²Ìƒ)

The associated first-order condition is proportional to gÌ‚N (Î²Ì‚liml ) = 0, where

                         1 0                 QÌ‚N (Î²Ìƒ) 0
      gÌ‚N (Î²Ìƒ) = âˆ’         XâŠ¥ (YâŠ¥ âˆ’ XâŠ¥ Î²Ìƒ) +         XâŠ¥ MZâŠ¥ (YâŠ¥ âˆ’ XâŠ¥ Î²Ìƒ)
                         N                     N
The derivative of the first-order condition is given by:

     0          X0âŠ¥ XâŠ¥                                   2gÌ‚N (Î²Ìƒ)
   gÌ‚N (Î²Ìƒ) =          âˆ’ QÌ‚N (Î²Ìƒ)X0âŠ¥ MZâŠ¥ XâŠ¥ +                               X0âŠ¥ MZâŠ¥ (YâŠ¥ âˆ’ XâŠ¥ Î²Ìƒ)
                  N                                       0
                                              (YâŠ¥ âˆ’ XâŠ¥ Î²Ìƒ) MZâŠ¥ (YâŠ¥ âˆ’ XâŠ¥ Î²Ìƒ)
                                                     p
We will show that for any estimator Î²Ì‚ with Î²Ì‚ â†’ Î²:
        0       p
      gÌ‚N (Î²Ì‚) â†’ Î›22                                                                               (A.6)

Secondly, we will show that at the true value:
     âˆš
                                                                 
                  d                  Î±K (1 âˆ’ Î±L )            2
                                                                
       N gÌ‚N (Î²) â†’ N 0, Î£12 Î›22 +                 Î£11 Î£22 âˆ’ Î£12                                    (A.7)
                                    1 âˆ’ Î±K âˆ’ Î±L

                       0 (Î²Ì‚) does not depend on Î² and it is positive, and since Î²Ì‚    p
Because the limit of gÌ‚N                                                           liml â†’ Î² is consistent
by Theorem 1, assertion ((i)) the theorem will follow (see Newey and McFadden, 1994)
   We first prove (A.6). Let Ï† = (1, âˆ’Î²). By Lemma A.3 and consistency of Î²Ì‚:

                  Ï†0 (Î¨ + (1 âˆ’ Î±L )â„¦)Ï†
                    p                          1 âˆ’ Î±L
      QÌ‚N (Î²Ì‚) â†’                   0
                                         =               â‰¡ kliml
                  (1 âˆ’ Î±L âˆ’ Î±K )Ï† â„¦Ï†        1 âˆ’ Î±L âˆ’ Î±K
                p                       1 âˆ’ Î±L
       gÌ‚N (Î²Ì‚) â†’ âˆ’(1 âˆ’ Î±L )Î£12 +                 (1 âˆ’ Î±K âˆ’ Î±L )Î£12 = 0
                                     1 âˆ’ Î±L âˆ’ Î±K


                                                     [29]
where we use the fact that Î›11 = Î›12 = 0 since Î³ = 0. Hence:
        0      p
      gÌ‚N (Î²Ì‚) â†’ Î›22 + (1 âˆ’ Î±L )Î£22 âˆ’ kÎ£22 + 0 = Î›22

which proves (A.6). It remains to show that gÌ‚N (Î²liml ) satisfies a central limit theorem. Let Î½Ìƒ =
Î½ âˆ’ %, where % = Î£12 /Î£11 be a projection of Î½ onto space orthogonal to . We have:
       âˆš                               0 MW 
                                                          
                       âˆ’1/2      0                  0
         N gÌ‚N (Î²) = N       Î½ MZ  0           âˆ’ X MW 
                                        MZ 
                                       0 MW 
                                                                    
                   = N âˆ’1/2 Î½Ìƒ 0 MZ  0         âˆ’ (ZâŠ¥ Ï€12 + Î½Ìƒ)0 MW 
                                        MZ 
                   = N âˆ’1/2 Î½Ìƒ 0 MZ  Â· kliml âˆ’ (ZâŠ¥ Ï€12 + Î½Ìƒ)0 MW  + op (1)
                                                                   

                                   0
where the third line follows since 0M W
                                      MZ  = kliml +op (1) by arguments in Lemma A.3, and N
                                                                                            âˆ’1/2 Î½Ìƒ 0 M 
                                                                                                       Z
is Op (1). Therefore, we can write:
       âˆš
         N gÌ‚N (Î²) = N âˆ’1/2 (ZâŠ¥ Ï€12 + Î½Ìƒ)0 kliml MZ âˆ’ MW 
                                                             


This expression is the (2,1) element of the quadratic form:
                            0
      N âˆ’1/2  ZâŠ¥ Ï€12 + Î½Ìƒ C  ZâŠ¥ Ï€12 + Î½Ìƒ
                                              


where C = kliml MZ âˆ’MW . To establish (A.7), we need to check the assumptions of Lemma A.1(ii).
We have:
                                                                Î±K (1 âˆ’ Î±L )
      tr(C) = o(N âˆ’1/2 )                                 Ï„C 2 =                                  (A.8a)
                                                                1 âˆ’ Î±L âˆ’ Î±K
                                                                                
                0 0                                      i       Î£11        0
      QCM =                                         cov       =                                  (A.8b)
                0 Î›22                                    Î½Ëœi       0 Î£22 âˆ’ Î£212 /Î£11

Applying Lemma A.1 (ii) then yields (A.7).
   Proof of part (ii) We can write:
      âˆš                                           âˆ’1                              
          N Î²Ì‚mbtsls âˆ’ Î² = X0 (MW âˆ’ kÌ‚mbtsls MZ )X/N     N âˆ’1/2 X0 (MW âˆ’ kÌ‚mbtsls MZ )

By Lemma A.3, we have:
                                      âˆ’1
          X0 (MW âˆ’ kÌ‚mbtsls MZ )X/N          = Î›22 + op (1)                                       (A.9)

The second term is a (2,1) element of the quadratic form:
                           0
      N âˆ’1/2  ZâŠ¥ Ï€12 + Î½ C  ZâŠ¥ Ï€12 + Î½
                                             


where C = (MW âˆ’ kÌ‚mbtsls MZ ). Applying Lemma A.1 (ii) with tr(C), Ï„C 2 and QCM given by




                                                     [30]
Equation (A.8), and cov(i , Î½i ) = Î£ then yields:
                                                                                  
       âˆ’1/2
            
               0
                                       
                                         d            Î±K (1 âˆ’ Î±L )             2
     N        X (MW âˆ’ kÌ‚mbtsls MZ ) â†’ N 0, Î£11 Î›22 +              (Î£11 Î£22 + Î£12 )
                                                      1 âˆ’ Î±L âˆ’ Î±K

Combining this result with (A.9) yields part ((ii)) in the Theorem.
  Proof of Part (iii) Write the estimator as:
    âˆš              
      N Î²Ì‚mjive âˆ’ Î² = (X0 (MW âˆ’ (1 âˆ’ LN /N )HZ )X/N )âˆ’1 N âˆ’1/2 X0 (MW âˆ’ (1 âˆ’ LN /N )HZ ).

By Lemma A.3, the first term satisfies:

      (X0 (MW âˆ’ (1 âˆ’ LN /N )HZ )X/N )âˆ’1 = Î›âˆ’1
                                           22 + op (1)                                                           (A.10)

The second term is the (2,1) element of:
                           0
      N âˆ’1/2  ZâŠ¥ Ï€12 + Î½ C  ZâŠ¥ Ï€12 + Î½
                                         


where C = MW âˆ’ (1 âˆ’ LN /N )HZ . Because tr(HZ (I âˆ’ DZ )âˆ’1 ) = tr((I âˆ’ DZ )âˆ’1 ), we have:

           tr(C) = N âˆ’ LN âˆ’ (1 âˆ’ LN /N )N = 0
      tr(C 2 /N ) = (LN /N âˆ’ 1) + (1 âˆ’ LN /N )2 tr((I âˆ’ DZ )âˆ’1 /N )
                       p
                      â†’ (Î±L âˆ’ 1) + (1 âˆ’ Î±L )2 Ï„

QCM is given by Equation (A.8), and cov(i , Î½i ) = Î£. Moreover, by Assumption 1:
                    N
                    X                      N
                                           X
      sup max         |cij | â‰¤ 1 + sup max   |(PW )ij |
       N   iâ‰¤N                       N       iâ‰¤N
                    j=1                            j=1
                                                                  N
                                                                  X
                                 + (1 âˆ’ LN /N ) sup max             |(Iij âˆ’ (PW )ij âˆ’ (PZâŠ¥ )ij )||((I âˆ’ DZ )âˆ’1 )jj |
                                                     N    iâ‰¤N
                                                                  j=1
                                             CP
                                â‰¤ 1 + CP +        <âˆž
                                           1 âˆ’ CD

Applying Lemma A.1 (ii) and combining it with (A.10) then yields the result.                                           

Proof of Theorem 3. Under Assumption 2, we have:

                   (Z0âŠ¥ ZâŠ¥ )âˆ’1/2 Z0âŠ¥ Y
                                                                                          
      âˆš                                                       Ï€Ìƒ12 Î² + Î³Ìƒ
          Î±K                                 Zâˆ¼N                                , Î±K â„¦ âŠ— IKN
                   (Z0âŠ¥ ZâŠ¥ )âˆ’1/2 Z0âŠ¥ X                            Ï€Ìƒ12
                            0
                           YâŠ¥ MZâŠ¥ YâŠ¥ | Z âˆ¼ W2 (N âˆ’ KN âˆ’ LN , â„¦)

Moreover, these two statistics are independent. Let b = (1, âˆ’Î²)0 and a = (Î², 1). Assumption 5




                                                                 [31]
then implies that unconditionally:
                                                               âˆ’1
         0                      0                 0
        YâŠ¥ PZâŠ¥ YâŠ¥ âˆ¼ W2 (KN , Î“âˆ’1 ÎžÎ“âˆ’1 /Î±K + â„¦, Î“âˆ’1 ÎžÎ“âˆ’1 /Î±K + â„¦     KN aa0 Âµ2Ï€ /Î±K )
            0
        YâŠ¥ MZâŠ¥ YâŠ¥ âˆ¼ W2 (N âˆ’ KN âˆ’ LN , â„¦)

with the independence property preserved. Applying Lemma A.2 then after some algebra yields:
                                                    d
        N 1/2 X0âŠ¥ MZâŠ¥ YâŠ¥ b/N âˆ’ (1 âˆ’ Î±K âˆ’ Î±L )Î£12 â†’ N (0, (1 âˆ’ Î±K âˆ’ Î±L )VÎ£ )                               (A.11a)
                                                    d
                  N 1/2 X0âŠ¥ PZâŠ¥ YâŠ¥ /N b âˆ’ (Î±K Î£12 ) â†’ N (0, Î±K VÎ£ + VÎž )                                  (A.11b)

where

        VÎ£ = Î£22 Î£11 + Î£212
                                  âˆ’1
        VÎž = Î›22 Î£11 + Î›11 Î£22 + Î±K  Î›22 Î›11

Equations (A.11) imply:
                                                                                                          
                                                                               d           Î±K (1 âˆ’ Î±L )
            1/2
                    X0âŠ¥ PZâŠ¥ YâŠ¥ /N            kmbtsls )X0âŠ¥ MZâŠ¥ YâŠ¥ /N
                                                                         
        N                           + (1 âˆ’                                   b â†’ N 0, VÎž +              VÎ£
                                                                                           1 âˆ’ Î±K âˆ’ Î±L
                                                                                          p
Because, by Lemma A.3, (X0âŠ¥ PZâŠ¥ XâŠ¥ N + (1 âˆ’ kmbtsls )X0âŠ¥ MZâŠ¥ XâŠ¥ /N )âˆ’1 â†’ Î›âˆ’1
                                                                          22 + op (1), this yields
the claim in the theorem.
    Now consider mjive. Write the estimator as:
 âˆš              
  N Î²Ì‚mjive âˆ’ Î² = (X0 (MW âˆ’(1âˆ’LN /N )HZ )X/N )âˆ’1 N âˆ’1/2 X0 (MW âˆ’(1âˆ’LN /N )HZ )(ZâŠ¥ Î³+).

By Lemma A.3, the first term satisfies:

        (X0 (MW âˆ’ (1 âˆ’ LN /N )HZ )X/N )âˆ’1 = Î›âˆ’1
                                             22 + op (1)                                                   (A.12)

Let where Ëœ = (Z0âŠ¥ ZâŠ¥ )âˆ’1/2 Z0âŠ¥  and Î½Ìƒ = (Z0âŠ¥ ZâŠ¥ )âˆ’1/2 Z0âŠ¥ Î½. The second term can be rewritten as:

 N âˆ’1/2 X0 (MW âˆ’ (1 âˆ’ LN /N )HZ )(ZâŠ¥ Î³ + )
                = N âˆ’1/2 Ï€12
                          0
                             Z0âŠ¥ ZâŠ¥ Î³ + Ï€12
                                         0
                                            Z0âŠ¥  + Î³ 0 Z0âŠ¥ Î½ + Î½ 0 (MW âˆ’ (1 âˆ’ LN /N )HZ )
                                                                                           
                                                                                                     
                            âˆ’1/2             âˆ’1/2
                = N âˆ’1/2 (Î±K Ï€Ìƒ12 + Î½Ìƒ)0 (Î±K Ï€Ìƒ12 + Ëœ) + Î½ 0 ((I âˆ’ (1 âˆ’ LN /N )(I âˆ’ DZ )âˆ’1 )MW MZâŠ¥ )


Because (Ëœ, Î½Ìƒ) is independent of MZâŠ¥ (, Î½), the two terms are independent. The distribution of the
first term is given by the (2,1) element of a random variable with distribution
                                                                  
                            âˆ’1              âˆ’1         0     0
        W2 KN , â„¦ +        Î±K  Îž, (â„¦   +   Î±K  Îž)âˆ’1        KN 2
                                                       0   Î±K Âµ Ï€




                                                           [32]
so that by Lemma A.2:
                                                    
               âˆ’1/2            âˆ’1/2
      N âˆ’1/2 (Î±K Ï€Ìƒ12 + Î½Ìƒ)0 (Î±K Ï€Ìƒ12 + Ëœ) âˆ’ KN â„¦12
                                         d
                                         â†’ N 0, Î£11 Î›22 + Î±K (Î£11 Î£22 + Î£212 ) + Î›11 (Î£11 + Î›22 /Î±K )
                                                                                                     



Applying Lemma A.1(ii) to the second term yields:

      N âˆ’1/2 Î½ 0 ((I âˆ’ (1 âˆ’ LN /N )(I âˆ’ DZ )âˆ’1 )MW MZâŠ¥ ) + KN â„¦12
                                                                               

                                                 d
                                                 â†’ N 0, (Î±L âˆ’ 1 âˆ’ Î±K + (1 âˆ’ Î±L )2 Ï„ )(Î£11 Î£22 + Î£212 )
                                                                                                      



Adding the variances of these limit distributions yields the result.                                      
                                                p                       p
Proof of Theorem 4. Because Î³Ìƒk =                q(1 âˆ’ q)Î³k and Ï€Ìƒ12,k = q(1 âˆ’ q)Ï€12,k , we can write
the estimator as:
                                               P                                         
                       1
                                      q(1 âˆ’ q) N1q i : Qi =1 i âˆ’ N (1âˆ’q)
                                                                     1
                           P        p                                     P
                      KN      k Î³Ìƒk +                                       i : Qi =0  i
      Î²Ì‚wald = Î² +                               P                                         
                      1                          1                     1
                          P          p                                     P
                     KN      Ï€Ìƒ
                            k 12,k +   q(1 âˆ’ q)  Nq           Î½
                                                     i : Qi =1 i âˆ’ N (1âˆ’q)               Î½
                                                                               i : Qi =0 i

By law of large numbers, we have:
                                ï£«                          ï£¶
       1 X            p           1  X          1     X      p
              Ï€Ìƒ12,k + q(1 âˆ’ q) ï£­      Î½i âˆ’             Î½i ï£¸ â†’ ÂµÏ€
      KN                          Nq        N (1 âˆ’ q)
            k                              i : Qi =1               i : Qi =0

Therefore, because ÂµÏ€ 6= 0:
                                                               âˆš
                          ï£«p                                                        ï£¶
  âˆš 
                                        p
                      1 ï£­ N/KN  X       (1 âˆ’ q)  X               q        X
    N Î²Ì‚wald âˆ’ Î² =          âˆš      Î³Ìƒk + âˆš              i âˆ’ p                   i ï£¸ + op (1)
                      ÂµÏ€      KN           N q i : Q =1       N (1 âˆ’ q)
                                 k                                 i    i : Q =0                i


All three terms are Normally distributed and mutually independent. Adding up the variances yields
the result.                                                                                    

Proof of Theorem 5. Denote the matrices of instruments and exogenous regressors in the
model (5.7) by âˆ¼, so that WÌƒ = [Q, W], where Q is an N -vector of basic instruments, ZÌƒ is
the matrix of first KN âˆ’ 1 columns of Z, and ZÌƒâŠ¥ = MWÌƒ ZÌƒ. Then PWÌƒ = PW + PQâŠ¥ , where
           (Q âˆ’q)(Qj âˆ’q)
(PQâŠ¥ )ij = iN q(1âˆ’q)     . Note that Z remains the same.
   Let Î½Ì„k = KNN i : Gi =k Î½i denote group averages, let Î½Ì„1,k = K
                  P                                                   P
                                                                          i : Qi =1,Gi =k Î½i denote group
                                                                    N
                                                                  qN
                                                        KN P
averages for individuals with Qi = 1, and let Î½Ì„0,k = (1âˆ’q)N i : Qi =0,Gi =k Î½i denote group averages




                                                       [33]
for individuals with Qi = 0. Define:
                      KN     X                                                                             KN     X
      Î£Ì‚12,k =                         Î½i i âˆ’ Î½Ì„k Â¯k                                     Î£Ì‚22,k =                         Î½i2 âˆ’ Î½Ì„k2
                      N                                                                                    N
                           i : Gi =k                                                                            i : Gi =k
                   KN                Q âˆ’q
                                    p i                                                      sÎ³,
                             X                    p
         10,k   =                            i = q(1 âˆ’ q)(Â¯
                                                            1,k âˆ’ Â¯0,k )                    k = Î³Ìƒk âˆ’ ÂµÎ³ + 10,k
                   N                 (1 âˆ’ q)q
                           i : G =k
                               i


Some tedious algebra shows that the mbtsls estimator is given by:

                        (1 âˆ’ kÌ‚mbtsls )X0 MWÌƒ Y + kÌ‚mbtsls X0 PZÌƒâŠ¥ Y
         Î²Ì‚mbtsls =
                      (1 âˆ’ kÌ‚mbtsls )X0 MWÌƒ X + kÌ‚mbtsls X0 PZÌƒâŠ¥ X
                                                                                             
                            1 P          Î³, Ï€12 ,Î½                                                     P P Ï€12 ,Î½ Î³,
                          KN      k    s k ks       + (1 âˆ’ kÌ‚ mbtsls )(Î£Ì‚ 12,k âˆ’     Î½
                                                                                  10,k 10,k )   âˆ’ K12    k     l sl    sk
                                                                                                    N
                     =Î²+                                                               
                              1 P           Ï€12 ,Î½ 2                               2 ) âˆ’ 1
                                                                                                  P P Ï€12 ,Î½ Ï€12 ,Î½
                             KN      k (sk        ) + (1 âˆ’ kÌ‚mbtsls )(Î£Ì‚22,k âˆ’ Î½10,k           K2     k   l sl      sk
                                                                                                       N


By the weak law of large numbers, we have:
         1 X         p                                                       1 X 2      p
              Î£Ì‚22,k â†’ (1 âˆ’ Î±K )Î£22                                               Î½10,k â†’ Î±K Î£22                                  (A.13a)
        KN                                                                 KN
            k                                                                  k
       1 X Ï€12 ,Î½ 2 p                                                       1 X Ï€12 ,Î½ p
           (sk ) â†’ Îž22 + Î±K Î£22                                                  sk     â†’0                                       (A.13b)
      KN                                                                   KN
                 k                                                                  k

Hence:
   P                                                                                                âˆ’1
  1
              (sÏ€k 12 ,Î½ )2 + (1 âˆ’ kÌ‚mbtsls )(Î£Ì‚22,k âˆ’ Î½10,k
                                                        2 ) âˆ’         1                Ï€12 ,Î½ Ï€12 ,Î½
                                                                           P P
 KN       k                                                          KN2        k   l sl     sk              = Îž22 +op (1) (A.14)

The nominator can be written as:

       1 X  Î³, Ï€12 ,Î½                                           1 X X Ï€12 ,Î½ Î³,
            sk sk       + (1 âˆ’ kÌ‚mbtsls )(Î£Ì‚12,k âˆ’ 10,k Î½10,k ) âˆ’ 2      sl    sk =
      KN                                                          KN
         k                                                            k l
                          1 X                     1 X Î³, Ï€12 ,Î½      1 X
                                  Dk,kÌ‚mbtsls âˆ’ 2          sk sk   =      Dk,kÌ‚mbtsls + Op (1/KN )
                         KN                      KN                  KN
                                               k                     k                           k



where:
                                                                                      1 X Ï€12 ,Î½ Î³,
      Dk,kÌ‚mbtsls = sÎ³, Ï€12 ,Î½
                                                                                                sk + sÏ€k 12 ,Î½ sÎ³,
                                                                                                                    
                     k sk       + (1 âˆ’ kÌ‚mbtsls )(Î£Ì‚12,k âˆ’ 10,k Î½10,k ) âˆ’                sl                    l
                                                                                     KN
                                                                                           l<k

                                                                         âˆ’1/2
Note that under the Assumption that Îž12 = 0, {KN Dk,kÌ‚mbtsls }kâ‰¥1 is a martingale difference
sequence with respect to the filtration Fk = Ïƒ(Î³k , Ï€12,k , {i : Gi = k}, {Î½i : Gi = k}).




                                                              [34]
    The next step is to show that:
       âˆš                                                                                   
        NX             d                                        (1 âˆ’ Î±K )Î±K            2
                                                                                          
           Dk,kÌ‚mbtsls â†’ N 0, Îž11 Îž22 /Î±K + Îž11 Î£22 + Îž22 Î£11 +             Î£11 Î£22 + Î£12
       KN                                                        (1 âˆ’ 2Î±K )
                  k
                                                                                                                      (A.15)

by applying the martingale central limit theorem. The claim of the theorem for mbtsls will then
follow by combining (A.15) with (A.14). To show (A.15), we first need to check that:

        KN                                                                            2
      1 X     2                      p                                      (1 âˆ’ Î±K )Î±K
                                                                                        Î£11 Î£22 + Î£212
                                                                                                       
           E[Dk,          | FN,kâˆ’1 ] â†’ Îž11 Îž22 + Î±K (Îž11 Î£22 + Îž22 Î£ 11 ) +
     KN         kÌ‚ mbtsls                                                    (1 âˆ’ 2Î±K )
            k=1
                                                                                                                      (A.16)

Expanding the left-hand side yields:

   2                         KN             KN             KN      1 X X Ï€12 ,Î½ Î³,
E[Dk,kÌ‚
                | FN,kâˆ’1 ] = Îž11 Îž22 +
                                Îž11 Î£22 +      Îž22 Î£11 + 2     Î£12 2         sl sm
       mbtsls                N               N             N      KN
                                                                     l<k m<k
                KN       1 X X Ï€12 ,Î½ Ï€12 ,Î½          KN        1 X X Î³, Î³,
       + (Îž11 +    Î£11 ) 2      sl   sm + (Îž22 +          Î£22 ) 2        sl sm
                N       KN                             N       KN
                                       l<k m<k                                             l<k m<k
                                                               2
          + 2kÌ‚mbtsls (1 âˆ’ kÌ‚mbtsls )E10,k Î½10,k Î£Ì‚12,k +   kÌ‚mbtsls (KN /N )2 (Î£11 Î£22   + 2Î£212 )   + (1 âˆ’ kÌ‚mbtsls )2 EÎ£Ì‚212,k
                                                                                                                      (A.17)

where the expectations in the last line equal
                                  KN       KN                  KN 2
                      EÎ£Ì‚212,k =     (1 âˆ’      )Î£11 Î£22 + (1 âˆ’   )Î£12
                                  N         N                  N
                                  KN 2
       10,k Î½10,k Î£12,k
      EË†                        =      Î£11 Î£22 + Î±Î£212
                                  N
We can therefore write:
          KN
        1 X     2                                 KN
             E[Dk,kÌ‚mbtsls
                           | FN,kâˆ’1 ] = Îž11 Îž22 +    Îž11 Î£22
       KN                                         N
              k=1
         KN           (1 âˆ’ KN /N )(KN /N )2                     KN      1 X X X Ï€12 ,Î½ Î³,
                                             Î£11 Î£22 + Î£212 + 2
                                                           
       +    Îž22 Î£11 +                   2
                                                                   Î£12 3            sl  sm
         N               (1 âˆ’ 2(KN /N ))                        N      KN
                                                                          k l<k m<k
                   KN        1 X X X Ï€12 ,Î½ Ï€12 ,Î½             KN       1 X X X Î³, Î³,
         + (Îž11 +     Î£11 ) 3             sl   sm + (Îž22 +        Î£22 ) 3           sl sm
                    N      KN                                   N      KN
                                          k   l<k m<k                                              k     l<k m<k



Now, for a, b âˆˆ {(Î³, ), (Ï€12 , Î½)}, note that:
        1 XX X b a    1 X                  1 X          X
         3    sl sm = 3  (KN âˆ’ l)sbl sal + 3   (KN âˆ’ l)   (sbl sam + sbm sal ) = op (1)
       KN            KN                   KN
               k      l<k m<k                 l                            l               m<l


                                                             [35]
                                                                                         âˆ’2             P         4
Therefore, the last three terms are op (1), which proves (A.16). One can also show that KN                  k   EDk,kÌ‚
                                                                                                                              â†’
                                                                                                                     mbtsls
0, which implies (A.15). Next consider the mjive estimator. Let

                  KN     X                               KN     X
      Î£Ì‚112,k =                    Qi vi i   Î£012,k =                    (1 âˆ’ Qi )vi i
                  N                                      N
                       i : Gi =k                              i : Gi =k
                  q                                   1âˆ’q                                    
      t12
       k =                Î£Ì‚112,k âˆ’ qÎ½Ì„1,k Â¯1,k +                 Î£Ì‚012,k âˆ’ (1 âˆ’ q)Î½Ì„0,k Â¯0,k
              q âˆ’ KN /N                            1 âˆ’ q âˆ’ KN /N

Then we can write the mjive estimator as:
                                                        âˆ’1
                                 KN
               X MWÌƒ Y âˆ’ 1 âˆ’ N X0 MZ I âˆ’ DZ
                0                                              Y
     Î²Ì‚mjive =                       
                                                            âˆ’1
               X0 MWÌƒ X âˆ’ 1 âˆ’ KNN X0 MZ I âˆ’ DZ
                                                          
                                                               X
                                                                              
                    1 P      Î³, Ï€12 ,Î½                               KN 12          âˆ’2 P P Î³, Ï€,Î½
                   KN   k   sk ks       + Î£Ì‚ 12,k âˆ’     Î½
                                                     10,k 10,k âˆ’ (1 âˆ’  N  )t k   âˆ’ KN     k     l sk sl
             =Î²+                                                           
                                 Ï€12 ,Î½ 2                                         âˆ’2 P P Ï€,Î½ Ï€,Î½
                      1 P
                     KN    k (sk
                                                       2
                                       ) + Î£Ì‚22,k âˆ’ Î½10,k   âˆ’ (1 âˆ’ KNN )t22
                                                                         k     âˆ’ KN     k   l sk sl

                                                                                               p
Using (A.13) and the fact that by the weak law of large numbers t22
                                                                 k â†’ Î£22 , we get:

                                                                       !âˆ’1
           1 X     Ï€12 ,Î½ 2        2           KN 22      âˆ’2
                                                             X X Ï€,Î½ Ï€,Î½
                 (sk ) + Î£Ì‚22,k âˆ’ Î½10,k âˆ’ (1 âˆ’   )t    âˆ’ KN     sk sl        = Îž22 + op (1)
          KN                                   N k
              k                                                                        k       l
                                                                                                            (A.18)

We can rewrite the nominator as:
                                                     
 1 X Î³, Ï€12 ,Î½                                 KN 12   âˆ’2
                                                           X X Î³, Ï€,Î½  1 X           âˆ’1
         sk sk    + Î£Ì‚12,k âˆ’ 10,k Î½10,k âˆ’ (1 âˆ’   )tk âˆ’KN     sk sl =       DÌƒk +op (KN  )
KN                                              N                      KN
      k                                                                            k       l        k


where:
                               KN 12
      DÌƒk = Dk,0 âˆ’ (1 âˆ’          )t
                               N k
                                                                                       âˆ’1/2
Like in the case of mbtsls, under the Assumption that Îž12 = 0, {KN Dk,kÌ‚mbtsls }kâ‰¥1 is a martingale
difference sequence with respect to the filtration Fk = Ïƒ(Î³k , Ï€12,k , {i : Gi = k}, {Î½i : Gi = k}). To
prove the claim of the theorem for mjive, it therefore remains to check that:

         KN
       1 X                    p
            E[DÌƒk2 | FN,kâˆ’1 ] â†’
      KN
            k=1
             Îž11 Îž22 + Î±K (Îž11 Î£22 + Îž22 Î£11 ) + Î±K (1 âˆ’ Î±K )((1 âˆ’ Î±K )Ï„ âˆ’ 1) Î£11 Î£22 + Î£212
                                                                                                        
                                                                                                            (A.19)




                                                          [36]
and that:
             X
      âˆ’2
     KN          EDÌƒk4 â†’ 0                                                                      (A.20)
             k

                                                                                     âˆ’1         P
These two conditions will allow us to apply the martingale central limit theorem to KN           k   DÌƒk .
We first establish (A.19). Using the expansion in (A.17), we get that:
     KN
   1 X
        E[DÌƒk2 | FN,kâˆ’1 ] = Îž11 Îž22 + Î±K (Îž11 Î£22 + Îž22 Î£11 ) + Î±K (1 âˆ’ Î±K )Î£11 Î£22 + (1 âˆ’ Î±K )Î£212
  KN
       k=1
                                + (1 âˆ’ KN /N )2 E[t12 12                         12
                                                   k tk ] âˆ’ 2(1 âˆ’ KN /N )E[Dk,0 tk ] + op (1)

The remaining expectations are given by:
                               KN
      E[Dk,0 t12
              k | FN,kâˆ’1 ] =       Î£11 Î£22 + Î£212
                                N
                                     q2           (1 âˆ’ q)2
                                                            
                                                               KN
       E[t12 12
          k tk    | FN,kâˆ’1 ] =              +                     (Î£11 Î£22 + Î£212 ) + Î£212
                                 1 âˆ’ KN /N     1 âˆ’ q âˆ’ KN /N   N

Substituting them in the expansion above yields (A.19). It can also be shown that (A.20) holds,
which proves the result.                                                                     




                                                 [37]
References
Ackerberg, D. A. and Devereux, P. J. (2009). Improved Jive estimators for overidentified
  linear models with and without heteroskedasticity. Review of Economics and Statistics, 91 (2),
  351â€“362.

Aizer, A. and Doyle, Jr., J. J. (2011). Effects of Juvenile Incarceration: Evidence from
  Randomly-Assigned Judges, unpublished manuscript.

Anatolyev, S. (2011). Instrumental variables estimation and inference in the presence of many
  exogenous regressors, unpublished manuscript.

Anderson, T. W., Kunitomo, N. and Matsushita, Y. (2010). On the asymptotic optimality of
  the LIML estimator with possibly many instruments. Journal of Econometrics, 157 (2), 191â€“204.

â€” and Rubin, H. (1949). Estimation of the Parameters of a Single Equation in a Complete System
  of Stochastic Equations. The Annals of Mathematical Statistics, 20 (1), 46â€“63.

Andrews, D. W. K., Moreira, M. J. and Stock, J. H. (2006). Optimal Two-Sided Invariant
  Similar Tests for Instrumental Variables Regression. Econometrica, 74 (3), 715â€“752.

Angrist, J. D., Imbens, G. W. and Krueger, A. B. (1999). Jackknife instrumental variables
  estimation. Journal of Applied Econometrics, 14 (1), 57â€“67.

â€” and Krueger, A. B. (1991). Does compulsory school attendance affect schooling and earnings?
  The Quarterly Journal of Economics, 106 (4), 979â€“1014.

â€” and Pischke, J.-S. (2009). Mostly Harmless Econometrics: An Empiricistâ€™s Companion.
  Princeton: Princeton University Press.

Ashley, R. (2009). Assessing the credibility of instrumental variables inference with imperfect
  instruments via sensitivity analysis. Journal of Applied Econometrics, 24 (2), 325â€“337.

Basmann, R. L. (1957). A generalized classical method of linear estimation of coefficients in a
  structural equation. Econometrica, 25 (1), 77â€“83.

Bekker, P. A. (1994). Alternative Approximations to the Distributions of Instrumental Variable
  Estimators. Econometrica, 62 (3), 657â€“681.

â€” and van der Ploeg, J. (2005). Instrumental variable estimation based on grouped data.
  Statistica Neerlandica, 59 (3), 239â€“267.



                                               [38]
Belloni, A., Chen, D., Chernozhukov, V. and Hansen, C. B. (2011). Sparse models
  and methods for optimal instruments with an application to eminent domain, unpublished
  manuscript.

Berkowitz, D., Caner, M. and Fang, Y. (2008). Are â€œNearly Exogenous Instrumentsâ€ reliable?
  Economics Letters, 101 (1), 20â€“23.

Caner, M. (2007). Near Exogeneity and Weak Identification in Generalized Empirical Likelihood
  Estimators: Many Moment Asymptotics, unpublished manuscript.

Chamberlain, G. and Imbens, G. W. (2004). Random Effects Estimators with Many Instru-
  mental Variables. Econometrica, 72 (1), 295â€“306.

Chao, J. C. and Swanson, N. R. (2005). Consistent estimation with a large number of weak
  instruments. Econometrica, 73 (5), 1673â€“1692.

â€”, â€”, Hausman, J. A., Newey, W. K. and Woutersen, T. (2010). Asymptotic Distribu-
  tion of JIVE in a Heteroskedastic IV Regression with Many Instruments. Econometric Theory,
  (forthcoming).

Chetty, R., Friedman, J. N., Hilger, N., Saez, E., Schanzenbach, D. W. and Yagan,
  D. (2011). How does your kindergarten classroom affect your earnings? Quarterly Journal of
  Economics, (forthcoming).

Chioda, L. and Jansson, M. (2009). Optimal Invariant Inference When the Number of Instru-
  ments Is Large. Econometric Theory, 25 (3), 793â€“805.

Conley, T. G., Hansen, C. B. and Rossi, P. E. (2007). Plausibly Exogenous, unpublished
  manuscript.

Davidson, R. and MacKinnon, J. G. (1993). Estimation and Inference in Econometrics. Oxford:
  Oxford University Press.

Donald, S. G. and Newey, W. K. (2001). Choosing the Number of Instruments. Econometrica,
  69 (5), 1161â€“1191.

Fisher, F. M. (1961). On the cost of approximate specification in simultaneous equation estima-
  tion. Econometrica, 29 (2), 139â€“170.

â€” (1966). The relative sensitivity to specification error of different k-class estimators. Journal of
  the American Statistical Association, 61 (314), 345â€“356.



                                                [39]
â€” (1967). Approximate Specification and the Choice of a k-Class Estimator. Journal of the Amer-
  ican Statistical Association, 62 (320), 1265â€“1276.

Flores, C. A. and Flores-Lagunes, A. (2010). Partial Identification of Local Average Treat-
  ment Effects with an Invalid Instrument, unpublished manuscript.

Fryer, R. G. (2011). Financial Incentives and Student Achievement: Evidence from Randomized
  Trials. Quarterly Journal of Economics, (forthcoming).

Gautier, E. and Tsybakov, A. B. (2011). High-dimensional instrumental variables regression
  and confidence sets, unpublished manuscript.

Guggenberger, P. (2010). On the Asymptotic Size Distortion of Tests When Instruments Locally
  Violate the Exogeneity Assumption, unpublished manuscript.

Hahn, J. (2002). Optimal inference with many instruments. Econometric Theory, 18 (1), 140â€“168.

â€” and Hausman, J. A. (2005). IV Estimation with Valid and Invalid Instruments. Annales
  dâ€™EÌconomie et de Statistique, (79/80), 25â€“57.

Hansen, C. B., Hausman, J. A. and Newey, W. K. (2008). Estimation With Many Instrumental
  Variables. Journal of Business and Economic Statistics, 26 (4), 398â€“422.

Hausman, J. A., Newey, W. K., Woutersen, T., Chao, J. C. and Swanson, N. R. (2009).
  Instrumental Variable Estimation with Heteroskedasticity and Many Instruments, unpublished
  manuscript.

Kraay, A. (2008). Instrumental Variables Regressions with Honestly Uncertain Exclusion Restric-
  tions, unpublished manuscript.

Kunitomo, N. (1980). Asymptotic expansions of the distributions of estimators in a linear func-
  tional relationship and simultaneous equations. Journal of the American Statistical Association,
  75 (371), 693â€“700.

Levitt, S. D., List, J. A., Neckermann, S. and Sadoff, S. (2011). The Impact of Short-term
  Incentives on Student Performance, unpublished manuscript.

Mariano, R. S. (1973). Approximations to the Distribution Functions of Theilâ€™s k-Class Estima-
  tors. Econometrica, 41 (4), 715â€“721.

Morimune, K. (1983). Approximate distributions of k-class estimators when the degree of overi-
  dentifiability is large compared with the sample size. Econometrica, 51 (3), 821â€“841.


                                               [40]
Nagar, A. L. (1959). The bias and moment matrix of the general k-class estimators of the pa-
  rameters in simultaneous equations. Econometrica, 27 (4), 575â€“595.

Nagin, D. and Snodgrass, M. G. (2011). The Effect of Incarceration on Offending: Evidence
  from a Natural Experiment in Pennsylvania, unpublished manuscript.

Nevo, A. and Rosen, A. M. (2010). Identification with Imperfect Instruments. Review of Eco-
  nomics and Statistics, (forthcoming).

Newey, W. K. and McFadden, D. L. (1994). Large sample estimation and hypothesis testing. In
  R. F. Engle and D. L. McFadden (eds.), Handbook of Econometrics, vol. 4, Chapter 36, Elsevier,
  pp. 2111â€“2245.

Phillips, G. D. A. and Hale, C. (1977). The Bias of Instrumental Variable Estimators of Si-
  multaneous Equation Systems. International Economic Review, 18 (1), 219â€“228.

Reinhold, S. and Woutersen, T. (2011). Endogeneity and Imperfect Instruments in Applied
  Work : Deriving Bounds in a Semiparametric Model, unpublished manuscript.

Rothenberg, T. J. (1984). Approximating the distributions of econometric estimators and test
  statistics. In Z. Griliches and M. D. Intriligator (eds.), Handbook of econometrics, vol. 2, Chapter
  15, Elsevier, pp. 881â€“935.

Staiger, D. and Stock, J. H. (1997). Instrumental Variables Regression with Weak Instruments.
  Econometrica, 65 (3), 557â€“586.

Theil, H. (1961). Economic Forecasts and Policy. Amsterdam: Horth-Holland, 2nd edn.

â€” (1971). Principles of Econometrics. New York: John Wiley & Sons.

van Hasselt, M. (2010). Many Instruments Asymptotic Approximations Under Nonnormal Error
  Distributions. Econometric Theory, 26 (02), 633â€“645.

Wooldridge, J. M. (2002). Econometric Analysis of Cross Section and Panel Data. Cambridge,
  MA: MIT Press.




                                                [41]
Table 1: Estimates for Angrist-Krueger Data (N = 162, 487)

                                           Standard Error
    Estimator       Î²Ì‚    classic    bekker   many exo      Î›11 > 0
 single qob dummy
     tsls      0.089 (0.021)
     liml      0.089 (0.021) (0.021)            (0.021)
     btsls     0.089 (0.021) (0.021)
     mbtsls    0.089 (0.021) (0.021)            (0.021)     (0.021)
     jive      0.090 (0.021) (0.021)
     mjive     0.089 (0.021) (0.021)            (0.021)     (0.021)

 qob interacted   with year and state of birth
    tsls          0.073 (0.017)
    liml          0.095 (0.017) (0.042)        (0.042)
    btsls         0.097 (0.017) (0.039)
    mbtsls        0.098 (0.017) (0.040)        (0.040)      (0.039)
    jive          0.056 (0.017) (0.053)
    mjive         0.096 (0.017) (0.054)        (0.040)      (0.040)

 qob interacted   with year and state of birth, qob exogenous variable
    tsls          0.069 (0.033)
    liml          0.093 (0.034) (0.128)        (0.128)
    btsls         0.099 (0.034) (0.131)
    mbtsls        0.102 (0.034) (0.132)        (0.132)   (0.132)
    jive          0.064 (0.033) (0.180)
    mjive         0.096 (0.034) (0.184)        (0.133)   (0.133)




                                    [42]
