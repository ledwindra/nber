NBER WORKING PAPER SERIES

A MAXIMUM LIKELIHOOD METHOD FOR THE INCIDENTAL PARAMETER
PROBLEM
Marcelo Moreira
Working Paper 13787
http://www.nber.org/papers/w13787

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
February 2008

The author thanks Gary Chamberlain, Rustam Ibragimov, Humberto Moreira, Whitney Newey, Thomas
Rothenberg, and Tiemen Woutersen for helpful comments. José Miguel Torres provided outstanding
research assistance. E-mail: moreira@fas.harvard.edu. The views expressed herein are those of the
author(s) and do not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2008 by Marcelo Moreira. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.

A Maximum Likelihood Method for the Incidental Parameter Problem
Marcelo Moreira
NBER Working Paper No. 13787
February 2008
JEL No. C13,C23,C30
ABSTRACT
This paper uses the invariance principle to solve the incidental parameter problem. We seek group
actions that preserve the structural parameter and yield a maximal invariant in the parameter space
with fixed dimension. M-estimation from the likelihood of the maximal invariant statistic yields the
maximum invariant likelihood estimator (MILE). We apply our method to (i) a stationary autoregressive
model with fixed effects; (ii) an agent-specific monotonic transformation model; (iii) an instrumental
variable (IV) model; and (iv) a dynamic panel data model with fixed effects. In the first two examples,
there exist group actions that completely discard the incidental parameters. In a stationary autoregressive
model with fixed effects, MILE coincides with existing conditional and integrated likelihood methods.
The invariance principle also gives a new perspective to the marginal likelihood approach. In an agent-specific
monotonic transformation model, our approach yields an estimator that is consistent and asymptotically
normal when errors are Gaussian. In an instrumental variable (IV) model, this paper unifies asymptotic
results under strong instruments (SIV) and many weak instruments (MWIV) frameworks. We obtain
consistency, asymptotic normality, and optimality results for the limited information maximum likelihood
estimator directly from the invariant likelihood. Our approach is parallel to M-estimation in problems
in which the number of parameters does not change with the sample size. In a dynamic panel data
model with N individuals and T time periods, MILE is consistent as long as NT goes to infinity. We
obtain a large N, fixed T bound; this bound coincides with Hahn and Kuersteiner's (2002) bound when
T goes to infinity. MILE reaches (i) our bound when N is large and T is fixed; and (ii) Hahn and Kuersteiner's
(2002) bound when both N and T are large.

Marcelo Moreira
Littauer Center M-6
Harvard University
Cambridge, MA 02138
and NBER
moreira@fas.harvard.edu

1

Introduction

The maximum likelihood estimator (MLE) is a commonly used procedure to
estimate a parameter in stochastic models. Under regularity conditions, the MLE
is not only consistent but also has asymptotic optimality properties (e.g., Le Cam
and Yang (2000)). In the presence of incidental parameters, however, the MLE of
structural parameters may not even be consistent. This failure occurs because the
dimension of incidental parameters increases with the sample size, affecting the ability
of MLE to consistently estimate the structural parameters. This is the so-called
incidental parameter problem after the seminal paper by Neyman and Scott (1948).
Lancaster (2000) and Arellano and Honoré (1991) provide excellent overviews of the
subject.
This paper appeals to the invariance principle to solve the incidental parameter problem. We propose to find a group action that preserves the model and the
structural parameter. This yields a maximal invariant statistic. Its distribution depends on the parameters only through the maximal invariant in the parameter space.
Maximization of the invariant likelihood yields the maximum invariant likelihood estimator (MILE). Distinct group actions in general yield different estimators. We seek
group actions whose maximal invariant in the parameter space has fixed dimension
regardless of the sample size.
As is customary in the literature, we illustrate our approach with a series of
examples.
Section 3 considers two groups of transformations that completely discard the incidental parameters. The first example is the stationary autoregressive model with fixed
effects. For a particular group action, our solution coincides with Andersen’s (1970)
conditional and Lancaster’s (2002) integrated likelihood approaches. The invariance
principle also provides a new perspective on the marginal likelihood approach, e.g.,
Arellano (2003, Section 2.4.3). The second example is the monotonic transformation
model. The proposed transformation is agent-specific and has infinite dimension. The
conditional and integrated likelihood approaches do not seem to be applicable here.
The invariant principle provides an estimator that is consistent and asymptotically
normal under the assumption of normal errors.
We then proceed to the two main sections of the paper.
Section 4 considers an instrumental variable (IV) model with N observations and
K instruments. In this section, we provide a likelihood maximization approach. It
unifies asymptotic results under both the strong instruments (SIV) and many weak
instruments (MWIV) asymptotics, e.g., Kunitomo (1980), Morimune (1983), and
2

Bekker (1994). This framework parallels standard M-estimation in problems in which
the number of parameters does not change with the sample size. In particular, we
are able to (i) show consistency of the MLE in the IV setup even under MWIV asymptotics from the perspective of likelihood maximization; (ii) derive the asymptotic
distribution of the MLE directly from the objective function under SIV and MWIV
asymptotics; and (iii) provide an explanation for optimality of MLE within the class
of regular invariant estimators.
Section 5 presents a simple dynamic panel data model with N individuals and T
time periods. We propose to use MILE based on the orthogonal group of transformations. This estimator is consistent as long as N T goes to infinity (regardless of the
relative rate of N and T ) and asymptotically normal under (i) large N , fixed T ; and
(ii) large N , large T asymptotics when the autoregressive parameter is smaller than
one. We derive a bound for large N , fixed T asymptotics when errors are normal; our
bound coincides with Hahn and Kuersteiner’s (2002) bound when T → ∞. MILE
reaches (i) our bound when N is large and T is fixed; and (ii) Hahn and Kuersteiner’s
(2002) bound when both N and T are large. Finally, this paper provides further
support to work by Arellano and Bond (1991) and Ahn and Schmidt (1995) from
a maximal invariant perspective. Together with Chamberlain and Moreira (2006),
we establish a connection between the GMM/MD and the integrated likelihood approaches in the dynamic panel data model.
Section 6 compares MILE with existing fixed-effects estimators for the dynamic
panel data model.
Section 7 provides proofs for our results.

2

The Maximum Invariant Likelihood Estimator

Let Pγ,η denote the distribution of the data set Y ∈ Y when the structural
parameter is γ ∈ Γ and the incidental parameter is η ∈ N: L (Y ) = Pγ,η ∈ P.
We seek a group G and actions A1 (·, Y ) and A2 (·, (γ, η)) in the sample and
parameter spaces that preserve the model P:
L (Y ) = Pγ,η ⇒ L (A1 (g, Y )) = PA2 (g,(γ,η)), for any Pγ,η ∈ P.
We are interested in γ. This yields the following definition.
Definition 1 Suppose that A2 : G×Γ×N → Γ×N induces an action A3 : G×N →
N such that
A2 (g, (γ, η)) = (γ, A3 (g, η)).
3

Then the parameter γ is said to be preserved. The incidental parameter space N is
preserved if
N = {η ∈ N; η = A3 (g, e
η ) for some e
η ∈ N} .
Suppose that both γ and N are preserved. We then can appeal to the invariance
principle and focus on invariant statistics φ(Y ) in which φ (A1 (g, Y )) = φ (Y ) for
every Y ∈ Y and g ∈ G. Any invariant statistic can be written as a function of a
maximal invariant statistic defined below.
Definition 2 A statistic M ≡ M (Y ) is a maximal invariant in the sample space if
M (Ye ) = M (Y ) if and only if Ye = A1 (g, Y ) for some g ∈ G.
Comment: If M is a maximal invariant then e
c · M is also a maximal invariant
statistic (for any scalar e
c 6= 0). This shows that the maximal invariant statistic is not
unique.
An orbit of G is an equivalence class of elements Y , where Ye ∼ Y (mod G) if
there exists g ∈ G such that Ye = A1 (g, Y ). By definition, M is a maximal invariant
statistic if it is invariant and takes distinct values on different orbits of G. Every
invariant procedure can be written as a function of a maximal invariant. Hence, we
restrict our attention to the class of decision rules that depend only on the maximal
invariant statistic. An analogous definition holds for the parameter space.
Definition 3 A parameter θ ≡ θ(γ, η) is a maximal invariant in the parameter space
if θ(γ, η) is invariant and takes different values on different orbits of G: Oγ,η =
{A2 (g, (γ, η)) ∈ Γ × N; for some g ∈ G}.
The distribution of a maximal invariant M depends on (γ, η) only through θ. If
A2 : G × Γ × N → Γ × N induces a group action A3 : G × N → N, then θ ≡ (γ, λ),
where λ ∈ Λ is the maximal invariant in the nuisance parameter space N. The
parameter set Λ is allowed to be the empty set.
Definition 4 Let f (M ; θ) be the pdf/pmf of a maximal invariant statistic (we shall
abbreviate f (M ; θ) as the invariant likelihood). The maximum invariant likelihood
estimator (MILE) is defined as
b
θ ≡ arg maxf (M ; θ).
θ∈Θ

4

Comments: 1. Hereinafter, we assume the set Θ to be compact.
2. In general, different group actions A1 (·, Y ) and A2 (·, (γ, η)) yield different
estimators. Hence, a better notation for b
θ would indicate its dependence on the
choice of group actions. For brevity, we omit its dependence here.
3. Suppose that G = {1}, A1 (g, Y ) = Y , and A3 (g, η) = η. Then M = Y is
a maximal invariant statistic and θ = (γ, η) is a maximal invariant parameter. This
shows that MILE is a generalization of the MLE concept.
4. In general we seek group actions A1 (·, Y ) and A2 (·, (γ, η)) that preserve the
model P and the structural parameter γ, and yield a maximal invariant λ in N which
has fixed dimension with the sample size.
5. MILE is a marginal approach. The use of invariance suggests which likelihoods
we should maximize.
We introduce some additional notation. The superscript ∗ indicates the true value
of a parameter, e.g., γ ∗ is the true value of the structural parameter γ. The subscript
N denotes dependence on the sample size N , e.g., λ∗N is the true value of the maximal
invariant λ when the sample size is N . In addition, let 1T be a T -dimensional vector
of ones, Oj×k be a j × k matrix with entries zero, ej be a vector with entry j equals
one and other entries zero.
Hereinafter, additional notation is specific to each example.

3

Transformations Within Individuals

In this section, we present three examples of transformations within individuals.
i
Instead of Pγ,η , we work with Pγ,η
, the probability of the model for agent i. This
i
clarifies our exposition and highlights the fact that the likelihood of each maximal
invariant M = (M1 , ..., MN ) is the sum of marginal likelihoods. In all examples below,
the maximal invariant in the parameter space is θ = γ, with the objective function
simplifying to
N
1 P
ln fi (mi ; θ) ,
(1)
QN (θ) =
N i=1
where fi (mi ; θ) is the marginal density of the maximal invariant Mi for each individual
i. Because the MILE b
θN maximizes QN (θ), consistency, asymptotic normality, and
b
optimality of θN follow from standard results.

5

Lemma 1 Let QN (θ) be defined as in (1) and take all limits as N → ∞.
(a) Suppose that (i) sup θ∈Θ |QN (θ) − Q(θ)| →p 0 for a fixed, nonstochastic function
∗
∗
Q(θ), and (ii) ∀² > 0, inf θ∈B(θ
/
,²) Q(θ) > Q(θ ). Then
b
θN →p θ∗ .
(b) Suppose that (i) b
θN →p θ∗ , (ii) θ∗ ∈ int (Θ), (iii) QN (θ) is twice continuously
√
differentiable in some neighborhood of θ∗ , (iv) N ∂QN (θ∗ ) /∂θ →d N (0, I (θ∗ )), and
(v) sup θ∈Θ |∂ 2 QN (θ∗ ) /∂θ∂θ0 + I (θ) | →p 0 for some nonstochastic matrix that is
continuous at θ∗ where I (θ∗ ) is nonsingular. Then
√
N (b
θN − θ∗ ) →d N (0, I (θ∗ )−1 ).
(c) Suppose that (i) {QN (θ); θ ∈ Θ} is differentiable in quadratic mean at θ∗ with non√
√
singular information matrix I (θ∗ ) , and (ii) N (b
θN −θ∗ ) = I (θ∗ )−1 N ∂QN (θ∗ ) /∂θ+
oQN (θ∗ ) (1). Then
QN (θ + h · N −1/2 )
1
ln
= h0 SN − h0 I (θ∗ ) h + oQN (θ∗ ) (1) ,
QN (θ)
2
where SN →d N (0, I (θ∗ )) under QN (θ∗ ), and b
θN is the best regular invariant esti∗
mator of θ .

3.1

A Linear Stationary Panel Data Model

As an introductory example, consider a linear stationary panel data model with
exogenous regressors and fixed effects:
yit = η i + x0it β + uit ,
where yit ∈ R and xit ∈ RK are observable variables; uit are unobservable (possibly
autocorrelated) errors, i = 1, ..., N , t = 1, ..., T ; η i ∈ R are incidental parameters,
i = 1, ..., N ; and γ = (β, σ 2 ) ∈ RK × R are the structural parameters.
The model for yi· = [yi1 , ..., yiT ]0 ∈ RT conditional on xi· = [xi1 , ..., xiT ]0 ∈ RT ×K is


1
ρ · · · ρT −1



ρ
1
¢
¡
1 
iid
2

 .
(2)
yi· ∼ N η i 1T + xi· β, σ ΣT , where ΣT =
.


..
1 − ρ2  ..

ρT −1
This is Example 3 of Lancaster (2002).
6

1

Both the model and the structural parameter γ = (β, σ 2 , ρ) are preserved by
translations g · 1T (where g is a scalar):
¡
¢
iid
yi· + g · 1T ∼ N (η i + g)1T + xi· β, σ 2 ΣT .
Proposition 1 Let g be elements of the real line with g1 ◦ g2 = g1 + g2 . If the actions
on the sample and parameter spaces are, respectively, A1 (g, yi· ) = (yi· + g · 1T ) and
A2 (g, (β, σ 2 , ρ, η i )) = (β, σ 2 , ρ, η i + g), then
(a) the vector Mi = Dyi· is a maximal invariant in the sample space, where D is a
T − 1 × T differencing matrix with typical row (0, ..., 0, 1, −1, 0, ..., 0),
(b) γ is a maximal invariant in the parameter space, and
iid
(c) Mi ≡ M (yi· ) ∼ N (Dxi· β, σ 2 DΣT D0 ) with density at mi = Dyi· given by
¡
¢ ¡
¢− (T −1)
−1/2
2
fi mi ; β, ρ, σ 2 = 2πσ 2
|DΣT D0 |
½
¾
1
0
0
0 −1
× exp − 2 (yi· − xi· β) D (DΣT D ) D (yi· − xi· β) .
2σ
Comments: 1. The density fi (mi |β, ρ, σ 2 ) is free of the incidental parameter η i (as
it should be).
P
0
2. Under the assumption that N1 N
i=1 vec(xi· )vec(xi· ) →p ΩXX p.d., we can use
Lemma 1 to show that b
θN is consistent and asymptotically normal.
3. Maximization of the invariant likelihood coincides with maximization of the
integrated likelihood if the prior on η i is left unrestricted, e.g., Arellano (2003, Section
bN , σ
2.4). The use of invariance gives an additional result, with b
θ N = (β
b2N , γ
bN ) being
asymptotically optimal within the class of invariant regular estimators.
Finally, we give an example in which MILE may not be admissible. Suppose that
ρ is known to be equal to zero. The model given by (2) simplifies to
¡
¢
iid
yi· ∼ N η i 1T + xi· β, σ 2 IT ,

(3)

which is Example 2 of Lancaster (2002). The Proposition 1(a),(b) still holds true.
The density of Mi at mi = Dyi· is given by
¢− (T −1)
¢ ¡
¡
−1/2
2
fi mi ; β, σ 2 = 2πσ 2
|DD0 |
¾
½
1
0
0
0 −1
× exp − 2 (yi· − xi· β) D (DD ) D (yi· − xi· β) .
2σ

7

2
b ,σ
The MILE estimator b
θ N = (β
N bN ) is given by
PN 0 0
0 −1
bN = P i=1 xi· D (DD ) Dyi· and
β
N
0
0
0 −1 Dx
i·
i=1 xi· D (DD )
N
P
1
b )0 D0 (DD0 )−1 D(yi· − xi· β
b ).
σ
b2N =
(yi· − xi· β
N
N
N (T − 1) i=1

b is unbiased, but the estimator σ
The estimator β
b2N is biased and not even admissible
N
for a quadratic loss function. This example shows that the MILE method yields
consistent, but not necessarily admissible estimators of structural parameters.1

3.2

A Linear Transformation Model
Consider a simple panel data transformation model:
η i (yit ) = x0it β + uit ,

where yit ∈ R and xit ∈ RK are observable variables; uit ∈ R are unobservable errors,
i = 1, ..., N , t = 1, ..., T , with T > K; η i : R → R is an unknown, continuous, strictly
increasing incidental function; and β ∈ RK is the structural parameter. Unlike Abreiid
vaya (2000), we shall parameterize the distribution of the errors: uit ∼ N (αi , σ 2 ).
Because of location and scale normalizations, we shall assume without loss of generiid
ality that uit ∼ N (0, 1).
The model for yi· = (yi1 , yi2 , ..., yiT ) ∈ RT is then given by
P (yi· ≤ v) =

T
Y

Φ (η i (vt ) − x0it β) , where v = [v1 , v2 , ..., vT ]0 .

t=1

Both the model and the structural parameter γ ≡ β are preserved by continuous,
strictly increasing transformations.
Proposition 2 Let g be elements of the group of continuous, strictly increasing
transformations, with g1 ◦ g2 = g1 (g2 ). If the actions on the sample and parameter spaces are, respectively, A1 (g, (yi1 , yi2 , ..., yiT )) = (g(yi1 ), g(yi2 ), ..., g(yiT )) and
A2 (g, (β, η i )) = (β, η i (g)), then
1

We can of course fix this problem by finding the model for Y = vec(y1· , ..., yN · ) and considering
an action group that eliminates both the structural parameter β and the incidental parameters η i ,
e.g., Harville (1974). This yields a likelihood whose maximum likelihood estimator of σ 2 is unbiased
and consistent as N → ∞.

8

(a) the statistic Mi = (Mi1 , ..., MiT ) is the maximal invariant in the sample space,
where Mit is the rank of yit in the collection yi1 , ..., yiT ,
(b) the vector β is the maximal invariant in the parameter space, and
(c) Mi , i = 1, ..., N , are independent with marginal probability mass function of Mi
at mi given by
"
(Ã T
! )#
(
Ã T
! )
X
X
1
1
fi (mi1 , ..., miT ; β) = E exp
V(mit ) x0it β
exp − β 0
xit x0it β ,
T!
2
t=1
t=1
where V(1) , ..., V(T ) is an ordered sample from a N (0, 1) distribution.
The likelihood of the maximal invariant also yields semiparametric methods. For
example, consider the case in which T = 2. If x0i2 β > x0i1 β, then it is likely that yi2 >
yi1 . This yields the semiparametric estimator of Abrevaya (2000). This estimator
maximizes
Qn (β) =

N
1 X
{H (yi2 , yi1 ) I (4x0i β > 0) + H (yi1 , yi2 ) I (4x0i β < 0)}
N i=1

where H is an arbitrary function increasing in the first and decreasing in the second
argument. This estimator is very appealing as it is consistent under more general error
distributions. For asymptotic normality, Abrevaya (2000) proposes to smoothen the
objective function to obtain asymptotic normality whose convergence rate can be
made arbitrarily close to N −1/2 . In contrast, the MILE estimator suggested here
does not require arbitrary choices of H or smoothing.

4

An Instrumental Variables Model

Consider a simple simultaneous equations model with one endogenous variable,
multiple instrumental variables (IVs), and errors that are normal with known covariance matrix. The model consists of a structural equation and a reduced-form
equation:
y1 = y2 β + u,
y2 = Zπ + v2 ,
where y1 , y2 ∈ RN and Z ∈ RN ×K are observed variables; u, v2 ∈ RN are unobserved
errors; and β ∈ R and π ∈ RK are unknown parameters. The matrix Z has full
9

column rank K; the N × 2 matrix of errors [u : v2 ] is assumed to be iid across rows
with each row having a mean zero bivariate normal distribution with a nonsingular
covariance matrix; π is the incidental parameter; and β is the parameter of interest.
The two equation reduced-form model can be written in matrix notation as
Y = Zπa0 + V,
where Y = [y1 : y2 ], V = [v1 : v2 ], and a = (β, 1)0 . The distribution of Y ∈ RN ×2 is
multivariate normal with mean matrix Zπa0 , independence across rows, and covariance matrix Σ for each row.
Because the multivariate normal is a member of the exponential family of distributions, Moreira (2001) shows that low dimensional sufficient statistics are available
for the parameter (β, π 0 )0 . Andrews, Moreira, and Stock (2006) and Chamberlain
(2007) propose to use orthogonal transformations applied to the sufficient statistic
(Z 0 Z)−1/2 Z 0 Y . The maximal invariant is Y 0 NZ Y , where NZ = Z(Z 0 Z)−1 Z 0 .
Reducing the data to a sufficient statistic before applying invariance is a delicate
argument. For example, suppose that there is a (nearly) optimal invariant decision
rule based on a sufficient statistic. This does not imply that it is (nearly) optimal
within invariant decision rules based on the initial data. This problem arises because
there may exist invariant decision rules whose equivalent procedures based on the
sufficient statistic are not invariant. See for example Hall, Wijsman, and Ghosh (1965)
and Lehmann and Romano (2005). To avoid this issue, we shall use an invariance
argument without reducing the data to a sufficient statistic.
For convenience, it is useful to write the model in a canonical form. The matrix Z
has the polar decomposition Z = ω(ρ0 , 0K×(N −K) )0 , where ω is an N × N orthogonal
matrix, and ρ is the unique symmetric, positive definite square root of Z 0 Z. Define
R = ω 0 Y and let η = ρπ. Then the canonical model is
Ã
!
ηa0
d
R=
+ V, L (V ) = N (0, IN ⊗ Σ) .
0
Both model and structural parameters β and Σ are preserved by transformations
O (K) in the first K rows of R. The next proposition obtains the maximal invariants
in the sample and parameter spaces.
Proposition 3 Let g be elements of the orthogonal group of transformations O (K)
and partition the sample space R = (R10 , R20 )0 , where R1 is K ×2 and R2 is (N − K)×
2. If the actions on the sample and parameter spaces are, respectively, A1 (g, R) =
10

((gR1 )0 , R20 )0 and A2 (g, (β, Σ, η)) = (β, Σ, gη), then
(a) the maximal invariant in the sample space is M = (R10 R1 , R2 ), and
(b) the maximal invariant in the parameter space is θN = (β, Σ, λN ), where λN ≡
η 0 η/N .
To illustrate the approach we assume for simplicity that Σ is known. Hence, we
omit Σ from now on, e.g., θN = (β, λN ).
The density of M is the product of the marginal densities of R10 R1 and R2 . Since
R2 is an ancillary statistic, we can focus on the marginal density of R10 R1 ≡ Y 0 NZ Y in
the maximization of the log-likelihood. As the density of Y 0 NZ Y is not well-behaved
as N goes to infinity, we work with the density of WN ≡ N −1 Y 0 NZ Y instead.
Theorem 1 The density of WN ≡ N −1 Y 0 NZ Y evaluated at w is
µ
¶
µ
¶
K−3
N λN 0 −1
N
−K/2
K
−1
g (w; β, λN ) = C1,K · N · exp −
a Σ a |Σ|
|w| 2 exp − tr(Σ w)
2
2
³
´− K−2
´
³ p
p
2
(4)
× N λN · a0 Σ−1 wΣ−1 a
I K−2 N λN · a0 Σ−1 wΣ−1 a ,
2

¡
¢
K+2
1
−1
where C1,K
= 2 2 π 2 Γ K−1
, Iν (·) denotes the modified Bessel function of the first
2
kind of order ν, and Γ(·) is the gamma function.
Define MILE as
b
θN ≡ arg max QN (θ) ,
θ∈Θ

where QN (θ) ≡ N ln g (WN ; θN ) and θN = (β, λN ).2 The next result shows that
b
θN = θ∗N + op (1) under general conditions.
−1

Theorem 2 (a) Under the assumption that N → ∞ with K fixed or K/N → 0,
(i) if λ∗N is fixed at λ∗ > 0, then b
θN →p θ∗ = (β ∗ , λ∗ ), (ii) if λ∗N →p λ∗ > 0,
then b
θN →p θ∗ = (β ∗ , λ∗ ), and (iii) if 0 < lim inf λ∗N ≤ lim sup λ∗N < ∞, then
b
θN = θ∗N + op (1).
(b) Under the assumption that N → ∞ with K/N → α > 0, (i) if λ∗N is fixed at
λ∗ > 0, then b
θN →p θ∗ = (β ∗ , λ∗ ), (ii) if λ∗N →p λ∗ > 0, then b
θN →p θ∗ = (β ∗ , λ∗ ), and
(iii) if 0 < lim inf λ∗N ≤ lim sup λ∗N < ∞, then b
θN = θ∗N + op (1), where θ∗N = (β ∗ , λ∗N ).
2

The objective function QN (θ) is not defined if WN is not positive definite (due to the term
ln |WN |). To avoid this technical issue, we can instead maximize only the terms of QN (θ) that
depend on θ.

11

Comments: 1. Parts (a),(b)(i) yield consistency results conditional on λ∗N ; the
remaining results of the theorem are unconditional on λ∗N . Parts (a),(b)(ii) yield
consistency results for β ∗ under SIV and MWIV asymptotics when λ∗N →p λ∗ . The
assumption of λ∗N →p λ∗ is standard in the literature, but parts (a),(b)(iii) show that
bN →p β ∗ without imposing convergence of λ∗ .
β
N
N
2. This result also holds under nonnormal errors as long as V (WN ) → 0.
Proposition 4 MILE of β is the limited information maximum likelihood (LIMLK)
estimator.
Proposition 4 together with Theorem 2 explain why the LIMLK estimator is
consistent when the number of instruments increases. The MILE estimator maximizes
a log-likelihood function that is well-behaved as it depends on a finite number of
parameters. Because MILE is consistent and LIMLK is equivalent to MILE in the
instrumental variable problem, LIMLK is consistent as well.
The next result derives the limiting distribution of LIMLK.
Theorem 3 Let the score statistic and the Hessian matrix be
SN (θ) =

∂ ln QN (θ)
∂ 2 ln QN (θ)
and HN (θ) =
,
∂θ
∂θ∂θ0

respectively, and define the matrix
"
#
∗
∗2 a∗0 Σ−1 a∗ ·e01 Σ−1 e1 (α+2λ a∗0 Σ−1 a∗ )+α(a∗0 Σ−1 e1 )2
∗ a∗0 Σ−1 e1 ·a∗0 Σ−1 a∗
λ
λ
∗
∗
∗
∗0
−1
∗
∗0
−1
∗
∗0
−1
∗
(α+λ a Σ a )(α+2λ a Σ a )
α+2λ a Σ a
Iα (θ∗ ) =
.
∗0 Σ−1 e ·a∗0 Σ−1 a∗
(a∗0 Σ−1 a∗ )2
1
λ∗ a α+2λ
∗ ∗0 −1 ∗
a Σ a
2(α+2λ∗ a∗0 Σ−1 a∗ )
(a) Suppose that λ∗N is fixed at λ∗ > 0 and N → ∞ with K fixed.
√
√
N SN (θ∗ ) →d N (0, I0 (θ∗ )), (ii) HN (θ∗ ) →p −I0 (θ∗ ), and (iii) N (b
θN
∗ −1
N (0, I0 (θ ) ).
(b) Suppose that λ∗N is fixed at λ∗ > 0 and N → ∞ with K/N → α.
√
√
N SN (θ∗ ) →d N (0, Iα (θ∗ )), (ii) HN (θ∗ ) →p −Iα (θ∗ ), and (iii) N (b
θN
∗ −1
N (0, Iα (θ ) ).

Then (i)
− θ ∗ ) →d
Then (i)
− θ ∗ ) →d

Comments: 1. For convenience we provide asymptotic results only for the case in
which λ∗N is fixed at λ∗ > 0. Small changes in the proofs also yield asymptotic results
√
for λ∗N →p λ∗ . Alternatively, if the convergence for N (b
θN − θ∗ ) is uniform in a
12

compact set containing λ∗ , we can use Theorem 3 and Sweeting (1989) to show that
√
N (b
θN − θ∗N ) converges to N (0, Iα (θ∗ )).
√
2. If λ∗N does not converge, then N (b
θN − θ∗N ) does not converge. However,
√
θN − θ∗N ) is uniform on a compact set that
if the conditional convergence for N (b
eventually contains λ∗N , then N (0, Iα (θ∗N )) provides an approximation to the finite
√
sample distribution in the sense that N (b
θN −θ∗N ) conditioned on λ∗N = λ∗ converges
in distribution to N (0, Iα (θ∗ )).
3. It is possible to extend the asymptotic distribution to nonnormal errors, e.g.,
Bekker and der Ploeg (2005), Hansen, Hausman, and Newey (2006), and van Hasselt
√
(2007). Our approach entails finding the asymptotic distribution N SN (θ∗ ) for
nonnormal errors.
As a corollary, we find the limiting distribution of LIMLK. This result of course
coincides with those obtained by Bekker (1994).
Corollary 1 Define σ 2u = b0 Σb. Under SIV asymptotics (or under MWIV asymptotics with α = 0), conditional on λ∗N = λ∗ > 0,
µ
¶
2
√
σ
∗
u
b − β ) →d N 0,
N (β
.
(5)
N
λ∗
Under MWIV asymptotics, conditional on λ∗N = λ∗ > 0,
µ
½
¾¶
√
σ 2u
1
∗
∗
b
b N (β N − β ) →d N 0, ∗2 λ + α ∗0 −1 ∗
.
a Σ a
λ

(6)

Comments: 1. The limiting distribution given in (6) simplifies to the one given in
(5) as α → 0.
2. Instead of using the invariant likelihood to obtain an estimator, we could
instead use only its first moment. Define
µ 0 ¶
µ
¶
R1 R1
K
0
m (WN ; θN ) = vech
− vech aa · λN + Σ .
(7)
N
N
If λ∗N > 0, then the following holds (for possibly nonnormal errors):
Eθ∗N (m (WN ; θ)) = 0 if and only if θN = θ∗N .

(8)

Because the number of moment conditions does not increase under SIV or MWIV
asymptotics, we can show that the MD estimator based on (7) and (8) is consistent
and asymptotically normal.
13

Finally, Chioda and Jansson (2007) derive limits of experiments from the maximal
invariant’s likelihood. In our setup, we obtain the following result under SIV and
MWIV asymptotics.
Theorem 4 Define the log-likelihood ratio
¡
¢
¡
¢
ΛN θ∗ + h · N −1/2 , θ∗ = N (QN θ∗ + h · N −1/2 − QN (θ∗ )).
(a) Under SIV asymptotics,
√
¡
¢
1
ΛN θ∗ + h · N −1/2 , θ∗ = h0 N SN (θ∗ ) − h0 I0 (θ∗ ) h + oQN (θ∗ ) (1),
(9)
2
√
where N SN (θ∗ ) →d N (0, I0 (θ∗ )) under QN (θ∗ ).
(b) Under MWIV asymptotics,
√
¡
¢
1
ΛN θ∗ + h · N −1/2 , θ∗ = h0 N SN (θ∗ ) − h0 Iα (θ∗ ) h + oQN (θ∗ ) (1),
(10)
2
√
where N SN (θ∗ ) →d N (0, Iα (θ∗ )) under QN (θ∗ ).
Furthermore, the LIMLK estimator is asymptotically efficient within the class of regular invariant estimators under both SIV and MWIV asymptotics.
Comments: 1. Chioda and Jansson’s (2007) proof uses Johnson and Kotz’s (1970)
asymptotic results for Wishart distributions. The standard literature on limit of
experiments instead typically provides expansions around the score, e.g., Lehmann
and Romano (2005). Theorem 3 shows that the score is asymptotically normal with
variance given by the reciprocal of the inverse of the limit of the Hessian matrix. As
the remainder terms are asymptotically negligible, (9) and (10) hold true.
2. Theorem 4 requires the assumption of normal errors. Anderson, Kunitomo,
and Matsushita (2006) exploit the fact that WN involves double sums (in terms of N
and K) to obtain optimality results for nonnormal errors.
Under SIV asymptotics, the bound (I0 (θ∗ )−1 )11 for regular invariant estimators of
β is the same as the one achieved by limit of experiments applied to the likelihood of
Y . Hence, there is no loss of efficiency in focusing on the class of invariant procedures
under SIV asymptotics.
The LIMLK achieves the bound (Iα (θ∗ )−1 )11 under MWIV asymptotics. Proposition 4 and Theorem 4(b) explain why. Standard optimality results apply to an
estimator that maximizes a (marginal) likelihood function that is locally asymptotically normal (LAN). Applying this principle to invariant likelihood delivers optimality
of MILE (within the class of regular invariant estimators). Because LIMLK coincides
with MILE, the LIMLK estimator must be optimal as well.
14

5

A Nonstationary Dynamic Panel Data Model
Consider a simple dynamic panel data model with fixed effects:
yi,t = ρyi,t−1 + η i + uit ,
iid

where yit ∈ R are observable variables and uit ∼ N (0, σ 2 ) are unobservable errors,
i = 1, ..., N , t = 1, ..., T ; η i ∈ R are incidental parameters, i = 1, ..., N ; θ = (ρ, σ 2 ) ∈
RK × R are structural parameters; and yi,0 are the initial values of the stochastic
process. We follow Lancaster (2002) and seek inference conditional on the initial
values yi,0 . Writing the model as
(yi,t − yi,0 ) = ρ(yi,t−1 − yi,0 ) + (η i − yi,0 (1 − ρ)) + uit ,
we can assume that yi,0 = 0 without loss of generality.
In its matrix form, we have
[y·1 , y·2 , ..., y·T ] = ρ [y·0 , y·1 , ..., y·T −1 ] + η10T + [u·1 , u·2 , ..., u·T ] ,

(11)

where y·t = [y1,t , y2,t , ..., yN,t ]0 ∈ RN , u·t = [u1,t , u2,t , ..., uN,t ]0 ∈ RN , and η = [η 1 , ..., η N ]0 ∈
RN . Solving (11) recursively yields
[y·1 , y·2 , ..., y·T ] = η(B1T )0 + [u·1 , u·2 , ..., u·T ] B 0 , where


1


...
B =  ...
.
ρT −1 · · ·

(12)

1

The inverse of B has a simple form:
"
B −1 ≡ D = IT − ρ · JT , where JT =

00T −1

0

#

IT −1 0T −1

and 0T −1 is a T − 1-dimensional column vector with zero entries.
If individuals i are treated equally, the coordinate system used to specify the
vectors y·t should not affect inference based on them. In consequence, it is reasonable
to restrict attention to coordinate-free functions of y·t . Indeed, we find that orthogonal
transformations preserve both the model given in (12) and the structural parameter
γ = (ρ, σ 2 ).

15

Proposition 5 Let g be elements of the orthogonal group of transformations O (N ).
If the actions on the sample and parameter spaces are, respectively, A1 (g, R) =
((gR1 )0 , R20 )0 and A2 (g, (ρ, σ 2 , η)) = (ρ, σ 2 , gη), then
(a) the maximal invariant in the sample space is M = Y 0 Y , and
(b) the maximal invariant in the parameter space is θN = (γ, λN ), where λN =
η 0 η/ (N σ 2 ).
Comments: 1. The dimension of the maximal invariant M is T (T + 1) /2. For
example, if T = 2, the maximal invariant has dimension three.
2. The maximal invariant M has a noncentral Wishart distribution with T degrees
0
of freedom, covariance matrix Σ = σ 2 BB 0 , and noncentrality matrix Ω = Σ−1 M M
where M = η (B1T )0 . We write that M is WT (K, Σ, Ω). If there is autocorrelation ΣT
that is homogeneous across individuals, the maximal invariant M remains the same.
The covariance matrix however changes to Σ = σ 2 BΣT B 0 .
For convenience, we standardize the distribution of M = Y 0 Y .
Theorem 5 If N ≥ T , the density of WN ≡ N −1 Y 0 NZ Y evaluated at w is
µ
¶
µ
¶
¡
¢
¡ 2 ¢− N2T
N −T −1
1
N
T
2
0
g w; ρ, σ , λN = C2,N · σ
|w| 2 exp − 2 tr(DwD ) exp −
λN
2σ
2
Ã r
!− K−2
Ã r
!
2
NT
10T DwD0 1T
10T DwD0 1T
2 ,
K−2
I
× N λN
N
λ
·
N
(13)
N
2
σ2
σ2
−1
where C2,N
=2

NT
2

− N 2−2

π

T (T −1)
4

QT −1 ¡ N −i ¢
.
i=1 Γ
2

Define MILE as
b
θN ≡ arg max QN (θ) ,
θ∈Θ

where QN (θ) ≡ (N T ) ln g (WN ; β, λ) and θN = (ρ, σ 2 , λN ).3 The next result shows
that b
θN = θ∗N + op (1) under general conditions.
−1

Theorem 6 (a) Under the assumption that N → ∞ with T fixed, (i) if λ∗N is fixed
at λ∗ > 0, then b
θN →p θ∗ = (ρ∗ , σ ∗2 , λ∗ ), (ii) if λ∗N →p λ∗ > 0, then b
θN →p
∗
∗
∗
∗
∗
∗2
b
θ = (ρ , σ , λ ), and (iii) if lim sup λN < ∞, then θN = θN + op (1), where
3

If N < T , WN is not absolutely continuous with respect to the Lebesgue measure. We will still
maximize the pseudo-likelihood to find b
θN .

16

θ∗N = (ρ∗ , σ ∗2 , λ∗N ).
(b) Under the assumption that T → ∞ and |ρ∗ | < 1, (i) if λ∗N is fixed at λ∗ > 0, then
b
θN →p θ∗ = (β ∗ , λ∗ ), (ii) if λ∗N →p λ∗ > 0, then b
θN →p θ∗ = (β ∗ , λ∗ ), and (iii) if
θN = θ∗N + op (1), where θ∗N = (ρ∗ , σ ∗2 , λ∗N ).
lim sup λ∗N < ∞, then b
Comments: 1. This result also holds under nonnormal errors.
2. This theorem implies that b
ρN →p ρ∗ under the assumption that N T → ∞
(regardless of the growing rate of N and T ).
The next result derives the limiting distribution of MILE when N → ∞.
Theorem 7 Suppose that λ∗N is fixed at λ∗ > 0, and let the score statistic and the
Hessian matrix be
∂ ln QN (θ)
∂ 2 ln QN (θ)
,
and HN (θ) =
∂θ
∂θ∂θ0
respectively, and define the matrix


0
0
λ∗ 1T F 1T
1+λ∗ T 1T F 1T
h1,T + h2,T + h3,T
∗
∗2
2σ
T
1+2λ T
T
0


λ∗ 1T F 1T
1
λ∗
2λ∗ T
1
IT (θ∗ ) = 
+
,
∗
∗2
∗2
2
∗2
∗2
2σ
T
2(σ )
4σ 1+2λ T
4σ
SN (θ) =

0
1+λ∗ T 1T F 1T
∗
1+2λ T
T

1
4σ ∗2

1
4λ∗

where DB ∗ ≡ IT + (ρ∗ − ρ) F and the three terms in the (1, 1) entry of HT (θ∗ ) are
tr(F F 0 )
(10T F 1T )2
10 F 0 F 1T
2λ∗2
+ λ∗ T
, h2,T =
, and
T
T
(1 + 2λ∗ T )
T
(
)
2
0
λ∗
10T F 0 F 1T
(1
F
1
)
T
=−
+ λ∗ T
.
1 + λ∗ T
T
T

h1,T =
h3,T

As N → ∞ with T fixed,
√
√
θN −
(a) (i) N T SN (θ) →d N (0, IT (θ∗ )), (ii) HN (θ∗ ) →p −IT (θ∗ ), and (iii) N T (b
∗
∗ −1
θ ) →d N (0, IT (θ ) ), and
(b) the log-likelihood ratio is
¢
¡
¡
¢
(14)
ΛN θ∗ + h · (N T )−1/2 , θ∗ = N T (QN θ∗ + h · (N T )−1/2 − QN (θ∗ ))
√
1
= h0 N T SN (θ∗ ) − h0 IT (θ∗ ) h + oQN (θ∗ ) (1),
2
√
N T SN (θ∗ ) →d N (0, IT (θ∗ )) under QN (θ∗ ). Furthermore, b
θN is asymptotically
efficient within the class of regular invariant estimators under large N , fixed T asymptotics.
17

Comments: 1. If the convergence is uniform on a compact set that eventually contains λ∗N , N (0, Hα (θ∗N )) provides an approximation of the finite sample distribution
√
of N T (b
θN − θ∗N ) in the sense of Sweeting (1989). Because b
θN = θ∗N + op (1) and
IT (·) is continuous, N (0, IT (b
θN )) also provides a valid asymptotic approximation to
√
the distribution of N T (b
θN − θ∗N ).
2. It is possible to extend parts (a)(i),(iii) to nonnormal errors by finding the
√
appropriate asymptotic distribution of N T SN (θ∗ ).
¡
¢
3. The MILE estimator b
ρN achieves the bound IT (θ∗ )−1 11 as N → ∞, whereas
the bias-corrected OLS estimator does not.
The next proposition considers minimum distance (MD) estimation based on the
expectation of WN ; standard semiparametric efficiency arguments (e.g., Chamberlain
(1987)) show that the MD estimator is optimal. This proposition also provides a
connection between the GMM and integrated likelihood approaches for the dynamic
panel data model. It shows that Arellano and Bond’s (1991) and Ahn and Schmidt’s
(1995) moment conditions are transformations of the expectation of the maximal
invariant. This result connects and builds on work by Chamberlain and Moreira
(2006) who show that the likelihood integrated with respect to the Haar measure (for
orthogonal groups) coincides with the marginal likelihood of the maximal invariant.
Proposition 6 Let wi = yi· yi·0 , where yi· = [yi,1 , yi,2 , ..., yi,T ]0 ∈ RT , and define
¡
¢
1 XN
m wi ; ρ, σ 2 , (η i /σ)2 where
m (WN ; θN ) =
N µi=1
½
¾ ¶
³ η ´2
¡
¢
2
i
2
2
0
m wi ; ρ, σ , (η i /σ) = vech wi − σ B IT +
· 1T 1T B .
σ

(15)

(a) Arellano and Bond’s (1991) and Ahn and Schmidt’s (1995) moment conditions
are subsets of the T (T + 1)/2 moment conditions given by
¡
¡
¢¢
Eθ∗N (m (WN ; θN )) = Eθ∗N vech WN − σ 2 B {IT + λN · 1T 10T } B
(16)
= 0 if and only if θN = θ∗N .
(b) Consider the minimum distance (MD) estimator e
θN that minimizes
Q (θN ) = m (WN ; θN )0 AN m (WN ; θN ) .

(17)

Under the assumptions N → ∞ with T fixed, AN →p A p.d., and λ∗N is fixed at λ∗ ,
√
e
θN →p θ∗ = (ρ∗ , σ ∗2 , λ∗ ) and N (e
θN − θ∗ ) →d N (0, (ζ 0 Aζ)−1 ζ 0 AΞAζ(ζ 0 Aζ)−1 ), where
Ξ and ζ are defined as
√
∂m (WN ; θ∗ )
N m (WN ; θ∗ ) →d N (0, Ξ) and
→p ζ.
∂θ
18

(c) The optimal MD estimator e
θN achieves the semiparametric efficiency bound derived under the assumption that (η ∗i /σ ∗ )2 , i = 1, ..., N , are fixed at λ∗ .
¡
¢
iid
Comments: 1. The additional random effects assumption η i ∼ N 0, σ 2η specifies a
data covariance structure that depends on a finite number of parameters. Specifically,
¡
¡
¢¢
iid
yi· ∼ N 0, Ψ ρ, σ 2 , σ 2η for some covariance matrix Ψ that depends on ρ, σ 2 , and σ 2η ,
and we can proceed as in Arellano (2003, Section 5.4) to make inference on ρ. This
approach differs from ours. We do not impose additional distribution assumptions.
As a result, the distribution of yi· depends on ρ, σ 2 , and η 2i , i = 1, ..., N . Use of
invariance, however, shows that the expectation of sample averages of wi = yi· yi·0
depends on only three parameters: ρ, σ 2 , and λN .
2. For T = 2, the number of nonredundant moments given by (16) equals the
dimension of θN , and the parameter θN is said to be just-identified.
3. The MD estimator dominates MILE under nonnormal errors with large N and
small T . For large T , the MD estimator does not perform well. If T grows sufficiently
fast with the sample size, the MD estimator is no longer consistent. Consistency of
MILE does not depend on particular rates at which both N and T grow with the
sample size.
4. If there is autocorrelation ΣT that is homogeneous across individuals, the
maximal invariant remains the same, but (16) changes to
µ
µ
½
¾ ¶¶
η0η
0
Eθ∗N (m (WN ; θN )) = Eθ∗N vech WN − B ΣT +
· 1T 1T B
.
N
In the IV model, the number of moment conditions does not increase with N or
K ; see Comment 2 to Corollary 1. In the panel data model, the number of moment
conditions increases (too quickly) with T . As a result, semiparametric efficiency
results (e.g., Newey (2004)) do not apply to (16) as T → ∞. Instead, Hahn and
Kuersteiner (2002) cleverly use Hájek’s convolution theorem to obtain an efficiency
bound for normal errors as T → ∞ for the stationary case |ρ∗ | < 1. The biascorrected OLS estimator of ρ achieves Hahn and Kuersteiner’s (2002) bound for large
N , large T asymptotics.
¡
¢
Our efficiency bound IT (θ∗ )−1 11 reduces to Hahn and Kuersteiner’s (2002)
bound when T → ∞. This shows that there is no loss of efficiency in focusing
on the class of invariant procedures under large N , large T asymptotics.

19

Corollary 2 Under the assumption that |ρ∗ | < 1, the efficiency bound given by the
³
´−1
∗ −1
∗
(1, 1) coordinate of the inverse of I∞ (θ ) ≡ lim IT (θ )
converges to Hahn and
T →∞

Kuersteiner’s (2002) efficiency bound of (1 − ρ∗2 ) as T → ∞.
¡
¢
As a final result, the MILE estimator b
ρN also achieves the bound IT (θ∗ )−1 11 for
large N , large T asymptotics.
Theorem 8 Under the assumption that N ≥ T → ∞, |ρ∗ | < 1, and λ∗N is fixed
√
at λ∗ > 0, (i) N T SN (θ) →d N (0, I∞ (θ∗ )), (ii) HN (θ∗ ) →p −I∞ (θ∗ ), and (iii)
√
N T (b
θN − θ∗ ) →d N (0, I∞ (θ∗ )−1 ).

6

Numerical Results

This section illustrates the MILE approach for estimation of the autoregressive
parameter ρ in the dynamic panel data model described in Section 5. The numerical results are presented as means and mean squared errors (MSEs) based on 1,000
Monte Carlo simulations. These results are also available for other fixed-effects estimators: Arellano-Bond (AB), Ahn-Schmidt (AS), and bias-corrected OLS (BCOLS)
estimators.
We consider different combinations between short and large panels: N = 5, 10,
25, 100, and T = 2, 3, 5, 10, 25, 100.
Table I presents the initial design from which several variations are drawn.4 This
iid
iid
design assumes that η ∗i ∼ N (0, 4) (random effects), uit ∼ N (0, 1) (normal errors),
and ρ∗ = 0.5 (positive autocorrelation). The value σ ∗ is fixed at one for all designs.
MILE seems to be correctly centered around 0.5. Even in a very short panel with
N = 5 and T = 2, its bias of 0.0408 seems quite small. As N and/or T increases, its
mean approaches 0.5. For example, for N = 5 and T = 25, the bias is around 0.0129;
for N = 25 and T = 2, the simulation mean is around 0.0040. BCOLS estimator
seems to have smaller bias than the AB and AS estimators for small N and large T .
The AB and AS estimators have large bias with small N and T , but their performance
improves with large N and small T .
MILE also seems to have smaller MSE than the other estimators. The AS estimator outperforms the AB estimator in terms of MSE. The BCOLS estimator has
4

The full set of results for ρ, σ 2 , and λN using different designs will be available at
http://www.economics.harvard.edu/faculty/moreira/softwaresimulations.html.

20

smaller MSE than AS. The MSE of the BCOLS estimator, however, does not decrease
if N increases but T is held constant. For T ≥ 25, its performance is comparable to
that of MILE. This provides numerical support for the theoretical finding that both
MILE and BCOLS reach our large N , large T bound.
Table II reports results for λ∗N = N (nonconvergent effects), normal errors, and
√
iid
ρ∗ = 0.5. Table III presents results for random effects, uit ∼ (χ2 (1) − 1)/ 2 (nonnormal errors), and ρ∗ = 0.5. In both cases, MILE continues to have smaller bias
and MSE than the other estimators. This result is surprising with nonnormal errors
as the AB and AS estimators could potentially dominate MILE when N is large and
T is small.
Tables IV and V differ from Table I only in the autoregressive parameter; respectively, ρ∗ = −0.5 (negative autocorrelation) and ρ∗ = 1.0 (integrated model). Most
—but not all— conclusions drawn from Table I hold here. MILE continues to outperform the AB and AS estimators in terms of mean and MSE. If ρ∗ = −0.5, MILE
and BCOLS seem to perform similarly. If ρ∗ = 1.0, MILE again performs better than
BCOLS for small values of T .

21

7

Appendix of Proofs

7.1

Proofs of Results Stated in Section 3

Proof of Lemma 1. Parts (a) and (b) follow from Newey and McFadden (1994) or
Potscher and Prucha (1997). Part (c) follows from Theorem 12.2.3 of Lehmann and
Romano (2005) and Lemma 8.14 of van der Vaart (1998).
Proof of Proposition 1. For part (a), we need to show that M (yi· ) = M (e
yi· ) if
and only if yei· = yi· + ge · 1T for some ge. Clearly, M (yi· ) is an invariant statistic:
M (yi· + g · 1T ) = D (yi· + g · 1T ) = Dyi· + g · D1T = Dyi· = M (yi· ) .
Now, suppose that M (yi· ) = M (e
yi· ). This implies that Dzi = 0 for zi = yei· − yi· ,
which means that zi belongs to the space orthogonal to the row space of D. Because
rank (D) = T − 1, the orthogonal space has dimension one. As this space contains
the vector 1T , it must be the case that zi = ge · 1T for some scalar ge. Therefore,
yei· = yi· + ge · 1T .
Part (b) follows from the fact that the group of transformations acts transitively
on η i . Part (c) follows from the formula of the density of a normal distribution.
Proof of Proposition 2. For part (a), let Mit be the rank of yit in the collection
yi1 , ..., yiT . Formally, we can define Mit through yit = yi(Mit ) . We shall abbreviate
the notation, e.g., (g(yi1 ), g(yi2 ), ..., g(yiT )) as g(yi· ). The maximal invariant is Mi =
(Mi1 , ...MiT ) = M (yi· ). We need to show that M (yi· ) = M (e
yi· ) if and only if yei· =
e
ge(yi· ). Consider the case that if t 6= t, then yit 6= yiet (this set has probability measure
equal to one). Clearly, Mi is an invariant statistic. Now, suppose that M (yi· ) =
fi1 , ..., MiT = M
fiT . Therefore, yi1 < ... < yiT and
M (e
yi· ). This implies that Mi1 = M
yei1 < ... < yeiT . There is a continuous, strictly increasing transformation ge such that
yeit = ge(yit ), t = 1, ..., T .
Part (b) follow from the fact that the group of transformations acts transitively
on η i .
For part (c), we note that because η i is an increasing transformation, Mit is also
∗
∗
∗
∗
the rank in the collection yi1
, ..., yiT
, where yit∗ = x0it β + uit . We note that yi1
, ..., yiT
are jointly independent with marginal densities
¾
½
1
1
2
0
fit (zit ; β) = √ exp − (zit − xit β) .
2
2π
Now, we note that
P (Mi1 = mi1 , ..., MiT = miT ) =

Z

Z
...
22

fi1 (zi1 ; β) ...fiT (ziT ; β) dzi1 ...dziT ,

integrated over the set in which zit is the mit -th smallest element of zi1 , ..., ziT . Transforming wmit = zit , we obtain
P (Mi1 = mi1 , ..., MiT

Z Y
T

Z Y
T
fit (wmit ; β)
f (wmit ) dw,
= miT ) =
fit (wmit ; β) dw =
f (wmit )
A t=1
A t=1

©
ª
where f (wt ) is the density of a N (0, 1) distribution and A = w ∈ RT ; w1 < ... < wT .
Simple algebraic manipulations show that
) T
(
Z
T
T
Y
1X 2
1X
2
0
P (Mi = mi ) =
(wmit − xit β) +
wmit
f (wmit ) dw
exp −
2 t=1
2 t=1
A
t=1
) T
( T
Z
T
X
Y
X
1
2
(x0it β)
f (wmit ) dw
=
exp
wmit x0it β −
2 t=1
A
t=1
t=1
(Ã T
!
Ã T
! )
Z
T
X
X
Y
1
1
0
0
0
=
f (wmit ) dw,
exp
wmit xit β − β
xit xit β T !
T! A
2
t=1
t=1
t=1
where T !

7.2

QT
t=1

f (wt ) for w1 < ... < wT is the pdf of the order statistics V(1) , ..., V(T ) .

Proofs of Results Stated in Section 4

Proof of Proposition 3. For part (a), we need to show that M (R1 , R2 ) =
e1 , R
e2 ) if and only if (R
e1 , R
e2 ) = (e
M (R
g R1 , R2 ) for some ge ∈ O (K). Clearly, M (yi· ) is
an invariant statistic:
M (gR1 , R2 ) = (R10 g 0 gR1 , R2 ) = (R10 R1 , R2 ) = M (R1 , R2 ) .
e1 , R
e2 ). This is equivalent to R10 R1 = R
e10 R
e1 and
Now, suppose that M (R1 , R2 ) = M (R
e2 . But this implies that R
e1 = geR1 (and, of course, R2 = R
e2 ).
R2 = R
Part (b) follows analogously.
Proof of Theorem 1. The matrix M has a noncentral Wishart distribution with
0
K degrees of freedom, covariance matrix Σ, and noncentrality matrix Ω = Σ−1 M M
where M = (Z 0 Z)1/2 πa0 . We write that M is W2 (K, Σ, Ω). Following Anderson
(1946), the density function of M at q is
µ
µ
¶
¶
K−3
1
1
0
−K/2
−1
−1
f (q) = C1,K · exp − tr(Σ M M ) |Σ|
|q| 2 exp − tr(Σ q)
2
2
µq
¶− K−2
¶
µ
q
2
0
0
×
tr(qΣ−1 M M Σ−1 )
I K−2
tr(qΣ−1 M M Σ−1 ) .
2

23

0

Using the fact that Σ−1 M M = Σ−1 aπ 0 Z 0 Zπa0 , we obtain
0

tr(Σ−1 M M ) = (N λN )a0 Σ−1 a and tr(qΣ−1 aπ 0 Z 0 Zπa0 Σ−1 ) = (N λN )a0 Σ−1 qΣ−1 a.
As a result, the density function of M at q simplifies to
µ
¶
µ
¶
K−3
1
N λN 0 −1
−K/2
−1
2
f (q) = C1,K · exp −
a Σ a |Σ|
|q|
exp − tr(Σ q)
2
2
´− K−2
´
³p
³
p
2
N λN · a0 Σ−1 qΣ−1 a
I K−2
N λN · a0 Σ−1 qΣ−1 a .
×
2

The density function of WN is then
g (w; β, λN ) = f (q (w)) · |q 0 (w)| = f (q (w)) N

2·3
2

,

which simplifies to (4).
Proof of Theorem 2. The log-likelihood function divided by N is
µ
µ
¶¶
N
1
1
− K−2
0 −1
2
QN (θ) = − λ · a Σ a + ln ZN
ZN
I K−2
(18)
2
2
N
2
K−2
3K+2
K −3
1
1
K
ln |Σ| −
ln |WN | − tr(Σ−1 WN ) + ln(2 2 N 4 C1,K ),
−
2N
2N
2
N
√
where ZN = 2 λ · a0 Σ−1 WN Σ−1 a.
All terms in the second line converge under both SIV and MWIV asymptotics
(the only exception is ln |WN | under SIV asymptotics and under MWIV asymptotics
with α = 0). For example, the last term is
!
!
Ã
Ã
K−2
K+2
K+2
³
´
2
2
2
K−2
K+2
1
1
1
2
N
N
¡
¢ + o(1)
ln 2 2 N 2 C1,K =
ln
ln
¡ K−1 ¢ =
K+2
1
N
N
N
Γ K−1
2 2 π2Γ 2
2
under both SIV and MWIV asymptotics. Under SIV asymptotics,
!
Ã
K+2
1
N 2
¡
¢ → 0.
ln
N
Γ K−1
2
Under MWIV asymptotics, we can use Stirling’s formula to obtain


Ã
!
K+2
K+2
1 
N 2
1
N 2
¡ K−1 ¢ =
ln
ln
¡
¢ K−2
¡ K−1 ¢  + o(1)
N
N
Γ 2
2
exp
− 2
(2π)1/2 K−1
2
n
³
´o
α
α
→−
1 + ln
.
2
2
24

However, the second line in (18) does not depend on θ. As a result, these terms
can be ignored in finding the limiting behavior of b
θN . Hence, define the objective
function
µ
¶¶
µ
1
1
N
− K−2
0
−1
bN (θ) = − λ · a Σ a + ln Z 2 I K−2
Q
ZN
.
N
2
2
N
2
The quantity ZN depends on WN . Following Muirhead (2005, Section10.2):
0

K ·Σ+M M
K · Σ + π 0 Z 0 Zπ · a∗ a∗0
K
E (WN ) =
=
= Σ + λ∗N · a∗ a∗0 ,
N
N
N
From here, we split the result into SIV or MWIV with α = 0 asymptotics, and MWIV
with α > 0.
For part (a), define
WN∗ ≡ λ∗N · a∗ a∗0 .
Because V (WN ) → 0, we have WN = WN∗ + op (1). Hence, ZN = ZN∗ + op (1), where
q
∗
ZN ≡ 2 λ · λ∗N (a0 Σ−1 a∗ )2 .
The same holds for nonnormal errors as long as V (WN ) → 0.
bN (θ) = QN (θ) + op (1) (uniformly in θ ∈ Θ
Because K is fixed and N → ∞, Q
compact), where
1
∗1/2
QN (θ) = − λ · a0 Σ−1 a + λ1/2 λN a∗0 Σ−1 a.
2
The first order condition (FOC) for QN (θ) is given by
∂QN (θ)
∗1/2
= −λ · a0 Σ−1 e1 + λ1/2 λN a∗0 Σ−1 e1
∂β
∂QN (θ)
1
1
∗1/2
= − a0 Σ−1 a + λ−1/2 λN a∗0 Σ−1 a.
∂λ
2
2
The value θ∗ = (β ∗ , λ∗N ) minimizes QN (θ), setting the FOC to zero.
For parts (a)(i),(ii), QN (θ) →p Q (θ) given by
1
Q (θ) = − λ · a0 Σ−1 a + λ1/2 λ∗1/2 a∗0 Σ−1 a.
2
Since θ ∈ Θ compact and Q (θ) is continuous, b
θN →p θ.

25

For part (a)(iii), we can define τ (θ, θ∗N ) ≡ QN (θ) which is continuous. For each
point θ∗N , the function τ (θ, θ∗N ) reaches the minimum at θ = θ∗N . Because θ ∈ Θ
compact and τ (·, θ∗N ) is continuous,
QN (θ) − QN (θ∗N ) =
max
QN (θ) − QN (θ∗N ) ≡ δ (θ∗N ) < 0.
sup
∗
∗
θ∈Θ;kθ−θN k≥²
θ∈Θ;kθ−θ N k≥²
Because 0 < lim inf λ∗N and lim sup λ∗N < ∞, there exists a compact set Θ∗ such that
0∈
/ Θ∗ in which θ∗N ∈ Θ∗ eventually. Using continuity of δ (·),
inf
δ (θ∗N ) = min
δ (θ∗N ) = δ < 0
∗
∗
∗
θN

θ N ∈Θ

for large enough N . This implies θ∗N is an identifiably unique sequence of maximizers
of QN (θ):
lim sup
sup
QN (θ) − QN (θ∗N ) < 0.
∗
θ∈Θ;kθ−θN k≥²
The result now follows from Potscher and Prucha (1997, Lemma 3.1).
For part (b), define
WN∗ = αΣ + λ∗N · a∗ a∗0 .
Because V (WN ) goes to zero under SIV and MWIV asymptotics, we have WN =
WN∗ + op (1). Hence, ZN = ZN∗ + op (1), where ZN∗ is defined as
p
ZN∗ ≡ 2 λ · a0 Σ−1 (αΣ + λ∗N · a∗ a∗0 ) Σ−1 a.
The same holds for nonnormal errors as long as V (WN ) → 0. Because K/N → α > 0,
bN (θ) = QN (θ) + op (1) (uniformly in θ ∈ Θ compact), where
Q
Ã
µ
¶
µ
¶ !
∗2 1/2
∗2 1/2
α
1
α
Z
Z
QN (θ) = − λ · a0 Σ−1 a +
1 + N2
− ln 1 + 1 + N2
.
2
2
α
2
α
The first order condition (FOC) for QN (θ) is given by
2λ α · a0 Σ−1 e1 + λ∗N · a∗0 Σ−1 a · a∗0 Σ−1 e1
∂QN (θ)
0 −1
= −λ · a Σ e1 +
³
´1/2
∂β
α
Z ∗2
1 + 1 + αN2
1
a0 Σ−1 a α + λ∗N · a0 Σ−1 a
∂QN (θ)
= − a0 Σ−1 a +
³
´ .
∗2 1/2
∂λ
2
α
ZN
1 + 1 + α2
The value θ∗N = (β ∗ , λ∗N ) minimizes QN (θ), setting the FOC to zero.
26

For parts (b)(i),(ii), QN (θ) →p Q (θ) given by
1
α
Q (θ) = − λ · a0 Σ−1 a +
2
2

µ
1+

ZN∗2
α2

Ã

¶1/2
−

µ

α
ln 1 + 1 +
2

ZN∗2
α2

¶1/2 !
,

p
where Z ∗ ≡ 2 λ · a0 Σ−1 (αΣ + λ∗ · a∗ a∗0 ) Σ−1 a. Since θ ∈ Θ compact and Q (θ) is
continuous, b
θN →p θ.
Part (b)(iii) follows analogously to Part (a)(iii).
Proof of Proposition 4. It follows from Chamberlain (2007) that (in his notation)
the Bayes estimator of φ (integrated over Haar measures for orthogonal groups of
transformations) equals the MLE. The integrated likelihood equals the marginal likelihood of the maximal invariant and φ is a transformation of β. As a result, MILE is
equivalent to LIMLK.
Proof of Theorem 3. For part (a), when K is fixed or K/N → 0,
¡
¢
¡
¢
bN (θ) = − 1 λ · a0 Σ−1 a + λ1/2 a0 Σ−1 WN Σ−1 a 1/2 + op N −1 .
Q
2
¡
¢
All results below hold up to op N −1/2 order.
The components of the score function SN (θ) are

(19)

∂QN (θ)
a0 Σ−1 WN Σ−1 e1
= −λ · a0 Σ−1 e1 + λ1/2
∂β
(a0 Σ−1 WN Σ−1 a)1/2
∂QN (θ)
a0 Σ−1 a (a0 Σ−1 WN Σ−1 a)
=−
+
∂λ
2
2λ1/2

1/2

.

The components of the Hessian matrix HN (θ) ≡ H (WN ; θ) are
2

0 −1
−1
0 −1
−1
∂ 2 QN (θ)
1/2 (a Σ WN Σ e1 )
1/2 e1 Σ WN Σ e1
0 −1
−
λ
=
−λ
·
e
Σ
e
+
λ
1
1
∂β 2
(a0 Σ−1 WN Σ−1 a)1/2
(a0 Σ−1 WN Σ−1 a)3/2

∂ 2 QN (θ)
a0 Σ−1 WN Σ−1 e1
= −a0 Σ−1 e1 + 1/2
∂β∂λ
2λ (a0 Σ−1 WN Σ−1 a)1/2
1/2

1 (a0 Σ−1 WN Σ−1 e1 )
∂ 2 QN (θ)
=
−
4
∂λ2
λ3/2

.

Because WN →p W ∗ , HN (θ) →p −I0 (θ∗ ). Furthermore, HN (θ) →p H (WN∗ ; θ)
uniformly on θ = (β, λ) for a compact set containing θ∗ as long as λ > 0. This
completes part (a)(ii). To show part (a)(i), we write
√
√
√
N SN (θ∗ ) ≡ N S (WN ; θ∗ ) ≡ N [S (WN ; θ∗ ) − S (W ∗ ; θ∗ )] .
27

Using vec (WN ) = DT vech (WN ), where DT is the duplication matrix (e.g. Magnus
and Neudecker (1988)), we write
√
√
N SN (θ∗ ) ≡ N [L (vech (WN ) ; θ∗ ) − L (vech (W ∗ ) ; θ∗ )] ,
√
where L : R3 → R2 . Now, N (vech (WN ) − vech (W ∗ )) converges to a normal distribution by a standard CLT. As a result, using the delta method and the information
√
identity, N SN (θ∗ ) converges to a normal distribution with zero mean and variance
Iα (θ∗ ). Part (iii) follows from Newey and McFadden (1994).
For part (b), when K/N → α > 0,
Ã
µ
¶1/2
µ
¶1/2 !
2
2
Z
1
α
α
Z
bN (θ) = − λ · a0 Σ−1 a +
1 + N2
Q
− ln 1 + 1 + N2
(20)
2
2
α
2
α
¡
¢
up to an op (N −1 ) term. All results below hold up to op N −1/2 order.
The components of the score function SN (θ) are
∂QN (θ)
2λ a0 Σ−1 WN Σ−1 e1
= −λ · a0 Σ−1 e1 +
³
´1/2
∂β
α
Z2
1 + 1 + αN2
∂QN (θ)
a0 Σ−1 a 1 a0 Σ−1 WN Σ−1 a
= −
+
³
´1/2 .
2
∂λ
2
α
ZN
1 + 1 + α2
The components of the Hessian matrix HN (θ) are
2

∂ 2 QN (θ)
8λ2
(a0 Σ−1 WN Σ−1 e1 )
2λ e01 Σ−1 WN Σ−1 e1
0 −1
−
=
−λ
·
e
Σ
e
+
µ
1
³
´
³
´
1
1/2
1/2
³
´1/2 ¶2
2
2
α
ZN
ZN
2
∂β 2
Z
3
N
1 + 1 + α2
α 1 + α2
1+ 1+ 2
α

4λ · a0 Σ−1 WN Σ−1 e1
a0 Σ−1 WN Σ−1 a
∂ 2 QN (θ)
2 a0 Σ−1 WN Σ−1 e1
= −a0 Σ−1 e1 +
−
³
´1/2
³
´1/2 µ
³
´1/2 ¶2
∂β∂λ
α
Z2
Z2
2
ZN
1 + 1 + αN2
α3 1 + αN2
1+ 1+ 2
α

∂ 2 QN (θ)
=−
∂λ2

2

(a0 Σ−1 WN Σ−1 a)
µ
³
´
1/2
³
´1/2 ¶2 .
2
Z2
ZN
α3 1 + αN2
1 + 1 + α2
2

Parts (b)(i)-(iii) follow analogously to parts (a)(i)-(iii).
Proof of Corollary 1. The determinant of Iα (θ∗ ) simplifies to
2

|Iα (θ∗ )| =

λ∗2 (a∗0 Σ−1 a∗ ) a∗0 Σ−1 a∗ · e01 Σ−1 e1 − (a∗0 Σ−1 e1 )2
.
α + 2λ∗ · a∗0 Σ−1 a∗
2 (α + λ∗ · a∗0 Σ−1 a∗ )
28

Hence, the entry (1, 1) of the inverse of Iα (θ∗ ) equals
¡

Iα (θ∗ )−1

¢
11

(a∗0 Σ−1 a∗ )2
|Iα (θ∗ )|−1
2(α + 2λ∗ a∗0 Σ−1 a∗ )
α + λ∗ · a∗0 Σ−1 a∗
a∗0 Σ−1 a∗
=
λ∗2 · a∗0 Σ−1 a∗ a∗0 Σ−1 a∗ · e01 Σ−1 e01 − (a∗0 Σ−1 e1 )2
o
σ 2u n ∗
α
= ∗2
λ + ∗0 −1 ∗ .
a Σ a
λ
=

This expression coincides with the asymptotic variance of LIMLK as described in
equation (4.7) of Bekker (1994):
(
)
2
0
2
¡
¢
(b
Σe
)
σ
2
u
Iα (θ∗ )−1 11 = ∗2
λ∗ + α · e02 Σe2 − α 0
.
b Σb
λ
Proof of Theorem 4. This result follows from standard limit of experiment arguments; see Chioda and Jansson (2007). Part (a) follows from expansions based on
(19). Part (b) follows from expansions based on (20).

7.3

Proofs of Results Stated in Section 5

For the next proofs, define the following four quantities:
c1 = tr (DB ∗ B ∗0 D0 ) + λ∗N 10T B ∗0 D0 DB ∗ 1T
2

c2 = 10T DB ∗ B ∗0 D0 1T + λ∗N (10T DB ∗ 1T )

c3 = 10T F 1T + (ρ∗ − ρ) 10T F 0 F 1T + λ∗ 10T DB ∗ 1T · 10T F 1T
c4 = (ρ∗ − ρ) tr (F 0 F ) + λ∗ {10T F 1T + (ρ∗ − ρ) 10T F 0 F 1T } .
Proof of Proposition 5. We omit the original proof here as it has been generalized
by Chamberlain and Moreira (2006).
Proof of Theorem 5. The density function of M at q is
¶
µ
¶
µ
N −T −1
1
1
0
−N/2
−1
−1
2
f (q) = C2,N · exp − tr(Σ M M ) |Σ|
|q|
exp − tr(Σ q)
2
2
µq
¶− K−2
µ
¶
q
2
0
0
×
I K−2
tr(qΣ−1 M M Σ−1 )
tr(qΣ−1 M M Σ−1 ) .
2

We obtain
Σ−1 =

D0 D
η0η
η0η 0
0
0
0
−1
−1
−1
,
tr(Σ
M
M
)
=
T
,
and
tr(qΣ
M
M
Σ
)
=
2 1T DqD 1T
2
σ2
σ2
(σ )
29

to simplify the density function of M to
µ
¶
µ 0 ¶
¡ 2 ¢− N2T
N −T −1
1
ηη
0
2
exp − 2 tr(DqD )
f (q) = C2,N · exp − 2 T σ
|q|
2σ
2σ
Ãs
Ãs
!− K−2
!
2
0η
η0η 0
η
0
0
0
×
I K−2
.
2 1T DqD 1T
2 1T DqD 1T
2
2
2
(σ )
(σ )
The density function of WN is then
g (w; β, λN ) = f (q (w)) · |q 0 (w)| = f (q (w)) N

T (T +1)
2

,

which simplifies to (13).
Proof of Theorem 6. The log-likelihood divided by N T is
µ
µ
¶¶
1
1 tr(DWN D0 ) 1
1
N
− N 2−2
2
QN (θ) = − ln σ − 2
− λ+
ln ZN
I N −2
ZN
2
2
2σ
T
2
NT
2
³ N −2 N T N −2
´
N −T −1
1
− 2
2
2
+
ln |WN | +
ln 2
N
C2,N ,
(21)
2N T
NT
q 0
1 DW D0 1
where ZN = 2 λ T σN2 T .
The second line is well-behaved when N → ∞ with T fixed. Using Stirling’s
formula,
!
Ã
NT
³ N −2 N T N −2
´
− N 2−2 1/2
2
2
1
1
N
ln 2 2 N 2 − 2 C2,N = ln Q
¡ N −t ¢ + o(1)
N −t−1
T −1
NT
T
2N
exp
− 2N
(N
−
t)
t=1
Ã
µ
¶
µ ¶!
1/2
T
−1
Q
ln (2)
1
t
1
=
− ln
1−
exp −
+ o (1)
2T
T
N
2
t=1
=

ln (2) T − 1
+
+ o (1) .
2T
2T

In addition,
0

N ·Σ+M M
E (WN ) =
= σ ∗2 B ∗ (IT + λ∗N 1T 10T ) B ∗0 ≡ WN∗ ,
N
Because V (WN ) → 0, we have WN = WN∗ + op (1). Now,
¯
¯
¡ ¢T
¡ ¢T
|WN∗ | = |B ∗ |· ¯σ ∗2 (IT + λ∗N 1T 10T )¯ ·|B ∗0 | = σ ∗2 |IT + λ∗N 1T 10T | = σ ∗2 (1 + λ∗N T ) .
As a result, ln (WN ) = T ln (σ ∗2 ) + ln (1 + λ∗N T ) + op (1) .
30

It is unknown whether the second line in (21) is well-behaved with T → ∞.
However, since it does not depend on θ, it can be ignored when finding the limiting
behavior of b
θN . Hence, define the objective function
µ
µ
¶¶
N
1
1 tr(DWN D0 ) 1
1
− N 2−2
2
b
I N −2
QN (θ) = − ln σ − 2
− λ+
ln ZN
ZN
.
2
2
2σ
T
2
NT
2
From here, we split the result into fixed T and large T asymptotics.
For part (a), in which N → ∞ with T fixed, ZN = ZN∗ + op (1), where
r
10 DWN∗ D0 1T
∗
ZN ≡ 2 λ T
.
σ2
bN (θ) = QN (θ) + op (1), where
Furthermore, Q
³
¢1/2 1
¡
¢1/2 ´
1
1 tr(DWN∗ D0 ) 1
1 ¡
−
QN (θ) = − ln σ 2 − 2
− λ+
1 + ZN∗2
ln 1 + 1 + ZN∗2
.
2
2σ
T
2
2T
2T
The first order condition (FOC) for QN (θ) is given by
∂QN (θ)
σ ∗2 (ρ∗ − ρ) tr (F F 0 ) + λ∗ {10T F 1T + (ρ∗ − ρ) 10T F 0 F 1T }
= 2
∂ρ
σ
T
∗
0
∗2
∗
λ
1T F 1T + (ρ − ρ) 10T F 0 F 1T − λ∗ (T + (ρ∗ − ρ) 10T F 1T )
σ
− 2
σ 1 + (1 + ZN∗2 )1/2
T
∗
∗2
∗2
∂QN (θ)
1
σ
c1
σ
λN
c2
=
−
+
−
2
2
∗2
∂σ 2
2σ 2 2 (σ 2 ) T
(σ 2 ) 1 + (1 + ZN )1/2 T
∂QN (θ)
1 σ ∗2
1
c2
=− + 2
.
∗2 1/2
∂λ
2
σ 1 + (1 + ZN ) T
The value θ∗ = (ρ∗ , σ ∗2 , λ∗N ) minimizes QN (θ), setting the FOC to zero.
For parts (a)(i),(ii), QN (θ) →p Q (θ) (uniformly in Θ compact) given by
³
¢
¡
¢ ´
1
1 tr(DW ∗ D0 ) 1
1 ¡
1
2
∗2 1/2
∗2 1/2
QN (θ) = − ln σ − 2
− λ+
1+Z
−
ln 1 + 1 + Z
,
2
2σ
T
2
2T
2T
where W ∗ and Z ∗ are defined as
r
W ∗ = σ ∗2 B ∗ (IT + λ∗ 1T 10T ) B ∗0 and Z ∗ = 2

λ

Since θ ∈ Θ compact and Q (θ) is continuous, b
θN →p θ.
Part (a)(iii) follows analogously to Theorem 2-(a)(iii).

31

10T DW ∗ D0 1T
.
σ2

(22)

For part (b), the dimension of WN changes as T → ∞. Yet, for |ρ∗ | < 1,
tr(DWN∗ D0 )
tr(DWN D0 )
=
+ op (1) and
T
T
10T DWN D0 1T
10T DWN∗ D0 1T
=
+ op (1) .
T2
T2
As a result, QN (θ) = QN (θ) + op (1), where
1
1 tr(DWN∗ D0 ) 1
1 ZN∗
QN (θ) = − ln σ 2 − 2
− λ+
.
2
2σ
T
2
2 T
The first order condition (FOC) for QN (θ) is given by
∂QN (θ)
σ ∗2 (ρ∗ − ρ) tr (F F 0 ) + λ∗ {10T F 1T + (ρ∗ − ρ) 10T F 0 F 1T }
= 2
∂ρ
σ
T
1/2

−

(σ ∗2 )

(σ 2 )1/2

λ∗1/2 λ1/2
10T F 1T
1 + (1 + ZN∗2 )1/2 T
1/2

∂QN (θ)
1
σ ∗2 c1 (σ ∗2 ) λ1/2 λ∗1/2 10T DB ∗ 1T
=
−
+
−
∂σ 2
2σ 2 2 (σ 2 )2 T
T
2 (σ 2 )3/2
1/2

∂QN (θ)
1 (σ ∗2 ) λ∗1/2 10T DB ∗ 1T
=− +
.
∂λ
2 2 (σ 2 )1/2 λ1/2
T
The value θ∗ = (ρ∗ , σ ∗2 , λ∗N ) minimizes QN (θ), setting the FOC to zero.
For parts (b)(i),(ii), QN (θ) = Q (θ) + op (1) (uniformly in Θ compact), given by
1
1
tr(DW ∗ D0 ) 1
Q (θ) = − ln σ 2 − 2 lim
− λ
2
2σ T →∞
T
2
³
¢
¡
¢1/2 ´
1 ¡
1
1/2
+ lim
1 + Z ∗2
− lim
ln 1 + 1 + Z ∗2
.
T →∞ 2T
T →∞ 2T
where W ∗ and Z ∗ are defined in (22). Since θ ∈ Θ compact and Q (θ) is continuous,
b
θN →p θ.
Part (b)(iii) follows analogously to Theorem 2-(a)(iii).
Proof of Theorem 7. First, we prove part (a). The objective function is
³
´
2 1/2
ln
1
+
(1
+
Z
)
2
0
2 1/2
N
bN (θ) = − ln σ − tr(DWN D ) − λ + (1 + ZN ) −
Q
2
2
2σ T
2
2T
2T
¢
¡
up to an op (N −1 ) term. All results below hold up to op N −1/2 order.
32

(23)

The components of the score function SN (θ) are
1 tr (JT WN D0 )
2λ
10T JT WN D0 1T
∂QN (θ)
= 2
−
∂ρ
σ
T
1 + (1 + ZN2 )1/2
T
0
∂QN (θ)
1
1 tr (DWN D )
1
λ
10T DWN D0 1T
=
−
+
−
∂σ 2
2σ 2 2 (σ 2 )2
T
T
(σ 2 )2 1 + (1 + ZN2 )1/2
0
0
∂QN (θ)
1
1
1
1T DWN D 1T
=− + 2
.
2 1/2
∂λ
2 σ 1 + (1 + ZN )
T
The Hessian matrix HN (θ) →p −IT (θ), whose components are
10T F 0 F 1T + λ (10T F 1T )2 σ 2 tr (F 0 F ) + λ∗ 10T F 0 F 1T
∂ 2 QN (θ)
σ2
2λ
=
− 2
∂ρ2
σ 2 1 + (1 + ZN∗2 )1/2
T
σ
T
µ ∗ 2 ¶2
2
1
(c3 )
σ
8λ2
−
2
∗2
2
1/2
∗2
σ
T
(1 + (1 + ZN )1/2 ) (1 + ZN )
∗
∗
2
∂N
Q (θ)
σ 2 c4
σ2
2λ
c3
=
−
+
2
2
∗2
2
1/2
∂ρ∂σ
(σ 2 ) T
(σ 2 ) 1 + (1 + ZN ) T
½
¾
∗2
σ
2λc2
1
× 1− 2
σ 1 + (1 + ZN∗2 )1/2 (1 + ZN∗2 )1/2
½
¾
∗
∗
2
∂N
Q (θ)
σ2
2
c3
σ2
2λc2
1
= − 2
1− 2
∂ρ∂λ
σ 1 + (1 + ZN∗2 )1/2 T
σ 1 + (1 + ZN∗2 )1/2 (1 + ZN∗2 )1/2
¡ ∗ 2 ¢2
2
σ
2λ2
Q (θ)
1
(c2 )2
∂N
=
−
∂ (σ 2 )2
(σ 2 )4 (1 + (1 + ZN∗2 )1/2 )2 (1 + ZN∗2 )1/2 T
∗
∗
2λ
1
σ 2 c1
σ2
c2
+
−
+
2
3
3
∗2
1/2
2 (σ 2 )
(σ 2 ) T
(σ 2 ) 1 + (1 + ZN ) T
½
¾
∗
∗
2
∂N
1
Q (θ)
σ2
c2
σ2
2λc2
1
= −
1− 2
∂σ 2 ∂λ
σ 1 + (1 + ZN∗2 )1/2 (1 + ZN∗2 )1/2
(σ 2 )2 1 + (1 + ZN∗2 )1/2 T
µ ∗ 2 ¶2
2
∂N
Q (θ)
σ
2
1
(c2 )2
=
−
.
2
∗2
σ2
∂λ2
(1 + (1 + ZN∗2 )1/2 ) (1 + ZN )1/2 T
∗

∗

This convergence is uniform on θ = (β, λ) for a compact set containing θ∗ as long as
λ > 0. This completes part (a)(ii). To show part (a)(i), we write
√
√
√
N T SN (θ∗ ) ≡ N T S (WN ; θ∗ ) ≡ N T [S (WN ; θ∗ ) − S (W ∗ ; θ∗ )] .
Using vec (WN ) = DT vech (WN ), where DT is the duplication matrix (e.g. Magnus
and Neudecker (1988)), we write
√
√
N T SN (θ∗ ) ≡ N T [L (vech (WN ) ; θ∗ ) − L (vech (W ∗ ) ; θ∗ )] ,
33

√
T (T +1)
where L : R 2 → R3 . Now, N T (vech (WN ) − vech (W ∗ )) converges to a normal distribution by a standard CLT. As a result, using the delta method and the
√
information identity, N T SN (θ∗ ) converges to a normal distribution with zero mean
and variance IT (θ). Part (iii) follows from Newey and McFadden (1994).
Part (b) follows from the asymptotic normality of the score (whose variance is
given by the reciprocal of the inverse of the limit of the Hessian matrix). As the
remainder terms from expansions based on (23) are asymptotically negligible, (14)
holds true.
Proof of Proposition 6. First, we prove part (a). The first moment of WN is
EθN [WN ] = σ 2 B {IT + λN · 1T 10T } B.

(24)

The matrix EθN [WN ] is symmetric and has T (T + 1) /2 nonredundant elements.
¡
¢
For each observation i, m wi ; ρ, σ 2 , (η i /σ)2 depends on a different parameter
(η i /σ)2 . By averaging out (15), we can identify the parameter θ∗N = (ρ∗ , σ ∗2 , λ∗N ):
¸
· X
¡
N
1
2¢
2
m wi ; ρ, σ , (η i /σ)
Eθ∗N [m (WN ; θN )] = Eθ∗N
i=1
N
£
¡
¢¤
= Eθ∗N vech WN − σ 2 B {IT + λN · 1T 10T } B
= 0 if and only if θN = θ∗N .
Every GMM estimator of ρ that we are aware of is invariant to orthogonal transformations and implicitly uses a subset of the T (T + 1)/2 moment conditions given
by (16). This includes Arellano and Bond’s (1991) and Ahn and Schmidt’s (1995)
estimators. Specifically, the existing GMM estimators use the moment given by
£
¤
EθN m0 (WN ; θN ) = 0,

(25)

where m0 (WN ; θN ) = δ 00 m (WN ; θN ) for a suitably chosen matrix δ 0 with T (T + 1)/2
columns. For example, Arellano and Bond’s (1991) differentiates the data to construct
a (T − 2)(T − 1)/2-dimensional function m0 (WN ; θN ) with entries
1 XN
yi,t0 (4yi,t − ρ · 4yi,t−1 ) , t = 3, . . . , T, t0 = 1, . . . , t − 2.
i=1
N
This choice of m0 (wi ; θN ) yields the (T − 2)(T − 1)/2 × T (T + 1)/2 block-diagonal

34

matrix




δ =


0





HT −2
HT −3



 , where Ht is a matrix with t lines:



...
H1



ρ − (1 + ρ)
1


ρ
− (1 + ρ) 1
Ht = 
..
.. ..

.
.
.




.



(26)

ρ − (1 + ρ) 1
It is interesting to see Arellano and Bond’s (1991) moment conditions using
m (WN ; θN ) = δ 00 m (WN ; θN ). Their moment conditions arise exactly because
£©
¡
¢ª¤
δ 00 m (WN ; θN ) = δ 00 vech (WN ) − vech σ 2 B {IT + λN · 1T 10T } B 0
0

= δ 00 vech (WN ) − δ 00 DT+ (B ⊗ B)vec(σ 2 {IT + λN · 1T 10T })
= δ 00 vech (WN ) ,

(27)

where DT+ is the Moore-Penrose inverse of DT (e.g. Magnus and Neudecker (1988)).
Expressions (26) and (27) also illustrate how differentiating the data imposes particular structures on δ 0 :
¡
¢
Eθ∗N [m (WN ; θN )] = vech σ ∗2 B ∗ {IT + λ∗N · 1T 10T } B ∗ − σ 2 B {IT + λN · 1T 10T } B
£
¤
£
¡
¢¤
Eθ∗N m0 (WN ; θN ) = Eθ∗N δ 00 vech σ ∗2 B ∗ {IT + λ∗N · 1T 10T } B ∗ .
Part (b) follows from standard results, e.g., Theorems 2.1 and 3.2 of Newey and
McFadden (1994).
For part (c), we assume that (η ∗i /σ ∗ )2 is known to be fixed at λ∗ , i = 1, ..., N :
¡
¢
¡
¢
m wi ; ρ, σ 2 , λ = vech (wi ) − vech σ 2 B {IT + λ · 1T 10T } B .
Hence, we can apply Chamberlain’s (1987) efficiency bound to moment conditions:
¶0
½ µ
µ
¶¾−1
¢ª−1
∂m (wi ; θ∗ ) © ¡
∂m (wi ; θ∗ )
∗
∗ 0
E m (wi ; θ ) m (wi ; θ )
E
E
= (ζ 0 Ξζ)−1 .
∂θ
∂θ0
Proof of Corollary 2. As a preliminary result, we need to find the limits of
0
T −1 tr(F F ), T −1 10T F 1T , and T −1 10T F 0 F 1T , as T → ∞. For the first term,
j
−2 P
−1
−1
1 TP
1
T − 1 TP
1 TP
1
tr (F F 0 ) =
,
ρ∗2i =
ρ∗2i −
iρ∗2i →
T
T j=0 i=0
T i=0
T i=0
1 − ρ∗2

35

P −1 ∗2 i
because Ti=0
i(ρ ) is a convergent series. This is true because a sufficient condition
p
PT
for a series i=0 ai to converge is that lim T |aT | < 1 as T → ∞. Taking ai = i(ρ∗2 )i ,
p
p
√
lim T |aT | = lim T |T (ρ∗2 )T | = ρ∗2 lim T T = ρ∗2 < 1. Analogously,
j
−2 P
−1
−1
1 0
1 TP
T − 1 TP
1 TP
1
1T F 1T =
ρ∗i =
ρ∗i −
iρ∗i →
.
T
T j=0 i=0
T i=0
T i=0
1 − ρ∗

because

PT −1
i=0

µ

iρ∗i also converges. Finally, by the Cauchy–Schwarz inequality,

1 0
1 F 1T
T T

¶2

µ j
¶2
µ
¶2
−2 P
1 0 0
1 TP
T −1
1
∗i
≤ 1T F F 1T =
ρ
≤
.
T
T j=0 i=0
T
1 − ρ∗

Taking limits, we obtain
1
1
1
1 0 0
1T F F 1T ≤ lim sup 10T F 0 F 1T ≤
.
2 ≤ lim inf
∗
T
T
(1 − ρ )
(1 − ρ∗ )2
Hence, the limit of T −1 10T F 0 F 1T exists and equals (1 − ρ∗ )−2 . This result implies that
2

(T −1 10T F 1T )
(10T F 1T )2
hxT , yT i2
= 0
=
→ 1 as T → ∞,
T −1 10T F 0 F 1T
(1T 1T ) (10T F 0 F 1T )
hxT , xT i hyT , yT i
where xT and yT are sequences of elements in the Hilbert space (with hxT , yT i as the
usual inner product) in which the first T entries equal 1T and F 1T , respectively, and
zero otherwise.
Therefore, the limiting information matrix I∞ (θ∗ ) simplifies to


I∞ (θ∗ ) = 

1
1−ρ∗2

+

λ∗
(1−ρ∗ )2

λ∗
2σ ∗2 (1−ρ∗ )
1
2(1−ρ∗ )

λ∗
2σ ∗2 (1−ρ∗ )
2+λ∗
4(σ ∗2 )2
1
4σ ∗2

1
2(1−ρ∗ )
1
4σ ∗2
1
4λ∗



.

The entry (1, 1) of the inverse of I∞ (θ∗ ) is
¡

I∞ (θ∗ )−1

¢
11

−1
= (A11 − A12 A−1
22 A21 ) ,

where the matrices Ajk are partitions of I∞ (θ∗ ):
A11 =
A21 =

h
λ∗
1
+
,
A
=
12
1 − ρ∗2 (1 − ρ∗ )2
"
#
"
∗
λ
2σ ∗2 (1−ρ∗ )
1
2(1−ρ∗ )

, and A22 =

36

λ∗

2σ ∗2 (1−ρ∗ )
2+λ∗
4(σ ∗2 )2
1
4σ ∗2

1
4σ ∗2
1
4λ∗

1
2(1−ρ∗ )

#
.

i

The following holds true:
h
1
=
(1 − ρ∗ )2

A12 A−1
22 A21

∗

λ
σ ∗2

i
1

"

2+λ∗
(σ ∗2 )2
1
σ ∗2

1
σ ∗2
1
λ∗

#−1 "

λ∗
σ ∗2

#

1

#
i
1
−1
λ∗
λ∗ (σ ∗2 )2 h λ∗
∗
λ
σ ∗2
σ ∗2
=
1
−1
2+λ∗
2
σ ∗2
∗
1
2 (1 − ρ )
σ ∗2
(σ ∗2 )2
½
¾
λ∗ (σ ∗2 )2
λ∗
2λ∗
2
λ∗
−
+
+
=
2 (1 − ρ∗ )2 (σ ∗2 )2 (σ ∗2 )2 (σ ∗2 )2 (σ ∗2 )2
λ∗
=
.
(1 − ρ∗ )2
"

#"

As a result, we obtain
¡

µ

¢
∗ −1

I∞ (θ )

11

=

λ∗
1
λ∗
+
−
1 − ρ∗2 (1 − ρ∗ )2 (1 − ρ∗ )2

¶−1
= 1 − ρ∗2 .

Proof of Theorem 8. When T → ∞, the objective function is
0
bN (θ) = − 1 ln σ 2 − 1 tr(DWN D ) − 1 λ − 1 ZN
Q
2
2σ 2
T
2
2T
¡
¢
up to an op (N −1 ) term. All results below hold up to op N −1/2 order.
The components of the score function SN (θ) are

∂QN (θ)
1 tr (JT WN D0 )
λ1/2
10T JT WN D0 1T
= 2
−
∂ρ
σ
T
(σ 2 )1/2 T (10T DWN D0 1T )1/2
∂QN (θ)
1
1 tr (DWN D0 )
λ1/2 (10T DWN D0 1T )1/2
=− 2+
−
∂σ 2
2σ
T
T
2 (σ 2 )2
2 (σ 2 )3/2
(10T DWN D0 1T )1/2
∂QN (θ)
1
1
=− +
.
∂λ
2 2 (σ 2 )1/2 λ1/2
T
If |ρ∗ | is bounded away from one, as T → ∞,
tr(JT WN D0 )
→
T
tr(DWN D0 )
→
T

tr(JT WN∗ D0 ) 10T JT WN D0 1T
10T JT WN∗ D0 1T
,
→
lim
p
T
T2
T2
∗
0
0
0
0
tr(DWN D )
1T DWN D 1T
1T DWN∗ D0 1T
lim
,
and
→
lim
.
p
p
T
T2
T2
p

lim

37

As a result, the Hessian matrix −HN (θ) →p I∞ (θ), whose components are limits of
∗

∂ 2 QN (θ)
σ 2 tr (F 0 F ) + λ∗ 10T F 0 F 1T
=
∂ρ2
σ2
T
¡ ∗ ¢1/2
∗
∂ 2 QN (θ)
σ 2 c4 λ1/2 λ∗1/2 σ 2
10T F 1T
−
=
−
∂ρ∂σ 2
T
(σ 2 )2 T
2 (σ 2 )3/2
−

1/2

∂ 2 QN (θ)
(σ ∗2 ) λ∗1/2 10T F 1T
−
=
∂ρ∂λ
2 (σ 2 )1/2 λ3/2 T
¡ ∗ ¢1/2 1/2 ∗1/2
∗
λ λ
∂ 2 QN (θ)
σ 2 c1 3 σ 2
10T DB ∗ 1T
1
−
=
−
−
2
3
5/2
2
2
2
4
T
∂ (σ )
(σ ) T
2 (σ 2 )2
(σ )
1/2

−

∂ 2 QN (θ)
(σ ∗2 ) λ∗1/2 10T DB ∗ 1T
=
∂σ 2 ∂λ
T
4 (σ 2 )3/2 λ1/2
1/2

∂ 2 QN (θ)
(σ ∗2 ) λ∗1/2 10T DB ∗ 1T
−
=
.
T
∂λ2
4 (σ 2 )1/2 λ3/2
This convergence is uniform on θ = (β, λ) for a compact set containing θ∗ as long as
|ρ∗ | is bounded away from one. This completes part (ii). To show part (i), define
³
´0
0
∗0
0 D ∗0 1
∗
0
∗0
∗0
10T D∗ WN
T
ND )
WN = tr(JT WT N D ) 1T JT WTN2 D 1T tr(D W
and
T
T2
³
´0
∗
∗0
0
∗
∗0
0
∗
∗
∗0
∗
∗
∗0
WN∗ = tr(JT WT N D ) 1T JT WTN2 D 1T tr(D WT N D ) 1T D WTN2 D 1T
,
and write

√

√
N T SN (θ∗ ) ≡ N T [L (WN ; θ∗ ) − L (WN∗ ; θ∗ )] ,
√
where L : R4 → R3 . Now, N T (WN − WN∗ ) converges to a normal distribution
by a standard CLT and the Cramér-Wold device. Using the delta method and the
√
information identity, N T SN (θ∗ ) converges to a normal distribution with zero mean
and variance I∞ (θ∗ ) as long as N ≥ T . Part (iii) follows from Newey and McFadden
(1994).

38

References
Abrevaya, J. (2000): “Rank Estimation of a Generalized Fixed-Effects Regression
Model,” Journal of Econometrics, 95, 1–23.
Ahn, S., and P. Schmidt (1995): “Efficient Estimation of Models for Dynamic
Panel Data,” Journal of Econometrics, 68, 5–27.
Andersen, E. B. (1970): “Asymptotic Properties of Conditional Maximum Likelihood Estimators,” Journal of the Royal Statistical Society, Series B, 32, 283–301.
Anderson, T. W. (1946): “The Noncentral Wishart Distribution and Certain Problems of Multivariate Statistics,” The Annals of Mathematical Statistics, 17, 409–
431.
Anderson, T. W., N. Kunitomo, and Y. Matsushita (2006): “A New Light
from Old Wisdoms: Alternative Estimation Methods of Simultaneous Equations
and Microeconometric Models,” Unpublished Manuscript, University of Tokyo.
Andrews, D. W. K., M. J. Moreira, and J. H. Stock (2006): “Optimal
Two-Sided Invariant Similar Tests for Instrumental Variables Regression,” Econometrica, 74, 715–752.
Arellano, M. (2003): “Panel Data Econometrics,” in Advanced Texts in Econometrics, ed. by M. Arellano, G. Imbens, G. Mizon, A. Pagan, and M. Watson.
Oxford.
Arellano, M., and S. R. Bond (1991): “Some Tests of Specification for Panel
Data: Monte Carlo Evidence and an Application to Employment Equations,” Review of Economic Studies, 58, 277–297.
Arellano, M., and B. Honoré (1991): “Panel Data Models: Some Recent Developments,” in Handbook of Econometrics, ed. by J. Heckman, and E. Leamer,
vol. 5, chap. 53, pp. 775–826. Elsevier Science, Amsterdam.
Bekker, P. A. (1994): “Alternative Approximations to the Distributions of Instrumental Variables Estimators,” Econometrica, 62, 657–681.
Bekker, P. A., and V. der Ploeg (2005): “Instrumental Variable Estimation
Based on Grouped Data,” Statistica Neerlandica, 59, 239–267.

39

Chamberlain, G. (1987): “Asymptotic Efficiency in Estimation With Conditional
Moment Restrictions,” Journal of Econometrics, 34, 305–334.
(2007): “Decision Theory Applied to an Instrumental Variables Model,”
Econometrica, 75, 609–652.
Chamberlain, G., and M. J. Moreira (2006): “Decision Theory Applied to a
Linear Panel Data Model,” Unpublished Manuscript, Harvard University.
Chioda, L., and M. Jansson (2007): “Optimal Invariant Inference when the Number of Instruments is Large,” Unpublished Manuscript, UC Berkeley.
Hahn, J., and G. Kuersteiner (2002): “Asymptotically Unbiased Inference for a
Dynamic Panel Model with Fixed Effects When Both N and T are Large,” Econometrica, 70, 1639–1657.
Hall, W., R. Wijsman, and J. Ghosh (1965): “The Relationship Between Sufficiency and Invariance with Applications in Sequential Analysis,” Annals of Mathematical Statistics, 36, 575–614.
Hansen, C., J. Hausman, and W. K. Newey (2006): “Estimation with Many
Instrumental Variables,” Unpublished Manuscript, University of Western Ontario.
Harville, D. (1974): “Bayesian Inference for Variance Components Using Only
Error Contrasts,” Biometrika, 61, 383–385.
Johnson, N. L., and S. Kotz (1970): Distributions in Statistics: Continuous
Multivariate Distributions. New York: John Wiley and Sons.
Kunitomo, N. (1980): “Asymptotic Expansions of Distributions of Estimators in
a Linear Functional Relationship and Simultaneous Equations,” Journal of the
American Statistical Association, 75, 693–700.
Lancaster, T. (2000): “The Incidental Parameter Problem Since 1948,” Journal
of Econometrics, 95, 391–413.
(2002): “Orthogonal Parameters and Panel Data,” Journal of Econometrics,
69, 647–666.
Le Cam, L., and G. L. Yang (2000): Asymptotics in Statistics: Some Basic
Concepts. 2nd edn., Springer-Verlag.

40

Lehmann, E. L., and J. P. Romano (2005): Testing Statistical Hypotheses. Third
edn., Springer.
Magnus, J. R., and H. Neudecker (1988): Matrix Differential Calculus with
Applications in Statistics and Econometrics. Wiley, New York.
Moreira, M. J. (2001): “Tests with Correct Size When Instruments Can Be Arbitrarily Weak,” Center for Labor Economics Working Paper Series, 37, UC Berkeley.
Morimune, K. (1983): “Approximate Distributions of k-Class Estimators When the
Degree of Overidentification is Large Compared With Sample Size,” Econometrica,
51, 821–841.
Muirhead, R. J. (2005): Aspects of Multivariate Statistical Theory.
Newey, W., and D. L. McFadden (1994): “Large Sample Estimation and Hypothesis Testing,” in Handbook of Econometrics, ed. by R. F. Engle, and D. L.
McFadden, vol. 4, chap. 36, pp. 2111–2245. Elsevier Science, Amsterdam.
Newey, W. K. (2004): “Efficient Semiparametric Estimation Via Moment Restrictions,” Econometrica, 72, 1877–1897.
Neyman, J., and E. L. Scott (1948): “Consistent estimates based on partially
consistent observations,” Econometrica, 16, 1–32.
Potscher, B. M., and I. R. Prucha (1997): Dynamic Nonlinear Econometric
Models. Springer-Verlag.
Sweeting, T. J. (1989): “On Conditional Weak Convergence,” Journal of Theoretical Probability, 2, 461–474.
van der Vaart, A. W. (1998): Asymptotic Statistics. Cambridge: Cambridge
University Press.
van Hasselt, M. (2007): “Instrumental Variables Estimators, Many Instruments,
and Nonnormality,” Unpublished Manuscript, University of Western Ontario.

41

42

TABLE I
Performance of Estimators for the Autoregressive Parameter ρ
(random effects, normal errors, and ρ = 0.50)
Mean
MSE
T
N
MILE BCOLS
AB
AS
MILE BCOLS
AB
2
5
0.4592
0.9651
*
*
0.1552
0.4602
*
2
10
0.4859
0.9500
*
*
0.0631
0.3109
*
2
25
0.4960
0.9523
*
*
0.0246
0.2394
*
2 100
0.4974
0.9474
*
*
0.0054
0.2083
*
3
5
0.4431
0.7695 -0.0578 0.8642
0.0631
0.1607 516.8489
3
10
0.4789
0.7903
0.9766 0.8954
0.0280
0.1165 153.1105
3
25
0.4908
0.8008
0.5705 0.9389
0.0115
0.1045
4.7087
3 100
0.4979
0.8068
0.5372 0.9632
0.0024
0.0975
0.0724
5
5
0.4626
0.6469
0.1980 0.6541
0.0231
0.0538
0.2323
5
10
0.4802
0.6657
0.2386 0.7162
0.0116
0.0422
0.2145
5
25
0.4935
0.6702
0.3768 0.7940
0.0044
0.0347
0.0869
5 100
0.4991
0.6799
0.4650 0.8667
0.0010
0.0336
0.0136
10
5
0.4731
0.5505
0.0385 0.3753
0.0122
0.0158
52.4500
10
10
0.4861
0.5660
0.3249 0.4518
0.0049
0.0107
0.0489
10
25
0.4937
0.5717
0.3977 0.5763
0.0021
0.0074
0.0211
10 100
0.4993
0.5736
0.4625 0.7223
0.0005
0.0060
0.0058
25
5
0.4871
0.5128
**
**
0.0048
0.0055
**
25
10
0.4930
0.5151
**
**
0.0025
0.0025
**
25
25
0.4966
0.5180
**
**
0.0010
0.0013
**
25 100
0.4997
0.5184
**
**
0.0002
0.0006
**
100
5
0.4941
0.5014
**
**
0.0014
0.0013
**
100
10
0.4978
0.5018
**
**
0.0007
0.0007
**
100
25
0.4990
0.5001
**
**
0.0003
0.0003
**
100 100
0.4997
0.5015
**
**
0.0001
0.0001
**
(*) The estimator is not available for T = 2.
(**) Computational cost is prohibitive for large T.
AS
*
*
*
*
0.3823
0.2559
0.2219
0.2204
0.0991
0.0820
0.1002
0.1371
0.0747
0.0437
0.0294
0.0550
**
**
**
**
**
**
**
**

43

TABLE II
Performance of Estimators for the Autoregressive Parameter ρ
(nonconvergent effects, normal errors, and ρ = 0.50)
Mean
MSE
T
N
MILE BCOLS
AB
AS
MILE BCOLS
AB
2
5
0.4770
1.0835
*
*
0.0818
0.5044
*
2
10
0.4911
1.1389
*
*
0.0196
0.4442
*
2
25
0.4989
1.1994
*
*
0.0037
0.4959
*
2 100
0.5000
1.2352
*
*
0.0002
0.5410
*
3
5
0.4773
0.8349
0.2500 0.9455
0.0346
0.1603
384.7828
3
10
0.4908
0.9110
0.5705 0.9203
0.0087
0.1818
0.5864
3
25
0.4981
0.9636
0.5160 0.8997
0.0013
0.2173
0.0173
3 100
0.4992
0.9904
0.5013 0.8231
0.0001
0.2406
0.0009
5
5
0.4727
0.6997
0.2452 0.7159
0.0165
0.0603
0.1766
5
10
0.4918
0.7415
0.4475 0.7635
0.0043
0.0640
0.0339
5
25
0.4991
0.7755
0.4912 0.7902
0.0007
0.0768
0.0046
5 100
0.4997
0.7936
0.4988 0.7854
0.0000
0.0863
0.0002
10
5
0.4789
0.5798 -0.9436 0.4278
0.0080
0.0151 1721.7952
10
10
0.4908
0.6104
0.4005 0.5980
0.0024
0.0148
0.0197
10
25
0.5027
0.6326
0.4806 0.7370
0.0014
0.0180
0.0022
10 100
0.5000
0.6452
0.4988 0.7765
0.0000
0.0211
0.0001
25
5
0.4884
0.5157
**
**
0.0040
0.0042
**
25
10
0.4949
0.5330
**
**
0.0014
0.0027
**
25
25
0.4995
0.5464
**
**
0.0003
0.0024
**
25 100
0.4999
0.5562
**
**
0.0000
0.0032
**
100
5
0.4964
0.4994
**
**
0.0013
0.0014
**
100
10
0.4987
0.5038
**
**
0.0006
0.0005
**
100
25
0.4994
0.5076
**
**
0.0002
0.0002
**
100 100
0.5001
0.5119
**
**
0.0000
0.0002
**
(*) The estimator is not available for T = 2.
(**) Computational cost is prohibitive for large T.
AS
*
*
*
*
0.3733
0.2215
0.1719
0.1049
0.0873
0.0795
0.0861
0.0816
0.0516
0.0281
0.0583
0.0765
**
**
**
**
**
**
**
**

44

TABLE III
Performance of Estimators for the Autoregressive Parameter ρ
(random effects, nonnormal errors, and ρ = 0.50)
Mean
MSE
T
N
MILE BCOLS
AB
AS
MILE BCOLS
AB
2
5
0.4520
0.9797
*
*
0.1430
0.5085
*
2
10
0.5024
0.9975
*
*
0.0869
0.3687
*
2
25
0.4993
0.9665
*
*
0.0414
0.2711
*
2 100
0.5042
0.9507
*
*
0.0105
0.2175
*
3
5
0.4666
0.7910 0.3562 0.8923
0.0687
0.1811
31.5729
3
10
0.4803
0.8056 0.4189 0.9204
0.0343
0.1373
59.3092
3
25
0.4951
0.8054 0.3363 0.9376
0.0143
0.1104
53.3848
3 100
0.4992
0.8091 0.5244 0.9683
0.0030
0.0999
0.0839
5
5
0.4712
0.6629 0.2628 0.6585
0.0268
0.0647
0.1905
5
10
0.4821
0.6704 0.3211 0.6975
0.0150
0.0456
0.1282
5
25
0.4928
0.6778 0.3899 0.7748
0.0045
0.0380
0.0810
5 100
0.4967
0.6798 0.4717 0.8539
0.0011
0.0339
0.0128
10
5
0.4722
0.5602 0.0781 0.3906
0.0110
0.0175 162.8453
10
10
0.4893
0.5663 0.3471 0.4507
0.0047
0.0105
0.0405
10
25
0.4946
0.5721 0.4084 0.5625
0.0020
0.0077
0.0178
10 100
0.4984
0.5745 0.4740 0.7154
0.0005
0.0061
0.0035
25
5
0.4819
0.5113
**
**
0.0052
0.0046
**
25
10
0.4890
0.5157
**
**
0.0024
0.0026
**
25
25
0.4974
0.5182
**
**
0.0010
0.0014
**
25 100
0.4990
0.5187
**
**
0.0003
0.0006
**
100
5
0.4949
0.4997
**
**
0.0015
0.0014
**
100
10
0.4972
0.5004
**
**
0.0007
0.0007
**
100
25
0.5000
0.5015
**
**
0.0003
0.0003
**
100 100
0.5000
0.5016
**
**
0.0001
0.0001
**
(*) The estimator is not available for T = 2.
(**) Computational cost is prohibitive for large T.
AS
*
*
*
*
0.4008
0.2723
0.2233
0.2278
0.1359
0.0872
0.0914
0.1291
0.0840
0.0516
0.0309
0.0514
**
**
**
**
**
**
**
**

45

TABLE IV
Performance of Estimators for the Autoregressive Parameter ρ
(random effects, normal errors, and ρ = -0.50)
Mean
MSE
T
N
MILE BCOLS
AB
AS
MILE BCOLS
AB
2
5
-0.5489 -0.5689
*
*
0.1706
0.2478
*
2
10
-0.5206 -0.5622
*
*
0.0694
0.1020
*
2
25
-0.5024 -0.5485
*
*
0.0269
0.0374
*
2 100
-0.5047 -0.5476
*
*
0.0058
0.0104
*
3
5
-0.4920 -0.4907 -0.0209 -0.3722
0.0801
0.0791
20.5152
3
10
-0.5006 -0.4994 -0.4555 -0.4485
0.0326
0.0352
4.0370
3
25
-0.5024 -0.5087 -0.4951 -0.4990
0.0117
0.0146
0.0409
3 100
-0.5020 -0.5063 -0.4948 -0.5368
0.0031
0.0033
0.0080
5
5
-0.4878 -0.4728 -0.5408 -0.3755
0.0339
0.0371
0.0549
5
10
-0.4971 -0.4871 -0.5262 -0.4113
0.0156
0.0202
0.0326
5
25
-0.5000 -0.5007 -0.5153 -0.4608
0.0069
0.0073
0.0136
5 100
-0.4992 -0.5021 -0.5030 -0.4860
0.0017
0.0017
0.0033
10
5
-0.4947 -0.4779
0.6536 -0.4602
0.0157
0.0181 3313.3070
10
10
-0.4965 -0.4944 -0.5334 -0.4563
0.0083
0.0078
0.0098
10
25
-0.4987 -0.4951 -0.5144 -0.4541
0.0031
0.0032
0.0046
10 100
-0.4995 -0.4984 -0.5024 -0.4552
0.0008
0.0008
0.0014
25
5
-0.4958 -0.4921
**
**
0.0061
0.0066
**
25
10
-0.4986 -0.4952
**
**
0.0033
0.0030
**
25
25
-0.4988 -0.4994
**
**
0.0013
0.0012
**
25 100
-0.4996 -0.4998
**
**
0.0003
0.0003
**
100
5
-0.4996 -0.4986
**
**
0.0016
0.0015
**
100
10
-0.5002 -0.4992
**
**
0.0008
0.0008
**
100
25
-0.4997 -0.4999
**
**
0.0003
0.0003
**
100 100
-0.5000 -0.4993
**
**
0.0001
0.0001
**
(*) The estimator is not available for T = 2.
(**) Computational cost is prohibitive for large T.
AS
*
*
*
*
0.3044
0.1651
0.0578
0.0129
0.1201
0.0713
0.0310
0.0069
0.0343
0.0211
0.0122
0.0041
**
**
**
**
**
**
**
**

46

TABLE V
Performance of Estimators for the Autoregressive Parameter ρ
(random effects, normal errors, and ρ = 1.00)
Mean
MSE
T
N
MILE BCOLS
AB
AS
MILE BCOLS
AB
2
5
0.9307
1.6990
*
*
0.1316
0.7595
*
2
10
0.9766
1.7115
*
*
0.0679
0.6034
*
2
25
1.0009
1.6943
*
*
0.0274
0.5166
*
2 100
0.9958
1.7047
*
*
0.0057
0.5048
*
3
5
0.9674
1.5029 1.0935 1.3267
0.0452
0.3211 36.9311
3
10
1.0072
1.5032 1.0299 1.3320
0.0224
0.2776
5.5735
3
25
0.9971
1.5156 1.0120 1.3469
0.0059
0.2733
0.0313
3 100
0.9975
1.5216 0.9996 1.3624
0.0015
0.2740
0.0068
5
5
0.9827
1.3241 0.9478 1.1497
0.0093
0.1190
0.0313
5
10
0.9949
1.3341 0.9838 1.1531
0.0032
0.1165
0.0089
5
25
0.9984
1.3403 0.9919 1.1659
0.0012
0.1174
0.0030
5 100
0.9999
1.3442 0.9986 1.1760
0.0003
0.1189
0.0007
10
5
0.9960
1.1774 1.2028 1.0534
0.0015
0.0330 55.2326
10
10
0.9989
1.1838 0.9892 1.0621
0.0004
0.0343
0.0007
10
25
0.9992
1.1839 0.9960 1.0680
0.0001
0.0340
0.0002
10 100
1.0000
1.1854 0.9991 1.0687
0.0000
0.0344
0.0001
25
5
0.9994
1.0765
**
**
0.0001
0.0059
**
25
10
1.0000
1.0767
**
**
0.0000
0.0059
**
25
25
0.9998
1.0776
**
**
0.0000
0.0060
**
25 100
1.0000
1.0776
**
**
0.0000
0.0060
**
100
5
1.0000
1.0197
**
**
0.0000
0.0004
**
100
10
0.9999
1.0198
**
**
0.0000
0.0004
**
100
25
1.0000
1.0198
**
**
0.0000
0.0004
**
100 100
1.0000
1.0198
**
**
0.0000
0.0004
**
(*) The estimator is not available for T = 2.
(**) Computational cost is prohibitive for large T.
AS
*
*
*
*
0.1953
0.1386
0.1318
0.1345
0.0363
0.0289
0.0294
0.0315
0.0065
0.0053
0.0051
0.0048
**
**
**
**
**
**
**
**

