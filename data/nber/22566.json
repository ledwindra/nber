{
    "id": 22566,
    "citation_title": "Predicting Experimental Results: Who Knows What?",
    "citation_author": [
        "Stefano DellaVigna",
        "Devin Pope"
    ],
    "citation_publication_date": "2016-08-25",
    "issue_date": "2016-08-25",
    "revision_date": "None",
    "topics": [
        "\n",
        "Econometrics",
        "\n",
        "Experimental Design",
        "\n",
        "Microeconomics",
        "\n",
        "Behavioral Economics",
        "\n"
    ],
    "program": [
        "\n",
        "Development Economics",
        "\n",
        "Economics of Education",
        "\n",
        "Economics of Health",
        "\n",
        "Labor Studies",
        "\n",
        "Public Economics",
        "\n",
        "Productivity, Innovation, and Entrepreneurship",
        "\n"
    ],
    "projects": null,
    "working_groups": [
        "\n",
        "Behavioral Finance",
        "\n",
        "Innovation Policy",
        "\n"
    ],
    "abstract": "\n\nAcademic experts frequently recommend policies and treatments. But how well do they anticipate the impact of different treatments? And how do their predictions compare to the predictions of non-experts? We analyze how 208 experts forecast the results of 15 treatments involving monetary and non-monetary motivators in a real-effort task. We compare these forecasts to those made by PhD students and non-experts: undergraduates, MBAs, and an online sample. We document seven main results. First, the average forecast of experts predicts quite well the experimental results. Second, there is a strong wisdom-of-crowds effect: the average forecast outperforms 96 percent of individual forecasts. Third, correlates of expertise---citations, academic rank, field, and contextual experience--do not improve forecasting accuracy. Fourth, experts as a group do better than non-experts, but not if accuracy is defined as rank ordering treatments. Fifth, measures of effort, confidence, and revealed ability are predictive of forecast accuracy to some extent, especially for non-experts. Sixth, using these measures we identify `superforecasters' among the non-experts who outperform the experts out of sample. Seventh, we document that these results on forecasting accuracy surprise the forecasters themselves. We present a simple model that organizes several of these results and we stress the implications for the collection of forecasts of future experimental results.\n\n",
    "acknowledgement": "\nWe thank Dan Benjamin, Jon de Quidt, Emir Kamenica, David Laibson, Barbara Mellers, Katie Milkman, Sendhil Mullainathan, Uri Simonsohn, Erik Snowberg, Richard Thaler, Kevin Volpp, and especially Ned Augenblick, Don Moore, Philipp Strack, and Dmitry Taubinsky for their comments and suggestions. We are also grateful to the audiences at Bonn University, Frankfurt University, the London School of Economics, the Max Planck Institute in Bonn, the Wharton School, at the University of California, Berkeley, at the 2016 JDM Preconference, the 2015 Munich Behavioral Economics Conference and at the 2016 EWEBE conference for useful comments. We thank Alden Cheng, Felix Chopra, Thomas Graeber, Johannes Hermle, Jana Hofmeier, Lukas Kiessling, Tobias Raabe, Michael Sheldon, Patricia Sun, and Brian Wheaton for excellent research assistance. We are also very appreciate of the time contributed by all the experts, as well as the PhD students, undergraduate students, MBA students, and MTurk workers who participated. We are very grateful for support from the Alfred P. Sloan Foundation (award FP061020). The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}