NBER WORKING PAPER SERIES

TECHNICAL PROBLEMS IN SOCIAL EXPERIMENTATION:
COST VERSUS EASE OF ANALYSIS

Jerry A. Hausman
David A. Wise

Working Paper No. 1061

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue

L-.4
.3_ AA fV1LJL)
'Q
aniui
Lube

(1

January 1983

Any opinions expressed are those of the authors and not those of
the National Bureau of Economic Research.

NBER Working Paper 111061
January 1983

Technical Problems in Social Experimentation:
Cost Versus Ease of Analysis

ABSTRACT

The goal of the paper is to set forth general guidelines that we believe
would enhance the usefulness of future social experiments and to suggest ways
of correcting for inherent limitations of them. Although the major motivation
for an experiment is to overcome the inherent limitations of structural
econometric models, in many instances the experimental designs have subverted
this motivation. The primary advantages of randomized controlled experiments
were often lost. The major complication for the analysis of the experiments
was induced by an endogenous sample selection and treatment assignment procedure that selected the experimental participants and assigned them to control
versus treatment groups partly on the basis of the variable whose response
the experiments were intended to measure. We propose that to overcome these
difficulties, the goal of an experimental design should be as nearly as possible
to allow analysis based on a simple analysis of variance model. Although
complexities attendant to endogenous stratification can be avoided, there are
inherent limitations of the experiments that cannot. Two major ones are
self-determination of participation and self-selection out, through attrition.
But these problems, we believe, can be corrected for with relative ease if
endogenous stratification is eliminated. Finally, we propose that as a guiding
principle, the experiments should have as a first priority the precise estimation of a single or a small number of treatment effects.

Jerry A. Hausman
Economics Department

MIT
Cambridge, MA 02139
(617) 253-3644
David A. Wise
Harvard University
J.F.K. School of Government
Cambridge, MA 02138
(617) 495-1178

Hausna n—Wise

TECHfl CAL

FRCLES IN SOCIAL EXP[R1MENTATJOt:

COST VERSUS EASE OF ANALYSIS

by

Jerry A. Hausman and David A. Wise

Over the past decade, a major portion of empirical economic research
has been based on what have come to be known as social experiments.
Primary examples include a series of income maintenance experiments, a
housing allowance demand experiment, several electricity pricing experiments, and a health insurance experiment.

luch of our discussion in

this paper is motivated by the income maintenance experiments but it
also draws from our experience with the housinq allowance and electricity
experiments as well.

The goal of the paper is to set forth general guidelines that we
believe would enhance the usefulness of future social experiments and

to suggest ways of correcting for inherent limitations of them. Our
conclusion and results may be summarized briefly.
Although the major motivation for an experiment is to overcome the
inherent limitations of structural econometric models, in many instances.

the experimental designs have subverted this motivation. The primary advantages of randomized controlled experiments were often lost.

In partic-

ular, it was in large measure impossible to estimate an experimental

effect using straightforward analysis of variance methods, as a standard

experimental design would suggest. Rather, a careful analysis of the
results often required complicated structural models based on
strong model specification assumptions, the necessity for which an experi-

nent shciud

Dc desicned tc oviate.

section I prDvice a mc exp1afl-

tion of tins goal and is intended to motivate tne remainder of trie paper.

The major compUction for the analysis of the experiments was
induced by an endooenous sample selection and treatment assignment procedure that selected the experimental participants and assigned them to

control versus treatment roups partly on the basis of an outcome variable
the chanqe in which1 the exneriments were intended to measure. To overcome
at the time of analysis of the exoerinental results the complications
caused by the endogenous sample selection and treatment assignment required
rather complex statistical techniques and detracted creatly from the
simplicity we believe should be a coal of experimental desiens.
We propose that to overcome these difficulties, an experimental design
should as nearly as possible allow analysis based on a simple analysis of

variance model. This would mean that sample selection and treatment assignment should be based on randomization and that stratification on response
variables should be avoided.
Aithouqh complexities attendant to endogenous stratification can be

avoided, there are inherent limitations of the experiments that cannot. Two
rna.ior ones are self-determination of participation and self-selection outs

through attrition. But these problems, we believe, can be corrected for
with relative ease if endogenous stratification is eliminated.
Finally, we propose that as a guiding principle, the experiments should
have as a first priority the precise estimation of a single or a small number

of treatment effects. The experiments to date have in general been hampered
by a large number of treatments together with small sample sizes so that no

sirigIe trCtrient COJ1

te eiec dccurateiy.

Foliowirc tne rti'tion in Section 1, we have elaborated in Section
Ii these several ceneral auidelnes that we believe would enhance the
effectiveness of future exDerirents. The problem of endooenous stratification and a way of avoidinc it are set forth in Section 111.

A method

of correcting for the inherent self-selection problems of social experiments is suggested in Section 1V.

1. Unbiased Estimates, Structural Models, and Randomization
To obtain unbiased estimates is the major motivation for a large
portion of econometric theory and for the application of econometric

techniques in empirical analysis. Econometricians generally have in
mind a model of the form

Y =

(1)

f(X,

c)

where X represents measured and c unmeasured determinants of Y. The
goal is to estimate the effects 0-f the elements of X on Y. A comon
specification of f in (1) is

YXc,

(2)

where

is a vector of parameters to be estimated, with each element of

measuring the effect on Y of a unit change in the corresponding element
o-f X.

The guiding principle for econometricians is that simple estimation

techniques (e.g., least squares) will yield unbiased estimates of

if

X is uncorrelated with c. Unbiased is understood to mean arid is indeed

defined to rnan an unbiased

the understood definition of

estirate of

the 'causal' effect of X on ,

in much, but not all, of econoietric analysis.

But although the principle is demonstrably true in theory, it is often
difficult to approximate in practice and its existence impossible to

verify without reservation. Nonetheless, the aoal remains.
To move toward it, econometricians use two ceneral modes of reasoning.
One is economic theory that restricts the functional form of f, although

usually only within broad bounds. The other is statistical theory,
that in large part prescribes methods to correct for correlation between
X and ., and thus to obtain unbiased estimates of .

The combination of

economic and statistical theory often leads--at least in the abstract--

to specification and estimation of structural models. Structural models
can be thought of as those in which the parameters have a causal interpretation, and with the concomitant property that if unbiased estimates
of them are obtained they also could be given a causal interpretation.
But although theoretical prescription of models and their empirical
estimation can restrict the form of f, they can do so only within limits.
The estimates must be interpreted within the constraints implicit in the

assumptions that underlie them. In particular, it is usually not possible
to know for sure that X is uncorrelated with c, or if not, that corrections have been made for correlations that exist.

4 response to this dilemma is to choose selected values of X in such
a way that they are by design uncorrelated with other determinants of Y,
and thus allowing unbiased estimation of the corresponding values of B.
The technique is randomization and it is most often employed within the

-5-

of a ranDomized con:rollea experiment.

tion,

we

For urposez of eXpD5-

shall hencefortn use as an example estimation of the effects of

income maintenance plans--taxes arid cuarantees--on earninqs.
Suppose that the plan is I, called the treatment, and that earnings

depend on T, other measured variables X, and on unmeasured determinants
c according to

Y =

(3)

If

f(X, e)

individuals (more often families) are chosen at random from the popula-

tion and
c

1T

on

values of T, in large samples I will be uncorrelated with

and with X as well. Then simple least squares analysis of variance

estimation of the model

+

(4)

where r is equal to f and treated as a disturbance term in this model,
will yield unbiased estimates of 6.
The primary motivation for this approach is to circumvent the uncertainties inherent in the assumptions of structural econometric models,
by constructing T in such a way that it is uncorrelated with other deter-

minants of Y and thus by construction assurinq unbiased estimation of
E1.

We have set forth these possibly oversimplified ideas to serve as

background and motivation for our subsequent discussion. In particular,
it is important to keep in mind the motivation for randomized controlled

experiments. Although in the large social experiments, we believe it is
impossible to create the theoretical paradigm of such an experiment, we

believe that the paradigm should serve as a guide to their designs as

——

v'll

as to

th analysis

of their results--mucn as the theoretical Qoal

of Xs uncorrelated with error terms serves as a guide to eipirical analysis
based on non-experimental data. We shall argue, for example, that the
use of complex structural models to analyze the data from social experi-

ments, or experimental designs that require such models or depend in large
part on structural model assumptions, are often in contradiction to the

primary motivation for the experiments and thus subvert their intent; they

are often inconsistent with the raison d'tre of experiments. We will
elaborate more on this and other general propositions in the next section.

Ii.

General Goals and Guiding Propositions
With the powerful advantage of hindsight, and we hope aided by our

part in the analysis of social experiments to date, we shall set forth
several propositions that we believe will enhance the value of future

experiments. To do this, we will explain what we believe to be the
major inherent limitations of such experiments. The primary ones are
self—determination of experimental participation and self-determination

of withdrawal from the experiment. These we believe can be corrected
for, and some suggestions for doing so are contained in the following

sections. There are other design characteristics of the experiments to
date that we believe unnecessarily complicated their analysis, and in
particular made it much more difficult to correct for the inherent limita-

tions of them. The primary design feature of this type is stratification

on endogenous variables. We

will

address this question first, then turn

—7—

to a dicusson o

innerert

Thmitation,

end

then address otner principles

that we believe Should nuide future exnerimental desions.

A.

Stratification or Endornous Variables
As described

in the

previous

section, the reason for an

experiment

is,

by randomization, to elininate correlation between the treatment variable
and other determinants of the response variable that is under study.

In

each of the income maintenance experiments, however, the experimental sample
was selected in part on the basis of the dependent variable and the assionnent to treatment versus control qroup was based in part on the dependent
variable as well.

In aeneral, the group eligible for selection—-based

on family status, race, age of family head, etc.--was stratified on the

basis of income (and other variables) and persons selected from within

the strata. In the New Jersey experiment, persons with incomes greater
than 1.5 times the poverty level were excluded altogether.

In the other

experiments, the stratification or income was lESS complete, but as a

result a bit more complicated. Assignment to control versus treatment
group was also based in part on income. Whether the outcome of interest
is income or hours worked, which is a component of income, such a procedure
induces correlation between right-hand variables, including the treatment

effect, and unmeasured determinants of income. Thus it is not straightforward to obtain unbiased estimates of treatment effects using simple
analysis of variance or covariance techniques.

Theoretically, a very e'aborate analysis of variance procedure that
allowed for estimation of separate treatment effects within each strata

-B-

would yield unbiased estimates. But because the strata
and the

treatments

were so numerous

so many and the sample sizes relatively small, this

rthoa of analysis was impractical because reasonably precise estimates
of

treatent effects cuid not be obtained. Thus to correct br endoac-

nous stratification and treatment assianment required rather complicated
models (Hausman and Wise [1977], [1979], and [1980]).

Analysis of experimental
least two major shortcomings.
requirinQ

results based on such
First, it

techniques has at

is relatively complicated—-

non-linear maximum likelihood estimation for example. This is

a shortcoming in itself, but seems especially troublesome in the context

of an experiment one of whose major advantages presumably is simplicity.
Second, and more important, it

necessitates

the imposition of functional

form constraints. The models proposed by Hausman and Wise are generally
structural in spirit, and in particular require distributional assumptions

against which the results may not be robust. To correct for endogenous
stratification, for example, requires analysis based on truncated distributions in which the distribution assumed is necessarily a key component.
Since the primary advantage of an experiment presumably is to lessen or

avoid the necessity for such assumptions, it seems contradictory to design
experiments whose effects cannot be evaluated accurately without them.

The elimination of stratification on endogenous variables would avoid

this source of complication. The most straightforward procedure would be
to randomly select an experimental group from the population and randomly
assign those selected to control or treatment status, without considera-

tion of income or other endogenous variables. There seem to be two major

objections to such a procedure: cost and political feasibility. Indeed

-

the two are not unrelated. fost

seriously

considered incoe support

proorams are intended to guarantee a minimum income to families who would
otherwise have relatively low incomes. And presumably it is primarily
this orcun whose labor supply

onetheiess,

and earninas would be affected by the

plan.

it has been difficult to obtain funds for experimental

programs that ouaranteed support for higher income families, even though
under most plans payments to this group would be small, since their
earnints would be unlikely to fall below the "breakeven' point at which
payments are zero.

In addition, if it is important to obtain a 'good'

estimate of the effect of the proqrarn on low income families, then it is

necessary to have a large enough number of low income families to do so.
Of course a large random sample from the population would also provide a

large number of low income families. But larger sample sizes of course
increase the cost of the experiment.

We do not present numbers on the marginal cost of an additional

experimental family. Preliminary investigation, however, suggests that
it is small relative to the fixed costs of running an experiment. Suppose
that for whatever reason, it is not feasible to select a random sample

from the population. We propose in this case that the sample be as random
as possible. That is, randomly select persons with incomes below a given
level, without endogenous stratification within this group. But what
should be the measure of income that determines eligibility?
We have proposed in.Section 111--after a more detailed description

of

the endogenous stratification problem--a method for selecting the

experimental group, based on predicted income, in

stratification

is not endogenous.

such a way that

the

—

—

:rren Ln:atrE sn rcc) am:le Se'e:tioI
;E have aroued tnat endoDenous stratification proceoures unduly

c:cate tns analysis of experirental results and that procedures that
avoid such strati fi cati on iould he preferable. Nonetheless, there are

inherent limitations
impossible

on randomization in social experiments.

It is surely

to attain the theoretical paradigm of a randomized-controlled

experiment. There are at least two major reasons for this, both involving
individual self-selection.

One is that persons cannot in general be made to participate in an

experiment if selected by a random procedure. Some of those randomly
selected will participate while others will not.

If the individual parti-

cipation decision is related to the effect that the treatment would have
on individuals, then the estimated treatment effect will be a biased estimate of the effect to be expected if the treatment were instituted as a
program applying to the entire population.

The 1954 Salk vaccine experiment provides a good example of this

effect. There were two primary versions of the experimental design. In
the "placebo control" areas, children who agreed to be inoculated (or,
more accurately, whose parents agreed to the inoculation) were randomly

assigned to the vaccine group or to the placebo group. In the "observed
control" area, second grade children who agreed to inoculation received
the vaccine, while first and third graders served as the control group.

Selected results arechown in table 1.

Chilaren in tne clacebo

control

areas wno were not inoculated con-

tracted polio at a rate of 54 per 100,000. Tne comparable ficure for
children who

who

participated

participated

in the experiment was 81, the rate for those

and received the placebo. Similarly in the observed

control areas, grade 2 children who were not inoculated had a substantially

lower rate: (53), than the rate for the control group (61). Thus apparently children who were more likely to contract polio and thus more likely
to be helped by the vaccine, were more likely to participate in the experi-

ment. This tends to exaggerate the effect of the vaccine. For example,
one might conclude on the basis of the vaccinated and control groups in
the observed control areas that the vaccine reduced the rate from 61 to

34. But apparently the rate for all children would have been less than 61
without the vaccine. It is of course apparent from these data that the
vaccine was effective, regardless of this uncertainty about the maonitude
of the effect. But if the effect had been less clear, this self—deter--

mination

of participation could have led to considerable uncertainty about

desirability

of universal inoculation.

A similar effect was apparent in the recent housing allowance demand

experiment. Because of the nature of the primary experimental allowance,
many families could benefit under the allowance plan only if they were

willing to move. It seems apparent from subsequent analysis that of low
income renters who were asked to participate in the experiment, those who
were less adverse to moving were more likely to participate in the experi-

ment. (See Venti and Wise [1982].) Thus the estimated experimental effect
tended to exaggerate the increase in rent that would be induced by the
allowance were it applied to all low income renters.

-12-

used

e

have suooested in

to

correct for this rotertial bias, assuminc that the self-selection

cannot

or

a procedure trat we bell eve could be

be avoided.

The other form

of

self-selection is attrition from the experimental

sample, once a sample has been

selected.

Again, the problem is that

determinants of dropping out rav be related to the experimental response

that would otherwise be observed. For example, persons who are not
affected by the treatment, possibly because they have high incomes for
example, may be more likely to drop out than those who are affected and

thus receive higher payments. This is the problem addressed by Hausman
and Wise fl979].

If the experimental design is not complicated by endogenous stratification and assignment, then correction for self-determination of parti-

cipation and attrition would be relatively simple. Indeed correction for
both simultaneously is quite feasible and this is the approach taken in

Section IV. Such a correction, however, is much more complicated if the
experimental design is also complicated by endogenous stratification and

assignment. This reinforces the proposal that such stratification be
avoided in favor of random sampling. Then analysis of experimental results
can address complications that are unavoidable without having to devote

extraordinary effort to correct for complications induced by the experimental design.

-13-

-dc:iora1 Corcern

C.

b characteristic of experiments to date has been a rather

larQe

number of treatments. The income maintenance experiments, for example,
entailed several

treatments defined by different combinations of income

ouarantee levels and tax rates.

In none of the experiments, ho.ever,

were tne sample sizes large enough to obtain precise estimates of the

effects of any particular treatment. Thus analysts aenerally resorted
to estimation of a single effect that did not distinguish the various

treatments, or they assumed a structural model that allowed interpolation

across individuals assigned to different treatments. The more the latter
procedure was followed, the less consistent the analysis was with the

motivation for an experiment. That is, it subverted the major goal of
using random selection and treatment assignment to circumvent the inherent
limitations of hypothesized structural models.
Thus it seems to us that priorities should be ordered in such a

way that the primary goals of an experiment are met first. The first
goal

we propose should be the estimation of an experimental effect for

a treatment. Then additional treatments should be added only if each

additional one can also be estimated with precision. The proposition

is that precise estimation of the effect of single treatment or the
effects of a few treatments is
of

to

be preferred to imprecise estimates

many.

This we propose should be done in such a way that simple analysis
of covariance estimates of treatment effects may be obtained, subject
to the limitations on randomization discussed above and detailed more

fully below. Thus we would propose an evaluation model of the form

—14-

—

•

+

—

rere the . are treatrert effects. We proooe an analysis

of covariance

model because our research (Hausmn •3nd Wise {l979]) has sugoested that
the use
effect

of exocenous control variables, represented by X, reduces the

of

attrition on estimated experimental effects and we presume that

it would be likely also to reduce the effect of self-determination of
participation.

The reader will note the absence of a structural parameterization
that attempts, for example, to describe income and substitution effects.
This is because we believe that simple precise estimates of a few effects

will be more readily understood by most observers and will thus carry
more weight in the decision-making process.

In addition, if for policy

purposes, it is desirable to estimate the effects of possible programs

not described by treatments, then interpolations can be made between
estimated treatment effects. If the experimental treatments are at
the bounds of possible programs, then of course this is easier. Although
it can be argued that structural models are necessary to make interpolations, we believe that for almost any situation we can think of, the
simplicity of say linear interpolations far outweigh the possible advantages

of interpolations based on a structural model. At the same time, it maintains the spirit of an experiment.
If the experiment is to inform the policy making process, we believe

that a single number that can be supported can be more confidently relied

on than more complex analysis. That the labor supply effect of a known
treatment is 16 percent and not 2 percent, for example, is we believe much

-1E—

rrre
is

irortant tan

wnetner tre effect of a plan close to tne treatment

16 percent or 17 percent.

This is not to say that experimental data should not be used to esti-

mate structural econometric models. These data can of course be used like
other survey data for this purpose. But the experiment should be thought
of in the first instance as a way to obtain accurate estimates of the

effects of particular proarams. Structural models with parameters estimated
on survey data could also be used to make such estimates. (Presumably this
would be done to a considerable extent before an experiment were undertaken,
if for no other reason than simply to help to inform the choice of experi-

mental treatment or treatments.) in this sense, the experiment could be
thought of as checking the accuracy of predictions based on analysis of

survey data. That is, the experiments should be designed to provide a
selected number of points "on' the response surface, defined for example

by tax rate and guarantee levels. it is rather straightforward to check
for example the degree to which alternative structural models fit these

"known' points on the response surface. In short, an experiment should be
used to avoid the inherent limitations of structural models in providing

accurate estimates of the effects of specified programs. Their major
advantage should not be lost sight of in an effort to estimate models that

will predict the result of any plan. A lack of confidence in such estimates
is the motivation for the experiments. To use the experimental data only to
provide more such estimates, or to set up the experiments in such a way that
only such estimates are possible, is to travel to Rome to buy canned peas.

— 1€.—

::.

rcooenous arTOUn aid Stratificetior
,L

discussed in the introduction above, a major feature of classical

e>perimertal desiqn is that it leads to a simple analysis of variance
(ANOVA)

model that minimizes the number of maintained assumptions implicit

in the interpretation of parameter estimates. That is, the analysis is
'model free" in two important asoects: (1) In the simplest cases a main

effects ANOVA specification is adequate. Questions about the need to
include further right-hand variables--as in much of econometric and statis-

tical analysis--for example, do not arise. Correct randomization assures
that disturbance terms have expectation equal to zero. Also, questions
of functional form are absent because each experimental treatment effect
is measured by a parameter.

(2) Distributional assumptions are kept to

a minimum in estimation. While distributions of test statistics are
certainly used in inference, asymptotic theory may provide a reasonably

good approximation in many cases. Classical experimental design together
with ANOVA

offer

the opportunity either to eliminate or to decrease greatly

a major problem that arises in econometric studies based on observational,

i.e., non-experimental data)
Yet in many of the social experiments the classical approach has not

been followed. Given a limited experimental budget and a "target population," the designers of the experiments, in concentrating sample
selection on that part of the population most likely to be affected by
the treatment policy, induced endogenous sample selection and treatment

assignment. The presence of endogenous sampling complicates the analysis
of the experiment greatly and thus limits our ability to treat other prob-

lems which arise, in particular, sample self-selection and attrition. And

-17-

ossibiy as
tional

irrJorsant,

it

tvicallv forces tne analyst to ninta'ir

distribu-

assumotions about tre random variables under study. Tnose distribu-

tional assumptions are not innocuous even in large samples. Sianificant
empirical dopartures from these assumptions may lead to larce biases in

estimation

of experimental effects (e.g., Goldberaer 1980). Most importantly,

if the endoenous sampiin is ianored in the analysis, extremely large
biases may result in estimated experimental effects.

In this section we

will present three examples of endoaenous sampling as well as techniques

developed to eliminate the problems that it creates. !e then propose an
alternative approach which attempts to choose selectively from the target
population without inducing endogenous sample selection.

The problems associated with endooenous sampling occur because a pre-

experimental endogenous variable is used in sample selection and in treat-

ment assignment. The effect on the estimated treatment effect arises
)ecause

of correlation between unmeasured determinants of the response

ariable in the experimental and pre-experimental periods. These time
effects have often been ignored in the experimental designs.2 We shall
illustrate the problem within the context of an ANOVA framework, which

when generalized to a random effects specification allows for serial
correlation. We consider a single period experiment with one period of

rE-e)rirrenta

it

t

-fT,
j3t--'.--'-.
i it

t = 1,2;

j =

1,

.

.

,3

.

(5)
=
1

1L

0; V(ij.)
1

c

2
;

2
C

=

=

Cn +C

;

We

have decomposed the disturbance term into a permanent individual

component
periods.

and another component

assumed independent across time

The indicator variable Tit is 1 if the individual is receiving

the experimental treatment j in period t and zero otherwise. Time effects
are absorbed into the constant terms Ut. The importance of the individ—
ual component j1

is

given by the correlation p between the disturbance

term in the two time periods. Such correlations often exceed .5 in
econometric studies.
Now suppose that the expected cost of an experimental treatment

varies across individuals and treatments as a function of Y11. Designers
of experiments have for this reason used

in sample selection and in

treatment assignment. Because of the presence of

in both periods the

endogenous sampling and treatment assignment based on pre-experimental data

carries over to the experimental period as well. A simple example will
help to make the point clear. Suppose we have two experimental treatments
called generous (G) and not-generous (NG). The G treatment is expected to

-1;fr hih Y' irdi vi d'ais because of an exiected ercentaoe
reduction n work effort. Therefore, the desianer forms two groups of
individuals

based on Y1. Low Y1 individuals are

plan or control status; the high

individuals

assianed either the C

receive either the NG

plan or control status. But when we use ANOVA to analyze the experimental
results we see from equation (5) that E( Tt) Y 0. Thus, our estimates
are biased for the population since we have not accounted for the presence

of individual effects that persist over time. Since it is unlikely in
most economic and social experiments that p

is

near zero, substantial biases

may arise from endogenous sample designs.

We shall now consider three experimental designs in which endogenous
sampling was used.

(i) In the New Jersey Negative Income Tax experiment

any individual whose pre-experimental income exceeded 1.5 times the

government set poverty limit was excluded from the sample. This sample
truncation was used because the major effect of an NIT program was expected

to be on low income individuals and families. A simple rule was thus used
to make the sample resemble the target population. Suppose a model like
equation (5) is used to analyze the effects on hours worked. Suppose
also that individuals' earnings are low in period one either because they
have low p

or

because

is negative even though p is positive. Low p

people with positive n1 have been excluded from the sample. The analyst

must maintain the assumption that the effect on hours worked for the sample
combination of low p and high p people (with negative n)

will

represent the

total population response. This assumption appears unlikely to hold true
because we might well expect

the behavioral response to differ among the

low p and high p people. In other words, if we were to chanqe the sample
truncation point from 1.5 times the poverty limit to another level, the

tmted eYOCriiTntai

Connecticut

(ii) in the
the

sacle

eect

'ld Dc

likcy

ime of Day

to crar:oe as we.

Eiectricity

Demonstration (1977),

was 9roued into quirtiies on the basis of electricity usaoe in

the year prior to the demonstration. Then households in the upper quintiles
were disproportionately sampled since the electric utility correctly thought
that their reaction to the introduction of time of day electricity rates
would have the laroest effects on system revenues.
(iii) In the Seattle-Denver Income iainteriance Experiment, (SITIE—DIME),

the Conlisk-Watts framework was used for treatment assignment. It allowed
the expected cost of an experimental treatment

c

for treatment T to vary

with "normal income" which in practice was very closely related to pre-

experimental income. Consider the Conlisk-Watts framework in the regression
form

+ c

Y =
=

(0,

.

.

.

, 0,

1, 0, .

.

. 0);

j =

1,

J

(6)

E = 0

;

V(s) =

Here X1 denotes the control observations and the j = 2,
the J -

1

.

.

.J

denote

experimental treatments and normal income classifications. The

Conlisk-Watts design uses as an optimization criterion the minimization
of the variance of linear functions P8 of the estimated coefficients,

subject to a budget constraint. We want to choose

=

1, J (the

number of individuals in a given row of the design matrix) in an optimal

manner. Let 0 =

PP.

The complete problem is an integer programming

-21-

roberr

t,itn a convex o:ectve urictlon SuDeCt to irEar constrairt.

mm q(n1, . .

.

, n)

=

tr[DE n:xxY'J
j1

(7)
>

For 1or'e

N

n.

0 for

all j

a suitable approximation is to treat the

n as continuous

arid to round off the results to the nearest inteaer. To estimate the
experimental effects in each class via the contrasts, B. -

the

appro-

priate P matrix is an (m — 1) x m matrix with the first column —l's

and each of the remainino columns with all zeroes and a single 1. Thus
[-1, 0, .

.

, 0,

1, .

. .

, 0].

We solve equation (7) to find

((3 -

n1C
(8)

n

=

E

C(cE)

E[(J-l)c1+ j=2
ECJTh.
The optimal design thus increases the probability of inclusion in the
sample for low c individuals. But since c is a function of pre-experimental income we see that E(uJX.)

0 which will lead to bias in the

estimation of experimental effects.
We do not want to give the erroneous impression that endogenous

sampling destroys the possibility of experimental analysis. In fact,
we have written several papers addressing the problem, Hausman-Wise

1

öfl

cr

,

1

77 ,

1230

ii j.

Arc er:rj:erou s1 ro can

reOuce tne COST

O

perrnt consicrab1y.4 But we emphasize tne mdel functional form

tn uionai ascuT2tions tnat enoenous

sampbng reQuires.

To illustrate the nature of these assumptions, we consider acain the

three eamples and for each we discuss possible model specifications.
(1) Sample truncation: In Hausman and Wise [1976, 1977], models to

correct for sample truncation are developed. The approach taken assumes
that the earnings conditional on personal attributes are distributed log-

normal. A two

period

model is necessary since sample truncation was

performed on the pre-experinental data. But since the correlation of the

disturbances across years ( in equation (5)) is not zero, truncation on
pre-experimental data will affect the analysis of the experimental results.

Therefore, we define a model of the form

+

(9)

t

=

1,

+

=

2

with the usual stochastic assumptions. We assume that f(y11, y.12Z.11, Z12)

is bivariate normal. The

include experimental treatments as well as

individual characteristics. Then the likelihood can be written

N

(10)

where

L

N

4(y1., y2.)

1li' y2) il (L1 Z1)Io]

is the bivariate normal density and

is the univariate normal

distribution. For the New Jersey NIT experiment we

estimate

b

.85 which

demonstrates the potential importance of correcting for truncation. The
log normal is a convenient distribution which leads to a likelihood function

tr is ouit&

tra:tabie usnc n:aern cDmuters. Still, if tne choice of

ion normal is not correct, it represents a specification error.
Ar

rather

even re difficult

problem arises if we

want to analyze hours

than earnings. Since truncation takes place on earnins we

must

analyze hours and wages jointly and the four-eouation model that results

leads to a liLelihood function that
equation (10).

is

considerably more complicated than

(See Hausman and Wise (1976, p.432).) Furthermore, given the

identity between earnings and the product of waoes and hours, we must now

assume that both wages and hours are distributed log normally. Almost no
other assumptions lead to a tractable ]ikelihood function even though some
evidence exists that hours might be better represented by a conditional

normal distribution.5 And lastly, because of the complications induced in
the likelihood function by truncation, our ability to handle other problems,

like sample attrition or taxation, are limited. Thus the analysis has been
greatly complicated by what seems to be a reasonable design criterion,
concentrating on the target population of the proposed policy.

(2) Stratification on the endocenous variables: To keep the analysis
simple we here assume that income has been grouped into two intervals,
even though in the Gary NIT experiment as well as the Connecticut TOD

demonstration quintiles were used. Assume that below some level L an
unknown proportion of a random sample of the population is sampled, P1,

and above L, a proportion P2.6 Then the density function is

-

-

Ff
h(y)

(11)

if y<L

=

P2f(y)

tne normal density function N(Z, 02). Only

wnere f is
P =

P2/P1

if

y>L

the ratio

can be identified. Therefore, we divide through tne expressions

in equation (11) by P1. Again using the normality assumption for y and
assuming N1 persons with y < L and N2 with y >

the log likelihood

1

function is

N1

L =

i=l

N1

in f(y) -

ln(. + P(l - .)) +

i=l

N2

N2

N2

i=1

i=l

E in P + E in f(y) -

(12)

1

1

N

N

i=1

i=l

in (.1 + P(i -

E in f(y) — Z in (P + (1 - P).) + N2

where

=

[(L

—

ZIB].

1

in

1

P

Aaain, a maintained distributional assumption is

necessary and a rather complicated maximum likelihood problem is presented.
Furthermore, when we want to do a two period analysis or consider other
problems, our ability to do so is

limited

by the rapidly increasing compli-

cations induced by the stratification on the endogenous variable.

(3) Treatment assignment usinq an endogenous variable: Our last

example is the SIME—DIME NIT experimental design. Here seven income

inervas, celled "E-ive,' ere ud to
cr1 fr0 or: of

ecuati ons

or c. E-1e'e .
rise

define ro's in

(6) -(18). Tne

Th5 e :ec:ed

with E-level because it was

costs c were

cost of a treatment

the Conlisk-tt
tier derived as
to

Dresumed

assumed that tax revenues would decline

and that NIT payments would increase.

The result was that no one in the

hinhest E-level interval was assioned treatment status; all were assioned
to be controls where, of course, the cost does not grow with E-level.

Furthermore, in general persons with higher E-levels were more likely to
be assigned to experimental treatments with more generous supoort levels.

Thus, treatment assignment was based on an endoaenous variable, preexperimental income, which was highly correlated with the response variable during the experiment.

Treatment assignment using endogenous variables does not in theory

prevent the use of ANOVA in the analysis phase of an experiment. What is
needed, however, is an elaborate specification allowing a separate

in

equation (5) for each E—level and treatment or control assignment. But, in
the SIME-DIME experiment for example, including manpower treatments, there
would be J = 59 columns in the X matrix. In fact, if full ANOVA were done
without deleting higher order interactions as did the design model, we would

have J exceedino 200. Thus, even for the comparatively large sample sizes
as in the SIME-DIME we cannot hope to obtain precise estimates of experi-

mental effects. And when other factors such as race and city are added to
the analysis, full ANOVA specifications with many fewer parameters than the

experimental design requires. One approach is to enter E-level as a righthand side variable in linear form. But we immediately lose the model free
aspect of ANOVA since correctness of functional form becomes an issue.

-2f Jr

fact, a

flCC

it

the

and

hn&är

coes not

:ec::r -ieve is
re:vs a COrreatior btoen

stochastic

Anain,

treatment

one outcome, a probit model
flCcessary distributional

assiGnment can be

But since

[1980] specify.

Hausman_Wise

the

dr---.

model of

a

totàflv

0

(or

treatment

rri

trtrflt variable

constructed,

assiGnment is

which

a zero-

loGit model) is required alono with the

assumptions

An additional complication arises

here because we must also specify the partly unknown model of treatment

assignment correctly.7 Thus, both distributional assumptions and functional
form assumptions are required for model estimation

The resulting likeli-

hood function used in estimation is even more complicated than equations
(10) and (12).

And as emphasized above, additional complications like

to treat jointly with the sample
sample attrition are almost impossible
assionnient issues.

A very simple solution exists to these design and analysis problems.
Randomize over pre-experjmental

income.

Then problems of endogenous

assignment or stratification do not occur. so that ANOVA specifications
choice, we give up the notion

again are appropriate

Sut in making such

of

so that the precision of our analysis for

a

target population,

a

ular group may decrease, given size and experimental budget.
the problem in an alternative manner, for
estimation,

a

a

partic-

Or to state

given level of precision in

the necessary budget for an experiment might increase substan-

tial ly.

An

alternative

approach is to stratify on exooenous variables only

and to approximate the goals of endogenous stratification by using predicted

values

of the

endogenous

variable8

-27-

We

shall

consider the first example, sample truncation, since the

issues Cãfl be seen quite clearly. Figure 1 represents the density of

commas with a truncation point T.9 Suppose our aim is to sample poople
in the area of the distribution marked I. Now instead of using pre-experimental income with its associated problems, consider the use of "exogenous'
income stratification, based on income predicted on the basis of exogenous
variables, say from the regression equation

(13)

where the prediction is Y. =

z. +

.

Z.6

Z(zzYz.

+

Note that

still

enters the last term through the product Zc.. But for a sample of size
N this term is of order 1/N, so that it quite rapidly disappears as the

sample becomes large. The variables included in Z. would be education,
training, union membership, age, etc. We could then base truncation,
so that problems which arise from the individual effect

=

-

being present in both periods no longer occur.
If the covariance between

and

were very high, we would have

solved the problem. Then the predicted value would do almost as well as
the actual endogenous variable. But for log earnings the R2 of the
regression is around .25 multiple correlation coefficients in the range
of .25 to .60 are quite common for many cross section regressions in

econometrics. Thus, if we use y. <

F

as the truncation point we expect

on average to do about 1.2 as well as pure random sampling in selecting
y. <

L.
While this is an improvement, we might do even better by choosing

a point k

L as our sample truncation point. Perhaps a useful approach

to the choice of k can be constructed as follows. Assume the benefit to

— 2;._

estimtior

of the experirertal effect has expected value of the form

- )2•

V(y) =

That is, e expect to learn little abut labor supply

response from low income or high income individuals. On the other hand,

cost is expected to grow linearly with income c(y) =

Suppose we then

cy...

want to solve for the optimum truncation point k, given our knDwledoe that

since we are using predicted income y. the actual y1

y. -

c

will differ.

The optimization problem is

(14)

1

k

We

s.t.

max /(y. -

cy1 < c

y.1

y.1

=

<k

-

—

solve the corresponding expected value problem

(15)

max L =

E(/(y.

+

1

k

1

)2)

+

1
?1E(C-Ec(y.+.))
1

+

2(k-y.)
1

The form of the solution can be seen by assuming that the variable has
been transformed to make the residuals approximately normal and that we
center the data to set y = 0.

Then we choose k to

max V /[var(Yi) +

1

-

N)

___

(16)

x(C -

- __

where a is the standard deviation of the residual distribution. The first
order conditions of equation (16) are straightforward and the problem
can be solved straightforwardly on a computer since the constraint will

be satisfied with eoualitv and all the functions are morDtOniC in k.

in

this problem the gains over random sampling increase as the variance of
the residuals decrease so that
we would expect.

if

y and y are

more hiohly correlated as

the correlatIon becomes very small, we

will

be quite

close to random samplinc. But in many cases random samplinc may be
ref&rable to endooenous sampling, which as we have attempted to show, can

le to difficult problems ir the analysis phase of an experiment.

IV.

Self-Determination of Participation and Attrition
We have addressed in the previous sections a problem that we believe

has been largely induced by experimental design and that we believe should
be avoided.

In this section we will address a major potential problem

that we believe cannot in general be avoided but that can be corrected for
withDut undue complication as •long as it is not accompanied by induced

endogenous stratification.
Suppose that it were possible to select a random sample of families

from the population, or from a subset of the population (say with predicted

income below some level). Of the families selected at random, some,when
asked to participate in the experiment,will do so while others will elect

not to participate. Even though a random sample is identified, those who
choose to participate may not represent a random sample. In experiments
to date there has been no systematic record kept of who when asked parti-

cipates and who does not. Thus it has not been possible to identify
systematic differences (and in particular unmeasured ones) between those
who participate and those who do not, and, of course, if there were differences, there has been no way to correct for them.

In the income maintenance

-

exerns,

-

for exanpE, a rcceure like

the

foliov.ing was used. Each

experiment .as conducted :ithin a single city or a small number of cities.
4ll families within the city, or within some section of the city were
canvassed to locate those with a few predetermined characteristics,

in

these experiments, income, race, age of family head, and number of dependents

were attributes that determined eligibility. Those who were found to meet
the eligibility criteria were asked to enroll in the experiment. Of those
who did enroll, some were assigned to a treatment group, and others to a
control group.

It is the enrollment decision that concerns us here.

Suppose that instead of using a procedure like the above, we were to

begin with an external source of data on families. The Census is a
loaical choice. Census data provide information on family income, race,
whether the family is single or two-parent, education of family head,

number of dependents, etc. Suppose that the known family attributes are
represented by a vector of characteristics X.

From families surveyed by

the Census Bureau, a random sample could be chosen.

For simplicity, suppose the oai is to estimate a single treatment

effect. Ideally we would like to randomly assign part of this randomly
selected sample to a control group and others to the treatment group. Then
after some time period, we would like to compare controls and experimentals,
with Y the outcome of interest, using a simple analysis of variance model

of the form

+

+

(17)

where T is an indicator variable with the value 1 for experimentals
and zero for controls.

-31-

Eut suppose that rot all of the random sample agrees to participate.
Suppose that participation depends on X and a random disturbance term r
in the following way:

where

X

=

(13)

+

is an unobserved index variable with the property that individual

i agrees to participate if P. > 0.

If

and P. are jointly normal with

correlation coefficient p, and n is normalized to have variance 1, we know

that the expected value of Y, aiven that individual i enrolls is given by

(19)

E(YP > 0)

=

B0

+

+ pc

Suppose that B.1 is estimated by least squares using the sample of participants

and ignoring the last term in equation (3). Let the inverse Mills ratio

(.)/t[.J be represented by M1. According to standard excluded variable
arguments, if M is correlated with I, the least squares estimate of B1

will be biased. As the sample of participants becomes large, the least
squares estimate goes to

(20)

B1 +

MTce

a-.

where MT is the correlation between Fl and T. If the treatment indicator
T, however, is assigned randomly, then it will be uncorrelated with X and
thus with Fl which is a function of X. Thus under these simple assumptions,
the least squares estimate of the treatment effect will be consistent, as

long as the assignment to control versus treatment groups is random. Each
participant could be randomly assigned or each of those in the Census

sample could be randomly

ass

prior

ed

to enrolimer:, as lono as at the

time of enrollment, prospective participants did riot know their assignment.
But the model

as

set out above hides by omission a potential major

source of self selection bias. Suppose that if the treatment were given
to all persons in the population, the responses would vary amonQ them.
It is clear that this is indeed the case (ever after controlling for

measured family characteristics). It seems plausible that the decision
to participate will deoend on the potential response. For example, it is
often hypothesized that persons whose behavior is most likely to be
affected will be most likely to participate, even thouah they do pot
know prior to enrollment whether they will be in the treatment or in the

control group. This is the essence of the examples given in Section Il—B.
The idea may be represented by a random effects model of the form

(21)

=

+ ( + b)T1

=

+

+ 1T.

+

bT

+

where from the perspective of the analyst b is random with mean zero.
Using (2i), the expected value of Y among participants is aiven y,

(22)

In

E(YIP > 0)

+

iT

+

(PbobTj

+

this case, it is clear that the last term will be correlated with

and a least squares estimate of

would be biased.

Joint, maximum likelihood estimation of (18) and (21), however, could

be used to obtain a consistent estimate of .

The procedure is similar

to the one proposed by Hausman and Wise [1979], except that the equations

-3pertain

to tne response variab'e and

response variable and attrition.

participation,

rather than to the

In this case, there are two possible

outcones: Individual i doesn't participate with probability.

(23)

1 —

or individual i participates with response

C

X.a

nbbi

4

C
crc

22
oT
+c 2
bi

(24)

12 T
2+
10

bj

a

21½

i-

-

IYi -

1

with likelihood

1i22 2 1
(0 1. +

\b1

a

c

The likelihood function

N1

L

(25)

E in Pii. +

"2
Z in P

i=l

can easily be maximized to obtain estimates of

2i
along with the other

parameters of the model.

The other component of self-selection that seems unavoidable in
social

experiments is attrition. Some participants will inevitably

drop out of the experiment before the treatment response is measured.

To take advantage of individual specific characteristics that persist

-3:.-

over tine,

period

it

is also advartaqeous to observe participants for some

of time before the treatment becomes effective. This will

lead

to four equations of the form
=

Xa

+

X16

C2j

(26)

X26 + 1T +

A1

Xy + C4.

where Y1 pertains to the response variable before the treatment period,
to the response variable during the experimental period and A is an
unobserved indicator variable with the property that individual i leaves

the experiment if A < 0.

This system of equations can also be estimated

readily with available maximum likelihood techniques. (See Venti and
Wise [1981]).

- 3EREVERENCE

Goldberger, Arthur. 1980.

Abnormal selection. Mimeographed. University

of Wisconsin.

Hausman, Jerry A. 1980. The effects of time in economic experiments.
Presented at the World Econometrics Society conference, Aix—en—Provence,
August.

Hausman, Jerry A., and Trimble, J.

1981. Sample design consideration

for the Vermont TOD use survey. Journal of Public Use Data 9.

Hausman, Jerry A., and Wise, David A. 1976. The evaluation of results from
truncated samples: the New Jersey negative income tax experiment.
Annals of Economic and Social Measurement 5: 421-45.

________________

1977.

Social experimentation, truncated distributions,

and efficient estimation. Econometrica 45: 319-39.

________________

1979.

Attrition bias in experimental and panel data:

the Gary income maintenance experiment. Econometrica 47: 455-73.
________________

1980

Earnings effects, sample selection, and treatment

assignment in the Seattle-Denver income maintenance experiment.
Working Paper.

________________

1981.

Stratification on endogenous variables and

estimation: the Gary income maintenance experiment. In Manski, Charles,
and McFadden, Daniel, eds., Structural analysis of discrete data: with

econometric applications. Cambridge: MIT Press.

- 3C-

Tars:i,

Cranes, and McFadder, tanie]. 191. Alternative estimators and sample

desians for discrete choice analysis. In Manski, Charles, and McFadden,
Darie] ,

eds.,

Structural analysis

of

discrete

data: with econometric

applications. CarnbridQe: MIT Press.

Meier, Paul. 1978. The biggest public experiment ever: the trial of the Salk
poliomyelitis vaccine. In Tanur Judith M. et al .,

eds.,

Statistics:

a guide to the unknown. 2nd edition, San Francisco: Holden-Day.

Venti, Steven F., and Wise, David A. 1981. Individual attributes and
self-selection of higher education: college attendance versus college

completion. Forthcoming in Journal of Public Economics. (Also under
title "Test scores and self-selection. .
_______________

1982.

. .", NBER

Working Paper #709.)

Moving and housing expenditure: transaction costs

and disequilibrium. NBER Working Paper #735.

Ha usrno n—Wise

FOOT N UT ES

1. We do not mean

to

disregard important problems which still remain.

Questions of interactions may still arise, for example.

2. For a further discussion of time effects in experimental design, see
Hausman (1980).
3.

Of course, with only 2 periods this assumption is only a nornializatior.

4. Manski-McFadden (1981) consider a similar question in attempting
to minimize sample survey costs in a discrete choice model framework.

5. The opportunity to do any type of nonparametric analysis is severely
limited here because we do not have observations on the part of the
sample that was truncated.
6.

If P1 and P2 are known, the analysis can be simplified somewhat. See
Hausman-Wise (1981).

7. The unknown aspect arises because there does not exist a straight-

forward model for assignment of E-level. Part of the assignment procedure
involved qualitative judgments.
Q

TL
iS

iii
u_n ja...u ,'vu. uS 4n

.

I,-uo
,..),, ,.,, 1 —+rrit-t,

use in Vermont by Hausman-Trirnble (1981).
9. We are assuming a common truncation point, although in the NIT
experiment it depended on family size, which partly defines the poverty

limit. But we can add varying truncation points to our analysis with
no added complications.

Ha usma n—Wise
-

-

Table 1. Reported Cases of Poliomyelitisa

All Reported
Cases oer
Study Group

Studs' Population

l0,0O

Placebo control areas
Vaccinated
Placebo
Not inoculated

201 .229

41
81

338,778

54

221,993
725,173
123,605

34
61

200,745

Observed control areas
Vaccinated
Controls
Grade 2 not inoculated

53

aNumbers are from Table 1, p. 11 of Paul Meier (1978).

Figure 1. Selection Eased on an Exonenous Variable

f(y)

y

L

FiQure 1

