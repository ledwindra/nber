NBER WORKING PAPER SERIES

LEARNING WHEN TO QUIT:
AN EMPIRICAL MODEL OF EXPERIMENTATION
Bernhard Ganglmair
Timothy Simcoe
Emanuele Tarantino
Working Paper 24358
http://www.nber.org/papers/w24358
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
February 2018, Revised May 2019
We thank Matt Backus, Daniel Ereshov, Alberto Galasso, María García-Vega, Alessandro Gavazza,
Scott Hiller, Ben Jones, Raphaël Levy, Xiaolin Li, Massimo Motta, Kathleen Nosal, Nicholas
Pairolero, Mark Roberts, Marc Rysman, Pasquale Schiraldi, Florian Schütt, Carlos Serrano, Dave
Waguespack, and seminar and conference participants at Bocconi University, Boston College,
Boston University, CEMFI, Cornerstone Research, EIEF, HEC (Paris), Johns Hopkins University,
Northeastern University, Queen's University, Toulouse School of Economics, Universidad Carlos III
de Madrid, Universitat de Barcelona, University of British Columbia, University of Graz, University
of Mannheim, University of Nebraska-Lincoln, University of Portsmouth, University of Salento, ZEW,
Applied Microeconomics Workshop (Bolzano), Barcelona GSE Summer Forum on Applied Industrial
Organization, CSEF-IGIER Symposium on Economics and Institutions, IIOC (Boston), MaCCI
Conference on Innovation (Bad Homburg), Searle Center Annual Conference on Innovation Economics,
TILEC Conference on Competition, Standardization, and Innovation, and the ZEW INNOPAT
Conference for comments and suggestions. Ajándék Peák, Sucheth Prasad, Gabriele Salnaite, and Jacob
Walsh provided excellent research assistance. Bernhard Ganglmair would like to thank the economics
departments at Boston University and Northeastern University for their hospitality during his visits.
Emanuele Tarantino acknowledges financial support from the Collaborative Research Center (CRC)
Transregio 224 Bonn-Mannheim. Timothy Simcoe has worked as a consultant for several
organizations that participate in the development of IETF standards, including Microsoft, Tellabs, and
Apple. The views expressed herein are those of the authors and do not necessarily reflect the views
of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2018 by Bernhard Ganglmair, Timothy Simcoe, and Emanuele Tarantino. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided
that full credit, including © notice, is given to the source.

Learning When to Quit: An Empirical Model of Experimentation
Bernhard Ganglmair, Timothy Simcoe, and Emanuele Tarantino
NBER Working Paper No. 24358
February 2018, Revised May 2019
JEL No. D83,O31,O32
ABSTRACT
Research productivity depends on the ability to discern whether an idea is promising, and a
willingness to abandon the ones that are not. Economists know little about this process, however,
because empirical studies of innovation typically begin with a sample of issued patents or
published papers that were already selected from a pool of promising ideas. This paper unpacks
the idea selection process using a unique dataset from the Internet Engineering Task Force
(IETF), a voluntary organization that develops protocols for managing Internet infrastructure. For
a large sample of IETF proposals, we observe a sequence of decisions to either revise, publish, or
abandon the underlying idea, along with changes to the proposal and the demographics of the
author team. Using these data, we provide a descriptive analysis of how R&D is conducted within
the IETF, and estimate a dynamic discrete choice model whose key parameters measure the speed
at which author teams learn whether they have a good (i.e., publishable) idea. The estimates
imply that sixty percent of IETF proposals are publishable, but only one-third of the good ideas
survive the review process. Author experience and increased attention from the IETF community
are associated with faster learning. Finally, we simulate two counterfactual innovation policies:
an R&D subsidy and a publication-prize. Subsidies have a larger impact on research output,
though prizes perform better when accounting for researchers' opportunity costs.
Bernhard Ganglmair
ZEW Leibniz Centre for
European Economic Research
L7, 1, 68161
Mannheim, Germany
b.ganglmair@gmail.com
Timothy Simcoe
Boston University Questrom
School of Business 595
Commonwealth Avenue
Boston, MA 02215
and NBER
tsimcoe@bu.edu

Emanuele Tarantino
University of Mannheim
L7, 3-5, 68131
Mannheim, Germany
etaranti@gmail.com

“I haven’t failed. I’ve just found 10,000 ways that won’t work.” - Thomas Edison

1

Introduction

In research, many ideas do not work out. Innovative productivity therefore depends
on a researcher’s ability to discern whether an idea is promising, and her willingness
to abandon the ones that are not. Economists know little about this process, however, because data on “dry wells” in academic and commercial research are scarce.
For example, several authors have recently argued that declining R&D productivity
implies a greater failure rate in the search for big ideas (Gordon, 2016; Bloom et al.,
2017), but none of these failures are directly observed. Rather, the vast majority of
empirical studies of innovation begin with a sample of issued patents or published
papers that were already selected from a pool of initially promising ideas.
This paper studies the idea selection process – specifically, the decision to continue
or abandon a line of research, and the factors that influence how quickly researchers
learn whether they have a good idea. We examine these issues in the context of technical standards development, exploiting a unique data set containing information on
16,000 proposals submitted to the Internet Engineering Task Force (IETF), a voluntary organization that develops protocols used to manage Internet infrastructure. For
each IETF proposal, we observe a sequence of decisions to either revise, publish, or
abandon the underlying idea, along with the demographics of the author team, any
changes to the proposal over time, and public communications about the proposal
posted to IETF listservs. Using these data, we provide a descriptive analysis of how
R&D is conducted within the IETF, and then estimate a dynamic discrete choice
model whose key parameters measure the speed at which author teams learn whether
they have a good (i.e., publishable) idea. Our main empirical results show how the
1

speed of learning varies with author experience, attention from the IETF community,
and the composition of the relevant IETF Working Group. We then use the model
to compare simulated impacts of two counterfactual innovation policies: a research
subsidy and a publication prize.
Although we use the IETF as a setting to study learning in R&D, it is an important institution in its own right, comprised of engineers from a wide variety of
firms, academic, government and non-profit organizations whose collective mission is
to “make the Internet work better by producing high quality, relevant technical documents that influence the way people design, use, and manage the Internet.” As of
2018, half-a-trillion dollars in retail transactions and millions of hours of digital video
were transmitted using equipment that would not work if it deviated from IETFdefined protocols.1 Our findings shed light on how these protocols are developed and
refined.
The paper proceeds in four steps. Section 2 describes the IETF and documents
several empirical facts about innovation within the standards development process.
Just twenty-four percent of proposals submitted to the IETF are ultimately published. While ideas are often abandoned quickly, some go through many rounds of
revision before getting dropped. The hazard of publication increases with revisions,
and text-based measures of proposal content suggest that early revisions are more significant than later ones. Finally, data from patent citations to IETF standards show
that, conditional on publication, revisions are positively correlated with long-term
commercial impact.
Motivated by these descriptive findings, Section 3 develops a model of the decision
to continue or abandon a proposal. Our model combines a two-armed bandit with
1

The IETF mission statement can be found at www.ietf.org/about/mission/. Estimates of
retail e-commerce transactions are from U.S. Census Publication CB18-125. Estimates of digital
video transmission come from the Cisco Visual Networking Index.

2

a more traditional finite-horizon optimal stopping problem to capture two different
phases of the research process.2 The two phases are separated by a breakthrough
that resolves any uncertainty about whether a project will succeed.3 Before a breakthrough, the decision to continue or abandon a project reflects the standard trade-off
between exploration and exploitation common to the literature on experimentation
(Roberts and Weitzman, 1981). Researchers choose between the “safe” option of
abandoning a project, and the “risky” option of undertaking a costly revision that
may or may not produce a breakthrough. After a breakthrough, researchers face a
simple optimal stopping problem, where they compare the marginal costs and benefits
of continuing to “polish” their idea.
Empirically, our model can be formulated as a non-stationary dynamic discrete
choice problem with an unobserved state variable (i.e., whether a breakthrough has
occurred). In each period, researchers decide whether to continue or abandon their
project, with payoffs realized if and only if they stop after a breakthrough occurs.
We develop a maximum likelihood estimator similar to Pakes (1986), which yields
estimates of researchers’ opportunity costs, their prior beliefs about the distribution
of project quality, and the rate of learning. Finally, we provide sufficient conditions
for the existence of a unique solution to the model, yielding point-identification of
the learning and cost parameters.
The third part of the paper (Section 4) uses our model to study how IETF participants learn about idea quality.4 Our baseline estimates suggest that the breakthrough
2

Our model and data do not capture any R&D activity that may happen before proposals are
submitted to the IETF. For our purposes, however, it is sufficient that experimentation and learning
continue after ideas reach the point of submission.
3
Because IETF proposals must achieve consensus in order to be published, that is how we
interpret the breakthrough in our application. In other settings, a breakthrough might correspond to
achieving some key technical milestone. We have retained the term “breakthrough” for consistency
with the theoretical literature.
4
In contrast to prior research that models standardization as technology choice, we treat each
proposal as a standalone project. In practice, the IETF seeks to block or merge competing proposals,

3

arrival rate – a key parameter that governs the speed of learning – is around 17 percent per revision, conditional on having a good idea to start with. Consistent with
our priors about how the IETF works, we obtain higher estimates of initial quality,
and faster rates of learning, for the sub-sample of proposals initiated inside an IETF
Working Group. Our baseline estimates also imply that around two-thirds of the
good ideas proposed to the IETF are abandoned. Although this sounds like a large
number, it can be rationalized by observing that even publishable ideas typically
have only a modest payoff, and that researchers may have high opportunity costs.
Nevertheless, a high rate of abandonment points to significant frictions in the review
process, and suggests that there is an important role for polices and institutions that
could accelerate learning among IETF participants.
To explore how various factors influence the speed of learning, we estimate models that specify the breakthrough arrival rate as a function of observable author and
proposal characteristics. Our estimates reveal that learning is faster for more experienced author teams, but uncorrelated with the size of the team. Learning is also
faster when ideas receive more attention in the form of emails to IETF listservs mentioning a specific proposal. Finally, we find a non-monotonic inverted-U relationship
between learning and commercial interest, measured as the share of proposal-related
emails originating from a dot-com address.
The last part of the paper (Section 5) uses the parameter estimates from our baseline model to conduct two counterfactual policy experiments. In the first experiment,
we compare an R&D subsidy that lowers the marginal cost of a revision for all projects
to a publication prize that increases the payoff only for successful projects (holding
fixed the total budget, in patent citations, of the two policies). This experiment was
so that competition within projects will be an important part of the learning process we seek to
measure.

4

motivated by our discussions with IETF participants, who noted that some companies award a financial prize to engineers who contribute to successful proposals, while
others merely subsidize their participation in the IETF. We find that although both
policies can increase research output, prizes perform better after accounting for the
private costs of over-developing bad ideas. Our second counterfactual considers the
cost of misaligned priors, and shows that within our model over-confidence is more
costly than pessimism.
This study contributes to several streams of literature. To our knowledge, it is the
first paper to estimate a structural model of learning in R&D, focusing on the decision
to continue or abandon a line of research.5 The seminal studies of learning in R&D,
including Cohen and Levinthal (1989) and Henderson and Cockburn (1996), focus on
inter-firm knowledge spillovers. Krieger (2017) builds on their work by studying how
a pharmaceutical firm’s decision to continue or terminate a drug development project
responds to news about its rivals’ clinical trials. An important difference between
our setting and Krieger’s is that he emphasizes learning through the arrival of “bad
news.” In our model, IETF participants learn through the arrival of “good news,”
and this distinction is useful for thinking about its generalizability. In particular,
our approach is well-suited to applications where there is fundamental uncertainty
about demand or the perceived importance of an idea (e.g., entrepreneurship or open
source software development) as opposed to the safety or efficacy of a new product.
Although late-stage pharmaceutical R&D corresponds to bad news learning (DiMasi
et al., 2016), the good news approach might still be useful for modeling exploratory
pre-clinical research. More generally, our model could be applied to study the tradeoff
between exploration and exploitation in a variety of “exploratory” research settings6
5

In a context that abstracts from learning, but related to R&D, Bhattacharya (2017) considers
a procurement model of multi-stage innovation.
6
Outside the pharmaceutical context, Allen (1966) provides an early descriptive analysis of how

5

or the factors associated with persistence and success in peer review processes, such
as those related to scientific funding (see, e.g., Li (2017) and Li et al. (2017)).
This paper also contributes to the economic literature on compatibility standards,
which has typically emphasized choice between competing technologies, either via a
decentralized standards war (Katz and Shapiro, 1985; Dranove and Gandal, 2003;
Augereau et al., 2006) or a centralized consensus decision-making process (Farrell
and Saloner, 1988; Simcoe, 2012; Lerner and Tirole, 2015). Our main contribution
is to highlight the importance of cooperative R&D within consensus standardization.
While cooperative innovation in standards development has received some attention
from theorists, such as Katz (1986), Cabral and Salant (2014), or Ganglmair and
Tarantino (2014), few empirical papers use standards development as a window onto
the micro-foundations of R&D. Exceptions include Baron et al. (2016), who study
upgrades to existing compatibility standards, and a pair of papers by Baron et al.
(2014) and Wen et al. (2015) that examine how the supply of standards influences
firms’ external patenting activity in similar technology areas.
Our estimate that over two-thirds of the good ideas proposed to IETF are abandoned complements recent work by Bell et al. (2017) on “missing Einsteins” in American innovation. They show that a child’s race, gender, and socioeconomic status
are strong predictors of future patenting. Whereas their findings highlight selection
problems on the extensive margin of the innovation process – specifically the decision
to participate in inventive occupations – we focus on the intensive margin associated
with idea selection and publication. Our results suggest that both “missing papers”
and “missing persons” are key drivers of research productivity.
engineers select among alternative problem-solving approaches that bears many similarities to our
paper. More recently, Howell (2017) and Gross (2017) have studied the relationship between feedback
and learning, and Thompson and Zyonts (2017) have analyzed experimentation in the adoption of
CRISPR, a DNA-editing technology.

6

Our theory builds upon early models of experimentation (Weitzman, 1979; Roberts
and Weitzman, 1981; Moscarini and Smith, 2001) that were later extended to consider
strategic experimentation (Bolton and Harris, 1999; Keller et al., 2005; Bonatti and
Hörner, 2011).7 To our knowledge, this is the first paper to estimate the parameters
that drive Bayesian learning in such a model. Our assumption that projects have
both a fundamental and a more implementation-focused dimension of quality, with
greater uncertainty and more learning about the former, is borrowed from the Ellison
(2002) model of learning in economics publishing. Kerr et al. (2014) survey the
literature on experimentation in innovation and entrepreneurship, and distinguish
between two fundamental forms of experimentation. The first takes place between
competing ideas on the market, and is characterized by Darwinian dynamics. The
second happens in the process of bringing new ideas to the market. Due to the goals
and the organizational structure of the IETF, the form of experimentation we study
is closer to the latter: the research teams in our dataset experiment by submitting
new versions of their proposal and learn about its potential for future success. This
form of experimentation is consistent with the idea of cooperative standardization,
while the first captures the idea of standard wars.
Finally, this study contributes to a broadly defined literature that estimates structural models of learning. Applications to consumer behavior include Erdem and
Keane (1996), as well as the various studies described in Ching et al. (2013) and
Ching et al. (2017). Both Crawford and Shum (2005) and Dickstein (2014) consider
learning in the context of matching patients to prescription drugs. Notably, the latter
paper formulates the problem as a multi-armed bandit. Chan and Hamilton (2006)
7

In the vein of the literature on strategic experimentation, Akcigit and Liu (2016) study investment decisions when firms do not know their competitors’ dead-end results. Instead, Moscarini and
Squintani (2010) analyze R&D competition when privately informed firms learn from exit of their
opponents.

7

study learning in a setting featuring patients deciding on when to quit a clinical trial.
Doraszelski et al. (2018) consider the learning that takes place as firms enter a new
market and converge to a competitive equilibrium.

2

The Internet Engineering Task Force

The IETF is a voluntary organization that contributes to the engineering and evolution of internet technologies. It is considered the principal entity engaged in the
development of new internet standards (Hoffman, 2012). During the early 1990s,
the IETF evolved from a small quasi-academic networking community into a highstakes forum for technical decision-making (Simcoe, 2012), and it is now populated
by researchers and engineers from public and private organizations, including firms,
universities, and government agencies.
The IETF has played a major role in the technological development of the internet. The most famous IETF standard is the TCP/IP protocol used to route all
internet traffic from source to destination. A more recent standard is the Session Initiation Protocol (SIP), which enables VoIP (“Voice over IP”) services, and is used for
video conferencing, instant messaging, file transfer, and online games. Other IETF
standards include the Network Address Translation (NAT) protocol, which defines
an interface between private and public networks (thereby dramatically expanding
the supply of IP addresses), and the Dynamic Host Configuration Protocol (DHCP),
which distributes addresses among machines attached to a network.8
8

Table A.1 lists some of the more prominent standards created and then certified by the IETF.

8

2.1

IETF Standards Development

Much of the IETF’s activity takes place on email lists, and in a series of face-to-face
meetings held three times each year. In both forums, IETF participants propose new
standards and extensions to existing protocols, debate the merits of these proposals,
and decide whether to collectively endorse them. Figure 1 provides an overview of
this process.
At the beginning of the IETF standard setting process, participants identify a
problem and form a working group (WG) to consider solutions.9 To prevent forum
shopping and overlapping technical agendas, new working groups must be approved
by an advisory board called the Internet Engineering Steering Group (IESG). Once
a working group is formed, anyone can submit a proposal by posting it to a public
repository. New proposals are called Internet Drafts (IDs). Although some IDs describe an entire protocol, such as SIP, NAT, or DHCP, the majority propose updates
and extensions to the functionality of existing standards.
The authors of a proposal can decide whether they want their ID to be considered
within a WG or as an individual submission (typically because there is no appropriate
WG, or because the suitable WG is busy on other projects). After their submission,
IDs are debated on the email discussion lists and at IETF meetings. Authors incorporate feedback from the community into their proposals. IDs are continually revised,
and, as a statutory rule, an unpublished ID expires after six months of inactivity,
leading to its removal from active online directories.10 This process continues until either the IETF reaches a rough consensus in favor of publishing the ID, or the
authors abandon their proposal.
9

This synopsis draws on Hoffman (2012) and Simcoe (2012).
Since an ID’s removal does not necessarily imply that authors cannot resume the project, in our
estimation sample construction, we take a more conservative approach to define an abandoned ID.
10

9

Figure 1: IETF Flow Chart
Internet
Draft
Working
Group

Request For Comments (RFC)
Yes

Consensus?

Proposed
Standard

Informational Experimental

No

Individual

Yes

No

Revise?

Expired Internet Draft

While the IETF provides no formal definition, rough consensus is often described
as the dominant view of the relevant working group and implies support from well over
51 percent of active participants. In practice, a working group chair decides whether
consensus has been reached.11 If the working group chair declares a consensus, there
is a last call for comments from the WG, and the ID is submitted to the IESG. The
IESG reviews the proposal and issues a second last call for comments from the entire
IETF community. Any comments or formal appeals are reviewed by the IESG and
may be referred back to the working group for resolution. If the IESG is satisfied
that a consensus exists within the working group and sees no problem with the ID,
it will be published as a Request For Comments (RFC).
The IETF publishes two types of RFCs: standards-track and nonstandards-track.
Standards-track RFCs define new features and protocols, which progress in maturity
from Proposed Standard to Draft Standard and then finally to Internet Standard.12
Nonstandards-track RFCs either provide information that complements a standard,
such as implementation guidelines and technical references (Informational RFCs),
11

There is also an appeal process for the authors who feel that a working group chair, or the IESG,
has taken the wrong choice on their ID.
12
The IETF requirements for advancing protocols along the standards track to the level of Draft
or Internet Standard include multiple independent implementations and significant experience in
the field. We do not study these later stages of the standardization process here.

10

or describe new protocols that are not yet ready for standardization (Experimental
RFCs). Although standards- and nonstandards-track RFCs go through an identical
publication process, nonstandards do not place normative constraints on new products
and therefore tend to be less controversial. In some of our empirical models, we treat
nonstandards-track RFCs as a control sample where consensus occurs automatically.
In the formal model described below, authors learn over time whether their proposal will produce a consensus and, therefore, a payoff. This leads to a dynamic decision problem in which Bayesian updating helps rationalize the decision to abandon
a proposal. In contrast to prior literature that treats standardization as technology
choice, we assume that each Internet Draft represents a standalone line of research.
This is consistent with the IETF’s emphasis on consensus. For instance, the Area
Directors who oversee several WGs and sit on the IESG are expected to block or
merge projects that are in clear conflict with one another. At the same time, IETF
participants can certainly have divergent interests. Thus, our model might be viewed
as a reduced-form representation of noncooperative bargaining, where the key parameters capture not only learning about proposal quality, but also considerations
such as gaining enough political support for a particular solution. Before introducing
the model, however, we describe our data and present several stylized facts that it is
meant to capture.

2.2

Data

Our primary data source is the online archives of the IETF. These archives contain the
full text for each version of every Internet Draft submitted after July 1990, for both
published RFCs and abandoned proposals, along with various pieces of bibliographic

11

information.13,14 From each version of every ID, we extract the names and affiliations
of all authors in order to construct measures of team size, experience, and composition.
We also download all of the archived IETF email lists and create proposal-specific
measures of feedback by counting the number of messages that mention a specific
Internet Draft. As a measure of ex-post commercial significance, we count the number
of non-patent prior art citations from U.S. patents to an Internet Draft or associated
RFC. The data appendix provides details on the process that we used to collect, clean,
link, and merge these data into a panel where the unit of analysis is an ID-version.
Table 1 reports variable means for five groups of IETF proposals: the full sample, proposals initially submitted to a WG, abandoned IDs, Proposed Standards,
and nonstandards-track RFCs. Note that in defining the last three samples, we are
conditioning on outcomes. The first three rows of the table report means of predetermined variables, while variables below the solid line are realized during or after the
publication process.
A number of interesting patterns can be seen in Table 1. First, although working
group proposals constitute just one quarter of the full sample, about two thirds of
standards and half of nonstandards-track RFCs originate within a WG. Second, the
author-teams on individual and WG submissions have similar size and experience
levels. In particular, even for non-WG proposals, the average team has an author with
15 prior submissions, suggesting that they will not differ in their rate of learning due
to inexperience. There is, however, a strong positive correlation between experience
13

Our raw sample ends in September 2015. We drop IDs that are still considered active at
that point. We further exclude IDs initiated before 1996 and in or after 2010 (to avoid selection on
outcomes). For more details on the final sample construction, see the data appendix (in Appendix F).
14
In most cases, the IETF file-naming convention allows us to identify the relevant working group,
and track successive revisions within a project. For instance, the ID draft-ietf-homenet-arch-02
(entitled “Home Networking Architecture for IPv6”) was associated with the homenet WG and
was succeeded by draft-ietf-homenet-arch-03 and eventually published as RFC 7368. The data
appendix (in Appendix F) describes how we link IDs to create a single project when there is a change
in file name.

12

Table 1: Summary Statistics
Full

Working

Standards-track

Nonstandards-

Sample

Group

Abandoned Published

track

WG (%)
Team Size (Author Count)
Experience (max Projects)

24.44
2.28
15.01

100.00
2.45
15.69

14.11
2.22
13.50

65.25
2.43
21.87

46.39
2.48
16.84

Versions
Communication (Emails)
Published RFC (%)
Citations
N (Projects)
N (Versions)

3.55
21.20
23.97
2.99
16,091
57,179

5.60
33.78
56.10
8.30
3,932
22,025

2.09
9.87
0.00
0.76
12,234
25,511

9.33
69.19
100.00
12.23
2,210
20,622

6.71
41.03
100.00
7.19
1,647
11,046

Notes: This table reports sample means at the Internet Draft (proposal) level. Team Size is the number of authors
of the initial draft; Experience (max Projects) is the number of IDs the most prolific author has completed before
the initial draft of a given project; Communication is the average number of emails that reference a project;
Citations is the number of U.S. patent prior art citations to a given project. For additional information on the
construction of these variables, see the data appendix (in Appendix F).

and publication that can be seen by comparing the average experience across the
Published and Abandoned columns. Moving to the endogenous variables, we see
that published IDs go through many more revisions than abandoned proposals and
are therefore mentioned in many more emails. And finally, we observe a twelve-fold
increase in U.S. patent citations for Published IDs, consistent with our assumption
that payoffs are contingent on achieving a breakthrough.

2.3

Descriptive Analysis

To motivate our theoretical model of learning, we now present several stylized facts
about the internet standard setting process. This part of the analysis has three objectives: (1) to establish that proposals are changing over time, (2) to characterize the
hazard rates for success and abandonment, and (3) to link revisions with commercial
significance.

13

Figure 2: Cosine Distance to First Draft

Fact #1: Revision Distance
Figure 2 shows how projects change during the revision process.15 To create the
figure, we treat each version of a proposal as a “bag of words,” calculate the cosine
distance between that version and original submission, and then plot the mean distance (conditional on the number of revisions) for standards- and nonstandards-track
proposals, respectively.16
Although it is hard to give a natural interpretation to this distance metric, the
figure suggests that each revision takes the proposal further from the initial submission, with the rate of change rapid at first, and gradually diminishing. The shape of
the plot is very similar for standards- and nonstandards-track proposals. In fact, if
one conditions on nonstandards submitted to a WG, or those eventually published as
15

We use the terms “revision” and “version” interchangeably. The number of revisions, therefore,
includes the very first version of the draft (and is thus equal to the total number of versions).
⋅x1
16
Formally, the cosine distance of a version T from the initial version is defined as: 1 − ∥xxTT∥∥x
,
1∥
where xT is the vector of word frequencies for version T and x1 the vector for the initial (i.e., first)
version. See Appendix F for more details.

14

Figure 3: Hazard Rates

an Experimental RFC, we cannot statistically distinguish between the two curves.17

Fact #2: Hazard Rates
Figure 3 plots the hazard rates (i.e., the probability of exit, conditional on survival to version t) for standards- and nonstandards-track proposals. We assume that
standards-track proposals can experience three possible events in each period (abandon, publish, or continue), whereas nonstandards-track IDs have just two potential
events (publish or continue).18
The most striking aspect of Figure 3 is the rapidly decreasing hazard of abandonment: almost 40 percent of proposals are never revised, but conditional on reaching
a fifth version, the probability of abandonment falls below 10 percent. At the same
17

Specifically, we estimate a regression model that includes a dummy for each version-number
interacted with a nonstandards-track indicator and cannot reject the hypothesis that the interaction
coefficients are jointly zero.
18
In practice, some standards-track proposals may morph into nonstandards, and it may initially
be unclear which track an individual ID is on. Our interviews with IETF participants suggest these
scenarios are infrequent.

15

time, the hazard of becoming a Proposed Standard climbs from zero on initial submission to about 10 percent by the eighth version. Together, these two hazard rates
imply that the probability of publication increases sharply with the number of revisions. The increasing odds of publication are consistent with the arrival of “good
news” over time, which will be a feature of our learning model.
Another feature of Figure 3 is that the standards-track publication hazard never
climbs above 16 percent, even though the hazard of abandonment falls to nearly zero.
This suggests that author-teams continue to revise proposals even after it becomes
clear that they will be published, and it leads us to propose a two-phase model rather
than a theory where consensus leads to immediate publication.
Finally, the hazard of nonstandards-track publication begins at around 8 percent
(implying that some are published immediately) and climbs to around 20 percent after 10 revisions. Thus, although standards and nonstandards exhibit a similar degree
of textual change (see Figure 2), nonstandards are more likely to be published in any
given period. This provides some justification for our later assumption that there is
no learning on the nonstandards-track – because lower commercial stakes imply immediate consensus – even though both tracks provide opportunities for improvement.

Fact #3: Revision and Citation
The final piece of our descriptive analysis examines the relationship between revisions
and U.S. patent citations to published RFCs (our proxy for commercial impact).19
Specifically, we estimate linear regressions of the form

Cites i = αy + (β1 + β2 Nonstandard i ) ∗ log(Versions i ) + εi
19

(1)

Roach and Cohen (2013) provide a discussion of measurement issues associated with counting
non-patent prior art references in U.S. patents, and suggests that these citations are a superior
measure of knowledge reuse than more commonly used metrics based on citations to other patents.

16

Figure 4: Expected U.S. Patent Citations

where Cites i is a count of citations to published RFC i, and αy are publicationyear effects that control for right-censoring in the citation process. Figure 4 plots
separate fitted values for standards- and nonstandards-track RFCs, assuming a publication year of 2000. The dotted lines correspond to estimates from a non-parametric
model where we replace log(Versions i ) with dummies for each value of the variable
Versions i .
There are two main take-aways from this analysis. First, citations increase (at a
declining rate) with the number of revisions. And second, citations to nonstandardstrack RFCs are lower than for standards. We use fitted values from these regressions
as our measure of expected payoffs in the structural model described below. Thus, in
the absence of monetary measures of cost and benefit, citations will serve as the numeraire, providing a unit of measure for teams’ opportunity costs and the evaluation
of counterfactual policies.

17

3

Experimentation and Learning in R&D: A Model

This section proposes a model of experimentation and learning in research that is
broadly consistent with the institutions and descriptive analysis of IETF standardization. We also discuss estimation and identification of key parameters.

3.1

Overview

Figure 5 illustrates the basic structure of our model. At t = 0, a team of researchers
is endowed with an idea that has one of two types, θ ∈ {good, bad}. This type might
represent technical or commercial feasibility, but in our application it stands for the
possibility of reaching consensus. We model the team as a single agent that does not
know its type but can learn through costly experimentation. With each experiment,
there is a strictly positive probability that good ideas yield a breakthrough, whereas
bad ideas always fail. Thus, teams that do not have a breakthrough grow increasingly
pessimistic about their type. Experimentation continues until the team decides to
stop.
Payoffs depend on whether a team’s idea is published (after breakthrough) or
abandoned (before breakthrough), and the number of revisions. More specifically,
the gross benefits of publication are π̂(t), a project’s scrap value is zero, and the
cumulative (i.e., up to t) value of the stochastic revision costs is Ft . This leads to
ex-post payoffs π̂(t) − Ft for published projects, and −Ft for abandoned projects.
Before a breakthrough, this model is a discrete time version of the two-armed
bandit problem analyzed by Keller et al. (2005). The decision to stop is the safe arm,
and the decision to continue is the risky arm.20 After a breakthrough, it becomes a
simple optimal stopping problem, where the team compares the expected costs and
20

Many mathematicians refer to this type of problem, with one “safe” arm, as a one-armed bandit,
or degenerate two-armed bandit problem.

18

−F2

−F3

t=1

2
3
Abandon

t=1

2

3

−F2

−F3

go

od

−F1

d

ba

−F1

−Fτ

τ

Breakthrough

Figure 5: Stylized Model of R&D Process
π̂(τ +1)
−Fτ +1

τ +1

Abandon
4
⋯

−F4

−F...

π̂(τ +2)
−Fτ +2

π̂(⋅)
−F...

⋯
τ +2
Publish

⋯

⋯

−F...

−F...

benefits of revision.
When fitting the model to data, we estimate π̂(t) using citations data and assume
that this function is known to both the team and the econometrician. Based on
publication outcomes, the econometrician also knows if a breakthrough occurred, but
not when. The key structural parameters introduced below are the prior probability
of having a good idea, p, the breakthrough arrival rate, b, and the (average) marginal
cost of a revision, F .21 When the hazard of breakthrough is constant, there is a
simple closed-form expression for the team’s posterior beliefs (Heidhues et al., 2015)
that simplifies estimation. We assume a finite horizon and solve the model using
backwards induction, which allows us to characterize all possible states in which a
breakthrough has occurred. In that sense, our estimation strategy is similar to Pakes
(1986), except that where he takes costs as known and estimates the distribution of
benefits, we do the reverse.
21

In each period the costs of an experiment are the sum of a deterministic revision cost F (t) and
a random cost shock εt that reflect both direct and opportunity costs to the researcher.

19

3.2

Model

A team of risk-neutral agents initiates a project with type θ ∈ {good, bad}.22 The
research environment is characterized by three state-variables. The project’s latent
type θ is unobserved to both the team and the econometrician. The integer t represents the number of versions created and is observed by both team and econometrician. Finally, the indicator variable σt denotes that the team has achieved a
breakthrough; it is observed by the team and revealed to the econometrician when
the project is either published or abandoned after T ≤ T versions.
In each period t ≥ 1, the team decides whether to revise its project and advance
to period t + 1. Revisions are costly, but increase the expected benefits of a successful
project, as explained below. Revisions also provide information about θ. Specifically,
we assume the team knows the following transition probabilities for σ:

Pr(σt+1 = 1∣σt = 0, θ = good) = b

(2)

Pr(σt+1 = 1∣σt = 0, θ = bad) = 0

(3)

Pr(σt+1 = 1∣σt = 1, θ = good) = 1.

(4)

Equation (2) states that good projects have a constant per-period breakthrough arrival rate b; Equation (3) states that bad projects never experience a breakthrough;
and Equation (4) states that breakthrough is an absorbing state – once a project is
revealed to be good, that information cannot be withdrawn.23
Now consider the team’s beliefs, Pr(θ = good∣t, σt ) ≡ p̂(t, σt ). At the start of the
22

We assume the initial version of the project comes at zero cost and can therefore ignore the
team’s entry decision.
23
Note that discrete time with endogenous costly revisions (which we interpret as a substantive
change to the underlying idea) make this a model of active learning, in the spirit of Ericson and
Pakes (1995), even though the stochastic process described by Equations (2)–(4) is quite general
and could also be assumed to operate independently of the team’s control.

20

research process, we assume the team’s priors over project type are p̂(0, 0) = p, with
0 < p < 1. After a breakthrough, the team knows its type, so p̂(t, 1) = 1. Before a
breakthrough, Equation (2) and repeated application of Bayes’ rule show that:
t

p (1 − b)
p̂(t, 0) =
t.
(1 − p) + p (1 − b)

(5)

For 0 < b < 1, the function p̂(t, 0) is strictly decreasing in t, so the team becomes more
pessimistic with time as its revisions fail to yield a breakthrough.
To see how learning influences a team’s decision to revise or quit, we must place
some additional structure on that choice. Let π̂(t) represent the expected quality of
an idea, and Ft ≡ ∑t−1
k=0 (F (k, σk ) + εk ) the cumulative cost of t versions, where F (t, σt )
is the deterministic portion of per-stage opportunity costs, and εt is a random cost
shock.24 The ex-post payoff to a team that stops after T versions is therefore

π(T, σT ) = σT π̂(T ) − FT .

(6)

We assume the team does not discount future payoffs.25 Because the benefits π̂(T )
are conditional on breakthrough, we use the terms “published” and “abandoned”
for projects terminated when σT = 1, 0, respectively. The sequence of steps in each
period t is:
- Step t.1: The team observes whether there was a breakthrough in t − 1, and it
observes the private cost shock εt .
24

Recall that we assume the initial version comes at zero cost so that F (0, 0) = ε0 = 0.
Magnac and Thesmar (2002) showed that the discount rate and payoffs in a dynamic decision
problem are not jointly identified, and it is common practice to fix the former parameter. In principle,
we could add a per-revision discount rate to π̂(t). However, setting the discount factor to unity seems
a reasonable choice in our empirical setting, where lags in ex-post deployment of the standard are
likely to be more consequential than those in initial protocol development.
25

21

- Step t.2: Given the state of the project, σt , the team’s beliefs, p̂(t, σt ), and the
marginal cost of a revision, F (t, σt ) + εt , the team decides whether to stop or
continue. Stopping implies a permanent decision to publish or abandon the
project.
We are now ready to analyze the team’s dynamic decision problem.

3.3

Recursive Characterization

The team’s objective is to choose a contingent plan of actions that maximizes its
payoff. In each period, stopping yields the payoffs in Equation (6). Alternatively,
the team can create a revision and preserve the option to continue the project. The
resulting value function is

V (t, σt ) = max {σt π̂(t), Eε,σ (V (t + 1, σt+1 )) − F (t, σt ) − εt } ,

(7)

where Eε,σ (⋅) is the expectation with respect to the realization of breakthrough and
future cost shocks, which we assume to be independent from each other. As can be
seen in (7), the value function depends on whether breakthrough has been reached in
the project’s development process. The two state variables, t and σt , evolve according
to a first-order Markov process. Because σt is not observed by the econometrician,
the team’s decision problem is non-stationary. This introduces serial correlation in
the controlled stochastic process generating the value functions.26 For this reason,
we find it useful to solve the team’s recursive decision problem under the assumption
that the time horizon is finite, with the value of the potential standard dropping to
We refer to the stochastic process generating {V (t)}∞
t=1 as “controlled” because, although it is
inherently random, it is also affected by the team’s decision to continue.
26

22

zero after T revisions.27 This will allow us to account for all possible states of the
problem (i.e., the stages when a breakthrough could have occurred).
The optimal decision rule consists of a series of cut-points {ε̄σt }Tt=1 , such that in each
period the team will revise the project if and only if εt ≤ ε̄σt . We assume i.i.d. cost
shocks with cumulative distribution G, and for notational convenience define the
state-contingent continuation probabilities Gσ (t) ≡ G(ε̄σt ). To find the optimal cutpoints, we first consider the stopping problem following a breakthrough, when σt = 1,
and then turn to the two-armed bandit problem that precedes a breakthrough, when
σt = 0.
Post-breakthrough. Suppose a breakthrough occurred at some t < T . A team
that reaches the terminal period will stop and obtain payoffs of π̂(T ) > 0.28 In T − 1,
the value function is V (T − 1, 1) = max{π̂(T − 1), π̂(T ) − F (T − 1, 1) − εT −1 } and the
team will revise the project if and only if

εT −1 ≤ ε̄1T −1 ≡ π̂(T ) − π̂(T − 1) − F (T − 1, 1).
Otherwise the team stops and publishes the project. This implies that the value
function in period T − 2 is V (T − 2, 1) = max{π̂(T − 2), Eε (V (T − 1, 1)) − F (T − 2, 1) −
εT −2 }, where Eε (⋅) is the expectation with respect to future cost shocks only, given
that uncertainty over σ realized in τ < t, and

Eε (V (T − 1, 1)) = G1 (T − 1) [π̂(T ) − F (T − 1, 1) − εT −1 ] + (1 − G1 (T − 1)) π̂(T − 1).
27

To grasp how strong this assumption is, note that only the projects with no breakthrough by T
are affected. In our empirical analysis, the probability that a good project receives cost shocks that
lead to T revisions without a breakthrough is infinitesimal at estimated parameter values.
28
The process is force-terminated in T so that there are no decisions and G1 (T ) = G0 (T ) = 0.

23

The team continues solving this problem backwards through t = 1. Hence, in each
t ∈ [1, T ) and given any continuation value function V (t + 1, 1), the team chooses to
⎧
⎪
⎪
⎪
if εt > ε̄1t
⎪ Stop
⎨
⎪
⎪
⎪
Continue if εt ≤ ε̄1t .
⎪
⎩
The value function is then
⎧
⎪
⎪
⎪
if εt > ε̄1t
⎪ π̂(t)
V (t, 1) = ⎨
⎪
⎪
⎪
E (V (t + 1, 1)) − F (t, 1) − εt if εt ≤ ε̄1t ,
⎪
⎩ ε

(8)

and the post-breakthrough cost cut-points are defined as

ε̄1t ≡ Eε (V (t + 1, 1)) − π̂(t) − F (t, 1).

(9)

Pre-breakthrough. Before a breakthrough, the team faces a two-armed bandit
problem. In particular, they must choose between stopping to obtain a fixed (nil)
retirement value, and running a costly experiment that may reveal their project’s
type. At any stage t where σt = 0, the value function is

V (t, 0) = max{0, Eε,σ (V (t + 1, σt+1 )) − F (t, 0) − εt },

(10)

where Eε,σ (⋅) is defined above. The project’s continuation value is

Eε,σ (V (t + 1, σt+1 )) = bp̂(t, 0)Eε (V (t + 1, 1)) + (1 − bp̂(t, 0)) Eε,σ (V (t + 1, 0)).

(11)

If the team continues in t, it incurs costs F (t, 0) + εt and expects continuation payoffs
Eε,σ (V (t + 1, σt+1 )) as in Equation (11), where breakthrough arrives with probability

24

bp̂(t, 0) and the post-breakthrough value function V (t+ 1, 1) is given by Equation (8).
To find the sequence {V (t, 0)}Tt=1 , we start in the terminal period, T , where, if
σT = 0, the process ends and the team’s payoffs are 0. In T − 1, the team will revise
the project if its expected payoffs are nonnegative, or

εT −1 ≤ ε̄0T −1 ≡ bp̂(T − 1, 0)π̂(T ) − F (T − 1, 0).
The team then solves this problem backwards through t = 1.
By analogy to the post-breakthrough phase, for each t ∈ [1, T ) and given any
continuation value function V (t + 1, 0), the team chooses to abandon the project if
and only if εt > ε̄0t . The value function is then
⎧
⎪
⎪
⎪
if εt > ε̄0t
⎪ 0
V (t, 0) = ⎨
⎪
⎪
⎪
E (V (t + 1, σt+1 )) − F (t, 0) − εt if εt ≤ ε̄0t ,
⎪
⎩ ε,σ
and the cost cut-points are defined as

ε̄0t ≡ Eε,σ (V (t + 1, σt+1 )) − F (t, 0).

(12)

Comparing Equations (9) and (12) shows how a breakthrough influences the
team’s decision. Before a breakthrough, the marginal benefits of continuation are
Eε,σ (V (t + 1, σt+1 )), whereas afterwards they are Eε (V (t + 1, 1)) − π̂(t). The first expression begins large but declines with t as the team grows more pessimistic. Although
the first term in the second expression is larger than pre-breakthrough expected benefits, the marginal gains are net of π̂(t), because after a breakthrough the team is only
“polishing” an idea that it knows will be published. Hazards of quitting and publication will reflect both processes, which in turn depend on costs F (t, σt ), benefits π̂(t),

25

and the speed of learning as governed by (b, p). Moreover, although the model does
not include persistent unobserved heterogeneity in payoffs, allowing π̂(t) to increase
with t helps to control for pre-breakthrough selection on unobserved quality.

3.4

Likelihood Function

To derive the likelihood function, assume that π̂(⋅) and G(⋅) are known, and consider a
project published after T versions. In order to account for the fact that a breakthrough
could have occurred at any τ = 0, ..., T −1 (and is observed by the team at the beginning
of any τ +1), it is helpful to define a function ρ(τ, T ) as the probability of breakthrough
in t = τ and publication in t = T . This function is equal to
τ

T −1

j=0

k=τ +1

τ
ρ(τ, T ) = b (1 − b) (1 − G1 (T )) ∏ G0 (j) ∏ G1 (k).

(13)

Summing over all periods when a breakthrough could occur (and accounting for the
prior probability of a project being good), we can write the likelihood of publication
in T as
T −1

Pr(T, σT = 1) = p ∑ ρ(τ, T ).

(14)

τ =0

The log-likelihood for projects published in period T is then equal to
T −1

LL(T, σT = 1∣b, p, F ) = log(p) + log ( ∑ ρ(τ, T )) ,
τ =0

where F denotes a vector of cost parameters.

26

(15)

Now consider the likelihood of abandoning a project after T versions:
T −1

T

T −1

Pr(T, σT = 0) = (1 − p) ∏ G0 (k) (1 − G0 (T )) + p (1 − b) ∏ G0 (k) (1 − G0 (T )) . (16)
k=0

k=0

The first term accounts for bad projects where σt = 0 for all t, and the second term
accounts for good projects that have no breakthrough. Hence, the log-likelihood for
abandonment in a given T is equal to
T −1

T
LL(T, σT = 0∣b, p, F ) = ∑ log(G0 (k)) + log(1 − G0 (T )) + log ((1 − p) + p (1 − b) ) . (17)
k=0

Given data on revisions and publication outcomes (Ti , σTi ), we can now write the
log-likelihood for a sample of N projects indexed by i:
N

LL(b, p, F ) = ∑ LL(Ti , σTi ∣b, p, F ),

(18)

i=1

and search for the parameter vector (b, p, F ) that maximizes this object.29

3.5

Identification

To illustrate how the data and model jointly identify the structural parameters, we
consider a two-period example.30 The team observes whether there was breakthrough
at t = 0 and decides whether to stop or continue in t = 1, with a revision providing the
possibility of breakthrough before the process ends at T = 2. Let St > 0 and At > 0
represent the number of projects published and abandoned, respectively, in period t,
29

We have ignored the possibility that projects could be censored. However, the likelihood can
easily accommodate right-censored IDs, because, for any value of the parameters, we know the
probability of a breakthrough by t, and the probability of a series of cost-shocks that imply continuation to that period. Appendix C provides a formal derivation of the log-likelihood function when
right-censored projects are included.
30
The arguments that we make in the two-period example generalize to T periods. We provide
formal proofs of such a T -period extension in Appendix B.

27

and define Ntσ as the (unobserved) number of projects in state σ at the start of each
period. We normalize total projects to one, so that N11 + N10 = 1 = ∑t (St + At ).
As a starting point, note that the model and data place bounds on the values
of p and b. We must have p ≥ S1 + S2 because breakthrough, and thus publication,
requires a good idea. We also know that b > 0, because if b = 0, then there would be no
breakthroughs, and therefore no publication. Similarly, if A2 > 0, then we must have
b < 1, because the marginal benefits of revision are strictly negative when b = 1 and
σ1 = 0. These bounds suggest, at an intuitive level, that p is identified by variation
in the total share of published projects, while b reflects variation in the timing of the
two outcomes (though timing, of course, also depends on costs).
Without further assumptions, we can say no more about the values of the structural parameters. Specifically, we can prove that:
Theorem 1. For any b ≥ b, there exists a unique p(b) and a unique value of F (1, 1)
and F (1, 0), both functions of b, such that for all t, the probability of publication
(abandonment) in period t equals St (At ).
Proof. For any choice of p and b, we can solve for N11 (the share of projects with a
breakthrough at t = 0, observed at the outset of stage t = 1) in two ways. First, our
assumptions about the learning process imply that N11 = pb. And second, rational
expectations imply that

N11 = S1 + S2 −

p̂(1, 0)b
A2 ,
1 − p̂(1, 0)b

(19)

where the last term represents the number of breakthroughs at t = 1, given posteriors

28

p̂(1, 0) =

p(1−b)
1−pb .

Combining these two solutions for N11 yields the following condition:

β(p, b) ≡ pb +

p̂(1, 0)b
A2 = S1 + S2 .
1 − p̂(1, 0)b

(20)

Because β(p, b) is continuous for all p, b on the unit interval, and strictly increasing
in p, there is a unique p(b) that solves (20) for any b ≥ b, where b is a sharp lower
bound for b given by β(1, b) = S1 + S2 . This establishes the first part of the claim.
To see that for each b there is a unique couple {F (1, 1), F (1, 0)}, note that knowledge of Ntσ allows us to construct the conditional choice probabilities. Combining
this information with observed payoffs π̂(t), we have:

Pr(stop∣σ1 = 1) = S1 /N11 = 1 − G(π̂(2) − π̂(1) − F (1, 1))

(21)

Pr(stop∣σ1 = 0) = A1 /N10 = 1 − G(p̂(1, 0)bπ̂(2) − F (1, 0)).

(22)

Given a strictly monotone G(⋅) and p = p(b), these equations can be inverted to find
the unique solution for F (t, σt ) as a function of b and the data (At , St ).31

Q.E.D.

Theorem 1 says that for any admissible value of b, we can choose the other structural parameters in a manner that rationalizes the observed data. Thus, more information is needed to achieve point-identification. The cost functions are a natural
place to look for two reasons. First, we have placed no restrictions on the cost functions up to this point. And second, there are substantive arguments for assuming
pre-breakthrough and post-breakthrough equality of the cost functions. For example,
the costs of revision may primarily be opportunity costs. Alternatively, breakthroughs
may depend on the “fundamental” quality of the underlying idea, whereas revisions
– both before and after a breakthrough – improve other dimensions of quality, such
31

In Claim B.3, Appendix B, we provide the proof for any T ≥ 2.

29

as exposition and generality. These observations lead us to make:
Assumption 1. Revision costs are independent of σt , F (t, 1) = F (t, 0) = F (t),
which leads immediately to the following result:
Theorem 2. Let G(⋅) be the logistic cdf, and make Assumption 1. If p ≥ 1 − A21 , then
there exists a unique solution to Equations (20), (21), and (22).
Appendix B proves this theorem, and then gives the conditions under which the
result extends to the T -period model.32 We provide some intuition in what follows.
Solving Equations (21) and (22) for F and setting the two expressions equal (based
on Assumption 1) leads to the following condition:

bp̂(1, 0) +

log(

N10 −A1
N11 −S1
S1 ) − log( A1 )

π̂(2)

=

π̂(2) − π̂(1)
.
π̂(2)

(23)

The right side of this equation is a constant, and the conditions in Theorem 2 are
sufficient to establish that as b increases, the expression on the left side crosses

π̂(2)−π̂(1)
π̂(2)

from below exactly once. If b is large, however, the left side of (23) can be nonmonotonic, possibly leading to multiple solutions. The economic intuition is that
when b is large, absence of breakthrough leads to pessimism, causing players to quit.
But high costs can also lead to quitting. For sufficiently high b, there may be solutions
that exhibit higher b with lower p and F , and vice versa. But for all of the estimates
we report below, the parameter estimates are consistent with a unique solution.33
Moreover, while the model can be identified by assuming that costs are equal in a
single period, in practice we assume that Assumption 1 holds for all t and exploit the
32

Specifically, Theorem 2 corresponds to Claims B.4 and B.5. The general T -period case is considered in Claim B.6.
33
Although the sufficient condition in Theorem 2 is not satisfied by our estimates, the estimates do
satisfy a necessary condition provided in Appendix B. That condition requires that p ≥ (2b − 1)/b2 .

30

over-identifying restrictions for estimation.34
Although our model is identified under Assumption 1, some readers may prefer
more clarity regarding the specific source of variation that identifies the costs relative
to the learning parameters. This leads us to consider a second identification strategy
that exploits nonstandards-track RFCs as a unique institutional feature of the IETF
standardization process. Specifically, we estimate a series of models that make
Assumption 2. Nonstandards-track projects have the same cost function F (t) as
standards-track projects, but are always published (i.e., p = b = 1).
Given this second assumption, we can estimate F (b) in an initial step (relying
only on the nonstandards-track proposals), and then feed the resulting cost estimates into the likelihood function to estimate b and p. This approach has costs and
benefits relative to relying exclusively on Assumption 1. The primary benefit is clarifying the relationship between data and parameter estimates. Under Assumption 2,
the cost function is identified by the payoffs and distribution of stopping times for
nonstandards-track RFCs, while b and p are identified by the payoffs and stopping
times for the standards-track. The obvious cost is the use of additional (strong)
assumptions about the relationship between the two tracks.
On a substantive level, we think Assumption 2 is reasonable. Recall that we
interpret breakthrough as the arrival of a consensus. In Section 2, we described how
standards- and nonstandards-track proposals are very similar in terms of content and
publication process, but have rather different commercial implications. The similar
substance justifies our assumption that cost functions are equal across the two tracks
(e.g., see Figure 2). The different commercial implications justify our assumption that
p = b = 1 for nonstandards. Because nonstandards do not produce winners and losers,
34

In one robustness check of the empirical analysis we assume equality up to a constant, so
F (t, 1) = F (t, 0) + κ.

31

IETF participants have few incentives to delay or prevent their success, and consensus
is more or less automatic. For example, Figure 3 shows that some nonstandards-track
projects are published immediately, without revision.

3.6

Estimation

With data, model, and identification strategy in hand, we now consider estimation.
Our model is a finite-horizon non-stationary dynamic discrete choice problem. Previous studies have used simulation to evaluate the likelihood for such models (e.g.,
Pakes, 1986; Fernandez-Villaverde and Rubio-Ramirez, 2007). We have derived an
explicit solution that exploits observation of the “hidden” state σt at the time when
a proposal is either published or abandoned, along with the particularly simple form
of state-transitions in our model.
We make several parametric assumptions when estimating the model. In particular, we set T = 25, adopt a quadratic specification of revision costs F (t), and assume
that the structural cost shocks εt have a logistic distribution with a variance of 1.
Finally, we assume that expected payoffs (measured in patent citations) are log-linear
in the number of versions, as in Equation (1). Given these assumptions, estimation
proceeds as follows:
1. Estimate the log-linear patent citation model in Equation (1), and set π̂(t) equal
to the predicted values for RFCs published in the year 2000. Do this separately
for standards and nonstandards.
2. If using Assumption 2, then estimate F for the sample of nonstandards-track
RFCs, based on nonstandards-track estimates of π̂(t) from Step 1.
3. Iteratively search for values of (b, p, F ) that maximize the log-likelihood, where
each iteration consists of two steps:
32

(a) Starting in period T , recursively compute the sequence of cut-points {ε̄σt }Tt=1−1 ,
along with the associated probabilities Gσ (t) and continuation values V (t, σ).
(b) Form LL(b, p, F ), retaining estimates of F from Step 2 if using Assumption 2.
The first step in this procedure yields estimates of agents’ beliefs. That is, we
assume authors know π̂(t) and decide whether to revise a proposal given that knowledge. This leads to a question about what sample we should use to estimate beliefs. A
rational expectations assumption suggests using the realized distribution of citations
for projects in the estimation sample. On the other hand, if the author-teams have
adaptive expectations, we might use a sample of RFCs published before the project is
started (in principle, this sample could be different for each observation). In practice,
we find that our citation model estimates are relatively stable, and so use the realized
citation distribution for projects in the estimation sample below.
We refer to the approach that uses nonstandards-track RFCs and Assumption 2 as
the three-step estimator, and the approach that relies only on Assumption 1 (skipping
over Step 2) as the two-step estimator. Our main estimates are based on the two-step
approach, and for those models we compute 95% confidence intervals based on 1,000
bootstrap replications.

4

Empirical Results

This section presents and discusses parameter estimates. Our baseline specification
uses the two-step estimator for the full sample of projects submitted from 1996
through 2009.35 We perform a variety of robustness checks on the baseline model
35

Limiting our sample to projects initiated until (and including) 2009 avoids issues of rightcensoring. Eliminating all projects initiated before 1996, on the other hand, may introduce left-

33

using different sub-samples and identification assumptions. Finally, we estimate a series of models where the breakthrough arrival rate, b, is parameterized as a function of
observables in order to explore the relationship between author experience, team-size,
communication/attention, or commercial interest and the speed of learning.

4.1

Baseline Estimates

Table 2 presents the estimates for our baseline model and a variety of robustness
checks. The top panel reports the learning parameters b and p, and the bottom panel
reports the estimated costs of a revision (measured in patent citations) at t = 1, t = 10,
and t = 20. For each parameter, we report either a 95% bootstrapped confidence
interval [in brackets] or a standard error based on the estimated covariance matrix
(in parentheses). The first five columns employ a two-step estimation approach based
on Assumption 1. The last two columns are from a three-step estimator that adds
Assumption 2.
The results for our baseline model are in Column (1). Ex ante, IETF members
anticipate that three out of five projects have the potential to achieve a breakthrough,
based on our estimate of p = 0.59. Conditional on having a good idea, the estimated
breakthrough arrival rate is b = 0.17. The bottom panel shows that the implied costs
of revision are decreasing and convex. We interpret these fitted values as opportunity
costs, including the foregone value of closing a given project and submitting a first
version of another project. Our results then suggest that switching to another project
is less attractive as the number of revisions increases.36
censoring. However, the IETF was relatively small and young at that time, so our results can be
interpreted as applying to the “mature” IETF.
36
Figure 2 provides a possible explanation (outside our model) for the estimated shape of the cost
function. The graphs in the figure capture a decreasing rate of textual changes: later versions are
increasingly different from the initial draft, but develop in this way at a decreasing rate. If smaller
textual changes come at lower cost because less effort is exerted (if, for instance, later revisions are
primarily minor edits whereas earlier revisions reflect more substantive changes in the text), then

34

35

Full
Sample
(1)

-2.138
14,444

0.17
[0.16, 0.20]
0.59
[0.51, 0.73]
-3.145
3,168

0.35
[0.32, 0.37]
0.75
[0.65, 0.84]

WG
Sample
(2)

-2.070
14,444

0.10
(0.003)
0.59
(0.008)

Area
Specific
(3)

-2.175
14,444

0.20
(0.003)
0.49
(0.003)

Calendar
Time
(4)

2.35
[1.95, 3.25]
1.25
[1.12, 1.81]
0.51
[0.48, 0.65]

4.44
[3.19, 6.13]
1.44
[1.13, 2.06]
0.77
[0.63, 0.98]

1.34
(0.015)
1.08
(0.010)
0.46
(0.015)

2.72
(0.029)
0.63
(0.005)
0.37
(0.009)

2.20
(0.028)
0.58
(0.023)
0.07
(0.010)
0.59
(0.019)

-2.124
14,444

0.26
(0.006)
0.42
(0.007)

Phase
Specific
(5)

-1.189
1,647

3.10
[2.03, 4.06]
0.71
[0.56, 0.86]
0.67
[0.50, 0.85]

-2.383
14,444

0.30
[0.27, 0.34]
0.45
[0.28, 0.59]

Full
Sample
(6)

-1.074
764

2.18
[0.68, 3.73]
0.71
[0.48, 0.96]
0.52
[0.27, 0.83]

-5.711
3,168

0.35
[0.28, 0.99]
0.32
[0.30, 0.53]

WG
Sample
(7)

3-Step Estimator

Notes: This table presents ML estimates for the baseline model and results from a set of sensitivity analyses. In Columns (1) and (2), we report full-sample
and WG-sample results for our 2-step estimator (using the sample of standards-track projects). In Columns (3), (4), and (5), we report full-sample results for
our 2-step estimator for the model versions with area-specific patent citations, calendar time, and breakthrough-phase-specific costs. In Columns (6) and (7),
we report full-sample and WG-sample results for our 3-step estimator. For these latter models, the top panel presents estimates for b and p from Step 3 of
our estimation procedure (using data on standards-track projects), given the cost estimates from Step 2 in the bottom panel (using data on nonstandards-track
projects). Estimates for project values π̂(t) are the predicted values from the citation model in Equation (1). For the models in Columns (1), (2), (6), and (7),
we report in brackets the 95% confidence interval from 1,000 bootstrap replications. The reported coefficient is the coefficient from a single ML estimation on the
complete sample. For the other models, we report coefficient and standard errors (in parentheses) from a single ML estimation on the complete sample; standard
errors for cost estimates are calculated using the delta method. For the model in Column (5), the costs for t = 1, t = 10, and t = 20 are for the pre-breakthrough
phase; post-breakthrough costs are increased by the amount of the post-breakthrough cost shift κ.

log-likelihood/Project
Projects (nonstandards-track)

Post-Breakthrough Cost Shift κ

Costs at t = 20

Costs at t = 10

Costs at t = 1

Cost Estimates (Standards-Track Projects; Nonstandards-Track Projects in Models (6) and (7))

log-likelihood/Project
Projects (standards-track)

Quality Prior: p

Rate of Learning: b

Parameter Estimates b and p (Standards-Track Projects)

Specification

2-Step Estimator

Table 2: Baseline and Robustness Results

Figure 6: Hazard Rates – Data and Simulation
Abandon

0

0

.1

.1

Pr(Outcome | Version)
.2
.3

Pr(Outcome | Version)
.2
.3

.4

.4

Publish

0

5

10
ID Version Number

15

0

20

(a) IETF Data

5

10
ID Version Number

15

20

(b) Simulated Data

Notes: Panel (a) plots the hazard rates for the published (solid line) and abandoned (dashed line) projects (versions 1
through 20) for the IETF data sample of 14,444 standards-track projects. See also Figure 3. Panel (b) plots the
hazard rates for published and abandoned projects for a simulated sample of 14,444 projects, with the values for
parameters b, p, and F from the model in Column (1) of Table 2.

One way to understand the magnitude of these estimates is to consider the share of
good projects that actually get published, given the speed of the learning process and
the (stochastic) opportunity costs of continuing a line of research. For our baseline
parameter estimates, the “success rate” for good projects is 31%. We believe this
statistic provides some sense of the real economic costs of information discovery, and
the potential benefits from policies that increase the rate of learning.37
Figure 6 plots the hazard of publication and abandonment for the raw data (lefthand panel) and for a simulated data set based on the parameter estimates from our
baseline model (right-hand panel). Although the actual publication hazard exhibits
more steady growth between t = 1 and t = 13, the model can apparently capture the
basic features of the IETF data reasonably well.
the depicted pattern in Figure 2 comports with the estimated costs in Table 2.
37
Appendix Figure D.1 plots ex-ante project value (at the time of the initial draft and before the
first continuation decision is made) against b to illustrate the benefits of faster learning.

36

4.2

Robustness Analysis

Columns (2) through (7) in Table 2 present the results for additional models that relax
or change our baseline assumptions. The results generally show that estimates from
our preferred baseline model are consistent across alternative specifications and subsamples, at least to the degree of precision afforded by the data. Further robustness
checks and sensitivity analyses are reported in Appendix A.38

4.2.1

Subsample of Working Group Projects (“WG Sample”)

We begin by evaluating estimates for the sub-sample of IETF projects initiated within
a working group.39 While anyone can start a project outside of a working group, the
threshold for the working group to initiate a project is expected to be higher, leading
to the higher publication rates that we observed in Table 1.
Column (2) of Table 2 presents estimates for the WG sample. Both learning
parameters, b and p, increase relative to the full sample (though we cannot reject the
hypothesis that p is equal in the two samples). The estimated costs of revision are
initially higher than for the full sample, but decline faster, and achieve a comparable
level by t = 10. One possible explanation for higher point estimates of b and p is
that the additional attention and feedback from the working group leads to a faster
consensus.40
38

Figures A.1 and A.2 plot the implied hazard rates for additional models in Table 2. Columns (1),
(2), and (6) in Table A.2 correspond to Columns (1), (6), and (7) in Table 2. All models in Table A.2
use an extended sample (including RFCs initiated between 2010 and 2015) to estimate and predict
patent citations π̂(t). All but the first model in the table use the three-step estimation approach.
Comparisons (within Table A.2) of the alternative models in Columns (3) through (6) are therefore
with the model in Column (2) and Column (8) with Column (7). Further descriptions can be found
in the table notes.
39
For this sub-sample, we cannot reject the hypothesis that standards-track and nonstandardstrack projects have the same revision distance function (i.e., the lines in Figure 2 are identical). This
result supports the assumption that the two types of projects feature comparable fixed costs.
40
We provide some additional support for this explanation in Section 4.3 below, when we consider
the impact of communication (as a proxy for attention) on learning.

37

4.2.2

Heterogeneity of Project Values (“Area Specific”)

In Column (3) of Table 2, we introduce ex-ante heterogeneity in the expected payoffs.
That is, we continue to assume that players have shared expectations regarding π̂(t)
but allow those expectations to differ across the seven broad Technology Areas defined
by the IETF.41 To implement this idea, we add technology-area fixed effects to our
model of expected citations (Step 1), and use the area-specific estimates of π̂(t)
when evaluating the likelihood. The estimates in Column (3) of Table 2 show that
accounting for ex-ante technological heterogeneity in expected project values has a
mixed effect on the estimates for b and p. While the point estimates for b are lower
and outside the confidence interval in Column (1), the point estimates for p fall
well within the interval.42 Thus, adding technological heterogeneity in (rationally
expected) payoffs does not change the priors about project types but suggests that
learning is slower than indicated by our baseline model.

4.2.3

Definition of Time (“Calendar Time”)

Our baseline specification assumes that costs are determined by the number of revisions t, and does not include discounting for time. In reality, some teams submit
revisions faster than others. If this reflects variation in the amount of revision, so that
faster resubmissions imply less substantive change, our model would fail to measure
the costs of revision accurately. To address this concern, we estimate a version of
the model where the data are re-shaped so that t is measured in calendar-quarters
41

The IETF technology areas correspond roughly to the various layers in the engineering “protocol
stack” as described in Simcoe (2012). From top to bottom, those layers/areas are: Applications,
Realtime Applications and Infrastructure, Transport, Internet, and Routing. The IETF also recognizes two areas that cut across the various layers: Operations and Security. For more details on
technology areas, see the data appendix (in Appendix F).
42
We switch from bootstrap to ordinary confidence intervals in Columns (3)–(5), because adding
heterogeneity to the model raises the computational costs associated with the bootstrap computations.

38

instead of proposal revisions. We set T equal to 6 years (24 quarters) and assume
that a project is abandoned in the quarter of its last version if that revision is followed
by 2 years of inactivity.
Column (4) of Table 2 presents the results of the calendar-time model. In general, the p parameter should not vary with our choice of time-units, while b should
change. Because the average time between versions is roughly three months, however, we expect b to have a similar magnitude to the version-time results. In practice,
the calendar-time estimate of p = 0.49 falls just below the lower bound of the 95%
confidence interval in the baseline model, and the two estimates for b are statistically
indistinguishable.

4.2.4

Pre- and Post-Breakthrough Specific Costs (“Phase Specific”)

For the model in Column (5) of Table 2, we relax Assumption 1 by allowing the
pre- and post-breakthrough revision costs to differ by a constant κ, so that F (t, 1) =
F (t, 0)+κ. This specification is meant to address concerns that the cost of “polishing”
(post-breakthrough) is small or even negligible compared to the costs of “experimentation” (pre-breakthrough). Contrary to this hypothesis, our estimates suggest that
the costs of revision increase after a breakthrough, with κ = 0.59. In this specification, p is a bit lower than the baseline estimates, b is somewhat higher, and the costs
(though not directly comparable) appear similar.

4.2.5

Three-Step Estimation

The last two models in Table 2 present the results for our three-step estimation
approach. For this, we use Assumption 2 and estimate the costs (in Step 2) using
only data on nonstandards-track projects while setting p = b = 1. In Step 3, we take
the results from Step 2 and estimate b and p using data on standards-track projects.
39

We present bootstrapped 95% confidence intervals in brackets.
The bottom panel of the table shows that using the nonstandards-track RFCs to
estimate F (t) leads to a cost function that starts higher, and declines more steeply
than in our baseline model. Given this cost function, estimates of p are somewhat
smaller, though not statistically different, from our baseline model. The estimated
breakthrough arrival rate, however, almost doubles. Interestingly, using Assumption 2
also produces estimates of b and p that are more similar across the full and WG
samples, shown in Columns (6) and (7), respectively.

4.3

Learning, Fast and Slow

Until now, with the exception of the area-specific model in the third column of Table 2, we have treated projects as ex-ante identical. It is straightforward, however,
to incorporate heterogeneity into the model by specifying p, b, or F (t) as function
of observable characteristics. This sub-section introduces heterogeneity in b in order
to explore several factors that might influence the speed of learning by IETF participants. In the appendix, we report estimates of models that allow for heterogeneity in
both p and b.43 In those regressions, we find very little variation in p, which suggests
that the estimates we report here actually measure variation in the speed of learning,
as opposed to ex-ante selection of better ideas.
Table 3 reports parameter estimates and bootstrapped standard errors from a
series of models where b ≡ b(x), and x measures four different project and authorteam characteristics: communication, author experience, commerciality, and team
size. Communication is the number of emails per revision that specifically mention
a focal project. Commerciality is the share of those emails that originate from a
dot-com domain. Experience is the maximum number of prior IETF submissions or
43

See Table D.4.

40

Table 3: Non-Parametric Heterogeneity Results
Specification

Communication

(Explanatory
variable x)

(Emails)
(1)

Experience
(RFCs)
(2)

(Projects)
(3)

Commerciality

Team Size

(Suit-to-Beard)
(4)

(Authors)
(5)

Step 3: Parameter Estimates b and p (Standard-Track Projects)
b (Category 1)
b (Category 2)
b (Category 3)
b (Category 4)
p
loglikelihood/Project
Projects
(standards-track)

0.15
[0.08, 0.21]
0.29
[0.24, 0.34]
0.33
[0.31, 0.37]
0.34
[0.32, 0.39]

0.26
[0.20, 0.31]
0.31
[0.28, 0.35]
0.34
[0.32, 0.38]
0.36
[0.34, 0.42]

0.24
[0.17, 0.30]
0.27
[0.22, 0.32]
0.31
[0.29, 0.35]
0.35
[0.33, 0.39]

0.18
[0.10, 0.27]
0.35
[0.33, 0.38]
0.37
[0.35, 0.40]
0.23
[0.13, 0.32]

0.28
[0.25, 0.32]
0.30
[0.27, 0.34]
0.33
[0.31, 0.37]
0.33
[0.31, 0.38]

0.46
[0.30, 0.59]

0.45
[0.29, 0.59]

0.45
[0.29, 0.59]

0.48
[0.35, 0.61]

0.45
[0.28, 0.59]

-2.343

-2.393

-2.392

-2.715

-2.400

14,444

13,922

13,922

10,710

13,922

Notes: This table presents ML estimates for the three-step model in Column (6) in Table 2 with the rate of
learning b varying in x. The top panel presents estimates for b and p from the third step of our estimation
procedure (using data on standards-track projects), given the cost estimates from the second step (using data
on nonstandards-track projects): 3.10 [2.03,4.06] for t = 1, 0.71 [0.56,0.86] for t = 10, and 0.67 [0.50,0.85] for
t = 20. The rate of learning b is estimated for four different categories of projects: in (1), a value bi for each
quartile; in (2) and (3), b1 for projects x = 0, and b2 through b4 for each tercile of remaining projects; in (4), b1
for x = 0, b4 for x = 1, and b2 and b3 for projects below and above the median of remaining projects; in (5), bi for
x = {{1} , {2, 3} , {4, 5} , {6, . . . , 72}}. Estimates for project values π̂(t) are the predicted values from the citation
model in Equation (1). We report in brackets the 95% confidence interval from 1,000 bootstrap replications. The
reported coefficient is the coefficient from a single ML estimation on the complete sample.

publications by an author-team member, and team size is an author count. Note that
our proxies for experience and team size are pre-determined, whereas communication
and commerciality may be endogenous to the revision process because they are based
on emails sent during the life of the proposal. Appendix D provides more details on
how we construct each of the explanatory variables. In estimation, we allow the key
explanatory variables to enter non-parametrically by estimating a separate value of b
for proposals in each quartile of the distribution of x.
The estimates in Table 3 are broadly consistent with our prior expectations about
the learning process. In Columns (1) through (3) we see that both Communication

41

and author Experience are positively correlated with the breakthrough arrival rate.
For both of these variables, the effect also appears to be concave, with a sharp increase
in the speed of learning between with first and second quartile, and a somewhat slower
increase thereafter. This is consistent with a model where IETF “newbies” and ideas
that receive little or no attention learn quite slowly whether or not they have a good
idea. In Column (5), we observe that author-team size is essentially uncorrelated
with the rate of learning.
Perhaps the most interesting result in Table 3 is the non-monotonic invertedU relationship between the “Suit-to-Beard” commerciality measure and the speed of
learning. This finding suggests that the consensus arrival rate for good ideas is higher
when the discussion of a proposal on IETF listservs reflects a mix of commercial
(dot-com) and not-for-profit (dot-gov, dot-org, dot-edu) participants. Conversely,
consensus arrives more slowly if the conversation is dominated by a single type of
participant. The decline in b when moving from the third to the fourth quartile
of Commerciality is consistent with the results in Simcoe (2012), where time-topublication increases with the suit-to-beard ratio. He attributes that result to a
greater chance that conflicting interests emerge as the commercial stakes of a proposal
increase. But the increase in b between the first and second quartile of Commerciality
suggests that this is not the entire story – a moderate level of commercial interest
may lead to better feedback, or perhaps indicate that it is easier to break logjams
when there is a diverse mix of interests.

5

Counterfactual Analysis

We now consider two counterfactual experiments based on estimates from our baseline
model (i.e., the first column of Table 2). Our baseline estimates imply that around
42

two-thirds of the good ideas proposed to the IETF are abandoned. This hints that
there is a role for polices and institutions that could spur learning among IETF
participants. Thus, in the first experiment we compare two policies meant to stimulate
research: a subsidy that lowers the cost of revisions, and a prize for publication of an
RFC. The second experiment examines the costs of misaligned priors by simulating
an over-/under-confident team that believes p is above/below its true value.

5.1

Prizes and Subsidies

Subsidies and prizes are both used as innovation policy instruments.44 Examples
include the R&D tax credit and the patent system. Much of the theoretical literature
on optimal innovation rewards considers trade-offs between ex-ante incentive and
ex-post market power when the prize is a patent (e.g., Gilbert and Shapiro, 1990;
Hopenhayn et al., 2006). We use our model to study a different question: how
do prizes and subsidies influence the decision to continue exploring a risky idea?
Specifically, we simulate the impact of prizes and subsidies on both research output
and the ex-ante value of a project, holding fixed the total budget allocated to each
policy. This question is managerially relevant for firms that participate in the IETF,
given that some companies simply subsidize their engineers time where others provide
a reward for successful publication of an RFC.45
We model “prizes” as a small increase in publication payoffs, δp , so the benefits
of an RFC become π̂(t) + δp . Each project’s scrap value remains zero. We model
subsidies as a small decrease in revision costs, δs , leading to the new cost function
F (t) − δs . To compare the two policies, we vary either δp or δs (holding the other
44

We use the term prize for a contingent benefit paid on successful completion of a project. In
the literature, the term is also used for a reward paid to the winner of a contest (e.g., Murray et al.,
2012; Galasso et al., 2016).
45
Based on several private conversations with IETF participants.

43

0

.9997

Additional Citations (Per Project)
.05
.1
.15
.2

Net Value per Project (% of Baseline)
.9998
.9999

1

.25

Figure 7: Prizes and Subsidies

0

.02
Prize

.04
Budget (Per Project)
Subsidies

.06

.08
Budget

0

.005

.01
.015
Budget (% of Baseline)
Prize

(a) Additional Citations

.02

.025

Subsidies

(b) Project Value

Notes: This figure depicts results of our counterfactual analysis in which we introduce (1) a prize δp for a published
RFC and (2) cost subsidies (per version) δs . We simulate N = 14,444 projects (our estimation sample size) for varying
levels of a fixed budget, using parameter estimates from the baseline model in Column (1) of Table 2. Varying the
fixed budget, we determine the respective publication prize δp and the respective per-period subsidies δs such that
the total expenditure does not exceed the budget. Panel (a) plots the number of additional citations (per project)
stemming from the introduction of a publication prize (solid) or per-version subsidies (dashed), for all projects. On
the x-axis, we have the per-project budget. Panel (b) plots the total net value of projects, measured as expected
(ex-ante) per-project value as percentage of the baseline ex-ante project value (where “baseline” means without prize
or subsidies). On the x-axis, we have the budget as percentage of the baseline ex-ante value.

structural parameters constant), and simulate outcomes in a sample of N projects.
For each simulation, we calculate the total cost of the policy along with private costs
and benefits, all measured in terms of expected patent citations.
Figure 7 shows the results of our counterfactual experiment. Panel (a) plots the
increase in gross citations as a function of the policy’s expenditure. Panel (b) plots the
total expected value of a project (i.e., citations net of budget and private opportunity
costs, expressed as share of the pre-policy baseline) against the expenditure (also
expressed as a share of baseline).
From Panel (a) we observe that both prizes and subsidies lead to an increase in research output, with an elasticity larger than unity. Moreover, for a given budget, the
subsidy produces more new citations than the prize. This picture changes, however,
when we examine Panel (b), which shows that after accounting for researchers’ op-

44

portunity cost, subsidies produce a decline in the ex-ante value of a project.46 Prizes
smaller than 2% of the pre-policy expected value of an idea can increase the expected
value of a project (net of opportunity costs) but larger prizes do not.47
Why do the two policies produce such different results? Both prizes and subsidies
provide incentives for pre-breakthrough exploration. But the prize-based incentive
declines over time as researchers grow more pessimistic, while the subsidy-based incentive remains constant. After a breakthrough, prizes have no impact on the decision
to revise, whereas subsidies continue to encourage development of the idea. Thus,
while both policies provide “early” exploration incentives, only subsidies provide incentives for “late” exploration and further refinement. In this sense, the subsidy
creates a larger distortion than the prize. It explains why subsidies generate more
new citations in Panel (a), and also why the net benefits of a prize are greater in
Panel (b). While the early incentive can lead to more breakthroughs, much of the
late incentive is wasted on bad ideas and post-breakthrough refinements that would
not be pursued by a team that fully internalized the revision costs.48
46

As we show in Appendix E, the last result reflects the following considerations: in addition to a
positive effect on the increase in expected benefits, the subsidy produces a negative effect from the
additional costs borne by the team and a negative effect caused by an increase in the average cost
shock borne by the team. We find that, in absence of discounting, the first two forces cancel out,
but the third remains.
47
In Panel (b), the x-axis corresponds to the budget value in percentage of baseline project net
value. All budget values up to 0.37% of baseline value (or equivalently, prizes up to 2% of the ex-ante
expected value of an idea) increase the expected net value of a project relative to pre-policy values.
To clarify our exercise in Panel (b) and the ensuing discussion, consider the following example.
Suppose we have 100 projects, and the expected pre-policy value of a project is 10. Suppose the
absolute budget is 100. The budget (as a % of baseline values) is then 10%. If, for this budget (and
the respective prize), 25 of the 100 projects are published, then, with a total budget of 100, paid to
25 successful projects, the absolute prize is 4, that is, 40% of the pre-policy value of a project.
48
This result is similar in spirit to Akcigit et al. (2016), who are concerned with the cross-sectional
inefficiencies produced by a uniform (cross-industry) subsidy in terms of investment value and research allocation.

45

5.2

Misaligned Priors

A number of researchers have considered the impact of behavioral biases on innovation
outcomes. For instance, Allen (1966) suggests that engineers are prone to a type
of sunk-cost fallacy and become over-committed to a particular solution. On the
other hand, Galasso and Simcoe (2011) and Hirshleifer et al. (2012) find that proxies
for CEO over-confidence are positively correlated with corporate innovation. In our
model, all research teams have rational expectations and are doing as well as they
can. One reason to study learning, however, is that, in reality, no one knows precisely
how likely it is that they have a good idea. Our second counterfactual exercise allows
for this possibility and studies the cost of misaligned priors, which take two forms:
overconfidence and pessimism.
Suppose a team can have biased perceptions of their own skills in choosing or
developing new projects. Specifically, if p∗ is the true (estimated) probability of a
good idea, and p represents the team’s subjective beliefs, then an over-confident team
has p > p∗ while an under-confident (or pessimistic) team has p < p∗ . In our model,
over-confident researchers will pursue unpromising lines of research for too long, and
pessimistic teams will cut loose their good ideas too early. To see which behavior is
more costly, we vary p – holding p∗ and the other structural parameters constant at
estimated values – and use simulated choice to calculate the expected payoffs under
misaligned priors. The results are shown in Figure 8.
Panel (a) graphs the expected value of a new project as a function of the team’s
subjective priors. This function achieves its maximum at p = p∗ , as it must. The
interesting feature of Panel (a) is the strong asymmetry in the costs of under- versus
over-confidence. Pessimistic teams manage to capture much of a project’s expected
value, whereas over-confident teams fare much worse.

46

25

Figure 8: Over-Confidence and Pessimism

0

-4

5

Number of Versions
10
15

Ex Ante Value (Per Project)
-2
0

20

2

All Projects
Good Projects
Bad Projects
To Consensus

0

.2

.4
.6
Authors' Subjective Prior (p)

.8

1

0

(a) Individual Project Value

.2

.4
.6
Authors' Subjective Prior (p)

.8

1

(b) Project Length

Notes: This figure depicts results of our counterfactual analysis in which we vary the author-team’s subjective prior
belief p (given a true quality prior of p∗ , depicted by the vertical line). Graphs are based on N = 14,444 simulated
standards-track projects (for each p ∈ {0.01, 0.02, . . . , 0.99}) using parameter estimates from the baseline model in
Column (1) of Table 2. Panel (a) plots a project’s ex-ante value against the author-team’s p when the true value is
p∗ . Panel (b) plots the average project length and the average duration to breakthrough.

The intuition for the asymmetry in Panel (a) is made clear in Panel (b), which
plots time to both consensus and project completion as a function of p. At low values
of p, the average duration is increasing slowly for both good and bad ideas. When
p exceeds p∗ , average duration increases faster, but the expected duration for good
projects levels off because almost all good ideas have been harvested. The average
duration for bad ideas, on the other hand, continues to increase, eventually surpassing
the duration of good projects. In economic terms, pessimistic teams may miss a few
opportunities but can still do fine as long as breakthroughs arrive relatively quickly.
Over-confident teams are more likely to publish their good ideas but can also persist in
costly exploration of “dry wells.” In our model, for the parameter estimates obtained
using the IETF data, over-confidence is more costly.

47

6

Concluding Remarks

In this paper, we study a dynamic discrete choice model of innovation in which
researchers learn over time about the quality of their ideas. Our model combines a
two-armed bandit process wherein researchers face a trade-off between exploration
and exploitation, with a traditional optimal stopping problem where they compare
the marginal costs and benefits of refining their ideas. In this model, a single failure
does not immediately terminate a line of research, but it does make researchers more
pessimistic about the quality of their idea. The resulting process of “learning when
to quit” rationalizes the observation that many ideas are abandoned, even though all
must (by revealed preference) have a positive expected value when initially pursued.
We believe this framework could potentially be applied to study the micro-economics
of R&D management in a variety of settings.
We estimate the structural parameters of the model using a unique data set that
contains information on every revision of both successful and abandoned projects
submitted to the Internet Engineering Task Force (IETF), along with citations to
successful (published) standards that we use to calibrate the expected payoffs. In
this empirical context, learning is associated with discovery that a proposed standard
has achieved “consensus” within the IETF community, and will therefore be published
with the group’s official recommendation.
Our baseline estimates suggest that 59% of proposals are capable of generating
a consensus, and that within that population, about 17% of active projects learn
that they will succeed in a given period. While these estimates vary somewhat, they
are relatively robust to a wide range of alternative assumptions and measurement
strategies. At this rate of learning, the model implies that around 31% of the “good
ideas” (i.e., proposals that might lead to consensus) are ultimately published, while

48

over two-thirds are abandoned due to a combination of uncertainty and opportunity
cost.
We also use our model to study how several observable characteristics of proposals
and author-teams are associated with the speed of learning. We find that learning
is faster for more experienced teams, but unrelated to team size. Intuitively, author
teams learn faster when their proposals receive more attention on IETF listservs.
More interesti ngly, we find an inverted-U relationship between learning and the
commercial interest a proposal. On interpretation of the latter results is that a
moderate level of commercial interest can help with vetting of ideas while higher
levels signal competition that can hinder the arrival of consensus.
We use the data, model, and parameter estimates to perform a pair of counterfactual experiments. One counterfactual compares the innovation-promoting effects of a
prize that awards more citations to a successful project and a subsidy that lowers the
costs of research. We find that both schemes can produce a net increase in innovation,
but that, for a given budget, the subsidy produces a larger increase. However, if one
also accounts for the private costs, which include providing incentives for researchers
to continue pursuing low-value projects, only the prize might actually increase total welfare. The second counterfactual examines the costs of misaligned priors, and
shows that over-confidence would be more costly than pessimism in this setting.
To our knowledge, this is first paper to estimate a structural model of learning
in the context of R&D. In practice, however, standardization combines elements of
collaborative R&D with competition to benefit from adopting one’s own technology.
One direction for future research might be to move beyond our reduced-form characterization of the non-cooperative bargaining elements of standardization towards
a dynamic-game model. Another opportunity is to apply this type of framework
to other settings, such as scientific review and publication, or open-source software
49

development. A key challenge for that agenda will be finding other data sets that
contain information on both successful and abandoned ideas, as well as the underlying
revision process.

50

Appendix – For Online Publication
A

Additional Tables and Figures
Table A.1: Examples of IETF Internet Standards

RTP
SIP
HTTP
IPV6
DHCP
POP3
NAT
FTP
TCP
IP

Description

RFC

Year

Real-time Transport Protocol
Session Initiation Protocol
Hypertext Transfer Protocol – HTTP/1.1
Internet Protocol, Version 6 (IPv6) Specification
Dynamic Host Configuration Protocol
Post Office Protocol – Version 3
Network Address Translator
File Transfer Protocol
Transmission Control Protocol
Internet Protocol

3550
3261
2616
2460
2131
1939
1631
959
793
791

2003
2002
1999
1998
1997
1996
1994
1985
1981
1981

Notes: RFC means “Request for Comments,” Year is the year in which the standard was certified by the IETF.

51

0

0

Pr(Outcome | Version)
.1
.2

Abandon

Pr(Outcome | Version)
.1
.2

Publish

.3

.3

Figure A.1: Hazard Rates – Data and Simulation (More Results)

0

5

10
ID Version Number

15

20

0

10
ID Version Number

15

20

(b) Simulation: WG Sample

0

0

.1

.1

Pr(Outcome | Version)
.2
.3

Pr(Outcome | Version)
.2
.3

.4

.4

(a) IETF Data: WG Sample

5

0

5

10
ID Version Number

15

20

0

5

10
ID Version Number

15

20

(d) Simulation: Area Specific

0

0

.1

.1

Pr(Outcome | Version)
.2
.3

Pr(Outcome | Version)
.2
.3

.4

.4

(c) Simulation: Baseline

0

5

10
ID Version Number

15

0

20

(e) Simulation: Phase Specific (κ)

5

10
ID Version Number

15

20

(f) Simulation: 3-Step Estimator

Notes: Panel (a) plots the hazard rates for the published (solid line) and abandoned (dashed line) projects (versions
1 through 20) for the IETF data sample of 3,168 standards-track projects (WG sample). Panel (b) plots the hazard
rates for published and abandoned projects for a simulated sample of 3,168 projects, with the values for parameters b,
p, and F from the model in Column (2) of Table 2. Panels (c), (d), (e), and (f) plot the hazard rates from simulated
samples for the baseline model in Column (1) and the models in Columns (3), (5), and (6) in Table 2.

52

Abandon

0

0

.1

.1

Pr(Outcome | Version)
.2
.3

Pr(Outcome | Version)
.2
.3
.4

.4

Publish

.5

.5

Figure A.2: Hazard Rates – Data and Simulation (Calendar Time)

0

5

10
Quarters

15

20

0

(a) IETF Data

5

10
Quarters

15

20

(b) Simulated Data

Notes: Panel (a) plots the hazard rates for the published (solid line) and abandoned (dashed line) projects (quarters
1 through 20) for the IETF data sample of 14,444 standards-track projects. Panel (b) plots the hazard rates for
published and abandoned projects for a simulated sample of 14,444 projects, with the values for parameters b, p, and
F from the model in Column (4) of Table 2.

53

54

Baseline
2-Step
(1)
Baseline
3-Step
(2)

Only
Experimental
(3)

-2.111
14,444

log-likelihood/Project
Projects (standards-track)

-2.231
14,444

0.28
(0.004)
0.39
(0.004)

2.25
(0.026)
0.98
(0.007)
0.46
(0.012)

-1.176
1,647

2.58
(0.053)
0.63
(0.008)
0.59
(0.031)

-1.271
262

3.79
(0.151)
0.77
(0.019)
0.62
(0.051)

-2.242
14,444

0.33
(0.002)
0.54
(0.002)

-1.148
1,999

2.52
(0.050)
0.62
(0.008)
0.61
(0.029)

-1.992
21,779

0.31
(0.004)
0.39
(0.004)

Censored
Projects
(4)

-1.153
1,647

1.55
(0.047)
0.48
(0.009)
0.44
(0.029)

-2.106
14,444

0.30
(0.008)
0.29
(0.008)

RFC
Citations
(5)

-1.235
1,647

2.15
(0.037)
0.71
(0.007)
0.39
(0.008)
0.43
(0.024)

-2.274
14,444

0.23
(0.004)
0.37
(0.004)

Deadline
T = 50
(6)

-1.072
764

2.12
(0.081)
0.70
(0.015)
0.51
(0.043)

-3.533
3,168

0.34
(0.011)
0.43
(0.011)

WG
3-Step
(7)

-0.980
179

1.95
(0.175)
0.71
(0.031)
0.56
(0.106)

-3.508
226

0.25
(0.026)
0.44
(0.026)

First
Project
(8)

Notes: This table presents ML estimates for the baseline models in Column (1) (using the two-step estimation approach) and Columns (2) and (7) (using the
three-step estimation approach; for the full sample and the WG sample) and the results from further sensitivity analyses in Columns (3) through (6) and Column
(8). In (3) (“Only Experimental”), we use only Experimental RFCs for nonstandards-track projects; in (4) (“Censored Projects”), we include active projects and
those completed projects that were initiated in or after 2010; in (5) (“RFC Citations”), we use citations by other RFCs for project values estimates π̂(t); in (6)
(“Deadline T = 50”), we extend the forced deadline to T = 50; in (8) (“First Project”), we consider only working group projects that were the first project of
their respective working group. For models (2) through (8), the top panel presents estimates for b and p from the third step of our estimation procedure (using
data on standards-track projects), given the cost estimates from the second step in the bottom panel (using data on nonstandards-track projects). Estimates for
project values π̂(t) are based on the extended sample, including RFCs initiated in year 2010 or after. Standard errors for the parameter estimates are reported in
parentheses. Standard errors for cost estimates are calculated using the delta method.

log-likelihood/Project
Projects (nonstandards-track)

Costs at t = 30

Costs at t = 20

Costs at t = 10

Costs at t = 1

Step 2: Cost Estimates (Nonstandards-Track Projects)

Quality Prior: p

0.18
(0.004)
0.51
(0.004)

Rate of Learning: b

Step 3: Parameter Estimates b and p (Standards-Track Projects)

Specification

Table A.2: More Robustness Results

B

Model Identification

In this appendix, we show the conditions under which we achieve point-identification
of the parameters in our empirical model. We proceed in two steps. We first show
that, without further restrictions, the statistical features of the data, combined with
our model, are not enough to uniquely identify all our parameters. We then make an
assumption on the fixed costs that renders point-identification feasible.

B.1

No Further Restrictions

We consider a recursive solution of the empirical model with finite-time horizon T ,
given p ∈ (0, 1), b ∈ (0, 1), G(ε) strictly monotone, and π̂(t) > π̂(t − 1) and π̂(1) > 0.
Moreover, to ease notation, we let p̂(t) ≡ p̂(t, 0) represent posterior beliefs in period
t (before a breakthrough). The number of parameters to identify is 2 × T : p, b and
F (t, σt ), with t = 1, . . . , T − 1, and σt = 0, 1. We start by constructing the expressions
for the choice probabilities (of publication and abandonment) in each t. Then, we
analyze whether these expressions are sufficient to identify all the parameters in our
model.
For the construction of the choice probabilities, we compute the number of projects
in the pre- and post-breakthrough phase in each stage t of the model. We start with
the count of projects in the pre-breakthrough phase. Recall that we have normalized
the total number of projects to one so that Nt0 + Nt1 = 1, where Nt0 and Nt1 have been
defined in Section 3.5.
Claim B.1. The number of projects in the pre-breakthrough phase in any t ∈ [1, T − 1]
is given by
T −1

Aj+1
.
j
j=t ∏k=t (1 − p̂(k)b)

Nt0 = At + ∑

(B.1)

Proof. To derive (B.1), we proceed recursively, starting from T − 1:
1
1 − p̂(T − 1)b
1
= AT −2 + N0T −1
1 − p̂(T − 2)b
AT
AT −1
= AT −2 +
+
.
1 − p̂(T − 2)b [1 − p̂(T − 2)b] [1 − p̂(T − 1)b]

NT0 −1 = AT −1 + AT
NT0 −2

By iteration of this rule, we obtain Equation (B.1) in the claim.

Q.E.D.

We now define the number of projects in the post-breakthrough phase in each
stage t.
55

Claim B.2. The number of projects in the post-breakthrough phase in any t ∈ [1, T − 1]
is given by
T

T −2

0
Nt1 = ∑ Sj − ∑ Nj+1
j=t

j=t

p̂(T − 1)b
p̂(j)b
− AT
.
1 − p̂(j)b
1 − p̂(T − 1)b

(B.2)

Proof. For the derivation of (B.2), we proceed recursively, starting from T − 1:
p̂(T − 1)b
1 − p̂(T − 1)b
p̂(T − 2)b
= ST −2 + NT1 −1 − NT0 −1
1 − p̂(T − 2)b
p̂(T − 3)b
,
= ST −3 + NT1 −2 − NT0 −2
1 − p̂(T − 3)b

NT1 −1 = ST + ST −1 − AT
NT1 −2
NT1 −3

which, after rearranging, yields
T

T −2

0
NT1 −3 = ∑ Sj − ∑ Nj+1
j=T −3

j=T −3

p̂(j)b
p̂(T − 1)b
− AT
.
1 − p̂(j)b
1 − p̂(T − 1)b

By iteration of this process, we obtain Equation (B.2) in the claim.

Q.E.D.

The formulas for Ntσt , σt = 0, 1, allow us to construct the 2 (T − 1) expressions for
the conditional choice probabilities in each t:
St
= 1 − G(π̂(t + 1) − π̂(t) − F (t, 1))
Nt1
At
= 1 − G(p̂(t)bπ̂(t + 1) − F (t, 0)),
Nt0

(B.3)
(B.4)

with t = 1, . . . , T − 1. Given any guess for b and p and the strict monotonicity assumption on G(⋅), the expressions in (B.3) and (B.4) can be inverted to solve for the costs
in each t. That is, for each t, F (t, 1) and F (t, 0) are uniquely identified by (B.3) and
(B.4), yielding expressions that depend on p, b and data.
To explore identification of p and b, we start exploiting the additional condition
stemming from the property that, by construction, pb = N11 . We then use (B.2) to
compute the number of post-breakthrough phase projects in t = 1, so that
T

T −2

0
pb = N11 = ∑ Sj − ∑ Nj+1
j=1

j=1

p̂(j)b
p̂(T − 1)b
− AT
.
1 − p̂(j)b
1 − p̂(T − 1)b

(B.5)

Claim B.3. If b ≥ b ∈ (0, 1), there exists a unique function p(b) that satisfies Equation
56

(B.5), with p(b) ∈ (∑t St , 1) and t = 1, ..., T .
Proof. To prove the claim, we proceed by induction. First, we show the claim for
T = 2. For this case, let
β2 (p, b) ≡ pb +

p̂(1)b
A2 = S1 + S2 .
1 − p̂(1)b

(B.6)

The right-hand side takes values within the unit interval. At the left-hand side of the
equation, function β2 (p, b) is continuous for all p, b ∈ (0, 1). Moreover, it is strictly
increasing in p. This is because p̂(1)b/ (1 − p̂(1)b) is increasing in p̂(1), and p̂(1)
and pb are both increasing in p. Finally, β2 (p, 1) → p, β2 (0, b) → 0 and β2 (1, b) →
(A2 b) / (1 − b) + b, increasing in b, with
lim β2 (1, b) = 0 and
b→0

lim β2 (1, b) = ∞.
b→1

Thus, there exists a lower bound for b, b2 ∈ (0, 1), such that β2 (1, b) ≥ S1 + S2 for all
b ≥ b2 .
We now show that the claim holds for T = 3. More specifically, T = 3 implies that
N11 = S1 + S2 + S3 − A2

b [p̂(1) + p̂(2) − bp̂(1)p̂(2)]
p̂(1)b
− A3
,
1 − p̂(1)b
[1 − p̂(1)b] [1 − p̂(2)b]

so that
β3 (p, b) ≡ pb + A2

p̂(1)b
b [p̂(1) + p̂(2) − bp̂(1)p̂(2)] 3
+ A3
= ∑ St .
1 − p̂(1)b
[1 − p̂(1)b] [1 − p̂(2)b]
t=1

(B.7)

As for (B.6), the right-hand side takes values within the unit interval. At the lefthand side of the equation, β3 (p, b) is continuous for all p, b ∈ (0, 1). Function β3 (p, b)
is strictly increasing in p, because pb, p̂(1)b/ (1 − p̂(1)b), and
b [p̂(1) + p̂(2) − bp̂(1)p̂(2)] / [1 − p̂(1)b] [1 − p̂(2)b]
are all strictly increasing in p. Finally, β3 (p, 1) → p, β3 (0, b) → 0 and β3 (1, b) →
2
2
(A2 b)/ (1 − b) + b − A3 + A3 / (1 − b) = β2 (1, b) − A3 + A3 / (1 − b) . Hence, the same
conclusions as for T = 2 apply (i.e., there exists a lower bound for b, b3 ∈ (0, 1), such
that β3 (1, b) ≥ S1 + S2 + S3 for all b ≥ b3 ).
Reiterating this analysis, we can invoke the principle of induction to conclude
that there exists a lower bound for b, denoted by bT ≡ b ∈ (0, 1), such that βT (1, b) ≥
S1 + S2 + . . . + ST for all b ≥ b. Hence, provided b ≥ b, a unique value of p(b) exists,
with p(b) ∈ (∑t St , 1) and t = 1, ..., T .
Q.E.D.
The main implication of the claims above is that, while we can pin down the
per-period costs from the choice probabilities, we can only recover an implicit rela57

tionship between b and p, as given by p(b). Therefore, our structural parameters
(p, b, F (t, σt )) are not uniquely determined by model and data alone, and we need to
impose additional restrictions to separately identify b and p.

B.2

Additional Restriction for Identification

In line with the main text, the assumption we make is that, in t = 1, the per-period
costs are the same pre- and post-breakthrough:
Assumption B.1. Pre- and post-breakthrough revision costs are equal in t = 1:
F (1, 1) = F (1, 0) = F (1).
Then, in any t > 1, the costs in the pre- and post-breakthrough phases, denoted
by (F (t, 0), F (t, 1)), can be recovered by inverting (B.3) and (B.4). Moreover, by
Assumption B.1, we can obtain F (1) by inverting (B.3) evaluated in t = 1:
k1 ≡ G−1 (1 − S1 /N11 ) = π̂(2) − π̂(1) − F (1).

(B.8)

Then, from (B.4) evaluated at t = 1, we obtain
k2 ≡ G−1 (1 − A1 /N10 ) = bp̂(1)π̂(2) − F (1).
Substituting F (1) from (B.8), and rearranging, yields
bp̂(1) − (k2 − k1 ) =

π̂(2) − π̂(1)
.
π̂(2)

(B.9)

In the next claim, we provide the sufficient conditions such that a value of b(p) solving
(B.9) exists and is unique.
1
. If p ≥ p ≡ 1 − A21 , then there exists a unique value
Claim B.4. Let G(ε) = 1+exp(−ε)
of b(p) ∈ (S1 /p, (1 − A1 )/p) such that

bp̂(1) −

k2 − k1 π̂(2) − π̂(1)
=
,
π̂(2)
π̂(2)

(B.10)

with 0 < S1 /p < (1 − A1 )/p < 1 for all p ≥ p.
Proof. Under the Logistic distribution,
k1 = log (

N11 − S1
)
S1

and k2 = log (

N10 − A1
).
A1

(B.11)

Thus, both sides of (B.10) are continuous for all N11 − S1 = pb − S1 > 0, N10 − A1 =
1 − pb − A1 > 0 and 1 > p, b > 0. After using N11 = pb and N10 = 1 − pb, Equation (B.10)
58

can be rewritten as
α(p, b) ≡ bp̂(1) − [log (

1 − bp − A1
bp − S1
1
π̂(2) − π̂(1)
) − log (
)]
=
.
A1
S1
π̂(2)
π̂(2)

(B.12)

Under our assumption of a strictly increasing and positive function π̂(⋅), the righthand side is a scalar that (strictly) lies within the unit interval. Moreover,
lim α(p, b) = −∞ and

b→S1 /p

lim

b→(1−A1 )/p

α(p, b) = +∞.

Taken together, the continuity of α and its behavior in the limits imply that a solution exists for all b ∈ (S1 /p, (1 − A1 )/p). We now determine the sufficient condition
guaranteeing that such a solution is unique. Specifically, this boils down to studying
the monotonicity of α(p, b) with respect to b.
We then take the derivative of α(p, b) with respect to b and find that
∂α(p, b) p [1 − b (2 − bp)]
p (1 − A1 − S1 )
=
.
+
2
∂b
(1 − bp − A1 ) (bp − S1 ) π̂(2)
(1 − bp)
Because 1 − A1 − S1 > 0, 1 − pb = N10 > A1 and pb = N11 > S1 , the second term in
the expression for ∂α(p,b)
is strictly positive. Using this fact, we provide below the
∂b
> 0.
sufficient restrictions under which ∂α(p,b)
∂b
To begin with, we note that if the first term of ∂α(p,b)
is weakly positive, or
∂b
p ≥ (2b − 1)/b2 , then α(p, b) is strictly monotone in b, with (2b − 1)/b2 increasing in b.
Hence, if p is larger than this threshold evaluated at b → (1 − A1 )/p (the upper bound
for the values of b such that (B.10) is well-defined), it is larger for all relevant values of
b. Doing so, we find that p ≥ 1 − A21 is sufficient to guarantee that a solution to (B.10)
is unique.49 Moreover, p ≥ 1 − A21 implies that 0 < S1 /p < (1 − A1 )/p < 1.
Q.E.D.
This claim gives us the sufficient conditions for the existence of a unique b(p),
meaning that, thanks to Assumption B.1, point-identification can be achieved by
jointly solving for p and b from p = p(b) and b = b(p). To conclude the analysis, we
proceed as follows. First, we prove that, given Claim B.4, a unique (constrained)
solution exists for p and b when T = 2. That is, we obtain point-identification in
T = 2. Second, we give the additional condition under which this result on point49

Were this condition violated (so that p < (2b − 1)/b2 ), monotonicity would hold if and only if the
second term in ∂α(p,b)
were large enough. Since that term is strictly increasing in A1 , this boils down
∂b
to finding the lower bound on A1 such that the whole expression is strictly positive. We obtain:
2

∂α(p, b)
(1 − bp) (1 − S1 ) + (1 − bp) [1 − b (2 − pb)] (bp − S1 ) π̂(2)
≥ 0 ⇐⇒ A1 ≥ A1 ≡
,
2
∂b
(1 − bp) + [1 − b (2 − bp)] (bp − S1 ) π̂(2)
with A1 < (0, N10 ) for sufficiently large values of π̂(2). Hence, if p < (2b − 1)/b2 , α(p, b) is strictly
increasing in b for all A1 > A1 .

59

identification extends to the case of a general T . To close the model, the value of p
and b can then be plugged into the expressions for the fixed costs.
Model Solution for T = 2 We first show that a unique (constrained) solution
exists in this case.
1
Claim B.5. Let T = 2 and G(ε) = 1+exp(−ε)
. Then, there exists a unique couple
2
∗
∗
∗
∗
∗
∗
(b , p ) provided p > 1 − A1 , b ≥ b and p b ∈ (S1 , 1 − A1 ).

Proof. To begin with, we show two properties of p(b) (Claim B.3) that are useful for
what follows. Namely, we prove that p′ (b) < 0 and p′′ (b) > 0 under p > p.
After taking the total derivative of Equation (B.6) in the proof of Claim B.3, we
find that
∂β(p,b)

A2 [1−b(2−bp)]

∂p

[1−b(2−bp)]

p 1 + [1−b(2−bp)]2
dp
∂b
= p′ (b) ≡ − ∂β(p,b)
=−
<0
db
b 1 + A2 (1−b) 2
for all 1 − b(2 − bp) > 0 (which is implied by p > 1 − A21 , see the proof of Claim B.4).
Moreover, we find that
A2 {1 − b[2 − b(2 − p)]} + B 4 + A2 B{2 − b[2 + p(4 − bC)]}
dp′ (b)
= p′′ (b) ≡ p 2
> 0,
db
b2 [A2 (1 − b) + B 2 ]2
where B ≡ 1 − b(2 − b)p, C ≡ 9 + 3b2 p − 2b(3 + p), [A2 (1 − b) + B 2 ] > 0, and the sign of
the numerator is strictly positive under 1 − b(2 − bp) > 0, or p > 1 − A21 .
We now plug p(b) into Equation (B.12). We obtain
1 − bp(b) − A1
bp(b) − S1
1
π̂(2) − π̂(1)
bp(b)(1 − b)
− [log (
) − log (
)]
=
,
(1 − p(b)) + p(b)(1 − b)
A1
S1
π̂(2)
π̂(2)
where the left-hand side corresponds α(p(b), b) (see the proof of Claim B.4). Our
goal is to establish that this equation has a unique solution in b.
We now study the behavior of α(p(b), b) over the support of b. Recall from Claim
B.4 that function α is continuous for all N11 − S1 = p(b)b − S1 > 0 and N10 − A1 =
1 − p(b)b − A1 > 0. Moreover,
lim α(p(b), b) = −∞ and

bp(b)→S1

lim

bp(b)→(1−A1 )

α(p(b), b) = +∞,

where, since p(b)b is increasing in b, it can be inverted to compute the lower and
upper bounds on b such that bp(b) ∈ (S1 , 1 − A1 ). This then implies that a solution
exists for all b such that bp(b) ∈ (S1 , 1 − A1 ).

60

Finally, we take the derivative of α(p(b), b) with respect to b and find that
dα(p(b), b)
(1 − b)(p(b) + bp′ (b)) − bp(b) (1 − b)(bp(b))(p(b) + bp′ (b))
=
+
db
1 − bp(b)
(1 − bp(b))2
(1 − A1 − S1 )(p(b) + bp′ (b))
.
+
π̂(2)(1 − bp(b) − A1 )(bp(b) − S1 )
After solving for the value of p(b) such that β2 (p, b) = S1 + S2 , we plug it into dα(p(b),b)
db
to establish that α(p(b), b) is increasing for π̂(2) sufficiently small. Combined with
the analysis above that yields existence of a solution for b, it implies that the solution
is unique. We denote this solution by b∗ .
To obtain the corresponding value of p∗ , we plug b∗ into p(b) defined in Claim
B.3, and obtain p∗ provided b∗ ≥ b. Finally, the couple (p∗ , b∗ ) must satisfy p∗ b∗ ∈
(S1 , 1 − A1 ).
Q.E.D.
The claim proves that, given Claim B.4, the two-period model discussed in Section
3.5 is point-identified under constraints.
Model Solution for any T > 2 We now provide the sufficient conditions that allow
us to extend the result in Claim B.5 to a general T . This boils down to providing
the conditions such that a solution to the system of two equations in two unknowns
p = p(b) and b = b(p) exists and is unique. We do this in the following claim.
1
. First, if a solution (b∗ , p∗ ), with b∗ ≥ b
Claim B.6. Let T > 2 and G(ε) = 1+exp(−ε)
and p∗ b∗ ∈ (S1 , 1 − A1 ), exists, it is unique if p∗ > h(A1 , T ), where h(⋅, T ) is the lower
bound for p in period T . Let bmin ≡ arg minb g(b) and bmax ≡ max{b, arg maxb g(b)},
with g(⋅) ≡ b−1 (⋅). The duple (b∗ , p∗ ) exists if and only if the following conditions are
satisfied: if g(bmax ) < p(bmax )(resp. g(bmax ) > p(bmax )) then g(bmin ) > p(bmin ) (resp.
g(bmin ) < p(bmin )).

Proof. To establish uniqueness conditional on existence, it is sufficient to prove the
monotonicity of p(b) and b(p). We begin with b(p). Taking the total derivative of
Equation (B.12), we find that
∂α(p,b)
1−A1 −S1
1−b
db
b (1−bp)2 + (1−bp−A1 )(bp−S1 )π̂(2)
∂p
′
= b (p) ≡ − ∂α(p,b) = − 1−b(2−bp)
<0
1−A1 −S1
dp
p
2 +
∂b

(1−bp)

(1−bp−A1 )(bp−S1 )π̂(2)

for all 1 − b(2 − bp) > 0 (which is implied by p > 1 − A21 ).
As far as p(b) is concerned, we prove in Claim B.4 that, for T = 2, p′ (b) < 0
2
if p > 2b−1
b2 (which holds true for p > 1 − A1 ). For T = 3, similar calculations show
(3−b)b−2
(3−b)b−2
that p′ (b) < 0 for all p > b2 (3−2b) > 2b−1
b2 . Because b2 (3−2b) increases in b, plugging
√
b = (1 − A1 )/p yields p > h(A1 , 3) ≡ 14 (1 − A1 )(3 + 3A1 + 1 + A1 (2 + 9A1 )), with
61

p = 1 − A21 < h(A1 , 3) < 1. Proceeding analogously for T = 4, we find that p′ (b) < 0 if
p > h(A1 , 4) > h(A1 , 3). Thus, we can conclude that, for T > 2, there exists h(A1 , T )
such that b(p) is strictly monotone for all p > h(A1 , T ).
We now develop the conditions for existence in the second part of the claim. First
we note that, since b(p) is monotone, there exists g(⋅) ≡ b−1 (⋅). Then, we use the
following facts:
- Function p(b) is such that p(b) = 1 and p(1) = ∑Tt=1 St , with t = 1, ..., T .
- The support of function g(b) is given by (S1 /p, (1−A1 )/p). Moreover, 1 > g(b) >
p.
Given these properties, existence is guaranteed if p(b) and b(p) cross over their
domain, which holds true under the conditions in the claim.
Q.E.D.
The claim accomplishes two things:
1. It establishes the condition guaranteeing the monotonicity of functions p(b)
and b(p). Specifically, we define a lower bound for p, denoted by h(A1 , T ), with
h(A1 , T ) > h(A1 , T − 1) > ... > h(A1 , 3) > 1 − A21 = p, such that b(p) and p(b) are
both strictly decreasing. This implies that, if a solution (b∗ , p∗ ) exists, it is also
unique.
2. Monotonicity alone only gives us that p(b) and b(p) cross at most once; that is,
it is not sufficient to prove existence. Then, in the second part, the claim gives
the conditions under which the functions cross over their domain.

C

Likelihood Function with Censored Projects

We define a censored project in T as a project that has not been either abandoned
or published by version T . This means, the project has not experienced any high
cost shock, which would have implied the termination of the process. Moreover, a
censored project might have experienced a breakthrough in any τ < T (implying the
post-breakthrough phase in T ). We denote the status of a censored project by σ = ∅,
and the probability that a censored project reaches version T by Pr(T, σT = ∅). This
probability is equal to:
Pr(T, σT = ∅) =

Pr(T, σT = 1) Pr(T, σT = 0)
+
1 − G1 (T )
1 − G0 (T )

where Pr(T, σT = 1) in Equation (14) is the probability that a project is published in
T (with status σT = 1), and Pr(T, σT = 0) in Equation (16) is the probability that a
project is abandoned in T (with status σT = 0). Moreover, we have
LL(T, σT = ∅∣b, p, F ) = log(Pr(T, σT = ∅)).
62

We can then rewrite the log-likelihood of the data in Equation (18) as
N

LL(b, p, F ) = ∑ LL(Ti , σTi ∣b, p, F )

(C.1)

i=1

with index i for a given project i = 1, . . . , N that reaches version Ti with status
σTi ∈ {0, 1, ∅} in Ti . This expression for the log-likelihood accounts for all censored
projects.

D

Heterogeneity Results

In this section, we estimate the rate of learning b as a function of additional project
and author-team characteristics: communication, experience (two versions), commerciality, and team size. For each of these, we estimate four different values for the rate
of learning, bi with i = 1, 2, 3, 4, for four different categories of the explanatory variable.
For our results in this section, we use the three-step estimator. For the estimation
of π̂(t) in the first step, we rely on a restricted sample (excluding RFCs initiated in
year 2010 or later) in Table D.2 (and the identical Table 3 in the main text) and
the the full sample of RFCs (including those initiated in or after the year 2010) for
Tables D.3 and D.4.50
Communication: We measure communication (or attention) using the average
number of email messages sent during the revision process (i.e., average number
of emails per version). We estimate the rate of learning for four categories: b1
for projects with average number of emails, x, at or below the 25th percentile;
b2 for projects with x above the 25th percentile and at or below the median; b3
for projects with x above the median and at or below the 75th percentile; b4 for
projects with x above the 75th percentile.
Experience: For the experience of an author-team, we construct two different
measures: a narrower one and a broader one). For the narrower, we count, for
each author in an author-team, the number of successfully published RFCs at
the time of the initial draft of a given project (max RFCs). The experience of
the author-team is then the experience of the most successful author. For the
broader, we count, for each author in an author-team, the number of completed
projects (successful or failed) at the time of the initial draft of a given project
50
For all results in Tables 2 and 3 in the main text, we use a restricted sample to estimate
patent citations π̂(t), excluding RFCs initiated in year 2010 or later. This is a consequence of our
bootstrapping approach, where we estimate the patent citations π̂(t) for each of the 1,000 random
samples. Because we restrict our ML estimation sample to projects initiated until 2009, later RFCs
are excluded for the estimation of patent citations. For the results in this appendix, we do not
face this restriction (we do not bootstrap standard errors) and therefore use an extended sample to
estimate patent citations.

63

(max Projects). The experience of the author-team is then the experience of
the most prolific author. We estimate the rate of learning for four categories:
b1 for projects with x = 0 and b2 through b4 for projects by the terciles of the
remaining projects (x > 0);
Commerciality: We measure commerciality as the share of corporate email
addresses from which emails in response to different versions of a project draft
are sent. This “suit-to-beard” ratio is meant to proxy the commercial interest
in the project. We estimate the rate of learning for four categories: b1 for x = 0,
b4 for x = 1, and b2 and b3 for projects with a suit-to-beard ratio below and
above the median of the remaining projects (x ∈ (0, 1)).
Team Size: We use the number of authors of the initial draft of a project
capture the size of an author-team. We estimate the rate of learning for four
different author-team sizes: b1 for author-teams with a single author, b2 for
projects with 2 or 3 authors, b3 for projects with 4 or 5 authors, and b4 for
projects with 6 or more authors.
In Table D.1, we provide summary statistics of our explanatory variables. We also
give the count of projects in each of the four categories. The proxies for experience
and team size are pre-determined, that is, exogenous to the revision process of a given
project. The proxies for communication and commerciality, on the other hand, are
endogenous to the revision process. They are both based on the number of emails
sent in response to a version of the project during the revision process. For the
results below, we first use these contemporaneous measures. We then also use an
alternative measure that is less plagued by endogeneity concerns, but available only
for the working group sample: the average number of emails sent in response to a
version of all other projects submitted to the same working group of a given project
during that project’s lifetime.
The results allow us to analyze how communication, authors’ experience, commerciality of a project, and the team size affect the process of learning and the prospects
for a project to experience a breakthrough. We show that, while heterogeneity gives
rise to differential patterns in terms of authors’ rate of learning, the qualitative features of our results remain similar to the ones discussed in the baseline analysis of
Section 4.

D.1

Communication

For our first set of heterogeneity results, we ask how project-related communication
drives learning. We let b vary with the amount of attention and feedback a project receives. We hypothesize that more attention (via more project-related communication)
is associated with a higher learning rate (i.e., higher values for b).

64

Table D.1: Summary Statistics: Explanatory Variables
Category
Explanatory variable x
Communication (Emails)
Full Sample
WG Sample
WG Sample (Exogenous)
Experience (max RFCs)
Experience (max Projects)
Commerciality (Suit-to-Beard)
Team Size (Author Count)

N

Mean

SD

Min

Max

1

2

3

4

14,444
3,168
3,086
13,922
13,922
10,710
13,922

4.99
5.20
4.66
3.82
14.80
0.78
2.25

9.65
8.08
3.35
8.13
26.45
0.27
1.85

0
0
0
0
0
0.00
5,935

292
188
26.6
68
254
1
72

3,571
717
772
7,081
2,885
626
5,935

3,647
865
771
2,645
3,720
4,751
5,750

3,614
794
771
1,984
3,567
851
1,708

3,612
792
772
2,212
3,750
4,482
529

Notes: This table provides summary statistics for the explanatory variables x used for the heterogeneity results
in Tables D.2, D.3, and D.4. The numbers are for the sample of standards-track projects only. The table reports
the number of observations (N), mean, standard deviation (SD), minimum and maximum value, as well as the
number of observations (i.e., project proposals) for each of the four categories. For communication, categories are
by quartile; for experience, category 1 is for x = 0, and categories 2 through 4 by tercile of remaining projects; for
commerciality, category 1 is for x = 0, category 4 for x = 1, and categories 2 and 3 are projects below and above the
median of remaining projects; for team size, categories are for x = {{1} , {2, 3} , {4, 5} , {6, . . . , 72}}. The difference
in the number of observations is the result of missing author information for 522 standards-track projects and no
emails for 3,734 projects.

The results for communication in Column (1) of Table D.2 show that more communication (and attention) increases the estimated value for b. The resulting differential
effects of communication on the rate of learning are as follows: the conditional probability of experiencing a breakthrough in any period t increases from b = 0.12 in the
lowest category to b = 0.34 in the highest category. This means that, compared to
projects with few emails, communication and attention increase the breakthrough
arrival rate by close to 200%, leading teams to more quickly abandon those projects
without a breakthrough. The reason for this is that a higher value of b induces faster
updating of beliefs, and players become pessimistic more rapidly. At the same time,
a breakthrough, if experienced, arrives faster, which results in higher continuation
value in the pre-breakthrough phase. We plot the ex-ante value of a project (before
the initial draft is observed) in Figure D.1 below.
Sound internal communication is said to be important for the effective functioning
of organizations (Arrow, 1974). Empirical evidence on the link between communication and productivity in organizations, however, is scant. Among the few exceptions
are Palacios-Huerta and Prat (2012), who use email exchange to study the relationship between communication and the importance of the members of an organization;
Bloom et al. (2014), who show that the introduction of intranet changes the extent of
delegation within an organization; and Battiston et al. (2017), who study the impact
of face-to-face communication on productivity. We contribute to this literature by
showing that communication is directly linked to the rate of learning in a research
organization.
Our measure of communication (the emails sent during the development of the
project) is likely to be plagued by endogeneity concerns. We argue that communication and attention affect the learning process and the development of the project.
65

Table D.2: Non-Parametric Heterogeneity Results
Specification

Communication

(Explanatory
variable x)

(Emails)
(1)

Experience
(RFCs)
(2)

(Projects)
(3)

Commerciality

Team Size

(Suit-to-Beard)
(4)

(Authors)
(5)

Step 3: Parameter Estimates b and p (Standard-Track Projects)
b (Category 1)
b (Category 2)
b (Category 3)
b (Category 4)
p
loglikelihood/Project
Projects
(standards-track)

0.15
[0.08, 0.21]
0.29
[0.24, 0.34]
0.33
[0.31, 0.37]
0.34
[0.32, 0.39]

0.26
[0.20, 0.31]
0.31
[0.28, 0.35]
0.34
[0.32, 0.38]
0.36
[0.34, 0.42]

0.24
[0.17, 0.30]
0.27
[0.22, 0.32]
0.31
[0.29, 0.35]
0.35
[0.33, 0.39]

0.18
[0.10, 0.27]
0.35
[0.33, 0.38]
0.37
[0.35, 0.40]
0.23
[0.13, 0.32]

0.28
[0.25, 0.32]
0.30
[0.27, 0.34]
0.33
[0.31, 0.37]
0.33
[0.31, 0.38]

0.46
[0.30, 0.59]

0.45
[0.29, 0.59]

0.45
[0.29, 0.59]

0.48
[0.35, 0.61]

0.45
[0.28, 0.59]

-2.343

-2.393

-2.392

-2.715

-2.400

14,444

13,922

13,922

10,710

13,922

Notes: This table presents ML estimates for the three-step model in Column (6) in Table 2 with the rate of
learning b varying in x. The top panel presents estimates for b and p from the third step of our estimation
procedure (using data on standards-track projects), given the cost estimates from the second step (using data
on nonstandards-track projects): 3.10 [2.03,4.06] for t = 1, 0.71 [0.56,0.86] for t = 10, and 0.67 [0.50,0.85] for
t = 20. The rate of learning b is estimated for four different categories of projects: in (1), a value bi for each
quartile; in (2) and (3), b1 for projects x = 0, and b2 through b4 for each tercile of remaining projects; in (4), b1
for x = 0, b4 for x = 1, and b2 and b3 for projects below and above the median of remaining projects; in (5), bi for
x = {{1} , {2, 3} , {4, 5} , {6, . . . , 72}}. Estimates for project values π̂(t) are the predicted values from the citation
model in Equation (1). We report in brackets the 95% confidence interval from 1,000 bootstrap replications. The
reported coefficient is the coefficient from a single ML estimation on the complete sample.

In return, we would expect an evolving project to influence communication and attention. As an alternative measure for communication for a given project i, we use
the number of emails that were sent within the working group, during the lifetime
of project i, but exclude emails that pertain to the specific project i. Assuming
that there are no communication spillovers (or little) between projects, this approach
allows us to treat communication as exogenous to project i itself. This measure,
however, can be constructed only for the working group sample. We provide the parameter estimates of this alternative approach in Table D.3. We find that the basic
pattern observed in Column (1) of Table D.2 survive.

D.2

Experience

Table D.1 provides a glimpse at the distribution of the measures of author experience. One half of the author-teams of standards-track projects have authors that,
at the time they submitted the initial draft of a given project, had not successfully
66

Table D.3: Non-Parametric Heterogeneity Results – WG Emails
Specification (Explanatory variable x)

Communication
(Full)
(1)

Communication
(WG)
(6)

Communication
(within WG)
(7)

Step 3: Parameter Estimates b and p (Standards-Track Projects)
b (Category 1)

0.12
(0.003)
0.27
(0.006)
0.33
(0.007)
0.34
(0.007)

0.15
(0.007)
0.24
(0.015)
0.37
(0.015)
0.39
(0.020)

0.20
(0.010)
0.38
(0.023)
0.35
(0.018)
0.36
(0.018)

p

0.41
(0.002)

0.46
(0.004)

0.45
(0.004)

log-likelihood/Project
Projects (standards-track)

-2.184
14,444

-3.496
3,168

-3.513
3,086

b (Category 2)
b (Category 3)
b (Category 4)

Notes: This table presents ML estimates for the three-step model in Column (7) in Table 2 with the rate of
learning b(x) varying in x. We reproduce Column (1) from Table D.2. Column (6) is the baseline model with
varying b(x) for the working group sample. For Column (7), we use emails sent within a working group (but
without those sent in response to the specific project) as communication measure. The difference in observations
stems from projects that are the only projects in a working group (at that time); these projects are dropped from
the sample for Column (7). The top panel presents estimates for b(x) and p from the third step of our estimation
procedure (using data on standards-track projects), given the cost estimates from the second step (using data
on nonstandards-track projects): 2.58 (0.053) for t = 1, 0.63 (0.008) for t = 10, and 0.59 (0.031) for t = 20 in
the full sample (with 1,647 observations) and 2.12 (0.081) for t = 1, 0.70 (0.015) for t = 10, and 0.51 (0.043) for
t = 20 in the WG sample (with 764 observations). The rate of learning b is estimated for four different categories
of projects, with bi the value for the i’s quartile of emails. Estimates for project values π̂(t) are based on the
extended sample, including RFCs initiated in year 2010 or after. Standard errors for the parameter estimates are
reported in parentheses. Standard errors for the cost estimates are calculated using the delta method.

completed a project (Category 1). The fact that such a large fraction of author-teams
is inexperienced is important for our estimation. It implies that for a large number
of projects the learning process is not tainted by repeat authors who have previously
figured out how to properly write to experience a breakthrough. This feature of the
data lends support to our model assumption that projects of these authors are not
correlated over time.
Columns (2) and (3) in Table D.2 summarize the results. We find that more experienced author-teams (both successful and prolific) exhibit a higher rate of learning
(with an increase of roughly 50% from the lowest to the highest category). This effect
might be the result of more prominent authors receiving more attention (Simcoe and
Waguespack, 2011) or simply accumulated knowledge about how to identify useful
comments and suggestions and incorporate them into a revision.

67

0

Ex Ante Value (Per Project)
2
4

6

Figure D.1: Ex-Ante Project Value (Varying with Rate of Learning b)

0

.2

.4
.6
Rate of Learning b

.8

1

Notes: This figure depicts the ex-ante project value (in t = 0 before authors are endowed with their initial version) as
a function of the rate of learning b. We plot the ex-ante value for the parameter estimates from the three-step model
in Column (2) of Table A.2. The vertical line depicts b∗ = 0.283.

D.3

Commerciality

The results in Column (4) of Table D.2 consider heterogeneity in the rate of learning
that varies with the commerciality (or “suit-to-beard” ratio) of a proposal. We find
a non-monotonic relationship between commerciality and the learning parameter b.51
The decline in b that we observe when moving from Category 2/3 to Category 4
is consistent with the findings in Simcoe (2012), where commerciality is linked to
slower standards production.52 For these results, we can interpret b as a reducedform parameter that captures slower compromise when commercial participants have
stronger vested interests in the outcome of the standardization process. The low value
of b for proposals at the minimum of commerciality is likely due to confounding with
communication. Specifically, projects that generate very few emails are more likely
to exhibit extreme values of the “suit-to-beard” measure, and these have lower values
of b as seen in Column (1).
51

This pattern is robust to a variety of different versions of the suit-to-beard ratio.
In that study, commercial interest was measured at the WG level, although supplemental results
included estimates based on a proposal-level measure of commerciality similar to the one used here.
52

68

D.4

Team Size

We find that larger author-teams have a higher rate of learning b. To interpret these
results we offer the following possible explanations. The first is that more authors
write better drafts because of more combined knowledge and skills (especially in the
presence of complementarities in skills), and thus experience a breakthrough faster.
This argument relies on insights that are similar to those offered to interpret the
results in columns (2) and (3), and is consistent with the evidence in Hamilton et al.
(2003) that more heterogeneous teams are more productive than other teams with
the same ability.
The second explanation is more mechanical, and relies on the idea that more
authors cover a larger spectrum/make up a larger part of the community and thus
experience a breakthrough easier (take 6 members: 1 author has to convince 5 others;
5 authors have to convince only 1 other member). A third explanation is that authors
write better drafts because of combined effort. Accordingly, for a given draft quality,
each author exerts less effort and incurs less cost; more authors can then exert more
effort at still lower cost and write a better draft. However, this argument relies on
agents’ effort choice, which is outside of our model.

D.5

Heterogeneity and Projects’ Ex-Ante Quality

For the results in Table D.2, we keep the value for the quality prior p fixed. In other
words, while allowing the rate of learning b to vary with communication, experience,
commerciality, and team size, we assume that the ex-ante quality of a project is a
constant p regardless of the explanatory variable x. We extend this approach by
allowing p(x) to vary in x alongside b(x) and report the results in Table D.4. We
find that the variation in b(x) is not the consequence of a constant value of p. In fact,
letting p(x) vary in x reveals that the point estimates for the rate of learning show
a stronger response to our explanatory variables than the estimates for the quality
prior. For author-team experience, for example, this result can be interpreted as
follows: while more experienced authors initiate better projects (with a higher exante quality), the effect of their experience on receiving consensus is stronger than
their skills in picking good projects.

69

Table D.4: Non-Parametric Heterogeneity Results (Varying b and p)
Specification (Variable x) Communication
(Emails)
(1)

Experience
(RFCs)
(2)

(Projects)
(3)

Commerciality

Team Size

(Suit-to-Beard)
(4)

(Authors)
(5)

Step 3: Parameter Estimates b and p (Standards-Track Projects)
b (Category 1)
b (Category 2)
b (Category 3)
b (Category 4)
p (Category 1)
p (Category 2)
p (Category 3)
p (Category 4)
log-likelihood/Project
Projects (standards-track)

0.16
(0.009)
0.29
(0.006)
0.33
(0.007)
0.34
(0.008)

0.25
(0.005)
0.29
(0.008)
0.32
(0.010)
0.36
(0.010)

0.23
(0.007)
0.26
(0.007)
0.30
(0.007)
0.34
(0.007)

0.22
(0.018)
0.34
(0.005)
0.38
(0.012)
0.28
(0.007)

0.27
(0.006)
0.29
(0.006)
0.32
(0.010)
0.32
(0.018)

0.34
(0.010)
0.40
(0.003)
0.42
(0.003)
0.42
(0.003)

0.38
(0.003)
0.40
(0.004)
0.41
(0.004)
0.42
(0.004)

0.38
(0.005)
0.39
(0.004)
0.40
(0.003)
0.41
(0.003)

0.36
(0.013)
0.46
(0.002)
0.52
(0.005)
0.36
(0.004)

0.39
(0.003)
0.40
(0.003)
0.40
(0.004)
0.41
(0.008)

-2.181
14,444

-2.237
13,922

-2.237
13,922

-2.483
10,710

-2.248
13,922

Notes: This table presents ML estimates for the three-step model in Column (2) in Table A.2 with both the rate
of learning b(x) and the quality prior p(x) varying in x. The top panel presents estimates for b(x) and p(x) from
the third step of our estimation procedure (using data on standards-track projects), given the cost estimates from
the second step (using data on nonstandards-track projects): 2.58 (0.053) for t = 1, 0.63 (0.008) for t = 10, and
0.59 (0.032) for t = 20. Parameters b and p are estimated for four different categories of projects: in (1), a value
bi , pi for each quartile; in (2) and (3), b1 , p1 for projects x = 0, and b2 , p2 through b4 , p4 for each tercile of
remaining projects; in (4), b1 , p1 for x = 0, b4 , p4 for x = 1, and b2 , p2 and b3 , p3 for projects below and above the
median of remaining projects; in (5), bi , pi for x = {{1} , {2, 3} , {4, 5} , {6, . . . , 72}}. Estimates for project values
π̂(t) are based on the extended sample, including RFCs initiated in year 2010 or after. Standard errors for the
parameter estimates are reported in parentheses. Standard errors for the cost estimates are calculated using the
delta method.

70

E

Counterfactual Analysis

In this section, we show that the subsidy has a negative impact on the ex-ante values of
projects as computed after deducting the subsidy transferred to the team and the cost
borne by the team. We consider the two-period example without post-breakthrough
phase, so that the project is published upon receiving consensus.53
Let the subsidy be denoted by s, then the net expected value of the project
(including the paid subsidy) to the team is
V (s) = pb [π̂(1) + (1 − b)G(ε̄)π̂(2)] + (1 − pb) G(ε̄) [−F + s − E[ε∣ε ≤ ε̄]] ,
with 0 < s < F and
ε̄ ≡ bp̂(1, 0)π̂(2) − F + s,

p̂(1, 0) =

p (1 − b)
.
1 − pb

The first term in V (s) corresponds to the expected benefits of a breakthrough, as
given by the first period payoff, and the second period payoff conditional on the
team deciding to continue in the first period. These terms are multiplied by the
total probability that a project is good, and that breakthrough happens in one of
the two periods. The expression in the second bracket corresponds to the expected
opportunity costs of revising the project, multiplied by the probability that a project
is not good, or a good project does not receive a breakthrough in any of the periods.
We further use Ṽ (s) ≡ V (s) − (1 − pb) G(ε)s to denote the net expected value of
the project to the organization as a whole (when it internalizes the team’s costs and
accounts for the cost of the subsidy). It is equal to the team’s value net of the direct
transfer flowing from the organization paying the subsidy to the team of authors.
This expression can be rewritten as
Ṽ (s) = pb [π̂(1) + (1 − b) G(ε̄)π̂(2)] + (1 − pb) G(ε̄) [−F − E[ε∣ε ≤ ε̄]] .
Note that, by construction, Ṽ (s) = V (0), that is, without a subsidy, the team’s and
organization’s value coincide. We are interested in establishing the sign of Ṽ (s)−V (0),
that is, the organization’s value added from subsidy s.
Given that V (0) is independent of the subsidy, the derivative of Ṽ (s) − V (0) with
53

As we discuss below, introducing the post-breakthrough phase does not change the results in
this appendix. The same holds with the analysis featuring T > 2. In both cases, the qualitative
nature of the conclusions remain, at the cost of making the calculations cumbersome.

71

respect to s is

̃ (s)−V (0)]
∂[V
∂s

=

̃ (s)
∂V
∂s

with

∂G(ε)
∂ Ṽ (s)
[pb (1 − b) π̂(2) − (1 − pb) [F + E[ε∣ε ≤ ε]] ]
=
∂s
∂s
∂E[ε∣ε ≤ ε]
(E.1)
− (1 − pb) G(ε)
∂s
∂G(ε)
∂E[ε∣ε ≤ ε]
= (1 − pb) [
[p̂(1, 0)bπ̂(2) − F − E[ε∣ε ≤ ε]] − G(ε)
].
∂s
∂s
Plugging
∂G(ε)
∂ε
= g(ε)
∂s
∂s

and

∂E[ε∣ε ≤ ε] ∂ε g(ε)
=
[ε − E[ε∣ε ≤ ε]]
∂s
∂s G(ε)

into the expression in Equation (E.1), we obtain
∂ε
∂ Ṽ (s)
= (1 − pb) g(ε) [p̂(1)bπ̂(2) − F − ε]
∂s
∂s
∂ε
= − (1 − pb) g(ε) s < 0.
∂s

(E.2)

The second equality follows from the definition of ε, and the inequality in Equation
∂ε
> 0. Together with Ṽ (0) − V (0) = 0, this property of Ṽ (s)
(E.2) follows from ∂s
implies that the net ex-ante value of the project to the organization falls with any
value of the subsidy.
This result reflects the following considerations: in addition to a positive effect
on the increase in expected benefits, the subsidy produces a negative effect from the
additional costs borne by the team and a negative effect caused by an increase in the
average cost shock borne by the team. We find that the first two forces cancel out,
∂ε
term in Equation (E.2).54
but the third remains—as given by the g(ε) ∂s

F
F.1

Data Appendix
Construction of ID Sample

Our primary data source is the online archives of the IETF, which contain the full
text for each version of every Internet Draft (ID) submitted after July 1990, along
with various pieces of bibliographic information.
54

Note that adding the post-breakthrough phase is likely to reinforce the result. As we discuss in
the main text, in the post-breakthrough phase, due to absence of discounting, the team’s dynamic
incentive to continue is only driven by the marginal increase in the payoffs, but the negative effects
remain.

72

We begin by obtaining a list of all IETF IDs from the Internet Draft Status
Summary file at
https://tools.ietf.org/id/all_id.txt.55
IDs in this Status Summary file are identified by a string of hyphenated alpha-numeric
characters. For example, the ID name for RFC 7368 (“IPv6 Home Networking Architecture Principles”) is
draft-ietf-homenet-arch.
All IDs begin with draft; the string ietf indicates that this particular ID is a working
group ID (see below); the remaining pieces identify a specific proposal. Different
versions of an ID are indexed by a suffix -00 for the initial version and -NN for the
NN’s revision. The Status Summary file contains a list of the ID-version strings for
the last version of an ID. For example, for RFC 7368, this last ID was draft-ietfhomenet-arch-17 (where -17 indicates the 17th revision of the initial proposal or
the overall 18th version).
The total number of IDs on the list is 28,627. For each, we download the text
document of all versions from
http://tools.ietf.org/id/[ID-version].txt
where [ID-version] is the respective ID-version combination. For example, for RFC
7368, we know the first version is draft-ietf-homenet-arch-00, and the last version
is draft-ietf-homenet-arch-17. If available, we download the files from the first
to the last version. Moreover, because the Status Summary file contains the date
only for the last ID-version, we parse the history information for a given ID at http:
//datatracker.ietf.org/doc/[ID]/history (with [ID] the ID name without the
version numbers) to obtain the online publication dates for each version. These are
the dates a given version is posted to the online repository. Especially for older IDs,
this date is not always the date the version was finished and circulated. We manually
clean the date information in two steps. First, assuming that a version t + 1 is not
published before a version t, we find the set of versions for which the time difference
between t + 1 and t is negative (and version t + 1 has a date before version t). We
then manually inspect the draft document to find the correct dates of publication.
If a complete publication date is not provided, we use the statutory expiration date
(six months after the ID version is posted; if listed in the document) to backtrack the
publication date; if the day of the month of the ID version is not given, we use the
first of the month (or the date of previous ID version, whichever comes later); if no
date is provided in the draft document, we use the date of the previous ID version.
IDs go through various stages of development and change their ID name along
the way. For instance, an ID may be initiated by an individual IETF contributor
55

Downloaded on September 10, 2015. This date marks our data cut-off.

73

and is later adopted by a working group (WG). Our example ID (RFC 7368) is such
a case. Its original ID name was draft-arkko-townsley-homenet-arch (with one
version). It was replaced (i.e., superseded) by ID draft-chown-homenet-arch (with
two versions), before being adopted by the homenet WG as ID draft-ietf-homenetarch (with 18 versions).

draft-arkko-townsley-homenet-arch

draft-chown-homenet-arch

draft-ietf-homenet-arch

RFC 7368

The Status Summary file contains the status of each ID, including information on
the ID name that replaced a given ID. We use this information to link such chains of
IDs to create a single project. In many cases, a subsequent ID supersedes an “expired”
ID. As a statutory rule of the IETF, an unpublished ID expires after six months of
inactivity (i.e., no submission of a new version), leading to its removal from “active”
status. Authors, however, can reactivate and resume an expired ID. In our data, we
see numerous cases where more than six months have passed between two versions of
a given ID (see below for how we handle such cases). Likewise, we see numerous cases
where more than six months have passed between two subsequent IDs in a project
chain. For the construction of our project chains, we implement the following rule
(referred to as the “24-months rule”). If less than 24 months have passed between the
last version of an ID i and the first version of its successor (the ID i + 1 that replaces
the previous i in the chain), then we link the two IDs to obtain a single project chain.
Instead, if more than 24 months have passed between the last version of an ID i and
the first version of its successor i + 1, we delink the two IDs. The first chain ends with
the last version of the predecessor ID i, the second chain begins with the first version
of the successor ID i + 1. We thus delink 32 project chains.
After linking IDs, our sample contains 25,532 project chains with 96,770 versions.
The longest chain links five IDs; 10 chains link four IDs; 205 chains link three IDs;
2,098 chains link two IDs, and 23,218 chains comprise only one ID.
We have applied our 24-months rule to delink chains if the gap between two
presumably related IDs—recall, we obtain the linking information from the IETF’s
Status Summary file—is too big. Moreover, for many projects, we see considerable
74

gaps between two versions of the same ID. Gaps longer than six months are in violation
of the IETF’s statutory rule for expiration, but is fairly common. We therefore take a
conservative approach when determining the de-facto status of a project chain. First,
if there is a gap of more than 24 months within a project chain (i.e., within an ID
of a given project chain), we drop all versions after the gap (a total of 1,780 versions
in 451 chains). Second, project chains that have a version that was published less
than 24 months before our data cut-off date (September 10, 2015) and that are not
published as an RFC are considered “active.” We thus get three different outcomes
for our project chains:
Published: A project chain is “published” if it is published as an RFC or in
the final stages (i.e., in the “Queue” but not yet assigned an RFC number); the
full sample contains 5,834 published project chains.
Abandoned: A project chain is “abandoned” if it is not published as an RFC
and no new version has been posted after September 10, 2013; the full sample
contains 16,038 abandoned project chains.
Active: A project chain is “active” if it is not published as an RFC and a
new version has been posted after September 10, 2013; the full sample contains
3,660 active project chains.
For the first version in each project chain, we take the first version for which
we have a document. Thus, by construction, we have all initial versions for each
project chain. We further miss 432 documents for later version numbers. For versionlevel information that we obtain from the ID documents (authors, text distance), we
interpolate to account for these missing values.
After constructing our raw sample, we take the following steps to construct our
estimation sample:
Step 1: Drop 456 project chains from six special technology areas. We provide
more details in Section F.2.3 below.
Step 2: Drop 325 non-IETF projects: These projects are 19 IESG projects (“Internet Engineering Steering Committee”), 3 IANA projects (“Internet Assigned
Numbers Authority”), 193 IRTF projects (“Internet Research Task Force”), and
110 IAB projects (“Internet Architecture Board”).
Step 3: Drop 444 project chains with specialized RFCs. We provide more
details in Section F.2.2 below.
Step 4: Drop 529 project chains initiated before 1996.
Step 5: Drop 3,526 active project chains (that means, active projects following
the 24-months rule).
75

Step 6: Drop 4,161 completed (published or abandoned) project chains initiated in 2010 or later. We thus minimize selection on outcomes by dropping
completed projects initiated in 2010 or later. Out of the 3,526 projects active
at the cut-off date, and 3,479 (98.7%) projects were initiated in 2010 or later.
Table F.1: Sampling
Step

Step
Step
Step
Step
Step
Step

Description

Projects

Versions

Full Sample (after applying the 24-months rule)

25,532

94,990

1 Drop six areas
2 Drop special non-IETF series
3 Drop special tracks
4 Drop projects initiated before 1996
5 Drop active projects
6 Drop completed projects initiated in or after 2010

25,076
24,751
24,307
23,778
20,252
16,091

92,249
90,845
88,187
86,757
73,654
57,179

16,091

57,179

Final Estimation Sample

We summarize these steps in Table F.1. Our final sample consists of 16,091 project
chains (3,857 published; 12,234 abandoned) with 57,179 versions. This is the sample
used for all results but those presented in Column (4) in Table A.2. For this latter set
of results, we use the information from right-censored observations. In other words,
we do not drop active project chains (Step 5). Moreover, because by including active
project chains we are not selecting on outcomes when keeping all projects initiated
in 2010 or later, we do not drop this set of recent projects (Step 6). This extended
sample consists of 23,778 project chains (4,781 published; 15,471 abandoned; 3,526
active) with 86,757 versions.

F.2
F.2.1

Further Information on IDs
Working Group

The file-naming convention of the IETF allows us to identify a project chain’s relevant
working group. IDs with names that begin with draft-ietf are IDs from a working
group, and the working group name is the third part of the name. For our example
project chain (RFC 7368), the third (and last) ID is from the homenet working
group. We construct a subsample of all project chains that are initiated by a working
group and refer to it as the working group sample (or WG sample). A project chain
is in the WG sample if its first ID (in the chain) is from a working group. The
WG sample consists of 3,932 project chains (2,206 published; 1,726 abandoned) with
22,025 versions. The project chains in this sample are initiated by 285 different

76

working groups (both active and closed) and (at some point in time) part of 315
different working groups.
F.2.2

Standards-Track and Nonstandards-Track

For the distinction of standards-track and nonstandards-track project chains, we use
information on the “status” of published RFCs. We first access the “RFC Index” at
http://www.rfc-editor.org/rfc-index.html to obtain information on the title,
the publication date, the status, area, and the working group of published RFCs. An
RFC’s status holds the information we need to identify a project chain’s track. An
RFC can be of one of a number of statuses: “Best Current Practice,” “Draft Standard,” “Experimental,” “Historic,” “Informational,” “Internet Standard,” “Proposed
Standard,” and “Unknown.” We obtain three track categories for RFCs:
No Track: An RFC with status equal to “Best Current Practice” (184 projects)
“Draft Standard” (104 projects) “Historic” (111 projects), “Internet Standard”
(45 projects) or “Unknown.” In Step 3 above (see Table F.1), we drop these
projects from our sample.
Nonstandards Track: An RFC with status equal to “Experimental” or “Informational.”
Standards Track: All other RFCs.
To link RFC-level information to our sample of project chains, we access metadata
for each RFC through
http://datatracker.ietf.org/doc/rfc[NNNN]
where [NNNN] is the four-digit RFC number. We parse the website to pull the last ID
name that lead to the published RFC. We then match our RFC-level information with
our project-chain information. For each published project chain, we thus obtain track
information through its RFC status. We further assume that all other project chains
(i.e., those that are abandoned in our final estimation sample) are standards-track
project chains.
Our final estimation sample consists of 14,444 project chains on the standards
track (2,210 published; 12,234 abandoned) and 1,647 project chains on the nonstandards track. Out of these nonstandards-track projects, 1,385 are “Informational” and
262 are “Experimental.”
F.2.3

Areas

We obtain area information via two separate avenues. First, working groups are assigned to specific technology areas. We download information for area-working group
assignments for active working groups from https://datatracker.ietf.org/wg and
77

https://tools.ietf.org/area (these lists are overlapping) and for concluded working groups from https://datatracker.ietf.org/group/concluded and https://
tools.ietf.org/wg/concluded (these lists are overlapping). We can thus assign an
area to each project chain that has at some point been adopted by a working group.
If a project chain has multiple areas, we choose the last area the project chain was
assigned to.
For the non-working group sample (i.e., individual projects), we obtain some
area information from http://datatracker.ietf.org/doc/[ID] (with [ID] the ID
name): for 1,020 ID names, the information on Document Type contains the line
“individual in [A] area” where [A] is an area identifier. This means, an individual
ID (outside a working group) has been assigned to an area. In our estimation sample,
we thus obtain area information for 712 additional project chains.
In Step 1 above, we have dropped projects from six areas from the sample: 246
projects (1,256 versions) from the gen area (“General Area”), 160 projects (1,308
versions) from the art area (“Applications and Real-Time Area”), 35 projects (134
version) from the usr area, 3 projects (11 versions) from the irtf area (“Internet
Research Task Force”), 1 project (6 versions) from the adm area, and 11 projects (26
versions) from the ora area. Moreover, we combine 60 projects from the sub area with
the rtg area (“Routing Area”) and the 53 projects from the mgt area with the ops
area (“Operations and Management Area”). After further applying Steps 2 through
6, we arrive at the following area count in Table F.2:
Table F.2: Areas
Projects
Area Description
app
rai

Applications Area
Real-Time Applications and Infrastructure Area
tsv
Transport Area
int
Internet Area
rtg/sub Routing Area
ops/mgt Operations and Management Area
sec
Security Area
Projects with area information
Projects without area information

Versions

Published

Abandoned

492
496

411
204

5,173
5,583

419
599
498
423
447

190
388
305
287
324

4,272
6,121
5,713
4,447
4,403

3,374
483

2,109
10,125

35,667
21,512

The IETF technology areas correspond roughly to the various layers in the engineering “protocol stack” as described in Simcoe (2012). From top to bottom, those
layers/areas are: Applications (app), Realtime Applications and Infrastructure (rai),
Transport (tsv), Internet (int), and Routing (rtg/sub). The IETF also recognizes two
78

areas that cut across the various layers: Operations (ops/mgt) and Security (sec).
For the full sample, we have area information for 7,948 project chains. For our
estimation sample, we have area information for 5,483 project chains.
F.2.4

Author Information

We use Jari Arkko’s authorstats tool, available at https://www.arkko.com/tools/
authorstats.html, to parse ID documents and extract author names, affiliations,
and email addresses. The goal is to identify authors and their project history at the
IETF.
After running the authorstats tool on all documents of our full sample, we
conduct some simple cleaning procedures. Using Python’s name-entity-recognition
libraries, we drop entries (for authors) in the output file that do not represent names.
We further extract, where possible, author names from email addresses to confirm
author names.
We then use two auxiliary data sets to further clean and complete the author
information. First, we use the author names and organizations of authors on published
RFCs. The information is manually collected (Simcoe, 2012) and serves as a reliable
source. We merge the RFC author data with our project chain data to correct and
complete the name information for project chains associated with an RFC. Second,
we use the attendance lists of IETF meetings (meeting 29 in April 1994 through
meeting 93 in July 2015) to construct a list of IETF contributors/members that
serve as the list of potential authors. For most of the meetings, we obtain author
names, affiliations, and email addresses. Email address information allows us to fill
gaps in the author information for project chains. In a last step, we conduct basic
author-name disambiguation to obtain unique author identifiers.
Using unique author-name identifiers for project chains, we obtain an author’s
project history at the IETF. For our measure of team size for a project chain, we
count the number of authors on the first version of the project. We further construct
two measures of author experience. First, for a given project chain, we count an
author’s number of successfully completed project chains (i.e., published as RFCs)
before that project chain is initiated. At the project-chain level, we then use the
experience of the most successful author of the author team (if multiple authors are
listed) of the initial draft. Second, for a given project chain, we count an author’s
number of completed project chains (i.e., published or abandoned) before that project
chain is initiated. At the project-chain level, we then use the experience of the most
prolific author of the author team (if multiple authors are listed) of the initial draft.
Because we have, by construction, all first version documents, author information
is missing for project chains only if the authorstats tool is not able to parse the
document.

79

F.2.5

Patent Citations

We use the number of citations from U.S. patents as a proxy for commercial impact.
In U.S. patents, IETF RFCs or IDs are cited in the non-patent literature (NPL)
references. We obtain the list of non-patent literature references for all U.S. utility
patents granted between 1976 and February 2016 from http://www.patentsview.
org/download/ (file: otherreference.zip). We count the number of citations of a given
project chain from patents in three steps:
Step 1: Using regular expressions that help us take typos and formatting errors
into account, we search the NPL references for entries in the form of ID names
(e.g., draft-ietf-homenet-arch). We pre-sample and use for the search only
NPL references that include the terms “IETF,” “internet,” or “draft” (we use
regular expressions to account for differences in capitalization). We use the
full sample of ID names (from all project chains) as our search sample. When
possible, we also extract the version number that is cited.
Step 2: Using regular expressions that help us take typos and formatting
errors into account, we search the NPL references for entries in the form of
RFC numbers (e.g., “RFC 7368”). We pre-sample and use for the search only
NPL references that include the terms “RFC,” “IETF,” “internet,” or “draft”
(we use regular expressions to account for differences in capitalization).
Step 3: As part of the initial download of ID and RFC metadata, we also
obtain the titles of all IDs and RFCs. We search the NPL references for these
titles, using only titles that have at least four words and 10 characters. We
pre-sample and use for the search only NPL references that include the terms
“IETF,” “internet,” “RFC,” “Request For Comment(s),” or “draft” (we use
regular expressions to account for differences in capitalization).
These three approaches result in overlapping lists of citing patents for a given project
chain. We therefore tally the number of unique patents that cite the given project
chain in its NPL references. Our approach (counting not only citations to RFC numbers but also including ID identifiers) allows us to capture citations from U.S. patents
that were made while the project chain was still active (before publication).
The project chain-level data we obtain from the above procedure is the basis
for our patent citation estimates π̂(t). For our main results in Table 2, we use the
patent citations for successful project chains (published RFCs) in the estimation
sample to estimate π̂(t). Because for the results presented in all other tables, we do
not bootstrap standard errors, we are not restricted to the estimation sample when
estimating π̂(t). For all other results, we therefore use the extended sample of RFCs
(before taking Step 6 in Table F.1) and the respective patent citations to estimate
π̂(t).

80

F.2.6

RFC Citations

In addition to patent citations, we also count the number of RFC citations each
project chain receives. Each RFC contains a list of references that typically contains
several citations to previously published RFCs. We download the RFC documents
via http://www.rfc-editor.org/rfc/rfc[NNNN].txt (where [NNNN] is the RFC
number)56 and scrape the documents to obtain a list of each citation. We construct
a count of RFC citations that is equal to the number of times a given RFC is cited
by some other RFC published at a later date. We then link the citations for a given
RFC with our project-chain data. Unlike the data for patent citations, our numbers
for RFC citations do not include citations to the unpublished RFC (while the project
chain was still active).
As with patent citations, we think of RFC citations as an indication that the
citing references build on the ideas contained in the cited references, so that citations
serve as a measure of the cited RFC’s technological importance in a cumulative innovation context. Simcoe (2012) shows that RFC and U.S. patent citations are highly
correlated. For our updated data (for both RFC citations and U.S. patent citations),
the correlation coefficient is 0.61.
F.2.7

Text Distances

The goal is to calculate the textual distance (or, dissimilarity) of an ID version t
from the initial version. For project-level information, we use the distance of the
last version, t = T , from the initial version, t = 1. We use R text-analysis libraries
to load and preprocess the documents. Preprocessing steps are: drop punctuation,
drop stop words, drop numbers, convert all text to lower case, and eliminate excessive
white space. We then create document-term matrices. Each row i of such a matrix
represents a document, and each column j represents a term in the corpus (the
collection of all documents). The value in cell (i, j) is the number of times a term j
is used in document i. A document-term matrix gives us vector-space representation
of a document i, where a document is a vector xi = (xi,1 , . . . , xi,n ) of term frequencies,
with n unique terms in the corpus.
As our measure of textual distance (or dissimilarity) dist(T, 1) between document
t = T and t = 1 we use the cosine dissimilarity (we refer to it as cosine distance).
More specifically, we calculate 1 − cosine similarity, where the latter is the cosine of
the angle between two term-frequency vectors xT and x1 , so that the cosine distance
is
¿
Án 2
xT ⋅ x1
À∑ x .
with ∣∣xi ∣∣ = Á
(F.1)
dist(T, 1) ≡ 1 −
i,l
∣∣xT ∣∣ ∣∣x1 ∣∣
l=1
56

Our data sample is as of June 2017.

81

F.3

Emails

We use emails exchanged among IETF contributors (authors and members of the
community who provide feedback) to construct two variables:
Email Count: The number of emails sent in response to a given version t of
a project chain. At the version level it is, for version t, the number of emails
sent in response to the previous version t − 1. At the project chain-level level
it is, for project chain i, the average number of emails sent in response to any
version of i.
Commerciality: The suit-to-beard ratio as the share of corporate email addresses (from which emails are sent) over all email addresses.
We construct our email data in a number of steps.
Step 1: We first download the “IETF Mail Archive” and the “Concluded WG
IETF Mail Archive” via ftp://ftp.ietf.org/.57 These email archives contain
all emails sent to various IETF email lists. We obtain the emails in monthly
batch files. We use a Python script to first split the monthly batch files into
individual emails.
Step 2: We extract the email address from which an email was sent and the
date and time it was sent.
Step 3: In order to assign a given email to a given version of a project chain,
we search for each ID name (from the complete list of all ID names) in the text
of that email. Any email that mentions an ID name is considered to be sent
in response to that ID. We also extract the version number of that listed ID
name (if possible). The email archive provides us with emails from all email
lists, including organizational lists and the i-d-announce lists (in its various
forms: before 1998, between 1998 and 2004, and after 2004). A new version of
an ID is announced to the community at large via this email list. It provides a
very clean account of the date the version is posted (with an occasional delay
of a day or two relative to the posted date listed on the draft). We do not use
ID announcement (or other organizational emails) for our email counts, but use
the date information (for each ID version) to fill missing version numbers in the
next step.
Step 4: If a version number (for the email assignment) is not available, we take
two approaches to fill the gap. First, if an email j without an assigned ID version
number was sent between two emails that do have the same version number,
we use the ID version number of the two embracing emails. Second, if an email
57

Our email data is as of October 17, 2015.

82

j without an assigned ID version number was sent between two consecutive
ID announcement emails, we use the earlier email’s ID version number as the
assigned ID version number for email j.
Step 5: We delete all ID announcement emails and other organizational emails
so as to not include them in email counts. These emails do not contain any
information pertinent to the content of the ID.
Step 6: In a last step, we delete all duplicate entries (where the same email is
assigned to the same version multiple times). This does not mean that we delete
duplicate emails, because an email j can be assigned to two or more project
chains. Such an email enters our email database multiple times. We also delete
all emails sent before the first ID version was posted and all emails sent more
than 24 months after the last ID version was posted.
We have at least one email for 20,532 project chains out of 25,532 project chains
in the full sample (a total of 645,064 emails); and at least one email for 12,396 project
chains out of 16,091 project chains in the estimation sample. This number amounts
to 331,094 emails in the estimation sample.
The emails in the full sample are sent from 20,968 unique addresses (from 7,533
domains). The earliest email in the full sample is from July 6, 1990. The last email
is from October 17, 2015. The emails in the estimation sample are sent from 15,404
email addresses (from 5,775 domains). The earliest email in the estimation sample
is from January 4, 1996, the last email from October 4, 2015 (assigned to a project
chain that was published as RFC).
To construct the suit-to-beard ratio, we categorize email addresses as corporate if
their top-level domain (TLD) is com, net, or biz. If the TLD is a country domain,
we use .co. as an identifier for corporate email addresses. In order to avoid counting
email addresses from email provider services, we compile a list of more than 3,000 free
(and for-cost) email providers. Many of these use, for instance, com as their TLD.
We do not categorize such email addresses as corporate.
The suit-to-beard ratio for a given project chain is the number of corporate email
addresses from which the project chain’s assigned emails are sent, divided by all email
addresses of that project chain. We have at least one email address (and are able to
construct the suit-to-beard ratio) for 10,710 standards-track projects (out of 14,444
standards-track projects in the estimation sample).

83

References
Akcigit, U., D. Hanley, and N. Serrano-Velarde (2016): “Back to Basics:
Basic Research Spillovers, Innovation Policy and Growth,” CEPR Discussion Paper
11707, Centre for Economic Policy Research, London, UK.
Akcigit, U. and Q. Liu (2016): “The Role of Information in Innovation and
Competition,” Journal of the European Economic Association, 14, 828–870.
Allen, T. J. (1966): “Studies of the Problem-Solving Process in Engineering Design,” IEEE Transactions on Engineering Management, 13, 72–83.
Arrow, K. J. (1974): The Limits of Organization, New York, N.Y.: W.W. Norton.
Augereau, A., S. Greenstein, and M. Rysman (2006): “Coordination vs. Differentiation in a Standards War: 56K Modems,” RAND Journal of Economics, 37,
887–909.
Baron, J., Y. Meniere, and T. Pohlmann (2014): “Standards, Consortia and
Innovation,” International Journal of Industrial Organization, 36, 22–35.
Baron, J., T. Pohlmann, and K. Blind (2016): “Essential Patents and Standard
Dynamics,” Research Policy, 45, 1762–1773.
Battiston, D., J. Blanes i Vidal, and T. Kirchmaier (2017): “Face-toFace Communication in Organisations,” London School of Economics, unpublished
manuscript.
Bell, A. M., R. Chetty, X. Jaravel, N. Petkova, and J. Van Reenen
(2017): “Who Becomes an Inventor in America? The Importance of Exposure to
Innovation,” NBER Working Paper 24062, National Bureau of Economic Research,
Cambridge, Mass.
Bhattacharya, V. (2017): “An Empirical Model of R&D Procurement Contests:
An Analysis of the DOD SBIR Program,” Massachusetts Institute of Technology,
unpublished manuscript.
Bloom, N., L. Garicano, R. Sadun, and J. V. Reenen (2014): “The Distinct Effects of Information Technology and Communication Technology on Firm
Organization,” Management Science, 60, 2859–2885.
Bloom, N., C. Jones, J. Van Reenen, and M. Webb (2017): “Are Ideas Getting
Harder to Find?” NBER Working Paper 23782, National Bureau of Economic
Research, Cambridge, Mass.
Bolton, P. and C. Harris (1999): “Strategic Experimentation,” Econometrica,
67, 349–374.
84

Bonatti, A. and J. Hörner (2011): “Collaborating,” American Economic Review,
101, 632–663.
Cabral, L. and D. Salant (2014): “Evolving Technologies and Standards Regulation,” International Journal of Industrial Organization, 36, 48–56.
Chan, T. Y. and B. H. Hamilton (2006): “Learning, Private Information, and the
Economic Evaluation of Randomized Experiments,” Journal of Political Economy,
114, 997–1040.
Ching, A. T., T. Erdem, and M. P. Keane (2013): “Learning Models: An
Assessment of Progress, Challenges, and New Developments,” Marketing Science,
32, 913–938.
——— (2017): “Empirical Models of Learning Dynamics: A Survey of Recent Developments,” in Handbook of Marketing Decision Models, ed. by B. Wierenga and
R. van der Lans, New York, N.Y.: Springer.
Cohen, W. and D. Levinthal (1989): “Innovation and Learning: The Two Faces
of R&D,” Economic Journal, 99, 569–596.
Crawford, G. and M. Shum (2005): “Uncertainty and Learning in Pharmaceutical Demand,” Econometrica, 73, 1137–1173.
Dickstein, M. J. (2014): “Efficient Provision of Experience Goods: Evidence from
Antidepressant Choice,” Stanford University, unpublished manuscript.
DiMasi, J. A., H. G. Grabowski, and R. W. Hansen (2016): “Innovation in
the Pharmaceutical Industry: New Estimates of R&D Costs,” Journal of Health
Economics, 47, 20–33.
Doraszelski, U., G. Lewis, and A. Pakes (2018): “Just Starting Out: Learning
and Equilibrium in a New Market,” American Economic Review, 108, 565–615.
Dranove, D. and N. Gandal (2003): “The DVD vs. DIVX Standards War:
Network Effects and Empirical Evidence of Preannouncement Effects,” Journal of
Economics and Management Strategy, 12, 363–386.
Ellison, G. (2002): “Evolving Standards for Academic Publishing: A qr Theory,”
Journal of Political Economy, 110, 994–1034.
Erdem, T. and M. P. Keane (1996): “Decision-Making under Uncertainty: Capturing Dynamic Brand Choice Processes in Turbulent Consumer Goods Markets,”
Marketing Science, 15, 1–20.
Ericson, R. and A. Pakes (1995): “Markov-Perfect Industry Dynamics: A Framework for Empirical Work,” Review of Economic Studies, 62, 53–82.
85

Farrell, J. and G. Saloner (1988): “Coordination through Committees and
Markets,” RAND Journal of Economics, 19, 235–252.
Fernandez-Villaverde, J. and J. F. Rubio-Ramirez (2007): “Estimating
Macroeconomic Models: A Likelihood Approach,” Review of Economic Studies,
74, 1059–1087.
Galasso, A., M. Mitchell, and G. Virag (2016): “Market Outcomes and
Dynamic Patent Buyouts,” International Journal of Industrial Organization, 48,
207–243.
Galasso, A. and T. Simcoe (2011): “CEO Overconfidence and Innovation,” Management Science, 57, 1469–1484.
Ganglmair, B. and E. Tarantino (2014): “Conversation with Secrets,” RAND
Journal of Economics, 45, 273–302.
Gilbert, R. and C. Shapiro (1990): “Optimal Patent Length and Breadth,”
RAND Journal of Economics, 21, 106–112.
Gordon, R. J. (2016): The Rise and Fall of American Growth: The U.S. Standard
of Living since the Civil War, Princeton University Press.
Gross, D. P. (2017): “Performance Feedback in Competitive Product Development,” RAND Journal of Economics, 48, 438–466.
Hamilton, B. H., J. A. Nickerson, and H. Owan (2003): “Team Incentives
and Worker Heterogeneity: An Empirical Analysis of the Impact of Teams on
Productivity and Participation,” Journal of Political Economy, 111, 465–497.
Heidhues, P., S. Rady, and P. Strack (2015): “Strategic Experimentation with
Private Payoffs,” Journal of Economic Theory, 159A, 531–551.
Henderson, R. and I. Cockburn (1996): “Scale, Scope, and Spillovers: The Determinants of Research Productivity in Drug Discovery,” RAND Journal of Economics, 27, 32–59.
Hirshleifer, D., A. Low, and S. Hong Teoh (2012): “Are Overconfident CEOs
Better Innovators?” Journal of Finance, 67, 1457–1498.
Hoffman, P. (2012): “The Tao of IETF: A Novice’s Guide to the Internet Engineering Task Force,” available at https://www.ietf.org/tao.html.
Hopenhayn, H., G. Llobet, and M. Mitchell (2006): “Rewarding Sequential
Innovators: Prizes, Patents, and Buyouts,” Journal of Political Economy, 114,
1041–1068.
86

Howell, S. T. (2017): “Learning From Feedback: Evidence From New Ventures,”
NBER Working Paper 23874, National Bureau of Economic Research, Cambridge,
Mass.
Katz, M. L. (1986): “An Analysis of Cooperative Research and Development,”
RAND Journal of Economics, 17, 527–543.
Katz, M. L. and C. Shapiro (1985): “Network Externalities, Competition and
Compatibility,” American Economic Review, 75, 424–440.
Keller, G., S. Rady, and M. Cripps (2005): “Strategic Experimentation with
Exponential Bandits,” Econometrica, 73, 39–68.
Kerr, W. R., R. Nanda, and M. Rhodes-Kropf (2014): “Entrepreneurship as
Experimentation,” Journal of Economic Perspectives, 28, 25–48.
Krieger, J. L. (2017): “Trials and Terminations: Learning from Competitors’ R&D
Failures,” Massachusetts Institute of Technology, unpublished manuscript.
Lerner, J. and J. Tirole (2015): “Standard-Essential Patents,” Journal of Political Economy, 123, 547–586.
Li, D. (2017): “Expertise versus Bias in Evaluation: Evidence from the NIH,” American Economic Journal: Applied Economics, 9, 60–92.
Li, D., P. Azoulay, and B. N. Sampat (2017): “The applied value of public
investments in biomedical research,” Science, 356, 78–81.
Magnac, T. and D. Thesmar (2002): “Identifying Dynamic Discrete Decision
Processes,” Econometrica, 70, 801–816.
Moscarini, G. and L. Smith (2001): “The Optimal Level of Experimentation,”
Econometrica, 69, 1629–1644.
Moscarini, G. and F. Squintani (2010): “Competitive Experimentation with
Private Information: The Survivor’s Curse,” Journal of Economic Theory, 145,
639–660.
Murray, F., S. Stern, G. Campbell, and A. MacKormack (2012): “Grand
Innovation Prizes: A Theoretical, Normative, and Empirical Evaluation,” Research
Policy, 41, 1779–1792.
Pakes, A. (1986): “Patents as Options: Some Estimates of the Value of Holding
European Patent Stocks,” Econometrica, 55, 911–922.
Palacios-Huerta, I. and A. Prat (2012): “The Impact Factor of Managers,”
Columbia University, unpublished manuscript.
87

Roach, M. and W. Cohen (2013): “Lens or Prism? Patent Citations as a Measure
of Knowledge Flows from Public Research,” Management Science, 59, 504–525.
Roberts, K. and M. L. Weitzman (1981): “Funding Criteria for Research, Development, and Exploration Projects,” Econometrica, 49, 1261–1288.
Simcoe, T. (2012): “Standard Setting Committees: Consensus Governance for
Shared Technology Platforms,” American Economic Review, 102, 305–336.
Simcoe, T. and D. M. Waguespack (2011): “Status, Quality, and Attention:
What’s in a (Missing) Name?” Management Science, 57, 274–290.
Thompson, N. C. and S. Zyonts (2017): “Who tries (and who succeeds) in
staying at the forefront of Science,” .
Weitzman, M. L. (1979): “Optimal Search for the Best Alternative,” Econometrica,
47, 641–654.
Wen, W., C. Forman, and S. Jarvenpaa (2015): “Standards, Intellectual Property Rights, and Strategic Patenting: Evidence from the IETF,” Unpublished
manuscript, available at https://ssrn.com/abstract=2709645.

88

