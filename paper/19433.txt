NBER WORKING PAPER SERIES

PRIVACY AND DATA-BASED RESEARCH
Ori Heffetz
Katrina Ligett
Working Paper 19433
http://www.nber.org/papers/w19433
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2013

For useful comments on an early draft, we thank Dan Benjamin, Avrim Blum, Hank Greely, Aleksandra
Korolova, Frank McSherry, Kobbi Nissim, Ted O‚ÄôDonoghue, Grant Schoenebeck, Moses Shayo, Latanya
Sweeney, Kunal Talwar, and Jonathan Ullman. Ligett‚Äôs work was supported in part by an NSF CAREER
award (CNS-1254169), the US-Israel Binational Science Foundation (grant 2012348), the Charles
Lee Powell Foundation, a Google Faculty Research Award, and a Microsoft Faculty Fellowship. The
views expressed herein are those of the authors and do not necessarily reflect the views of the National
Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
¬© 2013 by Ori Heffetz and Katrina Ligett. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including ¬© notice,
is given to the source.

Privacy and Data-Based Research
Ori Heffetz and Katrina Ligett
NBER Working Paper No. 19433
September 2013
JEL No. C49,C89,D89,Z00
ABSTRACT
What can we, as users of microdata, formally guarantee to the individuals (or firms) in our dataset,
regarding their privacy? We retell a few stories, well-known in data-privacy circles, of failed anonymization
attempts in publicly released datasets. We then provide a mostly informal introduction to several ideas
from the literature on differential privacy, an active literature in computer science that studies formal
approaches to preserving the privacy of individuals in statistical databases. We apply some of its insights
to situations routinely faced by applied economists, emphasizing big-data contexts.
Ori Heffetz
S.C. Johnson Graduate School of Management
Cornell University
324 Sage Hall
Ithaca, NY 14853
and NBER
oh33@cornell.edu
Katrina Ligett
Department of Computing and Mathematical Sciences
and
Division of Humanities and Social Sciences
California Institute of Technology
MC 305-16
Pasadena, CA 91125
katrina@caltech.edu

On August 9, 2006, the Technology section of the New York Times contained a news item titled
‚ÄúA Face Is Exposed for AOL Searcher No. 4417749.‚Äù In it, reporters Michael Barbaro and Tom
Zeller tell a story about big data and privacy:
Buried in a list of 20 million Web search queries collected by AOL and recently released
on the Internet is user No. 4417749. The number was assigned by the company to
protect the searcher‚Äôs anonymity, but it was not much of a shield.
No. 4417749 conducted hundreds of searches over a three-month period on topics ranging
from ‚Äúnumb fingers‚Äù to ‚Äú60 single men‚Äù to ‚Äúdog that urinates on everything.‚Äù
And search by search, click by click, the identity of AOL user No. 4417749 became
easier to discern. There are queries for ‚Äúlandscapers in Lilburn, Ga,‚Äù several people
with the last name Arnold and ‚Äúhomes sold in shadow lake subdivision gwinnett county
georgia.‚Äù
It did not take much investigating to follow that data trail to Thelma Arnold, a 62year-old widow who lives in Lilburn, Ga.
...
Ms. Arnold, who agreed to discuss her searches with a reporter, said she was shocked to
hear that AOL had saved and published three months‚Äô worth of them. ‚ÄúMy goodness,
it‚Äôs my whole personal life,‚Äù she said. ‚ÄúI had no idea somebody was looking over my
shoulder.‚Äù
...
‚ÄúWe all have a right to privacy,‚Äù she said. ‚ÄúNobody should have found this all out.‚Äù
Empirical economists are increasingly users, and even producers, of large datasets with potentially sensitive information. Some have for decades handled such data (e.g., Census data), and
routinely think and write about privacy. Many others, however, are not accustomed to think about
privacy, perhaps because their research traditionally relies on already-publicly-available data, or because they gather their data through relatively small, ‚Äúmostly harmless‚Äù surveys and experiments.
This ignorant bliss may not last long; detailed data of unprecedented quantity and accessibility
are now ubiquitous: a private database from an internet company, field experimental data on massive groups of unsuspecting subjects, or a government agency‚Äôs confidential administrative records

2

in digital form. And while big data become difficult to avoid, getting privacy right is far from
easy‚Äîeven for data scientists, as the AOL story demonstrates.
This paper aims to inspire data-based researchers to think more about issues such as privacy and
anonymity. Many of us routinely promise anonymity to the subjects who participate in our studies,
either directly through informed consent procedures, or indirectly through our correspondence with
Institutional Review Boards (IRBs). What is the informational content of such promises? Given
that our goal is, ultimately, to publish the results of our research‚Äîformally, to publish functions
of the data‚Äîunder what circumstances, and to what extent, can we guarantee that individuals‚Äô
privacy not be breached and anonymity not be compromised?
These questions may be particularly relevant in a big data context, where there may be a risk
of more harm‚Äîdue to often-sensitive content‚Äîto more people‚Äîdue to scale. As we discuss below,
it is also in a big data context that ‚Äúprivacy guarantees‚Äù of the sort we consider may be most
effective.
Our paper is divided into three parts. In the first, we retell the stories of several privacy
debacles, well-known in privacy-research circles. The first three stories concern intentional releases
of deidentified data for research purposes. In each case attempts were made to anonymize the
data‚Äîrecords of medical patients, web searchers (the AOL story above), and Netflix subscribers‚Äî
prior to their release, but it quickly became apparent that individuals could be reidentified. The
fourth story‚Äîinvolving Facebook ads‚Äîillustrates how individuals‚Äô privacy could be breached even
when the data themselves are not released. It demonstrates that a potential privacy risk exists
whenever some function of personal data‚Äîin this case, the seemingly innocuous count of ads
shown in a campaign‚Äîis visible to outsiders‚Äîin this case, advertisers. These often-told stories
serve as cautionary tales of how things can go terribly wrong when, in the absence of a formal
framework for thinking about privacy, one relies instead on intuition. We also discuss a number of
natural, more sophisticated proposals and attempts to disguise the data in each of the four cases;
while making reidentification more difficult, they do not eliminate the risk.
None of our stories involves security horrors such as stolen data, broken locks and passwords,
or compromised secure connections. Rather, in all of them information was released that had been
thought to have been anonymized, but, as was quickly pointed out, was rather revealing. A naive
reaction might be to attempt to roll back current policies and reverse current trends of increased
openness and data sharing. We believe such a reaction would be misguided. Moreover, as the
Facebook example demonstrates, not sharing the data does not eliminate the risks.

3

In the second part of our paper we shift gears, switching from storytelling to discussing differential privacy, a rigorous, portable privacy notion introduced roughly a decade ago by computer
scientists aiming to enable the release of information while providing provable privacy guarantees.
We formally introduce the differential privacy definition, at the heart of which is the idea that the
addition or removal of a single individual from a dataset should have nearly no effect on any publicly released functions of the data‚Äîa single statistic, a collection of statistics, synthetic data that
preserve many properties of the original database, or a complex policy recommendation. Achieving
this goal requires introducing randomness into the released outcome, with more randomness resulting in more privacy‚Äîbut less accuracy. We discuss simple applications of the definition, illustrating
this tension.
What insights can the differential privacy literature offer regarding our cautionary tales? What
guidance does it have for researchers working with data? In the third part of our paper, we offer
lessons and reflections, discuss some limitations, and briefly mention additional applications. We
conclude with reflections on current promises of ‚Äúanonymity‚Äù to study participants‚Äîpromises that,
given common practices in empirical research, are not guaranteed to be kept. We invite researchers
to consider either backing such promises with meaningful privacy-preserving techniques, or qualifying them. Especially in the case of big data, application-ready implementations of such techniques
may in the foreseeable future become a practical possibility, but only if increased awareness among
data-based researchers generates demand. Until then, more cautious promises may be warranted;
while we are not aware of major privacy debacles in economics research to date, the stakes are only
getting higher.

Intuition Regarding Privacy May Not Be Enough
Well-intentioned government or private entities in possession of a sensitive database may wish
to make an anonymized version of it public, for example to facilitate research. We retell and
discuss a few cautionary tales that illustrate how intuition-based anonymization attempts may fail,
sometimes spectacularly.1
1
As mentioned above, these stories are ‚Äúclassics,‚Äù well known in the computer science community that studies
privacy, and routinely cited in the introduction to many papers. The first three were recently revisited and discussed
by Ohm (2010), a legal scholar, who provides further references and links to primary sources.

4

Stories of Failed Anonymization
The first story is from the mid 1990s, when William Weld, then Governor of Massachusetts, approved the release of certain medical records of state employees to researchers, assuring the public
that individual anonymity would be protected by eliminating obvious identifiers from the data
(Greely, 2007). A few days after Weld‚Äôs announcement, Latanya Sweeney‚Äîthen a graduate student at MIT‚Äîreidentified Weld‚Äôs personal records (including diagnoses and prescriptions) in the
database; she then had his records delivered to his office.
While the medical data‚Äîofficially, the Massachusetts ‚ÄúGroup Insurance Commission‚Äù (GIC)
data‚Äîhad been ‚Äúdeidentified‚Äù by removing fields containing patients‚Äô name, address, and social
security number (SSN) prior to the the data release, the nearly one hundred remaining fields
included ZIP code, birth date, and sex. As Ohm (2010) tells the story, Sweeney
knew that Governor Weld resided in Cambridge, Massachusetts, a city of fifty-four
thousand residents and seven ZIP codes. For twenty dollars, she purchased the complete
voter rolls from the city of Cambridge‚Äîa database containing, among other things, the
name, address, ZIP code, birth date, and sex of every voter. By combining this data
with the GIC records, Sweeney found Governor Weld with ease. Only six people in
Cambridge shared his birth date; only three were men, and of the three, only he lived
in his ZIP code.2,3
The next story, involving Ms. Arnold above, is from roughly a decade later. In 2006, AOL
Research released detailed internet search records of 650,000 users covering a three-month period,
amounting to twenty million search queries.4 The stated purpose of the release was expressed by
2

Barth-Jones (2012) revisits and critiques this story. Perhaps in response, Sweeney, Abu and Winn (2013) use a
similar method to reidentify individuals in the publicly available Personal Genome Project database.
3
Sweeney‚Äôs ‚ÄúHow Unique Are You?‚Äù interactive website invites the visitor to ‚ÄúEnter your ZIP code, date of birth,
and gender to see how unique you are (and therefore how easy it is to identify you from these values).‚Äù Her simple
methodology is explained on the website: (http://aboutmyinfo.net, accessed on August 9, 2013)
‚ÄúBirthdate . . . , gender, and 5-digit postal code (ZIP) uniquely identifies most people in the United States.
Surprised? . . . 365 days in a year x 100 years x 2 genders = 73,000 unique combinations, and because
most postal codes have fewer people, the surprise fades. . . . there are more than 32,000 5-digit ZIP codes
in the United States; so 73,000 x 32,000 is more than 2 billion possible combinations but there are only
310 million people in the United States. In 1997, Latanya Sweeney did this kind of uniform calculation
on populations reported in the U.S. Census for age groups in each postal code and summed the results
to predict that at most 87 percent of the U.S. population had unique combinations . . . the maximum
percent of unique combinations may drop as you move from date of birth to age, and from 5-digit ZIP
code to county. Notice that even knowing the county, age, and gender can make some people unique.
They are few, and they tend to live in remote locations, but notice it is not 0.‚Äù
Golle (2006) revisits and updates this 87 percent figure.
4
As Ohm (2010) notes, different numbers appear in different accounts. The 650,000 figure above was described

5

then AOL Research head Abdur Chowdhury:
AOL is embarking on a new direction for its business - making its content and products
freely available to all consumers. To support those goals, AOL is also embracing the
vision of an open research community, which is creating opportunities for researchers
in academia and industry alike. . . . with the goal of facilitating closer collaboration
between AOL and anyone with a desire to work on interesting problems.5
Prior to the data release, the search logs were deidentified, for example by removing usernames and
IP addresses, using instead unique identifiers (such as ‚Äú4417749‚Äù) to link all of a single user‚Äôs queries.
This deidentification, however, quickly proved far from sufficient for the intended anonymization,
as illustrated by the New York Times article on Ms. Arnold. Within days of the release, AOL
apologized, removed the data website as well as a few employees, and silenced its Research division.
Of course, to this day, the data are widely available through a simple web search; once published,
you cannot take it back.
The third story is also from 2006, a bad year for privacy. About two months after the AOL
debacle, Netflix announced a competition‚Äîthe Netflix Prize‚Äîfor improving the company‚Äôs algorithm that predicts user ratings of films, using only past user ratings. To allow competitors to train
their algorithms, Netflix released a database with one hundred million ratings of 17,770 films by
half a million subscribers covering a six-year period. Each record contained a movie title, a rating
date, and a five-point rating. As in the GIC and AOL cases, records were deidentified prior to the
release, replacing user names with unique identifiers.
The illusion of protecting users‚Äô anonymity was, again, short-lived. Two weeks after the data
release, Narayanan and Shmatikov (2008; first version posted in 2006) demonstrated that, in their
(2008) words, ‚Äúan adversary who knows a little bit about some subscriber can easily identify her
record if it is present in the dataset, or, at the very least, identify a small set of records which
include the subscriber‚Äôs record.‚Äù
How little is ‚Äúa little bit‚Äù? In many cases, as little as knowing a user‚Äôs approximate dates and
ratings of two or three movies. In their demonstration, Narayanan and Shmatikov used ratings from
the Internet Movie Database (IMDB), which are publicly available and are linked to the raters‚Äô
as 500,000 in the original post, and the twenty million figure in the original post has later been reported by some as
thirty-six million.
5
Posting of Abdur Chowdhury, cabdur@aol.com, to SIGIR-IRList, irlist-editor@acm.org, http://sifaka.cs.
uiuc.edu/xshen/aol/20060803_SIG-IRListEmail.txt (cited in Ohm (2010), and accessed on August 9, 2013).

6

identities, and showed how a handful of a user‚Äôs IMDB ratings, even when they yield imprecise
information, could uniquely identify her in the Netflix database.
Whereas IMDB‚Äôs public ratings may reveal only those movies that an individual wants the world
to know she has watched, Netflix ratings may reveal all of the movies she has rated, including those
she may prefer to keep private‚Äîe.g., films that may reflect her sexual, social, political, or religious
preferences. Moreover, to be reidentified, one does not have to be on IMDB: as Ohm (2010) advises
his readers, ‚Äúthe next time your dinner party host asks you to list your six favorite obscure movies,
unless you want everybody at the table to know every movie you have ever rated on Netflix, say
nothing at all.‚Äù

Deidentification and Beyond
Deidentified data were defined by Sweeney (1997) as data in which ‚Äúall explicit identifiers, such as
SSN, name, address, and telephone number, are removed, generalized, or replaced with a made-up
alternative.‚Äù Her definition seems to accurately describe the released GIC, AOL, and Netflix data
in the stories above. Some more recent definitions (e.g., those under federal health records privacy
regulations) are stricter and would not consider the GIC data released by Weld deidentified, but
they too keep the focus on removing only specific kinds of information (Greely, 2007). Indeed,
more than fifteen years after Sweeney‚Äôs powerful demonstration, her definition still describes, more
or less accurately, commonplace practices among many researchers. For example, prior to publicly
posting their data online (as required by some journals), economists often deidentify their data by
merely withholding explicit identifiers such as subject names. However, as in the stories above, the
stated aim of such deidentification‚Äîand what is often promised to subjects, directly or via IRB‚Äîis
anonymization. In Sweeney‚Äôs (1997) definition, anonymous data ‚Äúcannot be manipulated or linked
to identify an individual.‚Äù Clearly, deidentification far from guarantees anonymization.
Sweeney‚Äôs GIC reidentification used birthday and 5-digit ZIP code, neither of which are typically
included in datasets publicly posted by economists. But it is not difficult to imagine reidentification of specific subjects based on combinations of demographics such as study major, age/class,
gender, and race, which are often not considered ‚Äúidentifiable private information‚Äù and are routinely included in posted data.6 Reidentification is still easier with knowledge regarding, e.g., the
6

For example, according to Cornell‚Äôs Office of Research Integrity and Assurance:
‚ÄúIdentifiable private information is defined as: name; address; elements of dates related to an individual (e.g., birth date); email address; numbers: telephone, fax, social security, medical record, health
beneficiary / health insurance, certificate or license numbers, vehicle, account numbers (e.g., credit

7

day and time in which a classmate or a roommate participated in a specific study session.7 But
it is possible even without such special knowledge, and it may be straightforward for specifically
vulnerable individuals, such as minorities, or women in the sciences.
This discussion highlights a weakness of deidentification: if one assumes no restrictions on
outside information (also referred to below as auxiliary information), then, short of removing all
data fields prior to a release, some individuals may be uniquely identified by the remaining fields.
One potential response to this weakness is an approach called k-anonymity, which combines some
restrictions on outside information with the removal (or partial removal) of some fields. Specifically,
assuming that outside information could only cover certain fields in the database, one could suppress
these fields or, when possible, generalize them (e.g., replace date of birth with year of birth), so that
any combination of the values reported in these fields would correspond to at least k individuals in
the data (see, e.g., Sweeney, 2002). This approach has several weaknesses, and in many applications
it implies either an unreasonably weak privacy guarantee or a massive suppression of data (notice
that the amount of information that can be released is expected to shrink as k grows and as
restrictions on outside information are weakened). See, e.g., Narayanan and Shmatikov (2008) for
a discussion in the Netflix context.
An alternative approach is to make it harder for an attacker to leverage outside information. For
example, prior to making the query logs publicly available, AOL could have replaced not only user
identities but also the search words themselves with uniquely identifying random strings. Similarly,
Netflix could have replaced movie names with unique identifiers. Such an approach, known as
‚Äútoken-based hashing,‚Äù would preserve many features of the data, hence maintaining usefulness of
the database for some (though, clearly, not all) research purposes. But it is these very preserved
features of the underlying data that make this scheme vulnerable as well.
Indeed, shortly after the disaster at AOL Research, a group at Yahoo! Research‚ÄîKumar et al.
(2007)‚Äîshowed that an attacker with access to a ‚Äúreference‚Äù query log (e.g., early logs released by
Excite or Altavista) could use it to extract statistical properties of tokenized words in the database,
and ‚Äúinvert the hash function,‚Äù i.e., break the coding scheme, based on co-occurrences of tokens
within searches. Along similar lines, Narayanan and Shmatikov (2008) speculate that in the Netflix
card), device identification numbers, serial numbers, any unique identifying numbers, characteristics, or
codes (e.g., Global Positioning System (GPS) readings); Web URLs; Internet Protocol (IP) addresses;
biometric identifiers (e.g., voice, fingerprints); full face photographs or comparable images.‚Äù
(http://www.irb.cornell.edu, accessed on August 13, 2013).
7
In a similar vein, Sweeney (2013) uses newspaper stories that contain the word ‚Äúhospitalized‚Äù to reidentify
individual patients in a publicly available health dataset in Washington State.

8

case, such an approach ‚Äúdoes not appear to make de-anonymization impossible, but merely harder.‚Äù

Privacy Risk Without Data Release
Our fourth story, of privacy compromised on Facebook by Korolova (2011),
illustrates how a real-world system designed with an intention to protect privacy but
without rigorous privacy guarantees can leak private information . . . Furthermore, it
shows that user privacy may be breached not only as a result of data publishing using
improper anonymization techniques, but also as a result of internal data-mining of that
data.
Facebook‚Äôs advertising system allows advertisers to specify characteristics of individuals to
whom an ad should be shown. At the time of Korolova‚Äôs attack, it was possible to specify those
characteristics (e.g., gender, age, location, workplace, alma mater) so finely that they would correspond to a unique Facebook user. Then, two versions of the ad campaign could be run‚Äîfor
example, one with those same characteristics plus ‚ÄúInterested in women;‚Äù the other with those
characteristics plus ‚ÄúInterested in men.‚Äù Even if this user‚Äôs interests were not visible to her friends,
if she had entered them in her profile, they would be used for ad targeting. Thus, if the advertiser
received a report that, e.g., the ‚ÄúInterested in women‚Äù version of her ad had been displayed, she
could infer the targeted individual‚Äôs private interests.
Other attacks were possible too. ‚ÄúUsing the microtargeting capability, one can estimate the
frequency of a particular person‚Äôs Facebook usage, determine whether they have logged in to the
site on a particular day, or infer the times of day during which a user tends to browse Facebook.‚Äù
Korolova‚Äôs paper quotes failed promises by Facebook executives, such as that Facebook ‚Äú[doesn‚Äôt]
share your personal information with services you don‚Äôt want‚Äù and ‚Äú[doesn‚Äôt] give advertisers access
to your personal information.‚Äù She notes:
We communicated our findings to Facebook on July 13, 2010, and received a very
prompt response. On July 20, 2010, Facebook launched a change to their advertising
system that made the kind of attacks we describe much more difficult to implement in
practice, even though, as we discuss, they remain possible in principle.
This Facebook story helps demonstrate that if one seeks to use functions of the data‚Äîbe it
via research findings, policy decisions, or commercial services and products‚Äîthe privacy of the

9

individuals comprising the data may be at risk without an approach providing (provable) privacy
guarantees. We discuss such an approach next.

Differential Privacy
A common theme in the examples above has been the crucial role played by auxiliary information,
i.e., knowledge from sources outside the dataset under consideration. In the examples above,
attackers consulted various outside sources not foreseen by the database owners‚Äîpublic records
such as voter rolls, complementary databases such as IMDB, or, simply, personal familiarity with
an individual in the database. To identify individuals, the attackers then carried out a variant of
a so-called ‚Äúlinkage attack:‚Äù they matched fields that overlap across the auxiliary data and the
attacked database.
More generally, one may invite trouble when making specific assumptions regarding what information a potential attacker might have or how she might use it. If such assumptions are ever
violated‚Äîeven in the future, as new technology and information become available‚Äîprivacy may
be compromised. One approach to addressing the auxiliary-information concern would be to seek
to provide privacy guarantees free from assumptions about the attacker and her information.
The approach we discuss here, differential privacy, seeks to do just that. It emerged from work
in computer science theory by Dinur and Nissim (2003), Dwork and Nissim (2004), and Dwork
et al. (2006a). Our discussion and examples draw on a number of surveys, including Dwork (2006),
Dwork and Smith (2010), Dwork (2011b,a), and Dwork et al. (2011). These surveys additionally
present historical aspects of the development of the differential privacy definition, more examples,
and a much broader range of applications than we discuss here.8

The Differential Privacy Definition
To fix ideas, consider the released outcome of some function of a database (for example, the released
number of Facebook users to whom an ad was displayed, or some published table of statistics in an
empirical research paper, or even a released version of the entire database). Consider a potential
participant in the database (for example, someone who considers joining Facebook, or someone who
considers participating in a research study), and compare two possible scenarios: in one, she joins
and is added to the database; in the other, she does not join and is hence not in the database.
8

We also recommend a recent popular article on differential privacy research by Klarreich (2012).

10

Informally, differential privacy seeks to guarantee to the potential participant that, irrespective
of her participation decision, almost the same things can be learned from the released outcome‚Äî
regardless of outside information, of the data already in the database, or of her own personal
data. We emphasize ‚Äúalmost‚Äù; without it, the function would have to yield identical results on any
two input databases‚Äîeven two that are wildly different‚Äîsince any database of individuals can be
constructed from any other via a (potentially long) chain of additions and removals of individuals.
Differential privacy hence gives participants (and nonparticipants) in the database a form of
plausible deniability: they could always deny that their data took specific values or even that they
participated (or not), and an observer would have almost no evidence either way.
Specifically, consider pairs of databases (D, D0 ) that are identical except that one of the databases
has one additional row (or record) over the other. We refer to such a pair as neighboring databases,
and think of each row as corresponding to one individual. Thus, two neighboring databases differ
by only the participation of one individual. Now consider some computation on such databases.
Denote the computation by a function K, and consider the space of possible outcomes of the computation. A differentially private computation (or function, or mechanism) selects its output using
randomness, such that the probability of any given outcome is similar under any two neighboring
databases.
Formally,
Definition (Differential Privacy; Dwork et al., 2006a). A randomized function K provides differential privacy if for every S ‚àà Range(K) and for all neighboring databases D and D0 ,
Prob[K(D) = S] ‚â§ e ¬∑ Prob[K(D0 ) = S],
for  ‚â• 0, and where the probability space in each case is over the randomness of K.9
Note that in particular, for any neighboring pair (D, D0 ), the definition must hold with the
larger quantity (i.e., max {Prob[K(D) = S], Prob[K(D0 ) = S]}) on the left, constraining it to be
larger by at most a multiplicative e .
This definition formalizes our discussion above: by ‚Äúalmost the same‚Äù we meant that  should
be small. How small? The definition does not prescribe an answer to this normative question,
9

There are other variants on this definition, which we do not emphasize here. A common generalization of
differential privacy allows an additive Œ¥ difference in the probabilities, in addition to the mulitiplicative difference
e (see, e.g., Dwork et al., 2006b; Machanavajjhala et al., 2008). Such generalization provides a weaker privacy
guarantee, but may allow for more accurate outcomes.

11

a point we return to below. In the limiting case of  = 0, we would drop the ‚Äúalmost‚Äù; but in
that limiting case, e would equal 1, requiring that the function K be indistinguishable on any two
input databases, as discussed above. In other words, maximum differential privacy means useless
published output. More generally, the definition makes precise an intuitive tradeoff between privacy
and usefulness.
The definition readily extends to provide a privacy guarantee to a group of individuals: an
-differentially private mechanism is k-differentially private from the point of view of a group of
k individuals (or one individual whose data comprise k rows in the database). It also immediately yields an elegant composition property: running l -differentially private mechanisms‚Äîe.g.,
publishing l statistics based on the database‚Äîgives a guarantee of l-differential privacy.10 This
composition property is particularly important in the context of academic research, where individuals may participate in more than one database, and where on each database typically more
than one analysis is conducted. Differential privacy hence provides a tool for understanding the
cumulative privacy harm incurred by an individual whose data appear in multiple databases, potentially used by different entities for different purposes and at different points in time. One could
discuss assessments of individuals‚Äô cumulative, lifelong privacy loss, and use them as an input into
the discussion of how small  should be.
Finally, it is also useful to note that differential privacy guarantees hold up under postprocessing
of their outputs: if one conducts an -differentially private computation, one is then free to perform
any subsequent computation on its output, and the result will still be -differentially private. This
means, for example, that once one has produced differentially private statistics on a dataset, those
statistics can made public for all eternity, without concern that at some later date a clever hacker
will find some new privacy-revealing weakness.
From a Bayesian point of view, differential privacy can be given the following interpretation: an
observer with access to the output of a differentially private function should draw almost the same
conclusions whether or not one individual‚Äôs data are included in the analyzed database, regardless of the observer‚Äôs prior. This interpretation highlights that differential privacy is a property
of the function (the randomized mapping from databases into outcomes), not of the output (a
particular outcome). Kasiviswanathan and Smith (2008) credit Dwork and McSherry with the first
formulation of this interpretation, which can be formalized and proven equivalent to differential
‚ÄúP
‚Äù
l
More generally, running any l differentially private mechanisms with guarantees 1 , . . . , l gives
i=1 i differential privacy. Equivalently, one may split a fixed total budget of  across a set of desired computations.
10

12

privacy.
‚ÄúObserver‚Äù may of course refer to anyone with access to the output of the function, including,
e.g., malicious attackers, (legitimate) advertisers on Facebook, or the readers of a research paper
that reports some statistic. Notice that this Bayesian interpretation does not rule out performing
analyses and reporting outcomes that vastly alter the adversary‚Äôs posterior view of the world, so
long as the outcomes are not very sensitive to the presence or absence of any one individual in the
original database. An often-used example: one could conduct a differentially private analysis that
revealed a surprising correlation between smoking and cancer, so long as that correlation depended
only negligibly on the participation of any one individual (or, for that matter, of any small group of
individuals) in the study. Revealing this correlation might allow observers to draw inferences about
a smoker, who might then feel that her privacy has been harmed. But since essentially the same
conclusions would have been drawn regardless of whether that smoker participated in the study,
her differential privacy has been respected.

From Definition to Application: Noise and Sensitivity
Armed with the above definition, consider the computation (and subsequent release) of the mean
income of individuals in a database. While the mean might seem like a fairly innocuous statistic,
all statistics reveal something about the data, and there are worst-case situations where the mean
might be quite revealing. For example, if the mean salary in a certain economics department prior
to hiring a new faculty member is known to an observer (for instance, due to a previous release),
then releasing the new mean after the hire reveals the new hire‚Äôs salary. This is a variant of the
so-called ‚Äúdifferencing attack.‚Äù
A differential privacy guarantee would require adding randomly generated noise to the true
mean prior to its release. How much noise? Since differential privacy is a worst-case guarantee
over all possible pairs of neighboring databases and over all possible outcomes, if the distribution of
incomes is not a priori bounded, the noise would have to be unboundedly large (to guarantee that
even the addition of an extreme outlier to the database would have little effect on the differentiallyprivate statistic). With a limit on the range of incomes, however, one could add a limited amount
of noise to the true mean in order to guarantee differential privacy.
Formally, when a function f that we wish to compute on a database returns a real number, we

13

say that the sensitivity of that function is
‚àÜf = max0 |f (D) ‚àí f (D0 )|,
D,D

for (D, D0 ) neighboring databases. The definition makes it clear that sensitivity is a property of the
function, given a universe of possible databases, and is independent of the actual input database.
Intuitively, the sensitivity of f is simply the maximum difference between the values it takes on any
two neighboring databases; this maximum difference must be hidden in order to preserve differential
privacy.
A simple technique for hiding this maximum difference, ‚àÜf , in an -differentially private manner,
‚àö
is to add to f noise from a Laplace distribution with mean = 0 and standard deviation = 2‚àÜf /
(Dwork et al., 2006a).11 We next focus on this technique in the context of a concrete example.

A Single-Statistic Example: Mean Salary
To illustrate some of the delicate issues involved in actually carrying out a differentially private
computation, consider the release of mean salary among the faculty in an economics department.
The technique above suggests calculating the (true) mean salary first, then adding to it Laplace
‚àö
noise with standard deviation 2‚àÜf / prior to releasing it. One therefore needs, first, a value for .
Such value could be determined through a combination of, e.g., philosophical and ethical inquiry,
and social, political, and legislative processes, and could depend on context; further research is
clearly needed.12 Second, one needs to calculate ‚àÜf . In our case, this requires making assumptions
regarding the universe of possible databases. We discuss these next.
Outcome range: As mentioned above, to yield practical results our technique requires ‚àÜf to
be bounded. Our example intentionally involves salary, rather than total income, because salary
‚àö
|x|
1 ‚àí b
With scale parameter b = ‚àÜf /, the pdf of this distribution is 2b
e
and its standard deviation is 2b. This

distribution is a natural choice because its exponential form satisfies the multiplicative e constraint in the differential
privacy definition.
12
As Dwork et al. (2011) note in a defense of differential privacy:
11

Yes, this research is incomplete. Yes, theorems of the following form seem frighteningly restrictive:
If an individual participates in 10,000 adversarially chosen databases, and if we wish to
ensure that her cumulative privacy loss will, with probability at least 1 ‚àí e‚àí32 , be bounded
by e1 , then it is sufficient that each of these databases will be  = 1/801-differentially
private.
But how else can we find a starting point for understanding how to relax our worst-case adversary
protection? How else can we measure the effect of doing so? And what other technology permits one
to prove such a claim?

14

is bounded from below (in the worst case, at zero). One still needs an upper bound, which cannot
be naively calculated from the data, but should be a property of the known universe of possible
salaries. For simplicity, we assume that it is known to be some yÃÑ. With these bounds, and with
mean salary as our outcome of interest, |f (D) ‚àí f (D0 )| ‚â§ yÃÑ/n, where n is the number of individuals
in the larger of the two neighboring databases.13
Database size: If the database size, n, is not publicly known, then the universe of possible
databases includes the case n = 1, and therefore ‚àÜf = yÃÑ. With such high sensitivity, a naive
application of the Laplace noise technique yields a uselessly uninformative outcome at any n: the
‚àö
noise added to the true mean has standard deviation 2yÃÑ/, which, even with  = 1, is larger than
the upper bound. An easy modification of the technique, however, yields noise that shrinks with
n. The idea is to think of the mean as the function sum/n, treating n itself, as well as the sum of
salaries, as two statistics to be calculated in a differentially private manner. One then divides the
privacy budget  between the two statistics: n + sum =  (recall the composition property). Since
the sensitivities of n and of the sum are, respectively, 1 and yÃÑ, the noise added would have standard
‚àö
‚àö
deviations 2/n and 2yÃÑ/sum . Since the two statistics increase with n, the noise-to-signal ratio
of each vanishes asymptotically. With  = 1 and a favorable setting‚Äîa very large department
with mean salary not much below yÃÑ‚Äîthe differentially private release may convey some usable
information about the true mean, but generally, the promise of the approach is more apparent on
bigger data. For example, consider mean salary among the American Economic Association (AEA)
membership (n = 18, 061 in 2012).14 With, e.g.,  = 0.1 and true mean that is yÃÑ/10, the standard
deviation on the noise added by the Laplace technique would be a much more tolerable 0.16% of
the true n (i.e., 28 members) and 1.6% of the true sum of salaries, assuming we divide the privacy
budget equally‚Äîrather than optimally‚Äîbetween the two statistics. Of course, things look still
better with still bigger data and cleverer techniques.
Dwork (2011a) suggests that ‚Äú[s]ometimes, for example, in the census, an individual‚Äôs participation is known, so hiding presence or absence makes no sense; instead we wish to hide the values
in an individual‚Äôs row.‚Äù Our examples above‚Äîof databases that include all faculty in an economics
department or all AEA members‚Äîcould be viewed and analyzed as settings where participation is
publicly known. In such settings, it may make sense to modify our above definition of neighboring
databases, from pairs ‚Äúthat are identical except that one of the databases has one additional row,‚Äù
13
For simplicity (and conservativeness), in a database with n = 0 individuals we define mean salary to be at the
lower bound 0.
14
AEA membership figure is taken from Rousseau (2013).

15

to pairs of known size n, that differ in the content of exactly one row. In this form, differential privacy guarantees a participant that if her true salary y were replaced with some fake salary
y 0 ‚àà [0, yÃÑ], the probability of any given outcome would not change by much. With this modification,
only the sum of salaries needs to be computed and released in a differentially private manner.
Historically, this alternate definition (with databases of fixed and publicly known n) was used
in the first papers that sparked the differential privacy literature, and it is still used in much of the
work on differential privacy and statistics‚Äîa body of work that has grown quickly over the past few
years. Work in this area has repeatedly established the feasibility of achieving common statistical
goals while maintaining differential privacy. Importantly for our purposes, differentially private
versions have been developed for large classes of estimators‚Äîincluding those used routinely by
empirical economists‚Äîoften with little effective cost in terms of accuracy of the released results.15

Multiple Statistics
Of course, researchers wish to publish more than one statistic per database. In our example
above, the privacy budget  was divided between two statistics, n and sum, and each was then
independently computed in a differential-privacy preserving way.
An alternative approach is to compute the two statistics jointly. Our above definition of sensitivity generalizes to multiple (real valued) statistics by turning f from a scalar function into a
vector function, and replacing the difference |f (D) ‚àí f (D0 )| with the sum of differences component
by component (known as taxicab distance, or L1 distance). The Laplace noise technique also gen‚àö
eralizes, to prescribe noise with standard deviation 2‚àÜf / as before (but with the generalized
‚àÜf ), drawn i.i.d. and added separately to each component of the the true f .
15
An early survey by Dwork and Smith (2010) discusses in detail some of the first results (see also Wasserman
and Zhou (2010) for an early discussion of nonparametric density estimators). Here, we briefly highlight some of
the statistical objectives treated in the literature. Connections between differential privacy and robust statistics
were first explored by Dwork and Lei (2009), who demonstrate differentially private algorithms for interquartile
distance, median, and linear regression. Lei (2011) and Nekipelov and Yakovlev (2011) study differentially private
M-estimators. Smith (2008, 2011), extending connections to robustness, finds that for almost any estimator that is
asymptotically normal on i.i.d. samples from the underlying distribution (e.g., parametric MLE estimators, logistic
regression, and linear regression, under regularity conditions), there are differentially private versions with asymptotically no additional perturbation. Chaudhuri and Hsu (2012) explore another aspect of robustness, showing that
the convergence rate of any accurate, differentially private estimator is tied to its gross error sensitivity. Chaudhuri,
Monteleoni and Sarwate (2011), Rubinstein et al. (2012), and Kifer, Smith and Thakurta (2012) provide approaches
specifically tailored to minimize a convex loss function, allowing for, e.g., privacy-preserving logistic regression and
support vector machines. Quite recently, Thakurta and Smith (2013) provide generic results establishing differentially private versions of stable model selection procedures. They give specific results for sparse linear regression via
a new analysis of the stability of the Lasso estimator. Finally, in addition to these theoretical results, a number of
papers empirically investigate the performance of differentially private estimators (see, e.g., Vu and Slavkovic, 2009;
Chaudhuri, Monteleoni and Sarwate, 2011; Abowd, Schneider and Vilhuber, 2013).

16

This alternative approach may significantly reduce the amount of added noise, as demonstrated
by the case of histograms (Dwork et al., 2006a). Consider the release of a frequency histogram
of salaries in some database. Treating each bin as a separate statistic (e.g., ‚Äúthe count of rows
with salary $0‚Äì10,000‚Äù is one statistic) would require dividing the privacy budget  between the
bins. The sensitivity (i.e., ‚àÜf ) of each such bin statistic, like that of any such count query, is 1.
But 1 is also the (generalized) sensitivity of the vector function consisting of the entire histogram,
since adding an individual to a database always adds 1 to the count of one of the bins and 0 to
all others. In this example, calculating all the bins jointly reduces the added noise because it saves
the need to first divide the privacy budget between the statistics‚Äîa division whose cost in added
noise increases with the number of bins. More generally, consider the maximum possible effect on
a statistic of adding one individual to the database; if such worst-case effect cannot occur on each
of a group of statistics at the same time, considering them jointly may improve results.
One of the main focuses of research in differential privacy in recent years has been to develop
algorithms that can handle very large numbers of queries jointly with far less noise than simple noise
addition would permit. This substantial literature, beginning with Blum, Ligett and Roth (2008)
and continuing most recently with Hardt and Rothblum (2010) and Hardt, Ligett and McSherry
(2012), develops techniques for generating ‚Äúsynthetic data‚Äù‚Äîa set of valid database rows‚Äîthat
approximate the correct answers to all of a large, fixed set of queries. The techniques go far beyond
simply perturbing the data, involving ideas from geometry and computational learning theory;
individual records in the resulting synthetic data are artificially generated in a sophisticated manner
and cannot be connected with a single or small number of records in the original data. These
approaches have started to show practicality, in the form of simple implementations that achieve
good accuracy when tested on common statistical tasks using standard benchmark data (Hardt,
Ligett and McSherry, 2012), but much remains to be done.16

From Intuitions to Provable Guarantees
What insights can the differential privacy literature offer regarding the cautionary tales above?
What tools could it provide for researchers working with data? We offer some thoughts, and
16
Another growing literature considers large sets of queries of a particular type, and aims to get a better understanding of the privacy-accuracy tradeoffs for a specific combined task. One application that has received substantial
attention is contingency tables, which are computed from sets of k-way marginal queries; see, e.g., Barak et al.
(2007), Fienberg, Rinaldo and Yang (2010), Kasiviswanathan et al. (2010), Thaler, Ullman and Vadhan (2012),
Chandrasekaran et al. (2013), and Dwork, Nikolov and Talwar (2013).

17

highlight how different approaches respond differently to the inherent, unavoidable tradeoff between
privacy and accuracy. We then discuss some of the limitations, as well as additional applications,
of differential privacy.

Lessons and Reflections
In the Massachusetts GIC case‚Äîand, more generally, regarding the ‚Äúanonymization‚Äù of complex
data sets‚Äîlessons from differential privacy suggest considering two alternatives:
1. One could have released a differentially private, synthetic (i.e., artificial) version of the original
database, after removing or coarsening complex fields such as text (which, without coarsening,
would have made the data too high-dimensional for synthetization to work in practice). The
synthetic data would only be useful for a pre-determined (though potentially quite large) set
of statistics.
2. One could have withheld the full data but provided a differentially private interface to allow
researchers (or possibly the general public) to issue queries against the database.
Both approaches‚Äîproviding a sanitized database, and providing sanitized answers to individual
queries‚Äîface the inescapable tradeoff between privacy and usefulness (or accuracy). To achieve
privacy, they limit usefulness in different ways: while the first approach limits in advance the
type of queries (and hence of analysis) possible, the second maintains flexibility but might more
severely limit the overall number of queries, since the system has to dynamically (hence potentially
less efficiently) manage a privacy budget to answer arbitrary queries as they arrive, and would
eventually run out of its  budget and have to refuse new queries. This idea of an overall limit‚Äîa
privacy budget that places a quantifiable constraint on any approach‚Äîis a useful metaphor that
highlights one of the costs of preserving privacy: it imposes fundamental limits on how much
information can be revealed about the data.
In the case of the AOL debacle, the data to be released were so high-dimensional (the space
of rows being all possible search histories) that they clearly could not be handled with differential
privacy without some initial dimension reduction. This in itself is worth observing‚Äîfree text and
other high-dimensional data (e.g., genetic information) are potentially extraordinarily revealing,
and deserve careful attention. Korolova et al. (2009), in response to AOL‚Äôs data release, propose
releasing an alternate data structure called a query click graph, and demonstrate on real search
log data that a differentially private query click graph can be used to successfully perform some
18

research tasks that one might typically run on search logs. As the authors note, it remains to be
seen how broadly useful such sanitized data are, but such findings ‚Äúoffer a glimmer of hope.‚Äù
Regarding the Netflix challenge, the manner in which it was carried out‚Äîreleasing a large,
very high-dimensional dataset‚Äîis unlikely to be successful under differential privacy. However, the
goals of the challenge‚Äînamely, producing recommendations from collective user behavior‚Äîcould
be achievable while guaranteeing differential privacy. To explore this possibility, McSherry and
Mironov (2009) evaluate several of the algorithmic approaches used in the challenge, showing that
they could have been implemented in a differentially private manner (via privacy-preserving queries
issued against the database) without significant impact on their accuracy.
The Facebook goal‚Äîgiving advertisers a count of the number of times their ad was shown‚Äîat
first sounds as if it might be well suited to differential privacy: one could simply add an appropriate
level of Laplace noise to the true count. However, charging advertisers based on noisy counts may
be considered objectionable, and regardless, privacy would then degrade with the number of ad
campaigns (or, alternatively, Facebook would have to discontinue the service once they ran out of
a certain  budget they had committed to). Even if we assume that advertisers do not share the
statistics Facebook reports to them (and so perhaps each advertiser can be apportioned a separate
privacy budget rather than sharing a single budget among them all), large advertisers likely run so
many campaigns that the noise necessary in order to ensure any reasonable level of privacy would
swamp any signal in the data. Korolova (2011) suggests that an approach like differential privacy
would provide the most robust starting point for privately addressing Facebook‚Äôs goal, and discusses
these and other challenges that leave the targeted-ads application an intriguing open problem.
More generally, what tools and other thoughts could differential privacy potentially offer to
those of us who work with data?
While no standardized implementations yet exist, and while conventions (e.g., regarding setting
) have not yet been established, a rich set of theoretical results already provides the foundations
for a useful toolbox for the data-based researcher. If one would like to publish a single statistic (or
a small set of statistics), differentially private estimator versions might already exist. As discussed
above, the accuracy cost imposed by the added noise may be negligible when n is sufficiently
large. Regardless of whether the statistic of interest has received attention in the differential
privacy literature, the study of differential privacy suggests that it may be helpful to understand
the sensitivity of the statistic to changes in one person‚Äôs information‚Äîhow much can varying one
entry of the database affect the statistic? Such understanding not only helps assess how much noise

19

one could add to achieve differential privacy in the simplest manner; it is also helpful for getting an
intuitive understanding of how and why a statistic might be revealing. There are also techniques for
differentially private release of statistics that may not have low sensitivity in the worst case, but are
suspected to have low sensitivity on the data of interest (Nissim, Raskhodnikova and Smith, 2007;
Dwork and Lei, 2009). Finally, if one wishes to publish a large set of statistics or produce sanitized
data, as we discussed, general purpose techniques for doing so already exist, but it is possible
that a researcher‚Äôs particular properties of interest would be even better served by a specialized
differentially private mechanism.
The centrality of the notion of sensitivity to work on differential privacy highlights an old
truth from a new perspective: it underscores the importance of thinking about the robustness of
the statistics we report. If reporting a statistic while preserving privacy requires introducing an
unacceptable level of randomness, this may indicate that one‚Äôs dataset is too small for one‚Äôs desired
levels of privacy and accuracy; but it may also suggest that worst-case scenarios exist under which
the statistic is simply not robust: it may be too sensitive to potential individual outliers.
Finally, the differential privacy definition offers one way to quantify the often-loosely-used notions of privacy and anonymity. Researchers may find such quantification helpful in thinking
about whether study participants should be given a different, more qualified, promise of privacy/anonymity than is standardly given‚Äîespecially in settings where implementing a specific
guarantee (not necessarily the one offered by differential privacy) is not practical.

Limitations
Like any other rigorous approach, the differential privacy approach makes some assumptions that
may be questioned. For example, it assumes that an individual‚Äôs private data are conveniently
represented as a row in a database, and it implicitly assumes that a particular definition captures
what we mean by privacy.
Strong privacy guarantees necessarily obscure information. The intentional introduction of randomness into published outcomes may require adjustments to specific implementations of scientific
replication. More generally, for some applications, the very idea of deliberately introducing randomness is problematic: preventable mistakes such as allocating the wrong resources to the wrong
individuals or making the wrong policy decisions could have grave consequences.
As hinted above, a potential limitation of differentially private mechanisms producing synthetic
data is that they require the data analyst to specify her query set in advance. Many times, one

20

may not know in advance exactly which statistics one wishes to compute or what properties of
a dataset must be preserved in order for it to be useful. There is a natural tension between an
analyst‚Äôs desire to ‚Äúlook at the data‚Äù before deciding what to do with it, and a privacy researcher‚Äôs
desire that all computations that touch the original data be made formal and privacy-preserving.
As a practical response to this limitation, rather than attempting to define the query set a priori,
one could consider using some of the privacy budget for interactive queries where the analyst poses
queries one at a time and receives privacy-preserving answers, and may then base her choice of
future queries on the answer she has received so far. Once the analyst has established via this
sequence of interactive queries what properties of the original database to preserve in the sanitized
version, she can use the rest of her privacy budget to produce sanitized data. More generally,
with the growth of big data, the ‚Äúlook at the data‚Äù approach is destined to change‚Äî‚Äúlooking‚Äù
at enormous datasets really does mean running analyses on them, and as soon as ‚Äúlooking at the
data‚Äù has a technical meaning, one can try to enable it in a privacy-preserving manner.
Finally, for particular applications, differentially private mechanisms may not yet have been
developed, or the existing technology may not enable a satisfying privacy-accuracy tradeoff. When
no strong lower-bound results are known for the application of interest, these are not weaknesses
of the differential privacy definition, but merely suggest that more research is needed. Even when
lower bounds are known, in many cases they are not specific to differential privacy, but rather
reflect that certain tasks are inherently revealing and hence may be fundamentally incompatible
with privacy.

Differential Privacy and Mechanism Design
The last few years have seen a growth of interest in a number of topics at the intersection of differential privacy and economics, in particular, privacy and mechanism design; see Pai and Roth (2013)
for a survey. Some of the key questions under consideration include how one might incorporate
privacy considerations into utility functions and how one might model the value of privacy.17
From a mechanism design point of view, the differential privacy guarantee‚Äîthat a participant‚Äôs
inclusion or removal from the database would have almost no effect on the outcome‚Äîcould be
viewed as a valuable guarantee even in the absence of privacy concerns. In particular, consider
settings where participants in a database can misrepresent their individual data, and have pref17

Work in this area includes Ghosh and Roth (2011), Nissim, Orlandi and Smorodinsky (2012), Fleischer and Lyu
(2012), Roth and Schoenebeck (2012), Ligett and Roth (2012), Xiao (2013), Chen et al. (2013), and Ghosh and Ligett
(2013).

21

erences over the possible outcomes of a function to be computed from the data. A differentially
private computation implies that such participants have only limited incentive to lie: lying would
have only a limited effect on the outcome. McSherry and Talwar (2007) were the first to observe
this connection, i.e., that differential privacy implies asymptotic (or approximate) strategyproofness (or truthfulness). Of course, under differential privacy, not only do individuals have almost no
incentive to lie; they also have almost no incentive to tell the truth (Nissim, Smorodinsky and Tennenholtz, 2012; Xiao, 2013). However, a small psychological cost of lying could strictly incentivize
truth-telling.
For our purposes, in the context of data collection, analysis, and reporting, this approximate
truthfulness implication may be of particular interest to researchers who wish to gather survey
data in settings where participation is voluntary and the accuracy of responses cannot be easily
verified. More generally, the asymptotic strategyproofness implied by differential privacy inherits
some of the latter‚Äôs useful additional properties, such as group privacy and composition. Hence, it
provides immediate guarantees in the presence of k colluding individuals (a collusion resistance that
deteriorates with the coalition size k), and it holds under repeated application of the mechanism
(with the same deterioration with the number of repetitions).
Finally, this asymptotic truthfulness has inspired further work on privacy-preserving mechanism
design (Huang and Kannan, 2012; Kearns et al., 2012) and has enabled differential privacy to be
used as a tool in the design of truly strategyproof mechanisms (see, e.g., Nissim, Smorodinsky and
Tennenholtz, 2012).

Concluding Thoughts
Privacy concerns in the face of unprecedented access to big data are nothing new. More than thirtyfive years ago, Dalenius (1977) already discusses ‚Äúthe proliferation of computerized information
system[s]‚Äù and ‚Äúthe present era of public concern about ‚Äòinvasion of privacy‚Äô.‚Äù But as big data get
bigger, so do the concerns. Greely (2007) discusses genomic databases, concluding that
[t]he size, the cost, the breadth, the desired broad researcher access, and the likely
high public profile of genomic databases will make these issues especially important to
them. Dealing with these issues will be both intellectually and politically difficult, timeconsuming, inconvenient, and possibly expensive. But it is not a solution to say that
‚Äúanonymity‚Äù means only ‚Äúnot terribly easy to identify,‚Äù . . . or that ‚Äúinformed consent‚Äù

22

is satisfied by largely ignorant blanket permission.
Replacing ‚Äúgenomic databases‚Äù with ‚Äúbig data‚Äù in general, our overall conclusion may be similar.
The stories in the first part of this paper demonstrate that relying on intuition when attempting
to protect subject privacy may not be enough. Moreover, privacy failures may occur even when
the raw data are never publicly released and only some seemingly innocuous function of the data,
such as a statistic, is published. The purpose of these stories is to increase awareness.
The ideas from the differential privacy literature we introduce in the second part of this paper
provide one formal way for thinking about the notion of privacy that researchers may want to
guarantee to subjects. They also provide a framework, or a tool, for thinking quantitatively about
privacy-accuracy tradeoffs. We would like to see more such thinking among data-based researchers.
In particular, with computer scientists using phrases such as ‚Äúthe amount of privacy loss‚Äù and ‚Äúthe
privacy budget,‚Äù the time seems ripe for more economists to join the conversation. Is a certain
lifetime amount of  a basic right? Is privacy a term in the utility function that can in principle
be compared against the utility from access to accurate data? Should fungible, transferable  be
allowed to be sold in markets from private individuals to potential data users, and if so, what would
its price be? Should a certain privacy budget be allocated across interested users of publicly owned
(e.g., Census) data, and if so, how? Such questions are beginning to receive attention, as mentioned
above. Increased attention may eventually bring change to common practices.
What kind of changes could one envision? In the third part of our paper we discuss specific
applications of differential privacy to concrete situations, highlighting some limitations. When
big data means large n, an increasing number of common computations can be achieved in a
differentially private manner, with little cost to precision. It is not inconceivable that within a few
years, many of the computations that have been‚Äîand those that are yet to be‚Äîproven achievable
in theory, will be applied in practice. Echoing Dwork and Smith (2010), who ‚Äúwould like to see a
library of differentially private versions of the algorithms in R and SAS,‚Äù we would be happy to have
a differentially private option in estimation commands in STATA. But ready-to-use, commercialgrade applications will not be developed without sufficient demand from potential users. We hope
that the incorporation of privacy considerations into the vocabulary of empirical researchers will
help raise demand, and stimulate further discussion and research‚Äîincluding, we hope, regarding
additional approaches to privacy.
Until such applications are available, it might be wise to pause and reconsider researchers‚Äô
promises and, more generally, obligations to subjects. When researchers (and IRBs!) are confi23

dent that the data pose only negligible privacy risks‚Äîe.g., some innocuous small surveys and lab
experiments‚Äîit may be preferable to replace promises of anonymity with promises for ‚Äúnot terribly
easy‚Äù identification or, indeed, with no promises at all. In particular, researchers could explicitly
inform consenting subjects that a determined attacker may be able to identify them in posted data,
or even learn things about them merely by looking at the empirical results of a research paper. We
caution against taking the naive alternate route of simply refraining from making harmless data
publicly available; freedom of information, access to data, transparency, and scientific replication
are all dear to us.18 Of course, the tradeoffs, and in particular the question of what privacy risks
are negligible and what data are harmless, should be carefully considered and discussed; a useful
question to ask ourselves may resemble a version of the old newspaper test: would our subjects
mind if their data were identified and published in the New York Times?

References
Abowd, John M., Matthew J. Schneider, and Lars Vilhuber. 2013. ‚ÄúDifferential Privacy
Applications to Bayesian and Linear Mixed Model Estimation.‚Äù Journal of Privacy and Confidentiality, 5(1).
Barak, Boaz, Kamalika Chaudhuri, Cynthia Dwork, Satyen Kale, Frank McSherry, and
Kunal Talwar. 2007. ‚ÄúPrivacy, accuracy, and consistency too: a holistic solution to contingency
table release.‚Äù In Symposium on Principles of Database Systems. 273‚Äì282.
Barth-Jones, Daniel C. 2012. ‚ÄúThe ‚ÄòRe-Identification‚Äô of Governor William Weld‚Äôs Medical Information: A Critical Re-Examination of Health Data Identification Risks and Privacy Protections,
Then and Now.‚Äù http://ssrn.com/abstract=2076397.
Blum, Avrim, Katrina Ligett, and Aaron Roth. 2008. ‚ÄúA learning theory approach to noninteractive database privacy.‚Äù In Symposium on the Theory of Computing. 609‚Äì618.
Chandrasekaran, Karthekeyan, Justin Thaler, Jonathan Ullman, and Andrew Wan.
2013. ‚ÄúFaster Private Release of Marginals on Small Databases.‚Äù http://arxiv.org/pdf/1304.
3754v1.pdf.
18
Flood et al. (2013) provide a comprehensive discussion of such a transparency-confidentiality tradeoff in a context
that is very different from ours, yet of great interest to economists‚Äîthat of financial supervision and regulation.

24

Chaudhuri, Kamalika, and Daniel J. Hsu. 2012. ‚ÄúConvergence Rates for Differentially Private
Statistical Estimation.‚Äù In International Conference on Machine Learning. 1327‚Äì1334.
Chaudhuri, Kamalika, Claire Monteleoni, and Anand D. Sarwate. 2011. ‚ÄúDifferentially
private empirical risk minimization.‚Äù Journal of Machine Learning Research, 12: 1069‚Äì1109.
Chen, Yiling, Stephen Chong, Ian A. Kash, Tal Moran, and Salil P. Vadhan. 2013.
‚ÄúTruthful mechanisms for agents that value privacy.‚Äù In Conference on Electronic Commerce.
215‚Äì232.
Dalenius, Tore. 1977. ‚ÄúTowards a methodology for statistical disclosure control.‚Äù Statistisk tidskrift, 15: 429‚Äì444.
Dinur, Irit, and Kobbi Nissim. 2003. ‚ÄúRevealing information while preserving privacy.‚Äù In
Symposium on Principles of Database Systems. 202‚Äì210.
Dwork, Cynthia. 2006. ‚ÄúDifferential privacy.‚Äù In International Conference on Automata, Languages and Programming. 1‚Äì12.
Dwork, Cynthia. 2011a. ‚ÄúA firm foundation for private data analysis.‚Äù Communications of the
ACM, 54(1): 86‚Äì95.
Dwork, Cynthia. 2011b. ‚ÄúThe promise of differential privacy: A tutorial on algorithmic techniques.‚Äù In Foundations of Computer Science. 1‚Äì2.
Dwork, Cynthia, Aleksandar Nikolov, and Kunal Talwar. 2013. ‚ÄúEfficient Algorithms for
Privately Releasing Marginals via Convex Relaxations.‚Äù http://arxiv.org/pdf/1308.1385v1.
pdf.
Dwork, Cynthia, and Adam Smith. 2010. ‚ÄúDifferential privacy for statistics: What we know
and what we want to learn.‚Äù Journal of Privacy and Confidentiality, 1(2): 135‚Äì154.
Dwork, Cynthia, and Jing Lei. 2009. ‚ÄúDifferential privacy and robust statistics.‚Äù In Symposium
on Theory of Computing. 371‚Äì380.
Dwork, Cynthia, and Kobbi Nissim. 2004. ‚ÄúPrivacy-preserving datamining on vertically partitioned databases.‚Äù In International Conference on Cryptology (CRYPTO). 528‚Äì544.

25

Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006a. ‚ÄúCalibrating
noise to sensitivity in private data analysis.‚Äù In Theory of Cryptography Conference. 265‚Äì284.
Dwork, Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2011. ‚ÄúDifferential
Privacy: A Primer for the Perplexed.‚Äù In Joint UNECE/Eurostat Work Session on Statistical
Data Confidentiality. http://www.unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.
46/2011/26_Dwork-Smith.pdf.
Dwork, Cynthia, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
Naor. 2006b. ‚ÄúOur Data, Ourselves: Privacy Via Distributed Noise Generation.‚Äù In International
Conference on the Theory and Applications of Cryptographic Techniques (EUROCRYPT). 486‚Äì
503.
Fienberg, Stephen E., Alessandro Rinaldo, and Xiaolin Yang. 2010. ‚ÄúDifferential privacy
and the risk-utility tradeoff for multi-dimensional contingency tables.‚Äù In Privacy in Statistical
Databases. 187‚Äì199.
Fleischer, Lisa, and Yu-Han Lyu. 2012. ‚ÄúApproximately Optimal Auctions for Selling Privacy
when Costs are Correlated with Data.‚Äù In Conference on Electronic Commerce. 568‚Äì585.
Flood, Mark, Jonathan Katz, Stephen Ong, and Adam Smith. 2013. ‚ÄúCryptography
and the Economics of Supervisory Information: Balancing Transparency and Condentiality.‚Äù
Unpublished.
Ghosh, Arpita, and Aaron Roth. 2011. ‚ÄúSelling privacy at auction.‚Äù In Conference on Electronic Commerce. 199‚Äì208.
Ghosh, Arpita, and Katrina Ligett. 2013. ‚ÄúPrivacy and Coordination: Computing on
Databases with Endogenous Participation.‚Äù In Conference on Electronic Commerce. 543‚Äì560.
Golle, Philippe. 2006. ‚ÄúRevisiting the uniqueness of simple demographics in the US population.‚Äù
In Workshop on Privacy in Electronic Society. 77‚Äì80.
Greely, Henry T. 2007. ‚ÄúThe uneasy ethical and legal underpinnings of large-scale genomic
biobanks.‚Äù Annual Review of Genomics and Human Genetics, 8: 343‚Äì364.
Hardt, Moritz, and Guy N. Rothblum. 2010. ‚ÄúA Multiplicative Weights Mechanism for
Privacy-Preserving Data Analysis.‚Äù In Foundations of Computer Science. 61‚Äì70.
26

Hardt, Moritz, Katrina Ligett, and Frank McSherry. 2012. ‚ÄúA Simple and Practical Algorithm for Differentially Private Data Release.‚Äù In Advances in Neural Information Processing
Systems. 2348‚Äì2356.
Huang, Zhiyi, and Sampath Kannan. 2012. ‚ÄúThe Exponential Mechanism for Social Welfare:
Private, Truthful, and Nearly Optimal.‚Äù In Foundations of Computer Science. 140‚Äì149.
Kasiviswanathan, Shiva Prasad, and Adam Smith. 2008. ‚ÄúA Note on Differential Privacy:
Defining Resistance to Arbitrary Side Information.‚Äù http://arxiv.org/pdf/0803.3946v1.pdf.
Kasiviswanathan, Shiva Prasad, Mark Rudelson, Adam Smith, and Jonathan Ullman.
2010. ‚ÄúThe price of privately releasing contingency tables and the spectra of random matrices
with correlated rows.‚Äù In Symposium on the Theory of Computing. 775‚Äì784.
Kearns, Michael, Mallesh Pai, Aaron Roth, and Jon Ullman. 2012. ‚ÄúMechanism Design
in Large Games: Incentives and Privacy.‚Äù http://arxiv.org/abs/1207.4084.
Kifer, Daniel, Adam Smith, and Abhradeep Thakurta. 2012. ‚ÄúPrivate convex empirical risk
minimization and high-dimensional regression.‚Äù In Conference on Learning Theory. 25.1‚Äì25.40.
Klarreich,

Erica.

guarding

Data.‚Äù

2012.

‚ÄúPrivacy

Quanta

by

the

Magazine.

Numbers:

A

New

Approach

to

Safe-

https://www.simonsfoundation.org/quanta/

20121210-privacy-by-the-numbers-a-new-approach-to-safeguarding-data/.
Korolova, Aleksandra. 2011. ‚ÄúPrivacy Violations Using Microtargeted Ads: A Case Study.‚Äù
Journal of Privacy and Confidentiality, 3(1).
Korolova, Aleksandra, Krishnaram Kenthapadi, Nina Mishra, and Alexandros
Ntoulas. 2009. ‚ÄúReleasing search queries and clicks privately.‚Äù In Proceedings of the 18th international conference on World wide web. 171‚Äì180.
Kumar, Ravi, Jasmine Novak, Bo Pang, and Andrew Tomkins. 2007. ‚ÄúOn anonymizing
query logs via token-based hashing.‚Äù In Conference on the World Wide Web. 629‚Äì638.
Lei, Jing. 2011. ‚ÄúDifferentially private m-estimators.‚Äù In Advances in Neural Information Processing Systems. 361‚Äì369.
Ligett, Katrina, and Aaron Roth. 2012. ‚ÄúTake it or Leave it: Running a Survey when Privacy
Comes at a Cost.‚Äù In Workshop on Internet and Network Economics (WINE). 378‚Äì391.
27

Machanavajjhala, Ashwin, Daniel Kifer, John M. Abowd, Johannes Gehrke, and Lars
Vilhuber. 2008. ‚ÄúPrivacy: Theory meets Practice on the Map.‚Äù In International Conference on
Data Engineering. 277‚Äì286.
McSherry, Frank, and Ilya Mironov. 2009. ‚ÄúDifferentially private recommender systems: building privacy into the net.‚Äù In Conference on Knowledge Discovery and Data Mining. 627‚Äì636.
McSherry, Frank, and Kunal Talwar. 2007. ‚ÄúMechanism Design via Differential Privacy.‚Äù In
Symposium on Foundations of Computer Science (FOCS). 94‚Äì103.
Narayanan, Arvind, and Vitaly Shmatikov. 2008. ‚ÄúRobust de-anonymization of large sparse
datasets.‚Äù In Symposium on Security and Privacy. 111‚Äì125.
Nekipelov, Denis, and Evegeny Yakovlev. 2011. ‚ÄúPrivate extremum estimation.‚Äù http://
emlab.berkeley.edu/~nekipelov/pdf_papers/paper3.pdf.
Nissim, Kobbi, Claudio Orlandi, and Rann Smorodinsky. 2012. ‚ÄúPrivacy-aware mechanism
design.‚Äù In Conference on Electronic Commerce. 774‚Äì789.
Nissim, Kobbi, Rann Smorodinsky, and Moshe Tennenholtz. 2012. ‚ÄúApproximately optimal mechanism design via differential privacy.‚Äù In Innovations in Theoretical Computer Science
Conference. 203‚Äì213.
Nissim, Kobbi, Sofya Raskhodnikova, and Adam Smith. 2007. ‚ÄúSmooth sensitivity and
sampling in private data analysis.‚Äù In Symposium on Theory of Computing. 75‚Äì84.
Ohm, Paul. 2010. ‚ÄúBroken promises of privacy: Responding to the surprising failure of anonymization.‚Äù UCLA Law Review, 57: 1701‚Äì1777.
Pai, Mallesh, and Aaron Roth. 2013. ‚ÄúPrivacy and Mechanism Design.‚Äù Sigecom Exchanges,
12(1).
Roth, Aaron, and Grant Schoenebeck. 2012. ‚ÄúConducting Truthful Surveys, Cheaply.‚Äù In
Conference on Electronic Commerce. 826‚Äì843.
Rousseau, Peter L. 2013. ‚ÄúReport of the Secretary.‚Äù American Economic Review, 103(3): 669‚Äì72.
Rubinstein, Benjamin I.P., Peter L. Bartlett, Ling Huang, and Nina Taft. 2012. ‚ÄúLearning in a Large Function Space: Privacy-Preserving Mechanisms for SVM Learning.‚Äù Journal of
Privacy and Confidentiality, 4(1): 65‚Äì100.
28

Smith, Adam. 2008. ‚ÄúEfficient, differentially private point estimators.‚Äù http://arxiv.org/pdf/
0809.4794v1.pdf.
Smith, Adam. 2011. ‚ÄúPrivacy-preserving statistical estimation with optimal convergence rates.‚Äù
In Symposium on Theory of Computing. 813‚Äì822.
Sweeney, Latanya. 1997. ‚ÄúWeaving technology and policy together to maintain confidentiality.‚Äù
The Journal of Law, Medicine & Ethics, 25(2-3): 98‚Äì110.
Sweeney, Latanya. 2002. ‚ÄúAchieving k-anonymity privacy protection using generalization and
suppression.‚Äù International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,
10(05): 571‚Äì588.
Sweeney, Latanya. 2013. ‚ÄúMatching Known Patients to Health Records in Washington State
Data.‚Äù http://thedatamap.org/1089-1.pdf.
Sweeney, Latanya, Akua Abu, and Julia Winn. 2013. ‚ÄúIdentifying Participants in the Personal Genome Project by Name.‚Äù http://dataprivacylab.org/projects/pgp/1021-1.pdf.
Thakurta, Abhradeep Guha, and Adam Smith. 2013. ‚ÄúDifferentially Private Feature Selection via Stability Arguments, and the Robustness of the Lasso.‚Äù In Conference on Learning
Theory. 819‚Äì850.
Thaler, Justin, Jonathan Ullman, and Salil Vadhan. 2012. ‚ÄúFaster algorithms for privately
releasing marginals.‚Äù In Automata, Languages, and Programming. 810‚Äì821.
Vu, Duy, and Aleksandra Slavkovic. 2009. ‚ÄúDifferential privacy for clinical trial data: Preliminary evaluations.‚Äù In Conference on Data Mining Workshops. 138‚Äì143.
Wasserman, Larry, and Shuheng Zhou. 2010. ‚ÄúA statistical framework for differential privacy.‚Äù
Journal of the American Statistical Association, 105(489): 375‚Äì389.
Xiao, David. 2013. ‚ÄúIs privacy compatible with truthfulness?‚Äù In Innovations in Theoretical
Computer Science (ITCS). 67‚Äì86.

29

