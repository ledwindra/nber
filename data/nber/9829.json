{
    "id": 9829,
    "citation_title": "Are More Data Always Better for Factor Analysis?",
    "citation_author": [
        "Jean Boivin",
        "Serena Ng"
    ],
    "citation_publication_date": "2003-07-14",
    "issue_date": "2003-07-14",
    "revision_date": "None",
    "topics": [
        "\n",
        "Macroeconomics",
        "\n",
        "Business Cycles",
        "\n",
        "Money and Interest Rates",
        "\n"
    ],
    "program": [
        "\n",
        "Monetary Economics",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nFactors estimated from large macroeconomic panels are being used in an increasing number of applications. However, little is known about how the size and the composition of the data affect the factor estimates. In this paper, we question whether it is possible to use more series to extract the factors, and yet the resulting factors are less useful for forecasting, and the answer is yes. Such a problem tends to arise when the idiosyncratic errors are cross-correlated. It can also arise if forecasting power is provided by a factor that is dominant in a small dataset but is a dominated factor in a larger dataset. In a real time forecasting exercise, we find that factors extracted from as few as 40 pre-screened series often yield satisfactory or even better results than using all 147 series. Weighting the data by their properties when constructing the factors also lead to improved forecasts. Our simulation analysis is unique in that special attention is paid to cross-correlated idiosyncratic errors, and we also allow the factors to have stronger loadings on some groups of series than others. It thus allows us to better understand the properties of the principal components estimator in empirical applications.\n\n",
    "acknowledgement": "\n"
}