NBER WORKING PAPER SERIES

SELECTION WITH VARIATION IN DIAGNOSTIC SKILL:
EVIDENCE FROM RADIOLOGISTS
David C. Chan Jr
Matthew Gentzkow
Chuan Yu
Working Paper 26467
http://www.nber.org/papers/w26467

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
November 2019

We thank Hanming Fang, Amy Finkelstein, Karam Kang, Pat Kline, Jon Kolstad, Pierre-Thomas
Leger, Jesse Shapiro, Chris Walters, and numerous seminar and conference participants for
helpful comments and suggestions. We also thank Zong Huang, Vidushi Jayathilak, Kevin
Kloiber, Douglas Laporte, Uyseok Lee, Christopher Lim, and Lisa Yi for excellent research
assistance. The Stanford Institute for Economic Policy Research provided generous funding and
support. Chan gratefully acknowledges support from NIH DP5OD019903-01. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National
Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2019 by David C. Chan Jr, Matthew Gentzkow, and Chuan Yu. All rights reserved. Short
sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.

Selection with Variation in Diagnostic Skill: Evidence from Radiologists
David C. Chan Jr, Matthew Gentzkow, and Chuan Yu
NBER Working Paper No. 26467
November 2019
JEL No. C26,D81,I1,J24
ABSTRACT
Physicians, judges, teachers, and agents in many other settings differ systematically in the
decisions they make when faced with similar cases. Standard approaches to interpreting and
exploiting such differences assume they arise solely from variation in preferences. We develop an
alternative framework that allows variation in both preferences and diagnostic skill, and show
that both dimensions are identified in standard settings under quasi-random assignment. We apply
this framework to study pneumonia diagnoses by radiologists. Diagnosis rates vary widely among
radiologists, and descriptive evidence suggests that a large component of this variation is due to
differences in diagnostic skill. Our estimated model suggests that radiologists view failing to
diagnose a patient with pneumonia as more costly than incorrectly diagnosing one without, and
that this leads less-skilled radiologists to optimally choose lower diagnosis thresholds. Variation
in skill can explain 44 percent of the variation in diagnostic decisions, and policies that improve
skill perform better than uniform decision guidelines. Failing to account for skill variation can
lead to highly misleading results in research designs that use agent assignments as instruments.

David C. Chan Jr
Center for Health Policy and
Center for Primary Care and Outcomes Research
117 Encina Commons
Stanford, CA 94305
and NBER
david.c.chan@stanford.edu
Matthew Gentzkow
Department of Economics
Stanford University
579 Serra Mall
Stanford, CA 94305
and NBER
gentzkow@stanford.edu

Chuan Yu
Department of Economics
Stanford University
579 Serra Mall
Stanford, CA 94305
USA
chuanyu@stanford.edu

1

Introduction

In a wide range of settings, agents facing similar problems make systematically different choices.
Physicians differ in their propensity to choose aggressive treatments or order expensive tests, even
when facing observably similar patients (Chandra et al. 2011; Van Parys and Skinner 2016; Molitor
2017). Judges differ in their propensity to hand down strict or lenient sentences, even when facing
observably similar defendants (Kleinberg et al. 2018). Similar patterns hold for teachers, managers,
and police officers (Bertrand and Schoar 2003; Figlio and Lucas 2004; Anwar and Fang 2006). Large
literatures examine the sources and implications of such variation (Bloom and Van Reenen 2010;
Syverson 2011), and also use it as a source of quasi-random variation for studying the effects of
decisions on outcomes (e.g., Kling 2006; Aizer and Doyle 2015; Bhuller et al. 2016; Tsugawa et al.
2017; Dobbie et al. 2018).
In all such settings, we can think of the decision process in two steps. First, there is an evaluation
step in which decision-makers assess the likely effects of the possible decisions given the case before
them. Physicians seek to diagnose a patient’s underlying condition and assess the potential effects
of treatment, judges seek to determine the facts of a crime and the likelihood of recidivism, and so
on. We refer to the accuracy of these assessments as an agent’s diagnostic skill. Second, there is a
selection step in which the decision-maker decides what preference weights to apply to the various
costs and benefits in determining the decision. We refer to these weights as an agent’s preferences. In
a stylized case of a binary decision d ∈ {0,1}, we can think of the first step as ranking cases in terms
of their appropriateness for d = 1 and the second step as choosing a cutoff in this ranking.
While systematic variation in decisions could in principle come from either skill or preferences, a
large part of the prior literature we cite below assumes that agents differ only in the latter. This matters
for the welfare evaluation of practice variation, as variation in preferences would suggest inefficiency
relative to a social planner’s preferred decision rule whereas variation in skill need not. It matters for
the types of policies that are most likely to improve welfare, as uniform decision guidelines may be
effective in the face of varying preferences but counterproductive in the face of varying skill. And
it matters for research designs that use agents’ decision rates as a source of identifying variation, as
variation in skill will typically lead the key monotonicity assumption in such designs to be violated.
In this paper, we introduce a framework to separate heterogeneity in skill and preferences when
cases are quasi-randomly assigned, and apply it to study heterogeneity in pneumonia diagnoses made
by radiologists. Our framework starts with a classification problem in which both decisions and

1

underlying states are binary. As in the standard one-sided selection model, the outcome only reveals
the true state conditional on one of the two decisions. In our setting, the decision is whether to
diagnose a patient and treat her with antibiotics, the state is whether the patient has pneumonia,
and the state is only observed if the patient is not treated, since once a patient is given antibiotics it is
usually impossible to tell whether she actually had pneumonia or not. We refer to the share of patients
diagnosed as a radiologist’s diagnosis rate and the share of patients who leave with undiagnosed
pneumonia as her type II error rate.
We draw close connections between two different representations of agent decisions in this setting: (i) the reduced-form relationship between diagnosis rates and type-II error rates, which we
observe directly in our data; and (ii) the relationship between true and false positive rates, commonly
known as the receiver operating characteristic (ROC) curve. Insights from these representations clarify how the distribution of agent skill and preferences is identified under quasi-random assignment.
They also suggest testable restrictions imposed by the monotonicity conditions assumed in research
designs using agent assignments as instrumental variables. We note that the ROC curve has a natural
economic interpretation as a production possibilities frontier for “true positive” and “true negative”
diagnoses.
Pneumonia affects 450 million people and causes 4 million deaths every year worldwide (Ruuskanen et al. 2011). While it is more common and deadly in the developing world, it remains the
eighth leading cause of death in the US, despite the availability of antibiotic treatment (Kung et al.
2008; File and Marrie 2010). The primary method of diagnosing pneumonia is by chest X-ray, but
there is nevertheless considerable variability in the diagnosis of pneumonia based on the same chest
X-rays, both across and within radiologists (Abujudeh et al. 2010; Self et al. 2013).
More broadly, getting the right diagnosis is a central function of health care (Institute of Medicine
2015): It provides an explanation of a patient’s health problem and informs subsequent health care
decisions. While errors in diagnosis have, until recently, been a blind spot in health care delivery,
the potential impact of preventing or delaying appropriate treatment, or of prompting unnecessary or
harmful treatment, seems large. Diagnostic errors account for 7 to 17 percent of adverse events in
hospitals (Leape et al. 1991; Thomas et al. 2000). Postmortem examination research suggests that
diagnostic errors contribute to 9 percent of patient deaths (Shojania et al. 2003).
Using Veterans Health Administration (VHA) data on 5.5 million chest X-rays in the emergency
department, we examine variation in diagnostic decisions and outcomes related to pneumonia across
radiologists who are assigned imaging cases in a quasi-random fashion. We measure type II error
2

rates by the share of patients not diagnosed in the ED who have a subsequent pneumonia diagnosis
in the next 10 days. We begin by demonstrating significant variation in both diagnosis rates and type
II error rates across radiologists. Reassigning patients from a radiologist in the 10th percentile of
diagnosis rates to a radiologist in the 90th percentile would increase the probability of a diagnosis
from 6.3 percent to 11.2 percent. Reassigning patients from a radiologist in the 10th percentile of
type II error rates to a radiologist in the 90th percentile would increase the probability of a type II
error from 0 percent to 2.2 percent.
We then turn to the relationship between diagnosis rates and type II error rates. At odds with
the prediction of a standard model with no skill variation, we find that radiologists who diagnose at
higher rates actually have higher rather than lower type II error rates. Note that this means that the
unconditional probability of a missed diagnosis is increasing in the diagnosis rate—i.e., a patient who
arrives at the hospital and is assigned to a high-diagnosis radiologist is more likely to go home with
untreated pneumonia than one assigned to a low-diagnosis radiologist. This fact alone rejects the
hypothesis that all radiologists operate on the same production possibilities frontier, and it suggests a
large role for variation in skill. In addition, we find that there is substantial variation in the probability
of false negatives conditional on diagnosis rate. For the same diagnosis rate, a radiologist in the 90th
percentile of type II error rates has 2.2 percentage points higher type II error rate than a radiologist in
the 10th percentile.
This evidence suggests that interpreting our data through a standard model that ignores skill could
be highly misleading. At a minimum, it means that policies that focus on harmonizing diagnosis rates
could miss important gains in improving skill. Moreover, such policies could be counter-productive if
skill variation makes varying diagnosis rates optimal. If missing a diagnosis (a false negative) is more
costly than falsely diagnosing a healthy patient (a false positive), a radiologist with noisier diagnostic
information (less skill) may optimally diagnose more patients, and requiring her to do otherwise could
reduce efficiency. Finally, a standard research design that uses the assignment of radiologists as an
instrument for pneumonia diagnosis would fail badly in this setting. We show that our reduced-form
facts strongly reject the monotonicity conditions necessary for such a design. Applying the standard
approach would yield the nonsensical conclusion that diagnosing a patient with pneumonia (and thus
giving her antibiotics) makes her more likely to return to the emergency room with pneumonia in the
near future, and also increases her likelihood of adverse health events including mortality.
In the final part of the paper, we estimate a structural model of diagnostic decisions to permit
a more precise characterization of these facts. Following our conceptual framework, radiologists
3

first evaluate chest X-rays to form a signal of the underlying disease state and then select cases with
signals above a certain threshold to diagnose with pneumonia. Undiagnosed patients who in fact
have pneumonia will eventually develop clear symptoms, thus revealing false negative diagnoses.
But among cases receiving a diagnosis, those who truly have pneumonia cannot be distinguished
from those who do not. Radiologists may vary in their diagnostic accuracy, and each radiologist
endogenously chooses a threshold selection rule in order to maximize utility. Radiologist utility
depends on false negative and false positive diagnoses, and the relative utility weighting of these
outcomes may vary across radiologists.
We find that the average radiologist receives a signal that has a correlation of 0.84 with the patient’s underlying latent state, but that the diagnostic accuracy varies widely, from a correlation of
0.72 in the 10th percentile of radiologists to 0.93 in the 90th percentile. The disutility of missing
diagnoses is on average 8.07 times as high as that of an unnecessary diagnosis; this ratio varies from
6.79 to 9.43 between the 10th and 90th radiologist percentiles. Overall, 44 percent of the variation in
decisions and 83 percent of the variation in outcomes can be explained by variation in skill. We then
consider the welfare implications of counterfactual policies. While eliminating variation in diagnosis
rates always improves welfare under the (incorrect) assumption of uniform diagnostic skill, we show
that this policy may actually reduce welfare. In contrast, increasing diagnostic accuracy can yield
much larger welfare gains.
Finally, we document how diagnostic skill and type II error rates vary across groups of radiologists. In all groups, we find the same increasing relationship between diagnosis rates and type II
error rates. In some groups, such as older radiologists or radiologists with higher chest X-ray volume,
diagnostic accuracy is generally higher. More accurate radiologists tend to issue shorter reports of
their findings but spend more time generating those reports, suggesting that effort (rather than raw
talent alone) may contribute to radiologist skill. Aversion to false negatives tends to be negatively
related to radiologist skill.
Our strategy for identifying causal effects relies on quasi-random assignment of cases to radiologists. This assumption is particularly plausible in our emergency department setting because of
idiosyncratic variation in the arrival of patients and the availability of radiologists conditional on time
and location controls. In support of this assumption, we show that patients assigned to high- and
low-diagnosing radiologists are nearly identical across a range of observable characteristics. While
some of these small differences are statistically significant in our large sample, our key results are
invariant to the set of observables we include as controls. We also identify a subset of 44 out of 104
4

VHA health care stations (comprising 1.5 million chest X-rays) for which there is no statistically
significant evidence of imbalance, and show that our key results hold in this restricted sample.
Our findings relate most directly to a large and influential literature on practice variation in health
care (Fisher et al. 2003a,b; Institute of Medicine 2013). This literature has robustly documented
variation in spending and treatment decisions that has little correlation with patient outcomes. The
seeming implication of this finding is that spending in health care provides little benefit to patients
(Garber and Skinner 2008), a provocative hypothesis that has spurred an active body of research
seeking to use natural experiments to identify the causal effect of spending (e.g., Doyle et al. 2015).
In this paper, we build on Chandra and Staiger (2007) in investigating the possibility of heterogeneous
productivity (e.g., physician skill) as an alternative explanation. By exploiting the joint distribution of
decisions and outcomes, we find significant variation in productivity, which rationalizes a large share
of the variation in diagnostic decisions. The same mechanism may explain the weak relationship
between decision rates and outcomes observed in other settings.1
Perhaps most closely related to our paper are evaluations by Abaluck et al. (2016) and Currie and
MacLeod (2017), both of which examine diagnostic decision-making in health care. Abaluck et al.
(2016) assume that physicians have the same diagnostic skill (i.e., the same ranking of cases) but
may differ in where they set their thresholds for diagnosis. Currie and MacLeod (2017) assume that
physicians have the same preferences but may differ in skill. Also related to our paper is a recent
study of hospitals by Chandra and Staiger (2017), who allow for comparative advantage and different
thresholds for treatment but also assume a common ranking of cases. Relative to these papers, a key
difference of our study is that we use quasi-random assignment of cases to providers.
Our paper also contributes to the “judges-design” literature, which estimates treatment effects by
exploiting quasi-random assignment to agents with different treatment propensities (e.g., Kling 2006).
We show how variation in skill relates to the standard monotonicity assumption in the literature, which
requires that all agents order cases in the same way but may draw different thresholds for treatment
(Imbens and Angrist 1994; Vytlacil 2002). Monotonicity can thus only hold if all agents have the
same skill. Our empirical insight that we can test and quantify violations of monotonicity (or variation
in skill) relates to conceptual work that exploits bounds on potential outcome distributions (Kitagawa
2015) and more recent work to test instrument validity in the judges design (Frandsen et al. 2019) and
1 For example, Kleinberg et al. (2018) finds that the increase in crime associated with judges that are more likely to
release defendants on bail is about the same as if these more lenient judges randomly picked the extra defendants to release
on bail. Arnold et al. (2018) finds a similar relationship for black defendants being released on bail. Judges that are most
likely to release defendants on bail in fact have slightly lower crime rates than judges that are less likely to grant bail.

5

to detect inconsistency in judicial decisions (Norris 2019).2
The remainder of this paper proceeds as follows. Sections 2 sets up a high-level empirical framework for our analysis. Section 3 describes the setting and data. Section 4 presents our reduced-form
analysis, with the key finding that radiologists who diagnose more cases also miss more cases of
pneumonia. Section 5 presents our structural analysis, separating radiologist diagnostic skill from
preferences. Section 6 considers policy counterfactuals. Section 7 concludes.

2

Empirical Framework

2.1

Setup

We consider a selection problem in which an agent j makes a binary decision di j ∈ {0,1} for a case i
(e.g., treat or not treat, convict or acquit). The goal is to align the decision with a binary state si ∈ {0,1}
(e.g., sick or healthy, guilty or innocent). The agent observes a signal wi j that is informative about
the underlying state si of the case. She then chooses di j based on this signal.
We define an agent’s diagnostic skill to be the informativeness of wi j in the Blackwell (1953)
sense, and we say that two radiologists have equal skill if their signal distributions are equal in informativeness.3 A population of agents has uniform skill if all of the agents have equal skill; otherwise,
we say that they vary in skill. We define an agent’s preferences to be the factors that determine her
choice of di j conditional on wi j . Assuming complete and transitive preferences over signals, we can

without loss of generality assign scalar values to wi j such that di j = 1 wi j > τj .
It will be helpful to represent this problem in the well-known framework of statistical classification. Panel A in Figure 1 illustrates a standard “classification matrix” representing the probabilities of four joint outcomes depending on decisions and states. For a given agent j with possibly
imperfect information and a decision rule, we can define the probabilities of four outcomes: true


negatives, or T N j ≡ Pr di j = 0,si = 0 ; false negatives or F N j ≡ Pr di j = 0,si = 1 ; true positives, or


T P j ≡ Pr di j = 1,si = 1 ; and false positives, or FP j ≡ Pr di j = 1,si = 0 . The agent’s diagnosis rate
is P j ≡ T P j + FP j , and her type-II error rate is simply F N j .
2 Kitagawa (2015) develops a test of instrument validity based on an older insight in the literature noting that instrument

validity implies non-negative densities of compliers for any potential outcome (Imbens and Rubin 1997; Balke and Pearl
1997; Heckman and Vytlacil 2005). Recent work by Machado et al. (2019) also exploits bounds in a binary outcome to test
instrument validity and to sign average treatment effects.
3 Note that the Blackwell ordering is incomplete, and agents who vary in skill may not be ordered by skill. Agent j’s
signal may be neither more nor less informative than the signal of agent j 0 , for example, if j has more accurate information
about some types of patients while j 0 has more accurate information about other types of patients.

6

2.2

ROC Curves and Agent Skill

A standard way to summarize the accuracy of classification is in terms of the receiver operating

TP
characteristic (ROC) curve. This plots the true positive rate, or T PR j ≡ Pr di j = 1 |si = 1 = T P j +Fj N j ,

FP
against the false positive rate, or FPR j ≡ Pr di j = 1 |si = 0 = F P j +Tj N j . Panel B in Figure 1 shows
several possible ROC curves.
Each agent j can be associated with a single ROC curve, which gives the set of classification
outcomes she can achieve taking as given her population of cases and the distribution of her signal
wi j . If she diagnoses no case, she will have T PR j = 0 and FPR j = 0. If she diagnoses all cases, she
will have T PR j = 1 and FPR j = 1. As she increases P j , both T PR j and FPR j must weakly increase

under the threshold rule di j = 1 wi j > τj . The ROC curve thus reveals a technological tradeoff
between the “sensitivity” (or T PR j ) and “specificity” (or 1 − FPR j ) of classification.
Higher ROC curves correspond to greater skill. By the definition of Blackwell (1953) informativeness, if j has higher skill than j 0, any outcome that is feasible for j 0 is also feasible for j. This
means that j’s ROC curve lies everywhere above that of j 0, and that j 0 can achieve higher utility with
access to j’s technology regardless of her preferences. Finally, if agents have equal skill, their ROC
curves must be identical.
Remark 1. The ROC curve of agent j lies everywhere above the ROC curve of agent j 0 if and only if
j has higher skill than j 0. If j and j 0 have equal skill, their ROC curves are identical.
This framework for selection is closely linked with the standard economic framework of production. An ROC curve can be viewed as a production possibilities frontier of T PR j and 1 − FPR j .
Agents on higher ROC curves are more productive (i.e., more skilled) in the evaluation stage. Where
an agent chooses to locate on an ROC curve is determined by her preferences, or the tangency between the ROC curve and an indifference curve. It is possible that agents differ in preferences but not
skill, so that they would lie along identical ROC curves, and we would observe a positive correlation
between T PR j and FPR j . It is also possible that they differ in skill but not preferences, so that they
would lie at the tangency point on different ROC curves, and we could observe a negative correlation
between T PR j and FPR j . Figure 2 illustrates these two cases with hypothetical data on the joint
distribution of decisions and outcomes. This figure suggests some intuition, which we will formalize
later, for how skill and preferences may be separately identified.
In the empirical analysis below, we will visualize the data in two different spaces. The first
is the ROC space of Figure 2. The second is a plot of false negative rates F N j against diagnosis

7


rates P j , which we will refer to as “reduced-form space.” Note that F N j = 1 − T PR j S j and P j =

T PR j S j + FPR j 1 − S j , where S j ≡ Pr (si = 1| j (i) = j). When cases are randomly assigned so that
S j is the same for all agents, this implies a tight correspondence between these two ways of looking
at the data.
Remark 2. Suppose S j is equal to a constant S for all j. Then

1. Conditional on S, there is a one-to-one correspondence between points T PR j , FPR j in ROC

space and points F N j , P j in reduced-form space.
2. If agents have uniform skill, type II error rates F N j decrease in diagnosis rates P j in reducedform space, with a slope bounded between 0 and −1.
We can thus use variation in reduced-form space to make inferences about agent skill. If agents
can be ordered in terms of skill, and if they face the same population of cases, we can infer that
radiologist j has lower skill than j 0 if

F N j −F N j 0
P j −P j 0

> 0 or has higher skill than j 0 if

thermore, we can obtain stronger restrictions on admissible slopes

F N j −F N j 0
P j −P j 0

F N j −F N j 0
P j −P j 0

< −1. Fur-

between any radiologist

pair ( j, j 0) who have equal skill. First, if incremental diagnoses match the underlying state at least as
well as random decisions, then ROC curves should lie above the 45-degree line in Panel B of Figure
1, and

F N j −F N j 0
P j −P j 0

< −S. Second, if agents choose optimally to minimize a weighted average of F N j

and FP j , then admissable slopes connecting agents with uniform skill in reduced-form space should
not only be negative but also convex, and ROC curves should be concave.4

2.3

Potential Outcomes and the Judges Design


When there is an outcome of interest yi j = yi di j that depends on the agent’s decision di j , we can
map our classification framework to the potential outcomes framework with heterogeneous treatment
effects (Rubin 1974; Imbens and Angrist 1994). In the case where di j is a judge’s bail decision, yi j
might be an indicator for whether a defendant commits a subsequent crime. In the case where di j
is a medical treatment decision, yi j might be a measure of subsequent health outcomes or mortality.
The object of interest is some average of the treatment effects yi (1) − yi (0) across individuals. We
observe case i assigned to only one agent j(i), so the identification challenge is that we only observe
Í
Í
di ≡ j 1 ( j = j (i)) di j and yi ≡ j 1 ( j = j (i)) yi j = yi (di ) corresponding to j = j (i).
4 In economics, the selection literature generally refers to rational expectations and utility maximization as “selection on

gains” or “Roy
and Honore 1990). Specifically, under utility ui j di j , j chooses di j = 1 for case i if
 selection” (Heckman

and only if E ui j (1) − ui j (0) > 0 (Cornelissen et al. 2016). In classification decisions, we may state ui j di j as u j di j ,si ,
such that u j (1,1) ≥ u j (0,1) and u j (0,0) ≥ u j (0,1) for all j. This implies linear indifference curves in ROC space, and agents
will never choose (FPR,T PR) outcomes within the convex hull of feasible (FPR,T PR).

8

A growing literature starting with Kling (2006) has proposed using heterogeneous decision propensities of agents to identify these average treatment effects in settings where cases i are randomly
assigned to agents j with different propensities of treatment. This empirical structure is popularly
known as the “judges design,” as early applications were to settings where the agents were judges.
The literature typically assumes conditions of instrumental variable (IV) validity from Imbens and
Angrist (1994).5
Condition 1 (IV Validity). Consider the potential outcome yi j and the treatment response indicator
di j ∈ {0,1} for case i under judge j. Case i is assigned to judge j (i). For a random sample of i and
j, the following conditions hold:
(i) Exclusion: yi j = yi (di j ) with probability 1.

(ii) Independence: yi (0), yi (1),di j is independent of j(i).
(iii) Strict Monotonicity: For any j and j 0, di j ≥ di j 0 ∀i, or di j ≤ di j 0 ∀i, with probability 1.
Vytlacil (2002) shows that Condition 1(iii) is equivalent to all agents ordering cases by the same

latent index wi and then choosing di j = 1 wi > τj , where τj is an agent-specific cutoff. Lower cutoffs
must correspond to weakly higher rates of both true and false positives. This condition thus greatly
restricts the pattern of outcomes in the classification framework.
Remark 3. Suppose Condition 1 holds. Then the observed data must be consistent with all agents
having uniform skill. By Remark 2, this implies that type II error rates must be decreasing in diagnosis
rates with a slope bounded between 0 and −1.
An alternative way to see the same intuition is to note that for any outcome yi j the Wald estimand


Yj −Y 0
comparing a population of cases assigned to agents j and j 0 is P j −Pj 0 = E yi (1) − yi (0)| di j > di j 0 ,
j

where Yj is the average of yi j among cases treated by j. If we define yi to be an indicator for a


false negative, or yi = f ni = 1 (di = 0,si = 1), we have E yi (1) − yi (0)| di j > di j 0 ∈ [−1,0], since
yi (1) − yi (0) ∈ {−1,0}.
By Remark 3, strict monotonicity in Condition 1(iii) of the judges design implies uniform skill.
The converse is not true, however. It is possible for agents to have uniform skill yet violate strict
monotonicity. A simple example would be if the agents’ signals wi j are distributed identically but
contain independent noise. This is a violation because strict monotonicity requires agents to order all
cases the same way with probability one.
5 In



addition to the assumption below, we also require instrument relevance, such that Pr di j = 1 , Pr di j 0 = 1 for
0
some j and j . This requirement can be assessed by a first stage regression of di on judge indicators.

9

One might ask whether a condition weaker than strict monotonicity might be both consistent with
our data and sufficient for the judges design to recover a well-defined local average treatment effect
(LATE). A more realistic condition might allow for idiosyncratic noise in the diagnostic signals that
agents receive, and require only that the probability that j diagnoses a patient is either higher or lower
than the probability j 0 diagnoses a patient for all i. A yet weaker condition would allow for systematic
variation in the way agents order cases (and thus the relative probability that different agents diagnose
different patients), provided that differences in ordering (e.g., due to varying skill) are orthogonal to
agents’ diagnostic propensities. In Appendix A.1, we define these conditions formally and show that
they are indeed sufficient for the judges design to recover a well defined LATE.6 We also show that
this weaker concept of monotonicity yields a testable implication.
Remark 4. Suppose that skill is not uniform but is independent of agents’ diagnostic propensities.
Then a regression of F N j on P j should yield a coefficient ∆ ∈ [−1,0].
This implies that the results we will show below reject not only the strict monotonicity of Condition 1(iii) but also the weaker monotonicity conditions as well. Not only can we reject uniform skill,
but skill must be systematically correlated with diagnostic propensities. In Section 5, we show that we
should expect these monotonicity conditions to be violated in our structural model: when radiologists
differ in skill and are aware of these differences, the optimal diagnostic threshold should depend on
radiologist skill. We also show that this relationship between skill and radiologist-chosen diagnostic
propensities raises the possibility that common diagnostic thresholds may reduce welfare.

3

Setting and Data

We apply our framework to study pneumonia diagnoses in the emergency department (ED). Pneumonia is a common and potentially deadly disease that is primarily diagnosed by chest X-rays. Reading
chest X-rays requires skill, as illustrated in Figure 3 from the medical literature. We focus on outcomes we observe from chest X-rays performed in the ED in the Veterans Health Administration
(VHA), the largest health care delivery system in the US.
In this setting, the diagnostic pathway for pneumonia is as follows:
1. A physician orders a radiology exam for a patient suspected to have the disease.
6 In Appendix A.1, we discuss the relationship of these monotonicity conditions to the “average monotonicity” concept
of Frandsen et al. (2019).

10

2. Once the radiology exam is performed, the image is assigned to a radiologist. Exams are typically assigned to radiologists based on whoever is on call at the time the exam needs to be read.
We argue below that this assignment is quasi-random conditional on appropriate covariates.
3. The radiologist issues a report on her findings.
4. The patient may be diagnosed and treated by the ordering physician in consultation with the
radiologist.
Pneumonia diagnosis is a joint decision by radiologists and physicians. Physician assignment to patients may be non-random, and physicians can affect diagnosis both via their selection of patients to
order X-rays for in step 1 and their diagnostic propensities in step 4. However, so long as assignment
of radiologists in step 2 is as good as random, we can accurately measure the causal effect of radiologists on the probability that the joint decision-making process leads to a diagnosis. While interactions
between radiologists and ordering physicians are interesting, we abstract from them in this paper and
focus on a radiologist’s average effect, taking as given the set of physicians with whom she works.
VHA facilities are divided into local units called “stations.” A station typically has a single major
tertiary care hospital and a single ED location, together with some medical centers and outpatient
clinics. These locations share the same electronic health record and order entry system. We study the
103 VHA stations that have at least one ED.
Our primary sample consists of the roughly 5.5 million completed chest X-rays in these stations
that were ordered in the ED and performed between October 1999 and September 2015.7 We refer to
these observations as “cases.” Each case is associated with a patient and with a radiologist assigned
to read it. In the rare cases where a patient received more than one X-ray on a single day, we assign
the case to the radiologist associated with the first X-ray observed in the day.
To define our main analysis sample, we first omit the roughly 600,000 cases for which the patient
had at least one chest X-ray ordered in the ED in the previous 30 days. We then omit cases that:
(i) have missing radiologist identity; (ii) have missing patient age or gender; (iii) are associated with
patients older than 100 or younger than 20; (iv) are associated with a radiologist-month pair with
fewer than 5 observations; (v) are associated with a radiologist with fewer than 100 observations in
total. In Appendix Table A.1 we report the number of observations dropped at each of these steps.
The final sample contains 4,663,826 cases.
7 We

define chest X-rays by the Current Procedural Terminology codes 71010 and 71020.

11

We define the diagnosis indicator di for case i equal to one if the patient has a pneumonia diagnosis
recorded in outpatient or inpatient within a 24-hour window centered at the time stamp of the chest
X-ray order.8 We confirm that 92.6 percent of patients who are recorded to have a diagnosis of
pneumonia are also prescribed an antibiotic consistent with pneumonia treatment within five days
after the chest X-ray.
We define an indicator f ni = 1 (di = 0,si = 1) for a type II error or “missed diagnosis” for case
i equal to one if di = 0 and the patient has a subsequent pneumonia diagnosis recorded between 12
hours and 10 days after the completion of the chest X-ray. Here we include diagnoses in both ED and
non-ED facilities, including outpatient, inpatient, and surgical encounters, as well as encounters that
began as transfers from other facilities.
We define the following patient characteristics for each case i: demographics (age, gender, marital
status, religion, race, veteran status, and distance from home to the VA facility where the X-ray
is ordered), prior health care utilization (counts of outpatient visits, inpatient admissions, and ED
visits in any VHA facility in the previous 365 days), prior medical comorbidities (indicators for
prior diagnosis of pneumonia and 31 Elixhauser comorbidity indicators in the previous 365 days),
vital signs (22 variables including blood pressure, pulse, pain score, and temperature), and and white
blood cell (WBC) count as of ED encounter.9 We also measure for each case a vector of characteristics
associated with the chest X-ray request. This contains an indicator for whether the request was marked
as urgent and a vector of requesting physician characteristics that we define below.
For each radiologist in the sample, we record gender, the date of birth, the start date of employment at the VHA, medical school identity, and the proportion of radiology exams that are chest
X-rays. For each chest X-ray in the sample, we record the time that a radiologist spends to generate
the report in minutes and the length of the report in words. For each requesting physician in the sample, we record the number of X-rays ordered across all patients, an above-/below-median indicator for
the average predicted diagnosis rate, and an above-/below-median indicator for the average predicted
type II error rate. The predicted diagnosis rate and type II error rate are formed by running a linear
8 Diagnoses do not have time stamps per se but are instead linked to visits, with time stamps for when the visits begin.
Therefore, the time associated with diagnoses is usually before the chest X-ray order; in a minority of cases, a secondary
visit (e.g., an inpatient visit) occurs shortly after the initial ED visit, and we will observe a diagnosis time after the chest
X-ray order. We include International Classification of Diseases, Ninth Revision, (ICD-9) codes 480-487 for pneumonia
diagnosis.
9 The vital sign variables are systolic blood pressure, diastolic blood pressure, pulse rate, pain score, pulse oximetry,
respiration rate, temperature, an indicator for fever, an indicator for whether there is supplemental oxygen administration,
and given it is provided, the flow rate and the concentration of the supplemental oxygen. If a case has multiple vital sign
measures, we use the first measure recorded. We include WBC count in this group of variables for compactness, though it
is not a vital sign. We also include indicators for missing values in each of these variables.

12

probability regression of di and f ni , respectively, on the demographic variables described above and
calculating the linear fit for each patient. We then average the predictions within each requesting
physician and divide all requesting physicians into above-/below-median groups.

4

Model-Free Analysis

4.1

Quasi-Random Assignment

To study the effect of radiologists on diagnoses and type II errors, we require that patients are as good
as randomly assigned to radiologists. Let Ti be a vector consisting of indicators for the hour of day,
day of week, and month-year of patient visit i. Let ` (i) denote the station (i.e., the specific ED) that i
visits, J`(i) denote the set of radiologists at that station, and j (i) ∈ J`(i) denote the radiologist assigned
to i.
Assumption 1 (Conditional Independence). Conditional on station ` (i) and time of visit Ti , the

state si and potential diagnosis decisions di j j ∈J for patient i are independent of the patient’s
`(i)

assigned radiologist j (i).
Our qualitative research suggests that the typical pattern is for patients to be assigned sequentially to available radiologists at the time their physician orders the chest X-ray. Such assignment
will plausibly satisfy Assumption 1 if the timing of patient arrival at the ED is independent of radiologist availability, conditional on interactions between ` (i) and Ti that capture regular variation in
scheduling (e.g., Chan 2018).
To assess Assumption 1, we report balance on observable characteristics between patients assigned to radiologists with above- vs. below-median diagnosis rates and type II error rates. We first
divide radiologists into above- and below-median groups based on the radiologist fixed effects from
regressions of diagnosis and type-II error rates on the vector of patient characteristics, controlling
for all patient characteristics and interactions between ` (i) and Ti . We next compute predicted values from patient-level regressions of diagnosis and type II error indicators on subsets of 77 patient
characteristic variables. We divide these variables into 5 groups: demographics, prior utilization,
prior diagnoses, vital signs and WBC count, and ordering characteristics. We then compute residuals
from regressions of these predicted values on ` (i) and Ti interactions, and we assess balance in these
residual predictions between groups of radiologists. Appendix A.2.1 provides further details.
Table 1 shows that the actual diagnosis and type II error rates differ substantially between these
13

groups as expected. In contrast, the differences in predicted values based on patient characteristics are
one to two orders of magnitude smaller, regardless of the characteristics used to form these predictions. Given the large size of our sample, some of these differences are statistically significant despite
their small size economically. In our main analyses, we will control for all patient observables used
in Table 1, and in Section 4.4, we will show that our results are qualitatively unchanged regardless of
which patient characteristics that we control for.
A complementary approach would be to isolate a subset of stations where evidence for balance
is even stronger. Because organization and procedures differ across stations, there is reason to think
that we may capture better conditioning sets for quasi-random assignment in some stations but not
in others.10 In Appendix A.2.2, we evaluate quasi-random assignment station-by-station using parametric tests of joint significance and randomization inference. The concordance between these tests
is high. We begin by focusing just on patient age as an observable and identify 44 out of 104 stations
for which we do not see any significant imbalance. We then show in Appendix Table A.2 that these
same 44 stations also appear balanced on the full set of 77 patient characteristic variables. We show
below that our main results are robust to focusing on these 44 stations.

4.2

Identification and Empirical Strategy

The first goal of our descriptive analysis is to flexibly identify the four elements of the classification
matrix in Figure 1 Panel A for each radiologist. This will allow us to plot the actual data in both
reduced-form space and in ROC space as in Figure 2.
The challenge is that we do not observe all four elements: For each radiologist, we observe sample
estimates of the diagnosis rate P j , the false negative probability F N j , and the remaining true negative
probability T N j . These would be sufficient to estimate the full matrix if we also knew the share of j’s
patients who had pneumonia S j = Pr ( si = 1| j (i) = j) since
T Pj

= Sj − F Nj ;

(1)

FP j

= P j − T P j ; and

(2)

T Nj

= 1 − F N j − T P j − FP j .

(3)

10 In our qualitative research,

we identify at least two types of conditioning sets that are unobserved to us. One is that the
population of radiologists in some stations includes both “regular” radiologists who are assigned chest X-rays according
to the normal sequential protocol and other radiologists who only read chest X-rays when the regular radiologists are not
available or in other special circumstances. A second is that some stations consist of multiple sub-locations, and both
patients and radiologists sort systematically to sub-locations. Since our fixed effects do not capture either radiologist
“types” or sub-locations, either of these could lead Assumption 1 to be violated.

14

Under Assumption 1, S j will be equal to the overall population share S ≡ Pr (si = 1) for all j.
Thus, knowing S would be sufficient for identification. Moreover, the observed data also provide
bounds on the possible values of S. If there exists a radiologist j such that P j = 0, we would be
able to learn S exactly as S = S j = F N j . Otherwise, letting j denote the radiologist with the lowest
h
i
diagnosis rate (i.e., j = arg min j P j ) we must have S ∈ F N j , F N j + D j . We show in Section 5.2 that
S is point identified under the additional functional form assumptions of our structural model.
The second goal of our descriptive analysis is to estimate the relationship between radiologists’
diagnosis rates P j and their type-II error rates F N j . We focus on the coefficient ∆ from a patientweighted regression of F N j on P j in the population of radiologists. By Remark 4, ∆ ∈ [−1,0] is a
necessary condition for both the standard monotonicity of Condition 1(iii) and the weaker versions of
monotonicity we consider as well. In order for ∆ < [−1,0], radiologists must not have uniform skill,
and skill must be systematically correlated with diagnostic propensities.
Exploiting quasi-experimental variation under Assumption 1, we can recover a consistent estimate
of ∆ from a 2SLS regression of f ni = 1 (di = 0,si = 1) on di instrumenting for the latter with j (i). In
these regressions, we control for a full set of interactions between station ` (i) and time categories Ti
as well as the vector Xi of 77 patient characteristics described in Section 4.1.
We consider two types of instruments. First, we simply use radiologist dummies. Second, we follow the standard practice in the judges-design literature by using a jackknife instrument of diagnosis
rates:
Zi =

1

Õ

I j(i) − 1 i0,i


1 i 0 ∈ I j(i) di0 ,

(4)

where I j is the set of patients assigned to radiologist j. The intuition behind the jackknife instrument is
that it prevents overfitting the first stage in finite samples, which would otherwise bias the coefficient
toward an OLS estimate of the relationship between f ni and di (Angrist et al. 1999).

4.3

Results

Figure 4 shows radiologist-specific true positive rates and false positive rates based on data of radiologistspecific diagnoses and false negatives. For this figure, we use an estimate of S = 0.0374 as well as
other disease-specific parameters that we detail later in Section 5.11 The results show clearly that the
11 In Section 5, we introduce three disease-related parameters: the proportion of chest X-rays that are not at risk for
pneumonia, κ; the proportion of at-risk chest X-rays with detectable pneumonia, 1−Φ (ν); and the proportion of at-risk cases
without detectable pneumonia at the time who subsequently develop pneumonia, λ.
 For a given observed P j , F N j , we
calculate the following adjustments: S 0 = 1−Φ (ν); P j0 = P j /(1 − κ); T N j0 = T N j − κ /(1 − κ) /(1 − λ); F N j0 = F N j /(1 − κ)−


λT N j0 ; T PR j = 1 − F N j0 /S 0 ; and FPR j = P j0 + F N j0 − S 0 /(1 − S 0 ). We assume κ = 0.196, λ = 0.021, and ν = 1.781.

15

data are inconsistent with the assumption of uniform skill.
Figure 5 shows the IV estimate as the slope in binned scatter plots, using radiologist dummies
as instruments (Panel A) and using the jackknife instrument (Panel B).12 The IV coefficient is significantly positive in both cases. Under Assumption 1, this implies that the monotonicity conditions
discussed above cannot hold in our data.
The strong upward slope shown in these plots is striking. It implies that the false negative rate is
higher for high-diagnosing radiologists not only conditionally (in the sense that the patients they do
not diagnose are more likely to have pneumonia) but unconditionally as well. Thus, being assigned
to a radiologist who diagnoses patients more aggressively increases the likelihood of leaving the
hospital with undiagnosed pneumonia. The only explanation for this under our framework is that
high-diagnosing radiologists have less accurate signals, and that this is true to a large enough degree
to offset the mechanical negative relationship between diagnosis and type II errors.
In Appendix Figure A.3 we show the full visual IV scatterplot corresponding to Panel A of Figure
5. This plot reveals substantial heterogeneity in type II error rates among radiologists with similar
diagnosis rates. This provides further evidence against the standard monotonicity assumption, which
implies that all radiologists with a given diagnosis rate must also have the same type-II error rate.
In Appendix A.4, we show that our data pass informal tests of monotonicity that are standard in
the literature (Bhuller et al. 2016; Dobbie et al. 2018). These tests require that diagnosis consistently
increases in P j in a range of patient subgroups.13 Thus, together with evidence of quasi-random
assignment in Section 4.1, the standard empirical framework would suggest this as a plausible setting
in which to use radiologist assignment as an instrument for the treatment variable di j .
Yet, were we to apply the standard approach and use radiologist assignment as an instrument to
estimate an average effect f ni (1) − f ni (0) of diagnosis di j on type II errors, we would reach the
nonsensical conclusion that diagnosing a patient with pneumonia (and thus giving them antibiotics)
makes them more likely to return with untreated pneumonia in the following days. Appendix Table
A.3 shows similar judges-design results for other welfare-relevant outcomes, such as mortality and
intensive care unit (ICU) stays. Applying the standard approach to these outcomes suggests that
diagnosing and treating pneumonia implausibly increases mortality, repeat ED visits, patient-days in
the hospital, and ICU admissions. We find increases in counts of adverse events even conditional on
12 We

discuss details of producing binned scatter plots to reflect the IV estimate in Appendix A.3.
this appendix, we also show the relationship between these standard tests and our test. We discuss that these results
suggest that: (i) radiologists consider unobserved patient characteristics in their diagnostic decisions; (ii) these unobserved
characteristics predict si ; and (iii) their use distinguishes high-skilled radiologists from low-skilled radiologists.
13 In

16

patients having type II errors, suggesting that skill could impact important outcomes not only through
the diagnosis decision but through other channels as well.14

4.4

Robustness

In Section 4.1, we detect small violations of quasi-random assignment (Assumption 1) in the overall
sample of stations; in Appendix A.2.2, we also show evidence that quasi-random assignment appears
to be satisfied statistically in 44 out of 104 stations, while we can reject quasi-random assignment in
the remainder of stations. With violations of quasi-random assignment, radiologists could systematically have higher probabilities of both diagnosis and false negatives not because they are less skilled
but because they are assigned more severe cases. Therefore, we examine the robustness of our results
to varying controls for patient characteristics as well as the set of stations we consider.
To examine robustness to controlling for patient characteristics, we first divide our 77 patient
characteristics into 10 groups: (i) age and gender; (ii) marital status; (iii) religion indicators (3 variables); (iv) veteran status (given that some patients are relatives of veterans); (v) race indicators (5
variables); (vi) distance between the patient’s residence and the closest VHA hospital (2 variables,
including an indicator for missing distance); (vii) prior utilization; (viii) prior diagnoses; (ix) vital
signs and WBC count; and (x) ordering characteristics.15 Next, we run separate regressions using
each of the 210 = 1,024 possible combinations of these 10 groups as controls.
Figure 6 shows the range of the coefficients ∆ˆ J IV E across these specifications. The number
of different specifications that corresponds to a given number of patient controls may differ. For
example, controlling for either no patient characteristics or all patient characteristics each results in
one specification. However, more generally, controlling for n patient characteristics results in “10
choose n” specifications. For each number of characteristics on the x-axis, we plot the minimum,
maximum, and mean slope statistic. The relationship is only slightly less positive with more controls,
and no specification yields a slope that is close to 0. Panel A displays results using observations from
all stations, and Panel B displays results using observations only from the 44 stations in which we
find even stronger evidence of balance. As expected, slope statistics are even more robust in Panel B
but, if anything, slightly larger in magnitude than the range of slope statistics in Panel A.
14 We also see increases in joint outcomes of adverse events and true negatives. This may suggest a violation of exclusion
in Condition 1(i). Note that increases in the joint outcome of being diagnosed and having an adverse event by themselves
do not imply violations of Condition 1, if the adverse event is binary and the increases are less than 1.
15 Variables in groups (vii)-(x) are described in Section 3.

17

5

Structural Analysis

In this section, we define and estimate a structural model that allows variation in both skill and preferences. It builds on the canonical selection framework by allowing radiologists to observe different
signals of patients’ true conditions, and so to rank cases differently in terms of their appropriateness
for diagnosis.

5.1

Model

Patient i’s true state si is determined by a latent index νi ∼ N (0,1). If νi is greater than ν, then the
patient has pneumonia:
si = 1 (νi > ν) .
We assume that ν > 0 so that the share S = 1 − Φ(ν) of patients with pneumonia is less than one half.16
The radiologist j assigned to patient i observes a noisy signal wi j correlated with νi , where the
strength of the correlation depends on the radiologist’s skill α j ∈ [0,1]:
©© 0 ª © 1
© νi ª
­
® ∼ N ­­ ® , ­
« wi j ¬
«« 0 ¬ « α j

α j ªª
®® .
1 ¬¬

(5)

We assume that radiologists know both the cutoff value ν and their own accuracies α j .
The radiologist’s utility is given by




−1, if di j = 1,si = 0,





ui j = −β j , if di j = 0,si = 1,






otherwise.
 0,


(6)

The key preference parameter β j captures the disutility of a false negative relative to a false positive.
Given that the health cost of undiagnosed pneumonia is potentially much greater than the cost of
inadvertently giving antibiotics to a patient who does not need them, we expect β j > 1. We normalize
the utility of correctly classifying patients to zero.
In Appendix A.5, we show that the radiologist’s optimal decision rule reduces to a cutoff value τj

such that di j = 1 wi j > τj . The optimal cutoff τ ∗ must be such that the agent’s posterior probability
16 This

assumption is consistent with the data and simplifies exposition but is not imposed in estimation.

18

that si = 0 after observing wi j = τ ∗ is equal to


τ∗ αj , β j =

βj
. The forumla for the optimal threshold is
1 + βj



q
β
ν − 1 − α2j Φ−1 1+βj j
αj

.

(7)

The cutoff value in turn implies FP j and F N j , which give expected utility
 

E ui j = − FP j + βF N j .

(8)

The comparative statics of the threshold τ ∗ with respect to ν and β j are intuitive. The higher is ν,
and thus the smaller the share S of patients who in fact have pneumonia, the higher is the threshold.
The higher is β j , and thus the greater the cost of a missed diagnosis relative to a false positive, the
lower is the threshold.
The effect of skill α j on the threshold is ambiguous. This arises because α j has two distinct effects
on the radiologist’s posterior on νi : (i) it shifts the posterior mean further from zero and closer to the
observed signal wi j ; and (ii) it reduces the posterior variance. For α j ≈ 0, the radiologist’s posterior is


β
close to the prior N (0,1) regardless of the signal. Provided that ν > Φ−1 1+βj j she will prefer not to
diagnose any patients, implying τ ∗ ≈ ∞. As α j increases, effect (i) dominates. This makes any given
wi j more informative and so causes the optimal threshold to fall. As α j increases further, effect (ii)
dominates. This makes the agent less concerned about the risk of false negatives and so causes the
optimal threshold to rise. Figure 7 shows the relationship between α j and τj∗ for different values of
βj .
In Appendix A.5.3, we consider a richer utility function in which radiologists’ utility functions
may also depend on the severity of a false negative (i.e., νi − ν) and show that this formulation yields
a similar threshold-crossing model with equivalent empirical implications. In Appendix A.6.4, we
also explore an alternative formulation in which τj depends on a potentially misinformed belief about

α j . From a social planner’s perspective, deviations from τ ∗ α j , β s —where β s represents the social
planner’s welfare weights on false negatives vs. false positives—yield equivalent welfare losses regardless of whether they derive from deviations of β j from β s or from deviations of beliefs about α j
from the truth.
We also allow for two additional parameters that relate to our institutional setting and reconcile the
data with the restrictive joint-normal signal structure in Equation (5). First, we allow for a proportion
of cases κ that are not at risk for pneumonia and are recognized as such by all radiologists. This
19

reflects the fact that we cannot distinguish chest X-rays in our data ordered for reasons other than
suspicion of pneumonia. Second, given that we only observe false negatives after some delay, we
allow for a share λ of cases that do not have pneumonia at the time of their visit to develop it and be
diagnosed subsequently, thus being incorrectly coded as false negatives.
If we know a radiologist’s FPR j and T PR j in ROC space, then we can identify her skill α j by
the shape of potential ROC curves, and her preference β j by her diagnosis rate and Equation (7).
Equation (5) determines the shape of potential ROC curves and implies that they are smooth. It also

guarantees that two ROC curves never intersect and that each FPR j ,T PR j point lies on only one
ROC curve. We also note that utility maximization and rational expectations imply selection on gains,
or concave ROC curves.
To see how λ is identified, note that under the joint-normal signal structure with λ = 0 a radiologist
with FPR j ≈ 0 must have a nearly perfectly informative signal and so should also have T PR j ≈ 1.
We in fact observe T PR j < 1 at this limit (i.e., some radiologists with no false positives still have
some false negatives) and the value of λ will be determined by the size of this gap. To see how κ is
identified, note that with κ = 0 we expect no radiologists with 0 < FPR j < 1 and T PR j = max j 0 T PR j 0 .
That is, we expect no radiologists who have no false negatives (adjusting for λ) yet also have a nontrivial number of false positives. Given these parameters and ν, the expected observed prevalence of
pneumonia among all chest X-rays will be S = (1 − Φ (ν) + λΦ (ν)) (1 − κ).17

5.2

Estimation

We estimate the model using observed data on diagnoses di and false negatives f ni . Recall that we
observe f ni = 0 for any i such that di = 1, and f ni = 1 is only possible if di = 0. We define the

following probabilities, conditional on γ j ≡ α j , β j :


≡ Pr wi j > τj∗ γ j ;



γ j ≡ Pr wi j < τj∗,νi > ν γ j ;



γ j ≡ Pr wi j < τj∗,νi < ν γ j .

p1j γ j
p2j
p3j



17 Because

we only observe data that include “false negatives” from later visits and chest X-rays that may not be at risk,
we refer to these reduced-form moments as P j , F N j , and S. To distinguish from the “observed prevalence” S, we denote
the actual prevalence at the time of the initial chest X-ray, only among cases at risk, to be S 0 = 1 − Φ (ν). By T PR j and
FPR j , we denote the respective true positive rate and false positive rate for a radiologist’s decisions on the initial chest
X-ray for patients at risk. In other words, T PR j and FPR j adjust the reduced-form moments P j and F N j by parameters ν,
κ, and λ.

20

The likelihood of observing (di , f ni ) for a case i assigned to radiologist j (i) is

Li






(1 − κ) p1j γ j(i) ,
if di = 1,




 


f ni ,di | γ j(i) = (1 − κ) p2j γ j(i) + λp3j γ j(i) , if di = 0, f ni = 1,







if di = 0, f ni = 0.
 (1 − κ) (1 − λ) p3j γ j(i) + κ,


For the set of patients assigned to j, I j ≡ {i : j (i) = j}, the likelihood of d j = {di }i ∈I j and fn j =
{ f ni }i ∈I j is
L j fn j ,d j γ j



=

Ö

Li f ni ,di | γ j(i)



i ∈I j

=

(1 − κ) p1j γ j(i)

· (1 − κ) (1 − λ) p3j
where ndj =

Í

fn

i ∈I j

di , n j =

Í


 n jf n
(1 − κ) p2j γ j(i) + λp3j γ j(i)

 n −n d −n f n
γ j(i) + κ j j j ,

 n dj

fn

i ∈I j

f ni , and n j = I j . From the above expression, ndj , n j , and n j

are sufficient statistics of the likelihood of d j and fn j , and we can write the radiologist likelihood as


fn
L j ndj ,n j ,n j γ j .
Although α j and β j are flexibly identified in principle, we make an assumption on their population
distribution to improve power. Specifically, we assume
2
©© µα ª © σα
© α̃ j ª
­
® ∼ N ­­
®,­
β̃
µ
j
β
«
««
¬
¬ « ρσα σβ

where α j =

1
2

ρσα σβ ªª
®® ,
σβ2 ¬¬


1 + tanh α̃ j and β j = exp β̃ j . We set ρ = 0 in our baseline specification.

We calibrate κ using a random forest algorithm that predicts pneumonia based on patient vital
signs, time categories, patient demographics, patient prior utilization, and words or phrases extracted
from the chest X-ray requisition. We conservatively set κ = 0.196 equal to the proportion of patients
with a random forest predicted probability of pneumonia less than 0.01.
Finally, to allow for potential deviations from random assignment, we risk-adjust observations of
diagnosis and type II error. Specifically, instead of using counts of diagnoses ndj and false negative
fn

outcomes n j , we first risk-adjust individual observations (di , f ni ) by patient characteristics Xi as well
as a full set of interactions between time dummies Ti and location identifiers ` (i), as we do in Section

21

4.2.18 Denoting risk-adjusted counts as ñdj and ñ j , we proceed in the second step by maximizing the

following log-likelihood to estimate the hyperparameter vector θ ≡ µα, µβ ,σα,σβ ,λ,ν :
y

θ̂ = arg max

Õ

θ

∫
log

Lj



y
ñdj , ñ j ,n j




γ j f γ j θ dγ j .

j

We compute the integral by simulation, described in further detail in Appendix A.6.2. Given our


y
estimate of γ and each radiologist’s risk-adjusted data, ñdj , ñ j ,n j , we can also form an empirical

Bayes posterior of each radiologist’s skill and preference α j , β j , which we describe in Appendix
A.6.3.

5.3

Results

Table 2 shows estimates of the hyperparameter vector θ in our baseline specification. We report
asymptotic standard errors. We show in Appendix A.7 that estimates are stable across alternative
specifications and are also qualitatively similar regardless of whether or not we adjust for patient
characteristics. The stability with respect to patient controls is consistent with the stability of our
reduced-form results in Section 4.4.
In Appendix Figure A.4, we compare the distributions of observed data moments with those
simulated from the model at the estimated parameter values. The observed moments we consider are:
(i) the distribution of radiologist diagnosis rates; (ii) the distribution of radiologist type II error rates;
and (iii) the correlation between diagnosis rates and type II error rates.19 In all cases, the simulated
data match the observed data closely.

Table 2 also shows moments in the distribution of α j , β j implied by the model parameters. In
the baseline specification, the mean radiologist accuracy is relatively high, at 0.84. This implies that
the average radiologist receives a signal that has a correlation of 0.84 with the patient’s underlying
latent state νi . A radiologist at the 10th percentile of this skill distribution receives a signal that has
a correlation of 0.72 with the state, while a radiologist at the 90th percentile of the skill distribution
receives a signal that has a correlation of 0.93 with the state. The average radiologist preference
weights a false negative 8.07 times as high as a false positive. The 10th percentile of the preference
18 We

describe this risk-adjustment procedure in further detail in Appendix A.6.1.
construct simulated moments as follows. We first fix the number of patients each radiologist examines to the
actual number. We then simulate patients at risk from a binomial distribution with the probability of being at risk of
1 − κ. For patients at risk, we simulate their underlying true signal and the radiologist-observed signal, or νi and wi j ,
respectively, using our posterior for α j . We determine which patients are diagnosed with pneumonia and which patients are
false negatives based on τ ∗ α j , β j , νi , and ν. We finally simulate patients who did not initially have pneumonia but later
develop it with λ.
19 We

22

distribution entails a false negative disutility that is 6.79 times as high as a false positive, while the
90th percentile of this distribution entails a false negative disutility that is 9.43 times as high as a false
positive. Table A.5 finally shows that these distributions of radiologist structural primitives are fairly
invariant to the specification of the structural estimation.

In Figure 7, we display predicted empirical Bayes posteriors for α j , β j in a space that represents
optimal diagnostic thresholds. The figure shows that, for the estimated parameters of the model (in
particular, for the preference parameters that we estimate), the relationship between accuracy and
diagnostic thresholds is mostly positive. As radiologists become more accurate, they diagnose fewer
people (their thresholds increase), since the costly possibility of making a false negative diagnosis
decreases. In Appendix Figure A.5, we show the distributions of the empirical Bayes posteriors for
α j , β j , and τj , and the joint distribution of α j and β j . Finally, in Figure A.6, we transform empirical

Bayes posteriors for α j , β j onto ROC space. The relationship between T PR j and FPR j implied by
the empirical Bayes posteriors is similar to that implied by the flexible projection shown earlier in
Figure 4.

5.4

Heterogeneity

To provide suggestive evidence on what may drive variation in skill and preferences, we project our

empirical Bayes posteriors for α j , β j onto observed radiologist characteristics. Figure 8 shows the
distribution of observed characteristics across bins defined by empirical Bayes posteriors of skill α j .
Appendix Figure A.7 shows analogous results for the preference parameter β j .
Panel A of Figure 8 shows that more skilled radiologists are older. This is the strongest relationship statistically among all the characteristics we consider. Panel B shows that higher-skilled
radiologists also tend to be more specialized in reading chest X-rays (in the sense that these account
for a larger share of the scans they read).
Panel C shows that those who are more skilled also spend more time generating their reports.
This suggests that skill may be a function of effort as well as characteristics like training or talent.
The median radiologist with 0.10 higher α (i.e., among radiologists who extract 10% more of the true
signal than another group of radiologists) spends 35.3% more time to generate her reports. Panel D
shows that more skilled radiologists also issue shorter rather than longer reports, perhaps suggesting
that clarity and efficiency of communication is more important than the volume of words produced.
Panel E shows that there is little correlation between skill and the rank of the medical school a
radiologist attended. If anything, the relationship is slightly negative. Finally, Panel F shows that
23

higher skilled radiologists are more likely to be male, in part reflecting the fact that male radiologists
are older and tend to be more specialized in reading chest X-rays.
The results for the preference parameter β j shown in Appendix Figure A.7 tend to go in the
opposite direction. This reflects the fact that our empirical Bayes estimates of α j and β j are slightly
negatively correlated.
It is important to emphasize that large variation in characteristics remains even conditional on
skill or preference. This finding is broadly consistent with the physician practice-style and teacher
value-added literature, which demonstrate large variation in decisions and outcomes that appear uncorrelated with physician or teacher characteristics (Epstein and Nicholson 2009; Staiger and Rockoff
2010).

6
6.1

Policy Implications
Decomposing Observed Variation

To assess the relative importance of skill and preferences in driving observed decisions and outcomes,
we simulate counterfactual distributions of decisions and outcomes in which we eliminate variation
in skill or preferences separately. We first simulate model primitives (α j , β j ) from the estimated
parameters. Then we eliminate variation in skill by imposing α j = ᾱ, where ᾱ is the median of α j ,
while keeping β j unchanged. Similarly, we eliminate variation in preferences by imposing β j =
β̄, where β̄ is the median of β j , while keeping α j unchanged. For each of these counterfactual


distributions of underlying primitives— ᾱ, β j and α j , β̄ —we simulate counterfactual distributions
of observed decisions and outcomes and compare them with those generated by (α j , β j ).
We find that eliminating variation in skill reduces variation in diagnosis rates by 44 percent and
variation in type II error rates by 83 percent. On the other hand, eliminating variation in preferences
reduces variation in diagnosis rates by 25 percent and has no significant effect on variation in type
II error rates. These decomposition results suggest that variation in skill can have first-order impacts
on variation in decisions, something the standard model of preference-based selection rules out by
assumption.

6.2

Policy Counterfactuals

We also evaluate the welfare implications of policies aimed at observed variation in decisions or at
underlying skill. Welfare depends on the overall false positive probability FP and the overall false
24

negative probability F N. We denote these objects under the status quo as FP0 and F N 0 , respectively.
We then define an index of welfare relative to the status quo:
W = 1−

FP + β s F N
,
FP0 + β s F N 0

(9)

where β s is the social planner’s relative welfare loss due to false negatives compared to false positives.
This index ranges from W = 0 at the status quo to W = 1 at the first best of FP = F N = 0. It is also
possible that W < 0 under a counterfactual policy that reduces welfare relative to the status quo.
We estimate FP0 and F N 0 based on our model estimates as
FP0 =
FN0 =

 
1 Õ
n j FP α j ,τ ∗ α j , β j ; ν̄ ; ν ;
j nj j
 
1 Õ
Í
n j F N α j ,τ ∗ α j , β j ; ν̄ ; ν .
j nj j
Í

Here, τ ∗ (α, β; ν̄) denotes the optimal threshold given the evaluation skill α, the preference β, and the
disease prevalence ν̄. (α j , β j ) are simulated model primitives from the estimated parameters. We
then consider welfare under counterfactual policies that eliminate diagnostic variation by imposing
diagnostic thresholds on radiologists.
In Table 3, we evaluate outcomes under two sets of counterfactual policies. Counterfactuals 1 and
2 focus on thresholds, while Counterfactuals 3 to 6 aim to improve skill.
Counterfactual 1 imposes a fixed diagnostic threshold to maximize welfare:





τ (β s ) = arg max 1 −
τ 



Í1
j nj

Í

j nj




FP α j ,τ; ν + β s F N α j ,τ; ν 


FP0 + β s F N 0

,






where α j and ν are given by our baseline model in Section 5. Despite the objective to maximize
welfare, a fixed diagnostic threshold may actually reduce welfare relative to the status quo by imposing this constraint. On the other hand, Counterfactual 2 allows diagnostic thresholds as a function of

α j , implementing τj (β s ) = τ ∗ α j , β s ; ν̄ . This policy should weakly increase welfare and outperform
Counterfactual 1.
In Counterfactuals 3 to 6, we consider alternative policies that improve diagnostic skill, for example by training radiologists, selecting radiologists with higher skill, or aggregating signals so that
decisions use better information. In Counterfactuals 3 to 5, we allow radiologists to choose their
own diagnostic thresholds, but we improve the skill α j of all radiologists at the bottom of the dis25

tribution to a minimum level. For example, in Counterfactual 3, we improve skill to the 25th percentile α25 , so we set α j = α25 for any radiologist below this level. The optimal thresholds are then

τj = τ ∗ (max α j ,α25 , β j ; ν̄). Counterfactual 6 forms random two-radiologist teams and aggregates
signals of each team member under the assumption that the two signals are drawn independently.
Table 3 shows outcomes and welfare under β s = 8, which is close to the median radiologist preference β j . We find that imposing a fixed diagnostic threshold (Counterfactual 1) would actually reduce
welfare. Although this policy reduces aggregate false positive errors, it increases aggregate false negative errors, which are costlier. Imposing a threshold that varies optimally with skill (Counterfactual
2) must improve welfare, but we find that the magnitude of this gain is small. In contrast, improving
diagnostic skill reduces both false negative and false positive outcomes and substantially outperforms
threshold-based policies. Combining two radiologist signals (Counterfactual 6) improves welfare by
36% of the difference between status quo and first best. Counterfactual policies that improve radiologist skill naturally reclassify a much higher number of cases than policies that simply change
diagnostic thresholds, since improving skill will reorder signals, while changing thresholds leaves
signals unchanged.20
Figure 9 shows welfare changes as a function of the social planner’s preferences β s . In this
figure, we consider Counterfactuals 1 and 4 from Table 3. We also show the welfare gain a planner
would expect if she set a fixed threshold under the incorrect assumption that radiologists have uniform
diagnostic skill. In this “mistaken policy counterfactual,” the planner would conclude that a fixed
threshold would modestly increase welfare.21 In the range of β s spanning radiologist preferences
(Table 2 and Figure A.5), the skill policy outperforms the threshold policy, regardless of the policymaker’s belief on the heterogeneity of skill. The threshold policy only outperforms the skill policy
when β s diverges significantly from radiologist preferences. For example, if β s = 0, the optimal
policy is trivial: no patient should be diagnosed with pneumonia. In this case, there is no gain to
improving skill but there is a large gain to imposing a fixed threshold if some radiologists do not
share the social planner’s preferences.
20 Reclassified cases are those that have a different classification (diagnosed or not) under the counterfactual policy than
under the status quo. We compute reclassified cases by holding fixed the noise term ω̃i j ∼ N (0,1), independent of νi , for
q
all cases i across counterfactual policies. A radiologist with accuracy α j will observe the signal wi j = α j νi + 1 − α2j ω̃i j .
Under this setup, if τj and α j are unchanged for all j, then no case will be reclassified.
21 We assume that the planner calculates a common diagnostic skill parameter α that rationalizes FP 0 and F N 0
with some estimate of disease prevalence ν 0 . Specifically, we solve two equations for two unknowns, α and ν 0 :
Í  −1 Í
Í  −1 Í
0
0
0
FP0 =
j nj
j n j FP α,τj ; ν and F N =
j nj
j n j F N α,τj ; ν . The common diagnostic threshold that
maximizes welfare under this assumption is τ (β s ) = τ ∗ (α, βs ; ν̄ 0 ).

26

6.3

Discussion

We show that dimensions of “preferences” and “skill” have different implications for welfare and
policy. Each of these dimensions likely captures a range of underlying factors. In our framework,
“preferences” encompass any distortion from the optimal threshold implied by (i) the social planner’s
relative disutility of false negatives, or β s , and (ii) the relationship between a patient’s underlying state
and a radiologist’s signals about that state, or α j . These distortions may arise from intrinsic preferences or external incentives that cause radiologist β j to differ from β s . Alternatively, as we elaborate
in Appendix A.6.4, equivalent distortions may arise from radiologists having incorrect beliefs about
the population prevalence parameter ν or their own skill α j .
What we call “skill” captures the relationship between a patient’s underlying state and a radiologist’s signals about the state. We attribute this mapping to the radiologist since quasi-random
assignment to radiologists implies that we are isolating the causal effect of radiologists. As suggested
by the evidence in Section 5.4, “skill” may reflect not only underlying ability but also effort. Furthermore, in this setting, radiologists may form their judgments with the aid of other clinicians (e.g.,
residents, fellows, non-radiologist clinicians) and must communicate their judgments to other physicians. Skill may therefore reflect not only the quality of signals that the radiologist observes directly,
but also the quality of signals that she (or her team) passes on to other clinicians.
For purposes of welfare analysis, the mechanisms underlying “preferences” or “skill” do not
matter in so far as they map to an optimal diagnostic threshold and deviations from it. However,
practical policy implications (e.g., whether we train radiologists to read chest X-rays, collaborate
with others, or communicate with others) will depend on institution-specific mechanisms.

7

Conclusion

In this paper, we decompose the roots of practice variation in decisions across radiologists into dimensions of skill and preferences. While systematic variation in decisions across agents exists in a
wide range of settings, the standard view in much of the literature is to assume that of such variation
results from variation in preferences. We first show descriptive evidence that runs counter to this
view: radiologists who diagnose more cases with a disease are also the ones who miss more cases
that actually have the disease. We then apply a framework of classification and a model of decisions
that depend on both diagnostic skill and preferences. Using this framework, we demonstrate that the
source of variation in decisions can have important implications for how policymakers should view
27

the efficiency of variation and for the ideal policies to address such variation. In our case, variation
in skill accounts for 44 percent of the variation in diagnostic decisions, and policies that select or
train providers to have higher skill result in potentially large welfare improvements, while policies to
impose uniform diagnosis rates may reduce welfare.
Our analysis relates not only to policy discussions centering on the causes and welfare implications of practice variation (e.g., Skinner 2012), but also to an active and growing literature that uses
variation across decision-makers to estimate the effect of a decision on outcomes (e.g., Kling 2006).
In the approach that we develop, we rely on prior information about the potential effect of the decision on outcomes. We show that such restrictions on potential outcomes may provide stronger tests of
monotonicity, particularly if potential outcomes capture important relationships with both unobserved
and observed case characteristics. Intuitively, the judges-design literature relies on comparisons between agents of the same skill. Thus, measuring skill may allow for research designs that correct for
bias due to monotonicity violations.

References
A BALUCK , J., L. AGHA , C. K ABRHEL , A. R AJA , AND A. V ENKATESH (2016): “The Determinants
of Productivity in Medical Testing: Intensity and Allocation of Care,” American Economic Review,
106, 3730–3764.
A BUJUDEH , H. H., G. W. B OLAND , R. K AEWLAI , P. R ABINER , E. F. H ALPERN , G. S. G AZELLE ,
AND

J. H. T HRALL (2010): “Abdominal and Pelvic Computed Tomography (CT) Interpretation:

Discrepancy Rates Among Experienced Radiologists,” European Radiology, 20, 1952–1957.
A IZER , A.

AND

J. J. D OYLE (2015): “Juvenile Incarceration, Human Capital, and Future Crime:

Evidence from Randomly Assigned Judges,” Quarterly Journal of Economics, 130, 759–803.
A NGRIST, J. D., G. W. I MBENS ,

AND

A. B. K RUEGER (1999): “Jackknife Instrumental Variables

Estimation,” Journal of Applied Econometrics, 14, 57–67.
A NWAR , S.

AND

H. FANG (2006): “An Alternative Test of Racial Prejudice in Motor Vehicle

Searches: Theory and Evidence,” American Economic Review, 96, 127–151.
A RNOLD , D., W. D OBBIE ,

AND

C. S. YANG (2018): “Racial Bias in Bail Decisions,” Quarterly

Journal of Economics, 133, 1885–1932.
28

BALKE , A. AND J. P EARL (1997): “Bounds on Treatment Effects from Studies with Imperfect Compliance,” Journal of the American Statistical Association, 92, 1171–1176.
B ERTRAND , M.

AND

A. S CHOAR (2003): “Managing with Style: The Effect of Managers on Firm

Policies,” Quarterly Journal of Economics, 118, 1169–1208.
B HULLER , M., G. B. DAHL , K. V. L OKEN , AND M. M OGSTAD (2016): “Incarceration, Recidivism
and Employment,” Working Paper 22648, National Bureau of Economic Research.
B LACKWELL , D. (1953): “Equivalent Comparisons of Experiments,” Annals of Mathematical Statistics, 24, 265–272.
B LOOM , N. AND J. VAN R EENEN (2010): “Why Do Management Practices Differ across Firms and
Countries?” Journal of Economic Perspectives, 24, 203–224.
C HAN , D. C. (2018): “The Efficiency of Slacking Off: Evidence from the Emergency Department,”
Econometrica, 86, 997–1030.
C HANDRA , A., D. C UTLER ,

AND

Z. S ONG (2011): “Who Ordered That? The Economics of Treat-

ment Choices in Medical Care,” in Handbook of Health Economics, Elsevier, vol. 2, 397–432.
C HANDRA , A. AND D. S TAIGER (2017): “Identifying Sources of Inefficiency in Health Care,” Working Paper 24035, National Bureau of Economic Research.
C HANDRA , A.

AND

D. O. S TAIGER (2007): “Productivity Spillovers in Healthcare: Evidence from

the Treatment of Heart Attacks,” Journal of Political Economy, 115, 103–140.
C ORNELISSEN , T., C. D USTMANN , A. R AUTE ,

AND

U. S CHOENBERG (2016): “From LATE to

MTE: Alternative Methods for the Evaluation of Policy Interventions,” Labour Economics, 41,
47–60.
C URRIE , J.

AND

W. B. M AC L EOD (2017): “Diagnosing Expertise: Human Capital, Decision Mak-

ing, and Performance among Physicians,” Journal of Labor Economics, 35, 1–43.
D OBBIE , W., J. G OLDIN , AND C. S. YANG (2018): “The Effects of Pretrial Detention on Conviction,
Future Crime, and Employment: Evidence from Randomly Assigned Judges,” American Economic
Review, 108, 201–240.

29

D OYLE , J. J., J. A. G RAVES , J. G RUBER , AND S. K LEINER (2015): “Measuring Returns to Hospital
Care: Evidence from Ambulance Referral Patterns,” Journal of Political Economy, 123, 170–214.
E PSTEIN , A. J. AND S. N ICHOLSON (2009): “The Formation and Evolution of Physician Treatment
Styles: An Application to Cesarean Sections,” Journal of Health Economics, 28, 1126–1140.
FABRE , C., M. P ROISY, C. C HAPUIS , S. J OUNEAU , P. A. L ENTZ , C. M EUNIER , G. M AHE ,

AND

M. L EDERLIN (2018): “Radiology Residents’ Skill Level in Chest X-Ray Reading,” Diagnostic
and Interventional Imaging, 99, 361–370.
F IGLIO , D. N.

AND

M. E. L UCAS (2004): “Do High Grading Standards Affect Student Perfor-

mance?” Journal of Public Economics, 88, 1815–1834.
F ILE , T. M.

AND

T. J. M ARRIE (2010): “Burden of Community-Acquired Pneumonia in North

American Adults,” Postgraduate Medicine, 122, 130–141.
F ISHER , E. S., D. E. W ENNBERG , T. A. S TUKEL , D. J. G OTTLIEB , F. L. L UCAS ,

AND

E. L.

P INDER (2003a): “The Implications of Regional Variations in Medicare Spending. Part 1: The
Content, Quality, and Accessibility of Care,” Annals of Internal Medicine, 138, 273–287.
——— (2003b): “The Implications of Regional Variations in Medicare Spending. Part 2: Health
Outcomes and Satisfaction with Care,” Annals of Internal Medicine, 138, 288–298.
F RANDSEN , B. R., L. J. L EFGREN ,

AND

E. C. L ESLIE (2019): “Judging Judge Fixed Effects,”

Working Paper 25528, National Bureau of Economic Research.
G ARBER , A. M. AND J. S KINNER (2008): “Is American Health Care Uniquely Inefficient?” Journal
of Economic Perspectives, 22, 27–50.
H ECKMAN , J. J.

AND

B. E. H ONORE (1990): “The Empirical Content of the Roy Model,” Econo-

metrica, 58, 1121.
H ECKMAN , J. J.

AND

E. V YTLACIL (2005): “Structural Equations, Treatment Effects, and Econo-

metric Policy Evaluation,” Econometrica, 73, 669–738.
I MBENS , G. W. AND J. D. A NGRIST (1994): “Identification and Estimation of Local Average Treatment Effects,” Econometrica, 62, 467–475.

30

I MBENS , G. W.

AND

D. B. RUBIN (1997): “Estimating Outcome Distributions for Compliers in

Instrumental Variables Models,” Review of Economic Studies, 64, 555–574.
I NSTITUTE OF M EDICINE (2013): Variation in Health Care Spending: Target Decision Making, Not
Geography, National Academies Press.
——— (2015): Improving Diagnosis in Health Care, National Academies Press.
K ITAGAWA , T. (2015): “A Test for Instrument Validity,” Econometrica, 83, 2043–2063.
K LEINBERG , J., H. L AKKARAJU , J. L ESKOVEC , J. L UDWIG ,

AND

S. M ULLAINATHAN (2018):

“Human Decisions and Machine Predictions,” Quarterly Journal of Economics, 133, 237–293.
K LING , J. R. (2006): “Incarceration Length, Employment, and Earnings,” American Economic Review, 96, 863–876.
K UNG , H.-C., D. L. H OYERT, J. X U ,

AND

S. L. M URPHY (2008): “Deaths: Final Data for 2005,”

National Vital Statistics Reports: From the Centers for Disease Control and Prevention, National
Center for Health Statistics, National Vital Statistics System, 56, 1–120.
L EAPE , L. L., T. A. B RENNAN , N. L AIRD , A. G. L AWTHERS , A. R. L OCALIO , B. A. BARNES ,
L. H EBERT, J. P. N EWHOUSE , P. C. W EILER ,

AND

H. H IATT (1991): “The Nature of Adverse

Events in Hospitalized Patients,” New England Journal of Medicine, 324, 377–384.
M ACHADO , C., A. M. S HAIKH , AND E. J. V YTLACIL (2019): “Instrumental Variables and the Sign
of the Average Treatment Effect,” Journal of Econometrics, 212, 522–555.
M OLITOR , D. (2017): “The Evolution of Physician Practice Styles: Evidence from Cardiologist
Migration,” American Economic Journal: Economic Policy, 10, 326–356.
N ORRIS , S. (2019): “Judicial Errors: Evidence from Refugee Appeals,” Working Paper 2018-75,
University of Chicago, Becker Friedman Institute of Economics.
RUBIN , D. B. (1974): “Estimating Causal Effects of Treatments in Randomized and Nonrandomized
Studies,” Journal of Educational Psychology, 66, 688–701.
RUUSKANEN , O., E. L AHTI , L. C. J ENNINGS ,

AND

Lancet (London, England), 377, 1264–1275.

31

D. R. M URDOCH (2011): “Viral Pneumonia,”

S ELF, W. H., D. M. C OURTNEY, C. D. M C NAUGHTON , R. G. W UNDERINK ,

AND

J. A. K LINE

(2013): “High Discordance of Chest X-Ray and Computed Tomography for Detection of Pulmonary Opacities in ED Patients: Implications for Diagnosing Pneumonia,” American Journal of
Emergency Medicine, 31, 401–405.
S HOJANIA , K. G., E. C. B URTON , K. M. M C D ONALD ,

AND

L. G OLDMAN (2003): “Changes

in Rates of Autopsy-Detected Diagnostic Errors Over Time: A Systematic Review,” JAMA, 289,
2849.
S KINNER , J. (2012): “Causes and Consequences of Regional Variations in Healthcare,” in Handbook
of Health Economics, ed. by M. V. Pauly, T. G. McGuire, and P. Barros, San Francisco: Elsevier,
vol. 2, 49–93.
S TAIGER , D. O.

AND

J. E. ROCKOFF (2010): “Searching for Effective Teachers with Imperfect

Information,” Journal of Economic Perspectives, 24, 97–118.
S YVERSON , C. (2011): “What Determines Productivity?” Journal of Economic Literature, 49, 326–
365.
T HOMAS , E. J., D. M. S TUDDERT, H. R. B URSTIN , E. J. O RAV, T. Z EENA , E. J. W ILLIAMS ,
K. M. H OWARD , P. C. W EILER , AND T. A. B RENNAN (2000): “Incidence and Types of Adverse
Events and Negligent Care in Utah and Colorado,” Medical Care, 38, 261.
T SUGAWA , Y., A. K. J HA , J. P. N EWHOUSE , A. M. Z ASLAVSKY,

AND

A. B. J ENA (2017): “Vari-

ation in Physician Spending and Association With Patient Outcomes,” JAMA Internal Medicine,
177, 675.
VAN PARYS , J.

AND

J. S KINNER (2016): “Physician Practice Style Variation: Implications for Pol-

icy,” JAMA Internal Medicine, 176, 1549.
V YTLACIL , E. (2002): “Independence, Monotonicity, and Latent Index Models: An Equivalence
Result,” Econometrica, 70, 331–341.

32

33
FP
FP + T N
0.00

0.25

0.50

0.75

1.00

0.00

0.25

0.50

False positive rate

0.75

Lower skill

Higher skill

1.00

1

Note: Panel A shows the standard classification matrix representing four joint outcomes depending on decisions and states. Each row represents a decision and
each column represents a state. The true positive rate (T PR) is defined as the probability of positive classification conditional on a positive state, or the ratio of
true positives over true positives plus false negatives. The false positive rate (FPR) is defined as the probability of positive classification conditional on a negative
state, or the ratio of false positives over false positives plus true negatives. Panel B plots the receiver operating characteristic (ROC) curve. It shows the relationship
between the true positive rate (T PR) and the false positive rate (FPR). An ROC curve illustrates the diagnostic skill of a binary classification system that applies
a threshold decision rule to observed “signals” on cases. In a single ROC curve, the threshold is varied, while the signals are fixed. This corresponds to a fixed
evaluation skill with varying diagnosis rates. Different ROC curves correspond to different evaluation skills. Agents on different ROC curves apply thresholds to
different signals. The particular ROC curves shown in this figure are formed assuming the signal structure in Equation (5), with more accurate ROC curves (higher
α j ) further from the 45-degree line. Regardless of the signal structure, ROC curves must be upward-sloping.

FPR =

TP
T P + FN

T PR =

False Positive Rate

True Negative
(T N)

False Negative
(FN)
Type II Error

Classified
Negative

True Positive Rate

False Positive
(FP)
Type I Error

True Positive
(T P)

Actual Negative

B: Receiver Operating Characteristic (ROC) Curve

1: Visualizing the Classification Problem

Classified
Positive

Actual Positive

A: Classification Matrix

Figure
Figure 1: Classification Matrix

True positive rate

Figure 2: Hypothetical Data Generated by Variation in Preferences vs. Skill
A: Varying Preferences
1.00

True positive rate

0.75

0.50

0.25

0.00
0.00

0.25

0.50

0.75

1.00

0.75

1.00

False positive rate

B: Varying Skill
1.00

True positive rate

0.75

0.50

0.25

0.00
0.00

0.25

0.50

False positive rate

Note: This figure demonstrates two possible models with hypothetical data. The top panel fixes the evaluation
skill and varies preferences. All agents are located on the same ROC curve and are faced with the tradeoff
between sensitivity (T PR) and specificity (1 − FPR). They draw different thresholds for selection as a result
of heterogeneous preferences. The bottom panel fixes the preference and varies diagnostic skills. Agents
are located on different ROC curves but have parallel indifference curves. They draw different thresholds for
selection as a result of heterogeneous skills.

34

Figure 1. Flow chart. Forty selected CXR were divided into 3 categories (selection phase) and presented to experts (validation phase).
Sixteen CXR did not reach experts’ consensus and were rejected of the analysis. The 24 CXR with experts’ consensus were presented to
residents (experiment phase) and then included in analysis.

Figure 3: Example Chest X-rays

Figure 2. Typical examples of radiographs expected to mobilize detection skills (A—C) and interpretation skills (D—F). Experts’ consensus
Note: This
example
chest
X-rays
reproduced
2 of— Fabre
al.usual
(2018).
Thesepneumonia
chest X-—
were:figure
miliaryshows
tuberculosis
— CXR#6
(A),
lung nodule
(cancer) from
in left Figure
upper lobe
CXR#19et
(B),
interstitial
diagnoses
(C), left upper
lobe
— CXR#3
(D), right
lower lobeand
infectious
CXR#14
(E) and
right upper lobe
atelectasis
CXR#27
rays represent
cases
onatelectasis
which there
is expert
consensus
whichpneumonia
are used —for
training
radiologists.
Only
Panel
with Golden sign — CXR#36 (F).

E represents a case of infectious pneumonia, and we have added a red oval to denote where the pneumonia lies,
in the right lower lobe. Panel A shows miliary tuberculosis; Panel B shows a lung nodule (cancer) in the left
upper lobe; Panel C shows usual interstitial pneumonitis; Panel D shows left upper lobe atelectasis; Panel E
shows right upper lobe atelectasis.

35

Figure 4: Projecting Data on ROC Space

1.00

True positive rate

0.75

0.50

0.25

0.00
0.00

0.25

0.50

0.75

1.00

False positive rate

Note: This figure plots the model-free true positive rate (T PR j ) and false positive rate (FPR j ) for each radiologist across 3,199 radiologists who have at least 100 chest X-rays. The figure is based on risk-adjusted diagnosis
and type II error rates for each radiologist (D j and F N j , respectively), which are shown in visual IV form in
Appendix Figure A.3 and as a binned scatter plot in Panel A of Figure 5. We then project these rates into ROC
space (i.e., onto T PR j and FPR j ). This projection does not require any behavioral model but only uses diseaserelated quantities, described in greater detail in Section 5. In brief, we use three disease-related parameters: (i)
the proportion of chest X-rays that are not at risk for pneumonia, κ; (ii) the proportion of at-risk chest X-rays
with detectable pneumonia, 1 − Φ (ν); and (iii) the proportion of at-risk cases without
 detectable pneumonia at
the time who subsequently develop pneumonia, λ. For a given observed P j , F N j , we calculate the following
adjustments: S 0 = 1 − Φ (ν); P j0 = P j /(1 − κ); T N j0 = T N j − κ /(1 − κ) /(1 − λ); F N j0 = F N j /(1 − κ) − λT N j0;


T PR j = 1 − F N j0/S 0; and FPR j = P j0 + F N j0 − S 0 /(1 − S 0). We use κ = 0.196, λ = 0.021, and ν = 1.781. For a
few radiologists, we impose additional restrictions that FPR j > 0 and T PR j > FPR j .

36

Figure 5: Diagnosis and Type II Error Rates
A: 2SLS

Type II error rate

.03

.025

.02

Coeff = 0.094 (0.007)
N = 4,663,840, J = 3,199

.015
.04

.06
.08
Diagnosis rate

.1

B: JIVE

Type II error rate

.03

.025

.02

Coeff = 0.263 (0.018)
N = 4,663,840, J = 3,199

.015
.01

.02
.03
Diagnosis rate

.04

Note: This figure plots the relationship between the probability of pneumonia (PNA) diagnoses and type II
errors across radiologists. Under the assumption of IV validity in the judges design, this relationship represents
the effect of diagnosis on type II error. Panel A shows results using radiologist dummies as instruments, and
Panel B shows results using radiologist jackknife propensities to diagnose, given in Equation (4), as instruments. In each panel, (first-stage) predictions of diagnoses due to radiologists are shown on the x-axis, and
(reduced-form) predictions of type II errors due to radiologists are shown on the y-axis. The coefficient in each
panel corresponds to the 2SLS estimate and standard error (in parentheses) for the corresponding IV regression,
as well as the number of cases (N) and the number of radiologists (J). Controls include 77 variables for patient
characteristics and time dummies interacted with station dummies. Further details are given in Appendix A.3.
The visual IV corresponding to Panel A is shown in Appendix Figure A.3.

37

Figure 6: Stability of Slope between Diagnosis and Type II Error Rates
A: Full Sample
.32

Slope

.28

.24

.2
0

2
4
6
8
Number of patient characteristic sets

10

B: Stations with Balance
.4

Slope

.36

.32

.28
0

2
4
6
8
Number of patient characteristic sets

10

Note: This figure shows the stability of the jackknife IV estimate on the relationship between type II error rates
and diagnosis rates, shown in Panel B of Figure 5. This relationship compares diagnosis and false negative
rates, D j and F N j . Details on how we calculate this slope are given in Figure 5. The benchmark sample
generating results in Figure 5 uses observations from all stations. Stability results from this benchmark (full)
sample are shown in Panel A; results from an alternative sample restricted to 44 stations with statistical evidence
of quasi-random assignment are shown in Panel B. Appendix A.2.2 provides further details on how we select
the 44 stations with evidence of quasi-random assignment. In each panel, we recalculate the IV estimate from
Equation (A.9), varying the number of sets of patient characteristics we use as controls. We use 10 possible
sets of patient characteristics, altogether composed of 77 variables, that are described in Section 4.4. Therefore,
each panel summarizes 210 = 1,024 different regression specifications. On the x-axis of each panel, we vary the
number of patient characteristic types that we control for. For x-axis values between 0 and 10 (the maximum),
we run more than one regression (10 choose x) and collect the slope statistic in each specification. In the figure,
we show the mean slope as a solid line and the minimum and maximum slopes as dashed lines.

38

Figure 7: Optimal Diagnostic Threshold
2.00

β=6

τ

1.75

β=8

1.50

β = 10
1.25

1.00
0.4

0.6

0.8

1.0

α

Note: This figure shows how the optimal diagnostic threshold as a function of skill α and preferences β with
iso-preference curves for β = 6,8,10. Each iso-preference curve illustrates how the optimal diagnostic threshold
varies with the evaluation skill for a fixed preference, given by Equation (7), using ν = 1.781 estimated from
the model. Dots on the figure represent the empirical Bayes posterior of α (on the x-axis) and β for each
radiologist, and the corresponding optimal diagnostic threshold τ (α, β; ν) (on the y-axis) for each radiologist.
The empirical Bayes posteriors are the same as those shown in Figure A.5. Details on the empirical Bayes
procedure are given in Appendix A.6.3.

39

Figure 8: Heterogeneity in Accuracy
A: Age

B: Chest X-rays Focus
.45
Percent of chest X−rays

Age (years)

70

60

50

Coeff = 54.8 (3.6)
N = 11,876

40
.8

.85

.9
α

.95

.35

.25
Coeff = 0.188 (0.066)
N = 3,199

.15
.7

1

4.2

7

6

5
Coeff = 3.53 (0.96)
N = 3,199

4
.7

.8
α

3.9

3.6
Coeff = −0.394 (0.171)
N = 3,133

3.3
.7

.9

E: Medical School Rank
Share of male radiologists

300
200
100
Coeff = −207 (112)
N = 1,697

0
.7

.8

.8
α

.9

F: Gender

400
Medical school rank

.9

D: Log Median Report Length
Median log report length

Median log time (minutes)

C: Log Median Time

.8
α

.9

.9

.8

.7
Coeff = 0.431 (0.167)
N = 2,604

.6
.7

α

.8
α

.9

Note: This figure shows the relationship between a radiologist’s empirical Bayes posterior of her accuracy
(α) on the x-axis and the following variables on the y-axis: (i) the radiologist’s age; (ii) the proportion of the
radiologist’s exams that are chest X-rays; (iii) the log median time that the radiologist spends to generate a
chest X-ray report; (iv) the log median length of the issue reports; (v) the rank of the medical school that the
radiologist attended according to U.S. News & World Report; and (vi) gender. Except for gender, the three
lines show the fitted values from the 25th, 50th, and 75th quantile regressions. For gender, the line shows the
fitted values from the usual regression. The dots are the median values of the variables on the y-axis within
each bin of α. 30 bins are used. Appendix Figure A.7 shows the corresponding plots with preferences (β) on
the x-axis.

40

Figure 9: Counterfactual Policies
0.100

Welfare change

0.075

0.050

0.025

0.000

−0.025
4

6

8

10

12

c

Social planner preference (β )
Fixed threshold
Fixed threshold (if skill were homogeneous)
Improve skill to 25th percentile

Note: This figure plots the counterfactual welfare gains of different policies. Welfare is defined in Equation (9)
and is normalized to 0 for the status quo and 1 for the first best (no false positive or false negative outcomes).
The x-axis represents different possible disutility weights that the social planner may place on false negatives
relative to false positives, or β s . The first policy imposes a common diagnostic threshold to maximize welfare.
The second policy also imposes a common diagnostic threshold to maximize welfare but incorrectly considers
implications under the assumption that radiologists have the same diagnostic skill. The third policy trains
radiologists to the 25th percentile of diagnostic skill (if their skills are below-median) and allows them to
choose their own diagnostic thresholds based on their preferences.

41

42
2,333,804
1,567

2,330,036
1,632

2,332,840
1,579

2,331,000
1,620

Type II error rate (p.p.)
Below-median Above-median Difference
1.89
2.46
0.57
(0.59)
(0.79)
(0.02)
2.17
2.17
-0.00
(0.20)
(0.20)
(0.01)
2.16
2.18
0.02
(0.14)
(0.15)
(0.01)
2.17
2.17
0.00
(0.10)
(0.10)
(0.00)
2.16
2.19
0.03
(0.29)
(0.29)
(0.01)
2.18
2.17
-0.01
(0.22)
(0.23)
(0.01)
2.16
2.19
0.03
(0.36)
(0.36)
(0.01)

Note: This table presents results assessing balance across radiologists in the benchmark sample according to patient characteristics. Columns 1 to 3 compare
radiologists with below- or above-median risk-adjusted diagnosis rates. Columns 4 to 6 compare radiologists with below- or above-median risk-adjusted type II
error rates. For context, the risk-adjusted diagnosis rate is given in the first row for below- and above-median radiologists in Columns 1 and 2, respectively; caseweighted standard deviations of diagnosis rates are also shown in parentheses for each of the groups. The difference between the two groups is given in Column 3,
with the standard error of the difference shown in parentheses. Similarly, the risk-adjusted type II error rates for the corresponding below- and above-median group
are displayed in Columns 4 and 5, respectively, in the first row; the difference between those two groups is given in Column 6. The subsequent six rows examine
balance in patient characteristics by showing analogous differences in predicted diagnosis rates (Columns 1 to 3) or predicted type II error rates (Columns 4 to 6),
where different sets of patient characteristics are used for linear predictions. Patient characteristic variables are described in further detail in Section 4.1. WBC
stands for white blood cell. In the last two rows, we display the number of cases and the number of radiologists in each group. Appendix A.2.1 provides further
details on the calculations. Appendix Table A.2 provides similar results restricted to the sample of 44 stations for which we cannot reject quasi-random assignment.

Number of cases
Number of radiologists

Predicted outcome using all variables

Predicted outcome using ordering characteristics

Predicted outcome using vitals and WBC count

Predicted outcome using prior utilization

Predicted outcome using prior diagnosis

Predicted outcome using demographics

Outcome

Diagnosis rate (p.p.)
Below-median Above-median Difference
6.27
7.70
1.43
(1.69)
(1.96)
(0.06)
6.95
7.02
0.07
(0.60)
(0.59)
(0.02)
6.96
7.02
0.06
(0.34)
(0.34)
(0.01)
6.98
6.99
0.01
(0.16)
(0.16)
(0.01)
6.91
7.07
0.16
(0.96)
(0.99)
(0.03)
6.96
7.01
0.05
(0.62)
(0.62)
(0.02)
6.89
7.09
0.20
(1.16)
(1.17)
(0.04)

Table 1: Balance

Table 2: Estimation Results
Panel A: Model Parameter Estimates
µα
0.897
(0.038)
σα
0.332
(0.010)
µβ
2.080
(0.056)
σβ
0.128
(0.006)
λ
0.021
(0.000)
ν̄
1.781
(0.020)
κ
0.196
Panel B: Radiologist Primitives
α
β
τ
Mean
0.839 8.067 1.361
10th percentile
25th percentile
Median
75th percentile
90th percentile

0.720
0.793
0.858
0.904
0.934

6.790
7.339
8.002
8.723
9.428

1.270
1.313
1.360
1.409
1.453

Note: This table shows model parameter estimates (Panel A) and radiologist primitives implied by the model
parameters (Panel B). Hyperparameters µα and σα determine the distribution of radiologist diagnostic skill α,
while hyperparameters µβ and σβ determine the distribution of radiologist preferences β (the disutility of a
false negative relative to a false positive). In the baseline model, we assume that α and β are uncorrelated.
κ is the proportion of chest X-rays at risk for pneumonia. λ is the proportion of at-risk chest X-rays with
no radiographic pneumonia at the time of exam but subsequent development of pneumonia. ν describes the
prevalence of pneumonia at the time of the exam among at-risk chest X-rays. Standard errors are shown in
parentheses. κ is calibrated as the proportion of patients with 0 probability of pneumonia on a random forest
model of pneumonia based on rich characteristics in the patient chart. Model parameters are described in
further detail in Section 5.

43

44

Welfare
0.0000
-0.0033
0.0032
0.0669
0.1647
0.3011
0.3607

False
Negative
0.212
0.221
0.212
0.188
0.160
0.125
0.114

False
Positive
1.542
1.484
1.538
1.518
1.427
1.264
1.163
Diagnosed
2.329
2.263
2.326
2.329
2.267
2.139
2.050

Reclassified
0.000
0.245
0.147
0.101
0.247
0.462
0.583

Note: This table shows outcomes and welfare under the status quo and counterfactual policies, further described in Section 6. Welfare is normalized to 0 for the
status quo and 1 for the first best of no false negative or false positive outcomes. Numbers of cases that are false negative, false positive, diagnosed, and reclassified
are all divided by the prevalence of pneumonia. Reclassified cases are those with a classification (i.e., diagnosed or not) that is different under the counterfactual
policy than under the status quo. The first row shows outcomes and welfare under the status quo. Subsequent rows show outcomes and welfare under counterfactual
policies. Counterfactuals 1 to 2 impose diagnostic thresholds: Counterfactual 1 imposes a fixed diagnostic rate for all radiologists; Counterfactual 2 imposes
diagnostic rates as a function of diagnostic skill. Counterfactuals 3 to 5 improve diagnostic skill to the 25th, 50th, and 75th percentile respectively. Counterfactual
6 allows two radiologists to diagnose a single patient and combine the signals they receive.

Policy
0. Status quo
1. Fixed threshold
2. Threshold as function of skill
3. Improve skill to 25th percentile
4. Improve skill to 50th percentile
5. Improve skill to 75th percentile
6. Combine two signals

Table 3: Counterfactual Policies

Appendix
A.1

Sufficiency of Skill-Propensity Independence

We first define the notion of probabilistic monotonicity and a sufficient condition for the judges design
to recover a well defined LATE.
Definition (Probabilistic Monotonicity). Consider a set of judges J . There exists probabilistic
monotonicity among judges in J if, for any j and j 0 in J ,




Pr di j = 1 ≥ Pr di j 0 = 1 or Pr di j = 1 ≤ Pr di j 0 = 1 , for all i.

(A.1)

Condition A.1 (Skill-Propensity Independence). There exists a function that assigns a skill α j to

each judge j ∈ J such that (i) probabilistic monotonicity holds in all sets J α ≡ j ∈ J : α j = α ;
(ii) P j is independent of α j .
In this section, we detail proofs of the sufficiency of Condition A.1 for the judges-design 2SLS
estimand to represent properly weighted treatment effects. Condition A.1 is a weaker version of the
standard (strict) monotonicity assumption of Imbens and Angrist (1994), stated in Condition 1(iii).
We also show that Condition A.1 implies the “average monotonicity” concept of Frandsen et al.
(2019).
We consider a population of cases I and a population of agents J . Assignment to agents drives
treatment decisions; we denote the potential treatment decision for case i ∈ I under any agent j ∈ J
by di j ∈ {0,1}. While we consider Condition A.1 in place of Condition 1(iii), we assume the other
conditions for IV validity, namely Condition 1(i)-(ii). Specifically, potential outcomes for a given

case depend only on treatment decisions yi j = yi di j and potential outcomes and potential treatment
decisions are independent of agent assignments. As in the paper, we denote the assigned agent for case

i as j (i), and we denote an agent j’s treatment propensity as P j ≡ Pr di j = 1 j (i) = j . For each case
Í
Í
i, we observe only one decision and one outcome: di ≡ j 1 ( j = j (i)) di j and yi ≡ j 1 ( j = j (i)) yi j =
yi (di ).

We adopt the concept of monotonicity-consistent skill α j such that Pr di j = 1 is characterized
for all i by α j and P j . The definition of monotonicity-consistent skill is such that, for any j and j 0
with α j = α j 0 , probabilistic monotonicity holds, or




Pr di j = 1 ≥ Pr di j 0 = 1 or Pr di j = 1 ≤ Pr di j 0 = 1 , for all i.


Therefore, if both α j = α j 0 and P j = P j 0 , then we must have Pr di j = 1 = Pr di j 0 = 1 , for all i. We
denote the probability of treatment for case i, conditional on α j(i) = α and P j(i) = p, as πi (α, p). We
work with the above concept of probabilistic monotonicity. Since probabilistic monotonicity is a
generalization of strict monotonicity, all proofs will also apply to the more specific case of skill being
defined by strict monotonicity.

A.1

A.1.1

Proper Weighting of Treatment Effects in Estimand

Following Imbens and Angrist (1994), we consider a discrete distribution of α j ∈ A and P j ∈ P.
This setup reduces notation but is without loss of generality. As a first object, we define δ (p0, p) ≡




Ei yi | P j(i) = p0 − Ei yi | P j(i) = p . Unlike the standard case, we first start with an infinite population of judges at each p ∈ P in order to exploit Condition A.1. We turn to a finite set of judges and
convergence properties as this set grows in Appendix A.1.2. δ (p0, p) is the difference in average outcomes comparing cases assigned to an agent with P j = p0 with those assigned to an agent with P j = p;
this object is identified from data. We also define the treatment effect for case i as yi (1) − yi (0), which
is not identified from data, since only one of the potential outcomes yi (di ) is observed.
Proposition 5. Under Condition 1(i)-(ii) and Condition A.1, for p0 > p, δ (p0, p) is a proper weighted
average of treatment effects, or Ei [ωi (yi (1) − yi (0))], where ωi ≥ 0 for all i.
Proof. By iteration of expectations, we have




δ (p0, p) ≡ Ei yi | P j(i) = p0 − Ei yi | P j(i) = p
 


= Eα Ei yi | α j(i) = α, P j(i) = p0 P j(i) = p0
 


−Eα Ei yi | α j(i) = α, P j(i) = p P j(i) = p .
By Condition A.1, the distribution of α j is the same for P j = p0 as it is for P j = p. Thus,



 
δ (p0, p) = Eα Ei yi | α j(i) = α,P j(i) = p0 − Ei yi | α j(i) = α, P j(i) = p .
Condition 1(i)-(ii) and further operations yield
δ (p0, p) = Eα [Ei [(πi (α, p0) − πi (α, p)) (yi (1) − yi (0))]]
= Ei [Eα [(πi (α, p0) − πi (α, p)) (yi (1) − yi (0))]]
= Ei [ωi (yi (1) − yi (0))],
where ωi = Eα [πi (α, p0) − πi (α, p)] is the incremental probability of treatment for case i between
assignment to agents with P j = p0 and assignment to agents with P j = p. From the definition of
probabilistic monotonicity in Condition A.1, ωi ≥ 0 for all i.
Note that δ (p0, p) is the reduced-form numerator of a Wald estimand


δ(p0 ,p)
p0 −p

which identifies

the average treatment effect for compliers induced into treatment when reassigned from judges with
P j = p to agents with P j = p0. Next, we consider the IV estimand. As in the standard case, the IV
estimand is a weighted average of the Wald estimands, with weights summing to 1.
Proposition 6. The judges-design IV estimand,
β

IV

=

Cov yi , P j(i)
Cov di , P j(i)
A.2



,

is a weighted average of Wald estimands δ (p0, p) /(p0 − p), where the weights are non-negative and
sum to 1.

Proof. Index p as pk for k = 1,. . .,K, such that pk 0 > pk for k 0 > k. Denote λk = Pr P j(i) = pk . The
IV estimand is given by
β

IV

=

Cov yi , P j(i)




Cov di , P j(i)


Ei yi P j(i) − E [di ]
= 
 .
Ei di P j(i) − E [di ]

We will proceed by iterating expectations in the numerator and the denominator. In the numerator,
K



 Õ

Ei yi P j(i) − E [di ] =
λk Ei yi P j(i) − E [di ] P j(i) = pk .
k=1





By definition, Ei yi | P j(i) = pk = δ (pk , p1 ) + Ei yi | P j(i) = p1 . Therefore, the numerator is equal to
K
Õ

K
Õ


λk Ei yi | P j(i) = p1 (pk − E [di ]) +
λk δ (pk , p1 ) (pk − E [di ]) .

k=1

k=2

|

{z

}

0

Since δ (pk , p1 ) =
K
Õ
k=2

k 0 =2 δ (pk

Ík

λk

k
Õ

0

, pk 0−1 ), we can also state the numerator as

δ (pk 0 , pk 0−1 ) (pk − E [di ]) =

k 0 =2

K
Õ

δ (pk , pk−1 )

K
Õ

λk 0 (pk 0 − E [di ]) .

k 0 =k

k=2

Similar operations in the denominator gives
β

IV

=

ÍK
0
0
k=2 δ (pk , pk−1 ) k 0 =k λk (pk − E [di ])
.
ÍK
ÍK
0
0
k=2 (pk − pk−1 ) k 0 =k λk (pk − E [di ])
ÍK

Thus,
β IV =

K
Õ

Ωk

k=2

with weights

δ (pk , pk−1 )
,
pk − pk−1

0
0
k 0 =k λk (pk − E [di ])
.
ÍK
0
0
00
00
k 0 =2 (pk − pk −1 ) k 00 =k 0 λk (pk − E [di ])

Ωk = Í K

(pk − pk−1 )

ÍK


By construction, the weights Ωk ≥ 0, and

ÍK

k=2 Ωk

= 1. Since Ωk is proportional to (pk − pk−1 ),

Wald estimands corresponding to larger first-stage changes in treatment propensity receive higher

A.3

weights. The second component of Ωk gives more weight to Wald estimands closer to the center of
the distribution of P.

A.1.2

Consistency of the Estimator

In practice, the judges-design estimator makes use of a finite number of judges. We now consider a
finite set J of judges and analyze the convergence properties of the judges-design estimator as k J k
increases to infinity.
We begin with the assumption that an infinite number of cases are assigned to each judge j ∈
J, denoting the probability of assignment to judge j as ρ j ≡ Pr ( j (i) = j). We partition the set by

Ð
propensity to treat, denoting Jp ≡ j ∈ J : P j = p , such that J = p Jp . We denote the expected


outcome, conditional on assignment to Jp , as Ei yi | j (i) ∈ Jp . As in Appendix A.1.1, we denote

the corresponding expected outcome in an infinite population of agents Jp = j ∈ J : P j = p as


Ei yi | P j = p .
Assumption A.1. Suppose that an infinite number cases are assigned to each agent j in a finite

sample of agents, J. Let Jp ≡ j ∈ J : P j = p and assume that as k J k approaches infinity, so does
Jp for all p.




Lemma 7. Under Assumption A.1, Ei yi | j (i) ∈ Jp converges in probability to Ei yi | P j(i) = p as
k J k approaches infinity.
Proof. By iteration of expectations, the expectation conditional on assignment to Jp is
Ei yi | j (i) ∈ Jp =




Õ

Í

j ∈J p

α∈A


 
ρ j 1 α j = α Ei yi | α j(i) = α,P j(i) = p
Í
.
j ∈J p ρ j

By the law of large numbers, as Jp → ∞, conditional on P j = p, the sample probability of
assignment to an agent with α j = α converges to the population probability of assignment to an agent
with α j :
Í
lim
k Jp k →∞


ρ j 1 αj = α

Í
= Pr α j(i) = α P j(i) = p .
j ∈J p ρ j

j ∈J p

Thus,


lim Ei yi | j (i) ∈ Jp
k Jp k →∞

=

Õ


 
Pr α j(i) = α P j(i) = p Ei yi | α j(i) = α,P j(i) = p

α∈A



= Ei yi | P j(i) = p .

Similarly, we can describe the convergence properties of the sample reduced-form estimate δ̂ (p0, p) ≡



Ei yi | j (i) ∈ Jp0 − Ei yi | j (i) ∈ Jp .


A.4

Lemma 8. Under Assumption A.1, for all p and p0 in P, δ̂ (p0, p) converges in probability to δ (p0, p)
as k J k approaches infinity.
Proof. Under Lemma 7,


= Ei yi | P j(i) = p ;



lim Ei yi | j (i) ∈ Jp
k Jp k →∞


lim Ei yi | j (i) ∈ Jp0
k Jp0 k →∞



= Ei yi | P j(i) = p0 .

Under Assumption A.1, Jp and Jp0 both approach infinity as k J k approaches infinity. Then
applying the continuous mapping theorem, we have
lim δ̂ (p0, p) = δ (p0, p) .

kJ k→∞


We now consider the 2SLS estimator in a finite sample of agents. For now, we continue to assume
an infinite sample of cases. Define the finite-judge IV estimand as
β̂JIV




Ei yi P j(i) − E [di ] j (i) ∈ J
.
= 

Ei di P j(i) − E [di ] j (i) ∈ J

Lemma 9. Under Assumption A.1, β̂JIV converges in probability to β IV as k J k approaches infinity.
 Í

Proof. Let λ̂k ≡ Pr P j(i) = pk j (i) ∈ J = j ∈J ρ j 1 P j = pk . Taking a similar approach as in Proposition 6, we can show that
β̂JIV =

K
Õ

Ω̂k

k=2

where

δ̂ (pk , pk−1 )
,
pk − pk−1

0
0
k 0 =k λ̂k (pk − E [di ])
.
ÍK
0 − pk 0 −1 )
00 (pk 00 − E [di ])
(p
λ̂
0
00
0
k
k
k =2
k =k

Ω̂k = ÍK

(pk − pk−1 )

ÍK

By the law of large numbers, lim kJ k→∞ λ̂k = λk . From Lemma 8, lim kJ k→∞ δ̂ (p0, p) = δ (p0, p) .
Applying the continuous mapping theorem, we have
lim β̂JIV = β IV .

kJ k→∞


We finally consider a finite sample of cases i = 1,. . ., N assigned to a finite sample of judges
Ð
J ≡ i j (i). Denote the set of cases assigned to j as I j . The IV estimator is


[d
]
y
P̂
−
Ê
i
j(i)
i=1 i

,
=Í
N
[d
]
d
P̂
−
Ê
i
j(i)
i=1 i
ÍN

IV
β̂ N
,J

A.5

where P̂ j is a consistent estimator of P j , such as the jackknife instrument, and Ê [di ] =

1
N

i=1 di .

ÍN

Proposition 10. Consider that both N and k J k approach infinity. Assume that I j approaches
infinity for all j ∈ J, where I j = {i : j (i) = j} is the set of patients assigned to radiologist j. Assume
that Jp approaches infinity for all p. Then
 d
√  IV
N β̂ N ,J − β IV → N (0,Σ),
where Σ =

E [εi2 (di −E[di ])2 ]
,
Cov2 ( di , P j(i) )

and εi = yi − E [yi ] − β IV (di − E [di ]) .

Proof. First consider a finite sample J, but that N approaches infinity such that I j approaches
infinity for all j ∈ J. Then Imbens and Angrist (1994) follows, and
 d


√  IV
N β̂ N ,J − β̂JIV → N 0, Σ̂J ,
where Σ̂J =

h
i
E εi2, J (di −E[ di | j(i)∈J])2
Cov2 ( di , P j(i) | j(i)∈J )

, and εi,J = yi − E [ yi | j (i) ∈ J] − β̂JIV (di − E [ di | j (i) ∈ J]).

As k J k approaches infinity, such that Jp approaches infinity for all p, and maintaining an infinite
p

p

sample I j for each j, β̂JIV → β IV from Lemma 9, and Σ̂J → Σ from the continuous mapping theorem.
So under the assumed asymptotics,
lim

kJ k→∞

 d
√  IV
N β̂ N ,J − β IV → N (0,Σ) .


A.1.3

Average Monotonicity (Frandsen et al. 2019)

We finally consider how Condition A.1 relates to “average monotonicity” in Frandsen et al. (2019).
We first define average monotonicity among a set of judges J.
Definition (Average Monotonicity). Consider a population of cases I. Average monotonicity exists
in a set of judges J if, for all i ∈ I,
Õ




ρ j P j − P di j − Di ≥ 0,

j ∈J

where ρ j ≡ Pr ( j (i) = j), P ≡

Í

j ∈J

ρ j P j , and Di ≡

Í

j ∈J


ρ j Pr di j = 1 .

We show that in a large population of judges, Condition A.1 implies average monotonicity. We
begin by showing that under Condition A.1 in a infinite population of judges, the probability of
treatment increases when randomly reassigning any case i from a judge with propensity p to a judge
with propensity p0 > p.

A.6

Lemma 11. With an infinite population of judges at each propensity p ∈ P, Condition A.1 implies
that for all i and any pair p0 and p in P such that p0 > p,




E j di j P j = p0 ≥ E j di j P j = p .
Proof. Iterating expectations, for case i and some p ∈ P,


E j di j P j = p



 
= Eα E j di j α j = α, P j = p P j = p


= Eα πi (α, p)| P j = p
= Eα [πi (α, p)],

where the second equality makes use of the definition of skill-consistent monotonicity in Condition
A.1, and the third equality invokes independence between skill and propensities in Condition A.1.
For p0 > p, πi (α, p0) ≥ πi (α, p) for all i and α. Therefore, for p0 and p in P such that p0 > p,




E j di j P j = p0 ≥ E j di j P j = p .

Proposition 12. With an infinite population of judges at each propensity p ∈ P, Condition A.1 implies
average monotonicity.
Proof. We restate the expression in the definition of average monotonicity in a population of judges:
lim

Õ

kJ k→∞




h

i
ρ j P j − P di j − Di = E j P j − P di j − Di

j ∈J

= Ej

h

 i
P j − P di j ,

h 
i
where the second equality makes use of the fact that E j Di P j − P = 0.

Index p ∈ P by k = 1,. . .,K, and define λk ≡ Pr P j = pk . Iteration of expectations yields
Ej

h

 i
P j − P di j

=
=

K
Õ
k=1
K
Õ

λk E j

h


i
P j − P di j P j = pk


 

λk pk − P E j di j P j = pk .

k=1







Now consider P̃ = inf p| p > P . By Lemma 11, for all i, E j di j P j = pk ≥ E j di j P j = P̃ for

A.7





any pk > P, while E j di j P j = pk ≤ E j di j P j = P̃ for any pk < P. Thus, for all i,
Ej

h



P j − P di j

i

=

K
Õ


 

λk pk − P E j di j P j = pk

k=1

≥

K
Õ


 

λk pk − P E j di j P j = P̃

k=1
K



Õ
= E j di j P j = P̃
λk pk − P
k=1

= 0.


A.2

Quasi-Random Assignment

A.2.1

Balance Between Radiologist Groups

This appendix details the construction of Tables 1 and A.2. In the first step, we categorize each
radiologist as having either above- or below-median risk-adjusted diagnostic rates and as having either
above- or below-median risk-adjusted type II error rates. In particular, we calculate radiologist riskfn
adjusted rates of diagnosis and type II error as ζˆd and ζˆ , respectively, as described in Appendix
j

j

A.6.1.
In the second step, we form a predicted diagnosis and a predicted type II error, based on linear
regressions with sets of patient characteristics as predictors. We consider six sets of patient characteristics: demographics (14 variables), prior utilization (3 variables), prior diagnoses (32 variables),
vital signs and WBC count (24 variables), ordering characteristics (4 variables), and all previously
listed characteristics (77 variables). In other words, for patient characteristics Xic , indexed by c, we
run the following linear probability models:
di = Xic β d,c + εid ;

(A.2)

y
Xic β y,c + εi .

(A.3)

f ni =

We then form predictions dˆic = Xic β̂ d,c and ŷic = Xic β̂ y,c .
In the third step, we compute average actual and predicted diagnoses and type II errors at the ra

diologist level. Specifically, for each measure xi ∈ di , f ni , dˆic , ŷic c , we average residual measures
−1 Í
∗
for patients assigned to each radiologist j: x j = I j
i ∈I j xi , where I j = {i : j (i) = j} is the set of
patients assigned to radiologist j. In Tables 1 and A.2, we display the respective patient-weighted

A.8

average and standard deviation of x j for radiologists belonging in each group J:
µJx

=

Í

v
u
t
σJx

=

j ∈J

Í

j ∈J

Ij x j
Ij

kJk
k J − 1k

Í

(A.4)

;
j ∈J

Ij
Í

x j − µJx

j ∈J

Ij

2
.

(A.5)

We also display the difference between the averages of two groups µJx2 − µJx1 where J1 and J2 correspond to a below-median and above-median
pair of groups. For inference on this difference of means,
r
 2
 2
we calculate a standard error of k J1 k −1 σJx1 + k J2 k −1 σJx2 , which focuses on variation at the

radiologist level.

A.2.2

Stations with Quasi-Random Assignment

In a complementary approach, we first identify stations with evidence of quasi-random assignment
based only on patient age and then assess robustness of this categorization by utilizing other “holdout” patient characteristics. For the latter assessment, we predict diagnosis and type II error using
the full matrix of 77 patient characteristic variables Xi in Equations (A.2) and (A.3). Therefore, in
each station, we separately assess whether three patient-level measures appear as good as randomly
assigned to radiologists: age; predicted diagnosis; and predicted type II error.
For each of these assessments, we use two methods: a parametric F-test of the joint statistical
significance of radiologist fixed effects in each station; and a permutation (“randomization inference”)
test of whether variation in radiologist fixed effects is larger than what would be obtained under
random assignment.

1. F-test. For each measure xi ∈ Agei , dˆi , ŷi and for each station `, we regress observations in
{i : ` (i) = `} as follows:
x
xi = Ti γ`x + ζ j(i)
+ εix ,

(A.6)

Clustering at the radiologist level, we then assess quasi-random assignment of xi in station `
by an F-test nof the
o joint significance of the set of fixed effects for the set of radiologists J` at
station `, or ζ jx

j ∈J`

.


2. Randomization Inference. For each measure xi ∈ Agei , dˆi , ŷi and for each station `, we form
residual xi∗ = xi − Ti δ̂`x , where δ̂`x is estimated from a station-specific regression xi = Ti δ`x + ηix .
We then regress these residual measures on radiologist fixed effects, as
x
xi∗ = ξ j(i)
+ εix ,

and measure the case-weighted standard deviation of estimated fixed effects, similar to Equa-

A.9

tion (A.5):
v
u
u
u
t

Í

x

where ξ ` =

Í

j ∈J`

Ij
Í



x
ξˆjx − ξ J`

2

k J` k
,
k J` − 1k
j ∈J` I j

 Í
I j ξˆjx / j ∈J` I j . Next, we randomly assign the residuals to radioloσ`x =

j ∈J`

gists in station `, keeping the number of observations assigned to each j ∈ J` fixed. Based on
these random placebo assignments j (i;r), for each i in each iteration r, we re-estimate placebo
x
fixed effects ξˆj(i;r)
and we re-calculate the patient-weighted standard deviation of these fixed
x . We repeat this for iterations r = {1,2,. . .,100} and count the number of iterations
effects σ`;r
x > σ x . This count is the randomization inference p-value for measure x and
for which σ`;r
`

station `.
First using age as the patient characteristic of interest, we identify stations that appear to feature quasirandom assignment. In Figure A.1, we find a high degree of concordance across stations between pvalues from the F-test and from the randomization inference, based on age. Forty-four stations pass
their F-tests with a p-value greater than 0.10, while 52 stations pass their randomization inference
tests with a p-value greater than 0.10. The former set of stations is a strict subset of the latter set, so
that 44 stations pass both their F-tests and their randomization inference tests. Aside from the mass
of stations with a p-value of 0, the remaining distribution of p-values from both tests appears uniform.
We then test whether “hold-out” characteristics continue to suggest quasi-random assignment
among the 44 stations selected based on patient age. In Figure A.2, we show the distribution of F-test
and randomization inference p-values among these 44 stations, based on the 77 patient characteristic
variables projected onto predicted pneumonia diagnosis and predicted type II error. We find that the
p-values continue to be roughly uniformly distributed with little mass at the p-value of 0.

A.3

Graphical Presentation of IV Estimates

In our descriptive analysis, we evaluate the relationship between radiologist effects on diagnostic
decisions di and type II errors f ni . This evaluation corresponds to the following 2SLS first-stage and
reduced-form regressions:
di = Zi ζ1 + Xi π1 + T̃i γ1 + ε1,i ;

(A.7)

f ni = Zi ζ2 + Xi π2 + T̃i γ2 + ε2,i ,

(A.8)

where Zi is potentially a vector-valued instrument depending on the assigned radiologist j (i) assigned
to case i, Xi is the full vector of 77 patient characteristic variables described in Section 4.1, and T̃i is
a vector of time-station interactions.
Define Z, X, and T̃ as matrices of stacked vectors Zi , Xi , and T̃i , respectively; similarly define
d and fn as vectors of di and f ni , respectively. Then the standard 2SLS estimator corresponding to

A.10

Equations (A.7) and (A.8) is
∆ˆ = X̃0P Z X̃

 −1

X̃0P Z fn,
(A.9)




 −1
where X̃ ≡ d X T̃ , Z̃ ≡ Z X T̃ , and P Z ≡ Z̃ Z̃0Z̃ Z̃0. Under Assumptions 1 and A.1, ∆ˆ is a
consistent estimator of ∆ in the following second-stage relationship:
f ni = ∆di + Xi β + Ti δ + i .
ˆ ∆ˆ IV , which uses radiologist dummies as instruments, and ∆ˆ J IV E ,
We estimate two versions of ∆:
which uses the jackknife instrument defined in Equation (4).
To show ∆ˆ IV graphically, we estimate radiologist fixed effects in the following reduced-form and
first-stage equations corresponding to Equations (A.7) and (A.8):
di = ζ1, j(i) + Xi π1 + T̃i γ1 + ε1,i ;
f ni = ζ2, j(i) + Xi π2 + T̃i γ2 + ε2,i .
This yields ζˆ1, j and ζˆ2, j for each j.
To each observation i, we assign values ξ1,i = ζˆ1, j(i) and ξ2,i = ζˆ2, j(i) . We residualize ξ1,i and
∗ and ξ ∗ . We average the residuals within each
ξ2,i by Xi and T̃i , calling the respective residuals ξ1,i
2,i

radiologist:
ξ 1, j

=

ξ 2, j

=

1 Õ ∗
ξ ;
I j i ∈I j 1,i
1 Õ ∗
ξ .
I j i ∈I j 2,i

We finally add a constant to all ξ 1, j to ensure that the patient-weighted average of ξ 1, j is equal to
the observed overall diagnosis rate; we similarly add a constant to all ξ 2, j to ensure that the patientweighted average of ξ 2, j is equal to the observed overall type II error rate.22
To create the visual IV in Figure A.3, we plot each point with ξ 1, j on the x-axis and ξ 2, j on the
y-axis. The patient-weighted slope of the line fitting these points is equal to β̂IV using radiologist
dummies as instruments for di . To create the binned scatter plot in Panel A of Figure 5, we first
residualize f ni by Xi and T̃i , calling the residual f ni∗ . We then divide the data at the patient level into
∗ , and we plot the mean ξ ∗ for each bin on the x-axis and the mean f n∗ for each bin on the
bins of ξ1,i
i
1,i

y-axis.
To show ∆ˆ J IV E graphically, we use the jackknife instrument,
Zi =
22 Without

1

Õ

I j(i) − 1 i0,i


1 i 0 ∈ I j(i) di0 ,

adding these constants, the patient-weighted averages of ξ 1, j and ξ 2, j would both be 0.

A.11

and estimate the first-stage regression,
di = αZi + Xi π + T̃i γ + εi ,
saving our estimate of α. We also residualize Zi by Xi and T̃i , denoting this residual as Zi∗ . To create
the binned scatter plot in Panel B of Figure 5, we divide the data at the patient level into bins of Zi∗ ,
and we plot the mean α̂Zi∗ for each bin on the x-axis and the mean f ni∗ for each bin on the y-axis.

A.4

Informal Tests of Monotonicity

Under monotonicity, when comparing a radiologist j 0 who diagnoses more cases than radiologist j,
there cannot be a case i such that di j = 1 and di j 0 = 0. In this appendix, we conduct informal tests
of this assumption, along the lines of tests in Bhuller et al. (2016) and Dobbie et al. (2018). In the
judges-design literature, these monotonicity tests confirm whether the first-stage estimates are nonnegative in subsamples of cases. We first present results of implementing these standard tests. We
then draw relationships between these tests, which do not reject monotonicity, and our analysis in
Section 4, which strongly rejects monotonicity.

A.4.1

Results

We define subsamples of cases based on patient characteristics. We consider four characterstics:
probability of diagnosis (based on patient characteristics); age; arrival time; and race. We define two
subsamples for each of the characteristics, for a total of eight subsamples: (i) above-median age;
(ii) below-median age; (iii) above-median probability of diagnosis; (iv) below-median probability
of diagnosis; (v) arrival time during the day (between 7 a.m. and 7 p.m.); (vi) arrival time at night
(between 7 p.m. and 7 a.m.); (vii) white race; and (viii) non-white race.
The first testable implication follows from the following intuition: Under monotonicity, a radiologist who generally increases the probability of diagnosis should increase the probability of diagnosis
in any subsample of cases. Following the judges-design literature, we construct leave-out propensities for pneumonia diagnosis and use these propensities as instruments for whether an index case is
diagnosed with pneumonia. In other words, for our baseline jackknife instrument, we construct
Z j−i =

Õ
1
di0 ,
I j − 1 i0 ∈I \i
j

where I j ≡ {i : j (i) = j}. This leave-out instrument for radiologist j averages diagnostic decisions
over other cases assigned to j, excluding the index case i.
In each of the 12 subsamples, defined by some patient characteristic m (e.g., age) and binary indicator x (e.g., older vs. younger), we estimate the following first-stage regression, using observations
in subsample I(x,m) :
di = αx,m Z j−i + Xi πx,m + T̃i γx,m + εi .
A.12

(A.10)

Consistent with our quasi-experiment in Assumption 1, we control for time categories interacted with
station identities, or T̃i . We also control for patient characteristics Xi as in our baseline first-stage
regression in Equation (A.7). Under monotonicity, we should have πx,m ≥ 0 for (m, x).
The second testable implication is slightly stronger: Under monotonicity, an increase in the probability of diagnosis by changing radiologists in any subsample of patients should correspond to increases in the probability of diagnosis in all other subsamples of patients. To capture this intuition,
we construct “reverse-sample” instruments that exclude any case with the same characteristic value x
of some characteristic function m:
Z j−(m,x) =

1
I j \ I(x,m)

Õ

di ,

i ∈I j \I(x , m)

where I(x,m) ≡ {i : m (i) = x} is the subsample of observations such that the characteristic value of m
is x. We estimate the first-stage regression, using observations in subsample I(x,m) :
−(m,x)
di = αx,m Z j(i)
+ Xi πx,m + T̃i γx,m + εi .

(A.11)

As before, we control for patient characteristics Xi and time categories interacted with station dummies T̃i , and we check whether πx,m ≥ 0 for all (x,m).
In Table A.4, we show results for these informal monotonicity tests, based on Equations (A.10)
and (A.11). Panel A shows results corresponding to the standard jackknife instrument, or πx,m from
the Equation (A.10). Panel B shows results corresponding to the reverse-sample instrument, or πx,m
from Equation (A.11). Each column corresponds to a different subsample. All 16 regressions yield
strongly positive first-stage coefficients.

A.4.2

Relationship with Reduced-Form Analysis

At a high level, the informal tests of monotonicity in the judges-design literature use information
about observable case characteristics and treatment decisions, while our analysis in Section 4 exploits
additional information about potential outcomes. In this subsection, we will clarify the relationship
between these analyses.
We begin with the standard condition for IV validity, Condition 1. Following Imbens and Angrist
(1994), we abstract from covariates, assuming unconditional random assignment in Condition 1(ii),
and consider a discrete multivalued instrument Zi . In the judges design, the instrument can be thought
of as the agent’s treatment propensity, or Zi = P j(i) ∈ {p1, p2,. . ., pK }, which the jackknife instrument
approaches with infinite data. We assume that p1 < p2 < · · · < pK . We also introduce the notation
di (Zi ) ∈ {0,1} to denote potential treatment decisions as a function of the instrument; in our main
framework, this amounts to di j = di (p) for all j such that P j = p.
Now consider some binary characteristic xi ∈ {0,1}. We first note that the following Wald estimand between two consecutive values pk and pk+1 of the instrument characterizes the probability that
xi = 1 among compliers i such that di (pk ) > di (pk+1 ):
A.13

E [ xi di | Zi = pk+1 ] − E [ xi di | Zi = pk ]
= E [ xi | di (pk+1 ) > di (pk )] .
E [ di | Zi = pk+1 ] − E [ di | Zi = pk ]
Since xi is binary, this Wald estimand gives us Pr ( xi | di (pk+1 ) > di (pk )) ∈ [0,1].
Under Imbens and Angrist (1994), 2SLS of xi di as an “outcome variable,” instrumenting di with
all values of Zi , will give us a weighted average of the Wald estimands over k ∈ {1,. . .,K − 1}. Specifically, consider the following equations:
xi di = ∆x di + uix ;

(A.12)

Zi + vix .

(A.13)

di = α

x

The 2SLS estimator of ∆x in this set of equations should converge to a weighted average:
∆x =

K−1
Õ

Ωk Pr ( xi | di (pk+1 ) > di (pk )),

k=1

where weights Ωk are positive and sum to 1. Therefore, we would expect that ∆ˆ x ∈ [0,1].
The informal monotonicity tests we conducted above ask whether some weighted average of
Pr ( di (pk+1 ) > di (pk )| xi ) is greater than 0. Since Pr (xi ) > 0 and Pr (di (pk+1 ) > di (pk )) > 0, the two
conditions—Pr ( di (pk+1 ) > di (pk )| xi ) > 0 and Pr ( xi | di (pk+1 ) > di (pk )) > 0—are equivalent. Therefore, if we were to estimate Equations (A.12) and (A.13) by 2SLS, we would in essence be evaluating
the same implication as the informal monotonicity tests standard in the literature.
In contrast, in a stylized representation of Section 4, we are performing 2SLS on the following
equations:
f ni = ∆di + ui ;

(A.14)

di = αZi + vi .

(A.15)

Recall that f ni = 1 (di = 0,si = 1) = si (1 − di ). Following the same reasoning above, we can state the
estimand ∆ as follows:
∆=−

K−1
Õ

Ωk Pr ( si | di (pk+1 ) > di (pk )),

k=1

which is a negative weighted average of conditional probabilities. This yields the same prediction
that we stated in Remark 3, i.e., that ∆ ∈ [−1,0]. Weaker implications that we consider in Appendix
A.1 would leave this prediction unchanged, as in Remark 4.
More generally, we could apply the same reasoning to any binary potential outcome yi (d) ∈ {0,1}
under treatment choice d ∈ {0,1}. It is straightforward to show that, if we replace f ni with yi di in
Equation (A.14), the 2SLS system of Equations (A.14) and (A.15), would yield
∆=

K−1
Õ

Ωk Pr ( yi (1)| di (pk+1 ) > di (pk )) ∈ [0,1] .

k=1

A.14

Alternatively, replacing f ni with −yi (1 − di ) in Equation (A.14) would imply
∆=

K−1
Õ

Ωk Pr ( yi (0)| di (pk+1 ) > di (pk )) ∈ [0,1] .

k=1

How might we interpret our results together in Section 4 and in this appendix? We show above that
the informal monotonicity tests are necessary for demonstrating that binary observable characteristics
have admissable probabilities among compliers. On the other hand, our analysis in Section 4 strongly
rejects that a potential outcome yi (0) = si has admissable probabilities among compliers. Observable
characteristics may be correlated with si , but si is undoubtedly related to characteristics that are
unobservable to the econometrician but, importantly, observable to radiologists. The importance of
these unobservable characteristics will drive the difference between our analysis and the standard
informal tests for monotonicity, and it implies that an analysis based on a potential outcome should
generally be stronger than an analysis based only on observable characteristics.

A.5

Optimal Diagnostic Threshold

A.5.1

Derivation

We provide a derivation of the optimal diagnostic threshold, given by Equation (7) in Section 5.1. We
start with a general expression for the joint distribution of the latent index for each patient, or νi , and
radiologist signals, or wi j . These signals determine each patient’s true disease status and diagnosis
status:
si = 1 (νi > ν) ;
di j


= 1 wi j > τj .


We then form expectations of type I error rates and type II error rates, or FP j ≡ Pr di j = 1,si = 0

and F N j ≡ Pr di j = 0,si = 1 , respectively. Consider the radiologist-specific joint distribution of

wi j ,νi as f j (x, y). Then
F Nj


= Pr wi j < τj ,νi > ν =

FP j


= Pr wi j > τj ,νi < ν =

∫

τj

∫

+∞

−∞ ν
∫ +∞ ∫ ν
τj

f j (x, y) dydx;
f j (x, y) dydx.

−∞

The joint distribution f j (x, y) and ν are known to the radiologist. Given her expected utility function
in Equation (6),
 

E ui j = − FP j + β j F N j ,
where β j is the disutility of a type II error relative to a type I error, the radiologist sets τj to maximize
her expected utility.

A.15

Denote the marginal density of wi j as g j . Denote the conditional density of νi given wi j as
∫y
f (x,y)
f j (y|x) = gj j (x) and the conditional cumulative distribution as Fj (y|x) = −∞ f j (t|x) dt.
The first order condition is
 
∂E ui j
∂FP j
∂F N j
= −
− βj
∂τj
∂τj
∂τj
∫ ν
∫ +∞


=
f j τj , y dy − β j
f j τj , y dy
−∞
ν
∫ ν
∫ +∞




=
f j y| τj g j τj dy
f j y| τj g j τj dy − β j
ν
−∞




= Fj ν| τj g j τj − β j 1 − Fj ν| τj g j τj
= 0.
The solution to the first order condition τj∗ satisfies


Fj ν| τj∗ =

βj
.
1 + βj

(A.16)

Equation (A.16) can alternatively be stated as


Fj ν| τj∗

.
βj =
1 − Fj ν| τj∗
This condition intuitively states that at the optimal threshold, the likelihood ratio of a type I error over
a type II error is equal to the relative disutility of a type II error.

As a specialcase, when w
distribution, as in Equation (5), we
 i j ,νi follows a joint-normal

 know
q
2
2
that νi | wi j ∼ N α j wi j ,1 − α j , or νi − α j wi j / 1 − α j wi j ∼ N (0,1). This implies that Fj ν| τj∗ =

 q

Φ ν − α j τj∗ / 1 − α2j . Plugging in Equation (A.16) and rearranging, we obtain Equation (7):


τ∗ αj , β j =



q
β
ν − 1 − α2j Φ−1 1+βj j
αj

.

 
In Section A.5.2, we verify that ∂ 2 E ui j /∂τj2 < 0 at τj∗ in a more general case, so τj∗ is the optimal
threshold that maximizes expected utility.

A.5.2

Comparative Statics

Returning to the general case, we need to impose a monotone likelihood ratio property to ensure that
Equation (A.16) implies a unique solution and to analyze comparative statics.

A.16

Assumption A.2 (Monotone Likelihood Ratio Property). The joint distribution f j (x, y) satisfies
f j (x2, y2 )
f j (x1, y2 )
>
,∀x2 > x1, y2 > y1, j.
f j (x2, y1 )
f j (x1, y1 )
We can rewrite the property using the conditional density:
f j ( y2 | x1 )
f j ( y2 | x2 )
>
,∀x2 > x1, y2 > y1, j.
f j ( y1 | x2 )
f j ( y1 | x1 )
That is, the likelihood ratio f j ( y2 | x2 ) / f j ( y1 | x2 ), for y2 > y1 and any j, always increases with x. In
the context of our model, when a higher signal wi j is observed, the likelihood ratio of a higher νi
over a lower νi is higher than when a lower wi j is observed. Intuitively, this means that the signal
a radiologist receives is informative of the patient’s true condition. As a special case, if f (x, y) is
a bivariate normal distribution, the monotone likelihood ratio property is equivalent to a positive
correlation coefficient.
Assumption A.2 implies first-order stochastic dominance. Fixing x2 > x1 and considering any
y2 > y1 , Assumption A.2 implies
f j ( y2 | x2 ) f j ( y1 | x1 ) > f j ( y2 | x1 ) f j ( y1 | x2 ) .

(A.17)

Integrating this expression with respect to y1 from −∞ to y2 yields
y2

∫

f j ( y2 | x2 ) f j ( y1 | x1 ) dy1 >

∫

−∞

Rearranging, we have

y2

f j ( y2 | x1 ) f j ( y1 | x2 ) dy1 .

−∞

f j ( y2 | x2 ) Fj ( y2 | x2 )
>
,∀y2 .
f j ( y2 | x1 ) Fj ( y2 | x1 )

Similarly, integrating Equation (A.17) with respect to y2 from y1 to ∞ yields
∫

+∞

f j ( y2 | x2 ) f j ( y1 | x1 ) dy2 >

∫

y1

Rearranging, we have

+∞

f j ( y2 | x1 ) f j ( y1 | x2 ) dy2 .

y1

f j ( y1 | x2 )
1 − Fj ( y1 | x2 )
>
,∀y1 .
1 − Fj ( y1 | x1 )
f j ( y1 | x1 )

Combining the two inequalities, we have
Fj ( y| x1 ) > Fj ( y| x2 ),∀y.

(A.18)



Under Equation (A.18), for a fixed ν, Fj ν| τj decreases with τ, i.e., ∂Fj ν| τj /∂τj < 0. We

A.17

can now verify that
 
∂ 2 E ui j
∂τj2

  ∂Fj ν| τj 
= 1 + β j g j τj∗
∂τj

< 0.



τ j =τ ∗j

τ j =τ ∗j

Therefore, τj∗ represents an optimal threshold that maximizes expected utility.
Using Equation (A.18) and the Implicit Function Theorem, we can also derive two reasonable
comparative static properties of the optimal threshold. First, τj∗ decreases with β j :
∂τj∗
∂ βj

=



1
1 + βj

2

∂Fj ν| τj
∂τj

  −1

< 0.
τ j =τ ∗j

Second, τj∗ increases with ν:
∂τj∗
∂ν

= − fj



ν| τj∗

  ∂Fj ν| τj   −1
∂τj

> 0.
τ j =τ ∗j

In other words, holding fixed the signal structure, a radiologist will increase her diagnostic rate when
the relative disutility of false negatives increases and will decrease her diagnostic rate when pneumonia is less prevalent.
We next turn to analyzing the comparative statics of the optimal threshold with respect to accuracy. For a convenient specification with single-dimensional accuracy, we return to the specific case
of joint-normal signals:
νi
wi j

!

!

0

∼N

,

0

1

αj

αj

1

!!
.

Taking the derivative of the optimal threshold with respect to α j in Equation (7), we have
∂τj∗
∂α j

Φ−1
=



βj
1+β j

α2j



q
− ν 1 − α2j

q
1 − α2j

.

These relationships yield the following observations. When α j = 1, τj∗ = ν. When α j = 0, the radiologist diagnoses no one if β j <

Φ(ν)
1−Φ(ν)

(i.e., τj∗ = ∞), and the radiologist diagnoses everyone if β j >

Φ(ν)
1−Φ(ν)

(i.e., τj∗ = −∞). When α j ∈ (0,1), the relationship between τj∗ and α j depends on the prevalence parameter ν. Generally, if β j is greater than some upper threshold β, τj∗ will always increase with α j ; if


β j is less than some lower threshold β, τj∗ will always decrease with α j ; if β j ∈ β, β is in between
the lower and upper thresholds, τj∗ will first increase then decrease with α j . The thresholds for β j

A.18

depend on ν:



Φ (ν)
,1 ;
1 − Φ (ν)


Φ (ν)
β = max
,1 .
1 − Φ (ν)
β = min

The closer ν is to 0, the less space there will be between the thresholds. The range of β j between the
thresholds generally decreases as ν decreases.
Intuitively, there are two forces that drive the relationship between τj∗ and α j . First, the threshold
radiologists with low accuracy will depend on the overall prevalence of pneumonia. If pneumonia is
uncommon, then radiologists with low accuracy will tend to diagnose fewer patients; if pneumonia is
common, then radiologists with low accuracy will tend to diagnose more patients. Second, the threshold will depend on the relative disutility of type II errors, β j . If β j is high enough, then radiologists
with lower accuracy will tend to diagnose more patients with pneumonia. Depending on the size of
β j , this mechanism may not be enough to have τj∗ always increasing in α j .

A.5.3

General Loss for Type II Error

While we consider a fixed loss for any type II error in our baseline specification of utility in Equation
(6), we show here that implications are qualitatively unchanged under a more general model with
losses for type II errors that may increase for more “severe” cases. We consider the following utility
function:



−1,
if di j = 1,si = 0,




ui j = −β j h (νi ), if di j = 0,si = 1,




 0,
otherwise,

where h (νi ) is bounded, differentiable, and weakly increasing in νi .23 As before, si ≡ 1 (νi > ν), and
β j > 0. Without loss of generality, we assume h(v̄) = 1, so h(vi ) ≥ 1,∀vi .

Denote the conditional density of νi given wi j as f j νi | wi j and the corresponding conditional

cumulative density as Fj νi | wi j . Expected utility, conditional on wi j and di j = 0, is



Eνi ui j νi ,di j = 0 wi j




= −β j Eνi h (νi ) 1 di j = 0,si = 1 wi j
∫ +∞
= −β j
h(νi ) f j (νi |wi j )dνi .
ν̄

The corresponding expectation when di j = 1 is



Eνi ui j νi ,di j = 1 wi j


= − Pr si = 0,di j = 1 wi j
∫ ν̄
∫
= −
f j (νi |wi j )dνi =
−∞

23 The

ν̄

+∞

f j (νi |wi j )dνi − 1.

boundedness assumption ensures that the integrals below are well-defined. This is a sufficient condition but not
necessary. The differentiability assumption simplifies calculation.

A.19







The radiologist chooses di j = 1 if and only if Eνi ui j νi ,di j = 1 wi j > Eνi ui j νi ,di j = 0 wi j , or
+∞

∫
ν



1 + β j h (νi ) f j νi | wi j dνi > 1.



If h (νi ) = 1 for all νi , then this condition reduces to Pr νi > ν| wi j = 1 − Fj ν| wi j >
general form, if the radiologist is indifferent in diagnosing or not diagnosing, we have

1=
=

∫

+∞

∫ν +∞
ν

1
. In the
1 + βj



1 + β j h (νi ) f j νi | wi j dνi
∫ +∞



β j (h (νi ) − 1) f j νi | wi j dνi
1 + β j f j νi | wi j dνi +
ν

≥ (1 + β j )(1 − Fj (v̄|wi j )),
as we assume h(νi ) ≥ 1. Now the marginal patient may have a lower conditional probability of having
penumonia than the case where h(νi ) = 1,∀vi , as false negatives may be more costly.
Define the optimal diagnosis rule as
d j (wi j ) = 1

∫

+∞



(1 + β j h(νi )) f j (νi |wi j )dνi > 1 .

v̄

Proposition 13 shows conditions under which the optimal diagnosis rule satisfies the threshold crossing property.
Proposition 13. Suppose the following two conditions hold:
1. For any wi0j > wi j , the conditional distribution of νi given i0j first-order dominates (FOSD) the
conditional distribution of νi given i j , i.e., Fj (νi |wi0j ) < Fj (νi |wi j ) , ∀νi ,
2. 0 < Fj (ν̄|wi j ) < 1, ∀wi j .

lim Fj (ν̄|wi j ) = 1 and

wi j →−∞

lim Fj (ν̄|wi j ) = 0.

wi j →+∞

Then the optimal diagnosis rule satisfies the threshold-crossing property, i.e., for any radiologist j,
there exists τj∗ such that


 0, wi j < τj∗,

d j (wi j ) =

 1, wi j ≥ τj∗ .

We first prove the following lemma.
Lemma 14. Suppose wi0j > wi j . If Fj (νi |wi0j ) < Fj (νi |wi j ), for each νi , then d j (wi j ) = 1 implies
d j (wi0j ) = 1.
Proof. Using integration by parts, we have

A.20

+∞

∫




f j νi |wi0j − f j νi |wi j dνi
ν
∫ +∞



  +∞
 
−
β j h 0(νi ) Fj (νi |wi0j ) − Fj (νi |wi j ) dνi
= 1 + β j h (νi ) Fj νi |wi0j − Fj νi |wi j
v̄
v̄
 ∫ +∞




 
β j h 0(νi ) Fj (νi |wi0j ) − Fj (νi |wi j ) dνi > 0,
= − 1 + β j Fj ν̄|wi0j − Fj ν̄|wi j −
1 + β j h (νi )



v̄

since Fj (νi |wi0j ) < Fj (νi |wi j ), ∀νi , h(νi ) is bounded, h(v̄) = 1, and h 0(νi ) ≥ 0.
We now proceed to the proof of Proposition 13.



Proof. The second condition of Proposition 13 ensures that
+∞

∫
lim

wi j →−∞ ν

∫

wi j →−∞

+∞

lim

wi j →+∞ ν


1 + β j h (νi ) f j (νi |wi j )dνi ≤ (1 + M β j )(1 − lim Fj (ν̄|wi j )) = 0 < 1;


1 + β j h (νi ) f j (νi |wi j )dνi ≥ (1 + β j )(1 − lim Fj (ν̄|wi j )) = 1 + β j > 1,
wi j →+∞

where M = sup h(νi ). So

lim d j (wi j ) = 0 and

wi j →−∞

lim d j (wi j ) = 1. Using Lemma 14, the optimal

wi j →+∞

diagnosis rule satisfies the threshold-crossing property. In particular, the optimal threshold τj∗ satisfies
∫

+∞

v̄


1 + β j h (νi ) f j (νi |τj∗ )dνi = 1.


Proposition 15. Suppose the conditions in Proposition 13 hold and f j is fixed. Then the optimal
threshold τj∗ decreases with β j . In particular, τj∗ → +∞ as β j → 0+ and τj∗ → −∞ as β j → +∞.
Proof. Consider radiologists j and j 0with β j > β j 0 . Denote their optimal thresholds as τj∗ and τj∗0 ,
∫ +∞

respectively. We have ν̄
1 + β j h (νi ) f j (νi |τj∗ )dνi = 1 and
+∞

1 + β j 0 h (νi )
1 + β j h (νi ) f j (νi |τj∗ )dνi
ν̄
ν̄
∫ +∞
= (β j 0 − β j )
h(νi ) f j (νi |τj∗ )dνi < 0.

∫

+∞



f j (νi |τj∗ )dνi −

∫

ν̄

So

∫ +∞
v̄


1 + β j 0 h (νi ) f j (νi |τj∗ )dνi < 1, or d j 0 (τj∗ ) = 0. By Proposition 13, we know that τj∗ < τj∗0 .

Since τj∗ decreases with β j , if bounded below or above, it must have limits as β j approaches +∞

or 0+ . We can confirm that this is not the case. For example, suppose τj∗ is bounded below. The limit

A.21

exists and is denoted by τ. Take β j ≥
∫
ν̄

+∞

1
. Then
1 − F(ν̄|τ)
1
)(1 − Fj (ν̄|τj∗ ))
1 − F(ν̄|τ)
1
> (1 +
)(1 − Fj (ν̄|τ)) = 2 − Fj (ν̄|τ).
1 − F(ν̄|τ)


1 + β j h (νi ) f j (νi |τj∗ )dνi ≥ (1 +

The second inequality holds since τj∗ > τ. Take the limit and we have
∫

+∞

lim

β j →+∞ ν̄


1 + β j h (νi ) f j (νi |τj∗ )dνi ≥ 2 − Fj (ν̄|τ) > 1.

This is a contraction, so τj∗ is not bounded below. Similarly, we can show τj∗ is not bounded above. 
From now on, we assume wi j and νi follow a bivariate normal distribution:
wi j
νi

!
∼N

0

!

0

,

1

αj

αj

1

!!
.

Conditional on observing wi j , the true signal νi follows a normal distribution N (α j wi j ,1 − α2j ). So
Fj (νi |wi j ) = Φ

νi −α j wi j
q
1−α2j

!
,

where Φ (·) is the CDF of the standard normal distribution.
Corollary 16. Suppose wi j and νi follow the bivariate normal distribution specified above. Then if
α j > 0, the optimal diagnosis rule satisfies the threshold-crossing property.
Proof. When wi j and νi follow the bivariate normal distribution with the correlation coefficient being
© νi − α j wi j ª

®. It is easy to verify that the two conditions in Proposition
α j , we have Fj νi | wi j = Φ ­­ q
®
1 − α2j
«
¬
13 hold if α j > 0.
Define the optimal threshold τj∗ = τj (α j , β j ; h̄(·)) by
∫
ν̄

+∞

1

1 + β j h (νi ) q
φ
1 − α2j


νi −α j τ ∗j
q
1−α2j

!
dν i = 1,

where φ(·) is the CDF of the standard normal distribution.



Corollary 17. The optimal threshold satisfies


q
β M
ν̄ − 1 − α2j Φ−1 1+βj j M
αj

≤ τj∗ ≤
A.22



q
β
ν̄ − 1 − α2j Φ−1 1+βj j
αj

,

where M = sup h(νi ).
Proof. Since h(νi ) ≥ 1, we have

1=

∫

+∞

!

νi −α j τ ∗j
q
1−α2j

1

(1 + β j h(νi )) q
φ
dν i
1 − α2j
!
∫ +∞
νi −α j τ ∗j
1
φ q 2 dν i
≥ (1 + β j )
q
1−α j
ν̄
1 − α2j
!!
ν̄

= (1 + β j ) 1 − Φ

ν̄−α j τ ∗j
q
1−α2j

.

Rearrange and we can get the upper bound of τj∗ . Similarly, we can derive the lower bound of τj∗ .
The proposition below summarizes the relation between the general case and case where h(νi ) =
1,∀vi .



Proposition 18. Let τj∗ = τj (α j , β j ; h(·)). Define
∫ +∞
ν̄

β 0j

β 0j (α j , β j ; h(·))

=

h(νi )φ

= βj
∫ +∞
ν̄

φ

νi −α j τ ∗j
q
1−α2j

νi −α j τ ∗j
q
1−α2j

!
dνi
.

!
dνi

Then we can use the new β 0j to characterize the optimal threshold:
τj (α j , β j ; h(·)) = τj (α j , β 0j ; h(·) = 1).
Proof. Let τj∗ = τj (α j , β j ; h(·)) and τj∗0 = τj (α j , β 0j ; h(·) = 1). Then
∫
ν̄

+∞

1

φ
1 + β j h (νi ) q
1 − α2j


νi −α j τ ∗j
q
1−α2j

!
dνi =

∫
ν̄

+∞



1 + β 0j



1

φ
q
1 − α2j

νi −α j τ ∗0
j
q
1−α2j

Substitute the expression of β 0j into the second equality and we have

∫
ν̄

©
­
­1 + β j
­
­
­
«

+∞ ­

∫ +∞
ν̄

h(νi )φ

νi −α j τ ∗j
q
1−α2j

!

dνi ª®
®
®q 1
!
φ
®
∫ +∞ νi −α j τ ∗j
® 1 − α2
j
φ q 2 dνi ®
ν̄
1−α j
¬

A.23

νi −α j τ ∗0
j
q
2
1−α j

!
dνi = 1

!
dνi = 1.

∫ +∞

∫

+∞ ν̄

νi −α j τ ∗j
q
1−α2j

(1 + β j h(νi ))φ

⇒
ν̄

∫ +∞
ν̄

+∞

∫

1

⇒q
1 − α2j
|

ν̄

φ

νi −α j τ ∗j
q
1−α2j

!
dνi

νi −α j τ ∗0
j
q
1−α2j

!

∫ +∞

!

φ

νi −α j τ ∗0
j
q
1−α2j

!

φ

νi −α j τ ∗j
q
1−α2j

φ
q
1 − α2j

!
dνi

ν̄

!

νi −α j τ ∗j
q
1−α2j

(1 + β j h(νi ))φ

1

dνi
∫ +∞
ν̄

{z

}

=1

+∞

∫
⇒
ν̄

φ

νi −α j τ ∗0
j
q
2
1−α j

!
dνi =

+∞

∫
ν̄

φ

νi −α j τ ∗j
q
1−α2j

dνi = 1

dνi
=1
dνi

!
dνi .

So we have τj∗0 = τj∗ .



Proposition 19. For fixed β j and h(·), β 0j = β 0j (α j , β j ; h(·)) decreases with α j .
Proof. The optimal threshold τj∗ = τj (α j , β j ; h(·)) is given by
∫

+∞

1 + β j h (νi ) q

1



ν̄

1 − α2j

φ

νi −α j τ ∗j
q
1−α2j

!
dνi = 1.

By Proposition 18, we can write

∫ +∞
ν̄

β 0j

h(νi )φ

= βj
∫ +∞
ν̄

∫ +∞
ν̄

φ

νi −α j τ ∗j
q
1−α2j

νi −α j τ ∗j
q
1−α2j

!
dνi

dνi

ν̄

∫ +∞
ν̄

!
dνi −

νi −α j τ ∗j
q
1−α2j

φ

νi −α j τ ∗j
q
1−α2j

(1 + β j h(νi ) − 1)φ
∫ +∞

νi −α j τ ∗j
q
1−α2j

=

ν̄

=

!

(1 + β j h(νi ))φ

∫ +∞

∫ +∞
ν̄

φ

φ

νi −α j τ ∗j
q
1−α2j

νi −α j τ ∗j
q
1−α2j

!
dνi

!
dνi

!
dνi

q
1 − α2j

=

!

∫ +∞

dνi

ν̄

φ

νi −α j τ ∗j
q
1−α2j

− 1.

!
dνi

q
νi − α j τj∗
Define xi = q
. Then dνi = 1 − α2j dxi . Using variable transformation, we have
1 − α∗j

β 0j =

q
1 − α2j
∫ +∞
ν̄

φ

νi −α j τ ∗j
q
1−α2j

1

−1 =

!
dνi

A.24

1−Φ

ν̄−α j τ ∗j
q
1−α2j

! − 1.

νi − α j τj∗
Denote Q(νi ,α j , β j ) = q
. For fixed β j , the relationship between β 0j and α j reduces the relation2
1 − αj
ship between Q(ν̄,α j , β j ) and α j . Using integration by parts for the formula of the optimal threshold,
we have

1=

∫

+∞

νi −α j τ ∗j
q
1−α2j

1

φ
1 + β j h (νi ) q
1 − α2j
! +∞ ∫
νi −α j τ ∗j
= (1 + β j h(νi ))Φ q 2
−


ν̄

1−α j

ν̄

ν

+∞

!
dνi =

β j h (νi )Φ
0

= 1 + β j M − (1 + β j )Φ(Q(ν̄,α j , β j )) − β j

∫

+∞

ν̄

+∞

∫
ν̄

∂Φ
1 + β j h (νi )

vi −α j τ ∗j
q
1−α2j



νi −α j τ ∗j
q
1−α2j

!

∂vi

dνi

!
dνi

h 0(νi )Φ(Q(νi ,α j , β j ))dνi ,

where M = sup h(νi ). Take the derivative with respect to α j ,
∂Q(ν̄,α j , β j )
0 = −(1 + β j )φ(Q(ν̄,α j , β j ))
∂αi
∫ +∞
∂Q(νi ,α j , β j )
−β j
dνi .
h 0(νi )φ(Q(νi ,α j , β j ))
∂α j
ν̄

(A.19)

∂Q(ν̄,α j , β j )
≤ 0 for all α j ∈ (0,1). We prove this by contradiction. Assume
∂αi
∂ 2 Q(vi ,α j , β j )
αj
∂Q(ν̄,α j , β j )
> 0, we
=
that for some α j0 ∈ (0,1), we have
> 0. Since
∂αi
∂α j ∂vi
(1 − α j )3/2
α j =α0j
∂Q(ν̄,α j , β j )
know that
increases with vi for any fixed α j ∈ (0,1), in particular for α j = α j0 . Then
∂αi
∂Q(vi ,α j , β j )
∂Q(ν̄,α j , β j )
≥
> 0 for any νi ≥ ν̄. Since h 0(νi ) ≥ 0, we have
∂αi
∂α
0
0
i
α j =α
α j =α
We want to show that

j

j

∂Q(ν̄,α j , β j )
|α j =α0j > 0,
∂αi

∫
ν̄

+∞

h 0(νi )φ(Q(νi ,α j , β j ))

∂Q(νi ,α j , β j )
dνi |α j =α0j ≥ 0.
∂α j

Then Equation (A.19) cannot hold for α j = α j0, as the right hand is strictly negative, a contradiction.
∂Q(ν̄,α j , β j )
So, we must have
≤ 0, ∀α j ∈ (0,1). Therefore,
∂αi
∂ β 0j
∂α j

∂Q(ν̄,α j , β j )
∂α j
≤ 0.
(1 − Φ(Q(ν̄,α j , β j )))2

φ(Q(ν̄,α j , β j ))
=



A.25

A.6

Structural Estimation

A.6.1

Risk-Adjustment Procedure

Because quasi-random assignment is conditional and because we find that quasi-random assignment
does not strictly hold in all VHA stations, we use risk-adjusted data instead of raw data for the baseline
estimation of our structural model. We form the risk-adjusted data using the following procedure:
1. Estimate linear probability models of diagnoses, or di , and type II errors, or f ni , controlling
for patient characteristics Xi and interactions between time categories Ti and station identities
` (i):
d
d
di = ζ j(i)
+ Xi β d + Ti γ`(i)
+ εid ;
fn

fn

fn

f ni = ζ j(i) + Xi β f n + Ti γ`(i) + εi .
Note that first equation is the same as the first-stage equation in reduced-form 2SLS regressions
fn

using radiologist dummies as instruments. The estimates of ζ jd and ζ j

are also the same as

those used for radiologist risk-adjusted rates in Appendix A.2.1.
2. Ensure that the patient-weighted average risk-adjusted rate in each station is equal to the population rate:
Í
µ`d + j ∈J` n j ζˆjd
Í
j ∈J` n j
fn Í
fn
µ` + j ∈J` n j ζˆj
Í
j ∈J` n j

Í
=
=

d
j nj

;

j nj
Í fn
j nj

Í

Í

j nj

,

fn

for all `, by setting µ`d and µ` to equalize the relevant station-specific rate to the population
Í
Í
fn
rate. As in Section 5.2, we define ndj ≡ i ∈I j 1 (di = 1), n j ≡ i ∈I j 1 ( f ni = 1), n j ≡ I j , and
I j ≡ {i : j (i) = j}.
3. Truncate the risk-adjusted rates at 0:
!
ζ˜jd = max 0, ζˆjd +

Õ

1 ( j ∈ J` ) µ`d ;

`

!
fn
ζ˜j

= max

fn
0, ζˆj +

Õ
`

1(j ∈

fn
J` ) µ`

.

4. Use the resulting rates to impute risk-adjusted diagnosis and type II error counts, which are not
fn
fn
necessarily integers: ñd = n j ζ˜d and ñ = n j ζ˜ .
j

j

j

j

Since d˜j and ỹ j are estimated objects, we redraw patient samples, stratified by radiologist, with replacement, in order to compute standard errors of our second-step structural estimates.
A.26

A.6.2

Simulated Maximum Likelihood


In Section 5.2, we estimate the hyperparameter vector θ ≡ µα, µβ ,σα,σβ ,λ,ν by maximum likelihood:
θ̂ = arg max

Õ

θ

∫
log




fn
L j ñdj , ñ j ,n j γ j f γ j θ dγ j .

j

To calculate the radiologist-specific likelihood,
Lj



fn
ñdj , ñ j ,n j



θ =

∫




fn
L j ñdj , ñ j ,n j γ j f γ j θ dγ j ,

we need to evaluate the integral numerically. We use Monte Carlo integration, which generates a large

number R of random draws γrj following the density f γ j θ , given any hyperparameter vector θ.
These draws are taken as the realizations of γ j . Then we take the average across all realizations of
the likelihood as a simulated approximation of the integral:
R

 1Õ


fn
fn
L j ñdj , ñ j ,n j θ ≈
L j ñdj , ñ j ,n j γrj .
R r=1

The overall log-likelihood becomes
log L

A.6.3



fn
ñdj , ñ j ,n j

J
j=1

!
R


1Õ
d fn
r
L j ñ j , ñ j ,n j γ j .
θ ≈
log
R r=1
j=1


J
Õ

Empirical Bayes Posteriors



After estimating θ̂, we want to find the empirical Bayes posterior mean γ̂ j = α̂ j , β̂ j for each radiologist j. Using Bayes’ theorem, the empirical conditional posterior distribution of γ j is



 

fn
fn
f ñdj , ñ j ,n j γ j f γ j θ̂
f γ j , ñdj , ñ j ,n j θ̂
fn

 =∫ 
 

f γ j ñdj , ñ j ,n j ; θ̂ =
,
fn
fn
f ñdj , ñ j ,n j θ̂
f ñdj , ñ j ,n j γ j f γ j θ̂ dγ j








fn
fn
where f ñdj , ñ j ,n j γ j is equivalent to L j ñdj , ñ j ,n j γ j . The denominator is then equivalent to


fn
the likelihood L j ñdj , ñ j ,n j θ . The empirical Bayes predictions are the posterior means

γ̂ j =

∫

 


fn
γ j f ñdj , ñ j ,n j γ j f γ j θ̂ dγ j
fn
 

.
γ j f γ j ñdj , ñ j ,n j ; θ̂ dγ j = ∫ 
fn
f ñdj , ñ j ,n j γ j f γ j θ̂ dγ j
∫





A.27

As above, the integrals are evaluated numerically. We generate R random draws γrj following the


distribution f γ j θ̂ and calculate the empirical Bayes posterior means as


r f ñ d , ñ f n,n γ r
γ
j
r=1 j
j
j j


.
γ̂ j =
1 ÍR
d , ñ f n,n γ r
f
ñ
j
R r=1
j
j j
1
R

A.6.4

ÍR

Potentially Incorrect Beliefs

Under the model of radiologist signals implied by Equation (5), we can identify each radiologist’s
skill α j and her diagnostic threshold τj . The utility in Equation (6) implies the optimal threshold in
Equation (7), as a function of skill α j and preference β j . If radiologists know their skill, then this
allows us to infer β j from α j and τj .
In this appendix, we allow for the possibility that radiologists may be misinformed about their
skills: A radiologist may believe she has skill α j0 even though her true skill is α j . Since only (true) α j
and τj are identified, we cannot separately identify α j0 and β j from Equation (7). In this exercise, we
therefore assume β j , in order to infer α j0 for each radiologist.

We start with our baseline model and form an empirical Bayes posterior of α j , β j for each radiologist. We use Equation (7) to impute the empirical Bayes posterior of τj . Thus, for each radiologist,

we have an empirical Bayes posterior of α j , β j ,τj from our baseline model; the distributions of the
posteriors for α j , β j , and τj are shown in separate panels of Appendix Figure A.5.
To extend this analysis to impute each radiologist’s belief about her skill, α j0 , we perform the following two additional steps: First, we take the mode of the distribution of empirical Bayes posteriors

α j j ∈ J , which we calculate as 8.1 within one decimal place. Second, we set all radiologists to have
β j = 8.1. We use each radiologist’s empirical Bayes posterior of τj and the formula for the optimal
threshold in Equation (7) to infer her belief about her skill, α j0 .
The relationship between α j0 , β j , and τj is shown in Figure 7. As shown in the figure, for β j ≈ 8.1,
the comparative statics of τj∗ are first decreasing and then increasing with a radiologist’s perceived
α j0 . Thus, holding fixed β j = 8.1, an observed τj does not generally imply with a single value of α j0 .
If τj is too low, then there will not be a value of α j0 to generate τj with β j = 8.1; this case occurs only
for a minority of radiologists. Other τj generally can be consistent with either a value of α j0 on the
downward-sloping part of the curve or with a value of α j0 on the upward-sloping part of the curve. In
this case, we take the higher value of α j0 , since the vast majority of empirical Bayes posteriors of α j
are on the upward-sloping part of Figure 7.
Appendix Figure A.8 plots each radiologist’s perceived skill, or α j0 , on the y-axis and her actual
skill, or α j , on the x-axis. The plot shows that the radiologists’ perceptions of their skill generally
correlate well with their actual skill, particularly among higher-skilled radiologists. Lower-skilled
radiologists, however, tend to over-estimate their skill relative to the truth.

A.28

A.7

Alternative Implementations

In this appendix, we discuss alternative empirical implementations from the baseline approach. Appendix Table A.5 presents results for the following empirical approaches, which vary with respect to
sample selection, risk adjustment, and outcome variable definition:
1. Baseline. This column presents results for the baseline empirical approach. This approach
uses observations from all stations; the sample selection procedure is given in Appendix Table
A.1. We risk-adjust diagnosis and type II error by 77 patient characteristic variables, described
in Section 4.1, in addition to the controls for time dummies interacted with stations dummies
required for plausible quasi-random assignment in Assumption 1. We define a type II error as
a case that was not diagnosed initially with pneumonia but returned within 10 days and was
diagnosed at that time with pneumonia.
2. Balanced. This approach modifies the baseline approach by restricting to 44 stations we select
in Appendix A.2.2 with stronger evidence for quasi-random assignment. Risk-adjustment and
the definition of a type II error are unchanged from baseline.
3. No controls. This approach modifies the baseline approach by controlling for no patient characteristics. The only controls for risk-adjustment are time dummies interacted with station
dummies, as specified by Assumption 1. The sample and outcome definition are unchanged
from baseline.
4. VA users. This approach restricts attention to a sample of veterans who use VA care more than
non-VA care. We identify this sample among dual enrollees in Medicare and the VA. We access
both VA and Medicare records of care inside and outside the VA, respectively. We count the
number of outpatient, ED, and inpatient visits in the VA and in Medicare, and keep veterans
who have more total visits in the VA than in Medicare. The risk-adjustment and outcome
definition are unchanged from baseline.
5. Admission. This approach redefines a type II error to only occur among patients with a greater
than 50% predicted chance of admission. Patients with a lower predicted probability of admission are all coded to have f ni = 0. The sample selection and risk adjustment are the same as in
baseline.

A.7.1

Rationale

Relative to the baseline approach, the “balanced” and “no controls” approaches respectively evaluate
the importance of selecting stations with stronger evidence of quasi-random assignment and of controlling for rich patient observable characteristics. If results are qualitatively unchanged under these
approaches, then it is less likely that potential non-random assignment could be driving our results.
We evaluate results under the “VA users” approach in order to assess the potential threat that type
II errors may be unobserved if patients fail to return to the VA and therefore be detected as having a
A.29

missed initial diagnosis. Although the process of returning to the VA is endogenous, it is only a concern under non-random assignment of patients to radiologists or under exclusion violations in which
radiologists may influence the likelihood that a patient returns to the VA, regardless of actually incurring a type II error. Veterans who predominantly use the VA relatively to non-VA options are more
likely to return to the VA for unresolved symptoms. Therefore, if results are qualitatively unchanged
from baseline, then exclusion violations and endogenous return visits are unlikely to explain our key
findings.
Similarly, we assess an alternative definition of a type II error in the “admission” approach, requiring that patients are highly likely to be admitted as an inpatient based on their observed characteristics.
Admitted patients have a built-in pathway for re-evaluation if signs and symptoms persist, worsen,
or emerge; they need not decide to return to the VA. This approach also addresses a related threat
that fellow ED radiologists may be more reluctant to contradict some radiologists than others, since
admitted patients typically receive radiological evaluation from other divisions of radiology.

A.7.2

Results

Table A.5 provides results for each empirical approach in four panels. Panel A reports sample statistics and reduced-form moments. All empirical implementations result in similarly large variation in
diagnosis rates and type II error rates across radiologists. Weighted standard deviations for both rates
are calculated from Equation (A.5). More importantly, the standard deviation of residual type II error
rates, after controlling for radiologist diagnosis rates, reveals that substantial heterogeneity in outcomes remains even after controlling for heterogeneity in decisions. This suggests violations, under
all approaches, in the strict version of monotonicity in Condition 1(iii). Finally, the slope statistics corresponding to 2SLS (using radiologist dummies as instruments) and JIVE remain similarly
positive across approaches. This suggests consistently strong violations in the weaker monotonicity
condition in Condition A.1.
Panel B reports model parameter estimates under each approach. The estimates are very stable
across approaches. While point estimates under the “balanced” approach suggest that radiologists
may be more accurate than under the approaches, the set of radiologists measured under this approach are by construction different than the set of radiologists in the other approaches. Furthermore,
estimates are less precise in the “balanced” approach, likely because it involves fewer observations
and radiologists.

Panel C presents corresponding moments in the distribution of α j , β j implied by the model
parameters. The implementations again suggest qualitatively similar distributions of α, β, and τ.
Interestingly, radiologists seem to incur higher relative disutility for a type II error among patients
who are likely to be admitted. This could reflect the fact that these patients are sicker and may suffer
worse outcomes under a type II error than healthier patients.
Panel D summarizes policy implications from decomposing variation into skill and preference
components, as described in Section 6. In all implementations, more variation in diagnosis can be
explained by heterogeneity in skill than by heterogeneity in preferences. An even larger proportion of
A.30

variation in type II errors can be explained by heterogeneity in skill; essentially none of the variation
in type II errors can be explained by heterogeneity in preferences.

References
A NDREWS , M. J., L. G ILL , T. S CHANK ,

AND

R. U PWARD (2008): “High Wage Workers and Low

Wage Firms: Negative Assortative Matching or Limited Mobility Bias?” Journal of the Royal
Statistical Society: Series A (Statistics in Society), 171, 673-697.

A.31

Figure A.1: Concordance Between Tests of Quasi-Random Assignment
1

F−Test p−Value

.8

.6

.4

.2

0
0

.2

.4

.6

.8

1

RI p−Value
Note: This figure shows the the concordance between p-values of tests of quasi-random assignment of patient
age across radiologists in each station. On the x-axis, we plot the p-value for randomization inference (RI); on
the y-axis, we plot the p-value of an F-test for the joint significance of radiologist dummies. We condition on
time dummies interacted with station dummies in both tests. Appendix A.2.2 provides further details.

A.32

Figure A.2: Quasi-Random Assignment of Hold-Out Characteristics
B: Diagnosis, F-test

0

0

10

10

20

20

30

40

30

A: Diagnosis, RI

0

.2

.4

.6

.8

1

0

.4

.6

.8

1

.8

1

0

0

10

20

20

30

40

40

60

D: Type II Error, F-test

50

C: Type II Error, RI

.2

0

.2

.4

.6

.8

1

0

.2

.4

.6

Note: This figure plots histograms of p-values of tests of quasi-random assignment across radiologists in each
station. Randomization inference (RI) p-values are shown in Panels A and C; F-test p-values are shown in
Panels B and D. Using either randomization inference or F-tests, we first test whether age is quasi-randomly
assigned across radiologists in a given station. From these tests, we identify 44 out of 104 stations in which
we cannot reject the null of quasi-random assignment. Among these 44 stations, we then confirm whether the
stations originally identified to feature quasi-random assignment with respect to age also pass tests with respect
to predicted diagnosis or predicted type II error. These predictions are based on 77 “hold-out” variables of
rich patient characteristics. In each panel, light gray bars represent station counts among the 60 stations that
failed the test according to age; dark gray bars represent station counts out of the 44 stations that passed the test
according to age. We condition on time dummies interacted with station dummies in all tests. Appendix A.2.2
provides further details.

A.33

Figure A.3: Visual IV
.08

Type II error rate

.06

.04

.02

Coeff = 0.094 (0.007)
N = 4,663,840, J = 3,199

0
0

.05

.1

.15

Diagnosis rate
Note: This figure shows the visual IV plot corresponding to a 2SLS regression with radiologist dummies as
instruments. For each radiologist with more than 100 chest X-rays, we plot a dot with average risk-adjusted
predictions of diagnosis on the x-axis and average risk-adjusted predictions of type II error on the y-axis.
Diagnosis predictions correspond to a first-stage regression in Equation (A.7), and type II error predictions
correspond to a reduced-form regression in Equation (A.8). The best-fit line in the visual IV plot replicates the
coefficient from the 2SLS regression with radiologist dummies as instruments, which we perform to obtain the
standard error (in parentheses); the coefficient and standard error are identical to those shown in Panel A of
5. As in our baseline specification, we control for all patient characteristics and time dummies interacted with
station dummies. Further details are given in Appendix A.3.

A.34

A.35

Frequency

0

10

20

30

0

10

20

0.00

0.00

0.05
0.10
Diagnostic rate

0.05
0.10
Diagnostic rate

0.15

0.15

0

20

40

60

0

20

40

60

0.00

0.00

0.02
0.04
0.06
Type II error rate

B: Simulated Moments

0.02
0.04
0.06
Type II error rate

A: Observed Moments

0.08

0.08

0.00

0.02

0.04

0.06

0.08

0.00

0.02

0.04

0.06

0.08

0.00

0.00

0.05
0.10
Diagnostic rate

0.05
0.10
Diagnostic rate

0.15

Coef. = 0.063

0.15

Coef. = 0.082

Note: This figure compares the actual moments observed in the data (the first row) with the moments simulated using the estimated parameters and simulated
primitives from the main specification (the second row). To arrive at simulated moments in the second row, we first fix the number of patients each radiologist
examines to the actual number and simulate the primitives for each radiologist, α j and β j . We then simulate patients at risk from a binomial distribution with the
probability of being at risk 1 − κ. For patients at risk, we simulate their νi and wi j and determine whether they have pneumonia and the radiologist’s diagnosis
decisions, given the threshold ν for pneumonia and the radiologist’s diagnostic threshold τj computed using simulated primitives. For patients that are at risk, not
diagnosed, and do not have pneumonia, we simulate cases where they simply get worse using a binomial distribution with the probability of getting worse λ. We
then calculate the diagnosis rate and the type II error rate for each radiologist. These parameters are described in further detail in Section 5.

Frequency

30
Frequency
Frequency

Figure A.4: Model Fit

Type II error rate
Type II error rate

Figure A.5: Distributions of Radiologist Posterior Means
400

400
Frequency

Frequency

300

200

200

100

0

0
0.4

0.6

0.8

1.0

1.0

1.2

1.4
τ

α

1.6

1.8

12

500

10
300
β

Frequency

400

8

200
100

6
0
6

8

10

12

β

Correlation = −0.29

0.4

0.6

0.8

1.0

α

Note: This figure plots the distributions of radiologist empirical Bayes posterior means of our main specification. The first three subfigures plot the distributions of evaluation skills, the diagnostic thresholds, and the
preferences. The last subfigure plots the joint distribution of the evaluation skills and preferences.

A.36

Figure A.6: ROC Curve with Model-Generated Moments

1.00

True positive rate

0.75

0.50

0.25

0.00
0.00

0.25

0.50

0.75

1.00

False positive rate

Note: This figure presents the radiologist posterior means of our main specification in ROC space. Radiologist
posterior means are the same as shown in Figure A.5 and are formed using empirical Bayes posteriors. The
figure also plots the iso-preference curves for β = 6,8 and 10 from (0,0) to (0,1) in ROC space. Each isopreference curve illustrates how the optimal point in ROC space varies with the evaluation skill for a fixed
preference.

A.37

Figure A.7: Heterogeneity in Preference
A: Age

B: Chest X-rays Focus
.45
Percent of chest X−rays

Age (years)

70

60

50
Coeff = −0.000 (0.205)
N = 11,876

40
5

6

7

.35

.25
Coeff = −0.014 (0.007)
N = 3,199

.15

8

7

8

β

10

D: Log Median Report Length

7

4.2
Median log report length

Median log time (minutes)

C: Log Median Time

6

5
Coeff = −0.368 (0.097)
N = 3,199

4
7

8

9

3.9

3.6
Coeff = 0.032 (0.018)
N = 3,133

3.3

10

7

8

β

10

F: Gender
Share of male radiologists

400
300
200
100
Coeff = 17.7 (10.8)
N = 1,697

0
7

9
β

E: Medical School Rank

Medical school rank

9
β

8

9

10

β

.9

.8

.7
Coeff = −0.026 (0.017)
N = 2,604

.6
7

8

9

10

β

Note: This figure shows the relationship between a radiologist’s empirical Bayes posterior of her accuracy
(α) on the x-axis and the following variables on the y-axis: (i) the radiologist’s age; (ii) the proportion of the
radiologist’s exams that are chest X-rays; (iii) the log median time that the radiologist spends to generate a
chest X-ray report; (iv) the log median length of the issue reports; (v) the rank of the medical school that the
radiologist attended according to U.S. News & World Report; and (vi) gender. Except for gender, the three
lines show the fitted values from the 25th, 50th, and 75th quantile regressions. For gender, the line shows the
fitted values from the usual regression. The dots are the median values of the variables on the y-axis within
each bin of β. 30 bins are used. Figure 8 shows the corresponding plots with diagnostic skills (α) on the x-axis.

A.38

Figure A.8: Possibly Incorrect Beliefs about Accuracy

1.0

Preceived accuracy

0.9

0.8

0.7

0.6
0.6

0.7

0.8

0.9

1.0

True accuracy

Note: This figure plots the relationship between radiologists’ true accuracy and perceived accuracy, in an
alternative model in which variation in diagnostic thresholds for a given skill is driven by variation in perceived
skill, holding preferences fixed. This contrasts with the baseline model in which radiologists perceive their
true skill but may vary in their preferences. We calculate the modal preference from our benchmark estimation
results at β = 8, and we assign this preference parameter to all radiologists. We then use the formula for the
optimal threshold as a function of β = 8 and (perceived) accuracy to calculate perceived accuracy. Appendix
A.6.4 describes this procedure to calculate perceived accuracy in further detail.

A.39

A.40

78,680

Note: This table describes key sample selection steps, the observations dropped, and the observations remaining after each step.

7. Drop radiologists with fewer than
100 remaining cases

75,281

6. Drop radiologist-month pairs with
fewer than 5 observations

This mitigates against limited mobility bias
(Andrews et al. 2008), since we include
month-year interactions as part of Ti in all our
regression specifications of risk-adjustment

6,198

599,291

5. Drop patients with age greater than
100 or less than 20

Since we are interested in subsequent outcomes
(e.g., return visits), we focus on initial chest
X-rays with no prior chest X-rays within 30 days

3. Retain patient-days that are at least
30 days from the last chest X-ray

96,154

4,565

If there are multiple radiologists among the
chest X-rays, we assign the patient-day to the
radiologist corresponding to the first chest X-ray
in the patient-day

2. Collapse multiple chest X-rays in a
patient-day into one observation

4,663,826

4,742,506

4,817,787

4,823,985

4,828,550

5,427,841

Observations
Dropped
Remaining
5,523,995

4. Drop observations with missing
radiologist identity or patient age or
gender

Description
We define chest X-rays by the Current
Procedural Terminology (CPT) codes of 71010
and 71020, and we require the status of the chest
X-ray to be “complete”

Sample step
1. Pull chest X-ray observations from
October 1999 to September 2015,
inclusive

Table A.1: Sample Selection

A.41
733,627
553

731,015
541

744,595
535

720,047
559

Type II error rate (p.p.)
Below-median Above-median Difference
2.00
2.46
0.46
(0.64)
(0.84)
(0.05)
2.23
2.23
0.00
(0.20)
(0.21)
(0.01)
2.22
2.23
0.00
(0.15)
(0.14)
(0.01)
2.23
2.23
-0.00
(0.09)
(0.09)
(0.01)
2.22
2.23
0.02
(0.33)
(0.35)
(0.02)
2.23
2.23
-0.00
(0.20)
(0.20)
(0.01)
2.22
2.24
0.02
(0.37)
(0.39)
(0.02)

Note: This table presents results assessing balance across radiologists according to patient characteristics. Unlike the main balance table (Table 1), this table
restricts to the sample of 44 stations for which we cannot reject quasi-random assignment, described in Appendix A.2.2. Columns 1 to 3 compare radiologists with
below- or above-median risk-adjusted diagnosis rates. Columns 4 to 6 compare radiologists with below- or above-median risk-adjusted type II error rates. For
context, the risk-adjusted diagnosis rate is given in the first row for below- and above-median radiologists in Columns 1 and 2, respectively; case-weighted standard
deviations of diagnosis rates are also shown in parentheses for each of the groups. The difference between the two groups is given in Column 3, with the standard
error of the difference shown in parentheses. Similarly, the risk-adjusted type II error rates for the corresponding below- and above-median group are displayed in
Columns 4 and 5, respectively, in the first row; the difference between those two groups is given in Column 6. The subsequent six rows examine balance in patient
characteristics by showing analogous differences in predicted diagnosis rates (Columns 1 to 3) or predicted type II error rates (Columns 4 to 6), where different sets
of patient characteristics are used for linear predictions. Patient characteristic variables are described in further detail in Section 4.1. WBC stands for white blood
cell. In the last two rows, we display the number of cases and the number of radiologists in each group. Appendix A.2.1 provides further details on the calculations.

Number of cases
Number of radiologists

Predicted outcome using all variables

Predicted outcome using ordering characteristics

Predicted outcome using vitals and WBC count

Predicted outcome using prior utilization

Predicted outcome using prior diagnosis

Predicted outcome using demographics

Outcome

Diagnosis rate (p.p.)
Below-median Above-median Difference
6.89
8.10
1.21
(1.68)
(1.99)
(0.11)
7.49
7.50
0.01
(0.61)
(0.55)
(0.03)
7.49
7.50
0.01
(0.35)
(0.35)
(0.02)
7.49
7.50
0.02
(0.14)
(0.14)
(0.01)
7.44
7.54
0.10
(1.06)
(1.13)
(0.07)
7.49
7.50
0.01
(0.60)
(0.59)
(0.04)
7.45
7.53
0.08
(1.26)
(1.29)
(0.08)

Table A.2: Balance in the Subset of Stations

Table A.3: JIVE Estimates of Slopes between Diagnosis and Other Outcomes
Outcome
Admissions within 30 days

Alive within 30 days

ED visits within 30 days

ICU visits within 30 days

Inpatient-days in initial admission

Inpatient-days within 30 days

Mortality within 30 days

All
0.834
(0.072)
[0.633]
-0.121
(0.019)
[0.967]
0.162
(0.072)
[0.290]
0.170
(0.025)
[0.044]
8.309
(0.950)
[2.530]
8.798
(0.636)
[3.330]
0.121
(0.019)
[0.033]

Diagnosed
0.872
(0.019)
[0.065]
0.943
(0.008)
[0.064]
0.297
(0.018)
[0.020]
0.088
(0.009)
[0.006]
5.070
(0.271)
[0.333]
5.655
(0.199)
[0.396]
0.057
(0.008)
[0.006]

False negative
0.321
(0.024)
[0.027]
0.229
(0.016)
[0.019]
0.108
(0.016)
[0.011]
0.042
(0.008)
[0.004]
1.327
(0.216)
[0.133]
2.015
(0.193)
[0.183]
0.034
(0.006)
[0.003]

True negative
-0.358
(0.069)
[0.542]
-1.294
(0.024)
[0.884]
-0.242
(0.069)
[0.260]
0.040
(0.022)
[0.034]
1.912
(0.887)
[2.064]
1.128
(0.580)
[2.751]
0.030
(0.016)
[0.025]

Note: This table presents results for other outcomes, using the jackknife instrumental variable estimator (JIVE),
shown for the benchmark outcome of type II error in Panel B of Figure 5. The estimator uses the jackknife
instrument in Equation (4) to calculate the effect of diagnosis on each outcome. The formula for the estimator
is given in Equation (A.9) and controls for 77 variables for patient characteristics and time dummies interacted
with location dummies. Column 1 gives results for the main outcome. Columns 2-4 gives results for joint
dependent variables of the outcome interacted with diagnosis and type II error dummies. For example for
outcome yi , diagnosis decision di , and disease state (only observed for undiagnosed patients upon a return
visit) si , patients who are diagnosed have 1(di = 1), patients who are a false negative have 1 (di = 0,si = 1),
and patients who are a true negative have 1 (di = 0,si = 0). The joint outcomes in Columns 2-4 are then,
respectively, yi 1 (di = 1), yi 1 (di = 0,si = 1), and yi 1 (di = 0,si = 0). Standard errors for the IV estimate are
given in parentheses, and mean dependent variables are given in brackets.

A.42

A.43
Yes
Yes

0.199
(0.009)
0.051
2,331,955

0.276
(0.013)
0.051
2,331,955

Older

Yes
Yes

0.430
(0.016)
0.089
2,331,853

0.471
(0.015)
0.089
2,331,853

Younger

Yes
Yes

0.125
(0.006)
0.023
2,331,892

0.199
(0.009)
0.023
2,331,892

Yes
Yes

0.769
(0.030)
0.117
2,331,904

0.542
(0.018)
0.117
2,331,904

Yes
Yes

0.217
(0.010)
0.075
3,046,639

0.410
(0.012)
0.075
3,088,640

Outcome: Diagnosed, di
Low
White
Pr (di )

Yes
Yes

0.267
(0.014)
0.059
1,570,738

0.303
(0.017)
0.059
1,575,011

Non-White

Yes
Yes

0.155
(0.008)
0.069
3,321,557

0.404
(0.011)
0.069
3,456,457

Daytime

Yes
Yes

0.277
(0.019)
0.073
1,200,497

0.278
(0.021)
0.073
1,207,245

Nighttime

Note: This table shows results from informal tests of monotonicity that are standard in the judges-design literature. Each column corresponds to a different
subsample of observations. In each subsample, we run first stage regressions of the effect of a judges-design instrument on diagnosis, controlling for 77 variables
for patient characteristics and time dummies interacted with location dummies. Panel A shows results from Equation (A.10), using a standard jackknife instrument.
Panel B shows results from Equation (A.11), using a reverse-sample instrument.

Time × station fixed effects
Patient controls

Mean outcome
Observations

Panel B: Reverse-Sample
Instrument, Z j−(m,x)

Mean outcome
Observations

Panel A: Baseline
Instrument, Z j−i

Subsample

High
Pr (di )

Table A.4: Informal Monotonicity Tests

Table A.5: Alternative Implementations
Baseline
Balanced No controls VA users
Panel A: Data and Reduced-Form Moments
SD of diagnosis
1.060
1.037
1.229
1.125
SD of type II error
0.504
0.459
0.531
0.584
SD of residual type II error
0.496
0.456
0.510
0.580
Slope, 2SLS
0.094
0.064
0.140
0.063
Slope, JIVE
0.263
0.342
0.270
0.315
Number of observations
4,663,840
1,464,642
4,663,840
3,099,211
Number of radiologists
3,199
1,094
3,199
3,199
Panel B: Model Parameter Estimates
µα
0.897
0.445
0.979
1.009
(0.038)
(0.047)
(0.034)
(0.045)
σα
0.332
0.255
0.408
0.450
(0.010)
(0.012)
(0.010)
(0.013)
µβ
2.080
2.840
2.116
1.831
(0.056)
(0.128)
(0.044)
(0.053)
σβ
0.128
0.073
0.144
0.190
(0.006)
(0.007)
(0.006)
(0.008)
λ
0.021
0.024
0.022
0.018
(0.000)
(0.001)
(0.000)
(0.000)
ν̄
1.781
2.046
1.775
1.730
(0.020)
(0.047)
(0.016)
(0.017)
κ
0.196
0.196
0.196
0.196
Panel C: Radiologist Primitives
Mean α
0.839
0.699
0.851
0.853
10th percentile
0.719
0.558
0.713
0.703
90th percentile
0.934
0.824
0.953
0.960
Mean β
8.063
17.156
8.380
6.349
10th percentile
6.795
15.602
6.901
4.898
90th percentile
9.416
18.769
9.971
7.944
Mean τ
1.362
1.325
1.363
1.411
10th percentile
1.270
1.253
1.249
1.296
90th percentile
1.453
1.403
1.479
1.516
Panel D: Variation Decomposition
Diagnosis
Uniform skill
0.563
0.576
0.463
0.601
Uniform preference
0.749
0.782
0.805
0.671
Type II error
Uniform skill
0.171
0.127
0.150
0.180
Uniform preference
0.979
0.990
0.981
0.977

Admission
1.064
0.429
0.427
0.060
0.181
4,663,601
3,199
0.720
(0.027)
0.287
(0.007)
2.365
(0.055)
0.125
(0.005)
0.014
(0.000)
1.890
(0.019)
0.196
0.794
0.669
0.898
10.724
9.078
12.480
1.361
1.269
1.453

0.636
0.695
0.190
0.976

Note: This table shows robustness of results under alternative implementations. “Baseline” presents our baseline results. “Balanced” presents results estimated only on the 44 stations we identify with quasi-random
assignment. “No controls” performs no risk-adjustment. “VA users” restricts to a sample of veterans with
above-median VA usage. “Admission” requires a type II error to occur in a patient with a high probability of
admission. Appendix A.7 provides rationale for each of these implementations and further discussion.

A.44

