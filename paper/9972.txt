NBER WORKING PAPER SERIES

HOW DO HOSPITALS RESPOND TO PRICE CHANGES?
Leemore S. Dafny
Working Paper 9972
http://www.nber.org/papers/w9972
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2003

I am grateful to Jonathan Gruber, James Poterba, David Cutler, and Glenn Ellison for excellent guidance. I
thank Josh Angrist, William Collins, Joseph Doyle, Mark Duggan, Julian Jamison, David Levine, Joseph
Newhouse, Scott Stern, Nancy Rose, and seminar participants at several universities and the NBER
Universities Research Conference for helpful comments. Support from the National Science Foundation, the
National Bureau of Economic Research, and the National Institute on Aging is gratefully acknowledged. The
MedPAR data are confidential and cannot be released. The views expressed herein are those of the authors
and are not necessarily those of the National Bureau of Economic Research.
©2003 by Leemore S. Dafny. All rights reserved. Short sections of text, not to exceed two paragraphs, may
be quoted without explicit permission provided that full credit, including © notice, is given to the source.

How Do Hospitals Respond to Price Changes
Leemore S. Dafny
NBER Working Paper No. 9972
September 2003
JEL No. H0, I0, L0
ABSTRACT
This paper investigates whether hospitals respond in profit-maximizing ways to changes in
diagnosis-specific prices, as determined by Medicare's Prospective Payment System and other public
and private insurers. Previous studies have been unable to isolate this response because changes in
reimbursement amounts (prices) are typically endogenous: they are adjusted to reflect changes in
hospital costs. I exploit an exogenous 1988 policy change that generated large price changes for 43
percent of all Medicare admissions. I find that hospitals responded to these price changes by
"upcoding" patients to diagnosis codes associated with large reimbursement increases, garnering
$330-$425 million in extra reimbursement annually. This response was particularly strong among
for-profit hospitals. With the important exception of elective diagnoses, I find little evidence that
hospitals increased the intensity of care in diagnoses subject to price increases, where intensity is
measured by total costs, length of stay, number of surgical procedures, and number of intensivecare-unit days. Neither did hospitals increase the volume of patients admitted to more remunerative
diagnoses, notwithstanding the strong a priori expectation that such a response should prevail in
fixed-price settings. Taken together, these findings suggest that, for the most part, hospitals do not
alter their treatment or admissions policies based on diagnosis-specific prices; however, they employ
sophisticated coding strategies in order to maximize total reimbursement. The results also suggest
that models of quality competition among hospitals may be inappropriate at the level of specific
diagnoses ("products").
Leemore S. Dafny
Department of Management and Strategy
Kellogg School of Management
Northwestern University
2001 Sheridan Road
Evanston, IL 60208
and NBER
l-dafny@kellogg.northwestern.edu

1

Introduction

The vast majority of U.S. healthcare is privately provided. Yet until the 1980s, the sector was largely
immune from standard market forces promoting efficiency in production. The canonical healthcare
market imperfections – informational asymmetries between providers and consumers, and an
insurance-induced wedge between marginal out-of-pocket costs and patient benefits – were
exacerbated by a cost-plus reimbursement system and primarily not-for-profit providers. So long as
providers could always earn non-negative profits, there was little supply-side incentive to cut costs, and
consumers’ incentives via co-payments and deductibles were weak. In 1984, the federal government
injected market discipline into the system by establishing fixed prices for Medicare hospitalizations.
Other public and private insurers soon followed suit, wresting price-setting control from providers and
imposing yardstick competition.
A large literature documents hospitals’ responses to the introduction of fixed prices, but few
studies have explored reactions to changes in these prices. Yet once the transition to a fixed-price
regime is completed, price levels constitute the sole lever in the system, and there remain several
unanswered empirical questions regarding their effect. In the face of a price increase for a particular
diagnosis or treatment, will hospitals find ways to attract more such patients? Will they compete more
vigorously for these patients by improving the quality of their care, thereby dissipating some of the
rents from the price increase? The answers to these questions are critical to ongoing policy decisions,
and can also provide valuable insights into hospital industry conduct and the effectiveness of fixedprice regulation.
This study focuses on inpatient care for Medicare beneficiaries, who account for 37 percent of
hospital discharges and 31 percent of total revenues.1 Since 1984, hospital reimbursement for
Medicare patients has been governed by the Prospective Payment System (PPS), which provides a
1

2002 Data Compendium, Centers for Medicare and Medicaid Services, and author’s tabulations from the 2000 Survey of
Hospitals (administered by the American Hospital Association). Figures are for 2000.

1

fixed payment for each Medicare patient in a given hospital and diagnosis-related group (DRG).
Standard models of hospital behavior, reviewed in section 2, predict that hospitals will respond to a
diagnosis-specific price increase by raising the intensity of care provided to patients in that diagnosis.
According to these models, hospitals behave much like multiproduct firms, where the products are
DRGs and the choice variables are not prices but rather intensity of care within each DRG. Both
patient volume and hospital costs are assumed to increase in the intensity of care provided. A price
increase for a given DRG raises the profitability of that DRG, creating an incentive to attract more
patients by increasing the intensity of care that is provided. Indeed, the few studies that investigate the
effect of DRG-level price changes on intensity levels all find a positive relationship, where intensity of
care is measured by length of stay, number of surgical procedures, and/or death rates (Cutler 1990,
1995; Gilman 2000). Thus, all evidence to date suggests that a “flypaper effect” operates in the
hospital industry: additional income is allocated to the clinical area in which it is earned, rather than
spread across a broad range of activities.
All of the aforementioned studies utilize data from a transition to a prospective payment
system, either PPS or one of the many systems implemented by state Medicaid programs. These
studies therefore face the formidable challenge of separating two simultaneous changes in incentives:
the elimination of marginal reimbursement, and changes in the average level of payments for each
DRG. By investigating responses to average payment levels (i.e., prices) in the post-implementation
period, I circumvent both this challenge and the concern that transitory responses are driving previous
results. Estimating responses to price changes in post-implementation eras is difficult, however,
because price changes are typically endogenous: they are adjusted to reflect changes in hospital costs.
Thus, positive associations between changes in price and changes in spending or intensity likely reflect
bilateral causality, and do not constitute a priori evidence that hospitals alter treatment patterns in
response to price changes.

2

To obtain unbiased estimates of hospital responses to price changes, this study exploits an
exogenous 1988 policy change that generated large price changes for 43 percent of Medicare
admissions. The policy change was simply the elimination of “age over 69” and “age under 70” in the
descriptions for the diagnosis-related groups (DRGs) to which patients may be assigned. Qualifiers
that formerly read “with complications or age over 69” and “without complications and age under 69”
now read “with complications” or “without complications.” This seemingly innocuous change, which
is described in greater detail in Section 3, actually led to large increases in reimbursement for patients
assigned to DRG codes with these qualifiers (“affected DRGs”), as compared to patients in other codes
(“unaffected DRGs”).
I consider both nominal and real responses to these price changes, where “nominal” refers to
hospital coding practices and “real” refers to admissions volumes and intensity of care actually
provided. Because hospitals are responsible for coding patients to the appropriate DRGs, raising prices
for certain DRGs may simply entice hospitals to “upcode,” or switch patients from lower-paying DRGs
into higher-paying DRGs. While upcoding does not affect real elements of patient care, it inflates
hospital reimbursements. This was the primary response of hospitals to the 1988 policy change.
Hospitals also demonstrated a keen awareness of risk-reward tradeoffs in their upcoding practices:
although the policy shock created a blanket incentive to increase upcoding in dozens of diagnoses,
hospitals upcoded more in those diagnoses where the incentive to do so was larger. The upcoding
response was also strongest among for-profit hospitals, a finding that is consistent with prior research.
Using the unaffected DRGs as a control group, I find that hospitals did not increase the
intensity or quality of care provided to patients in affected DRGs, where intensity is measured by total
costs, length of stay, number of surgical procedures, and number of intensive-care-unit (ICU) days, and
quality by the in-hospital death rate. DRGs in which the plurality of admissions are elective were the
sole exception: hospitals did increase their spending in affected DRGs relative to unaffected DRGs in
this category, although this increased spending did not translate into significant increases in the other

3

dimensions of intensity that I measure. Across the board, hospitals did not increase the volume of
patients admitted to more remunerative diagnoses, a finding that is theoretically consistent with the
general intensity non-response, but perhaps surprising given theoretical predictions of firm behavior in
fixed-price settings. I do find evidence that hospitals spent the extra funds they earned on patient care,
but these funds were spread across all admissions. Correspondingly, overall hospital volume growth
was also stronger for hospitals with larger price gains (and therefore intensity increases) arising from
the policy change.
Taken together, these findings indicate that hospitals generally do not alter their treatment or
admissions policies based on diagnosis-specific prices; however, they employ sophisticated coding
strategies in order to maximize total reimbursement. The results also suggest that healthcare insurers
cannot effect an increase in the quality of care provided to patients with a particular diagnosis simply
by increasing their reimbursement rates for that diagnosis. Another important implication is that
models of quality competition among hospitals may be inappropriate at the level of specific diagnoses.
Finally, this research illustrates the difficulties inherent in regulating prices in an industry where the
products are hard to define.
The remainder of the paper is organized into 5 sections. Section 2 describes PPS and prior
related research, and introduces a hospital objective function that provides a theoretical framework for
the empirical sections that follow. Section 3 gives a detailed explanation of the 1988 policy change.
The data are presented in Section 4, followed by an evaluation of the aggregate impact of the policy
change on price levels in Section 5. Section 6 quantifies the share of the price change attributable to
mistakes by price-setting authorities (the exogenous or mechanical component), as compared to true
changes in patient mix (the severity component), and hospital upcoding (the upcoding component).
Section 7 explores the intensity and volume responses to the exogenous component of the price change,
and Section 8 concludes.

4

2

Background

2.1 A PPS Primer
The Prospective Payment System (PPS) for hospitalizations of Medicare beneficiaries was
implemented in October 1984 by the Health Care Financing Administration (HCFA), now known as
the Centers for Medicare and Medicaid Services (CMS). The defining element of the system is a
reimbursement amount that is fixed regardless of a hospital’s actual expenditures on a patient. This
payment does vary, however, by the patient’s medical diagnosis. Diagnoses are grouped into
approximately 500 Diagnosis-Related Groups (DRGs). Each DRG is assigned a weight (called a
“DRG weight”) that reflects the relative resource intensity of admissions within that group.
Reimbursement to hospital h for an admission in DRG d is given by
Phd = Ph · (1 + IMEh) · (1+ DSHh) · DRG weightd
where Ph is a hospital-specific amount (inflated annually by a Congressionally-approved “update
factor”), IME represents an adjustment for indirect medical education (teaching), and DSH adjusts
payment levels to compensate hospitals with a disproportionate share of indigent patients.2 Most of the
variation in Phd is due to the DRG weights, which range between .09 (DRG 448 for allergic reactions)
to 22.8 (DRG 480 for liver transplants).3 CMS uses hospital charge data (deflated by hospital
cost:charge ratios) to recalibrate the weights annually, raising weights for DRGs that experience
relative increases in average charges, and reducing weights for DRGs with relative decreases in
average charges. The average DRG weight per hospital admission has risen substantially over time,
from 1.13 in 1984 to 1.36 in 1996.4 This phenomenon has been termed “DRG creep,” as patients are

2

This simplified formula appears in Cutler (1995).
The range for DRG weights is given for 1985-1996.
4
Steinwald and Dummit (1989), author’s calculations. The original 1984 weights were constructed so that the average DRG
weight for hospitals, called the case-mix index, would equal 1.
3

5

increasingly coded into DRGs with higher weights. A one-percent increase in the average case weight
is associated with an additional $930 million in annual Medicare payments to hospitals.5
Although the implementation of PPS eliminated marginal reimbursement for services rendered
(within a given DRG, hospitals are not compensated more when they spend more on a patient),
economists have noted that average payment incentives remain. If Phd is low relative to actual costs in
DRG d, hospitals have an incentive to reduce the intensity of care and the number of admissions in that
DRG. Section 2.2 illustrates this incentive more formally.
Due to the regular recalibrations described above, it is difficult to identify hospital responses to
changes in average payment incentives (hereafter DRG prices or weights). When costs increase, DRG
prices increase. Thus, the coefficient on DRG price in a regression of costs (or some other measure of
intensity of care) on DRG price would suffer from a strong upward bias. To obtain an unbiased
estimate of this coefficient, exogenous variation in payment levels is required. This variation is
provided by the natural experiment described in section 3.

2.2 Hospital Objective Functions
To illustrate how changes in DRG prices might affect hospital behavior, it is helpful to introduce a
simple model for the hospital objective function. I begin with the traditional assumptions that hospitals
attach non-negative weights to both patient care (often called “intensity” or quality) and profits, and
that the objective function is separable in these arguments:
max G h = α h f (I h ) + (1 - α h )π h

where 0 < α < 1, h is a hospital index, I denotes intensity, and π denotes profits.
The PPS system effectively defines D “product lines” for every hospital, where D is the number
of DRGs. Each hospital selects an intensity level Ihd for each DRG d, attracting Nhd(Ihd ,I~hd) patients,
where ~h denotes hospital h’s competitors. Patient demand is increasing in a hospital’s own intensity
5

“Program Information,” Centers for Medicare and Medicaid Services, June 2002.

6

level (at a decreasing rate), and decreasing in that of its competitors. Because higher intensity levels
attract sicker patients, the severity of patients served, Shd(Ihd), is also increasing in a hospital’s intensity
level. For each admission, the hospital earns Phd – Chd(Ihd ,Shd(Ihd)), where Phd is as defined above, Chd
is the average cost per patient
assigned to DRG d, and ∂C hd and ∂C hd are greater than zero.6 Thus, the hospital’s problem becomes
∂I hd

∂S hd

D

max G h = α h f ( I h1 , I h 2 ...I hD ) + (1 − α h ) ∑ ([Phd − C hd ( I hd , Shd )]N hd (I hd , I ~ hd ) ) ,
d =1

and the first-order condition for Ihd, taking competitors’ behavior as given, is


 ∂C
∂N
∂C
∂S 
∂G h
∂f
= αh
+ (1 − α h ) (Phd − C hd ) hd − N hd  hd + hd • hd  = 0
∂I hd
∂I hd
∂I hd

 ∂I hd ∂Shd ∂I hd 
For every DRG, the hospital equates the marginal benefit of intensity with its marginal cost. This
expression implicitly defines the optimal intensity choice, I *hd . To illustrate that an increase in Phd
raises optimal intensity, I set

dI*
∂G h *
(I hd , Phd ) = 0 , differentiate with respect to Phd, and solve for hd .
∂I hd
dPhd

Under the assumptions that G h is twice differentiable and concave in I hd ∀ d, and that I hd and

I ~ hd are strategic complements ∀ d,

(

)

dI *hd − (1 − α h )(∂N hd ∂I hd ) − ∂ 2 G h ∂I hd ∂I ~ hd • dI ~ hd dP hd
=
> 0. 7
2
2
dPhd
∂ G h ∂I hd

This result suggests that price increases should be associated with a “flypaper effect” of the sort
widely-documented in the public sector: additional funds are not treated as general income but are
spent where they are raised. My primary empirical objective is to test this prediction explicitly by

6

This model is based on Dranove (1987), Hodgkin and McGuire (1994), Ellis and McGuire (1996), and Gilman (2000).
I adopt the definition of Bulow, Geanakoplos, and Klemperer (1985) by using ∂2Gh ∂Ihd∂I~hd>0 to denote strategic
complements.
7

7

investigating whether hospital costs and other measures of intensity increased more for DRGs that were
more highly reimbursed after the policy change. This analysis is presented in Section 7.
Section 7 tests another prediction that follows from the flypaper effect: the volume of
admissions in DRGs subject to price increases should grow.8 If intensity levels rise as a result of price
increases, by assumption volume should increase as well. This is the classical response expected in
fixed-price industries: when price increases, so long as it exceeds marginal cost, firms will want to
produce more.
There are several reasons these results may not obtain. First, ∂Nhd/∂Ihd may be very small,
reducing the effect of a price increase on intensity levels. Patients may respond to a hospital’s overall
choice of intensity (“Ih”), but not to Ihd, which is more difficult to ascertain.9 Second, hospitals may be
unable to select different intensity levels for each DRG (i.e., intensity is “lumpy” across DRGs). New
technologies or practice patterns, once put in place, may be difficult to apply to only a select group of
patients. Third, if intensity choices are not initially in equilibrium, a hospital may allocate new funds
earned in affected DRGs to overdue investments in unaffected DRGs. Finally, hospitals may maximize
objectives that are not captured in the functional form above, such as the total volume of patients.
The objective function Gh is quite general, allowing for heterogeneity in hospitals’ responses to
the same payment incentives. Any characteristic that affects the parameter α h will affect the intensity
response to a price increase. For example, for-profit hospitals should place a higher weight on profits
(lower α h ), as should hospitals under financial duress. The “mission” of a hospital, reflected by such
characteristics as teaching status, may also affect the tradeoff between intensity and profits.
Alternatively, different hospitals with the same α h may be differentially-equipped to respond to
reimbursement incentives. Small hospitals in particular lack the resources needed to reoptimize

8

Strictly speaking, this is true so long as the price-induced changes in a hospital’s own intensity have a greater impact on its
volume than the price-induced changes in the intensity of its competitor(s).
9
Note that patients themselves need not have detailed knowledge of intensity levels; their primary care physicians and
specialists may refer them to hospitals based on their assessments of intensity.

8

quickly in the face of price changes. Finally, there are important regional differences in hospital
behavior, although there are few theoretical explanations for this phenomenon apart from “cultural
norms.”
Differences across hospitals are one possible source of variation in intensity responses;
differences across DRGs are another. For example, patient demand for planned or elective admissions
may be more sensitive to changes in intensity than demand for urgent care. When a hospitalization is
anticipated, a patient can “shop around,” soliciting advice and information directly from the hospital, as
well as from physicians and friends. The elasticity of demand with respect to quality is therefore larger
for such admissions, raising hospitals’ incentives to increase quality in the face of price increases.
Thus, the same price increase may elicit different intensity responses across DRGs. I explore
differences in intensity and volume responses across hospitals and admission types in sections 7.2.1
and 7.2.2, respectively.

2.2.1 Incorporating Upcoding
The general model outlined above can be easily expanded to include upcoding effects. Using Uhd to
denote an “upcoding index,” the number of patients Nhd can be redefined as an increasing function of
Uhd and a decreasing function of Uh~d, the degree of upcoding in other DRGs. Holding the number of
patients constant, if more patients are upcoded into DRG d, fewer patients are assigned to other DRGs.
Upcoding a patient to DRG d also reduces average severity in DRG d (else it would not be upcoding);
the effect on average severity in the original DRG is ambiguous. To summarize,

N hd = N hd (I hd ,I ~ hd ,U hd ,U h ~d ) ,

Shd = Shd (I hd ,U hd ,U h ~d ) ,

∂N hd
∂N
∂N
∂N hd
> 0, hd < 0, hd > 0,
<0
∂I hd
∂I ~ hd
∂U hd
∂U h ~ d

∂Shd
∂S
∂S
> 0, hd < 0, hd < > 0 .
∂I hd
∂U hd
∂U h ~ d

9

Adding a probability of detection µ h that is increasing in the level of upcoding, a penalty Th if the
hospital is caught upcoding, and a total cost of upcoding R, the objective function becomes

D
 ∑([Phd −C hd (I hd ,Shd (I hd ,U h1 ,U h 2 ..U hD ))]N hd (I hd ,I ~ hd ,U h1 ,U h 2 ..U hD ))

d =1
G h = α h f (I h1 ,I h 2 ..I hD )+(1−α h ) 


 −µ ( U h1 ,U h 2 ...U hD )Th −R ( U h1 ,U h 2 ...U hD )


with the following first-order condition for Uhd:

D
∂N hj
∂C hj ∂S hj 
∂G h
∂µ
∂R 
 = 0.
•
= (1 − α h )  ∑  (Phj − C hj )
− N hj (
) −
Th −
∂U hd
∂U hd
∂S hj ∂U hd  ∂U hd
∂U hd 
 j =1

Hospitals trade off the added revenue (less any change in treatment costs) from shifting patients into
higher-weighted DRGs against the increased risk of detection plus the cost of upcoding. In its purest
form, upcoding implies no effect whatsoever on the amount of care received by patients, so treatment
costs are unchanged. Holding the penalties and costs associated with upcoding constant, a price
increase for a given DRG increases the incentive to upcode patients into that DRG.10
The coding of patient conditions is performed by administrative staff, who use hospital charts
and the ICD-9 diagnosis codes provided by physicians to map patient conditions into DRGs (Silverman
and Skinner 2000). Upcoding costs therefore depend upon the availability of multiple DRG codes for
similar diagnoses. It is theoretically possible to assign a patient with bronchitis to the heart transplant
DRG, but such overt upcoding requires altering or misinterpreting medical records substantially and
increases the risk of detection later on.11 The policy change I study involves DRGs that are particularly
susceptible to upcoding because these are DRGs in which the coding of patient complications results in
a substantially higher price. One former manager from the largest for-profit hospital chain,
10

The conditions for this prediction to hold are analogous to those in section 2.2: Gh must be twice differentiable and concave
in Uhd, and the cross-partial ∂2Gh ∂Uhd∂Ihd≥0. This cross-partial can reasonably be expected to equal zero, as the marginal
benefit of intensity should not vary with upcoding.
11
Regulatory agencies known as “Peer Review Organizations” regularly audit DRG assignments. CMS works with the Office
of the Inspector General (OIG), the FBI, and the US Attorney’s Office to levy fines, recover funds, and prosecute providers
who defraud the Medicare program. There are qui tam provisions to reward and protect whistle-blowers.

10

Columbia/HCA (now HCA), reported that hospital managers were rewarded for upcoding patients with
these diagnoses into the more-remunerative “with complications” codes (Lagnado 1997). Section 6
presents results on upcoding following the 1988 policy change.
As with intensity levels, there are many reasons that upcoding behavior may differ across
hospitals and DRGs. Hospitals with a lower α h should upcode more, while hospitals with a greater
penalty Th (real or perceived, monetary or otherwise) or a higher probability of detection µ h should
upcode less. There are a number of theories of the effect of hospital ownership on upcoding, but few
consensus predictions (see Silverman and Skinner 2000 for a comprehensive discussion). Hospitals
experiencing financial distress should be more willing to risk detection, all things equal, while larger
hospitals may be “savvier” in training their coding personnel. Practices of competitors may also affect
upcoding indirectly through pressure on hospital profits, or directly via the dissemination of upcoding
practices.12
Finally, upcoding may also vary across DRGs. Diagnoses based on subjective interpretations
of patient conditions are more prone to upcoding, as are diagnoses for which minor variations (e.g.,
presence of a complication) are associated with large reimbursement differences. The upcoding
analysis in section 6 focuses on diagnoses in this latter group. Within this subset of conditions, I also
investigate the relationship between the extent of upcoding in a particular diagnosis and the financial
incentive to upcode.

2.3 Previous Research
2.3.1 Average Reimbursement Effects
Virtually all of the papers that evaluate the impacts of PPS do not distinguish between the effects due to
changes in marginal reimbursement (during the phase-in of the system) and those due to changes in
12

Several recent studies document this indirect channel, e.g., Duggan (2002), which finds that not-for-profit hospitals respond
more strongly to financial incentives to treat indigent patients in markets with greater for-profit penetration.

11

average reimbursement levels (Pdh).13 The first papers to distinguish these effects at the diagnosis level
are Cutler (1990) and Cutler (1995).14 Cutler (1990) studies the transition to PPS in Massachusetts,
finding that length of stay and number of procedures per patient declined the most in DRGs subject to
the largest price reductions. Despite finding an elasticity of intensity with respect to price of .2, Cutler
does not find a corresponding volume response.15 Cutler (1995) studies the impact of PPS on adverse
medical outcomes, again finding an intensity response: reductions in average price levels are
associated with a compression of mortality rates into the immediate post-discharge period, although
there is no change in mortality at one year post-discharge. Both papers assume that eliminating the
marginal reimbursement incentive affects all DRGs equally. However, intensity reductions may be
easier to make in certain DRGs and/or hospitals, and to the extent that price reductions were more
prevalent in such DRGs and/or hospitals (the very goal of the price-setting process), the intensity
responses to price changes will be overstated. More generally, the elasticity estimate will be biased by
any omitted factor influencing both price and intensity changes during the transition to PPS.16
The two additional studies addressing DRG-specific intensity responses to price changes
employ different identification strategies but reach the same conclusion. Gilman (2000) investigates
the impact of a 1994 reform to Medicaid DRGs for HIV diagnoses in New York. He finds that length
of stay increased in procedure-based DRGs, which were subject to price increases, and decreased in
13

Hodgkin and McGuire (1994) provide an excellent overview of empirical research on this subject.
Studies of hospital-level responses to changes in average reimbursement amounts include Hadley, Zuckerman, and Feder
(1989) and Staiger and Gaumer (1992). These works find positive intensity responses as measured by length of stay and
patient survival, respectively. Cutler (1998) studies responses to average payment reductions implemented through the annual
update factor. He finds cost-shifting to private payors in the early PPS era (1985-1990), and cost-cutting through capacity and
nursing staff reductions in the later PPS era (1990-1995).
15
Such a result could be consistent with a model in which volume is not a function of intensity, and hospitals simply
maximize intensity within each DRG subject to a DRG-specific breakeven constraint.
16
Cutler’s methodology for calculating the change in average payment incentives following the implementation of PPS is
likely to lead to upward-biased elasticity estimates. Cutler defines the change in average price as the difference between the
1988 PPS price and the price that Medicare would have paid in 1988 were cost-plus reimbursement still in effect. To estimate
this latter figure, he inflates 1984 costs for each DRG by the overall cost-growth rate for 55-64 year-olds. However, DRGs
with disproportionately stronger cost growth between 1984 and 1988 received weight increases, yielding higher 1988 PPS
prices and generating the concern that the positive relationship between price changes and intensity levels may be spurious.
The possibility that these estimated price changes are not exogenous is reinforced by the use of hospital-specific prices in the
specifications. The average price changes are therefore related to hospitals’ pre-PPS DRG-specific costs; hospitals with high
costs faced price reductions when transitioning to national payment standards. Such hospitals may have had “more fat to
trim” in terms of intensity provision.
14

12

non-procedure-based DRGs, which were subject to price decreases. Assuming the controls for patient
severity adequately capture the severity changes in the patient population for both admission types,
these results also suggest that hospitals adjust DRG-specific intensity in response to price changes.
Newhouse (1989) finds some evidence that private hospitals successfully shifted patients in
unprofitable DRGs to public hospitals following the implementation of PPS; the mechanism for this
shift is not specified, but the finding is consistent with real responses to incentives at the DRG level.17
As with the Cutler studies, these works investigate simultaneous changes in marginal and average
reimbursement incentives. The policy change I assess affects only average reimbursement levels,
eliminating the need to disentangle the responses to changes in marginal incentives. In addition,
because the policy change affected a large proportion of DRG codes (40 percent), the analysis produces
representative estimates of DRG-specific intensity responses.

2.3.2 Upcoding
Because the single largest source of increased hospital spending by Medicare is the rapid rise in the
average case weight, the subject of upcoding has generated a substantial literature. Coulam and
Gaumer (1991) review this literature through 1990, concluding that there is evidence of upcoding
during the first few years of PPS, but the amount of the case-mix increase attributable to this practice is
unknown. There are two general empirical approaches to estimating the magnitude of upcoding:
detailed chart review, and comparisons of case-mix trends over time and across hospitals.
Carter, Newhouse, and Relles (1990) use the ‘gold standard’ in chart review to estimate the
role of upcoding in the case-mix increase between 1986 and 1987: they send a nationally representative
sample of discharge records from 1986 and 1987 to an expert coding group (called the “SuperPRO”)
that regularly reviews samples of discharges to enforce coding accuracy. They find that one-third of

17

Newhouse specifically considers the possibility that private hospitals transferred unprofitable patients to public hospitals
after admission, but does not find any evidence to support this mechanism for case redistribution.

13

the case-mix increase was due to upcoding, although the standard error of this estimate is large. More
recently, Psaty et al (1999) use detailed chart review to estimate that upcoding is responsible for over
one-third of admissions assigned to the heart failure DRG (DRG 127).
Most of the non-medical analyses of case-mix increases (e.g., Steinwald and Dummit 1989) are
descriptive, focusing on which types of hospitals exhibit faster case-mix growth (large, urban, and
teaching hospitals), and when these increases occur (there is a big jump in the first year a hospital is
paid under PPS). Because these studies use data from the transition period, the results are again
difficult to interpret; patient severity changed dramatically due to changes in patient composition
following the implementation of PPS.
A recent study by Silverman and Skinner (2000) presents strong evidence of post transition-era
upcoding for pneumonia and respiratory infections between 1989 and 1996. Focusing on the share of
patients with these diagnoses that are assigned to the most expensive DRG possible, Silverman and
Skinner document large increases in upcoding, despite a downward trend in mortality rates.
Interestingly, the authors find that for-profit hospitals upcode the most, and that not-for-profit hospitals
are more likely to engage in upcoding when area market share of for-profit hospitals is higher,
independently of financial distress and other control variables. This finding is consistent with a
contagion model like that described in Cutler and Horwitz (1999), or with the “cultural norms”
hypothesis. In addition, Silverman and Skinner find that hospitals under financial distress upcode less
than financially sound institutions.
My upcoding analysis takes a similar approach, but the policy change I analyze offers two
important advantages. First, I study an abrupt change in upcoding incentives that should be met with a
similarly abrupt change in upcoding if hospitals are responsive to these incentives. Second, because
the policy change created upcoding incentives that vary by diagnosis, I am able to investigate not only
whether hospitals respond to upcoding incentives in general, but also whether they respond to upcoding
incentives on the margin, upcoding more when the payoff is greater.

14

3

A Price Shock: The Elimination of the Age Criterion

Although there were 473 individual DRG codes in 1987, 40 percent of these codes belonged to a “pair”
of codes that shared the same main diagnosis. Within each pair, the codes were distinguished by age
restrictions and presence of complications (CC). For example, the description for DRG 138 was
“cardiac arrhythmia and conduction disorders age>69 and/or CC,” while that for DRG 139 was
“cardiac arrhythmia and conduction disorders without CC.” Accordingly, the DRG weight for the top
code in each pair exceeded that for the bottom code. There were 95 such pairs of codes, and 283
“single” codes.
In 1987, separate analyses by HCFA and the Prospective Payment Assessment Commission
(ProPAC) revealed that “in all but a few cases, grouping patients who are over 69 with the CC patients
is inappropriate” (52 Federal Register 18877).18 The ProPAC analysis found that hospital charges for
uncomplicated patients over 69 were only 4 percent higher than for uncomplicated patients under 70,
while average charges for patients with a CC were 30 percent higher than for patients without a CC. In
order to minimize the variation in resource intensity within DRGs and to reimburse hospitals more
accurately for the affected diagnoses, HCFA eliminated the age over 69/under 70 criterion beginning in
1988. The agency recalibrated the weights for all DRGs to reflect the new classification system. This
recalibration resulted in large increases in the weights for top codes within DRG pairs, and moderate
declines for bottom codes.
Table 1 gives the three most commonly-coded pairs and their DRG weights before and after
the policy change.19 These examples are fairly representative of the change overall. Using 1987
admissions from a 20 percent sample of Medicare discharge data as weights, the weighted average
increase in the top code for all DRG pairs was 11.3 percent, while the weighted average decrease in the
18

ProPAC, now incorporated into MedPAC (Medicare Payment Advisory Commission), was an independent federal agency
that reported to Congress on all PPS matters.
19
The large volume increase for the bottom code in each pair is due to the new requirement that uncomplicated patients over
69 be switched from the top to the bottom code.

15

bottom code was 6.2 percent. In the final notice of the policy change, HCFA clearly states that the goal
of the recalibration was to ensure no overall change in reimbursement to hospitals; that is, the average
national DRG weight should have been constant whether the 1987 or the 1988 classification system
(called the GROUPER program) was employed on a given set of discharge records.20 It is worth
emphasizing, however, that while annual recalibrations are intended to be revenue-neutral overall,
there is no requirement that they be revenue-neutral for any subset of DRGs.
Indeed, as the analysis in Section 5 reveals, this policy change resulted in a large relative price
increase (7 percent) for discharges coded in DRG pairs, and a moderate absolute price increase (1-2
percent). There are three sources of this price increase: a mechanical component, an upcoding
component, and a severity component. The mechanical component is the effect of the recalibration on
prices, holding the incidence of reported complications constant – essentially, it captures mistakes
made by HCFA in its recalibration. The upcoding component captures the opportunistic coding of
complications, while the severity component is associated with an increase in the true incidence of
complications. In 1989, HCFA published its own (unfortunately flawed) estimate of the contribution of
recalibration mistakes to the large increase in average DRG weight between 1986 and 1988 (54 Federal
Register 169). HCFA concluded that .93 percentage points could be attributed to faulty recalibration of
DRG weights for 1988, and an additional .29 percentage points to similar errors in 1987. These
estimates motivated an across-the-board reduction of 1.22 percent in all DRG weights beginning in
1990. Because this reduction applied uniformly to all DRGs, the large relative effects on the DRG
pairs were unabated.
The 1988 policy change provides an excellent opportunity to study hospital responses to
changes in DRG-specific prices. After describing the data, I analyze the effects of this price shock in
three parts. First, I estimate the magnitude of the shock to prices for affected DRGs. Second, I
disaggregate this price increase into its mechanical, upcoding and severity components. Third, I
20

There were only a few minor changes to the GROUPER program between 1987 and 1988 that were not associated with the
elimination of the age criterion.

16

investigate the elasticity of DRG-specific intensity and volume with respect to price, using the
mechanical component as an instrument for price.

4

Data

My primary data sources are the 20 percent Medicare Provider Analysis and Review (MedPAR) files
(FY85-FY91), the annual tables of DRG weights published in the Federal Register (FY85-FY91), the
Medicare Cost Reports (FY85-FY91), and the Annual Survey of Hospitals by the American Hospital
Association (1987). The MedPAR files contain data on all hospitalizations of Medicare enrollees,
including select patient demographics, DRG code, measures of intensity of care (e.g., length of stay and
number of surgeries), and hospital identification number. The data span the three years before and after
the policy change.
The MedPAR discharge records are matched to DRG weights from the Federal Register and
hospital characteristics from the Annual Survey of Hospitals and the Medicare Cost Reports for 1987,
the year preceding the policy change.21 Due to the poor quality of hospital financial data, the debt:asset
ratio from the Medicare Cost Reports is among the best measures of financial distress. I also construct
two additional financial distress measures, Medicare “bite” (the fraction of a hospital’s discharges
reimbursed by Medicare) and Medicaid “bite” (similarly defined). Appendix Table 1 presents
descriptive statistics for these measures, together with other hospital characteristics that may be
associated with responses to the shock (ownership status, region, teaching status, number of beds, and
service offerings). Because price varies at the hospital and DRG level, the individual discharge records
are aggregated to form DRG-year or hospital-year cells. Descriptive statistics for these cells are
reported in Table 2.

21
The Cost Reports also contain an indicator for whether a hospital is paid under the PPS system (certain hospitals are
exempted). I omit exempt hospitals from my sample.

17

5

Assessing the Magnitude of the Price Shock

The elimination of the age criterion resulted in large price changes for individual DRGs, as described in
section 3. However, it would not be informative to investigate whether intensity levels rose (fell) for
patients admitted to the top (bottom) code of DRG pairs, because the composition of patients admitted
to each code changed as a result of the policy reform. Top codes, which were formerly assigned to all
older patients as well as to young patients with CC, are now intended to be used exclusively for
patients with CC, young or old. A finding that average intensity of care increased in top codes would
not yield information on whether hospitals increased intensity of care for patients with CC, the only
patients for whom a price increase was enacted. Furthermore, policy-induced upcoding from bottom to
top codes exacerbates the problem of compositional changes within each DRG code.22 In order to keep
the reference population constant before and after the policy reform, I combine data from the top and
bottom codes, effectively creating a single DRG for each pair. It is therefore critical to illustrate that
the average price paid for patients in these newly-created paired DRGs did indeed increase following
the 1988 elimination of the age criterion.
To assess the magnitude of this price increase, I employ a differences-in-differences technique,
comparing the time-series changes in price for the paired DRGs (henceforth the “affected DRGs”) with
the changes in price for the single DRGs (the “unaffected” DRGs). While prices for unaffected DRGs
are given annually by HCFA, prices for affected DRGs must be calculated by taking a weighted
average of the prices for the top and bottom codes in each pair. For example,

price DRG138 / 139,1988 =

price DRG 138, 1988 * N DRG 138, 1988 + price DRG 139, 1988 * N DRG 139, 1988
N DRG138,1988 + N DRG 139, 1988
=

.8535 * 35,233 + .5912 *16,829
= .7687
35,233 + 16,289

22

If the sample were restricted to patients under 70, the first of these compositional problems would not apply. However, the
second would bias any intensity response estimated using individual DRGs as the unit of observation.

18

where N denotes the number of admissions in the MedPAR sample. I use this formula to calculate
prices for the affected DRGs in every year. To evaluate the aggregate impact of the policy change, I
assemble a dataset of annual prices for the affected and unaffected DRGs between 1985 and 1991, and
estimate the following specification:

(1) ln(price)dt = α + ςDRG d + δyeart + γaffected DRG d • post t + ε dt
where d indexes DRGs and t indexes years, affected DRG is a dummy variable that equals one for the
treatment group (DRGs affected by the policy change), post is an indicator for the years following the
policy change (1988-91), and the dimensions of the coefficient vectors are ς (1 x 387), δ (1 x 6), and
γ (1 x 1).23 Note that the affected DRG main effect is absorbed by the inclusion of the DRG fixed

effects. The coefficient of interest, γ , captures the average price change for paired DRGs relative to
single DRGs during the post period. Each observation is weighted by the number of discharges for that
DRG-year cell.
The results from specification (1) are displayed in column 1 of Table 3. Column 2 adds a time
trend for affected DRGs, and column 3 includes individual DRG trends. The γ̂ reveal a robust and
statistically significant price increase of 7 percent for affected DRGs in the post-shock period. To
illustrate the time path of this change, I replace affected•post in specification (1) with individual
affected•year dummies. The coefficients on these dummies, graphed in Figure 1, demonstrate that
prices for affected DRGs did not display a different trend from prices for unaffected DRGs in the years
prior to the shock. This finding supports the contention that the price change was in fact exogenous,
and cannot be attributed to different pre-existing trends in costs for the two groups. The relative price
increase of 7 percent and the absolute price increase of 1-2 percent (obtained by summing the year and
affected•post coefficients) are considerable because they represent pure profits in an industry in which
total profit margins are on the order of 1-2 percent.
23

Of the 95 DRG pairs and 300 single DRGs in place by 1991, 2 pairs are dropped because the age criterion was eliminated
one year early for these pairs, and 5 single DRGs are dropped because there were no admissions coded in these DRGs in the
MedPAR sample.

19

6

Decomposing the Price Shock

6.1 The Mechanical Component
To estimate the mechanical component of the price increase, as described in section 3, I replace price
for DRG pairs in 1988-1991 with a Laspeyres price index, calculated using the 1987 volumes of young
patients in each code as the weights, i.e.
Laspeyres price DRG138 / 139, t =

price DRG 138, t • N(young) DRG 138, 1987 + price DRG 139, t • N(young) DRG 139, 1987
N(young) DRG138,1987 + N(young) DRG 139, 1987

This fixed-weight index approximates the average price hospitals would have earned in each postreform year had the fraction of patients with CC remained constant at the 1987 fraction for young
patients. Because the fraction of old patients with CC cannot be ascertained in 1987, the fraction for
young patients must proxy for this measure.24
Estimating specification (1) with this dependent variable produces a coefficient of .046 (.011),
implying that .046/.071=65 percent of the aggregate relative price increase is associated with price
recalibrations. In Section 7, I use this mechanical component, ∆ln(Laspeyres price)dt =ln(Laspeyres
price)dt – ln(price)d,1987, as an instrument for price. Because this instrument includes all of the effects of
annual recalibrations, the appendix details the methods used to eliminate the component related to
lagged cost growth, leaving only the price increase associated with the policy change. Estimates of

(2) ln(price)dt = α + ςDRG d + δyeart + γaffected DRG d • post t +
κ 1affected DRG d • post t • ∆ ln(Laspeyres price) dt + ε dt
are given in Table 3, column 4. Columns 5 and 6 present results with an affected DRG trend and
individual DRG trends, respectively. Rather than pool the affected DRGs into one treatment group, as
in specification (1), specification (2) exploits the fact that the policy change imposed different
mechanical price increases for each affected DRG. The positive and significant estimates of κ1 indicate

24

During the post-policy period, the correlation between fraction(old)dt and fraction(young)dt is .94.

20

.

that this refined policy variable captures the differences in the treatment across the affected DRGs.
This variable will permit more precise estimates of the elasticity of intensity with respect to price.

6.2 The Upcoding Component
Although DRG creep was known to be a pervasive problem by 1987, HCFA’s policy change
nevertheless increased the reward for upcoding. The increase in prices for the top codes in affected
DRGs, together with the decrease in prices for the bottom codes, provided a strong incentive to
continue using the top code for all older patients (not just those with CC), and to use it more frequently
for younger patients. Because all older patients were assigned to the top codes during the pre-shock
years, upcoding older patients is the easier of the two options; a hospital assigning a large proportion of
older patients to the top codes following the policy change could argue that its older patients had
always been relatively complicated. After all, it was not necessary to code complications for older
patients during the pre-shock period, so a comparison of pre/post behavior would not be conclusive.
Upcoding among the young requires shifting patients into the top codes, and is therefore easier to
detect. For this reason, my identification strategy provides upper and lower bounds for upcoding
among the young, but only lower bounds for upcoding among the old.

6.2.1 Aggregate Upcoding Analysis
The dependent variable for this analysis is fractiondt, the share of admissions to pair d in year t that is
assigned to the top code in that pair. Because this variable can only be defined for DRG pairs, single
DRGs cannot serve as a control group. For young patients, time-series identification is a possibility; a
discrete jump in the fraction of patients coded with complications after 1988 suggests an upcoding
response to the classification change. However, confounding factors such as an increasing trend in the
true severity of patients’ conditions may also generate increases in fractiondt. For old patients, it is

21

impossible to use the time-series decline in fractiondt to estimate the upcoding response because the
magnitude of the decline that would have occurred in the absence of upcoding cannot be determined. I
therefore introduce a new independent variable,
spreaddt = DRG weight in top codedt – DRG weight in bottom codedt ,
e.g.

spreadDRG 138/139, 1988

= weight DRG 138, 1988 – weight DRG

139, 1988

= .8535 - .5912 = .2623.
spreaddt is simply a measure of the upcoding incentive in pair d at time t. Between 1987 and 1988,
mean spread increased by .20, approximately $875.25 The standard deviation of this increase was .16,
however, indicating substantial variation in spread changes across DRGs. In the regression

(3) fraction dt = α + ςDRG d + δyeart + ψ∆spread d,88-87 • post t + ε dt
δ captures the average impact of the policy reform on all DRGs, while ψ captures the marginal effect
ˆ > 0 signifies that hospitals upcoded more in DRGs where the
of differential upcoding incentives. ψ
incentive to do so increased more. The estimation results for equation (3) are reported separately by
age group in Table 4. For older patients, I include fraction(young)d,87•post as an estimate of the
underlying complication rate in each DRG pair.26
Table 4 reveals that upcoding is sensitive to changes in spread, even after controlling for the
large average increase in spread between 1987 and 1988. As hypothesized, the upcoding response
appears to be larger for older patients: the coefficient estimates imply a spread-induced increase of .022
in the fraction of old patients coded with CC, as compared to .015 for younger patients. The year
coefficients indicate that the fraction of older patients assigned to the top code declined in 1988 as
expected, but this decline was least where the incentive to retain patients in the top code was greatest.
Young patients experienced an increase of .02 in reported complications between 1987 and 1989;

25
26

This dollar amount is based on Ph for urban hospitals in 2001, which was $4,376.
An alternative specification using ∆spreadd,88-87•post as an instrument for spreaddt yields similar results.

22

however, due to the strong upward trend in fractiondt throughout the study period, this increase cannot
be unequivocally attributed to the policy change.
The spread-related upcoding alone translates into a price increase of .7 percent and .9 percent
for young and old patients admitted to DRG pairs, respectively.27 The estimate for young patients rises
to 1.5 percent if the jump between 1987 and 1989 is included, although this is an upper-bound estimate
due to the potential role of confounding factors. These figures imply increased annual payments of
$330 to $425 million, a substantial reward for altering coding practices. 28 Table 5 summarizes the
contributions of the various components of the total relative price increase for DRG pairs. The severity
component is the residual remaining after the mechanical and upcoding components are taken into
account. Table 5 shows that the vast majority of the relative price increase for DRG pairs can be
attributed to HCFA’s recalibration errors, despite the large and costly upcoding response. Thus, the
policy-induced price change remains an excellent instrument for DRG price even after the upcoding
and severity components are removed.
HCFA’s 1990 across-the-board reduction in DRG weights decreased annual payments by $1.13
billion, more than wiping out these estimated windfalls. However, while this reduction affected all
hospitals equally, the rewards from upcoding only accrued to those hospitals engaging in it. The
following section investigates the relationship between hospital characteristics and upcoding responses.

6.2.2 Hospital Upcoding Analysis
To determine whether individual hospitals responded differently to the changes in upcoding incentives,
I estimate equation (3) separately for subsets of hospitals. For example, I compare the results obtained
using data solely from teaching hospitals with those obtained using the sample of non-teaching

27

These estimates are calculated using the average spread in 1988 (.45), together with the average weights for DRG pairs in
1987 (1.05 for young patients, 1.13 for older patients).
28
These estimates are conservative because upcoding among the old is underestimated. This is likely to be important both
because the old account for 70 percent of Medicare admissions, and because upcoding is more prevalent in this group. Dollar
figures are calculated using PPS expenditures in 2000.

23

hospitals.29 I also consider stratifications by ownership type (for-profit, not-for-profit, government),
financial status, region, size, and market-level Herfindahl index.30
Table 6 presents estimates of δ and ψ by hospital ownership type, financial status, and region.
Figure 2 plots the δ̂ from these specifications. For both young and old patients, there are no statistically
significant differences in the response to ∆spread across the hospital groups. The discussion here is
therefore limited to the results for young patients, for whom the year coefficients are relevant.
The main finding is that for-profit hospitals upcoded more than government or not-for-profit
facilities following the 1988 reform. Consistent with the incentive provided to some for-profit
managers to globally code more patients with complications, the heightened for-profit response is
manifested in the time-series increase in fractiondt, not in the spread coefficient. Figure 2 illustrates
that upcoding trends were the same for all three ownership types until 1987, but thereafter the trend for
for-profits diverges substantially. By 1991, the fraction of young patients with complications had risen
by .18 in for-profit hospitals, compared with ~.13 for the other two groups. Given a universal mean of
.65 in 1987, these figures are extremely large.
Hospitals with high debt:asset ratios and hospitals in the South also exhibited very large increases
in fraction, although Figure 2 illustrates that these trends pre-date the policy change. Moreover, the
strong presence of for-profits in the South and the tendency of for-profits to be highly-leveraged
suggests that for-profit ownership is driving the large fraction gains in these subsamples as well. All
other hospital characteristics were not associated with changes in upcoding proclivity.
To summarize, HCFA’s decision to increase the difference between the prices for complicated
and uncomplicated patients with the same diagnosis unleashed a substantial upcoding response. I
29

One alternative to this approach is to disaggregate the data into hospital-DRG-year cells, and to estimate the following
equation separately for each subset of hospitals: fractionhdt= α + µhospitalh + ςDRGd + δyeart +ψ∆spreadd,88-87·postt + ε hdt,
where hospitalh is a vector of hospital fixed effects. However, since the independent variable of interest varies at the drg-year
level, using drg-year cells is the more conservative approach. Moreover, the size of the dataset precludes estimation of an
analogous equation for the intensity response (section 7.2), hence for consistency I employ specification (3).
30
The Herfindahl index is calculated as the sum of squared market shares for all hospitals within a health service area. I
constructed two such measures, one using the health service areas reported in the AHA data and another using the health
service areas defined by the Dartmouth Atlas on Health Care (1996).

24

estimate that upcoding generated by the 1988 recalibration alone increased the average price for
patients in DRG pairs by approximately one percent. These estimates come from an especially robust
and comprehensive empirical investigation; I study not only the time-series response to an
unanticipated policy reform, but also differential responses across 93 DRG pairs.

7

Intensity and Volume Responses

Given that HCFA’s recalibration mistakes following the 1988 policy change resulted in substantial
mechanical price increases for affected DRGs, intensity and volume responses to price changes can be
identified using a differences-in-differences specification. If the flypaper effect operates in this setting,
intensity levels should rise in affected DRGs relative to unaffected DRGs after 1988. Furthermore, this
response should be greater in those DRGs subjected to larger mechanical price increases.
I use five different measures of intensity and quality of patient care to investigate this response:
total costs (=total charges from MedPAR deflated by annual cost:charge ratios from the Cost Reports
and converted to $1990 using the hospital services CPI), length of stay, number of surgeries, number of
ICU days, and in-hospital deaths. All variables are normalized by the number of admissions in the
relevant cell (i.e., average cost per patient in “DRG” 138/139 in 1987). The first four measures are
strong indicators of hospital expenditures on behalf of patients.31 Death rate is clearly an important,
albeit limited, indicator of quality of care. Although these measures are commonly used in the health
economics literature, they are imperfect. One of the most common measures, length of stay, could be
correlated positively or negatively with quality of care. Better care may enable a patient to leave
sooner; on the other hand, hospitals may discharge patients too early in order to cut costs. (The latter

31

Total charges (deflated by hospital cost:charge ratios) should be positively correlated with the services provided to patients;
indeed, this is the measure HCFA uses to calculate DRG weights, so that diagnosis groups with higher average charges are
reimbursed more than diagnosis groups with lower average charges.

25

was of greater concern in the 1980s, as lengths of stay fell dramatically in response to PPS.) However,
the consistency of the aggregate results across all of the variables suggests that the findings are robust.
Given the model outlined in section 2, another way to identify an intensity response is to look at
the volume of patients admitted. If hospitals do increase intensity of care within affected DRGs, they
should also admit more patients in these DRGs. Stated another way, hospitals seeking to increase
volume in affected DRGs following the price shock must increase their investment in intensity.32

7.1 Aggregate Intensity and Volume Responses
To examine the effect of the policy change on intensity and volume, I estimate the same specification
used to study the effect of the policy change on price, replacing ln(price) with ln(intensity) or
ln(admissions):

(4) ln(intensity or admissions) dt = α + ςDRG d + δyeart + γaffected DRG d • post t
+ κ 2 affected DRG d • post t • ∆ ln(Laspeyres price) dt + ε dt .
γ captures the average response to the policy change, while κ 2 allows this response to vary with the

magnitude of the mechanical price increase. To ensure that γ̂ and κ̂ 2 are not capturing pre-existing
trends in intensity or volume, I again estimate this specification with separate affected DRG•year
dummies in place of affected DRG•post, an affected DRG trend, and finally individual DRG time
trends. For each dependent variable, the results from the latter specification are reported in Table 7, in
the row labeled “Reduced Form.”33
I find no evidence that hospitals altered their treatment policies or increased their admissions
differentially for patients in affected DRGs as a result of the 1988 classification change. The

32

Note that advertising can certainly be one component of intensity, although I do not have data on such expenditures.
Observations with a value of zero for the unlogged dependent variable are dropped. Regressions of 1 (intensity>0) reveal
no relationship with the year and affected•year dummies; hence, Tobit estimates using the unlogged dependent variables did
not differ from OLS results for the same specifications. The interpretation is that there are some DRGs for which an intensity
measure is typically zero, such as death rate in the DRG for tonsillectomy, and excluding such DRGs from the intensity
analyses does not affect the estimation.
33

26

nonresponse appears to be uniform across affected DRGs, regardless of the size of the mechanical price
increase. The point estimates of κ 2 are fairly small, statistically insignificant, and of the wrong sign for
3 of the 6 equations. The corresponding IV estimates of the elasticity of intensity and volume with
respect to price (= κ̂ 2 / κ̂1 from Table 3, column 6) are therefore small and imprecisely estimated. To
obtain upper bounds for these elasticities, I run OLS regressions of intensity on price. These estimated
elasticities, also reported in Table 7, are upward-biased due to the price recalibration method.34
Notwithstanding this bias, the point estimates are extremely small. For example, the OLS
estimate indicates that only 13 cents of every additional dollar of reimbursement within a DRG is spent
on care for patients in that DRG. The elasticity of length of stay with respect to price (.18) is similar to
the estimate reported in Cutler (1990) (.23), but the elasticity of surgeries is much smaller (-.03 as
compared to .23), and there is no evidence that in-hospital mortality rates decline in price, as reported
in Cutler (1995). Overall, Table 7 suggests that the flypaper effect is very weak in this sector.

7.2 Intensity and Volume Responses Across Hospitals and DRGs
7.2.1 Responses Across Hospitals
The aggregate analysis captures the average intensity and volume responses across all admissions, but
masks potentially different responses across hospitals. According to the model defined in section 2.2,
hospitals with stronger profit objectives and/or more quality-elastic demand should increase intensity
(and therefore volume) more in response to price increases. To determine whether individual hospitals
responded differently, I estimate equation (4) separately for the various hospital subsamples. Individual
DRG trends are included in all analyses to control for differences in underlying trends across DRGs.
The results provide little evidence of real responses to price increases during the study period.
Due to the large volume of coefficients generated by these models, tables are not included here. Out of
34

One manifestation of this bias is the positive estimated elasticity of death rate with respect to price; the explanation for this
puzzling result is simply that those DRGs that experience increases in death rates receive higher reimbursements because inhospital care for the dying is very expensive.

27

126 regressions (6 dependent variables * 21 subsamples), κ̂ 2 is statistically significant in only 9, and
in most of these cases, the responses are not consistent across the various intensity measures. 35 The
sole exceptions are large hospitals (300+ beds) and teaching hospitals (90 percent of which have more
than 300 beds). Hospitals in these subsamples increased relative ICU days substantially in response to
the price increases. The IV estimates of ICU elasticity are 1.01 (.451) and 1.47 (.63) for large hospitals
and teaching hospitals, respectively. The elasticity of total costs with respect to price is
correspondingly positive and statistically significant for these samples as well: .49 (.20) and .33 (.17),
respectively. There was no significant volume response for these or any other subset of hospitals;
indeed, the sign of the volume response was negative in 125 of the 126 specifications.
Section 2 offers several possible explanations for the scant evidence of real responses to the
very real price increases documented in Table 3. The arguments focus on the potential inability of
hospitals to alter intensity at the DRG level, and of patients in turn to respond. The quality elasticity of
demand is paramount in generating an intensity response, and there is reason to believe that this
elasticity is extremely low for certain diagnoses. For example, even if hospitals invest in improving
care for amputees, these investments are unlikely to yield additional volunteers for the surgery. This
reasoning suggests that it may be more fruitful to examine intensity responses separately by DRG type.

7.2.2 Responses Across DRGs
All admissions in the MedPAR files are assigned to one of 5 categories: emergency (admitted through
the ER, 44 percent of admissions in 1987); urgent (first available bed, 29 percent); elective (23
percent); newborn (0.1 percent); unknown (4 percent). To see how intensity and volume responses
differ across these admission types, I assign each DRG to the group accounting for the plurality of its
admissions in 1987. I then perform both stages of the intensity analysis (equations 2 and 4) separately
by group. The elasticity estimates (= κ̂ 2 / κ̂1 ) are reported in Table 8.
35

Tables are available upon request.

28

Again, the intensity and volume responses are fairly weak. Notwithstanding the strong
financial incentive to attract more patients in affected DRGs, hospitals did not increase volume
differentially for affected DRGs in any admission category following the 1988 relative price increase.
(The lack of a volume response also implies that hospitals neglected to upcode across DRGs by
shifting patients from unaffected to affected DRGs.) However, the estimated intensity elasticities are
largest – and in one case, statistically significant – for elective diagnoses. The point estimates provide
suggestive evidence that hospitals channeled extra funds to these quality-elastic admissions, but were
not rewarded with additional patients or improved outcomes.
Overall, there is little robust evidence of real responses to the changes in DRG prices
documented in sections 5 and 6. There is no evidence of volume responses in any subsample of the
data, indicating that concerns about hospitals “pushing” certain procedures are unfounded during this
time period. To the extent that an intensity response occurred, it was concentrated in elective
diagnoses, where patients are likeliest to respond, and in large and/or teaching hospitals, whose
operations are more conducive to fine-tuning at the diagnosis level.

7.3 Why Didn’t Hospitals Respond?
Given the simultaneous price increase for top codes and decrease for bottom codes within DRG pairs,
one possibility is that hospitals may not have realized they were receiving a relative price increase for
the pairs as a whole.36 Even if hospitals were cognizant of the price increase in affected DRGs, their
response may have been muted because of the simultaneous price decrease in unaffected DRGs. The
net result was that average prices for all admissions did not increase by much. A positive intensity
response would therefore involve a decrease in intensity for unaffected DRGs, and to the extent that
decreases are more difficult to implement than increases, the coefficients I obtain may underestimate
the true intensity-price relationship. This explanation, though certainly a possibility, is by no means a
36

I thank David Cutler for this insight.

29

certainty: immediately following the implementation of PPS, hospitals showed themselves quite
capable of reducing overall intensity in all of the dimensions I explore.
Another possibility is that hospitals optimize overall intensity, rather than intensity by DRG.
To investigate this hypothesis, I aggregate the individual data into hospital-year cells. The relationship
of interest is the elasticity of hospital intensity with respect to hospital price, which can be estimated
from
(5) ln(intensity)ht = α + µhospitalh + δyeart + β ln(price) ht + ε ht .

However, there are two sources of bias in the OLS estimate of β̂ : (1) the DRG recalibration method;
(2) the omission of an annual hospital-level measure of patient severity. As with the previous analyses,
the policy change can be used to identify β, but hospital-level variation in the impact of the policy
change is required – a differences-in-differences strategy comparing affected and unaffected DRGs
cannot be implemented with hospital-year data. Because hospitals with a large fraction of admissions
in the “with CC” DRGs benefited the most from the policy reform, the interaction between this
measure and a dummy for the post-reform years can serve as an instrument for average price in
equation (5).37
In constructing this instrument, I use the 1987 share of Medicare patients who are young (under
70) and coded with CC (hereafter called share CC). I select the pre-shock year because
contemporaneous share CC would be affected by post-shock upcoding responses, and I use young
patients only because the data do not indicate whether old patients had CC before the policy change.
This instrument captures the mechanical, or exogenous, component of the hospital-level price increase:
hospitals with a large share CC in 1987 enjoyed larger increases in their average DRG price
independently of their upcoding response to the policy change and any change in the true severity of

37

An alternative instrument for hospital price is ∆ln(Laspeyres price)ht = ln(Laspeyres price)ht – ln(price)h,1987. However,
because the actual DRGs sampled for each hospital varies substantially over time, and DRG controls cannot be included in
this specification, share CC is a much more accurate measure of the mechanical component at this level of aggregation.

30

patients. Eliminating the upcoding and severity components from the instrument ensures that the IV
estimate will be unbiased even if these components are associated with intensity decisions.
Table 9 gives the results from the first-stage regression of ln(price) on share CC•post,
(6) ln(price)ht = α + µhospitalh + δyeart + τ1share CCh • post t + ε ht ,

where hospitalh is a vector of hospital fixed effects. The mean (standard deviation) of share CC in
1987 is .086 (.043). A two-standard-deviation increase in share CC is associated with a two percent
increase in the average price paid to a hospital following the policy change. To illustrate that share CC
is uncorrelated with average hospital prices in the pre-reform years (after hospital fixed effects are
included), column 2 presents the results from a regression of ln(price) on share CC •year dummies.
Coefficient estimates from the reduced-form equation,
(7) ln(intensity)ht = α + µhospitalh + δyeart + τ2share CCh • post t + ε ht ,

are presented under “Reduced Form” in Table 10, followed by IV and OLS estimates of equation (5).
The IV estimates for the elasticity of hospital intensity with respect to average hospital price are
positive for 4 of the 5 intensity measures, and statistically significant for 3. The exception is the inhospital death rate, for which estimated elasticity is negative, but insignificant (a positive coefficient on
death rate implies a negative intensity response). The elasticity results reveal that an additional dollar
of reimbursement goes wholly toward patient care. Extra reimbursement is associated with longer

stays, more surgeries, more ICU days, and possibly worse outcomes.
Hospitals subjected to price increases not only increased their intensity of care, but also
succeeded in drawing in additional patients: for every one-percent increase in price, total admissions
increased by 1.7 percent. This volume response also explains the large, positive coefficient on inhospital mortality: if greater intensity of care attracts sicker patients, as posited in section 2, outcomes
may actually deteriorate.
The intensity results in Table 9 are consistent with two distinct models of hospital behavior:
competition in overall intensity, and maximization of overall intensity subject to a budget constraint.

31

The fact that volume responds to increases in intensity provides a motive for the former, but does not
rule out the latter. The preponderance of the evidence does not, however, support the commonlyassumed model of intensity competition at the diagnosis level. The lack of diagnosis-specific intensity
responses contrasts with earlier research and helps to explain why diagnosis specialization is very
limited in inpatient care.

8 Conclusion
As public and private healthcare insurers continue to strengthen financial incentives for efficiency in
the production of healthcare, it is critical to understand what the implications of such incentives are for
health care quality and expenditures. The fixed-price system used by many insurers makes hospitals
the residual claimants of profits earned on inpatient stays. These profits differ by diagnosis, creating
incentives for hospitals to increase the volume of admissions in profitable diagnoses relative to
unprofitable diagnoses. If hospitals respond to these incentives, we may see them encouraging certain
types of admissions and discouraging others, a practice that could be innocuous in other fixed-price
industries (e.g., utilities), but is potentially dangerous in this setting. For example, doctors at Redding
Medical Center, a for-profit hospital operated by Tenet Healthcare Corporation in Redding, California,
are currently under criminal investigation for performing lucrative open-heart surgeries in place of
medically managing symptoms of heart disease (Eichenwald 2003).
Resolving the question of how hospitals respond to changes in DRG prices, which are simply
shocks to the profitability of certain diagnoses or treatments, is therefore critical from a policy
standpoint. In addition, these responses provide a window into industry conduct. In theory, quality

32

erosion is kept in check by competition among hospitals.38 Responses to individual price changes can
reveal whether this competition occurs at the level of the DRG, or product line.
This study illustrates how a simple change in the DRG classification system in 1988 generated
large and exogenous relative price increases for 40 percent of DRG codes, accounting for 43 percent of
Medicare admissions. Hospitals responded to these price changes by upcoding patients to DRG codes
associated with large reimbursement increases, garnering $330-$425 million in extra reimbursement
annually. They proved quite sophisticated in their upcoding strategies, upcoding more in those DRGs
where the reward for doing so increased more. Finally, while all subsamples of hospitals upcoded
more following the policy change, for-profit facilities availed themselves of this opportunity to the
greatest extent.
Whereas coding behavior proved very responsive to financial incentives, admissions and
treatment policies did not. Using a differences-in-differences identification strategy, I find no evidence
of a relative increase in admissions to DRGs subjected to price increases, and very limited evidence of
increases in intensity of care. However, I find strong evidence that hospitals spent the extra funds they
received on patient care in all DRGs. This finding suggests that hospitals do not (or cannot) optimize
intensity choices by product line, and may compete instead in overall quality levels.39
These results may help to explain the relative lack of specialization in the hospital industry.
One anticipated benefit of PPS was that hospitals would specialize in admissions in which they were
relatively cost-efficient. If, however, hospitals do not balance costs and benefits within individual
product lines, such specialization is unlikely to occur. Another implication of these results is that
insurers may be unable to use prices to encourage quality improvements in specific diagnoses. More
generally, this research suggests that better models of hospital behavior are necessary for anticipating
the impacts of public and private-sector actions in this important industry.
38

Of course, physicians also play an important role in ensuring appropriate care for their patients, as highlighted by Arrow
(1963).
39
Previous studies have also found a positive relationship between overall hospital intensity and financial pressure; see
footnote 14.

33

References
Arrow, Kenneth J., 1963, “Uncertainty and the Welfare Economics of Medical Care,” American
Economic Review 53, No. 5: 941-973.
Bulow, J., Geanakoplos, J., and P. Klemperer, 1995, “Multimarket Oligopoly: Strategic Substitutes and
Complements,” Journal of Political Economy, 93: 488-511.
Carter, Grace M., Newhouse, Joseph P., and Daniel A. Relles, 1990, “How Much Change in the Case
Mix Index is DRG Creep?” Journal of Health Economics, 9(4): 411-428.
Center for the Evaluative Clinical Sciences, Dartmouth Medical School, 1996. The Dartmouth Atlas of
Health Care (Chicago: American Hospital Publishing).
Coulam, Robert F., and Gary L. Gaumer, 1991, “Medicare’s Prospective Payment System: A Critical
Appraisal,” Health Care Financing Review Annual Supplement: 45-77.
Cutler, David M., 1990, “Empirical Evidence on Hospital Delivery Under Prospective Payment,” MIT
mimeo.
Cutler, David M., 1995, “The Incidence of Adverse Medical Outcomes under Prospective Payment,”
Econometrica 63, No. 1: 29-50.
Cutler, David M., 1998, “Cost Shifting or Cost Cutting?: The Incidence of Reductions in Medicare
Payments,” in J. Poterba, ed., Tax Policy and the Economy, Volume 12, Cambridge, MA: MIT Press.
Cutler, David M. and Jill R. Horwitz, 1999. “Converting Hospitals from Not-for-Profit to For-Profit
Status: Why and What Effects?” in D. Cutler, ed., The Changing Hospital Industry: Comparing Notfor-Profit and For-Profit Institutions, Chicago: University of Chicago Press and NBER.

Data Compendium, 2002. Baltimore, MD.: U.S. Dept. of Health and Human Services, Health
Care Financing Administration, Bureau of Data Management and Strategy.
Dranove, David, 1987, “Rate-Setting by Diagnosis Related Groups and Hospital Specialization,”
RAND Journal of Economics 18: 417-427.
Duggan, Mark, 2000, “Hospital Ownership and Public Medical Spending,” Quarterly Journal of
Economics, 115(4): 1343-1373.
Duggan, Mark, 2002, “Hospital Market Structure and the Behavior of Not-for-Profit Hospitals,” Rand
Journal of Economics, Autumn.
Eichenwald, Kurt, 2003, “How One Hospital Benefited on Questionable Operations,” The New York
Times (August 12).
Ellis, Randall P. and Thomas McGuire, 1996, “Hospital Response to Prospective Payment: Moral
Hazard, Selection, and Practice-Style Effects,” Journal of Health Economics, 15: 257-277.

34

Federal Register, 1984-2000. U.S. Office of the Federal Register, National Archives and Records
Service, General Services Administration, Washington, D.C.:U.S. Government Printing Office.
Gilman, Boyd H., 2000, “Hospital Response to DRG Refinements: The Impact of Multiple
Reimbursement Incentives on Inpatient Length of Stay,” Health Economics, 9: 277-294.
Hadley, Jack, Zuckerman, Stephen, and Judith Feder, 1989, “Profits and Fiscal Pressure in the
Prospective Payment System: Their Impacts on Hospitals,” Inquiry, 26(3): 354-365.
Hodgkin, Dominic and Thomas G. McGuire, 1994, “Payment Levels and Hospital Response to
Prospective Payment,” Journal of Health Economics, 9: 1-29.
Lagnado, Lucette, 1997, “Columbia/HCA Graded Its Hospitals on Severity of Their Medicare Claims,”
The Wall Street Journal (May 30): A1, A6.
Newhouse, Joseph P., 1989, “Do Unprofitable Patients Face Access Problems?” Health Care Financing
Review, 11(2): 33-42.
Psaty, Bruce M., Robin Boineau, Lewis H. Kuller, and Russell V. Luepker, 1999, “The Potential Costs
of Upcoding for Heart Failure in the United States,” The American Journal of Cardiology, (July 1)
84:108-109.
Silverman, Elaine and Jonathan Skinner, 1999. “The Association between For-Profit Hospital
Ownership and Increased Medicare Spending,” New England Journal of Medicine, 341(6): 420-426.
Silverman, Elaine and Jonathan Skinner, 2000. “Are For-Profit Hospitals Really Different? Medicare
Upcoding and Market Structure,” NBER Working Paper W8133.
Staiger, Douglas and Gary L. Gaumer, “The Impact of Financial Pressure on Quality of Care in
Hospitals: Post-Admission Mortality Under Medicare’s Prospective Payment System,” Working Paper
prepared for the Health Care Financing Administration. Cambridge, MA. Abt Associates, Inc.
Steinwald, Bruce and Laura A. Dummit, 1989, “Hospital Case-Mix Change: Sicker Patients or DRG
Creep?” Health Affairs, 8(2): 35-47.
Statistical Abstract of the United States, 2001, 1998, 1991, 1987. U.S. Census Bureau, Administrative
and Customer Services Division, Statistical Compendia Branch. Washington, D.C.: U.S. Government
Printing Office.

35

Appendix
The total change in the price paid to hospitals for admissions to affected DRGs following the
elimination of the age criterion can be subdivided into three components: mechanical, upcoding, and
severity. As noted in the text, the mechanical component is the effect of the recalibration on prices,

holding the incidence of complications constant – essentially, it captures mistakes made by HCFA in its
recalibration. To estimate this component, I construct a Laspeyres price index for each paired DRG in
each year after 1987 using 1987 admissions of young patients as the fixed weights. I then subtract the
1987 price to obtain a measure of the price increase in each DRG-year, holding constant the fraction of
patients with CC. This measure (denoted ∆ln(Laspeyres price)dt in the text) incorporates all of the
price changes associated with the annual recalibrations after 1987 – i.e., that due to the policy change,
and that due to differences across DRGs in annual charge growth. To eliminate the latter from this
instrument, I regress the instrument on the lagged charges actually used in the updating process, and
use the residuals as the final instrument. Because HCFA operates with a 2-year lag for updating, this
process amounts to the following:
for each t>87, regress ∆ln(Laspeyres price)dt=(ln(Laspeyres price)dt – ln(price)d,1987) on
(ln(charges) d,t-2 -ln(charges) d,85) and a constant. The residuals from each regression constitute the
final instrument.

36

Figure 1. Effects of Policy Change on Relative Prices
for Affected DRGs, By Year

.10
.09
.08
.07
.06
.05
.04
.03
.02
.01
.00
1986

1987

1988

1989

1990

1991

Notes: Estimates of γ from ln(price)dt= α +ςDRGd + δyeart+
γaffected DRGd•yeart +εdt

37

Figure 2. Effect of Policy Change on Upcoding
of Young, by Hospital Characteristics

By Ownership Type

For-profit
Government
Not -for-profit

.18
.16
.14
.12
.10
.08
.06
.04
.02
1986

1987

1988

1989

By Financial State

1990

1991

Dist ressed
Not dist ressed

.18
.16
.14
.12
.10
.08
.06
.04
.02
1986

1987

1988

1989

1990

1991

Northeast
South
Midwest
West

By Region
.18
.16
.14
.12
.10
.08
.06
.04
.02
1986

1987

1988

1989

1990

Source: Year coefficients from Table 6.

38

1991

Table 1. Examples of Policy Change
DRG code Description in 1987
(Description in 1988)

1987 weight 1988 weight

% change
in weight

1987 volume 1988 volume
(20% sample) (20% sample)

% change
in volume

96

bronchitis and asthma age>69 and/or CC
(bronchitis and asthma age>17 with CC)

.8446

.9804

16%

44,989

42,314

-6%

97

bronchitis and asthma age 18-69 without CC
(bronchitis and asthma age>17 without CC)

.7091

.7151

1%

4,611

10,512

128%

138

cardiac arrhythmia and conduction disorders
age>69 and/or CC (cardiac arrhythmia and
conduction disorders with CC)

.8136

.8535

5%

45,080

35,233

-22%

cardiac arrhythmia and conduction disorders
age<70 without CC (cardiac arrhythmia and
conduction disorders without CC)

.6514

.5912

-9%

4,182

16,829

302%

nutritional and misc. metabolic disorders
age>69 and/or CC (nutritional and misc.
metabolic disorders age>17 with CC)

.8271

.9259

12%

45,903

38,805

-15%

nutritional and misc. metabolic disorders age
18-69 without CC (nutritional and misc.
metabolic disorders age>17 without CC)

.6984

.5791

-17%

2,033

12,363

508%

139

296

297

Notes:

Of the 95 affected pairs, these three occur most frequently in the 1987 20% MedPAR sample.

39

Table 2. Descriptive Statistics
Unit of Observation
N
price (DRG weight)
Laspeyres price
observations per cell
Nominal responses
fraction(young) in top code
fraction(old) in top code
Real Responses
mean cost ($)
mean LOS (days)
mean surgeries
mean ICU days
death rate
mean admissions
Instruments
1988 spread-1987 spread
(1988 spread-1987 spread) •post
affected
affected•post
∆ln(Laspeyres price)
affected•post•∆ln(Laspeyres price)
share CC
share CC•post

DRG-year
Mean
SD

2482
2482
2482

1.26
1.25
6128

(.91)
(.90)
(12817)

650
650

.66
.85

(.14)
(.15)

2474 6000 (4889)
2482 10.64 (5.64)
2450 1.15
(.73)
2290
.72 (1.18)
2123
.07
(.10)
2482 32921 (30981)
650
650
2482
2482
368
2478

.20
.12
.45
.26
.01
.00

N

Hospital-year
Mean
SD

36651

1.27

(.19)

36651

373

(389)

36169
36651
35897
28226
34992
36651

6450
8.81
1.21
.81
.06
778

(3005)
(2.21)
(.55)
(.59)
(.02)
(538)

36651
36651

.09
.05

(.03)
(.05)

(.16)
(.16)
(.50)
(.44)
(.05)
(.03)

Notes: Nominal responses are calculated for DRG pairs only. Means are weighted by the number of observations
in the 20 percent MedPAR sample, with the exception of observations per cell.

40

Table 3. Total Effect of Policy Change on DRG Prices

Affected•post

.071 ***
(.012)

Dependent Variable is ln(price)
mean(price) = 1.26
.066 ***
.065 ***
.062 ***
(.016)
(.013)
(.011)
1.233 ***
(.092)

.064 ***
(.014)
1.234 ***
(.092)

.064 ***
(.013)
.629 ***
(.124)

-.017
(.014)
-.017
(.014)
-.049 ***
(.015)
-.045 **
(.016)
-.055 ***
(.016)
-.061 ***
(.016)
Y
N
N
.977
2482

-.018
(.015)
-.018
(.015)
-.049 ***
(.015)
-.045 **
(.016)
-.057 ***
(.017)
-.062 ***
(.018)
Y
Y
N
.977
2482

-.017
(.014)
-.016
.015
-.048 ***
(.014)
-.043 **
(.016)
-.056 ***
(.017)
-.062 ***
(.018)
Y
Y
N
.979
2478

-.008
(.012)
.001
(.019)
-.019
(.027)
-.004
(.035)
-.006
(.044)
-.000
(.052)
Y
N/A
Y
.990
2478

Affected•post•∆ln(Laspeyres price)
Year dummies
1986
1987
1988
1989
1990
1991
DRG fixed effects
Affected DRG trend
DRG trends
Adj. R-squared
N

-.009
(.012)
.000
(.019)
-.019
(.027)
-.005
(.035)
-.006
(.044)
-.001
(.053)
Y
N/A
Y
.990
2482

-.017
(.013)
-.017
(.013)
-.048 ***
(.015)
-.043 **
(.015)
-.056 ***
(.015)
-.062 ***
(.015)
Y
N
N
.979
2478

Notes: The unit of observation is DRG-year (where "DRG" refers to single DRGs as well as to DRG pairs). All observations are weighted by the number of admissions
in the 20% MedPAR sample. The sum of the weights is 15.2 million. Standard errors are robust.
* signifies p<.05, ** signifies p<.01, *** signifies p<.001

41

Table 4. Effect of Policy Change on Upcoding
fraction(young)
mean = .66
∆spread88-87•post

.077 ***
(.016)

fraction(young)87•post
Year dummies
1986
1987
1988
1989
1990
1991
Adj. R-squared
N

fraction(old)
mean = .85
.108 ***
(.015)
.731
(.020) ***

.044 ***
(.008)
.077 ***
(.008)
.058 ***
(.011)
.097 ***
(.009)
.115 ***
(.009)
.128 ***
(.010)
.948
650

.000
(.005)
-.011 *
(.005)
-.813 ***
(.014)
-.780 ***
(.014)
-.764 ***
(.014)
-.748 ***
(.014)
.960
650

Notes: Regressions include DRG fixed effects. “Young” refers to Medicare
beneficiaries under 70; “Old” refers to beneficiaries aged 70+. The unit of
observation is DRG-year. Single DRGs are not included. All observations are
weighted by the number of admissions in the 20% MedPAR sample. The sum
of the weights is 1.9 million (young) and 5.0 million (old). Standard errors are
robust.
* signifies p<.05, ** signifies p<.01, *** signifies p<.001

42

Table 5. Decomposition of Price Change for Affected DRGs

Conservative estimate
Percent of total
Liberal estimate
Percent of total

Total Price
Change
7.1%
100.0%
7.1%
100.0%

Mechanical
Component
4.6%
64.8%
4.6%
64.8%

43

Upcoding
Component
0.8%
11.7%
1.1%
14.9%

Severity
Component
1.7%
23.5%
1.4%
20.3%

Table 6. Effect of Policy Change on Upcoding of Young, by Hospital Characteristics
By Ownership Type

∆spread88-87•post
Year fixed effects
1986
1987
1988
1989
1990
1991
δˆ89 − δˆ87

Adj. R-squared
N

By Financial State

By Region

For-profit
.071 **
(.024)

Not-forprofit
.080 ***
(.017)

Government
.058 ***
(.018)

Distressed
.082 ***
(.109)

Not
distressed
.074 ***
(.017)

Northeast
.083 ***
(.016)

Midwest
.062 ***
(.018)

South
.082 ***
(.017)

West
.079 ***
(.024)

.038 ***
(.009)
.081 ***
(.010)
.080 ***
(.012)
.140 ***
(.011)
.147 ***
(.011)
.179 ***
(.011)
.059 ***
(.010)

.047 ***
(.009)
.077 ***
(.008)
.055 ***
(.011)
.094 ***
(.009)
.114 ***
(.009)
.123 ***
(.011)
.016
(.009)

.040 ***
(.008)
.078 ***
(.008)
.065 ***
(.012)
.104 ***
(.009)
.120 ***
(.010)
.136 ***
(.010)
.027 ***
(.008)

.059 ***
(.008)
.099 ***
(.008)
.083 ***
(.011)
.127 ***
(.009)
.144 ***
(.009)
.161 ***
(.010)
.027 **
(.009)

.041 ***
(.009)
.073 ***
(.008)
.054 ***
(.011)
.094 ***
(.009)
.112 ***
(.010)
.124 ***
(.010)
.022 *
(.009)

.095 ***
(.008)
.123 ***
(.007)
.104 ***
(.010)
.131 ***
(.009)
.148 ***
(.008)
.159 ***
(.010)
.007
(.008)

.027 **
(.009)
.054 ***
(.009)
.036 ***
(.010)
.075 ***
(.010)
.092 ***
(.009)
.103 ***
(.011)
.021 *
(.010)

.033 ***
(.009)
.078 ***
(.008)
.063 ***
(.012)
.111 ***
(.009)
.132 ***
(.010)
.148 ***
(.010)
.033 ***
(.008)

.035 ***
(.012)
.052 ***
(.012)
.024 ***
(.014)
.067 ***
(.012)
.080 ***
(.013)
.091 ***
(.014)
.015
(.014)

.914
650

.946
650

.927
650

.933
650

.947
650

.939
650

.940
650

.940
650

.915
650

Notes: Regressions include DRG fixed effects. “Young” refers to Medicare beneficiaries under 70. "Distressed" denotes hospitals with 1987 debt:asset ratios at the 75th percentile
or above. The unit of observation is DRG-year. Single DRGs are not included. All observations are weighted by the number of admissions in the 20% MedPAR sample.
Hospitals with missing values for any of the hospital characteristics are dropped. The sum of the weights is 1.45 million. Standard errors are robust.
* signifies p<.05, ** signifies p<.01, *** signifies p<.001

44

Table 7. Real Responses to Changes in DRG Prices
ln(cost)

ln(LOS)

Dependent Variable
ln(surg)
ln(ICU)

mean=$5,995 mean=10.63

ln(death rate)

ln(volume)
mean=32,944

mean=1.15

mean=.72

mean=.07

-.005
(.016)
-.094
(.127)

-.023
(.034)
.395
(.271)

-.019
(.038)
.619
(.437)

.040
(.025)
-.107
(.373)

.090
.190
-.149
.627
(.189)
(.197)
(.194)
(.450)
Parametric Tests of H0: IV estimate>=x; H1: IV estimate<x (p-values are reported)
x = .5
.02
.06
.00
.62
x=1
.00
.00
.00
.21

.984
(.758)

-.171
(.604)

.03
.00

.13
.03

Reduced Form
Affected•post
Affected•post•
∆ln(Laspeyres price)

.007
(.011)
.057
(.118)

.012
(.017)
.119
(.126)

IV Estimate
ln(price)

OLS Estimate
ln(price)
N

.126 ***
(.037)
2470

.182 ***
(.043)
2478

-.029
(.047)
2446

.253 **
(.089)
2286

.258 *
(.115)
2119

-.048
(.092)
2478

Notes: Regressions include year fixed effects, DRG fixed effects, and DRG trends. Unlogged means are reported. The unit of observation is DRGyear (where "DRG" refers to single DRGs as well as to DRG pairs). All observations are weighted by the number of admissions in the 20%
MedPAR sample. The sum of the weights is 15.2 million. For ln(death rate), the tests presented are H0: IV estimate<=x; H1: IV estimate>x for x=-.5
and x=-1. Standard errors are robust.
* signifies p<.05, ** signifies p<.01, *** signifies p<.001

45

Table 8. Real Responses to Changes in DRG Prices, by DRG Type

Emergency DRGs
IV Estimate
ln(price)
N
Urgent DRGs
IV Estimate
ln(price)
N
Elective DRGs
IV Estimate
ln(price)
N

Dependent Variable
ln(surg)
ln(ICU)

ln(cost)

ln(LOS)

ln(death rate) ln(volume)

-.181
(.236)
1334

.164
(.298)
1334

-.211
(.260)
1324

.404
(.582)
1255

-.031
(.539)
1186

.003
(.528)
1334

.193
(.461)
240

-.530
(1.562)
245

.182
(.533)
229

-.668
(1.605)
188

.909
(1.517)
161

.506
(.614)
245

.977 *
(.451)
896

-.010
(.349)
899

.340
(.210)
893

1.937
(1.089)
843

3.550
(2.397)
772

-.939
(1.600)
899

Notes: Elasticities are estimated from regressions of the following form:
ln(intensity or admissions)dt= α +ςDRGd +δyeart + ωDRG trendsdt +γaffected DRGd•postt +βln(price)dt+εdt
where the instrument for ln(price) is affected DRGd•postt•∆ln(Laspeyres price)dt.
The unit of observation is DRG-year (where "DRG" refers to single DRGs as well as to DRG pairs). All observations are
weighted by the number of admissions in the 20% MedPAR sample. The sum of the weights is 10.9 million (emergency
DRGs), .83 million (urgent DRGs), or 3.4 million (elective DRGs). Standard errors are robust.
* signifies p<.05, ** signifies p<.01, *** signifies p<.001

46

Table 9. Effects of Policy Change on Average
Hospital Prices
Dependent Variable is ln(price)
mean(price) = 1.27
Share CC•post
.233 ***
(.021)
Share CC•year dummies
1986

-.022
(.040)
-.015
(.038)
.229 ***
(.038)
.212 ***
(.039)
.174 ***
(.040)
.270 ***
(.047)

1987
1988
1989
1990
1991
Year dummies
1986

.039 ***
(.001)
.057 ***
(.001)
.063 ***
(.002)
.088 ***
(.002)
.094 ***
(.002)
.119 ***
(.002)
.890
36,651

1987
1988
1989
1990
1991
Adj. R-squared
N

.041 ***
(.004)
.058 ***
(.004)
.064 ***
(.004)
.090 ***
(.004)
.099 ***
(.004)
.116 ***
(.004)
.890
36,651

Notes: Regressions include hospital fixed effects. The unit of
observation is hospital-year. All observations are weighted by the
number of admissions in the 20 percent MedPAR sample. The sum of
the weights is 13.7 million. Share CC•post = (1987 share of a hospital's
Medicare patients who are under 70 and assigned to the top code of a
DRG pair)•(indicator variable for year>1987). Standard errors are
robust.
* signifies p<.05, ** signifies p<.01, *** signifies p<.001

47

Table 10. Real Responses to Changes in Average Hospital Price

ln(cost)
ln(LOS)
mean=$9,014 mean=8.81
Reduced Form
Share CC•post

.234 ***
(.075)

.069 *
(.034)

Dependent Variable
ln(surg)
ln(ICU) ln(death rate) ln(volume)
mean=1.21 mean=.81
mean=.06 mean=778
.067
(.104)

.684 ***
(.186)

.122
(.097)

.403 ***
(.052)

.998 ***
.296*
.291
3.457 ***
.536
(.312)
(.141)
(.445)
(.950)
(.423)
Parametric Tests of H0: IV estimate>=x; H1: IV estimate<x (p-values are reported)
x = .5
.96
.06
.31
1.0
.00
x=1
.50
.00
.04
1.0
.00

1.728 ***
(.276)

IV Estimate
ln(price)

OLS Estimate
ln(price)
N

.769 ***
(.027)
36,169

.350 ***
(.011)
36,651

.867***
(.036)
35,897

1.483 ***
(.065)
28,226

.601 ***
(.031)
34,992

1.0
1.0

-.022
(.018)
36,651

Notes: Regressions include year fixed effects and hospital fixed effects. Unlogged means are reported. The unit of observation is
hospital-year. All observations are weighted by the number of admissions in the 20% MedPAR sample. Share CC•post = (1987
share of a hospital's Medicare patients who are under 70 and assigned to the top code of a DRG pair)•(indicator variable for
year>1987). The sum of the weights is 13.7 million. For ln(death rate), the tests presented are H0: IV estimate<=x; H1: IV
estimate>x for x=-.5 and x=-1. Standard errors are robust.
* signifies p<.05, ** signifies p<.01, *** signifies p<.001

48

Appendix Table 1. Descriptive Statistics for Hospital Characteristics
Variable
Ownership
For-profit
Non-profit
Government

Mean

SD

Min

Max

.14
.58
.28

.35
.49
.45

0
0
0

1
1
1

Financial Distress Measures
Debt:asset ratio
Medicare bite
Medicaid bite

.52
.37
.11

.32
.11
.08

0
0
0

2.17
1.00
.84

Region
Northeast
Midwest
South
West

.14
.29
.38
.18

.35
.46
.49
.38

0
0
0
0

1
1
1
1

.46
.37
.17

.50
.48
.37

0
0
0

1
1
1

.06
.13
.19
10.26

.23
.34
.39
12.29

0
0
0
0

1
1
1
194

.07
.65

.05
.36

0
0

1
1

Size
1-99 beds
100-299 beds
300+ beds
Service Offerings
Teaching program
Open heart surgery
Trauma facility
ICU beds (except neonatal)
Market Concentration
HSA Herfindahl (AHA)
HSA Herfindahl (Dartmouth Atlas)

Notes: N=5,336. Excludes hospitals with missing values for any of the variables, or with
debt:asset ratios in the 1% tails of the distribution.
Sources: HCFA Cost Reports (1987), American Hospital Association Annual Survey of Hospitals
(1987)

49

