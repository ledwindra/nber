                             NBER WORKING PAPER SERIES




        A NOTE ON VARIANCE DECOMPOSITION WITH LOCAL PROJECTIONS

                                    Yuriy Gorodnichenko
                                      Byoungchan Lee

                                     Working Paper 23998
                             http://www.nber.org/papers/w23998


                    NATIONAL BUREAU OF ECONOMIC RESEARCH
                             1050 Massachusetts Avenue
                               Cambridge, MA 02138
                                  November 2017




We thank Oscar Jorda and Mikkel Plagborg-MÃ¸ller for comments on an earlier version of the
paper. The views expressed herein are those of the authors and do not necessarily reflect the
views of the National Bureau of Economic Research.Ë›

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

Â© 2017 by Yuriy Gorodnichenko and Byoungchan Lee. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including Â© notice, is given to the source.
A Note on Variance Decomposition with Local Projections
Yuriy Gorodnichenko and Byoungchan Lee
NBER Working Paper No. 23998
November 2017
JEL No. C53,E37,E47

                                       ABSTRACT

We propose and study properties of several estimators of variance decomposition in the local-
projections framework. We find for empirically relevant sample sizes that, after being bias
corrected with bootstrap, our estimators perform well in simulations. We also illustrate the
workings of our estimators empirically for monetary policy and productivity shocks.


Yuriy Gorodnichenko
Department of Economics
530 Evans Hall #3880
University of California, Berkeley
Berkeley, CA 94720-3880
and IZA
and also NBER
ygorodni@econ.berkeley.edu

Byoungchan Lee
University of California, Berkeley
Department of Economics
Berkeley, CA
bc1105@berkeley.edu
    I. Introduction
Macroeconomists have been long interested in estimating dynamic responses of output, inflation and
other aggregates to structural shocks. While many analyses use vector autoregressions (VARs) or
dynamic stochastic general equilibrium (DSGE) models to construct estimated responses, an
increasing number of researchers focus on a single structural shock and employ single-equation
methods to study the dynamic responses. This approach allows concentrating on well-identified
shocks and leaving other sources of variation unspecified. In addition, these approaches often impose
no restrictions on the shape of the impulse response function. As a result, the local projections method
(JordÃ  2005, Stock and Watson 2007) has gained prominence in applied macroeconomic research.
        The properties of impulse responses estimated with these methods are well studied (see
e.g. Coibion 2012) but little is known about how one can estimate quantitative significance of
shocks in the single-equation framework. Specifically, the vast majority of studies using single-
equation approaches do not report variance decomposition for the variable of interest and hence
one does not know if a given shock accounts for a large share of variation for the variable. 1 This
practice contrasts sharply with the nearly universal convention to report variance decompositions
in VARs and DSGE models. In this paper, we propose several methods to construct variance
decomposition in the local projection framework.
        We show that local projections lead to a simple and intuitive way to assess the contribution
of identified shocks to variation at different horizons. However, there are several options to
implement this insight. While the details of implementation do not matter in large samples, we
observe heterogeneity in the performance of various options in small, empirically relevant samples.
To illustrate the properties of various methods, we use several data generating processes which cover
main profiles of variance decompositions documented in previous works. We show that estimated
contributions to variation may be biased in small samples and one should use bootstrap to correct
for possible biases in the local projectionsâ€™ estimates of variation decompositions. We also
demonstrate how our method works in settings with multiple identified shocks. We illustrate the
performance of our method using actual data and commonly used identified shocks as well as data
simulated according to the Smets and Wouters (2007) DSGE model. Our work is concurrent and



1
 Coibion et al. (2017) is among the very few papers reporting variance decomposition in the local projection method.
More precisely, if we use definitions of Plagborg-MÃ¸ller and Wolf (2017), the object of our analysis is forecast
variance ratio.

                                                                                                                  1
complementary to Plagborg-MÃ¸ller and Wolf (2017) who provide set-identified variance
decompositions in the local projections framework.
          The rest of the paper is structured as follows. Section II lays out a basic setting to derive
the estimators. Section III presents simulation results for bivariate and multivariate settings.
Section IV provide an application of our estimators to estimate the contribution of monetary policy
and productivity shocks to variation of output and inflation in the local projections framework.
Section V concludes.

    II. Basics of variance decomposition
Consider a generic setup encountered in studies using local projections. Let ğ‘¦ğ‘¦ğ‘¡ğ‘¡ be an endogenous
variable of interest. We assume that variation in ğ‘¦ğ‘¦ğ‘¡ğ‘¡ has two components: an identified white-noise
shocks series ğ‘¥ğ‘¥ğ‘¡ğ‘¡ with mean zero and variance ğœğœğ‘¥ğ‘¥2 and the â€œrestâ€ captured by series ğ‘§ğ‘§ğ‘¡ğ‘¡ so that
          ğ‘¦ğ‘¦ğ‘¡ğ‘¡ = âˆ‘âˆ
                  ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– + ğ‘§ğ‘§ğ‘¡ğ‘¡ = ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘§ğ‘§ğ‘¡ğ‘¡ .                                                   (1)
We are interested in estimating coefficients in the lag polynomial ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿) which provides us with the
impulse response function of variable ğ‘¦ğ‘¦ğ‘¡ğ‘¡ to shock ğ‘¥ğ‘¥ğ‘¡ğ‘¡ . We make only a few assumptions about
properties of ğ‘¥ğ‘¥ğ‘¡ğ‘¡ and ğ‘§ğ‘§ğ‘¡ğ‘¡ . Specifically, we assume that ğ‘§ğ‘§ğ‘¡ğ‘¡ admits an integrated ğ‘€ğ‘€ğ‘€ğ‘€(âˆ) representation,
          Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ = ğ‘”ğ‘”ğ‘¦ğ‘¦ + ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿)ğ‘’ğ‘’ğ‘¡ğ‘¡                                                                                  (2)
where ğ‘’ğ‘’ğ‘¡ğ‘¡ is a zero-mean white noise series with variance ğœğœğ‘’ğ‘’2 . Following the conventions of local
projection applications, we assume that ğ‘¥ğ‘¥ğ‘¡ğ‘¡ and ğ‘’ğ‘’ğ‘¡ğ‘¡ are uncorrelated and that âˆ‘âˆ      2
                                                                                 ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– < âˆ and

âˆ‘âˆ      2
 ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘’ğ‘’,ğ‘–ğ‘– < âˆ . Without loss of generality we set ğœ“ğœ“ğ‘’ğ‘’,0 = 1 . We assume that {(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ ): ğ‘¡ğ‘¡ =

1, â€¦ , ğ‘‡ğ‘‡ } is observable.
          Forecast error for h-period ahead value of the endogenous variable is given by
          ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 â‰¡ ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = (ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 ) âˆ’ ğ¸ğ¸[ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |Î©ğ‘¡ğ‘¡âˆ’1 ]
where ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 â‰¡ ğ¸ğ¸[ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ |Î©ğ‘¡ğ‘¡âˆ’1 ] is the prediction of ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ given information set Î©ğ‘¡ğ‘¡âˆ’1 â‰¡
{Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’2 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’2 , â€¦ }. We can decompose forecast error due to innovations in ğ‘¥ğ‘¥ğ‘¡ğ‘¡ and other
sources of variation as follows 2
          ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğœ“ğœ“ğ‘¥ğ‘¥,0 ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ + â‹¯ + ğœ“ğœ“ğ‘¥ğ‘¥,â„ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 .                                                       (3)


2
  If ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿) is invertible, ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 is equal to ğœ“ğœ“ğ‘’ğ‘’,0 ğ‘’ğ‘’ğ‘¡ğ‘¡+â„ + â‹¯ + ï¿½ğœ“ğœ“ğ‘’ğ‘’,0 + â‹¯ + ğœ“ğœ“ğ‘’ğ‘’,â„ ï¿½ğ‘’ğ‘’ğ‘¡ğ‘¡ . This representation in ğ‘’ğ‘’ğ‘¡ğ‘¡ â€™s is
obtained, because ğ‘’ğ‘’ğ‘¡ğ‘¡ âˆˆ Î©ğ‘¡ğ‘¡ . See Appendix A for details. Note that we do not need invertibility of ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿) to construct
the contribution of ğ‘¥ğ‘¥ğ‘¡ğ‘¡ to variability in ğ‘¦ğ‘¦ğ‘¡ğ‘¡ . Intuitively, we only need an estimate of either ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ in equations
(4) and (4â€™), or ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ in equation (4â€™â€™) which does not require us separating ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿) and ğ‘’ğ‘’ğ‘¡ğ‘¡ .

                                                                                                                                      2
Following Sims (1980), we can define the population share of variance explained by the future
innovations in ğ‘¥ğ‘¥ğ‘¡ğ‘¡ to the total variations in ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 :
                ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğœ“ğœ“ğ‘¥ğ‘¥,0 ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ + â‹¯ + ğœ“ğœ“ğ‘¥ğ‘¥,â„ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ ï¿½
        ğ‘ ğ‘ â„ =                                                                                       (4)
                         ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½
                  â„      2
                ï¿½âˆ‘ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– ï¿½ğœğœğ‘¥ğ‘¥2
            =                                                                                       (4â€²)
                ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½
                            ï¿½âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–
                                      2
                                            ï¿½ğœğœğ‘¥ğ‘¥2
            =                                                     .                                  (4â€²â€² )
                ï¿½âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–
                          2
                                ï¿½ğœğœğ‘¥ğ‘¥2   + ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½
Equation (4) demonstrates that we have several options for estimating ğ‘ ğ‘ â„ and these options vary
in their reliance on imposing parametric structure. In what follows, we propose and evaluate
several methods to estimate ğ‘ ğ‘ â„ .

 A. ğ‘…ğ‘…2 method
Let ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ = (ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ , â€¦ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ )â€². It can be shown with some algebra that equation (4) can be written as
                                                            âˆ’1
              ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 , ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ï¿½ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ï¿½ ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶(ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ , ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 )
        ğ‘ ğ‘ â„ =                                                                           .                 (5)
                                         ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 )
This quantity can be understood as an ğ‘…ğ‘…2 of the population projection of ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 on ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ , or
probability limit of sample ğ‘…ğ‘…2 . This observation suggests a natural estimator for ğ‘ ğ‘ â„ . First, the
forecast errors for each horizon â„ are estimated using local projections. Second, forecast error for
horizon â„ at time ğ‘¡ğ‘¡ ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 is regressed on shocks ğ‘¥ğ‘¥ that happen between ğ‘¡ğ‘¡ and ğ‘¡ğ‘¡ + â„. The ğ‘…ğ‘…2 in
this regression is an estimate of ğ‘ ğ‘ â„ .
        More precisely, the estimated forecast error ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 is the residual of the following
regression:
                                     ğ¿ğ¿ğ‘¦ğ‘¦                  ğ¿ğ¿ğ‘¥ğ‘¥

      ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 = ğ‘ğ‘â„ +       ï¿½ ğ›¾ğ›¾ğ‘–ğ‘–â„    Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– + ï¿½ ğ›½ğ›½ğ‘–ğ‘–â„ ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– + ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ,           (6)
                                    ğ‘–ğ‘–=1                   ğ‘–ğ‘–=1
                                                                                    â„
which is an approximation to ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 = ğ‘ğ‘â„ + âˆ‘âˆ                     âˆ
                                                      ğ‘–ğ‘–=1 ğ›¾ğ›¾ğ‘–ğ‘– Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– + âˆ‘ğ‘–ğ‘–=1 ğ›½ğ›½ğ‘–ğ‘– ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– + ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 in

population. Then we run the following regression and calculate its ğ‘…ğ‘…2 :
        ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğ›¼ğ›¼ğ‘¥ğ‘¥,0 ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ + â‹¯ + ğ›¼ğ›¼ğ‘¥ğ‘¥,â„ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘£ğ‘£ï¿½ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 .                            (7)
Thus, our first estimator is ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 = ğ‘…ğ‘…2 which, by construction, is between 0 and 1. Note that ğ›¼ğ›¼ğ‘¥ğ‘¥,ğ‘–ğ‘– in
equation (7) corresponds to ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– in equation (1). Because ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 in equation (7) is a residual of
an OLS regression with the intercept in equation (6) and ğ‘¥ğ‘¥ğ‘¡ğ‘¡ is assumed to be zero mean, an
                                                                                                                3
intercept term in equation (7) is not required. Moreover, the population mean of both ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 and
ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ are zeros, so both centered and non-centered ğ‘…ğ‘…2 â€™s are the same in population. We report results
for the non-centered ğ‘…ğ‘…2 , but properties are similar when we use the centered ğ‘…ğ‘…2 . The following
proposition derives the asymptotic distribution of the estimator.


                                                                                                                   â€²
Proposition        1.     Suppose            ğ‘“ğ‘“â„ = ï¿½ğ‘“ğ‘“ğ‘‡ğ‘‡|ğ‘‡ğ‘‡âˆ’â„âˆ’1 , ğ‘“ğ‘“ğ‘‡ğ‘‡âˆ’1|ğ‘‡ğ‘‡âˆ’â„âˆ’2 , â€¦ , ğ‘“ğ‘“ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š+â„+1 |ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ï¿½       and   ğ‘‹ğ‘‹â„ =
   â„        â„                    â€²
ï¿½ğ‘‹ğ‘‹ğ‘‡ğ‘‡âˆ’â„ , ğ‘‹ğ‘‹ğ‘‡ğ‘‡âˆ’1 , â€¦ , ğ‘‹ğ‘‹ğ¿ğ¿â„ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š+1 ï¿½ for all â„ â‰¥ 0 where ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š = maxï¿½ğ¿ğ¿ğ‘¥ğ‘¥ , ğ¿ğ¿ğ‘¦ğ‘¦ ï¿½ . Then the ğ‘…ğ‘…2 of the
regression of ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 on ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ , given by ï¿½ğ‘“ğ‘“â„â€² ğ‘ƒğ‘ƒğ‘‹ğ‘‹â„ ğ‘“ğ‘“â„ ï¿½/(ğ‘“ğ‘“â„â€² ğ‘“ğ‘“â„ ) where ğ‘ƒğ‘ƒğ‘‹ğ‘‹â„ = ğ‘‹ğ‘‹â„ (ğ‘‹ğ‘‹â„â€² ğ‘‹ğ‘‹â„ )âˆ’1 ğ‘‹ğ‘‹â„â€² , has the
following asymptotic distribution for some ğ‘‰ğ‘‰â„,ğ‘…ğ‘…2 :

                                              ğ‘“ğ‘“â„â€² ğ‘ƒğ‘ƒğ‘‹ğ‘‹â„ ğ‘“ğ‘“â„         ğ‘‘ğ‘‘
                                     âˆšğ‘‡ğ‘‡ ï¿½           â€²       âˆ’ ğ‘ ğ‘ â„ ï¿½ â†’ ğ’©ğ’©ï¿½0,        ğ‘‰ğ‘‰â„,ğ‘…ğ‘…2 ï¿½.
                                                 ğ‘“ğ‘“â„ ğ‘“ğ‘“â„

Proof. See Appendix B1.

In practice, we may plug the estimated forecast errors from equation (6) in the place of ğ‘“ğ‘“â„ .
Appendix B1 contains details of implementation. Note that, instead of using shocks ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , â€¦ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ in
equation (7), one may want to use residuals from projecting ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , â€¦ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ on lags of ğ‘¥ğ‘¥ğ‘¡ğ‘¡ and Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ from
equation (6) to guarantee that one does not use forecastable movements in ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , â€¦ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ to account
for variation in ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 . In practice, however, shocks ğ‘¥ğ‘¥ğ‘¡ğ‘¡ are constructed in ways to ensure that ğ‘¥ğ‘¥ğ‘¡ğ‘¡
is not predictable by lags of macroeconomic variables. As a result, we find in our simulations and
applications that purifying structural shocks make little difference. Relatedly, one may implement
this estimator by augmenting equation (6) with shocks ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , â€¦ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ and calculating partial R2. This
insight also justifies using ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 instead of ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 in Proposition 1.

 B. Local projection based methods
The ğ‘…ğ‘…2 approach requires estimation of two regressions for each horizon (first, construct forecast
errors; second, compute the contribution of shocks ğ‘¥ğ‘¥ between ğ‘¡ğ‘¡ and ğ‘¡ğ‘¡ + â„). However, one can
estimate variance decomposition from the local projection directly. Following JordÃ  (2005), we
can estimate ğœ“ğœ“ğ‘¥ğ‘¥,â„ from the following equation:
                                      ğ¿ğ¿ğ‘¦ğ‘¦                        ğ¿ğ¿ğ‘¥ğ‘¥

         ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 = ğ‘ğ‘â„LP + ï¿½ ğ›¾ğ›¾ğ‘–ğ‘–â„,ğ¿ğ¿ğ¿ğ¿ Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– + ï¿½ ğ›½ğ›½ğ‘–ğ‘–â„,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– + ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1                          (8)
                                     ğ‘–ğ‘–=1                        ğ‘–ğ‘–=0

                                                                                                                                4
where ğ›½ğ›½Ì‚0â„,ğ¿ğ¿ğ¿ğ¿ is an estimate of ğœ“ğœ“ğ‘¥ğ‘¥,â„ . Since we can estimate ğœğœğ‘¥ğ‘¥2 directly from ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , we can calculate
ï¿½âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–
          2
                ï¿½ğœğœğ‘¥ğ‘¥2 in the numerator of equation (4â€™). To compute the denominator in equation (4â€™), we
note that the residual in equation (8) can be related to the forecast error ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 in equation (6).

For example, ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡|ğ‘¡ğ‘¡âˆ’1 = ğ›½ğ›½Ì‚00,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘Ÿğ‘ŸÌ‚ğ‘¡ğ‘¡|ğ‘¡ğ‘¡âˆ’1 , that is, a part of forecast error ğ‘“ğ‘“ğ‘¡ğ‘¡|ğ‘¡ğ‘¡âˆ’1 is explained by shock
ğ‘¥ğ‘¥ happening at time ğ‘¡ğ‘¡ which is now included as one of the regressors in equation (8). In a similar
spirit, we can use equation (3) to compute ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğ›½ğ›½Ì‚0â„,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘Ÿğ‘ŸÌ‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 . With these estimates of
                               ï¿½ (ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ) where ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 , we can compute ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰                    ï¿½ (â‹…) denotes a sample variance. Using these

insights, we define a local projection estimator of variance decomposition â€œLPAâ€ as
                                                 2
                            ï¿½âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2
          ğ‘ ğ‘ â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ =                                                                                               (9)
                         ï¿½ ï¿½ğ›½ğ›½Ì‚0â„,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘Ÿğ‘ŸÌ‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½
                        ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
                ï¿½ (ğ‘¥ğ‘¥ğ‘¡ğ‘¡ ).
where ğœğœï¿½ğ‘¥ğ‘¥2 â‰¡ ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
          Although simple, LPA estimator does not guarantee that in small samples the estimated ğ‘ ğ‘ â„
is between 0 and 1. A simple solution to this issue is to split the denominator into variation due to
ğ‘¥ğ‘¥ and due to ğ‘£ğ‘£ so that ï¿½âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–
                                   2
                                         ï¿½ğœğœğ‘¥ğ‘¥2 appears in both the numerator and denominator as in equation
(4â€™â€™). Note that
          ğ‘£ğ‘£ï¿½ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ›½ğ›½Ì‚0â„,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ âˆ’ ğ›½ğ›½Ì‚0â„âˆ’1,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+1 âˆ’ â‹¯ âˆ’ ğ›½ğ›½Ì‚00,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„

                          = ğ‘Ÿğ‘ŸÌ‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ›½ğ›½Ì‚0â„âˆ’1,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+1 âˆ’ â‹¯ âˆ’ ğ›½ğ›½Ì‚00,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„
so that
                                                                                                                              2
                               ï¿½ ï¿½ğ‘Ÿğ‘ŸÌ‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ›½ğ›½Ì‚0â„âˆ’1,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+1 âˆ’ â‹¯ âˆ’ ğ›½ğ›½Ì‚00,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ ï¿½ + âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2
           ï¿½ ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ = ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
          ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
which we use to define another local projection estimator of variance decomposition â€œLPBâ€:
                                                                                    2
                                                               ï¿½âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2
           ğ‘ ğ‘ â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ =                      2                                                                             .       (9â€²)
                         âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2 + ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
                                                        ï¿½ ï¿½ğ‘Ÿğ‘ŸÌ‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ›½ğ›½Ì‚0â„âˆ’1,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+1 âˆ’ â‹¯ âˆ’ ğ›½ğ›½Ì‚00,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ ï¿½

Using tools from Proposition 1, we can derive the asymptotic distribution of the LPA and LPB
estimators.


Proposition 2. The local projections based estimators when ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 is observable have the
following asymptotic distributions for some ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ :




                                                                                                                                         5
                                                        2
                                   âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2         ğ‘‘ğ‘‘
                             âˆšğ‘‡ğ‘‡ ï¿½                             âˆ’ ğ‘ ğ‘ â„ ï¿½ â†’ ğ’©ğ’©ï¿½0, ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½,   and
                                    ï¿½ (ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 )
                                   ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
                                                            2
                                        âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2                             ğ‘‘ğ‘‘
       âˆšğ‘‡ğ‘‡ ï¿½                    2                                                       âˆ’ ğ‘ ğ‘ â„ ï¿½ â†’ ğ’©ğ’©ï¿½0,       ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½.
            âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2 + ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
                                           ï¿½ ï¿½ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ âˆ‘â„âˆ’1  Ì‚ ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘– ï¿½
                                                                 ğ›½ğ›½
                                                             ğ‘–ğ‘–=0 0

Proof. See Appendix B2.

    C. Small-sample refinements
To correct for potential small-sample biases in the estimates of ğ‘ ğ‘ â„ and to enhance coverage rates for
confidence bands, we bootstrap ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 , ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ , and ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ using an estimated VAR model which includes
two variables {ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ }. While our implementation of bootstrap is aimed to remove potential biases,
alternative implementations may also refine asymptotic inference. Details on how bootstrap is
implemented are relegated to Appendix E.

    D. Extension
While our analysis has focused on the bivariate case, the framework can be easily generalized to
include more controls in equation (6):
                               ğ¿ğ¿ğ‘¥ğ‘¥                  ğ¿ğ¿ğ¶ğ¶

         ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 =    ï¿½ ğ›½ğ›½ğ‘–ğ‘–â„                  â€²
                                        ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– + ï¿½ ğ¶ğ¶ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– ğ›¤ğ›¤ğ‘–ğ‘–â„ + ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1                                           (10)
                              ğ‘–ğ‘–=1                  ğ‘–ğ‘–=1

where ğ¶ğ¶ğ‘¡ğ‘¡ is the vector of control variables which may include structural shocks other than ğ‘¥ğ‘¥ğ‘¡ğ‘¡ . In the
base case, ğ¶ğ¶ğ‘¡ğ‘¡ consists only of Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ . Note that for VAR-based bootstrap, one has to include ğ‘¥ğ‘¥ğ‘¡ğ‘¡ and all
variables in ğ¶ğ¶ğ‘¡ğ‘¡ to simulate data. 3 Similar adjustments are also possible for LPA and LPB methods.
         One should bear in mind that, although including or excluding ğ¶ğ¶ğ‘¡ğ‘¡ or changing the
composition of variables in ğ¶ğ¶ğ‘¡ğ‘¡ should make little difference of impulse responses estimated with
local projections (provided ğ‘¥ğ‘¥ is uncorrelated with other shocks), what goes in ğ¶ğ¶ğ‘¡ğ‘¡ is potentially
important for variance decomposition. Intuitively, by including more controls in ğ¶ğ¶ğ‘¡ğ‘¡ , we (weakly)
reduce the size of the forecast error (that is, information set Î©ğ‘¡ğ‘¡ expands) and hence the amount of
variation to be explained shrinks. In other words, the regressand in equation (7) and therefore ğ‘ ğ‘ â„


3
  As the number of variables in ğ¶ğ¶ğ‘¡ğ‘¡ increases, the number of parameters in the VAR increases rapidly. When ğ¶ğ¶ğ‘¡ğ‘¡ is a large
vector, or when a VAR is not a good representation of the DGP for control variables, VAR-based bootstrap might not be
an appealing option. In such a case, one may correct for biases by simulating asymptotic distribution of primitive quantities
in (4) such as ğœ“ğœ“ï¿½ğ‘¥ğ‘¥,ğ‘–ğ‘– , ğœğœï¿½ğ‘¥ğ‘¥2 , and ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
                                        ï¿½ ï¿½ğ‘£ğ‘£ï¿½ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½. By considering ğ‘ ğ‘ â„ as a non-linear function of those parameters, such
simulations would detect biases due to the non-linearity. See Appendix B for implementation and E and F for the results.

                                                                                                                              6
change with a change in the list of variables included in ğ¶ğ¶ğ‘¡ğ‘¡ . Thus, one should not be surprised to
observe that the share of variation explained by ğ‘¥ğ‘¥ may be sensitive to changes in ğ¶ğ¶ğ‘¡ğ‘¡ .

III. Simulations
This section presents two sets of simulations. The first set shows results for the baseline bivariate
case and studies the performances of the three estimators for various profiles of contribution of ğ‘¥ğ‘¥
to variance of ğ‘¦ğ‘¦ at different horizons. The second set uses the estimated Smets and Wouters (2007)
model to investigate the performance in a setting with many control variables.
       For each data generating process (DGP), we simulate data 2,000 times. When we employ
bootstrap to correct for biases, the number of bootstrap replications is set to B=2,000. As a
benchmark, we report results based on a corresponding VAR. This benchmark corresponds to the
practice of including shocks into VARs directly (e.g., Basu et al. 2006, Ramey 2011, Barakchian
and Crowe 2013, Romer and Romer 2004, 2010). We choose the Hannan-Quinn information
criterion (HQIC) as our benchmark criterion to determine the number of lags in VAR. To make
VAR and LP models comparable, we use HQIC number of lags in the VAR to set ğ¿ğ¿ğ‘¥ğ‘¥ and ğ¿ğ¿ğ‘¦ğ‘¦ .
Results are similar when we use AIC instead of HQIC.
       The sample size for simulated data is ğ‘‡ğ‘‡ = 160. Results for other sample sizes are reported
in Appendices E and F. Standard errors are computed as the standard deviation of estimates across
                                                                                          ğ‘ ğ‘ Ì‚ â„ âˆ’ğ‘ ğ‘ â„
bootstrapped samples. The coverage rates are calculated as Pr ï¿½ï¿½                                          ï¿½ï¿½ â‰¤ 1.65.
                                                                                         ğ‘ ğ‘ .ğ‘’ğ‘’.(ğ‘ ğ‘ Ì‚ â„ )


   A. Bivariate Data Generating Processes
We study three DGPs to cover different shapes of ğ‘ ğ‘ â„ . The basic structure is as follows:
       ğ‘¦ğ‘¦ğ‘¡ğ‘¡ = ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘§ğ‘§ğ‘¡ğ‘¡
       ğ‘§ğ‘§ğ‘¡ğ‘¡ = ğ‘ğ‘ğ‘¡ğ‘¡ + ğ‘ğ‘ğ‘¡ğ‘¡ ,
                                                        ğ‘ğ‘             ğ‘ğ‘
       ï¿½Î”ğ‘ğ‘ğ‘¡ğ‘¡ âˆ’ ğ‘”ğ‘”ğ‘¦ğ‘¦ ï¿½ = ğœŒğœŒğ‘ğ‘ ï¿½Î”ğ‘ğ‘ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ‘”ğ‘”ğ‘¦ğ‘¦ ï¿½ + ğ‘’ğ‘’ğ‘¡ğ‘¡ ,            ğ‘’ğ‘’ğ‘¡ğ‘¡ ~ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– ğ‘ğ‘ï¿½0, ğœğœğ‘ğ‘2 ï¿½,
       ğ‘ğ‘ğ‘¡ğ‘¡ = ğœŒğœŒğ‘ğ‘ ğ‘ğ‘ğ‘¡ğ‘¡âˆ’1 + ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ , ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ ~ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– ğ‘ğ‘(0, ğœğœğ‘ğ‘2 ),
                              2 ),
       ğ‘¥ğ‘¥ğ‘¡ğ‘¡ ~ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– ğ‘ğ‘(0, ğœğœğ‘¥ğ‘¥
             ğ‘ğ‘
where ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , ğ‘’ğ‘’ğ‘¡ğ‘¡ and ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ are mutually independent, ğ‘ğ‘ğ‘¡ğ‘¡ and ğ‘ğ‘ğ‘¡ğ‘¡ are permanent and transitory
components of ğ‘§ğ‘§ğ‘¡ğ‘¡ . Appendix C derives the population ğ‘€ğ‘€ğ‘€ğ‘€(âˆ) representation of Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ .




                                                                                                                       7
         DGP1 is characterized by hump-shaped ğœ“ğœ“ğ‘¥ğ‘¥ and ğ‘ ğ‘ â„ . We assume that ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡ follows an
ğ‘€ğ‘€ğ‘€ğ‘€(100) process with the maximum response set to 3 after 8 periods. 4 DGP2 has a strong
response of ğ‘¦ğ‘¦ to ğ‘¥ğ‘¥ only in the short-run and thus the shape of ğ‘ ğ‘ â„ is downward-sloping. Finally,
DGP3 assumes that ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿) has a unit root so that ğ‘¥ğ‘¥ has persistent effects on ğ‘¦ğ‘¦ and the shape of ğ‘ ğ‘ â„
is upward-sloping. Table 1 reports parameter values for each DGP. Figure 1 plots true impulse
responses of ğ‘¦ğ‘¦ to ğ‘¥ğ‘¥ (Panel A) and the contribution of ğ‘¥ğ‘¥ to variation in ğ‘¦ğ‘¦ (Panel B).
         For DGP1, we find (Table 2) that local projections capture the hump-shaped impulse
response correctly but ğ‘ ğ‘  ğ‘…ğ‘…2 , ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ fail to match the hump-share dynamics of ğ‘ ğ‘ â„ . ğ‘ ğ‘  ğ‘…ğ‘…2 , ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
and ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ tend to monotonically increase with the horizon. The VAR misses the hump both in the
impulse response and variance decomposition as HQIC selects too few lags (on average the
number of lags is 1.27). Confidence bands yield poor coverage rates. This performance reflects
the fact that, by construction, shock ğ‘¥ğ‘¥ contributes zero variation in ğ‘¦ğ‘¦ for this DGP at short
horizons. Since ğ‘ ğ‘ â„ is between zero and one, we effectively have estimates close to the boundary
and, therefore, standard methods are likely to fail. While bootstrap appears to provide some
improvement (e.g., the bias at long horizons when ğ‘¥ğ‘¥ accounts for a larger share of variance in ğ‘¦ğ‘¦ is
corrected) 5, it does not perform consistently better because the parameter is at the boundary. When
we allow ğ‘¥ğ‘¥ to explain 5 percent or more of the variation in ğ‘¦ğ‘¦ at short horizons, bootstrap brings
coverage rates close to nominal (results are available upon request). Note that, although VAR is
strongly biased, the VAR estimates tend to have smaller variance so that the root mean squared
error (RMSE) is similar in magnitude to RMSE of the ğ‘ ğ‘  ğ‘…ğ‘…2 , ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimators. Finally, we
observe that the ğ‘ ğ‘  ğ‘…ğ‘…2 , ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimators have similar performance.
         Because DGP2 permits an exact, finite-order VAR representation, 6 VAR has good
properties in terms of bias, RMSE and coverage rates (Table 3). The local projections recover the
share of the impulse response correct, but the estimates of contribution of ğ‘¥ğ‘¥ to variance of ğ‘¦ğ‘¦ again
overstate the contribution at long horizons. Bootstrap can correct this bias. Given that VAR nests




4
  This value and pattern is motivated by a 3 percent response of real GDP to a 100bp monetary policy shock estimated
in Coibion (2012).
5
  The bias can be further reduced by using higher values of ğ¿ğ¿ğ‘¥ğ‘¥ and ğ¿ğ¿ğ‘¦ğ‘¦ by reducing errors in ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 due to the
truncation.
                                                                                                      ğ‘ğ‘
6
  Given the parameter values in Table 1, Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ = ğ‘”ğ‘”ğ‘¦ğ‘¦ + (1 âˆ’ ğ¿ğ¿)(1 âˆ’ 0.9ğ¿ğ¿)âˆ’1 ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + (1 âˆ’ 0.9ğ¿ğ¿)âˆ’1 ğ‘’ğ‘’ğ‘¡ğ‘¡ . By pre-multiplying
                                                                       ğ‘ğ‘
(1 âˆ’ 0.9ğ¿ğ¿), we have Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ = 0.1ğ‘”ğ‘”ğ‘¦ğ‘¦ + 0.9Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 + ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘’ğ‘’ğ‘¡ğ‘¡ .

                                                                                                                          8
the DGP and that VAR is more parsimonious than local projections, VAR has a better performance
than the ğ‘ ğ‘  ğ‘…ğ‘…2 , ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimators.
          Because ğ‘¥ğ‘¥ has long-lasting effects on ğ‘¦ğ‘¦ in DGP3, the VAR underestimates the responses at
long horizons in small samples. Impulse responses estimated with local projections perform better
but also exhibit a downward bias at long horizons. In a similar spirit, ğ‘ ğ‘ Ì‚â„ shows a strong downward
bias for VAR and a smaller, but still considerable bias for the ğ‘ ğ‘  ğ‘…ğ‘…2 , ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimators (this is
the case even after we use bootstrap to correct for possible biases). This performance reflects the fact
that HQIC chooses a low number of lags (1.34 lags on average across simulations). As a result,
VARs used to simulate bootstrap samples fail to capture the degree of persistence in the data. To
demonstrate the importance of the lag order, we report results (Table 4) when we use VAR(5) and
VAR(10) for bootstrap. As the number of lags increases, we observe some improvement but these
enhancements are achieved at the price of higher variance in the estimates. These results suggest that
one may want to overfit VAR for persistent processes at the bootstrap stage.
          In summary, we find for small samples that the ğ‘ ğ‘  ğ‘…ğ‘…2 , ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimators perform
reasonably well across the DGPs and that bootstrap helps to improve the estimatorsâ€™ properties. In
addition, there is relatively little difference between the ğ‘ ğ‘  ğ‘…ğ‘…2 , ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘ ğ‘  ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimators. In contrast,
VARs that include structural shock ğ‘¥ğ‘¥ tend to perform poorly when a DGP is not nested in a small-
order VAR.

     B. Smets-Wouters model
While the bivariate DGPs provide important insights about how the ğ‘…ğ‘…2 , ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimators
perform, researchers face potentially more complex DGPs and often have more information in
practice. In this section, we use the Smets and Wouters (2007) model to study performance of our
estimators in an environment with multiple shocks and many control variables.
          As discussed above, different information sets determine different population ğ‘ ğ‘ â„ . In the
simulations, we assume that the researcher is interested in explaining variation in output and that
the researcher observes output growth rate, inflation, federal funds rate, and monetary policy
shocks. 7 This choice of variables is motivated by the popularity of small VARs which include


7
  For this information set, we construct the true variance decomposition using a stationary Kalman filter similar to the
method in Appendix C. We also tried various combinations of shocks and endogenous variables and found similar
results. Figures for inflation and results with large samples are in Appendix F. Note that monetary policy shocks are
nearly invertible in the Smets-Wouters model (see Wolf (2017) for more details). While this may be a problem if we

                                                                                                                      9
output, inflation and a policy rate to study effects of monetary policy on the economy. In this
exercise, the shock is ordered first because the Smets-Wouters model allows contemporaneous
responses of macroeconomic variables to policy shocks. When estimating impulse responses using
local projections, we augment equation (8) with inflation and federal funds rate as controls.
         We find (Figure 2) that local projections correctly recover the response of output to
monetary policy shocks, while a low order VAR (lag length is chosen with HQIC) fails to capture
the transitory effect of monetary shocks on output. Consistent with our bivariate analysis, ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 ,
ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ increase with the horizons while the true ğ‘ ğ‘ â„ exhibits hump-shaped dynamics. ğ‘ ğ‘ â„
estimated with a VAR also fails to capture the true dynamics as ğ‘ ğ‘ Ì‚â„ flattens out after about â„ = 5.
Similar to our results in the previous section, we find that bias correction helps ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 , ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
to recover the true hump-shaped profile of ğ‘ ğ‘ â„ . Coverage rates (after bias correction) are 10
percentage points lower their nominal values at short horizons (â„ â‰¤ 5) but the coverage rates are
close to nominal at longer horizons. Again, although VAR estimates of ğ‘ ğ‘ â„ are strongly biased, the
variance of estimates is low so that RMSE is broadly similar cross methods. We conclude that our
proposed methods to estimate variance decomposition work well in more complex settings.

IV. Application
To illustrate the properties our estimators, we use two structural shocks identified in the literature.
The first shock is the monetary policy innovation identified as in Romer and Romer (2004) and
extended in Coibion et al. (2017). The second shock is the total factor productivity (TFP) shock
identified as in Fernald (2014). 8 The correlation between the shocks is -0.059. Our objective is to
quantify the contribution of these shocks to variation of output and inflation. The sample covers
1969Q1-2007Q4 which excludes the period of binding zero lower bound. The set of variables for
local projections includes inflation (annualized growth rate of GDP deflator, i.e. 400Î”ln(ğ‘ƒğ‘ƒğ‘¡ğ‘¡ )),
annual GDP growth rate (400Î” ln(ğ‘Œğ‘Œğ‘¡ğ‘¡ )), federal funds rate, and the two-shock series. We set ğ¿ğ¿ğ¶ğ¶ =
ğ¿ğ¿ğ‘¥ğ‘¥ = 4 in equation (10) and add control variables similarly when estimating impulse responses. In
the benchmark VAR, we have all five variables and allow four lags. 9

use shocks identified and recovered from a DSGE model, the spirit of our exercise is to assume that we have access
to other information (as in e.g. Romer and Romer (2004)) so that we can observe monetary policy shocks directly.
8
  When we use empirically identified shocks, measurement errors might be an issue. Given measurement errors, we
show that asymptotic biases of our estimators are negative in Appendix D. Therefore, results here can be understood
as conservative estimates. In addition, shocks are often estimated and thus are generated regressors, but if the
researcher is interested in testing the null of no response then there is no need to adjust inference (Pagan 1984).
9
  The ordering of shocks in the VAR is TFP shock, output growth rate, inflation, monetary policy shock, fed funds rate.

                                                                                                                    10
          Consistent with previous studies, we find (Figures 3 and 4) that a contractionary monetary
policy shocks lowers output and prices, and that a positive TFP shock raises output and lowers prices.
Impulse responses estimated with VAR and local projections are similar. VAR estimates for variance
decomposition suggest that each of the shocks accounts for approximately 10 percent of variation in
output. According to the VAR estimates, monetary policy shocks account for approximately 25
percent of variation in inflation at long horizons and little variation at short horizons while the
contribution of TFP shocks is generally small. Bias correction makes no material difference for the
variance decomposition estimates for all cases but one: the bias-corrected estimate of the contribution
of monetary policy shocks to variation of inflation at long horizons is reduced to about 10 percent.
          Local projections estimate that the contribution of the two shocks to variation of output is
approximately twice as large as the contribution in VAR estimates. Consistent with simulations,
bias correction tends to generate lower contributions but generally the magnitudes are similar.
Specifically, when we use the ğ‘ ğ‘  ğ‘…ğ‘…2 estimator, monetary policy shocks account for approximately
20 percent of variation in output according to local projection estimates (25 percent without bias
correction) and approximately 10 percent according to VAR estimates. While the ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimator
yields similar results, the ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimator assigns a much larger role to the monetary policy shocks.
This pattern reflects the fact that ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ may be greater than 1 in finite samples. Also, note that, in
contrast to the profile of ğ‘ ğ‘ â„ estimated with VAR for output (which is generally flat after â„ = 5),
ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 , ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ´ğ´ and ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ have richer dynamics.
          In a similar spirit, the contribution of TFP and monetary policy shocks to variation in
inflation is much greater according to our local-projections estimates. The difference is particularly
large for monetary shocks: ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 and ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ are close to 40 percent (after bias correction) and ğ‘ ğ‘ Ì‚â„ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ is
about 10 percent at long horizons. Again, ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ estimates an even greater contribution of monetary
shocks and confidence intervals are much wider for ğ‘ ğ‘ Ì‚ ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ than for ğ‘ ğ‘ Ì‚ ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ or ğ‘ ğ‘ Ì‚ ğ‘…ğ‘…2 . Again, this stems
from the fact that ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ may be greater than 1 in finite samples.


V. Concluding remarks
Single-equation methods can offer flexibility and parsimony that many economists seek. The
increasing popularity of these methods, specifically the local projections, calls for further
development of these tools. An important limitation for practitioners using this framework has
been a lack of simple tools to assess quantitative significance of a given set of shocks, that is, the


                                                                                                                11
contribution of the shocks to variance of the variable of interest. We propose several methods to
provide such a metric. In a series of simulation exercises, we document that these methods have
good small-sample properties. We also show that conventional approaches to assess the
quantitative significance of two popular structural shocks (monetary policy shocks and total factor
productivity shocks) could have been understated the importance of these two shocks.


References
Barakchian, S. Mahdi, and Christopher Crowe, 2013. â€œMonetary policy matters: Evidence from
       new shocks data,â€ Journal of Monetary Economics, 60(8): 950-966.
Basu, Susanto, John G. Fernald, and Miles S. Kimball, 2006. â€œAre Technology Improvements
       Contractionary?â€ American Economic Review, 96(5): 1418â€“1448.
Coibion, Olivier, 2012. â€œAre the Effects of Monetary Policy Shocks Big or Small?â€ American
       Economic Journal: Macroeconomics, 4(2): 1â€“32.
Coibion, Olivier, Yuriy Gorodnichenko, Lorenz Kueng, and John Silvia, 2017. â€œInnocent Bystanders?
       Monetary policy and inequality,â€ Journal of Monetary Economics, 88(C): 70â€“89.
Fernald, John, 2014. â€œA Quarterly, Utilization-Adjusted Series on Total Factor Productivity,â€
       Working Paper 2012-19.
JordÃ , Oscar, 2005. â€œEstimation and Inference of Impulse Responses by Local Projections,â€
       American Economic Review, 95(1): 161â€“182.
Pagan, Adrian, 1984. â€œEconometric Issues in the Analysis of Regressions with Generated
       Regressors,â€ International Economic Review, 25(1): 221-247.
Plagborg-MÃ¸ller, Mikkel, and Christian K. Wolf, 2017. â€œInstrumental Variable Identification of
       Dynamic Variance Decompositions,â€ manuscript.
Ramey, Valerie A., 2011. â€œIdentifying Government Spending Shocks: It's all in the Timing,â€
       Quarterly Journal of Economics, 126(1): 1â€“50.
Romer, Christina D., and David H. Romer, 2010. â€œThe Macroeconomic Effects of Tax Changes:
       Estimates Based on a New Measure of Fiscal Shocks,â€ American Economic Review,
       100(3): 763-801.
Romer, Christina, D., and David H. Romer, 2004. â€œA New Measure of Monetary Shocks:
       Derivation and Implications,â€ American Economic Review, 94(4): 1055-1084.
Sims, Christopher A., 1980. â€œMacroeconomics and reality.â€ Econometrica, 48(1): 1-48.
Smets, Frank, and Rafael Wouters, 2007. â€œShocks and frictions in US business cycles: A Bayesian
       DSGE approach,â€ American Economic Review, 97(3): 586-606.
Stock, James, and Mark Watson, 2007. â€œWhy Has U.S. Inflation Become Harder to Forecast?â€
       Journal of Money, Banking and Credit, 39(1): 3â€“33.
Wolf, Christian K., 2017. â€œMasquerading Shocks in Sign-Restricted VARs,â€ manuscript.




                                                                                                12
       Table 1. Parameter values for data generating processes (DGPs) used in simulations.

                     ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿)            ğœğœğ‘¥ğ‘¥      ğ‘”ğ‘”ğ‘¦ğ‘¦      ğœŒğœŒğ‘ğ‘       ğœğœğ‘ğ‘      ğœŒğœŒğ‘ğ‘      ğœğœğ‘ğ‘
DGP1             Hump-shaped               1        0.5       0.9       0.5        0.9        3
DGP2             (1 âˆ’ 0.9L)âˆ’1              3        0.5       0.9       1.5         -         -
DGP3        (1 âˆ’ L)âˆ’1 (1 âˆ’ 0.9L)âˆ’1         1        0.5       0.5         2        0.9        3




                                                                                                    13
                                        Table 2. Simulation results for DGP 1.

                                                                                 Horizon â„
                                                      0          4          8            12   16     20

Impulse response
   True                                             0.00       1.39       3.00         2.06   0.88   0.29
   Local projections                                0.00       1.36       2.99         2.03   0.85   0.29
   VAR(HQIC)                                        0.00       0.18       0.26         0.27   0.27   0.27

Variance decomposition
   True                                             0.00       0.04       0.19         0.21   0.18   0.14
   Average estimate
      R2                                            0.01       0.06       0.20         0.25   0.26   0.27
      LP A                                          0.01       0.04       0.18         0.23   0.23   0.23
      LP B                                          0.01       0.04       0.17         0.22   0.21   0.21
      VAR(HQIC)                                     0.01       0.02       0.02         0.03   0.03   0.03

   Root mean squared error
      R2                                            0.01       0.05       0.11         0.16   0.19   0.22
      LP A                                          0.01       0.04       0.11         0.15   0.18   0.20
      LP B                                          0.01       0.04       0.10         0.14   0.15   0.15
      VAR(HQIC)                                     0.01       0.03       0.17         0.20   0.16   0.14

   Coverage (90 % level) (asymptotic)
      R2                                            1.00       0.94       0.74         0.71   0.75   0.73
      LP A                                          1.00       0.94       0.53         0.57   0.74   0.80
      LP B                                          1.00       0.93       0.53         0.55   0.70   0.78
      VAR(HQIC)                                     1.00       0.57       0.06         0.05   0.07   0.09

Variance decomposition (bias corrected, VAR(HQIC))
   True                                       0.00             0.04       0.19         0.21   0.18   0.14
   Average estimate
      R2                                      0.00             0.02       0.13         0.16   0.13   0.11
      LP A                                    0.00             0.02       0.14         0.17   0.16   0.14
      LP B                                    0.00             0.02       0.14         0.17   0.15   0.13
      VAR(HQIC)                               0.00             0.00       0.01         0.02   0.02   0.02

   Root mean squared error
      R2                                            0.01       0.05       0.13         0.16   0.17   0.18
      LP A                                          0.01       0.04       0.12         0.16   0.17   0.18
      LP B                                          0.01       0.04       0.12         0.14   0.15   0.15
      VAR(HQIC)                                     0.01       0.04       0.19         0.21   0.18   0.15

   Coverage (90 % level)
      R2                                            1.00       0.93       0.59         0.61   0.67   0.79
      LP A                                          1.00       0.82       0.45         0.49   0.59   0.79
      LP B                                          1.00       0.81       0.46         0.47   0.57   0.73
      VAR(HQIC)                                     1.00       0.41       0.05         0.05   0.06   0.08




                                                                                                            14
                                        Table 3. Simulation results for DGP 2

                                                                                Horizon â„
                                                     0          4          8            12   16     20

Impulse response
   True                                             3.00       1.97       1.29        0.85   0.56   0.36
   Local projections                                2.99       1.88       1.18        0.71   0.43   0.22
   VAR(HQIC)                                        2.96       1.98       1.40        1.04   0.82   0.67

Variance decomposition
   True                                             0.80       0.25       0.10        0.05   0.03   0.02
   Average estimate
      R2                                            0.79       0.27       0.15        0.14   0.15   0.18
      LP A                                          0.80       0.27       0.13        0.10   0.09   0.09
      LP B                                          0.79       0.26       0.13        0.09   0.09   0.09
      VAR(HQIC)                                     0.80       0.27       0.13        0.08   0.06   0.05

   Root mean squared error
      R2                                            0.03       0.11       0.12        0.14   0.17   0.21
      LP A                                          0.03       0.09       0.08        0.08   0.09   0.11
      LP B                                          0.03       0.08       0.07        0.08   0.09   0.10
      VAR(HQIC)                                     0.03       0.08       0.07        0.06   0.05   0.05

   Coverage (90 % level) (asymptotic)
      R2                                            0.92       0.87       0.92        0.89   0.85   0.80
      LP A                                          0.93       0.90       0.94        0.95   0.93   0.93
      LP B                                          0.90       0.88       0.93        0.94   0.92   0.90
      VAR(HQIC)                                     0.90       0.88       0.91        0.97   0.99   0.99

Variance decomposition (bias corrected, VAR(HQIC))
   True                                       0.80             0.25       0.10        0.05   0.03   0.02
   Average estimate
      R2                                      0.81             0.25       0.09        0.04   0.01   0.00
      LP A                                    0.79             0.25       0.10        0.05   0.03   0.02
      LP B                                    0.81             0.25       0.10        0.05   0.03   0.02
      VAR(HQIC)                               0.80             0.25       0.10        0.05   0.03   0.02

   Root mean squared error
      R2                                            0.03       0.10       0.09        0.09   0.11   0.13
      LP A                                          0.03       0.08       0.07        0.07   0.07   0.08
      LP B                                          0.03       0.08       0.07        0.07   0.07   0.08
      VAR(HQIC)                                     0.03       0.08       0.06        0.05   0.04   0.04

   Coverage (90 % level)
      R2                                            0.91       0.90       0.97        0.98   0.98   0.97
      LP A                                          0.93       0.88       0.91        0.98   0.97   0.97
      LP B                                          0.89       0.83       0.89        0.97   0.97   0.96
      VAR(HQIC)                                     0.89       0.88       0.89        0.92   0.99   1.00




                                                                                                           15
                       Table 4. Simulation results for DGP 3 with alternative lag orders in VARs.

                                                                                   Horizon â„
                                                         0          4          8           12       16     20

Impulse response
   True                                                 1.00       4.10       6.13       7.46       8.33   8.91
   Local projections                                    0.99       3.96       5.78       6.86       7.44   7.66
   VAR(5)                                               0.93       3.74       4.76       5.01       5.10   5.14
   VAR(10)                                              0.92       3.65       5.34       6.05       6.17   6.23

Variance decomposition (bias corrected, VAR(5))
   True                                                 0.06       0.29       0.47       0.58       0.65   0.70
   Average estimate
      R2                                                0.06       0.26       0.41       0.50       0.55   0.58
      LP A                                              0.05       0.24       0.38       0.48       0.54   0.58
      LP B                                              0.06       0.25       0.40       0.49       0.54   0.57
      VAR(5)                                            0.06       0.24       0.33       0.36       0.38   0.39

   Root mean squared error
      R2                                                0.04       0.11       0.16       0.19       0.21   0.23
      LP A                                              0.04       0.11       0.17       0.21       0.25   0.28
      LP B                                              0.04       0.11       0.16       0.19       0.20   0.22
      VAR(5)                                            0.04       0.11       0.19       0.26       0.31   0.34

   Coverage (90 % level) (asymptotic)
      R2                                                0.77       0.80       0.80       0.81       0.82   0.83
      LP A                                              0.85       0.83       0.82       0.83       0.84   0.85
      LP B                                              0.84       0.79       0.78       0.78       0.79   0.80
      VAR(5)                                            0.83       0.78       0.66       0.51       0.40   0.33

Variance decomposition (bias corrected, VAR(10))
   True                                                 0.06       0.29       0.47       0.58       0.65   0.70
   Average estimate
      R2                                                0.07       0.30       0.47       0.57       0.63   0.66
      LP A                                              0.05       0.23       0.37       0.47       0.52   0.55
      LP B                                              0.05       0.27       0.44       0.54       0.60   0.63
      VAR(10)                                           0.06       0.27       0.42       0.50       0.54   0.56

   Root mean squared error
      R2                                                0.05       0.12       0.16       0.19       0.21   0.22
      LP A                                              0.04       0.12       0.17       0.21       0.25   0.29
      LP B                                              0.04       0.12       0.16       0.18       0.20   0.21
      VAR(10)                                           0.04       0.11       0.15       0.19       0.21   0.23

   Coverage (90 % level)
      R2                                                0.72       0.76       0.78       0.80       0.82   0.83
      LP A                                              0.87       0.87       0.89       0.90       0.91   0.92
      LP B                                              0.85       0.77       0.75       0.76       0.78   0.79
      VAR(10)                                           0.83       0.83       0.81       0.79       0.77   0.74




                                                                                                                  16
                   Figure 1. Population impulse responses and variance decomposition for each DGP




Notes: the left panel shows the impulse response functions for three bivariate data generating processes (DGPs). The right
panel shows the contribution of the identified shock to variation of an outcome variable for the DGPs.




                                                                                                                       17
                          Figure 2: Smets and Wouters (2007) model, real GDP and monetary policy shock, T = 160.
                               Impulse Response                                             Variance Decomposition
                  1                                                        0.2




        0.5                                                             0.15




                  0                                                        0.1




      -0.5                                                              0.05




             -1                                                                  0

                      0            5                10   15     20                    0                   5             10   15   20




Variance Decomposition, Bias-Corrected                                               Coverage Probability of 90% C.I.
        0.2                                                                      1




     0.15



                                                                           0.8

        0.1




     0.05
                                                                           0.6




                  0

                      0            5                10   15     20                    0                   5             10   15   20




                                         Root MSE
     0.15




        0.1




     0.05




                  0

                      0            5                10   15     20




                                                                                                                                       18
                                          Figure 3 Real GDP. 1969:Q1-2007:Q4.

              Impulse Response, VAR                                               Impulse Response, LP
   5                                                            10



   0                                                             0



  -5                                                           -10
       0        5           10            15         20              0            5            10           15    20

           Variance Decomposition, VAR                            Variance Decomposition, VAR, bias-corrected
0.6                                                            0.6

0.4                                                            0.4

0.2                                                            0.2

   0                                                             0

-0.2                                                          -0.2
       0        5           10            15         20              0            5            10           15    20

            Variance Decomposition, R2                               Variance Decomposition, R2, bias-corrected
0.6                                                            0.6

0.4                                                            0.4

0.2                                                            0.2

   0                                                             0

-0.2                                                          -0.2
       0        5           10            15         20              0            5            10           15    20

           Variance Decomposition, LP A                           Variance Decomposition, LP A, bias-corrected
0.6                                                            0.6

0.4                                                            0.4

0.2                                                            0.2

   0                                                             0

-0.2                                                          -0.2
       0        5           10            15         20              0            5            10           15    20

           Variance Decomposition, LP B                           Variance Decomposition, LP B, bias-corrected
0.6                                                            0.6

0.4                                                            0.4

0.2                                                            0.2

   0                                                             0

-0.2                                                          -0.2
       0        5           10            15         20              0            5            10           15    20


                                  TFP          TFP 90% CB            MP         MP 90% CB




                                                                                                                       19
                                          Figure 4. Inflation. 1969:Q1-2007:Q4.

              Impulse Response, VAR                                                 Impulse Response, LP
0.5                                                                2



   0                                                               0



-0.5                                                              -2
       0        5           10            15          20               0            5            10           15    20

           Variance Decomposition, VAR                             Variance Decomposition, VAR, bias-corrected

0.6                                                              0.6
0.4                                                              0.4
0.2                                                              0.2
   0                                                               0
-0.2                                                            -0.2
       0        5           10            15          20               0            5            10           15    20

            Variance Decomposition, R2                                 Variance Decomposition, R2, bias-corrected

0.6                                                              0.6
0.4                                                              0.4
0.2                                                              0.2
   0                                                               0
-0.2                                                            -0.2
       0        5           10            15          20               0            5            10           15    20

           Variance Decomposition, LP A                            Variance Decomposition, LP A, bias-corrected

0.6                                                              0.6
0.4                                                              0.4
0.2                                                              0.2
   0                                                               0
-0.2                                                            -0.2
       0        5           10            15          20               0            5            10           15    20

           Variance Decomposition, LP B                             Variance Decomposition, LP B, bias-corrected
0.8                                                              0.8
0.6                                                              0.6
0.4                                                              0.4
0.2                                                              0.2
   0                                                               0
-0.2                                                            -0.2
       0        5           10            15          20               0            5            10           15    20


                                    TFP           TFP 90% CB               MP        MP 90% CB




                                                                                                                         20
                          APPENDIX FOR



A NOTE ON VARIANCE DECOMPOSITION WITH LOCAL
                PROJECTIONS


      Yuriy Gorodnichenko                      Byoungchan Lee
University of California â€“ Berkeley   University of California â€“ Berkeley
            and NBER




                                                                       1
Appendix A. Identification of ğ‘’ğ‘’ğ‘¡ğ‘¡
In Section II, we derive the â„-period ahead forecast error as following:
                        ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = (ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 ) âˆ’ ğ¸ğ¸[ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |Î©ğ‘¡ğ‘¡âˆ’1 ]
                                                  = ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘œğ‘œ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ + â‹¯ + ğœ“ğœ“ğ‘¥ğ‘¥,â„ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1
where ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 â‰¡ ğ¸ğ¸[ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ |Î©ğ‘¡ğ‘¡âˆ’1 ] and Î©ğ‘¡ğ‘¡âˆ’1 = {Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’2 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’2 , â€¦ }. In the footnote 2, we
argue that given invertibility of ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿), ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğœ“ğœ“ğ‘’ğ‘’,0 ğ‘’ğ‘’ğ‘¡ğ‘¡+â„ + â‹¯ + ï¿½ğœ“ğœ“ğ‘’ğ‘’,0 + â‹¯ + ğœ“ğœ“ğ‘’ğ‘’,â„ ï¿½ğ‘’ğ‘’ğ‘¡ğ‘¡ .
             There is a technically subtle issue that the above forecast error seems to be based on the
information set Î©ğ‘¡ğ‘¡âˆ’1 âˆª {ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 , â€¦ }, not the set of observables Î©ğ‘¡ğ‘¡âˆ’1 . Thus, we need to prove that
knowing ğ‘’ğ‘’ğ‘¡ğ‘¡ is redundant, once we have Î©ğ‘¡ğ‘¡ . In other words, {ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 , ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2 , â€¦ } âŠ‚
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ï¿½ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (Î©ğ‘¡ğ‘¡âˆ’1 )ï¿½.
             Letâ€™s assume that we have only Î©ğ‘¡ğ‘¡ . Following the idea of JordÃ  (2005), ğœ“ğœ“ğ‘¥ğ‘¥,â„ is identified
     ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶(ğ‘¦ğ‘¦ğ‘¡ğ‘¡ âˆ’ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’â„âˆ’1 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’â„ )
as                                      for all â„. This implies that Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ is identified, because
                 ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ )

                                                        Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ = Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ âˆ’ (1 âˆ’ ğ¿ğ¿)ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡ .
The drift term of ğ‘§ğ‘§ğ‘¡ğ‘¡ is also easily identified because ğ‘”ğ‘”ğ‘¦ğ‘¦ = ğ¸ğ¸[Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ ] = ğ¸ğ¸[Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ ], where ğ¸ğ¸[â‹…] is the
unconditional expectation operator. Therefore,
                                         ğ‘¤ğ‘¤ğ‘¡ğ‘¡ â‰¡ ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿)ğ‘’ğ‘’ğ‘¡ğ‘¡ = Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ âˆ’ ğ‘”ğ‘”ğ‘¦ğ‘¦ âˆˆ closureï¿½span(Î©ğ‘¡ğ‘¡ )ï¿½.
             Finally, it follows from the uniqueness of the Wold decomposition 1 that
                                               ğ‘’ğ‘’ğ‘¡ğ‘¡ = ğ‘¤ğ‘¤ğ‘¡ğ‘¡ âˆ’ ğ‘ƒğ‘ƒğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ(ğ‘¤ğ‘¤ğ‘¡ğ‘¡ |ğ‘¤ğ‘¤ğ‘¡ğ‘¡âˆ’1 , ğ‘¤ğ‘¤ğ‘¡ğ‘¡âˆ’2 , â€¦ )
                    ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶(ğ‘¤ğ‘¤ğ‘¡ğ‘¡ âˆ’ğ‘¤ğ‘¤ğ‘¡ğ‘¡âˆ’â„âˆ’1 ,ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’â„ )
and ğœ“ğœ“ğ‘’ğ‘’,â„ =                                          for all â„, where ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘ğ‘|ğ´ğ´) is defined by the orthogonal
                              ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ‘’ğ‘’ğ‘¡ğ‘¡ )

projection of a vector ğ‘ğ‘ in a Hibert space to a closed subspace generated by a set of vectors ğ´ğ´,
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ï¿½span(ğ´ğ´)ï¿½ .              2
                                              Therefore, {ğ‘’ğ‘’ğ‘¡ğ‘¡ , ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 , â€¦ } âŠ‚ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ï¿½ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (Î©ğ‘¡ğ‘¡ )ï¿½ , and specifically,
ğ¸ğ¸[ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ |Î©ğ‘¡ğ‘¡âˆ’1 ] = E[ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ |Î©ğ‘¡ğ‘¡âˆ’1 âˆª {ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 , ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2 , â€¦ }].
             This result illustrates how we can back out ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 in practice. First, we consider ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’
ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 :
yğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 = ğœ“ğœ“ğ‘¥ğ‘¥,0 ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ + ğœ“ğœ“ğ‘¥ğ‘¥,1 ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’1 + â‹¯ + ğœ“ğœ“ğ‘¥ğ‘¥,â„ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + [(ğ‘†ğ‘† âˆ— )â„+1 âˆ’ ğ¼ğ¼]ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 + ğ‘§ğ‘§ğ‘¡ğ‘¡+â„ âˆ’ ğ‘§ğ‘§ğ‘¡ğ‘¡âˆ’1 ,




1
    See Brockwell and Davis (1991) for details.
2
    See Conway (1990) for details on projections.

                                                                                                                                2
where ğ‘†ğ‘† âˆ— is the adjoint operator of the unilateral shift on ğ‘™ğ‘™2 (ğ‘ğ‘) and ğ¼ğ¼ is the identity operator. In
other words,
              ğ‘†ğ‘†(ğœ“ğœ“0 , ğœ“ğœ“1 , â€¦ ) = (0, ğœ“ğœ“0 , ğœ“ğœ“1 , â€¦ ),   ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘     ğ‘†ğ‘† âˆ— (ğœ“ğœ“0 , ğœ“ğœ“1 , â€¦ ) = (ğœ“ğœ“1 , ğœ“ğœ“2 , â€¦ ).
        For simple notations, we additionally assume that âˆ‘âˆ
                                                           j=0 ğ‘—ğ‘— â‹… ï¿½ğœ“ğœ“ğ‘’ğ‘’,ğ‘—ğ‘— ï¿½ < âˆ. This condition holds

for any stationary ARMA processes. By applying the Beveridge-Nelson decomposition, we obtain
the followings:
            ğ‘§ğ‘§ğ‘¡ğ‘¡ = Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ + â‹¯ + Î”ğ‘§ğ‘§1 + ğ‘§ğ‘§0 = ğ‘”ğ‘”ğ‘¦ğ‘¦ â‹… ğ‘¡ğ‘¡ + ğœ“ğœ“ğ‘’ğ‘’ (1) â‹… (ğ‘’ğ‘’ğ‘¡ğ‘¡ + â‹¯ + ğ‘’ğ‘’1 ) + ğœğœğ‘¡ğ‘¡ âˆ’ ğœğœ0 + ğ‘§ğ‘§0 ,
                                         âˆ ï¿½                 ï¿½ï¿½ï¿½                                        ï¿½ï¿½ï¿½
where ğœ“ğœ“ğ‘’ğ‘’ (1) = âˆ‘âˆ                                                                               âˆ
                  ğ‘—ğ‘—=0 ğœ“ğœ“ğ‘’ğ‘’,ğ‘—ğ‘— , ğœğœğ‘¡ğ‘¡ = âˆ‘ğ‘—ğ‘—=0 ğœ“ğœ“ğ‘—ğ‘— ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’ğ‘—ğ‘— , ğœ“ğœ“ğš¥ğš¥ = âˆ’(ğœ“ğœ“ğ‘—ğ‘—+1 + ğœ“ğœ“ğ‘—ğ‘—+2 + â‹¯ ), and âˆ‘ğ‘—ğ‘—=0 |ğœ“ğœ“ğš¥ğš¥ | < âˆ.

Thus, we can rewrite ğ‘§ğ‘§ğ‘¡ğ‘¡+â„ âˆ’ ğ‘§ğ‘§ğ‘¡ğ‘¡âˆ’1 by ğœ“ğœ“ğ‘’ğ‘’ (1) â‹… (ğ‘’ğ‘’ğ‘¡ğ‘¡+â„ + â‹¯ + ğ‘’ğ‘’ğ‘¡ğ‘¡ ) + ğœğœğ‘¡ğ‘¡+â„ âˆ’ ğœğœğ‘¡ğ‘¡âˆ’1 . 3 Finally,
            ğ¸ğ¸(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |Î©ğ‘¡ğ‘¡âˆ’1 ) = ï¿½ğœ“ğœ“ğ‘¥ğ‘¥,â„+1 âˆ’ ğœ“ğœ“ğ‘¥ğ‘¥,0 ï¿½ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 + ï¿½ğœ“ğœ“ğ‘¥ğ‘¥,â„+2 âˆ’ ğœ“ğœ“ğ‘¥ğ‘¥,1 ï¿½ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’2 + â‹¯ +
                                               ğ¸ğ¸(ğœğœğ‘¡ğ‘¡+â„ âˆ’ ğœğœğ‘¡ğ‘¡âˆ’1 |Î©ğ‘¡ğ‘¡âˆ’1 ).
        This illustrates what we actually do when we try to estimate the forecast errors by taking
residuals after regressing ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 on Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 and their lagged values. In the regression,
ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 and its lagged values control for two things. First thing to be captured is the component
directly related to {ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 } through ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿) , which is [(ğ‘†ğ‘† âˆ— )â„+1 âˆ’ ğ¼ğ¼]ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 = ï¿½ğœ“ğœ“ğ‘¥ğ‘¥,â„+1 âˆ’
ğœ“ğœ“ğ‘¥ğ‘¥,0 ï¿½ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 + ï¿½ğœ“ğœ“ğ‘¥ğ‘¥,â„+2 âˆ’ ğœ“ğœ“ğ‘¥ğ‘¥,1 ï¿½ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’2 + â‹¯ in the above expression. Moreover, (1 âˆ’ ğ¿ğ¿)ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 in
Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 is also controlled, generating ğ‘¤ğ‘¤ğ‘¡ğ‘¡âˆ’1 = ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿)ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 . A closed subspace generated by ğ‘¤ğ‘¤ğ‘¡ğ‘¡âˆ’1 and
its lagged values will be the same as that by ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 , ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2 , â€¦. Finally, this part of the projection will
control for ğ¸ğ¸(ğœğœğ‘¡ğ‘¡+â„ âˆ’ ğœğœğ‘¡ğ‘¡âˆ’1 |Î©ğ‘¡ğ‘¡âˆ’1 ) because ğœğœğ‘¡ğ‘¡âˆ’1 is a limit of linear combinations of {ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 , ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’2 , â€¦ }.
This completes purification of the ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 to ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 .




3
 Of course, we can proceed without the additional assumption. In that case, notations become messy because
everything should be written in terms of ğ‘’ğ‘’ğ‘¡ğ‘¡ â€™s instead of ğœğœğ‘¡ğ‘¡+â„ âˆ’ ğœğœğ‘¡ğ‘¡âˆ’1 .

                                                                                                                 3
Appendix B1. Proof of Proposition 1 and implementation detail
                                                                                                                                       â€²
Proposition              1.          Suppose         ğ‘“ğ‘“â„ = ï¿½ğ‘“ğ‘“ğ‘‡ğ‘‡|ğ‘‡ğ‘‡âˆ’â„âˆ’1 , ğ‘“ğ‘“ğ‘‡ğ‘‡âˆ’1|ğ‘‡ğ‘‡âˆ’â„âˆ’2 , â€¦ , ğ‘“ğ‘“ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š+â„+1 |ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ï¿½                   and   ğ‘‹ğ‘‹â„ =
   â„        â„                              â€²
ï¿½ğ‘‹ğ‘‹ğ‘‡ğ‘‡âˆ’â„ , ğ‘‹ğ‘‹ğ‘‡ğ‘‡âˆ’1 , â€¦ , ğ‘‹ğ‘‹ğ¿ğ¿â„ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š+1 ï¿½ for all â„ â‰¥ 0 where ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š = maxï¿½ğ¿ğ¿ğ‘¥ğ‘¥ , ğ¿ğ¿ğ‘¦ğ‘¦ ï¿½ . Then the ğ‘…ğ‘…2 of the
regression of ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 on ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ , given by ï¿½ğ‘“ğ‘“â„â€² ğ‘ƒğ‘ƒğ‘‹ğ‘‹â„ ğ‘“ğ‘“â„ ï¿½/(ğ‘“ğ‘“â„â€² ğ‘“ğ‘“â„ ) where ğ‘ƒğ‘ƒğ‘‹ğ‘‹â„ = ğ‘‹ğ‘‹â„ (ğ‘‹ğ‘‹â„â€² ğ‘‹ğ‘‹â„ )âˆ’1 ğ‘‹ğ‘‹â„â€² , has the
following asymptotic distribution for some ğ‘‰ğ‘‰â„,ğ‘…ğ‘…2 :
                                                     ğ‘“ğ‘“â„â€² ğ‘ƒğ‘ƒğ‘‹ğ‘‹â„ ğ‘“ğ‘“â„         ğ‘‘ğ‘‘
                                                âˆšğ‘‡ğ‘‡ ï¿½ â€²             âˆ’ ğ‘ ğ‘ â„ ï¿½ â†’ ğ’©ğ’©ï¿½0,                ğ‘‰ğ‘‰â„,ğ‘…ğ‘…2 ï¿½.
                                                        ğ‘“ğ‘“â„ ğ‘“ğ‘“â„
Proof. Although ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 is a time ğ‘¡ğ‘¡ + â„ variable, not time ğ‘¡ğ‘¡, we can proceed without loss of
validity of the results below by considering the moment conditions below as time ğ‘¡ğ‘¡ + â„ conditions,
not ğ‘¡ğ‘¡. We use this notation instead of ğ‘“ğ‘“ğ‘¡ğ‘¡|ğ‘¡ğ‘¡âˆ’â„âˆ’1 for consistency of the presentation.
                       â€²       â€²       â€²
          Let ğœƒğœƒ0 = ï¿½ğœƒğœƒ1,0 , ğœƒğœƒ2,0 , ğœƒğœƒ3,0 ï¿½â€² where
                                                             ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ = (ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ , â€¦ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ )â€² ,
                                                          âˆ’1                                                                       â€²
                           ğœƒğœƒ1,0 = ï¿½ğ¸ğ¸[ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ â€²]ï¿½ ï¿½ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ï¿½ = ï¿½ğœ“ğœ“ğ‘¥ğ‘¥,0 , ğœ“ğœ“ğ‘¥ğ‘¥,1 , â€¦ , ğœ“ğœ“ğ‘¥ğ‘¥,â„ ï¿½ ,
                                                     ğœƒğœƒ2,0 = ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ = ğœƒğœƒ1,0 ğœğœğ‘¥ğ‘¥2 ,
                                                                       2               2
                                                         ğœƒğœƒ3,0 = ğ¸ğ¸ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ â‰¡ ğœğœğ‘“ğ‘“,â„ .
                                                                        â€²
We define the method of moments estimator ğœƒğœƒï¿½ = ï¿½ğœƒğœƒï¿½1â€² , ğœƒğœƒï¿½2â€² , ğœƒğœƒï¿½3â€² ï¿½ as following:
                                                                                      ğ‘‹ğ‘‹â„â€² ğ‘“ğ‘“â„                         ğ‘“ğ‘“â„â€² ğ‘“ğ‘“â„
                                     ğœƒğœƒï¿½1 = (ğ‘‹ğ‘‹â„â€² ğ‘‹ğ‘‹â„ )âˆ’1 (ğ‘‹ğ‘‹â„â€² ğ‘“ğ‘“â„ ),       ğœƒğœƒï¿½2 =            ,         ğœƒğœƒï¿½3 =
                                                                                        ğ‘‡ğ‘‡â„                              ğ‘‡ğ‘‡â„
                                                                                                         ğ‘“ğ‘“â„â€² ğ‘ƒğ‘ƒğ‘‹ğ‘‹â„ ğ‘“ğ‘“â„
where ğ‘‡ğ‘‡â„ = ğ‘‡ğ‘‡ âˆ’ (ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š + 1) . It follows that ğ‘ ğ‘ â„ = ğœ‰ğœ‰(ğœƒğœƒ0 ) and                                                         = ğœ‰ğœ‰ï¿½ğœƒğœƒï¿½ï¿½ where ğœ‰ğœ‰(ğœƒğœƒ) =
                                                                                                            ğ‘“ğ‘“â„â€² ğ‘“ğ‘“â„

                         ğœƒğœƒ2â€² ğœƒğœƒ1
ğœ‰ğœ‰(ğœƒğœƒ1 , ğœƒğœƒ2 , ğœƒğœƒ3 ) =              . Therefore, we first derive the asymptotic distribution of âˆšğ‘‡ğ‘‡ï¿½ğœƒğœƒï¿½ âˆ’ ğœƒğœƒ0 ï¿½ and then
                           ğœƒğœƒ3

apply the delta method.
          To begin, we consider the moment conditions that ğ¸ğ¸ï¿½ğ‘”ğ‘”ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 , ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ , ğœƒğœƒï¿½ï¿½ = 0 where
                                                                                                                         â€²
                                                                                 ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ ğœƒğœƒ1 ï¿½
                            ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ) â‰¡ ğ‘”ğ‘”ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 , ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ , ğœƒğœƒï¿½ = ï¿½              ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ğœƒğœƒ2                   ï¿½.
                                                                                              2
                                                                                           ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1    âˆ’ ğœƒğœƒ3
It is clear that the conditions are satisfied only when ğœƒğœƒ = ğœƒğœƒ0 and the system is just-identified. As
shown by Hansen (1982), we know that


                                                                                                                                                    4
                                                                 ğ‘‘ğ‘‘
                                          âˆšğ‘‡ğ‘‡ï¿½ğœƒğœƒï¿½ âˆ’ ğœƒğœƒ0 ï¿½ â†’ ğ’©ğ’©(0,                    ğºğº âˆ’1 Î©(ğºğº â€² )âˆ’1 )
where ğºğº = ğ¸ğ¸[âˆ‡Î¸ ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 )] and Î© = âˆ‘âˆ
                                         ğ‘™ğ‘™=âˆ’âˆ Î“(ğ‘™ğ‘™) and Î“(ğ‘™ğ‘™) is the autocovariance of ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 ) at lag
                                                                                                         â€²
ğ‘™ğ‘™. With some algebra, we can show that ğºğº = âˆ’ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ï¿½ğ¸ğ¸ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ ï¿½ , ğ¼ğ¼â„+2 ï¿½ where ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘(ğ´ğ´, ğµğµ) is the

block diagonal matrix whose diagonal components are ğ´ğ´ and ğµğµ in order.
                                                                                  ğœ•ğœ•ğœ•ğœ•(ğœƒğœƒ0 )         1          â€²       â€²
           Regarding the delta method, we define Î” â‰¡                                           =             ï¿½ğœƒğœƒ2,0 , ğœƒğœƒ1,0 , âˆ’ğ‘ ğ‘ â„ ï¿½. Combining the
                                                                                    ğœ•ğœ•ğœƒğœƒâ€²           ğœƒğœƒ3,0

above derivations, and being explicit about the fact that the moment conditions ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (â‹…) are for the
ğ‘…ğ‘…2 approach at horizon â„, we have the desired result.
                                               ğ‘“ğ‘“â„â€² ğ‘ƒğ‘ƒğ‘‹ğ‘‹â„ ğ‘“ğ‘“â„         ğ‘‘ğ‘‘
                                          âˆšğ‘‡ğ‘‡ ï¿½ â€²             âˆ’ ğ‘ ğ‘ â„ ï¿½ â†’ ğ’©ğ’©ï¿½0,                       ğ‘‰ğ‘‰â„,ğ‘…ğ‘…2 ï¿½
                                                  ğ‘“ğ‘“â„ ğ‘“ğ‘“â„
                                                                     â€²âˆ’1                            âˆ’1
                         where ğ‘‰ğ‘‰â„,ğ‘…ğ‘…2 = Î”â„,ğ‘…ğ‘…2 ï¿½ğºğºâ„,ğ‘…ğ‘…2 ï¿½ Î©â„,ğ‘…ğ‘…2 ï¿½ğºğºâ„,ğ‘…ğ‘… 2ï¿½ Î”â€²â„,ğ‘…ğ‘…2 .                                                â–¡


Implementation. We discuss how to implement Proposition 1. First of all, we use ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1
obtained from Equation (6) instead of ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 in practice, because ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 is not observable.
                                                             â€²
Then ğ‘“ğ‘“Ì‚â„ = ï¿½ğ‘“ğ‘“Ì‚ğ‘‡ğ‘‡|ğ‘‡ğ‘‡âˆ’â„âˆ’1 , â€¦ , ğ‘“ğ‘“Ì‚ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š+â„+1 |ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š ï¿½ , and ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 is given by ï¿½ğ‘“ğ‘“Ì‚â„â€² ğ‘ƒğ‘ƒğ‘‹ğ‘‹â„ ğ‘“ğ‘“Ì‚â„ ï¿½/(ğ‘“ğ‘“Ì‚â„â€² ğ‘“ğ‘“Ì‚â„ ).
           We also need to estimate ğ‘‰ğ‘‰â„,ğ‘…ğ‘…2 because it depends on the population parameters. Letâ€™s
                                                                                                                                          â€²
begin with Î”â„,ğ‘…ğ‘…2 . A practically feasible estimator of ğœƒğœƒ we use is ğœƒğœƒï¿½ = ï¿½ğœƒğœƒï¿½1â€² , ğœƒğœƒï¿½2â€² , ğœƒğœƒï¿½3â€² ï¿½ where
                                                                                      ğ‘‹ğ‘‹â„â€² ğ‘“ğ‘“Ì‚â„                        ğ‘“ğ‘“Ì‚â„â€² ğ‘“ğ‘“Ì‚â„
                              ğœƒğœƒï¿½1 = (ğ‘‹ğ‘‹â„â€² ğ‘‹ğ‘‹â„ )âˆ’1 ï¿½ğ‘‹ğ‘‹â„â€² ğ‘“ğ‘“Ì‚â„ ï¿½,            ğœƒğœƒï¿½2 =              ,             ğœƒğœƒï¿½3 =              .
                                                                                        ğ‘‡ğ‘‡â„                               ğ‘‡ğ‘‡â„
                                                               ï¿½ï¿½
                                                          ğœ•ğœ•ğœ•ğœ•ï¿½ğœƒğœƒ           1
                                 ï¿½â„,ğ‘…ğ‘…2 â‰¡
A natural estimator of Î”â„,ğ‘…ğ‘…2 is Î”                                    =    ï¿½   ï¿½ğœƒğœƒï¿½2â€² , ğœƒğœƒï¿½1â€² , âˆ’ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 ï¿½.   The last element is based on a
                                                           ğœ•ğœ•ğœƒğœƒâ€²           ğœƒğœƒ3

bias-corrected estimates instead of ğœ‰ğœ‰ï¿½ğœƒğœƒï¿½ ï¿½ because we find that this specification provides better
performances in simulations. 4 How to obtain the bias-corrected estimator ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 in this set-up will be
discussed later.
                                                             â€²
           We next turn to ğºğº = âˆ’ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ï¿½ğ¸ğ¸ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ ï¿½ , ğ¼ğ¼â„+2 ï¿½. It can be easily estimated by ğºğºï¿½â„,ğ‘…ğ‘…2 =
                                                                                      â€²
âˆ’ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘(ğ‘‹ğ‘‹â„â€² ğ‘‹ğ‘‹â„ /ğ‘‡ğ‘‡â„ , ğ¼ğ¼â„+2 ) = âˆ’ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ï¿½âˆ‘ğ‘‡ğ‘‡âˆ’â„              â„     â„
                                                ğ‘¡ğ‘¡=ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š +1 ğ‘‹ğ‘‹ğ‘¡ğ‘¡ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡ ï¿½ /ğ‘‡ğ‘‡â„ , ğ¼ğ¼â„+2 ï¿½.

           It remains to estimate Î©â„,ğ‘…ğ‘…2 = âˆ‘âˆ
                                            ğ‘™ğ‘™=âˆ’âˆ Î“(ğ‘™ğ‘™) where Î“(ğ‘™ğ‘™) is the autocovariance of ğ‘”ğ‘”ğ‘¡ğ‘¡ (ğœƒğœƒ0 ) at

lag ğ‘™ğ‘™. We use the pre-whitening procedure following Andrews and Monahan (1992) to avoid


4
    Results are available upon requests.

                                                                                                                                                 5
underestimation problem of the long-run variance of ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 ). To that end, we define a 2â„ + 3
dimensional vector ğ‘ğ‘ğ‘¡ğ‘¡+â„ as following:
                                                                               â€²
                                                 ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ ğœƒğœƒï¿½1 ï¿½
                                      ğ‘ğ‘ğ‘¡ğ‘¡+â„   â‰¡â›       ğ‘‹ğ‘‹ â„ ğ‘“ğ‘“Ì‚       âˆ’ ğœƒğœƒï¿½
                                                              ğ‘¡ğ‘¡ ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1            2
                                                                                       â.

                                                   â          ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1
                                                                  2
                                                                           âˆ’   ğœƒğœƒï¿½3          â 
                                                                                                  1
It is worth noting that the sample average of ğ‘ğ‘ğ‘¡ğ‘¡ is a zero vector, i.e.                               âˆ‘ğ‘‡ğ‘‡âˆ’â„
                                                                                                         ğ‘¡ğ‘¡=ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š +1 ğ‘ğ‘ğ‘¡ğ‘¡ = 0 given the
                                                                                                  ğ‘‡ğ‘‡â„

definition of ğœƒğœƒï¿½ . To whiten the series, we use a VAR(1) that ğ‘ğ‘ğ‘¡ğ‘¡ = ğ´ğ´ğ‘ğ‘ğ‘¡ğ‘¡âˆ’1 + ğ‘ˆğ‘ˆğ‘¡ğ‘¡ . The estimated
autoregressive matrix and the residual are denoted by ğ´ğ´Ì‚ and ğ‘ˆğ‘ˆ
                                                              ï¿½ğ‘¡ğ‘¡ . Then we estimate the long-run
            ï¿½ğ‘¡ğ‘¡ by applying the method suggested by Newey and West (1987) with the Bartlett kernel
variance of ğ‘ˆğ‘ˆ
                  ï¿½ğ¿ğ¿ +2 , â€¦ , ğ‘ˆğ‘ˆ
to the residuals ï¿½ğ‘ˆğ‘ˆ           ï¿½ğ‘‡ğ‘‡âˆ’â„ ï¿½. Specifically, the estimated long-run variance is given by
                     ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š

                                         1                                  ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘
            ï¿½ ï¿½ğ‘ˆğ‘ˆ
          ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ğ‘¡ğ‘¡ ï¿½ = Î“ï¿½ğ‘ˆğ‘ˆ,0 +              ï¿½Î“ï¿½ğ‘ˆğ‘ˆ,1 + Î“ï¿½ğ‘ˆğ‘ˆ,âˆ’1 ï¿½ + â‹¯ +           ï¿½Î“ï¿½        + Î“ï¿½ğ‘ˆğ‘ˆ,âˆ’ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘ ï¿½
                                    ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘ + 1                           ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘ + 1 ğ‘ˆğ‘ˆ,ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘
where Î“ï¿½ğ‘ˆğ‘ˆ,ğ‘™ğ‘™ is the estimated autocovariance matrix of ğ‘ˆğ‘ˆğ‘¡ğ‘¡ at lag ğ‘™ğ‘™. We use a simple rule suggested
by Stock and Watson (2011) to select the number of autocovariance matrices to be included in the
                                                                                  1/3          ï¿½ â„,ğ‘…ğ‘…2
estimation. Following the rule, ğ¿ğ¿ğ‘ğ‘ğ‘ğ‘ + 1 is the closest natural number of 0.75ğ‘‡ğ‘‡â„ . Finally, Î©
                             âˆ’1
is obtained by ï¿½ğ¼ğ¼2â„+3 âˆ’ ğ´ğ´Ì‚ï¿½ ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
                                ï¿½ ï¿½ğ‘ˆğ‘ˆï¿½ğ‘¡ğ‘¡ ï¿½ï¿½ğ¼ğ¼2â„+3 âˆ’ ğ´ğ´Ì‚â€² ï¿½âˆ’1 .

        In sum, the asymptotic standard error of ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 is given by
                                                    1
                         [ğ‘ ğ‘ . ğ‘’ğ‘’. (ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 )]2 =       ï¿½â„,ğ‘…ğ‘…2 ï¿½ğºğºï¿½â„,ğ‘…ğ‘…2 ï¿½âˆ’1 Î©
                                                       Î”                    ï¿½ â„,ğ‘…ğ‘…2 ï¿½ğºğºï¿½ â€² 2 ï¿½âˆ’1 Î”
                                                                                                 ï¿½â€² 2
                                                                                        â„,ğ‘…ğ‘…      â„,ğ‘…ğ‘…
                                                   ğ‘‡ğ‘‡â„
                                                                  1 â€² â€²
                                              ï¿½â„,ğ‘…ğ‘…2 =
                                        where Î”                      ï¿½ğœƒğœƒï¿½2 , ğœƒğœƒï¿½1 , âˆ’ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 ï¿½,
                                                                  ï¿½
                                                                 ğœƒğœƒ3
                                                                     ğ‘‹ğ‘‹â„â€² ğ‘‹ğ‘‹â„
                                           ğºğºï¿½â„,ğ‘…ğ‘…2 = âˆ’ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ï¿½             , ğ¼ğ¼â„+1 ï¿½,
                                                                       ğ‘‡ğ‘‡â„
                             ï¿½ â„,ğ‘…ğ‘…2 = ï¿½ğ¼ğ¼2â„+3 âˆ’ ğ´ğ´Ì‚ï¿½âˆ’1 ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
                             Î©                            ï¿½ ï¿½ğ‘ˆğ‘ˆï¿½ğ‘¡ğ‘¡ ï¿½ï¿½ğ¼ğ¼2â„+3 âˆ’ ğ´ğ´Ì‚â€² ï¿½âˆ’1 .


Bias-correction. We conjecture that most of the finite sample bias is due to the non-linear
transformation ğœ‰ğœ‰(â‹…), not estimation of ğœƒğœƒ. Note that ğœƒğœƒ consists of projection coefficients of ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 on
ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ , covariance between ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 and ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ , and variance of ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 . Estimation of all three quantities
are rather standard, and significant biases regarding the method of moments estimator have not been
reported. Below we suggest a method to capture biases originating from ğœ‰ğœ‰(â‹…) in small samples.

                                                                                                                                        6
                                        ğ‘‘ğ‘‘
       Because âˆšğ‘‡ğ‘‡ï¿½ğœƒğœƒï¿½ âˆ’ ğœƒğœƒ0 ï¿½ â†’ ğ’©ğ’©(0,                   ğºğº âˆ’1 Î©(ğºğº â€² )âˆ’1 ) , we can approximate the asymptotic
                                                                     âˆ’1
variance of the feasible estimator ğœƒğœƒï¿½ by
                                                     1
                                                          ï¿½ğºğºï¿½â„,ğ‘…ğ‘…2 ï¿½ Î© ï¿½ â„,ğ‘…ğ‘…2 ï¿½ğºğºï¿½ â€² 2 ï¿½âˆ’1 . We simulate ğœƒğœƒ ğ‘ğ‘ for ğµğµ times
                                                    ğ‘‡ğ‘‡â„                             â„,ğ‘…ğ‘…

from the following normal distribution:
                                                           1             âˆ’1
                              ğœƒğœƒ ğ‘ğ‘ âˆ¼ ğ’©ğ’© ï¿½ğœƒğœƒï¿½ ,               ï¿½ğºğºï¿½â„,ğ‘…ğ‘…2 ï¿½ Î© ï¿½ â„,ğ‘…ğ‘…2 ï¿½ğºğºï¿½ â€² 2 ï¿½âˆ’1 ï¿½.
                                                          ğ‘‡ğ‘‡â„                           â„,ğ‘…ğ‘…


       We discard cases with ğœƒğœƒ3ğ‘ğ‘ â‰¤ 0 while drawing ğœƒğœƒ ğ‘ğ‘ â€™s, because ğœƒğœƒ3 = ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½. Then
the bias is estimated by
                                                                                                  ğµğµ
                                    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½                                   ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½      1
                   ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘ â„ğ‘…ğ‘…2   â‰¡ ğœ‰ğœ‰(ğœƒğœƒ            ï¿½ ï¿½,
                                          ğ‘ğ‘ ) âˆ’ ğœ‰ğœ‰ï¿½ğœƒğœƒ          where         ğœ‰ğœ‰(ğœƒğœƒ ğ‘ğ‘ ) â‰¡    ï¿½ ğœ‰ğœ‰(ğœƒğœƒ ğ‘ğ‘ ).
                                                                                           ğµğµ
                                                                                             ğ‘ğ‘=1

       Finally, the bias-corrected estimator is given by
                                                                                    ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
                                    ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2 = ğœ‰ğœ‰ï¿½ğœƒğœƒï¿½ ï¿½ âˆ’ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘ â„ğ‘…ğ‘…2 = 2ğœ‰ğœ‰ï¿½ğœƒğœƒï¿½ ï¿½ âˆ’ ğœ‰ğœ‰(ğœƒğœƒ ğ‘ğ‘ ).




                                                                                                                           7
Appendix B2. Proof of Proposition 2 and implementation details
Proposition 2. The local projections based estimators when ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 is observable have the
following asymptotic distributions for some ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ and ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ :
                                                              2
                                        âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2         ğ‘‘ğ‘‘
                                  âˆšğ‘‡ğ‘‡ ï¿½                             âˆ’ ğ‘ ğ‘ â„ ï¿½ â†’ ğ’©ğ’©ï¿½0, ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½,                      and
                                         ï¿½ (ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 )
                                        ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
                                                                  2
                                             âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2                                                ğ‘‘ğ‘‘
      âˆšğ‘‡ğ‘‡ ï¿½                    2                                                          âˆ’ ğ‘ ğ‘ â„ ï¿½ â†’ ğ’©ğ’©ï¿½0,                     ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½
           âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2 + ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
                                          ï¿½ ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ âˆ‘â„ğ‘–ğ‘–=0 ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘– ï¿½

Proof. Similar to Proposition 1, the moment conditions below should be understood as time ğ‘¡ğ‘¡ + â„
conditions, not time ğ‘¡ğ‘¡.
    (i)          LP-A estimator
We first derive the joint distribution of ğœ“ğœ“ï¿½ğ‘¥ğ‘¥,ğ‘–ğ‘– â€™s, ğœğœï¿½ğ‘¥ğ‘¥2 , and ğœğœï¿½ğ‘“ğ‘“,â„
                                                                        2      ï¿½ ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½. Then we will use the
                                                                            â‰¡ ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
delta method to find ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ .
           To begin, we describe the moment conditions for the local projections for ğœ“ğœ“ï¿½ğ‘¥ğ‘¥,ğ‘–ğ‘– â€™s. We run
the following OLS regression and take the coefficient on ğ‘¥ğ‘¥ğ‘¡ğ‘¡ :
          ğ‘¦ğ‘¦ğ‘¡ğ‘¡+ğ‘–ğ‘– âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 = ğ›½ğ›½0ğ‘–ğ‘– ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + â‹¯ + ğ›½ğ›½ğ½ğ½ğ‘–ğ‘–ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ½ğ½ğ¿ğ¿ğ¿ğ¿ + ğ›¾ğ›¾1ğ‘–ğ‘– Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 + â‹¯ + ğ›¾ğ›¾ğ¼ğ¼ğ‘–ğ‘–ğ¿ğ¿ğ¿ğ¿ Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’ğ¼ğ¼ğ¿ğ¿ğ¿ğ¿ + ğ‘ğ‘ğ‘–ğ‘– + ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+ğ‘–ğ‘–|ğ‘¡ğ‘¡âˆ’1

for all ğ‘–ğ‘– = 0,1, â€¦ , â„. In the above representation, ğ›½ğ›½0ğ‘–ğ‘– = ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– . For a simple notation, we rewrite the
above equation by
                                                           ğ‘ğ‘ğ‘–ğ‘–,ğ‘¡ğ‘¡ = ğ‘ğ‘ğ‘¡ğ‘¡â€² Î’ğ‘–ğ‘– + ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+ğ‘–ğ‘–|ğ‘¡ğ‘¡âˆ’1
                                                    where ğ‘ğ‘ğ‘–ğ‘–,ğ‘¡ğ‘¡ = ğ‘¦ğ‘¦ğ‘¡ğ‘¡+ğ‘–ğ‘– âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 ,
                                                                                                                   â€²
                                        ğ‘ğ‘ğ‘¡ğ‘¡ = ï¿½1, ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , â€¦ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ½ğ½ğ¿ğ¿ğ¿ğ¿ , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 , â€¦ , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’ğ¼ğ¼ğ¿ğ¿ğ¿ğ¿ ï¿½ ,
                                                                                                             â€²
                                               Î’ğ‘–ğ‘– = ï¿½ğ‘ğ‘ğ‘–ğ‘– , ğ›½ğ›½0ğ‘–ğ‘– , â€¦ , ğ›½ğ›½ğ½ğ½ğ‘–ğ‘–ğ¿ğ¿ğ¿ğ¿ , ğ›¾ğ›¾1ğ‘–ğ‘– , â€¦ , ğ›¾ğ›¾ğ¼ğ¼ğ‘–ğ‘–ğ¿ğ¿ğ¿ğ¿ ï¿½ .
                       ï¿½ ğ‘–ğ‘– becomes the method of moments estimator of the following moment
Then the OLS estimator Î’
conditions that
                                                        ğ¸ğ¸ï¿½ğ‘ğ‘ğ‘¡ğ‘¡ ï¿½ğ‘ğ‘ğ‘–ğ‘–,ğ‘¡ğ‘¡ âˆ’ ğ‘ğ‘ğ‘¡ğ‘¡â€² Î’ğ‘–ğ‘– ï¿½ï¿½ = 0.
Also, ğœ“ğœ“ï¿½ğ‘¥ğ‘¥,ğ‘–ğ‘– is given by ğœ„ğœ„1â€² Î’
                                ï¿½ ğ‘–ğ‘– where ğœ„ğœ„1 is a ğ¼ğ¼ ğ¿ğ¿ğ¿ğ¿ + ğ½ğ½ğ¿ğ¿ğ¿ğ¿ + 2 dimensional vector whose first element is

one and the others are zero.




                                                                                                                                             8
                                                                                                  2
        To study all parameters required simultaneously, we let ğœƒğœƒ0 = ï¿½Î’0â€² , â€¦ , Î’â„â€² , ğœğœğ‘¥ğ‘¥2 , ğœğœğ‘“ğ‘“,â„ ï¿½â€² where
   2
ğœğœğ‘“ğ‘“,â„ â‰¡ ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½. We use the moment conditions such that ğ¸ğ¸[ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 )] = 0 where ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 )
is given as following:
                                                       ğ‘ğ‘ğ‘¡ğ‘¡ ï¿½ğ‘ğ‘0,ğ‘¡ğ‘¡ âˆ’ ğ‘ğ‘ğ‘¡ğ‘¡â€² Î’0 ï¿½
                                                     â›              â‹®            â
                                                                          â€²
                                     ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 ) = âœ ğ‘¡ğ‘¡ â„,ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘¡ğ‘¡ Î’â„ ï¿½âŸ.
                                                      ğ‘ğ‘   ï¿½ğ‘ğ‘      âˆ’
                                                     âœ                           âŸ
                                                              ğ‘¥ğ‘¥ğ‘¡ğ‘¡2 âˆ’ ğœğœğ‘¥ğ‘¥2
                                                           2                 2
                                                     â ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ğœğœğ‘“ğ‘“,â„ â 
We define ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ) similarly. It is clear that it is a just-identified system. Similar to the proof of
Proposition 1, we know that
                                                    ğ‘‘ğ‘‘
                                 âˆšğ‘‡ğ‘‡ï¿½ğœƒğœƒï¿½ âˆ’ ğœƒğœƒ0 ï¿½ â†’ ğ’©ğ’©(0,               ğºğº âˆ’1 Î©(ğºğº â€² )âˆ’1 )
where ğºğº = ğ¸ğ¸[âˆ‡Î¸ ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 )] and Î© = âˆ‘âˆ
                                         ğ‘™ğ‘™=âˆ’âˆ Î“(ğ‘™ğ‘™) and Î“(ğ‘™ğ‘™) is the autocovariance of ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 ) at lag

ğ‘™ğ‘™. With some algebra, we can show that
                                                  ğ¼ğ¼   âŠ— ğ‘ğ‘ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘¡ğ‘¡â€²             0
                                        ğºğº = âˆ’ğ¸ğ¸ ï¿½ â„+1                             ï¿½
                                                       0                       ğ¼ğ¼2
where âŠ— is the Kroneckerâ€™s product.
        A transformation ğœ‰ğœ‰ is required to connect ğœƒğœƒ with ğ‘ ğ‘ â„ . We define
                                                             âˆ‘â„ğ‘–ğ‘–=0(ğœ„ğœ„1â€² Î’ğ‘–ğ‘– )2 ğœğœğ‘¥ğ‘¥2
                                      ğœ‰ğœ‰(ğœƒğœƒ0 ) = ğ‘ ğ‘ â„ =                   2            .
                                                                    ğœğœğ‘“ğ‘“,â„
ğœ‰ğœ‰(ğœƒğœƒ) is also defined similarly.
                                                                ğœ•ğœ•ğœ•ğœ•(ğœƒğœƒ0 )
        Regarding the delta method, we need Î” â‰¡                           . With some algebra, we show that
                                                                  ğœ•ğœ•ğœƒğœƒâ€²
                                                                                 â€²
                                                     2ğœ“ğœ“ğ‘¥ğ‘¥,0 ğœğœğ‘¥ğ‘¥2 ğœ„ğœ„1
                                                   â›       â‹®
                                                                2 â
                                              1 âœ ğ‘¥ğ‘¥,â„ ğœğœğ‘¥ğ‘¥ ğœ„ğœ„1 âŸ
                                                     2ğœ“ğœ“
                                          Î”= 2 âœ â„                     âŸ.
                                            ğœğœğ‘“ğ‘“,â„
                                                   âœ ï¿½ ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– 2 âŸ

                                                              ğ‘–ğ‘–=0
                                                         â       âˆ’ğ‘ ğ‘ â„        â 
        Combining the above derivations, and being explicit about the fact that the moment
conditions ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (â‹…) are for the LP-A approach at horizon â„, we have the desired result.
                                                   2
                                  âˆ‘â„ğ‘–ğ‘–=0ï¿½ğœ“ğœ“ï¿½ğ‘¥ğ‘¥,ğ‘–ğ‘– ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2         ğ‘‘ğ‘‘
                            âˆšğ‘‡ğ‘‡ ï¿½                          âˆ’ ğ‘ ğ‘ â„ ï¿½ â†’ ğ’©ğ’©ï¿½0,                ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½.
                                   ï¿½ ï¿½ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½
                                  ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰



                                                                                                              9
                                                                    âˆ’1     â€²                     âˆ’1
                   where ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ = Î”â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ğºğºâ„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ Î©â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ğºğºâ„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ Î”â€²â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ .                 â–¡


     (ii)      LP-B estimator
            The joint distribution of ğœ“ğœ“ï¿½ğ‘¥ğ‘¥,ğ‘–ğ‘– â€™s is obtained similarly. To study all parameters required
simultaneously,            we       let         ğœƒğœƒ0 = ï¿½Î’0â€² , â€¦ , Î’â„â€² , ğœğœğ‘¥ğ‘¥2 , ğœğœğ‘£ğ‘£,â„
                                                                                  2
                                                                                      ï¿½â€²          where      2
                                                                                                          ğœğœğ‘£ğ‘£,â„ â‰¡ ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’
âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘– ï¿½. We use the moment conditions such that ğ¸ğ¸[ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 )] = 0 where ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 ) is
given as following:
                                                                     ğ‘ğ‘ğ‘¡ğ‘¡ ï¿½ğ‘ğ‘0,ğ‘¡ğ‘¡ âˆ’ ğ‘ğ‘ğ‘¡ğ‘¡â€² Î’0 ï¿½
                                        â›                                         â‹®       â
                                        âœ                            ğ‘ğ‘ğ‘¡ğ‘¡ ï¿½ğ‘ğ‘â„,ğ‘¡ğ‘¡ âˆ’ ğ‘ğ‘ğ‘¡ğ‘¡â€² Î’â„ ï¿½
                                                                                          âŸ
                                  (ğœƒğœƒ
                            ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ 0 =) âœ                                   ğ‘¥ğ‘¥ğ‘¡ğ‘¡2 âˆ’ ğœğœğ‘¥ğ‘¥2 âŸ.
                                        âœ                                       2         âŸ
                                        âœ                 â„                               âŸ
                                                                                      2
                                         ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ï¿½(ğœ„ğœ„1â€² Î’ğ‘–ğ‘– ) ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘– ï¿½ âˆ’ ğœğœğ‘£ğ‘£,â„
                                        â               ğ‘–ğ‘–=0                              â 
We define ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ) similarly. It is clear that it is a just-identified system. In such a case, the method
of moments estimator ğœƒğœƒï¿½ can be understood as a two-step estimator. It first find Î’
                                                                                  ï¿½ ğ‘–ğ‘– â€™s using the OLS
                                                                                                 2
moment conditions and then plug the estimates into the remaining conditions. Then ğœğœï¿½ğ‘¥ğ‘¥2 and ğœğœï¿½ğ‘£ğ‘£,â„
                  ï¿½ ğ‘–ğ‘– â€™s. It is worth noting that this is the same procedure we follow when we define
are derived given Î’
ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ . The only difference is that we are using here ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 instead of its estimate.
            Similar to the proof of Proposition 1, we know that
                                                               ğ‘‘ğ‘‘
                                       âˆšğ‘‡ğ‘‡ï¿½ğœƒğœƒï¿½ âˆ’ ğœƒğœƒ0 ï¿½ â†’ ğ’©ğ’©(0,                  ğºğº âˆ’1 Î©(ğºğº â€² )âˆ’1 )
where ğºğº = ğ¸ğ¸[âˆ‡Î¸ ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 )] and Î© = âˆ‘âˆ
                                         ğ‘™ğ‘™=âˆ’âˆ Î“(ğ‘™ğ‘™) and Î“(ğ‘™ğ‘™) is the autocovariance of ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (ğœƒğœƒ0 ) at lag

ğ‘™ğ‘™. With some algebra, we can show that

                                                                    ğ¼ğ¼â„+1 âŠ— ğ‘ğ‘ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘¡ğ‘¡â€²                 0
                                     â›                                                               ï¿½ â
                            ğºğº = âˆ’ğ¸ğ¸ âœ                                                                    âŸ
                                     âœ                  0                  â‹¯           0
                                                                                                     ï¿½ âŸ
                                              2ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ ğœ„ğœ„1â€²     â‹¯ 2ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ğ‘¥ğ‘¥ğ‘¡ğ‘¡+1 ğœ„ğœ„1â€² ğ¼ğ¼2
                                          â                                                               â 
where âŠ— is the Kroneckerâ€™s product. For the bottom left part, we use the fact that ğœ„ğœ„1â€² Î’ğ‘–ğ‘– = ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– and
ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘–       .        Because        ğ‘£ğ‘£ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğœ“ğœ“ğ‘’ğ‘’,0 ğ‘’ğ‘’ğ‘¡ğ‘¡+â„ + â‹¯ + ï¿½ğœ“ğœ“ğ‘’ğ‘’,0 + â‹¯ +
ğœ“ğœ“ğ‘’ğ‘’,â„ ï¿½ğ‘’ğ‘’ğ‘¡ğ‘¡ is orthogonal to {ğ‘¥ğ‘¥ğ‘¡ğ‘¡ }, the bottom left block of ğºğº becomes a zero matrix.


                                                                                                                                    10
         A transformation ğœ‰ğœ‰ is required to connect ğœƒğœƒ with ğ‘ ğ‘ â„ . We define
                                                                             âˆ‘â„ğ‘–ğ‘–=0(ğœ„ğœ„1â€² Î’ğ‘–ğ‘– )2 ğœğœğ‘¥ğ‘¥2
                                                ğœ‰ğœ‰(ğœƒğœƒ0 ) = ğ‘ ğ‘ â„ =                                      2 .
                                                                        âˆ‘â„ğ‘–ğ‘–=0(ğœ„ğœ„1â€² Î’ğ‘–ğ‘– )2 ğœğœğ‘¥ğ‘¥2 + ğœğœğ‘£ğ‘£,â„
ğœ‰ğœ‰(ğœƒğœƒ) is also defined similarly.
                                                                                     ğœ•ğœ•ğœ•ğœ•(ğœƒğœƒ0 )                                      2
         Regarding the delta method, we need Î” â‰¡                                                . For a simple notation, we write ğœğœğ‘“ğ‘“,â„ â‰¡
                                                                                       ğœ•ğœ•ğœƒğœƒâ€²

ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ = âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–
                                2                2
                                      ğœğœğ‘¥ğ‘¥2 + ğœğœğ‘£ğ‘£,â„ . With some algebra, we show that
                                                                                                      â€²
                                                                 2ğœ“ğœ“ğ‘¥ğ‘¥,0 ğœğœğ‘¥ğ‘¥2 ğœ„ğœ„1
                                                               â›       â‹®           â
                                                                            2
                                                       1 âˆ’ ğ‘ ğ‘ â„ âœ 2ğœ“ğœ“ğ‘¥ğ‘¥,â„ ğœğœğ‘¥ğ‘¥ ğœ„ğœ„1 âŸ
                                                     Î”= 2 âœ        â„               âŸ.
                                                        ğœğœğ‘“ğ‘“,â„ âœ           2       âŸ
                                                               âœ ï¿½ ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– âŸ
                                                                                  ğ‘–ğ‘–=0
                                                                        ââˆ’ğ‘ ğ‘ â„ /(1 âˆ’ ğ‘ ğ‘ â„ )â 
         Combining the above derivations, and being explicit about the fact that the moment
conditions ğ‘”ğ‘”ğ‘¡ğ‘¡+â„ (â‹…) are for the LPB approach at horizon â„, we have the desired result.
                                                                  2
                                               âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2                                         ğ‘‘ğ‘‘
       âˆšğ‘‡ğ‘‡ ï¿½                       2                                                                       âˆ’ ğ‘ ğ‘ â„ ï¿½ â†’ ğ’©ğ’©ï¿½0,       ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½.
               âˆ‘â„ğ‘–ğ‘–=0ï¿½ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ï¿½ ğœğœï¿½ğ‘¥ğ‘¥2      ï¿½ ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ âˆ‘â„ğ‘–ğ‘–=0 ğ›½ğ›½Ì‚0ğ‘–ğ‘–,ğ¿ğ¿ğ¿ğ¿ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘– ï¿½
                                             + ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
                                                                         âˆ’1 â€²                          âˆ’1
                    where ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ = Î”â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ğºğºâ„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ Î©â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ğºğºâ„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ Î”â€²â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ .                          â–¡


Joint inference. In the below, we explain how to obtain joint distribution of the LP-B estimator
(ğ‘ ğ‘ Ì‚0ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ , ğ‘ ğ‘ Ì‚1ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ , â€¦ , ğ‘ ğ‘ Ì‚ğ»ğ»ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ )â€² . Results for the LP-A estimator can be obtained similarly.
                                                            ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½
         We consider augmented moment conditions that ğ¸ğ¸ï¿½ğ‘”ğ‘”ğ‘¡ğ‘¡+ğ»ğ» (ğœƒğœƒ0 )ï¿½ = 0 where ğœƒğœƒ0 =
(Î’0â€² , â€¦ , Î’ğ»ğ»
            â€²               2
               , ğœğœğ‘¥ğ‘¥2 , ğœğœğ‘£ğ‘£,0          2
                                , â€¦ , ğœğœğ‘£ğ‘£,ğ»ğ» )â€² is a (ğ»ğ» + 1) âˆ— (ğ¼ğ¼ ğ¿ğ¿ğ¿ğ¿ + ğ½ğ½ğ¿ğ¿ğ¿ğ¿ + 3) + 1 dimensional vector, and
                                                                              ğ‘ğ‘ğ‘¡ğ‘¡ ï¿½ğ‘ğ‘0,ğ‘¡ğ‘¡ âˆ’ ğ‘ğ‘ğ‘¡ğ‘¡â€² Î’0 ï¿½
                                                  â›                                        â‹®                            â
                                                  âœ                          ğ‘ğ‘ğ‘¡ğ‘¡ ï¿½ğ‘ğ‘ğ»ğ»,ğ‘¡ğ‘¡ âˆ’ ğ‘ğ‘ğ‘¡ğ‘¡â€² Î’ğ»ğ» ï¿½                âŸ
                                                  âœ                                  ğ‘¥ğ‘¥ğ‘¡ğ‘¡2 âˆ’ ğœğœğ‘¥ğ‘¥2                      âŸ
                                                  âœ                              0                         2            âŸ
                                 ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½
                              ğ‘”ğ‘”ğ‘¡ğ‘¡+ğ»ğ» 0 = âœ (ğœƒğœƒ )
                                                              ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡|ğ‘¡ğ‘¡âˆ’1 âˆ’ ï¿½(ğœ„ğœ„1â€² Î’ğ‘–ğ‘– ) ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– ï¿½ âˆ’ ğœğœğ‘£ğ‘£,0
                                                                                                       2                âŸ.
                                                  âœ                                                                     âŸ
                                                  âœ                            ğ‘–ğ‘–=0                                     âŸ
                                                  âœ                                         â‹®                           âŸ
                                                                                ğ»ğ»                             2
                                                  âœ                                                                     âŸ
                                                          ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+ğ»ğ»|ğ‘¡ğ‘¡âˆ’1 âˆ’ ï¿½(ğœ„ğœ„1â€² Î’ğ‘–ğ‘– ) ğ‘¥ğ‘¥ğ‘¡ğ‘¡+ğ»ğ»âˆ’ğ‘–ğ‘– ï¿½ âˆ’ ğœğœğ‘£ğ‘£,ğ»ğ»
                                                                                                         2

                                                      â                        ğ‘–ğ‘–=0                                     â 

                                                                                                                                                 11
Then it is straightforward to extend Proposition 2 to the joint distribution of (ğ‘ ğ‘ Ì‚0ğ¿ğ¿ğ¿ğ¿ , ğ‘ ğ‘ Ì‚1ğ¿ğ¿ğ¿ğ¿ , â€¦ , ğ‘ ğ‘ Ì‚ğ»ğ»ğ¿ğ¿ğ¿ğ¿ )â€² . In
practice, both ğ¼ğ¼ ğ¿ğ¿ğ¿ğ¿ and ğ½ğ½ğ¿ğ¿ğ¿ğ¿ should be small not to make (ğ»ğ» + 1) âˆ— (ğ¼ğ¼ ğ¿ğ¿ğ¿ğ¿ + ğ½ğ½ğ¿ğ¿ğ¿ğ¿ + 3) + 1 too large
relative to available sample sizes.


Implementation. We discuss how to implement Proposition 2. In the below, we focus on the LP-
B estimator. Again, the LP-A estimator can be implemented in a similar way.
             First of all, we use ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 from Equation (6) instead of ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 in practice.
             We also need to estimate ğ‘‰ğ‘‰â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ because it depends on population parameters. Letâ€™s begin
with Gâ„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ = âˆ’ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘(ğ¼ğ¼â„+1 âŠ— ğ¸ğ¸[ğ‘ğ‘ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘¡ğ‘¡â€² ], ğ¼ğ¼2 ). It is natural to have
                                                                                              ğ‘‡ğ‘‡âˆ’â„
                                                                           1
                                       ğºğºï¿½â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿   = âˆ’ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ ï¿½ğ¼ğ¼â„+1 âŠ—                     ï¿½         ğ‘ğ‘ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘¡ğ‘¡ â€² , ğ¼ğ¼2 ï¿½.
                                                                          ğ‘‡ğ‘‡â„
                                                                                       ğ‘¡ğ‘¡=ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š +1

                                                                                             2 â€²
             The feasible estimator of ğœƒğœƒ is denoted by ğœƒğœƒï¿½ â‰¡ ï¿½Î’
                                                               ï¿½ 0â€² , â€¦ , Î’
                                                                          ï¿½â„â€² , ğœğœï¿½ğ‘¥ğ‘¥2 , ğœğœï¿½ğ‘£ğ‘£,â„ ï¿½ where ğœğœï¿½ğ‘¥ğ‘¥2 =
1                                    1                                                                      2
                             2
     âˆ‘ğ‘‡ğ‘‡ğ‘¡ğ‘¡=1 ğ‘¥ğ‘¥ğ‘¡ğ‘¡2 , and ğœğœï¿½ğ‘£ğ‘£,â„ =         âˆ‘ğ‘‡ğ‘‡âˆ’â„            Ì‚             â„      â€²ï¿½
                                            ğ‘¡ğ‘¡=ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š+1 ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ âˆ‘ğ‘–ğ‘–=0 ğœ„ğœ„1 Î’ğ‘–ğ‘– ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘– ï¿½ . We define (ğ»ğ» + 1) âˆ— (ğ¼ğ¼
                                                                                                    5                         ğ¿ğ¿ğ¿ğ¿
                                                                                                                                   +
ğ‘‡ğ‘‡                                   ğ‘‡ğ‘‡â„

ğ½ğ½ğ¿ğ¿ğ¿ğ¿ + 2) + 2 dimensional vector ğ‘ğ‘ğ‘¡ğ‘¡+â„ as following:
                                                                        ğ‘ğ‘ğ‘¡ğ‘¡ ï¿½ğ‘ğ‘0,ğ‘¡ğ‘¡ âˆ’ ğ‘ğ‘ğ‘¡ğ‘¡â€² Î’ï¿½0ï¿½
                                                â›                                    â‹®              â
                                                âœ                       ğ‘ğ‘ğ‘¡ğ‘¡ ï¿½ğ‘ğ‘â„,ğ‘¡ğ‘¡ âˆ’ ğ‘ğ‘ğ‘¡ğ‘¡â€² Î’ï¿½â„ ï¿½  âŸ
                                     ğ‘ğ‘ğ‘¡ğ‘¡+â„    â‰¡âœ                              ğ‘¥ğ‘¥ğ‘¡ğ‘¡2 âˆ’ ğœğœï¿½ğ‘¥ğ‘¥2       âŸ.
                                                âœ                                        2          âŸ
                                                âœ                  â„                                âŸ
                                                 ï¿½ğ‘“ğ‘“Ì‚ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 âˆ’ ï¿½ï¿½ğœ„ğœ„1â€² Î’
                                                                        ï¿½ğ‘–ğ‘– ï¿½ ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘– ï¿½ âˆ’ ğœğœï¿½ğ‘£ğ‘£,â„
                                                                                                2

                                                â                ğ‘–ğ‘–=0                               â 
     ï¿½ â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ is obtained by applying the Newey-West estimator to ğ‘ğ‘ğ‘¡ğ‘¡+â„ with pre-whitening
Then Î©
similar to Proposition 1.
             It remains to estimate Î”â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ . It is straightforward to define
                                                                                                                    â€²
                                                                                      2ğœ“ğœ“ï¿½ğ‘¥ğ‘¥,0 ğœğœï¿½ğ‘¥ğ‘¥2 ğœ„ğœ„1
                                                                              â›              â‹®                  â
                                                           1   âˆ’ ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿   âœ       2ğœ“ğœ“ï¿½ğ‘¥ğ‘¥,â„ ğœğœï¿½ğ‘¥ğ‘¥2 ğœ„ğœ„1       âŸ
                                             ï¿½â„.ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ =
                                             Î”                                âœ           â„                     âŸ
                                                                   2
                                                               ğœğœï¿½ğ‘“ğ‘“,â„        âœ                                 âŸ
                                                                              âœ         ï¿½ ğœ“ğœ“ï¿½ğ‘¥ğ‘¥,ğ‘–ğ‘–
                                                                                             2
                                                                                                                âŸ
                                                                                        ğ‘–ğ‘–=0
                                                                              ââˆ’ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ /(1 âˆ’ ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ )â 

5
     The denominator ğ‘‡ğ‘‡â„ ight be adjusted according to the degrees of freedom without affecting the asymptotics.

                                                                                                                                 12
                   1
          2
where ğœğœï¿½ğ‘“ğ‘“,â„ =         âˆ‘ğ‘‡ğ‘‡âˆ’â„            Ì‚2                                         ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
                         ğ‘¡ğ‘¡=ğ¿ğ¿ğ‘šğ‘šğ‘šğ‘šğ‘šğ‘š +1 ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 . We plug the bias-corrected ğ‘ ğ‘ Ì‚â„      in the place of ğ‘ ğ‘ â„ . How to
                  ğ‘‡ğ‘‡â„

obtain a bias-corrected estimator ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ in this set-up will be discussed later.
                 ï¿½â„.ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ , the standard error of ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ is given as following:
           Given Î”
                                                      1
                        [ğ‘ ğ‘ . ğ‘’ğ‘’. (ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ )]2 =       ï¿½â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ğºğºï¿½â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½âˆ’1 Î©
                                                         Î”                          ï¿½ â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ğºğºï¿½â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
                                                                                                   â€²         âˆ’1 â€²
                                                                                                            ï¿½ Î”ï¿½â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ .
                                                     ğ‘‡ğ‘‡â„


Bias-correction. Similar to discussion regarding Proposition 1, we conjecture that most of the
finite sample bias is due to the non-linear transformation ğœ‰ğœ‰(â‹…).
           We     approximate            the     asymptotic             variance         of     the      feasible       estimator ğœƒğœƒï¿½ by
1                   âˆ’1                          âˆ’1
      ï¿½ğºğºï¿½â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ Î© ï¿½ â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ğºğºï¿½â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
                                      â€²
                                               ï¿½ . Then we simulate ğœƒğœƒ ğ‘ğ‘ for ğµğµ times from the following normal
ğ‘‡ğ‘‡â„

distribution:
                                                              1                âˆ’1                          âˆ’1
                                 ğœƒğœƒ ğ‘ğ‘ âˆ¼ ğ’©ğ’© ï¿½ğœƒğœƒï¿½ ,               ï¿½ğºğºï¿½â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ Î© ï¿½ â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿ ï¿½ğºğºï¿½â„,ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿
                                                                                                 â€²
                                                                                                          ï¿½ ï¿½.
                                                             ğ‘‡ğ‘‡â„
                                                       2
           We drop cases when simulated ğœğœï¿½ğ‘¥ğ‘¥2 and ğœğœï¿½ğ‘£ğ‘£,â„ are negative. The bias is estimated by
                                                                                                          ğµğµ
                                         ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½                                       ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½      1
                         ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘ â„ğ¿ğ¿ğ¿ğ¿ â‰¡ ğœ‰ğœ‰(ğœƒğœƒ            ï¿½ ï¿½,
                                               ğ‘ğ‘ ) âˆ’ ğœ‰ğœ‰ï¿½ğœƒğœƒ             where          ğœ‰ğœ‰(ğœƒğœƒ ğ‘ğ‘ ) â‰¡    ï¿½ ğœ‰ğœ‰(ğœƒğœƒ ğ‘ğ‘ ).
                                                                                                    ğµğµ
                                                                                                         ğ‘ğ‘=1

           Finally, the bias-corrected estimator is given by
                                                                                          ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
                                        ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ = ğœ‰ğœ‰ï¿½ğœƒğœƒï¿½ ï¿½ âˆ’ ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘ â„ğ¿ğ¿ğ¿ğ¿ = 2ğœ‰ğœ‰ï¿½ğœƒğœƒï¿½ ï¿½ âˆ’ ğœ‰ğœ‰(ğœƒğœƒ ğ‘ğ‘ ).




                                                                                                                                     13
Appendix C. Finding a MA(âˆ) representation for a process driven by
multiple underlying shocks
Suppose the following data generating process as in Section II. In this section, we explain how an
infinite-order MA representation driven by a single white noise process is obtained for the residual
process Î”ğ‘ğ‘ğ‘¡ğ‘¡ + Î”ğ‘ğ‘ğ‘¡ğ‘¡ .
                                              ğ‘¦ğ‘¦ğ‘¡ğ‘¡ = ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘§ğ‘§ğ‘¡ğ‘¡            where              ğ‘§ğ‘§ğ‘¡ğ‘¡ = ğ‘ğ‘ğ‘¡ğ‘¡ + ğ‘ğ‘ğ‘¡ğ‘¡ ,
                                                                                                   ğ‘ğ‘           ğ‘ğ‘
                                ï¿½Î”ğ‘ğ‘ğ‘¡ğ‘¡ âˆ’ ğ‘”ğ‘”ğ‘¦ğ‘¦ ï¿½ = ğœŒğœŒğ‘ğ‘ ï¿½Î”ğ‘ğ‘ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ‘”ğ‘”ğ‘¦ğ‘¦ ï¿½ + ğœğœğ‘ğ‘ ğ‘’ğ‘’ğ‘¡ğ‘¡ ,                        ğ‘’ğ‘’ğ‘¡ğ‘¡ ~ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– ğ‘ğ‘(0,1),
                                                ğ‘ğ‘ğ‘¡ğ‘¡ = ğœŒğœŒğ‘ğ‘ ğ‘ğ‘ğ‘¡ğ‘¡âˆ’1 + ğœğœğ‘ğ‘ ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ ,              ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ ~ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– ğ‘ğ‘(0,1),
                                                                             ğ‘ğ‘
                       ğ‘¥ğ‘¥ğ‘¡ğ‘¡ ~ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– ğ‘ğ‘(0, ğœğœğ‘¥ğ‘¥2 ),         and {ğ‘¥ğ‘¥ğ‘¡ğ‘¡ }, ï¿½ğ‘’ğ‘’ğ‘¡ğ‘¡ ï¿½ and {ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ } are mutually independent.
             We first show why having a representation ğ‘”ğ‘”ğ‘¦ğ‘¦ + ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿)ğ‘’ğ‘’ğ‘¡ğ‘¡ of Î”ğ‘ğ‘ğ‘¡ğ‘¡ + Î”ğ‘ğ‘ğ‘¡ğ‘¡ is needed. When
                                                                                        ï¿½t =
all three shocks are in the information set, then the corresponding forecast error with Î©
                  ğ‘ğ‘
{ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ , ğ‘’ğ‘’ğ‘¡ğ‘¡ , ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ , â€¦ } is
                                  ğ‘“ğ‘“Ìƒğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ Eï¿½ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ |Î©
                                                                    ï¿½ ğ‘¡ğ‘¡âˆ’1 ï¿½ = ğœ“ğœ“ğ‘¥ğ‘¥,0 ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ + â‹¯ + ğœ“ğœ“ğ‘¥ğ‘¥,â„ ğ‘¥ğ‘¥ğ‘¡ğ‘¡
                                         ğ‘ğ‘                            ğ‘ğ‘                                                           ğ‘ğ‘
                               +ğœğœğ‘ğ‘ ğ‘’ğ‘’ğ‘¡ğ‘¡+â„ + ï¿½1 + ğœŒğœŒğ‘ğ‘ ï¿½ğœğœğ‘ğ‘ ğ‘’ğ‘’ğ‘¡ğ‘¡+â„âˆ’1 + â‹¯ + ï¿½1 + ğœŒğœŒğ‘ğ‘ + â‹¯ + ğœŒğœŒğ‘ğ‘â„ ï¿½ğœğœğ‘ğ‘ ğ‘’ğ‘’ğ‘¡ğ‘¡
                                                           ğ‘ğ‘                 ğ‘ğ‘
                                                  +ğœğœğ‘ğ‘ ğ‘’ğ‘’ğ‘¡ğ‘¡+â„ + ğœŒğœŒğ‘ğ‘ ğœğœğ‘ğ‘ ğ‘’ğ‘’ğ‘¡ğ‘¡+â„âˆ’1 + â‹¯ + ğœŒğœŒğ‘ğ‘â„ ğœğœğ‘ğ‘ ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ .
Then
                                                                            ï¿½âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–
                                                                                      2
                                                                                            ï¿½ ğœğœğ‘¥ğ‘¥2
                                   ğ‘ ğ‘ Ìƒâ„ =                                                               2                       .
                                                                                         ğ‘—ğ‘—
                                              ï¿½âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–
                                                        2
                                                              ï¿½ ğœğœğ‘¥ğ‘¥2 + âˆ‘â„ğ‘–ğ‘–=0ï¿½âˆ‘ğ‘–ğ‘–ğ‘—ğ‘—=0 ğœŒğœŒğ‘ğ‘ ï¿½ ğœğœğ‘ğ‘2 + âˆ‘â„ğ‘–ğ‘–=0 ğœŒğœŒğ‘ğ‘2ğ‘–ğ‘– ğœğœğ‘ğ‘2
             However, what we estimate in the simulations using Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ and ğ‘¥ğ‘¥ğ‘¡ğ‘¡ is ğ‘ ğ‘ â„ , not ğ‘ ğ‘ Ìƒâ„ . It is because
                                                   ï¿½ ğ‘¡ğ‘¡ . Because Î©ğ‘¡ğ‘¡ is coarser than Î©
our information set is Î©ğ‘¡ğ‘¡ , not the augmented one Î©                                  ï¿½ ğ‘¡ğ‘¡ , ğ‘ ğ‘ â„ â‰¤ ğ‘ ğ‘ Ìƒâ„ . To
obtain the true value of ğ‘ ğ‘ â„ , we need ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿) and ğœğœğ‘’ğ‘’ .
             We use a stationary Kalman filter (Hamilton (1994), pp.391-394) to that end. We cast the
above process in a form of state-space representation.
State equation:
                                                                   ğ‘ ğ‘ ğ‘¡ğ‘¡ = ğ¹ğ¹ğ‘ ğ‘ ğ‘¡ğ‘¡âˆ’1 + ğµğµğµğµğ‘¡ğ‘¡ ,
                                                    where             ğ‘ ğ‘ ğ‘¡ğ‘¡ = (ğ›¥ğ›¥ğ‘ğ‘ğ‘¡ğ‘¡ âˆ’ ğ‘”ğ‘”ğ‘¦ğ‘¦ , ğ›¥ğ›¥ğ‘ğ‘ğ‘¡ğ‘¡ , ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ )â€²,
                                                                              ğ‘ğ‘            â€²
                                                                ğ‘¢ğ‘¢ğ‘¡ğ‘¡ = ï¿½ğ‘’ğ‘’ğ‘¡ğ‘¡ , ğ‘’ğ‘’ğ‘¡ğ‘¡ğ‘ğ‘ ï¿½ ~(0, ğ¼ğ¼),
                                                                   ğœŒğœŒğ‘ğ‘               0          0
                                                                F=ï¿½0                 ğœŒğœŒğ‘ğ‘       âˆ’ğœğœğ‘ğ‘ ï¿½,
                                                                    0                 0          0
                                                                                                                                         14
                                                        ğœğœğ‘ğ‘       0
                                                     B=ï¿½0         ğœğœğ‘ğ‘ ï¿½.
                                                         0         1
Measurement equation: Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ = ğ‘”ğ‘”ğ‘¦ğ‘¦ + ğ»ğ» â€² ğ‘ ğ‘ ğ‘¡ğ‘¡               where          ğ»ğ» = (1,1,0)â€² .
         By defining ğ‘„ğ‘„ = ğµğµğµğµğµğµâ€² = ğµğµğµğµâ€² and ğ‘…ğ‘… = 0, the stationary ğ‘ƒğ‘ƒ and ğ¾ğ¾ are obtained from the
matrix equation (13.5.3) and (13.5.4) on Hamilton (1994).
                                   ğ‘ƒğ‘ƒ = ğ¹ğ¹[ğ‘ƒğ‘ƒ âˆ’ ğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ»ğ» â€² ğ‘ƒğ‘ƒğ‘ƒğ‘ƒ + ğ‘…ğ‘…)âˆ’1 ğ»ğ» â€² ğ‘ƒğ‘ƒ]ğ¹ğ¹ â€² + ğ‘„ğ‘„,
                                              ğ¾ğ¾ = ğ¹ğ¹ğ¹ğ¹ğ¹ğ¹(ğ»ğ» â€² ğ‘ƒğ‘ƒğ‘ƒğ‘ƒ + ğ‘…ğ‘…)âˆ’1 .
This is a discrete time algebraic Riccati equation for ğ‘ƒğ‘ƒ which can be solved numerically. Then
deriving ğ¾ğ¾ is straightforward from the second equation. Given ğ¾ğ¾, it is known that
    Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ = ğ‘”ğ‘”ğ‘¦ğ‘¦ + (ğ¼ğ¼ + ğ»ğ» â€² (ğ¼ğ¼ âˆ’ ğ¹ğ¹ğ¹ğ¹)âˆ’1 ğ¾ğ¾ğ¾ğ¾)ğ‘’ğ‘’ğ‘¡ğ‘¡ ,      ğ‘’ğ‘’ğ‘¡ğ‘¡ ~ğ‘Šğ‘Šğ‘Šğ‘Š(ğœğœğ‘’ğ‘’2 ),     and      ğœğœğ‘’ğ‘’ = âˆšğ»ğ» â€² ğ‘ƒğ‘ƒğ‘ƒğ‘ƒ + ğ‘…ğ‘….
         To convert (ğ¼ğ¼ + ğ»ğ» â€² (ğ¼ğ¼ âˆ’ ğ¹ğ¹ğ¹ğ¹)âˆ’1 ğ¾ğ¾ğ¾ğ¾) into ğœ“ğœ“ğ‘’ğ‘’ (ğ¿ğ¿), we use the identity that (ğ¼ğ¼ âˆ’ ğ¹ğ¹ğ¹ğ¹)âˆ’1 = ğ¼ğ¼ +
ğ¹ğ¹ğ¹ğ¹ + ğ¹ğ¹ 2 ğ¿ğ¿2 + â‹¯. Note all three eigenvalues of ğ¹ğ¹, ğœŒğœŒğ‘ğ‘ , ğœŒğœŒğ‘ğ‘ and 0, are less than one in absolute values.
         Given the MA representation of Î”ğ‘§ğ‘§ğ‘¡ğ‘¡ , we can find ğ‘ ğ‘ â„ accordingly.
         In Section III.B, the model of Smets and Wouters (2007) is analyzed. We find the ğ‘ ğ‘ â„ under
the assumed information set in a similar way.




                                                                                                                        15
Appendix D. Unobservable Shocks and Measurement Errors
In some cases, an estimated structural shock is only a part of the true shock that can be identified
with high confidences. For example, unexplained innovations in the Federal Funds Rates from
Romer and Romer (2004) may be a part of the entire change in the monetary policy including
changes in members on the board of governors, change in institutional details, or regime shifts as
in the Volcker periods. Similarly, legislative tax changes identified from narratives by Romer and
Romer (2010) would be understood as a part of the whole fiscal policy shocks affecting the U.S.
economy. The measurement error is yet another potential issue. It is unavoidable in practical
studies, especially when shocks are generated from narratives like Ramey (2011) and Romer and
Romer (2010). In this section, we show that our approach can still provide interesting and
meaningful quantities, because the estimates can be understood as a conservative estimate of the
â€˜trueâ€™ estimates available only when all hidden confounding factors are observable.
              We decompose the true shock into two components ğ‘¥ğ‘¥ğ‘¡ğ‘¡ = ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ + ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ . The superscript o means
observable, and u unobservable. We understand the vector process of two components has a
representation as
                                                            ğœğœğ‘œğ‘œ                   0
                                                ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ
                                               ï¿½ ğ‘¢ğ‘¢ ï¿½ = ï¿½                                    ï¿½ ğ›¿ğ›¿ğ‘¡ğ‘¡ ,
                                                ğ‘¥ğ‘¥ğ‘¡ğ‘¡     ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ ğœğœğ‘¢ğ‘¢            2 ğœğœ
                                                                          ï¿½1 âˆ’ ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ ğ‘¢ğ‘¢

                                                                           1/2                            1/2
where              ğ›¿ğ›¿ğ‘¡ğ‘¡ ~ ğ‘¤ğ‘¤ğ‘¤ğ‘¤(ğ¼ğ¼2 ), ğ›¿ğ›¿ğ‘¡ğ‘¡ âŠ¥ ğ‘’ğ‘’ğ‘¡ğ‘¡ , ğœğœğ‘œğ‘œ = ï¿½ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ )ï¿½     , ğœğœğ‘¢ğ‘¢ = ï¿½ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘Ÿğ‘Ÿ(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ )ï¿½     ,   and   ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ =
ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ ).
              Three different situations are possible for the sign of correlation between two components,
(1) ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ ) = 0: they are orthogonal, (2) ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ ) > 0: this might be the case when we
are able to observe only some parts of the true shock, and (3) ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ ) < 0: the presence of
measurement error ğ‘šğ‘šğ‘¡ğ‘¡ might impose such a correlation structure, since ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ = ğ‘¥ğ‘¥ğ‘¡ğ‘¡ + ğ‘šğ‘šğ‘¡ğ‘¡ , and ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ =
âˆ’ğ‘šğ‘šğ‘¡ğ‘¡ . Narrative approaches might be exposed to such a concern.
              Our claim is that the suggested estimators have a negative asymptotic bias, regardless of
the sign of correlation between two components. 6 The population variance share can be written as
a fraction of the amount explained by the innovations in {ğ‘¥ğ‘¥ğ‘¡ğ‘¡ } to the variance of forecast error, ğ‘ ğ‘ â„ =



6
 All of our estimators have the same probability limit. So, the discussion in this section applies to R2, LP-A, and
LP-B estimators.

                                                                                                                               16
    âˆ‘â„      2       2
     ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– ğœğœğ‘¥ğ‘¥
                         . We argue that there are (1) a positive asymptotic bias for the denominator, and (2) a
ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 )

negative asymptotic bias for the numerator when we apply our method to {ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ } only while ignoring
the existence of {ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ }. Therefore, the estimated share can be understood as a conservative estimate
for the population quantity with the full information about {ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ } and {ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ }.
                Letâ€™s start with the denominator. The first problem we encounter is about recovering the
forecast errors. It will be proven that the estimated forecast error without information on {ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ } will
have larger variances than the â€˜trueâ€™ forecast error ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 . With the full information, we can back
out the forecast error as a difference between ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 and its projected values on the
                                                          ğ‘œğ‘œ       ğ‘¢ğ‘¢                 ğ‘œğ‘œ       ğ‘¢ğ‘¢
information set at time ğ‘¡ğ‘¡ âˆ’ 1 where Î©ğ‘¡ğ‘¡âˆ’1 = {Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1 , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’2 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’2 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’2 , â€¦ } as
                                        ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ¸ğ¸(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |Î©ğ‘¡ğ‘¡âˆ’1 )
where the latter conditional expectation is the projection of ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 on the closed subspace
spanned by Î©ğ‘¡ğ‘¡âˆ’1 . However, an econometrician has only Î©ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 = {ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’1
                                                                    ğ‘œğ‘œ                  ğ‘œğ‘œ
                                                                         , ğ›¥ğ›¥ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 , ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’2 , ğ›¥ğ›¥ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’2 , â€¦ }. It is
evident that Î©ğ‘’ğ‘’ğ‘¡ğ‘¡ âŠ‚ Î©ğ‘¡ğ‘¡ . We define the closed subspaces spanned by variables in each information set as
                                                      ğ‘‰ğ‘‰ğ‘¡ğ‘¡ = ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (Î©ğ‘¡ğ‘¡ ) ),
                                                      ğ‘‰ğ‘‰ğ‘¡ğ‘¡ğ‘’ğ‘’ = ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ï¿½ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (Î©ğ‘’ğ‘’ğ‘¡ğ‘¡ )ï¿½.
                We now compare the true forecast errors and identifiable ones to econometricians having
Î©ğ‘’ğ‘’ğ‘¡ğ‘¡âˆ’1 . By using a notation of ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡(ğ‘ ğ‘ |ğ‘†ğ‘†) to denote the population least square projection of
the element s on the closed subspace ğ‘†ğ‘†, we can rewrite ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 as
ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 ) = ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |(ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 )âŠ¥ )
where ğ‘‰ğ‘‰ğ‘¡ğ‘¡âŠ¥ is the orthogonal complement of ğ‘‰ğ‘‰ğ‘¡ğ‘¡ .
                                                                          ğ‘’ğ‘’
                On the other hand, the econometricianâ€™s forecast error ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 is given by
                   ğ‘’ğ‘’                                                                    ğ‘’ğ‘’ )                    ğ‘’ğ‘’
                ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 âˆ’ ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1  = ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 + ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1
                     ğ‘’ğ‘’                                                                                                   ğ‘’ğ‘’ ).
where             ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 â‰¡ ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 ) âˆ’ ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1
                                                               ğ‘’ğ‘’
                It is worth to mention that ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 and ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 are orthogonal, because the first term is an
                                                                          ğ‘’ğ‘’
element of (ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 )âŠ¥ , and the second of ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 . 7 Therefore, ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ = ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ +



7
  This result is in fact due to a decomposition of the entire vector space, V, into a direct sum of three mutually
                                                         ğ‘’ğ‘’                       ğ‘’ğ‘’ )âŠ¥ )
orthogonal closed subspaces as V = ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1                     âŠ• (ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 âˆ© (ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1          âŠ• (ğ‘‰ğ‘‰ âˆ© (ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 )âŠ¥ ), where the symbol â€˜âŠ•â€™ means a
direct sum. From the representation, it directly follows that ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 = ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰) =
                                         ğ‘’ğ‘’ )                                                      ğ‘’ğ‘’ )âŠ¥ )
ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1  + ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 âˆ© (ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1     +

                                                                                                                                         17
          ğ‘’ğ‘’
ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ â‰¥ ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½. Also, the equality holds only when ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ and its lagged values have
                                                  ğ‘’ğ‘’
no additional power in explaining ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ given ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 implying ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 ) =
                                         ğ‘’ğ‘’ ).
ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1   This is not true except for some uninteresting special situations
such as ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿) = 0 or ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ ) = Â±1.
             The second step is to show the econometricianâ€™s numerator converges to a values less than
the true numerator in probability. The true numerator is âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–
                                                                  2
                                                                        ğœğœğ‘¥ğ‘¥2 as before. Defining ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ =
(ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„ , â€¦ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ )â€² , we can write it as following:
                                                                   â€²                 â€² âˆ’1
                                          ğ¸ğ¸ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 â‹… ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½
                                                                       â€²
                                   â€² âˆ’1                                                  â€²                   â€² âˆ’1
             = ï¿½ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ï¿½ ï¿½ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ï¿½ ï¿½ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ï¿½.

The term inside the last square bracket is a vector of population regression coefficients of ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1
                                                                                                                            â€²
on ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ . By the specification, we know that it is equal to Î¨ â„ = ï¿½ğœ“ğœ“ğ‘¥ğ‘¥,0 , â€¦ , ğœ“ğœ“ğ‘¥ğ‘¥,â„ ï¿½ .
                                                                                                                                              ğ‘’ğ‘’
             Now            we             investigate             the         econometricianâ€™s                 numerator              ğ¸ğ¸ ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 â‹…
      â€²                         â€² âˆ’1
ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ï¿½ ğ¸ğ¸ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ï¿½        ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1
                                                           ğ‘’ğ‘’
                                                                    ï¿½ where ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ = (ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„
                                                                                          ğ‘œğ‘œ                                ğ‘’ğ‘’
                                                                                               , â€¦ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ )â€² . Because ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 =
                                                                           ğ‘’ğ‘’ )âŠ¥ )
ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ğ‘ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡( ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 ) | (ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1           ğ‘’ğ‘’ )âŠ¥
                                                                                   âˆˆ (ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1                        ,       ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+ğ‘¡ğ‘¡|ğ‘¡ğ‘¡âˆ’1
                                                                                                                                             ğ‘’ğ‘’
                                                                                                                                                       ï¿½=

ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½. The corresponding projection coefficient is

                               â€² âˆ’1                                                  ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ )   ï¿½ğœğœğ‘œğ‘œ + ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ â‹… ğœğœğ‘¢ğ‘¢ ï¿½ â„
          ğ¸ğ¸ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ï¿½       ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ = ï¿½1 +                          ğ‘œğ‘œ ) ï¿½ Î¨ =
                                                                                                              â„
                                                                                                                                        Î¨ .
                                                                                        ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ‘¥ğ‘¥ğ‘¡ğ‘¡                     ğœğœğ‘œğ‘œ
We used the fact that ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 = âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– ğ‘¥ğ‘¥ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘– + âˆ‘â„ğ‘–ğ‘–=0 âˆ‘ğ‘–ğ‘–ğ‘—ğ‘—=0(ğœ“ğœ“ğ‘’ğ‘’.ğ‘—ğ‘— ) ğ‘’ğ‘’ğ‘¡ğ‘¡+â„âˆ’ğ‘–ğ‘– .
             Finally, the econometricianâ€™s numerator becomes
                              âˆ’1                                   â€²
                          â€²                                                              â€²                          â€² âˆ’1
  ï¿½ğ¸ğ¸ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’   ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ï¿½        ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’   â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ï¿½ ï¿½ğ¸ğ¸ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ï¿½ï¿½ ï¿½ğ¸ğ¸ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ï¿½      ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½ï¿½
                                                                                                       â„
                ï¿½ğœğœğ‘œğ‘œ + ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ â‹… ğœğœğ‘¢ğ‘¢ ï¿½ â„ â€²            ï¿½ğœğœğ‘œğ‘œ + ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ â‹… ğœğœğ‘¢ğ‘¢ ï¿½ â„       2                             2
              =                         ğ›¹ğ›¹ â‹… ğœğœğ‘œğ‘œ2 ğ¼ğ¼ â‹…                         ğ›¹ğ›¹ = ï¿½ ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘– ï¿½ğœğœğ‘œğ‘œ + ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ â‹… ğœğœğ‘¢ğ‘¢ ï¿½ .
                        ğœğœğ‘œğ‘œ                                    ğœğœğ‘œğ‘œ
                                                                                                      ğ‘–ğ‘–=0

Thus, any asymptotic bias in the numerators are from the differences between ğœğœğ‘¥ğ‘¥2 and
                          2                                                      2
ï¿½ğœğœğ‘œğ‘œ + ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ â‹… ğœğœğ‘¢ğ‘¢ ï¿½ . Because ğœğœğ‘¥ğ‘¥2 âˆ’ ï¿½ğœğœğ‘œğ‘œ + ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ â‹… ğœğœğ‘¢ğ‘¢ ï¿½ = ï¿½1 âˆ’ ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢
                                                                             2
                                                                                   ï¿½ğœğœğ‘¢ğ‘¢2 ,



                                                                                                   ğ‘’ğ‘’ )      ğ‘’ğ‘’
ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ âˆ© (ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1 )âŠ¥ ) = ğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒğ‘ƒ(ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 |ğ‘‰ğ‘‰ğ‘¡ğ‘¡âˆ’1  + ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 + ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 , and the last three
terms are mutually orthogonal.

                                                                                                                                                      18
                                                             â€²                 â€² âˆ’1
                                  ğ¸ğ¸ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 â‹… ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ ï¿½ ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„ â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½
                                                                                                         â„
                                       â€²                         â€² âˆ’1
                  ğ‘’ğ‘’
         = ğ¸ğ¸ ï¿½ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 â‹…   ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ï¿½ ğ¸ğ¸ ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’   ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’ ï¿½     ğ¸ğ¸ï¿½ğ‘‹ğ‘‹ğ‘¡ğ‘¡â„,ğ‘’ğ‘’        ğ‘’ğ‘’
                                                                                      â‹… ğ‘“ğ‘“ğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1         2
                                                                                                    ï¿½ + ï¿½ ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–        2
                                                                                                                  ï¿½1 âˆ’ ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ ï¿½ğœğœğ‘¢ğ‘¢2 .
                                                                                                        ğ‘–ğ‘–=0

          As claimed, the econometricianâ€™s numerator is asymptotically less and denominator is
asymptotically greater than their full information counterparts. Thus, we have a negative
asymptotic bias. So, we can understand our method as a conservative estimator for the true ğ‘ ğ‘ â„ .
Moreover, the size of bias becomes small when the observed and unobserved parts are highly
correlated, or variance of the unobserved parts is small. In such a case, both biases for the
                      ğ‘’ğ‘’
denominator ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ï¿½ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 ï¿½, and the numerator âˆ‘â„ğ‘–ğ‘–=0 ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–
                                                             2            2
                                                                   ï¿½1 âˆ’ ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ ï¿½ğœğœğ‘¢ğ‘¢2 are small. 8




8
  In the above, we assume that ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ and ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ have the sample impulse response polynomial ğœ“ğœ“ğ‘¥ğ‘¥ (ğ¿ğ¿) for simplicity. Instead,
we may consider ğœ“ğœ“ğ‘¥ğ‘¥ğ‘œğ‘œ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘œğ‘œ + ğœ“ğœ“ğ‘¥ğ‘¥ğ‘¢ğ‘¢ (ğ¿ğ¿)ğ‘¥ğ‘¥ğ‘¡ğ‘¡ğ‘¢ğ‘¢ . This does not change our result, and the above derivations admit a straight-
forward extension to the general case. In such a case, the difference between two numerators becomes
         ğ‘¢ğ‘¢ 2
âˆ‘â„ğ‘–ğ‘–=0ï¿½ğœ“ğœ“ğ‘¥ğ‘¥,ğ‘–ğ‘–          2
               ï¿½ ï¿½1 âˆ’ ğœŒğœŒğ‘œğ‘œ,ğ‘¢ğ‘¢ ï¿½ğœğœğ‘¢ğ‘¢2 . Therefore, we can conclude that if the unobservable component has a less contribution to
the endogenous variable than the observable component, the bias in the numerator would be small.

                                                                                                                                          19
Appendix E. Supplementary Figures for Univariate Simulations

Details on VAR-based Bootstrap
We first choose ğ¿ğ¿ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ using the HQIC. VAR models for (ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ )â€² are estimated for different lag
lengths between 1 and 10. The information criterion is given by
                                                               2ğ‘˜ğ‘˜ log(log(ğ‘‡ğ‘‡))
                                          log(ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘ğ‘‘(ğ‘‰ğ‘‰)) +
                                                                      ğ‘‡ğ‘‡
where ğ‘‰ğ‘‰ is the estimated variance matrix of the reduced-form residual process, ğ‘˜ğ‘˜ is the number of
parameters, in this bi-variate case, 4 times lag length, and ğ‘‡ğ‘‡ is the sample size. For a fair comparison,
we adjust the initial observation across ğ¿ğ¿ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ and make the effective sample sizes same. Once it is
selected as a minimizer of the HQIC, both ğ¿ğ¿ğ‘¥ğ‘¥ and ğ¿ğ¿ğ‘¦ğ‘¦ are set to ğ¿ğ¿ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ .
         We use the estimated ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ¿ğ¿ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ ) model to bootstrap. First of all, we randomly choose ğ‘¡ğ‘¡
                                                                                   â€²
between 1 and ğ‘‡ğ‘‡ âˆ’ ğ¿ğ¿ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ . Then (ğ‘¥ğ‘¥ğ‘¡ğ‘¡ , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡ )â€² , â€¦ , ï¿½ğ‘¥ğ‘¥ğ‘¡ğ‘¡+ğ¿ğ¿ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ , Î”ğ‘¦ğ‘¦ğ‘¡ğ‘¡+ğ¿ğ¿ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ ï¿½ are used as an initial condition
when simulating the bootstrapped time series.
         Second, we randomly draw the reduced form residuals with replacement. Using the
estimated model with the above initial conditions and shuffled residuals, artificial data points are
generated. The first ğ‘‡ğ‘‡ğµğµğµğµğµğµğµğµğµğµ number of observations are discarded as burn-in. ğ‘‡ğ‘‡ğµğµğµğµğµğµğµğµğµğµğµğµ = 100 in all
simulations.
         We apply our estimators to the bootstrapped time series obtaining ğ‘ ğ‘ Ì‚â„ğ‘…ğ‘…2,ğ‘ğ‘ , ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿,ğ‘ğ‘ , and ğ‘ ğ‘ Ì‚â„ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿,ğ‘ğ‘
for ğ‘ğ‘ = 1, â€¦ , 2000. Given ğ‘ ğ‘ Ì‚â„ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ obtained from the estimated ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ¿ğ¿ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ ) model, the biases for
local projection-based estimators are obtained by
                                                      ğµğµ
                                                  1
                                                     ï¿½ ğ‘ ğ‘ Ì‚â„ğ‘šğ‘š,ğ‘ğ‘ âˆ’ ğ‘ ğ‘ Ì‚â„ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
                                                  ğµğµ
                                                     ğ‘ğ‘=1

for ğ‘šğ‘š = ğ‘…ğ‘…2, ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿, and ğ¿ğ¿ğ¿ğ¿ğ¿ğ¿.
         For other details regarding simulations, see Section III.




                                                                                                                         20
How to read the legend:
   1. Impulse response
       -   The 90% bands are based on 5% and 95% quantiles of estimates across 2,000
           replications.


   2. Variance decomposition, Coverage probability, and Root MSE
   -   â€˜R2-VARâ€™ means the bias-corrected R2 estimator by bootstrapping an estimated VAR
       model. Its standard error is the standard deviation across bootstrap estimates.
   -   â€˜R2-Simâ€™ uses the method discussed in Appendix B. The coverage probability is based
       on the asymptotic standard error with pre-whitening as discussed in Appendix B.
   -   â€˜R2â€™ denotes for the estimator without any finite sample correction. It uses the same
       standard error as â€˜R2-VAR.â€™
   -   â€˜LP A/B-VARâ€™, â€˜LP A/B-Simâ€™, â€˜LP A/Bâ€™, â€˜VAR-VARâ€™, and â€˜VARâ€™ are defined
       similarly.




                                                                                               21
DGP 1, T = 160.




                  22
DGP1, T = 500.




                 23
DGP2, T = 160.




                 24
DGP2, T= 500.




                25
DGP3, T = 160, VAR(HQIC).




                            26
DGP3, T = 160, VAR(5).




                         27
DGP3, T = 160, VAR(10).




                          28
DGP3, T = 500, VAR(HQIC).




                            29
Appendix F. Supplementary Figures for Multivariate Simulations

For details regarding simulations, see Section III.
How to read the legend:
   1. Impulse response
       -   The 90% bands are based on 5% and 95% quantiles of estimates across 2,000
           replications.


   2. Variance decomposition, Coverage probability, and Root MSE
       -   â€˜R2-VARâ€™ means the bias-corrected R2 estimator by bootstrapping an estimated VAR
           model. Its standard error is the standard deviation across bootstrap estimates.
       -    â€˜R2-Simâ€™ uses the method discussed in Appendix B. The coverage probability is
           based on the asymptotic standard error with pre-whitening as discussed in Appendix
           B.
       -    â€˜R2â€™ denotes for the estimator without any finite sample correction. It uses the same
           standard error as â€˜R2-VAR.â€™
       -   â€˜LP A/B-VARâ€™, â€˜LP A/B-Simâ€™, â€˜LP A/Bâ€™, â€˜VAR-VARâ€™, and â€˜VARâ€™ are defined
           similarly.




                                                                                                30
Real GDP and monetary policy shock, T = 160




                                              31
Real GDP and monetary policy shock, T = 500.




                                               32
Price inflation and monetary policy shock, T = 160




                                                     33
Price inflation and monetary policy shock, T = 500.




                                                      34
Appendix G. Applications to Real GDP and Inflation
For the LP method, we use different local projections for bias-correction depending on whether
the estimated VAR or simulations are used. For example, suppose that ğ‘¥ğ‘¥ğ‘¡ğ‘¡ is the monetary policy
shock, and ğ‘¦ğ‘¦ğ‘¡ğ‘¡ is the real GDP. When we bootstrap the estimated VAR, the following local
projection is estimated to have bootstrap impulse responses.
                                                                     3
                     b
                   ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„   âˆ’     b
                                ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1   = ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ + ï¿½ ğ›½ğ›½ğ‘–ğ‘–â„ ğ‘¥ğ‘¥ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘–
                                                                        b
                                                                              + ğ¶ğ¶0,1 â‹… ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘¡ğ‘¡ğ‘ğ‘ + ğ¶ğ¶0,2 â‹… ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”â„ğ‘ğ‘ğ‘¡ğ‘¡
                                                                   ğ‘–ğ‘–=0
                                                                                    3
                                                                                       ğ‘ğ‘      â€²
                                            + ğ¶ğ¶0,3 â‹…   ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–bğ‘¡ğ‘¡   + ï¿½ï¿½ğ¶ğ¶ğ‘¡ğ‘¡âˆ’ğ‘–ğ‘– ï¿½ ğ›¤ğ›¤ğ‘–ğ‘–â„ + ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1
                                                                                                         ğ‘ğ‘
                                                                                                                  ,
                                                                                   ğ‘–ğ‘–=1

where ğ¶ğ¶ğ‘¡ğ‘¡ includes ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘ƒğ‘ƒğ‘¡ğ‘¡ , ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”â„ğ‘¡ğ‘¡ , ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘›ğ‘›ğ‘¡ğ‘¡ and ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘’ğ‘’ğ‘¡ğ‘¡ . It corresponds
to VAR(4) with the vector of TFP, output growth, inflation, monetary policy shock, and federal
funds rate.
           On the other hand, we cannot include many variables when we need asymptotic (joint)
variance estimated. In this case, the following regression is estimated.
      b        b
    ğ‘¦ğ‘¦ğ‘¡ğ‘¡+â„ âˆ’ ğ‘¦ğ‘¦ğ‘¡ğ‘¡âˆ’1 = ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ + ğ›½ğ›½0â„ ğ‘¥ğ‘¥ğ‘¡ğ‘¡b + ğ¶ğ¶0,1 â‹… ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘¡ğ‘¡ğ‘ğ‘ + ğ¶ğ¶0,2 â‹… ğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œğ‘œ ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”ğ‘”â„ğ‘ğ‘ğ‘¡ğ‘¡ + ğ¶ğ¶0,3 â‹… ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘ğ‘ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡bğ‘¡ğ‘¡
                                 ğ‘ğ‘
                            + ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡+â„|ğ‘¡ğ‘¡âˆ’1 .
In a similar logic, it corresponds to VAR(1) with the same ordering. Therefore, we preserve the
assumed ordering of variables in both cases.
           When ğ‘¥ğ‘¥ğ‘¡ğ‘¡ is ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘ƒğ‘ƒğ‘¡ğ‘¡ , ğ‘¥ğ‘¥ğ‘¡ğ‘¡ is the only time t variables on the right-hand side making it consistent
to the ordering.
           Figures below show the results for simulation based bias-corrections. Results are similar
to VAR-based ones.




                                                                                                                                                  35
1969:Q1-2007:Q4. Real GDP.


                        Impulse Response, VAR                                       Impulse Response, LP
          5                                                     10



          0                                                       0



         -5                                                    -10
              0           5           10             15   20          0             5             10          15   20

                  Variance Decomposition, VAR-VAR
        0.6
                                                                                            TFP
        0.4
                                                                                            TFP 90% CB
        0.2
                                                                                            MP
          0                                                                                 MP 90% CB

       -0.2
              0           5           10             15   20

                   Variance Decomposition, R2-VAR                         Variance Decomposition, R2-Simulation
        0.6                                                    0.6

        0.4                                                    0.4

        0.2                                                    0.2

          0                                                       0

       -0.2                                                    -0.2
              0           5          10              15   20          0             5             10          15   20

                  Variance Decomposition, LP-A-VAR                    Variance Decomposition, LP-A-Simulation
        0.6                                                    0.6

        0.4                                                    0.4

        0.2                                                    0.2

          0                                                       0

       -0.2                                                    -0.2
              0           5          10              15   20          0             5             10          15   20

                  Variance Decomposition, LP-B-VAR                    Variance Decomposition, LP-B-Simulation
        0.6                                                    0.6

        0.4                                                    0.4

        0.2                                                    0.2

          0                                                       0

       -0.2                                                    -0.2
              0           5          10              15   20          0             5             10          15   20




                                                                                                                        36
1969:Q1-2007:Q4, Inflation.


                         Impulse Response, VAR                                       Impulse Response, LP
         0.5                                                       2



           0                                                       0



        -0.5                                                      -2
               0           5           10             15   20          0             5              10         15   20

                   Variance Decomposition, VAR-VAR
         0.8
         0.6
                                                                                              TFP
         0.4
                                                                                              TFP 90% CB
         0.2
                                                                                              MP
           0
                                                                                              MP 90% CB
        -0.2
               0           5           10             15   20

                    Variance Decomposition, R2-VAR                         Variance Decomposition, R2-Simulation
         0.8                                                    0.8
         0.6                                                    0.6
         0.4                                                    0.4
         0.2                                                    0.2
           0                                                       0
        -0.2                                                    -0.2
               0           5           10             15   20          0             5              10         15   20

                   Variance Decomposition, LP-A-VAR                    Variance Decomposition, LP-A-Simulation
         0.8                                                    0.8
         0.6                                                    0.6
         0.4                                                    0.4
         0.2                                                    0.2
           0                                                       0
        -0.2                                                    -0.2
               0           5          10              15   20          0             5              10         15   20

                   Variance Decomposition, LP-B-VAR                    Variance Decomposition, LP-B-Simulation
         0.8                                                    0.8
         0.6                                                    0.6
         0.4                                                    0.4
         0.2                                                    0.2
           0                                                       0
        -0.2                                                    -0.2
               0           5          10              15   20          0             5              10         15   20




                                                                                                                         37
References
Andrews, Donald W.K., and J. Christopher Monahan, 1992. "An improved heteroskedasticity and
       autocorrelation consistent covariance matrix estimator." Econometrica, 60(4): 953-966.
Brockwell, Peter J., and Richard A. Davis, 1991, Time series: theory and methods. Springer
       Science & Business Media.
Conway, John B., 1990. A course in functional analysis. Vol. 96. Springer Science & Business
       Media.
Hamilton, James D., 1994. Time series analysis. Princeton: Princeton university press.
Hansen, Lars Peter, 1982. "Large sample properties of generalized method of moments
       estimators." Econometrica, 50(4): 1029-1054.
JordÃ , Oscar, 2005. â€œEstimation and Inference of Impulse Responses by Local Projections,â€
       American Economic Review, 95(1): 161â€“182.
Ramey, Valerie A., 2011. "Identifying Government Spending Shocks: It's all in the Timing,"
       Quarterly Journal of Economics, 126(1): 1â€“50.
Romer, Christina, D., and David H. Romer, 2004. "A New Measure of Monetary Shocks:
       Derivation and Implications," American Economic Review, 94(4): 1055-1084.
Romer, Christina D., and David H. Romer, 2010. "The Macroeconomic Effects of Tax Changes:
       Estimates Based on a New Measure of Fiscal Shocks," American Economic Review,
       100(3): 763-801.
Stock, James H., and Mark W. Watson, 2011. Introduction to Econometrics, 3/E. Boston:
       Addison Wesley.
Whitney K. Newey and Kenneth D. West, 1987. "A Simple, Positive Semi-Definite,
       Heteroskedasticity        and        Autocorrelation        Consistent        Covariance
       Matrix." Econometrica, 55(3): 703-708.




                                                                                                38
