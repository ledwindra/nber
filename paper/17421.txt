NBER WORKING PAPER SERIES

IMPROVING GDP MEASUREMENT:
A FORECAST COMBINATION PERSPECTIVE
S. Boragan Aruoba
Francis X. Diebold
Jeremy Nalewaik
Frank Schorfheide
Dongho Song
Working Paper 17421
http://www.nber.org/papers/w17421

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2011

The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research. For research support we thank the National Science Foundation
and the Real-Time Data Research Center at the Federal Reserve Bank of Philadelphia.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2011 by S. Boragan Aruoba, Francis X. Diebold, Jeremy Nalewaik, Frank Schorfheide, and Dongho
Song. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without
explicit permission provided that full credit, including © notice, is given to the source.

Improving GDP Measurement: A Forecast Combination Perspective
S. Boragan Aruoba, Francis X. Diebold, Jeremy Nalewaik, Frank Schorfheide, and Dongho
Song
NBER Working Paper No. 17421
September 2011
JEL No. E01,E32
ABSTRACT
Two often-divergent U.S. GDP estimates are available, a widely-used expenditure side version, GDPE,
and a much less widely-used income-side version GDPI . We propose and explore a "forecast combination"
approach to combining them. We then put the theory to work, producing a superior combined estimate
of GDP growth for the U.S., GDPC. We compare GDPC to GDPE and GDPI , with particular attention
to behavior over the business cycle. We discuss several variations and extensions.

S. Boragan Aruoba
Department of Economics
University of Maryland
3105 Tydings Hall
College Park, MD 20742-7211
aruoba@econ.umd.edu
Francis X. Diebold
Department of Economics
University of Pennsylvania
3718 Locust Walk
Philadelphia, PA 19104-6297
and NBER
fdiebold@sas.upenn.edu
Jeremy Nalewaik
Federal Reserve Board
20th and C, NW
Washington, DC 20551
jeremy.j.nalewaik@frb.gov

Frank Schorfheide
University of Pennsylvania
Department of Economics
3718 Locust Walk
McNeil 525
Philadelphia, PA 19104-6297
and NBER
schorf@ssc.upenn.edu
Dongho Song
University of Pennsylvania
Department of Economics
3718 Locust Walk
Philadelphia, PA 19104
donghos@sas.upenn.edu

“A growing number of economists say that the government should shift its approach to measuring growth. The current system emphasizes data on spending,
but the bureau also collects data on income. In theory the two should match
perfectly – a penny spent is a penny earned by someone else. But estimates of
the two measures can diverge widely, particularly in the short term...”
[Binyamin Appelbaum, New York Times, August 16, 2011]

1

Introduction

GDP growth is surely the most fundamental and important concept in empirical/applied
macroeconomics and business cycle monitoring, yet significant uncertainty still surrounds
its estimation. Two often-divergent estimates exist for the U.S., a widely-used expenditure
side version, GDPE , and a much less-widely-used income-side version, GDPI . Nalewaik
(2010) makes clear that, at the very least, GDPI deserves serious attention and may even
have properties in certain respects superior to those of GDPE . That is, if forced to choose
between GDPE and GDPI , a surprisingly strong case exists for GDPI .
But of course one is not forced to choose between GDPE and GDPI , and a combined
estimate that pools information in the two indicators GDPE and GDPI may improve on
both. In this paper we propose and explore a method for constructing such a combined
estimate, and we compare our new GDPC (“combined”) series to GDPE and GDPI over
many decades, with particular attention to behavior over the business cycle, emphasizing
comparative behavior during turning points.
Our work is motivated by, and builds upon, four key literatures. First, we obviously
build on the literature examining GDPI and its properties, notably Fixler and Nalewaik
(2009) and Nalewaik (2010). Second, our work is related to the literature distinguishing
between “forecast error” and “measurement error” data revisions, as for example in Mankiw
et al. (1984), Mankiw and Shapiro (1986), Faust et al. (2005), and Aruoba (2008). In this
paper we work largely in the forecast error tradition. Third, and related, we work in the
tradition of the forecast combination literature begun by Bates and Granger (1969), viewing
GDPE and GDPI as forecasts of GDP (actually a mix of “backcasts” and “nowcasts” in the
parlance of Aruoba and Diebold (2010)). We combine those forecasts by forming optimally

weighted averages.1 Finally, and most pleasing to us, our work is very much related to Hal
White’s, both in its focus on dynamic modeling and prediction and in its acknowledgment
of misspecification throughout.
We proceed as follows. In section 2 we consider GDP combination under quadratic loss.
This involves taking a stand on the values of certain unobservable parameters (or at least
reasonable ranges for those parameters), but we argue that a “quasi-Bayesian” calibration
procedure based on informed judgment is feasible, credible and robust. In section 3 we
consider GDP combination under minimax loss. Interestingly, as we show, it does not
require calibration. In section 4 we apply our methods to provide improved GDP estimates
for the U.S. In section 5 we sketch several extensions, and we conclude in section 6.

2

Combination Under Quadratic Loss

Optimal forecast combination typically requires knowledge (or, in practice, estimates) of
forecast error properties such as variances and covariances. In the present context, we have
two “forecasts,” of true GDP, namely GDPE and GDPI , but true GDP is never observed,
even after the fact. Hence we never see the “forecast errors,” which complicates matters
significantly but not hopelessly. In particular, in this section we work under quadratic loss
and show that a quasi-Bayesian calibration based on informed judgment is feasible and
credible, and simultaneously, that the efficacy of GDP combination is robust to the precise
weights used.

2.1

Basic Results and Calibration

First assume that the errors in GDPE and GDPI growth are uncorrelated. Consider the
convex combination2
GDPC = λ GDPE + (1 − λ) GDPI ,
1
2

For forecast combination surveys see Diebold and Lopez (1996) and Timmermann (2006).
Throughout this paper, GDP, GDPE and GDPI refer to growth rates.

2

where λ ∈ [0, 1].3 Then the associated errors follow the same weighting,
eC = λeE + (1 − λ)eI ,
where eC = GDP − GDPC , eE = GDP − GDPE and eI = GDP − GDPI . Assume that both
GDPE and GDPI are unbiased for GDP, in which case GDPC is also unbiased, because the
combining weights sum to unity.
Given the unbiasedness assumption, the minimum-MSE combining weights are just the
minimum-variance weights. Immediately, using the assumed zero correlation between the
errors,
σC2 = λ2 σE2 + (1 − λ)2 σI2 ,
(1)
where σC2 = var(eC ), σE2 = var(eE ) and σI2 = var(eI ). Minimization with respect to λ yields
the optimal combining weight,
λ∗ =

1
σI2
=
,
2
2
σI + σE
1 + φ2

(2)

where φ = σE /σI .
It is interesting and important to note that in the present context of zero correlation
between the errors,
var(eE ) + var(eI ) = var(GDPE − GDPI ).
(3)
The standard deviation of GDPE minus GDPI can be trivially estimated. Thus, an expression of a view about φ is in fact implicitly an expression of a view about not only the
ratio of var(eE ) and var(eI ), but about their actual values. We will use this fact (and its
generalization in the case of correlated errors) in several places in what follows.
Based on our judgment regarding U.S. GDPE and GDPI data, which we will subsequently
discuss in detail in section 2.2, we believe that a reasonable range for φ is φ ∈ [.75, 1.45],
with midpoint 1.10.4 One could think of this as a quasi-Bayesian statement that prior beliefs
regarding φ are centered at 1.10, with a ninety percent prior credible interval of [.75, 1.45].
3
Strictly speaking, we need not even impose λ ∈ [0, 1], but λ ∈
/ [0, 1] would be highly non-standard
for two valuable and sophisticated GDP estimates such as GDPE and GDPI . Moreover, as we shall see
subsequently, multiple perspectives suggest that for our application the interesting range of λ is well in the
interior of the unit interval.
4
Invoking equation (3), we see that the midpoint 1.10 corresponds to σI = 1.30 and σE = 1.43, given our
estimate of std(GDPE − GDPI ) = 1.93 percent using data 1947Q2-2009Q3.

3

1.00
0.75
λ

0.50
0.25
0.00
0.7

0.8

0.9

1.0

1.1

1.2

1.3

1.4

1.5

φ

Figure 1: λ∗ vs. φ. λ∗ constructed assuming uncorrelated errors. The horizontal line for visual reference
is at λ∗ = .5. See text for details.

In Figure 1 we graph λ∗ as a function of φ, for φ ∈ [.75, 1.45]. λ∗ is of course decreasing in
φ, but interestingly, it is only mildly sensitive to φ. Indeed, for our range of φ values, the
optimal combining weight remains close to 0.5, varying from roughly 0.65 to 0.30. At the
midpoint φ = 1.10, we have λ∗ = 0.45.
It is instructive to compare the error variance of combined GDP, σC2 , to σE2 for a range
of λ values (including λ = λ∗ , λ = 0, and λ = 1).5 From (1) we have:
σC2
(1 − λ)2
2
=
λ
+
.
σE2
φ2
In Figure 2 we graph σC2 /σE2 for λ ∈ [0, 1] with φ = 1.1. Obviously the maximum variance
reduction is obtained using λ∗ = 0.45, but even for non-optimal λ, such as simple equalweight combination (λ = 0.5), we achieve substantial variance reduction relative to using
GDPE alone. Indeed a key result is that for all λ (except those very close to 1, of course)
we achieve substantial variance reduction.
Now consider the more general and empirically-relevant case of correlated errors. Under
5

2
2
We choose to examine σC
relative to σE
, rather than to σI2 , because GDPE is the “standard” GDP
2
estimate used in practice almost universally. A graph of σC
/σI2 would be qualitatively identical, but the
drop below 1.0 would be less extreme.

4

1.0
0.9
0.8
0.7
0.4

0.5

0.6

σ2c σ2E

0.0

0.2

0.4

0.6

0.8

1.0

λ

Figure 2: σC2 /σE2 for λ ∈ [0, 1]. We assume φ = 1.1 and uncorrelated errors. See text for details.

the same conditions as earlier,
σc2 = λ2 σE2 + (1 − λ)2 σI2 + 2λ(1 − λ)σEI ,
so
λ∗ =
=

(4)

σI2 − σEI
σI2 + σE2 − 2σEI
1 − φρ
,
1 + φ2 − 2φρ

where σEI = cov(eE , eI ) and ρ = corr(eE , eI ).
It is noteworthy that – in parallel to the uncorrelated-error case in which beliefs about
φ map one-for-one into beliefs about σE and σI – beliefs about φ and ρ now similarly map
one-for-one into beliefs about σE and σI . Our definitions of σE2 and σI2 imply that
σj2 = var[GDPj ] − 2cov[GDPj , GDP ] + var[GDP ],

j ∈ {E, I}.

(5)

Moreover, the covariance between the GDPE and GDPI errors can be expressed as
σEI = cov[GDPE , GDPI ] − cov[GDPE , GDP ] − cov[GDPI , GDP ] + var[GDP ].

5

(6)

λ

0.00
1.1

1.3

1.5

0.7

0.9

1.1

φ

φ

ρ = 0.45

ρ = 0.6

1.3

1.5

1.3

1.5

λ

0.00

0.00

0.50

0.50

1.00

0.9

1.00

0.7

λ

0.50

0.50
0.00

λ

1.00

ρ = 0.3

1.00

ρ=0

0.7

0.9

1.1

1.3

1.5

0.7

φ

0.9

1.1
φ

Figure 3: λ∗ vs. φ for Various ρ Values. The horizontal line for visual reference is at λ∗ = .5. See
text for details.

Solving (5) for cov[GDPj , GDP ] and inserting the resulting expressions for j ∈ {E, I}
into (6) yields
σEI



1
2
2
= cov[GDPI , GDPE ] −
var[GDPI ] + var[GDPE ] − σI − σE .
2

(7)

Finally, let σEI = ρσE σI and σE2 = φ2 σI2 . Then we can solve (7) for σI2 :
σI2 =

cov[GDPI , GDPE ] − 21 (var[GDPI ] + var[GDPE ])
N
.
=
1
D
ρφ − 2 (1 + φ2 )

(8)

For given values of φ and ρ we can immediately evaluate the denominator D in (8), and using
data-based estimates of cov[GDPI , GDPE ], var[GDPI ] and var[GDPE ] we can evaluate the
numerator N .
Based on our judgment regarding U.S. GDPE and GDPI data (and again, we will discuss
that judgment in detail in section 2.2), we believe that a reasonable range for ρ is ρ ∈
[0.30, 0.60], with midpoint 0.45. One could think of this as a quasi-Bayesian statement that
prior beliefs regarding ρ are centered at 0.45, with a ninety percent prior credible interval of
6

λ

0.00
0.4

0.5

0.6

0.3

0.4

0.5
ρ

φ = 1.15

φ = 1.25

0.6

λ

0.00

0.00

0.50

0.50

1.00

ρ

1.00

0.3

λ

0.50

0.50
0.00

λ

1.00

φ = 1.05

1.00

φ = 0.95

0.3

0.4

0.5

0.6

0.3

ρ

0.4

0.5

0.6

ρ

Figure 4: λ∗ vs. ρ for Various φ Values. The horizontal line for visual reference is at λ∗ = .5. See
text for details.

[0.30, 0.60].6
In Figure 3 we show λ∗ as a function of φ for ρ = 0, 0.3, 0.45 and 0.6, in Figure 4 we
show λ∗ as a function of ρ for φ = 0.95, 1.05, 1.15 and 1.25, and in Figure 5 we show λ∗ as a
bivariate function of φ and ρ. For φ = 1 the optimal weight is 0.5 for all ρ, but for φ 6= 1 the
optimal weight differs from 0.5 and is more sensitive to φ as ρ grows. The crucial observation
remains, however, that under a wide range of conditions it is optimal to put significant weight
on both GDPE and GDPI , with the optimal weights not differing radically from equality.
Moreover, for all φ values greater than one, so that less weight is optimally placed on GDPE
under a zero-correlation assumption, allowance for positive correlation further decreases the
optimal weight placed on GDPE . For a benchmark calibration of φ = 1.1 and ρ = 0.45,
λ∗ ≈ 0.41.
Let us again compare σC2 to σE2 for a range of λ values (including λ = λ∗ , λ = 0, and
6

Again using GDPE and GDPI data 1947Q2-2009Q3, we obtain for the numerator N = −1.87 in equation
(7) above. And using the benchmark values of φ = 1.1 and ρ = 0.45, we obtain for the denominator
D = −0.61. This implies σI = 1.75 and σE = 1.92. For comparison, the standard deviation of GDPE and
GDPI growth rates is about 4.2. Hence our benchmark calibration implies that the error in measuring true
GDP by the reported GDPE and GDPI growth rates is potentially quite large.

7

0.8
0.6

λ

0.4
0.2
0.0
0.8
0.9
1.0
1.1
1.2
1.3
φ
1.4

0.2

0.3

0.4

0.5

0.6

ρ

Figure 5: λ∗ vs. ρ and φ. See text for details.

λ = 1). From (4) we have:
(1 − λ)2
ρ
σC2
2
=λ +
+ 2λ(1 − λ) .
2
2
σE
φ
φ
In Figure 6 we graph σC2 /σE2 for λ ∈ [0, 1] with φ = 1.1 and ρ = 0.45. Obviously the
maximum variance reduction is obtained using λ∗ = 0.41, but even for non-optimal λ, such
as simple equal-weight combination (λ = 0.5), we achieve substantial variance reduction
relative to using GDPE alone.

2.2

On the Rationale for our Calibration

We have thus far implicitly asked the reader to defer to our judgment regarding calibration,
focusing on φ ∈ [.75, 1.45] and ρ ∈ [0.30, 0.60] with benchmark midpoint values of φ = 1.10
and ρ = 0.45. Here we explain the experience, reasoning, and research that supports that
judgment.
8

1.0
0.9
0.8
0.6

0.7

σ2c σ2E

0.0

0.2

0.4

0.6

0.8

1.0

λ

Figure 6: σC2 /σE2 for λ ∈ [0, 1]. We assume φ = 1.1 and ρ = 0.45. See text for details.

2.2.1

Calibrating φ

The key prior view embedded in our choice of φ ∈ [.75, 1.45], with midpoint 1.10, is that
GDPI is likely a somewhat more accurate estimate than GDPE . This accords with the
results of Nalewaik (2010), who examines the relative accuracy of the GDPE and GDPI in
several ways, with results favorable to GDPI , suggesting φ > 1.
Let us elaborate. The first source of information on likely values of φ is from detailed
examination of the source data underlying GDPE and GDPI . The largest component of
GDPI , wage and salary income, is computed using quarterly data from tax records that are
essentially universe counts, contaminated by neither sampling nor non-sampling errors. Two
other very important components of GDPI , corporate profits and proprietors’ income, are
also computed using annual data from tax records.7 Underreporting and non-reporting of
income on tax forms (especially by proprietors) is an issue with these data, but the statistical
agencies make adjustments for misreporting, and in any event the same misreporting issues
plague GDPE as well as GDPI , as we discuss below.
7

The tax authorities do not release the universe counts for corporate profits and proprietors’ income;
rather, they release results from a random sample of tax returns. But the sample they employ is enormous,
so the variance of the sampling error is tiny for the top-line estimates. Moreover, the tax authorities obviously
know the universe count, so it seems unlikely that they would release tabulations that are very different from
the universe counts.

9

In contrast to GDPI , very little of the quarterly or annual data used to compute GDPE
is based on universe counts.8 Rather, most of the quarterly GDPE source data is from
business surveys where response is voluntary. Non-response rates can be high, potentially
introducing important sample-selection effects that may, moreover, vary with the state of the
business cycle. Much annual GDPE source data is from business surveys with mandatory
response, but some businesses still do not respond to the surveys, and surely the auditing
of these non-respondents is less rigorous than the auditing of tax non-filers. In addition,
even the annual surveys do not attempt to collect data on some types of small businesses,
particularly non-employer businesses (i.e. businesses with no employees). The statistical
agencies attempt to correct some of these omissions by incorporating data from tax records
(making underreporting and non-reporting of income on tax forms an issue for GDPE as
well as GDPI ), but it is not entirely clear whether they adequately plug all the holes in the
survey data.
Although these problems plague most categories of GDPE , some categories appear moreseverely plagued. In particular, over most of history, government statistical agencies have
collected annual source data on less than half of personal consumption expenditures (PCE)
for services, a very large category comprising between a quarter and a half of the nominal value of GDPE over our sample. At the quarterly frequency, statistical agencies have
collected even less source data on services PCE.9 For this reason, statistical agencies have
been forced to cobble together less-reliable data from numerous non-governmental sources
to estimate services PCE.
A second source of information on the relative reliability of GDPE and GDPI is the
correlation of the two measures with other variables that should be correlated with output
growth, as examined in Nalewaik (2010). Nalewaik (2010) is careful to pick variables that
are not used in the construction of either GDPE or GDPI , to avoid spurious correlation
resulting from correlated measurement errors.10 The results are uniformly favorable to GDPI
and suggest that it is a more accurate measure of output growth than GDPE . In particular,
from the mid-1980s to the mid-2000s, the period of maximum divergence between GDPE
and GDPI , Nalewaik (2010) finds that GDPI growth has higher correlation with lagged
stock price changes, the lagged slope of the yield curve, the lagged spread between high8

Motor vehicle sales are a notable exception.
This has begun to change recently, as the Census Bureau has expanded its surveys, but φ is meant to
represent the average relative reliability over the sample we employ, so these facts are highly relevant.
10
For example, the survey of households used to compute the unemployment rate is used in the construction
of neither GDPE nor GDPI , so use of variables from that survey is fine.
9

10

yield corporate bonds and treasury bonds, short and long differences of the unemployment
rate (both contemporaneously and at leads and lags), a measure of employment growth
computed from the same household survey, the manufacturing ISM PMI (Institute for Supply
Management, Purchasing Managers Index) the non-manufacturing ISM PMI, and dummies
for NBER recessions. In addition, lags of GDPI growth also predict GDPE growth (and
GDPI growth) better than lags of GDPE growth itself.
It is worth noting that, as regards our benchmark midpoint calibration of φ = 1.10, we
have deviated only slightly from a “ignorance prior” midpoint of 1.00. Hence our choice of
midpoint reflects a conservative interpretation of the evidence discussed above. Similarly,
regarding the width of the credible interval as opposed to its midpoint, we considered employing intervals such as φ ∈ [.95, 1.25], for which φ > 1 over most of the mass of the interval.
The evidence discussed above, if interpreted aggressively, might justify such a tight interval
in favor of GDPI , but again we opted for a more conservative approach with φ < 1 over
more than a third of the mass of the interval.
2.2.2

Calibrating ρ

The key prior view embedded in our choice of ρ ∈ [0.30, 0.60], with midpoint 0.45, is that
the errors in GDPE and GDPI are likely positively correlated, with a moderately but not
extremely large correlation value. This again accords with the results in Nalewaik (2010),
who shows that 26 percent of the nominal value of GDPE and GDPI is identical. Any
measurement errors in that 26 percent will be perfectly correlated across the two estimates.
Furthermore, GDPE and GDPI are both likely to miss fluctuations in output occurring
in the underground or “gray” economy, transactions that do not appear on tax forms or
government surveys. In addition, the same price deflator is used to convert GDPE and
GDPI from nominal to real values, so any measurement errors in that price deflator will be
perfectly correlated across the two estimates.
These considerations suggest the lower bound for ρ should be well above zero, as reflected
in our chosen interval. However, the evidence favoring an upper bound well below one is also
quite strong, as also reflected in our chosen interval. First, and most obviously, the standard
deviation of the difference between GDPE and GDPI is 1.9 percent, far from the 0.0 percent
that would be the case if ρ = 1.0. Second, as discussed in the previous subsection, the source
data used to construct GDPE is quite different from the source data used to construct GDPI ,
implying the measurement errors are likely to be far from perfectly correlated.
11

Of course, ρ could still be quite high if GDPE and GDPI were contaminated with enormous common measurement errors, as well as smaller, uncorrelated measurement errors. But
if that were the case, GDPE and GDPI would fail to be correlated with other cyclicallysensitive variables such as the unemployment rate, as they both are. The R2 values from
regressions of the output growth measures on the change in the unemployment rate are each
around 0.50 over our sample, suggesting that at least half of the variance of GDPE and GDPI
is true variation in output growth, rather than measurement error. The standard deviation
of the residual from these regressions is 2.81 percent using GDPI and 2.95 percent using
GDPE . For comparison, taking our benchmark value φ = 1.1 and our upper bound ρ = 0.6
produces σI = 2.05 and σE = 2.25. Increasing ρ to 0.7 produces σI = 2.36 and σE = 2.60,
approaching the residual standard error from our regression. This seems like an unreasonably high amount of measurement error, since the explained variation from such a simple
regression is probably not measurement error, and indeed some of the unexplained variation
from the regression is probably also not measurement error. Hence the upper bound of 0.6
for ρ seems about right.

3

Combination Under Minimax Loss

Here we take a more conservative perspective on forecast combination, solving a different
but potentially important optimization problem. In particular, by solving a game between
a benevolent scholar (the Econometrician) and a malevolent opponent (Nature), we obtain
“minimax” combining weights, which produce the smallest chance of the worst outcome for
the Econometrician.
Minimax combining weights are of interest for at least two reasons. First, minimax calculations are the central decision-theoretic approach for imposing conservatism, and minimax
combining weights are therefore of intrinsic interest. Moreover, to the best of our knowledge,
minimax forecast combination has not yet been considered in the literature.
Second, and of particular importance in the present context of GDPE and GDPI combination, it transpires that optimal minimax combining weights do not depend on properties
of the forecast errors. In particular, knowledge or calibration of objects like φ and ρ is unnecessary, enabling us to dispense with judgment, for better or worse. Instead, as we shall
show, the minimax optimization determines the minimax combining weights completely.

12

We obtain the minimax weights by solving for the Nash equilibrium in a two-player zerosum game. Nature chooses the properties of the forecast errors and the Econometrician
chooses the combining weights λ. For expositional purposes, we begin with the case of
uncorrelated errors, constraining Nature to choose ρ = 0. To impose some constraints on
the magnitude of forecast errors that Nature can choose, it is useful to re-parameterize the
vector (σI , σE )0 in terms of polar coordinates; that is, we let σI = ψ cos ϕ and σE = ψ sin ϕ.
We restrict ψ to the interval [0, ψ̄] and let ϕ ∈ [0, π/2]. Because cos2 ϕ + sin2 ϕ = 1, the
sum of the forecast error variances associated with GDPE and GDPI is constrained to be
less than or equal to ψ̄ 2 . The error associated with the combined forecast is given by


σC2 (ψ, ϕ, λ) = ψ 2 λ2 sin2 ϕ + (1 − λ)2 cos2 ϕ .

(9)

so that the minimax problem is
min σC2 (ψ, ϕ, λ).

max

(10)

ψ∈[0,ψ̄], ϕ∈[0,π/2] λ∈[0,1]

The best response of the Econometrician was derived in (2) and can be expressed in
terms of polar coordinates as λ∗ = cos2 ϕ. In turn, Nature’s’ problem simplifies to
ψ 2 (1 − sin2 ϕ) sin2 ϕ,

max
ψ∈[0,ψ̄], ϕ∈[0,π/2]

which leads to the solution
ϕ∗ = arc sin

p
1/2,

ψ ∗ = ψ̄,

λ∗ = 1/2.

(11)

Nature’s optimal choice implies a unit forecast error variance ratio, φ = σE /σI = 1, and
hence that the optimal combining weight is 1/2. If, instead, Nature set ϕ = 0 or ϕ = π/2,
that is φ = 0 or φ = ∞, then either GDPE or GDPI is perfect and the Econometrician
could choose λ = 0 or λ = 1 to achieve a perfect forecast leading to a suboptimal outcome
for Nature.
Now we consider the case in which Nature can choose a non-zero correlation between the
forecast errors of GDPE and GDPI . The loss of the combined forecast can be expressed as


σC2 (ψ, ρ, ϕ, λ) = ψ 2 λ2 sin2 ϕ + (1 − λ)2 cos2 ϕ + 2λ(1 − λ)ρ sin ϕ cos ϕ .

13

(12)

It is apparent from (12) that as long as λ lies in the unit interval the most devious choice
of ρ is ρ∗ = 1. We will now verify that conditional on ρ∗ = 1 the solution in (11) remains
a Nash Equilibrium. Suppose that the Econometrician chooses equal weights λ∗ = 1/2. In
this case


1
2
∗
∗
2 1
+ sin ϕ cos ϕ .
σC (ψ, ρ , ϕ, λ ) = ψ
4 2
We can deduce immediately that ψ ∗ = ψ̄. Moreover, first-order conditions for the maximizap
tion with respect to ϕ imply that cos2 ϕ∗ = sin2 ϕ∗ which in turn leads to ϕ∗ = arc sin 1/2.
Conditional on Nature choosing ρ∗ , ψ ∗ , and ϕ∗ , the Econometrician has no incentive to
deviate from the equal-weights combination λ∗ = 1/2, because
σC2 (ψ ∗ , ρ∗ , ϕ∗ , λ)



ψ̄
ψ̄ 2
2
λ + (1 − λ) + 2λ(1 − λ) = .
=
2
2

In sum, the minimax analysis provides a rational for combining GDPE and GDPI with equal
weights of λ = 1/2.

4

Empirics

We have shown that combining using a quasi-Bayesian calibration under quadratic loss produces λ close to but less than 0.5, given our prior means for φ and ρ. Moreover, we showed
that combining with λ near 0.5 is likely better – often much better – than simply using
GDPE or GDPI alone, for wide ranges of φ and ρ. We also showed that combining under
minimax loss always implies an optimal λ of exactly 0.5.
Here we put the theory to work for the U.S., providing arguably-superior combined
estimates of GDP growth. We focus on quasi-Bayesian calibration under quadratic loss.
Because the resulting combining weights are near 0.50, however, one could also view our
combinations as approximately minimax. The point is that a variety of perspectives lead
to combinations with weights near 0.50, and they suggest that such combinations are likely
superior to using either of GDPE or GDPI alone, so that empirical examination of GDPC
is of maximal interest.

14

20
15
10
5
0

Annualized Percent Change

−10 −5

1950

1960

1970

1980

1990

2000

2010

5
0
−5
−10

Annualized Percent Change

10

Time

2006Q1

2006Q3

2007Q1

2007Q3

2008Q1

2008Q3

2009Q1

2009Q3

Time

Figure 7: U.S. GDPC and GDPE Growth Rates. GDPC constructed assuming φ = 1.1 and ρ = 0.45.
GDPC is solid and GDPE is dashed. In the top panel we show a long sample, 1947Q2-2009Q3. In the bottom
panel, we show a recent sample, 2006Q1-2009Q3. See text for details.

4.1

A Combined U.S. GDP Series

In the top panel of Figure 7 we plot GDPC constructed using λ = 0.41, which is optimal for
our benchmark calibration of φ = 1.1 and ρ = 0.45, together with the “conventional” GDPE .
The two appear to move closely together, and indeed they do, at least at the low frequencies
emphasized by the long time-series plot. Hence for low-frequency analyses, such as studies
of long-term economic growth, use of GDPE , GDPI or GDPC is not likely to make a major
difference.
At higher frequencies, however, important divergences can occur. In the bottom panel
of Figure 7, for example, we emphasize business cycle frequencies by focusing on a short
sample 2006-2010, which contains the severe U.S. recession of 2007-2009. There are two
important points to notice. First, the bottom panel of Figure 7 makes clear that growth-rate
assessments on particular dates can differ in important ways depending on whether GDPC
or GDPE is used. For example, GDPE is strongly positive for 2007Q3, whereas GDPC for
that quarter is close to zero, as GDPI was strongly negative. Second, the bottom panel
15

Figure 8: Inferred U.S. Recession Regime Probabilities, Calculated Using GDPC vs. GDPE .
Solid lines are posterior median smoothed recession regime probabilities calculated using GDPC , which we
show with ninety percent posterior intervals. Dashed lines are posterior median smoothed recession regime
probabilities calculated using GDPE . Sample period is 1947Q2-2009Q3. Dark shaded bars denote NBER
recessions. See text and appendix for details.

of Figure 7 also makes clear that differing assessments can persist over several quarters, as
for example during the financial crisis episode of 2007Q1-2007Q3, when GDPE growth was
consistently larger than GDPC growth. One might naturally conjecture that such persistent
and cumulative data distortions might similarly distort inferences, based on those data,
about whether and when the U.S. economy was in recession. We now consider recession
dating in some detail.

4.2

U.S. Recession and Volatility Regime Probabilities

Thus far we have assessed how combining produces changes in measured GDP. Now we assess
whether and how it changes a certain important transformation of GDP, namely measured
probabilities of recession regimes or high-volatility regimes based on measured GDP. We

16

proceed by fitting a regime-switching model in the tradition of Hamilton (1989), generalized
to allow for switching in both means and variances, as in Kim and Nelson (1999a),
(GDPt − µsµt ) = β(GDPt−1 − µsµt−1 ) + σsσt εt

(13)

εt ∼ iidN (0, 1)
sµt ∼ M arkov(Pµ ),

sσt ∼ M arkov(Pσ ).

Then, conditional on observed data, we infer the sequences of recession probabilities (P (sµt =
L), where L (“low”) denotes the recession regime) and high-volatility regime probabilities
(P (sσt = H), where H (“high”) denotes the high-volatility regime). We perform this exercise
using both GDPE and GDPC , and we compare the results.
We implement Bayesian estimation and state extraction using data 1947Q2-2009Q3.11
In Figure 8 we show posterior median smoothed recession probabilities. We show those
calculated using GDPC as solid lines with ninety percent posterior intervals, we show those
calculated using GDPE as dashed lines, and we also show shaded NBER recession episodes
to help provide context. Similarly, in Figure 9 we show posterior median smoothed volatility
regime probabilities.
Numerous interesting substantive results emerge. For example, posterior median smoothed
recession regime probabilities calculated using GDPC tend to be greater than those calculated using GDPE , sometimes significantly so, as for example during the financial crisis of
2007. Indeed using GDPC one might date the start of the recent recession significantly earlier than did the NBER. As regards volatilities, posterior median smoothed high-volatility
regime probabilities calculated by either GDPE or GDPC tend to show the post-1984 “great
moderation” effect asserted by McConnell and Perez-Quiros (2000) and Stock and Watson
(2002). Interestingly, however, those calculated using GDPE also show the “higher recession
volatility” effect in recent decades documented by Bloom et al. (2009) (using GDPE data),
whereas those calculated using GDPC do not.
For our present purposes, however, none of those substantive results are of first-order
importance, as the present paper is not about business cycle dating, low-frequency vs. highfrequency volatility regime dating, or revisionist history, per se. Indeed thorough explorations
of each would require separate and lengthy papers for each. Rather, our point here is simply
11

We provide a detailed description in Appendix A.

17

Figure 9: Inferred U.S. High-Volatility Regime Probabilities, Calculated Using GDPC vs.
GDPE . Solid lines are posterior median smoothed high-volatility regime probabilities calculated using
GDPC , which we show with ninety percent posterior intervals. Dashed lines are posterior median smoothed
high-volatility regime probabilities calculated using GDPE . Sample period is 1947Q2-2009Q3. Dark shaded
bars denote NBER recessions. See text and appendices for details.

that one’s assessment and characterization of macroeconomic behavior can, and often does,
depend significantly on use of GDPC vs. GDPE . That is, choice of GDPC vs. GDPE can
matter for important tasks, whether based on direct observation of measured GDP, or on
transformations of measured GDP such as extracted regime chronologies.

5

Extensions

Before concluding, we offer sketches of what we see as two important avenues for future
research. The first involves real-time analysis and non-constant combining weights, and the
second involves combining from a measurement error as opposed to efficient forecast error
perspective.

18

5.1

Vintage Data, Time-Varying Combining Weights, and RealTime Analysis

It is important to note that everything that we have done in this paper has a retrospective,
or “off-line,” character. We work with a single vintage of GDPE and GDPI data and
combine them, estimating objects of interest (combining weights, regime probabilities, etc.)
for any period t using all data t = 1, ..., T . In all of our analysis, moreover, we have used
time-invariant combining weights. Those two characteristics of our work thus far are not
unrelated, and one may want to relax them eventually, allowing for time-varying weights,
and ultimately, a truly real-time-analysis.
One may want to consider time-varying combining weights for several reasons. One reason
is of near-universal and hence great interest, at least under quadratic loss. For any given
vintage of data, error variances and covariances may naturally change, as we pass backward
from preliminary data for the recent past, all the way through to “final revised” data for
the more distant past.12 More precisely, let t index time measured in quarters, and consider
moving backward from “the present” quarter t = T . At instant v ∈ T (with apologies for the
slightly abusive notation), we have vintage-v data. Consider moving backward, constructing
v
combined GDP estimates GDPC,T
−k , k = 1, . . . ∞. For small k, the optimal calibrations
might be quite far from benchmark values. As k grows, however, ρ and φ should approach
benchmark values as the final revision is approached. The obvious question is how quickly
and with what pattern should an optimal calibration move toward benchmark values as
k → ∞. We can offer a few speculative observations.
First consider ρ. GDPI and GDPE share a considerable amount of source data in their
early releases, before common source data is swapped out of GDPI (e.g., when tax returns
eventually become available and can be used). Indeed Fixler and Nalewaik (2009) show that
the correlation between the earlier estimates of GDPI and GDPE growth is higher than the
correlation between the later estimates. Hence ρ is likely higher for dates near the present
(small k). This suggests calibrations with ρ dropping monotonically toward the benchmark
value of 0.45 as k grows.
Now consider φ. How φ should deviate from its benchmark calibration value of 1.1 is less
clear. On the one hand, early releases of GDPI are missing some of its most informative
source data (tax returns), which suggests a lower-than-benchmark φ for small k. On the
12

This is the so-called “apples and oranges” problem. To the best of our knowledge, the usage in our
context traces to Kishor and Koenig (2011).

19

other hand, early releases of GDPE growth appear to be noisier than the early releases of
GDPI growth (see below), which suggests a higher-than-benchmark φ for small k. All told,
we feel that a reasonable small-k calibration of φ is less than 1.1 but still above 1.
Note that our conjectured small-k effects work in different directions. Other things equal,
bigger ρ pushes the optimal combining weight downward, away from 0.5, and smaller φ pushes
the optimal combining weight upward, toward from 0.5. In any particular dataset the effects
could conceivably offset more-or-less exactly, so that combination using constant weights for
all dates would be fully optimal, but there is of course no guarantee.
Several approaches are possible to implement the time-varying weights sketched in the
preceding paragraphs. One is a quasi-Bayesian calibration, elaborating on the approach we
have taken in this paper. However, such an approach would be more difficult in the more
challenging environment of time-varying parameters. Another is to construct a real-time
dataset, one that records a snapshot of the data available at each point in time, such as the
one maintained by the Federal Reserve Bank of Philadelphia. The key is to recognize that
each quarter we get not simply one new observation on GDPE and GDPI , but rather an
entire new vintage of data, all the elements of which could (in principle) change. One might
be able to use the different data vintages, and related objects like revision histories, to infer
properties of “forecast errors” of relevance for construction of optimal combining weights
across various k.
One could go even farther in principle, progressing to a truly real-time analysis, which is
of intrinsic interest quite apart from addressing the issue of time-varying combining weights
in the above “apples and oranges” environments. Tracking vintages, modeling the associated
dynamics of revisions, and putting it all together to produce superior combined forecasts remains an outstanding challenge.13 We look forward to its solution in future work, potentially
in the state-space framework that we describe next.

5.2

A Model of Measurement Error

In parallel work in progress, Aruoba et al. (2011), we pursue a complementary approach
based on a state-space model of measurement error. The basic model is
13

Nalewaik (2011) makes some progress toward real-time analysis in a Markov-switching environment.

20

GDPE,t
GDPI,t

!
=

!
1
GDPt +
1

εEt
εIt

!

GDPt = β0 + β1 GDPt−1 + ηt ,
where εt = (εEt , εIt )0 ∼ W N (0, Σε ), ηt ∼ W N (0, ση2 ), and εt and ηt are uncorrelated at
all leads and lags. In this model, both GDPE and GDPI are noisy measures of the latent
true GDP process, which evolves dynamically. The expectation of true GDP conditional
upon observed measurements may be extracted using optimal filtering techniques such as
the Kalman filter.
The basic state-space model can be extended in various directions, for example to incorporate richer dynamics, and to account for data revisions and missing advance and preliminary
releases of GDPI .14 Perhaps most importantly, the measurement errors ε may be allowed to
be correlated with GDP, or more precisely, correlated with GDP innovations, ηt . Fixler and
Nalewaik (2009) and Nalewaik (2010) document cyclicality in the “statistical discrepancy”
(GDPE − GDPI ), which implies failure of the assumption that εt and ηt are uncorrelated at
all leads and lags. Of particular concern is contemporaneous correlation between ηt and εt .
The standard Kalman filter can not handle this, but appropriate modifications are available.

6

Conclusions

GDP growth is a central concept in macroeconomics and business cycle monitoring, so its
accurate measurement is crucial. Unfortunately, however, the two available expenditure-side
and income-side U.S. GDP estimates often diverge. In this paper we proposed a technology
for optimally combining the competing GDP estimates, we examined several variations on
the basic theme, and we constructed and examined combined estimates for the U.S.
Our results strongly suggest the desirability of separate and careful calculation of both
GDPE and GDPI , followed by combination, which may lead to different and more accurate
insights than those obtained by simply using expenditure-side or estimates alone. This
14
The first official estimate of GDPI is released a month or two after the first official estimate of GDPE , so
−1
v
for vintage v the available GDPEv data might be {GDPE,t
}Tt=1
whereas the available GDPIv vintage might
v T −2
be {GDPI,t }t=1 . Note that for any vintage v, the available GDPI data differs by at most one quarter from
the available GDPE data.

21

prescription differs fundamentally from U.S. practice, where both are calculated but the
income-side estimate is routinely ignored.
Our call for a combined U.S. GDP measure is hardly radical, particularly given current
best-practice procedures at various non-U.S. statistical agencies. European countries, for
example, tend to use sophisticated GDP balancing procedures to harmonize GDP estimates
from different sources.15 The balancing procedure recognizes the potential inaccuracies of
source data and has a similar effect to our forecast combination approach: the final GDP
number lies between the alternative estimates. Other countries use other approaches to
combination. Strikingly, for example, Australia uses an approach reminiscent of the one
that we advocate in this paper, albeit not on the grounds of our formal analysis.16 In
addition to GDPE and GDPI , the Australian Bureau of Statistics produces a productionside estimate, GDPP , defined as total gross value added plus taxes and less subsidies, and its
headline GDP number is the simple average of the three GDP estimates. We look forward
to the U.S. producing a similarly-combined headline GDP estimate, potentially using the
methods introduced in this paper.

15
16

Germany’s procedures, for example, are described in Statistisches Bundesamt (2009).
See http://www.abs.gov.au, under Australian National Accounts, Explanatory Notes for Australia.

22

Appendices
A

Estimation of U.S. Recession Probabilities

Here we provide details of Bayesian analysis of our regime-switching model.

A.1

Baseline Model

We work with a simple model with Markov regime-switching in mean and variance:
(GDPt − µsµt ) = β(GDPt−1 − µsµt−1 ) + σsσt εt

(A.1)

εt ∼ iidN (0, 1)
sµt ∼ M arkov(Pµ ),

sσt ∼ M arkov(Pσ ),

where Pµ and Pσ denote transition matrices for high and low mean and variance regimes,
"

pµ H
1 − pµ L

1 − pµH
pµ L

#

"

pσH
1 − pσL

1 − pσ H
pσL

#

Pµ =

Pσ =

.

Overall, then, there are four regimes:
St = 1 if sµt = H, sσt = H
St = 2 if sµt = H, sσt = L
St = 3 if sµt = L, sσt = H
St = 4 if sµt = L, sσt = L.
For t = 0 the hidden Markov states are governed by the ergodic distribution associated with
Pµ and Pσ .

23

Table 1: Prior Choices and Posterior Distributions

µH − µL
µH
µL
σH
σL
β
pµ H
pµL
pσH
pσ L

A.2

Prior
Choice

Median

GDPE
5%

95%

Median

GDPC
5%

95%

Gamma(2,1)
–
Normal(0,0.5)
InvGamma(2,2)
InvGamma(1,2)
Normal(0,1)
Beta(25,5)
Beta(25,5)
Beta(25,5)
Beta(25,5)

–
3.50
1.25
4.82
1.92
0.31
0.91
0.79
0.91
0.89

–
[3.03
[0.34
[4.35
[1.55
[0.17
[0.82
[0.64
[0.83
[0.81

–
4.12]
2.29]
5.43]
2.34]
0.45]
0.96]
0.87]
0.96]
0.95]

–
3.76
0.82
4.64
1.71
0.37
0.92
0.80
0.91
0.91

–
[2.97
[0.17
[4.21
[1.74
[0.27
[0.85
[0.67
[0.83
[0.85

–
4.28]
1.64]
5.13]
2.05]
0.53]
0.96]
0.88]
0.96]
0.95]

Bayesian Inference

Priors. Bayesian inference combines a prior distribution with a likelihood function to obtain
a posterior distribution of the model parameters and states. We summarize our benchmark
priors in Table 1. We employ a normal prior for µL , a gamma prior for µH − µL , inverted
gamma priors for σH and σL , beta priors for the transition probabilities, and finally, a normal
prior for β. Our prior ensures that µH ≥ µL and thereby deals with the “label switching”
identification problem.
For µL , the average growth rate in the low-growth state, we use a prior distribution that
is centered at 0, with standard deviation 0.7%. Note that a priori we do not restrict the
average growth rate to be negative. We also allow for (mildly) positive values. Wh choose
the prior for µH − µL such that the mean difference between the average growth rates in the
two regimes is 2%, with standard deviation 1%. Our priors for the transition probabilities
pµ and pσ are symmetric and imply a mean regime duration between 3 and 14 quarters.
Finally, our choice for the prior of the autoregressive parameter β is normal with zero mean
and unit variance, allowing a priori for both stable and unstable dynamics of output growth
rates.
Implementation of Posterior Inference. Posterior inference is implemented with a
Metropolis-within-Gibbs sampler, building on work by Carter and Kohn (1994) and Kim
and Nelson (1999b). We denote the sequence of observations by GDP1:T . Moreover, let S1:T

24

be the sequence of hidden states, and let
θ = (µH , µL , σH , σL , β)0 ,

and φ = (pµH , pµL , pσL , pσH )0 .

Our Metropolis-within-Gibbs algorithm involves sampling iteratively from three conditional
posterior distributions. To initialize the sampler we start from (θ0 , φ0 ).
Algorithm: Metropolis-within-Gibbs Sampler
For i = 1, . . . , N :
i+1
1. Draw S1:T
conditional on θi , φi , GDP1:T . This step is implemented using the multimove simulation smoother described in Section 9.1.1 of Kim and Nelson (1999b).
i+1
2. Draw φi+1 conditional on θi , S1:T
, GDP1:T . If the dependence of the distribution of
the initial state S1 on φ is ignored, then it can be shown that the conditional posterior
of φ is of the Beta form (see Section 9.1.2 of Kim and Nelson (1999b)). We use the
resulting Beta distribution as a proposal distribution in a Metropolis-Hastings step.
i+1
3. Draw θi+1 , conditional on φi+1 , S1:T
, GDP1:T . Since our prior distribution is nonconjugate, we are using a random-walk Metropolis step to generate a draw from the
conditional posterior of θ. The proposal distribution is N (θi , cΩ).

We obtain the covariance matrix Ω of the proposal distribution in Step 3 as follows.
Following Schorfheide (2005) we maximize the posterior density,
p(θ, φ|GDP1:T ) ∝ p(GDP1:T |θ, φ)p(θ, φ),
to obtain the posterior mode (θ̃, φ̃). We then construct the negative inverse of the Hessian
at the mode and let Ω be the sub-matrix that corresponds to the parameter sub-vector θ.
We choose the scaling factor c to obtain an acceptance rate of approximately 40%. We
initialize our algorithm choosing (θ0 , φ0 ) in the neighborhood of (θ̃, φ̃) and use it to generate
N = 100, 000 draws from the posterior distribution.17
Posterior Estimates. Table 1 also contains percentiles of posterior parameter distributions.
The posterior estimates for the volatility parameters and the transition probabilities are
similar across GDPE and GDPC . However, the posterior estimate for µL is higher using
17

We performed several tests confirming that our choice of N yields an accurate posterior approximation.

25

GDPE than using GDPC , while the opposite is true for β. Moreover, the differential between
high and low mean regimes is bigger in the case of GDPC , all of which can influence the
time-series plot of the recession probabilities.
The Markov-switching means capture low-frequency shifts while the autoregressive coefficient captures high-frequency dynamics. Thus, the presence of the autoregressive term
may complicate our analysis, because we are trying to decompose the GDP measurement
discrepancy into both low and high frequency components. As a robustness check, we remove the autoregressive term in (A.1) and estimate an iid model specification. Although
the posterior estimates for µL change, the remaining parameters are essentially identical to
Table 1. The smoothed recession probabilities remain nearly identical to Figure 8.

26

References
Aruoba, B. (2008), “Data Revisions are not Well-Behaved,” Journal of Money, Credit and
Banking, 40, 319–340.
Aruoba, S.B. and F.X. Diebold (2010), “Real-Time Macroeconomic Monitoring: Real Activity, Inflation, and Interactions,” American Economic Review , 100, 20–24.
Aruoba, S.B., F.X. Diebold, J. Nalewaik, F. Schorfheide, and D. Song (2011), “Improving
GDP Measurement: A Measurement Error Perspective,” Manuscript in progress, University of Maryland, University of Pennsylvania and Federal Reserve Board.
Bates, J.M. and C.W.J. Granger (1969), “The Combination of Forecasts,” Operations Research Quarterly, 20, 451–468.
Bloom, N., M. Floetotto, and N. Jaimovich (2009), “Really Uncertain Business Cycles,”
Manuscript, Stanford University.
Carter, C.K. and R. Kohn (1994), “On Gibbs Sampling for State Space Models,” Biometrika,
81, 541–553.
Diebold, F.X. and J.A. Lopez (1996), “Forecast Evaluation and Combination,” In G.S.
Maddala and C.R. Rao (eds.) Handbook of Statistics (Statistical Methods in Finance),
North- Holland, 241-268.
Faust, J., J.H. Rogers, and J.H. Wright (2005), “News and Noise in G-7 GDP Announcements,” Journal of Money, Credit and Banking, 37, 403–417.
Fixler, D.J. and J.J. Nalewaik (2009), “News, Noise, and Estimates of the “True” Unobserved
State of the Economy,” Manuscript, Bureau of Labor Statistics and Federal Reserve Board.
Hamilton, J.D. (1989), “A New Approach to the Economic Analysis of Nonstationary Time
Series and the Business Cycle,” Econometrica, 57, 357–384.
Kim, C.-J. and C.R. Nelson (1999a), “Has the U.S. Economy Become More Stable? A
Bayesian Approach Based on a Markov-Switching Model of the Business Cycle,” Review
of Economics and Statistics, 81, 608–616.
Kim, C.-J. and C.R. Nelson (1999b), State Space Models with Regime Switching, MIT Press.

27

Kishor, N.K. and E.F. Koenig (2011), “VAR Estimation and Forecasting When Data are
Subject to Revision,” Journal of Business and Economic Statistics, in press.
Mankiw, N.G., D.E. Runkle, and M.D. Shapiro (1984), “Are Preliminary Announcements of
the Money Stock Rational Forecasts?” Journal of Monetary Economics, 14, 15–27.
Mankiw, N.G. and M.D. Shapiro (1986), “News or Noise: An Analysis of GNP Revisions,”
Survey of Current Business, May, 20–25.
McConnell, M. and G. Perez-Quiros (2000), “Output Fluctuations in the United States:
What Has Changed Since the Early 1980s?” American Economic Review , 90, 1464–1476.
Nalewaik, J.J. (2010), “The Income- and Expenditure-Side Estimates of U.S. Output
Growth,” Brookings Papers on Economic Activity, 1, 71–127 (with discussion).
Nalewaik, J.J. (2011), “Estimating Probabilities of Recession in Real Time Using GDP and
GDI,” Journal of Money, Credit and Banking, in press.
Schorfheide, F. (2005), “Learning and Monetary Policy Shifts,” Review of Economic Dynamics, 8, 392–419.
Statistisches Bundesamt, Wiesbaden (2009), “National Accounts: Gross Domestic Product
in Germany in Accordance with ESA 1995 - Methods and Sources,” Subject Matter Series,
18.
Stock, J.H. and M.W. Watson (2002), “Has the Business Cycle Changed and Why?” In M.
Gertler and K. Rogoff (eds.), NBER Macroeconomics Annual, Cambridge, Mass.: MIT
Press, 159-218.
Timmermann, A. (2006), “Forecast Combinations,” In G. Elliot, C.W.J. Granger and A.
Timmermann (eds.), Handbook of Economic Forecasting, North-Holland, 136-196.

28

