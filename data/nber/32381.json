{
    "id": 32381,
    "citation_title": "Automated Social Science: Language Models as Scientist and Subjects",
    "citation_author": [
        "Benjamin S. Manning",
        "Kehang Zhu",
        "John J. Horton"
    ],
    "citation_publication_date": "2024-04-29",
    "issue_date": "2024-04-25",
    "revision_date": "None",
    "topics": [
        "\n",
        "Microeconomics",
        "\n",
        "Behavioral Economics",
        "\n"
    ],
    "program": [
        "\n",
        "Productivity, Innovation, and Entrepreneurship",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nWe present an approach for automatically generating and testing, in silico, social scientific hypotheses. This automation is made possible by recent advances in large language models (LLM), but the key feature of the approach is the use of structural causal models. Structural causal models provide a language to state hypotheses, a blueprint for constructing LLM-based agents, an experimental design, and a plan for data analysis. The fitted structural causal model becomes an object available for prediction or the planning of follow-on experiments. We demonstrate the approach with several scenarios: a negotiation, a bail hearing, a job interview, and an auction. In each case, causal relationships are both proposed and tested by the system, finding evidence for some and not others. We provide evidence that the insights from these simulations of social interactions are not available to the LLM purely through direct elicitation. When given its proposed structural causal model for each scenario, the LLM is good at predicting the signs of estimated effects, but it cannot reliably predict the magnitudes of those estimates. In the auction experiment, the in silico simulation results closely match the predictions of auction theory, but elicited predictions of the clearing prices from the LLM are inaccurate. However, the LLM's predictions are dramatically improved if the model can condition on the fitted structural causal model. In short, the LLM knows more than it can (immediately) tell.\n\n",
    "acknowledgement": "\nThis research was made possible by a generous grant from Dropbox Inc. Thanks to Jordan Ellenberg, Benjamin Lira Luttges, David Holtz, Bruce Sacerdote, Paul R\u00f6ttger, Mohammed Alsobay, Ray Duch, Matt Schwartz, David Autor, and Dean Eckles for their helpful feedback. Author's contact information, code, and data are currently or will be available at http://www.benjaminmanning.io/. Both Benjamin S. Manning and Kehang Zhu contributed equally to this work.  John J. Horton is a co-founder of a company, Expected Parrot Inc., using generative AI models for market research.  The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}