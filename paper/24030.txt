NBER WORKING PAPER SERIES

PROBABILISTIC STATES VERSUS MULTIPLE CERTAINTIES:
THE OBSTACLE OF UNCERTAINTY IN CONTINGENT REASONING
Alejandro Martínez-Marquina
Muriel Niederle
Emanuel Vespa
Working Paper 24030
http://www.nber.org/papers/w24030

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
November 2017

The views expressed herein are those of the authors and do not necessarily reflect the views of the
National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2017 by Alejandro Martínez-Marquina, Muriel Niederle, and Emanuel Vespa. All rights
reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit
permission provided that full credit, including © notice, is given to the source.

Probabilistic States versus Multiple Certainties: The Obstacle of Uncertainty in Contingent
Reasoning
Alejandro Martínez-Marquina, Muriel Niederle, and Emanuel Vespa
NBER Working Paper No. 24030
November 2017
JEL No. C90,D81
ABSTRACT
We propose a new hypothesis, the Power of Certainty, to help explain agents' difficulties in
making choices when there are multiple possible payoff-relevant states. In the probabilistic
‘Acquiring-a-Company’ problem an agent submits a price to a firm before knowing whether the
firm is of low or high value. We construct a deterministic problem with a low and high value
firm, where the agent submits a price that is sent to each firm separately. Subjects are much more
likely to use dominant strategies in deterministic than in probabilistic problems, even though
computations for profit maximization are identical for risk-neutral agents.

Alejandro Martínez-Marquina
Stanford University
amm15@stanford.edu
Muriel Niederle
Department of Economics
579 Serra Mall
Stanford University
Stanford, CA 94305-6072
and NBER
niederle@stanford.edu

Emanuel Vespa
2127 North Hall
University of California
Santa Barbara, CA 93106
vespa@ucsb.edu

An online appendix and instructions appendix are available at http://www.nber.org/data-appendix/w24030

Probabilistic States versus Multiple Certainties:
The Obstacle of Uncertainty in Contingent Reasoning

November 9, 2017

Alejandro Martínez-Marquina

Muriel Niederle

(Stanford University)

(Stanford University, NBER and SIEPR)
Emanuel Vespa
(UC Santa Barbara)
Abstract

We propose a new hypothesis, the Power of Certainty, to help explain agents’ difficulties
in making choices when there are multiple possible payoff-relevant states. In the probabilistic
‘Acquiring-a-Company’ problem an agent submits a price to a firm before knowing whether the
firm is of low or high value. We construct a deterministic problem with a low and high value firm,
where the agent submits a price that is sent to each firm separately. Subjects are much more
likely to use dominant strategies in deterministic than in probabilistic problems, even though
computations for profit maximization are identical for risk-neutral agents.

1

Introduction

We have to engage almost daily in hypothetical or conditional reasoning: making choices in environments with multiple possible payoff-relevant states. However, individuals routinely fail to make
profit-maximizing choices in many such environments.1 A few papers have explicitly shown that
even agents who are able to make profit-maximizing choices for a given state fail to do so once the
state is uncertain (Esponda and Vespa, 2014 and Fragiadakis, Knoepfle and Niederle, 2017). In
this paper we propose a new hypothesis that accounts for a large portion of this problem in simple
settings. We claim that it is not (only) the multiple states that generate difficulties, but the fact
that the states are uncertain.
1

For evidence see for example Shafir and Tversky (1992), Charness and Levin (2009), Eyster and Weizsäcker (2010),
Esponda and Vespa (2014), Enke (2017), Ngangoué and Weizsäcker (2017), Esponda and Vespa (2017a), Esponda
and Vespa (2017b) and Araujo, Wang and Wilson (2017).

1

This paper is part of a larger literature showing that, and trying to understand why, agents fail to
make profit-maximizing choices. Results in strategic settings, even in environments where fairness
concerns play no role, generated a literature accounting for agents’ mistakes.2 There is also a
literature in decision making that has documented behavioral biases that can explain failures to
maximize payoffs.3 In addition, active work on heuristics (Tversky and Kahneman, 1974) often
studies difficulties related to prediction or updating exercises.4 Here we focus on a simple decision
problem with a few states in which the probability of each state is known and where evaluating the
payoffs for an action only requires aggregating state-specific payoffs. We propose that aggregating
over multiple contingencies is especially difficult when those contingencies (states) are uncertain.
While there are several recent models aiming to understand difficulties in such basic environments
(for example Sims, 2003, Gennaioli and Shleifer, 2010, Bordalo, Gennaioli and Shleifer, 2012, Kőszegi
and Szeidl, 2012, Gabaix, 2014, Caplin and Dean, 2015 and Caplin, Dean and Leahy, 2016), none
directly differentiates between multiple probabilistic states and multiple certainties.
As an illustration, consider a two-state version of the Acquiring-a-Company problem (Bazerman and
Samuelson, 1983), which we refer to as the ‘probabilistic’ problem. An agent decides whether or not
to purchase a firm of value v. Once purchased, the value of the firm is 1.5 × v and the agent knows
that with equal chance the firm’s value is either vL or vH , where 0 < vL < vH . Without knowing the
realization of v, the agent submits a price p for the firm. The agent acquires the firm when p ≥ v
and her payoff is 1.5 × v − p, otherwise the individual does not buy the firm and has a payoff of
zero. Charness and Levin (2009) show that even in such a simple two-state version of the problem
a large proportion of participants fail to select the profit-maximizing price. This could be because
individuals have a hard time thinking about more than one state at the time or because, while agents
are able to think about two states, they have a limited computational capacity to aggregate over
states. These explanations can be summarized as the computational complexity hypothesis.
We propose a new hypothesis, namely that a prominent source of difficulties in optimization lies not
only in there being two states, but also in the fact that which state materializes is unknown. That is,
agents may be better at coping with two states or contingencies, as long as there is no uncertainty.
We label this hypothesis as the Power of Certainty. Specifically, compare the ‘probabilistic’ problem
described above to the following ‘deterministic’ problem in which we retain the complexity of two
firms but eliminate any uncertainty. There are two firms of value known to the agent: one of value
vL and another of value vH . The agent submits a price p that is sent to each firm separately and
determines which firms she buys: if p < vL , the agent buys none of the firms, if vL ≤ p < vH , the
2

For empirical evidence see for example Kagel and Levin (1986), Nagel (1995), Costa-Gomes and Crawford (2006),
Costa-Gomes and Weizsäcker (2008), Ivanov, Levin and Niederle (2010), Eyster, Rabin and Weizsäcker (2015),
Esponda and Vespa (2017b) and Fragiadakis, Knoepfle and Niederle (2017). For theoretical literature see e.g. Eyster
and Rabin (2005), Jehiel (2005), Crawford and Iriberri (2007), Esponda (2008), Crawford, Costa-Gomes and Iriberri
(2013).
3
Failure for profit maximization can be due to various behavioral biases such a hyperbolic discounting (see e.g.
Laibson, 1997 and O’Donoghue and Rabin, 1999) or probability weighting (Kahneman and Tversky, 1979).
4
For recent empirical work see for example Mobius et al. (2014), Levin, Peck and Ivanov (2016), Vespa and Wilson
(2016), Ambuehl and Li (2017), Enke and Zimmermann (2017); and for models see e.g. Rabin and Schrag (1999),
Caplin and Leahy (2001), Bénabou and Tirole (2002) and Brunnermeier and Parker (2005).

2

agent only buys the firm of value vL at a price p, and if p ≥ vH , the agent buys both firms, each
at a price p. In the deterministic problem there is only one state and no uncertainty. However, the
problem still requires that the agent evaluates two values of the firm, two “certainties.” To compute
the payoff-maximizing price, the agent has to determine the profits of buying only the firm vL or
both firms vL and vH . The profit-maximizing computations are identical to the computations a
risk-neutral agent would need to do to optimize in the probabilistic problem (for details see the
next section). We show that individuals are more likely to submit profit-maximizing prices in this
deterministic problem than in the related probabilistic problem, providing evidence for the Power
of Certainty, or the obstacle of uncertainty.
Specifically, we conduct an experiment using the probabilistic and deterministic problems described
above. We consider the classic case where the dominant strategy of a risk-neutral or risk-averse agent
is to submit a price p = vL (we use values such that 2vL < vH ) and compare subjects’ choices across
the two treatments, the probabilistic treatment with probabilistic problems and the deterministic
treatment with related deterministic problems. We show that substantially more subjects submit
payoff-maximizing prices in the deterministic than in the probabilistic problems. However, one worry
is that this result may not be due to higher instances of payoff maximization but rather due to rules
of thumb that prescribe prices independent of the values of the firms and that may guide choices
differently between the two treatments. We are particularly concerned with classifying too many
subjects as payoff maximizers in the deterministic treatment, thereby overstating the role of our
new hypothesis: the Power of Certainty. We therefore also study environments where we select the
values of the two firms such that submitting p = vH is a dominant strategy (by guaranteeing that
1.5vL > vH ). When we consider only subjects who both submit p = vL when 2vL < vH and p = vH
when 1.5vL > vH as payoff maximizing, we find that there are twice as many payoff-maximizing
subjects in the deterministic than in the probabilistic treatment.
One caveat of our analysis so far is that agents who are very risk seeking might prefer to submit
a price p = vH even in the case where 2vL < vH in the probabilistic treatment (though such a
price p = vH is dominated in the deterministic treatment). While experimental subjects are in
general risk averse, and often even excessively so given the low stakes, see Rabin (2000), we may
still have some subjects who are risk seeking and hence we may have underestimated the fraction of
subjects who are profit maximizers in the probabilistic treatment. We therefore introduce a measure
of whether subjects are risk seeking in environments that mirrors the outcomes of the probabilistic
treatment. Specifically, in three questions subjects choose between two lotteries that are constructed
the following way: Take a specific vL and vH realization that is close to parameters encountered by
subjects in the acquiring a company problem with 2vL < vH . Then the safer lottery corresponds to
the lottery obtained when submitting a price p = vL in the probabilistic treatment, and the riskier
lottery to the outcomes obtained when submitting a price p = vH . This allows us to measure whether
subjects might be risk seeking in the relevant domain. In addition, we can use those lottery choices
to estimate the welfare loss subjects incur by having to submit prices in the probabilistic problems
rather than directly selecting between the lotteries that correspond to the only two candidates of
3

potentially payoff-maximizing prices: vL and vH .
We show not only the existence, but also evaluate the quantitative significance of our proposed Power
of Certainty hypothesis. To benchmark the extent to which agents are profit maximizing in a simple
version of the problem, with only one firm of known value, we have a treatment where subjects
make decisions in exactly such an environment. We use this treatment as well as the insight from
lottery choices to quantify the effect of the Power of Certainty. We show that part of the welfare
loss is due to moving from an environment that has one firm of known value to an environment
with two firms of known value. We attribute this loss to what could be labelled as computational
complexity. However, a substantial part of the welfare loss subjects incur when moving from a simple
problem with one firm of known value to a problem where the firm might have two values is due to
uncertainty, confirming the quantitative importance of the Power of Certainty in this environment.
Finally, we aim to shed some light as to the reason behind the Power of Certainty, why the problem
with two firms of known value is so much simpler than the problem of one firm with two possible
values. Clearly these two scenarios are distinct theoretically: one involves one state of the world
that contains two firms, one of value vL and one of value vH , and the other involves two states of
the world, where each state corresponds to a given value of the firm, either vL or vH . However,
to compute payoff-maximizing prices, subjects have to take into account four outcomes: for each
firm vL and vH the payoff from submitting either p = vL or p = vH . Note that these four payoffs
are exactly the ones that correspond to the outcomes of the lotteries corresponding to submitting
p = vL or p = vH .
To gain some insight into the subjects’ thought processes, we had subjects provide incentivized advice
to a new individual in the case of vL = 20 and vH = 120. Analysis of the advice suggests that in the
deterministic problems subjects are much more likely to think of all four outcomes corresponding to
v, p ∈ {vL , vH }. Furthermore, controlling for whether subjects’ advice mentions those four outcomes
accounts for the differences between the probabilistic and the deterministic treatments.
In a final treatment, subjects who first encounter the ‘one firm of known value’ version of the problem
are then presented with either probabilistic or deterministic problems. We confirm that the Power
of Certainty plays a role even among subjects who first made decisions in a simpler problem and
hence may be more likely to think about the consequences of submitting a price p.
In the next section we describe the experimental design as well as some hypotheses and predictions.
We also connect the Power of Certainty hypothesis to the psychology literature. The main results
are in section 3. In section 4 we provide some insight as to the underlying cause of the Power of
Certainty. We then discuss the related literature and possible applications, with specific attention
to Li (2017), and finally, we summarize and conclude.

4

2
2.1

Experimental Design
Basic Environment

The experimental design is built around a simplification of the Acquiring-A-Company game (Bazerman and Samuelson, 1983). A firm of value v is for sale. The value of the firm is unknown to
the agent, who knows that v is equally likely to be vL or vH , where 0 < vL < vH < ∞. The agent
buys the firm if the agent’s price p is at least as high as v. If the agent does not buy the firm, her
profit is zero. If she buys the firm, its value increases by 50 percent, so that profits equal (1.5v − p).
Whenever subjects submit prices for firms, p had to satisfy p ∈ {0, 1, 2, ..., 150} and this was known
to subjects.
When deciding about a price, note that any price p not in {vL , vH } is dominated. For instance,
making an offer higher than vH is dominated by p = vH , which also leads to buying the firm
regardless of v but at a lower price. Likewise, making an offer p = vL dominates any price strictly
between vL and vH . Finally, offering a price below vL guarantees that a firm will not be purchased,
but is dominated by p = vL , which leads to either zero or strictly positive profits.
Consider the case of a risk-neutral agent. Depending on the agent’s price p, the expected profit is:

 1 (1.5v − v )
if p = vL
L
L
π(p) = 2
.
 1 (1.5v − v ) + 1 (1.5v − v ) if p = v
H
H
L
H
H
2
2

(1)

Note that when 1.5vL < vH , the agent receives a negative profit when v = vL and p = vH . She
may still prefer to submit p = vH if the gains in case of v = vH are sufficiently large. A risk-neutral
agent is indifferent between a price of vH or vL when 2vL = vH . When 2vL is strictly lower than
vH , both a risk-neutral and a risk-averse agent prefer p = vL . A risk-seeking agent may still prefer
to submit a price p = vH due to the positive returns when v = vH . In contrast, when 1.5vL ≥ vH ,
all agents prefer a price p = vH to a price p = vL .

2.2

Main Treatments

We first describe the two main treatments of our between-subjects design, the Probabilistic (prob)
and the Deterministic (det) treatment. In both treatments subjects faced the same five parts,
differing only as much as described below.

Probabilistic Treatment (prob)
In prob the subject submits a price for a single firm that could have either of two values in {vL , vH },
each with 50 percent chance. The subject did not know the value of the firm when submitting a

5

price. If p < vL , the subject buys no firm and the profit is zero. If vL ≤ p < vH , she only buys the
firm if the firm is of value vL at a price p. Finally, if p ≥ vH , she always buys the firm.
In order to simplify the comparison with det described next, payoffs are multiplied by two, so that
expected profits are:



0
if p < vL


P rob
π
(p) = (1.5vL − p)
if vL ≤ p < vH .



(1.5v − p) + (1.5v − p) if v ≤ p
H
L
H

(2)

Deterministic Treatment (det)
In det there are two firms for sale, one of value vL and another of value vH . The subject can submit
a unique price p that is sent to each firm separately. The price p determines whether the agent buys
none, one or two firms, each at price p. Profits are given by:



0
if p < vL


π Det (p) = (1.5vL − p)
if vL ≤ p < vH .



(1.5v − p) + (1.5v − p) if v ≤ p
H
L
H

(3)

Parts 1-5 of the experiment
In part 1 of each treatment subjects faced 20 rounds with values {vL , vH } such that 2vL < vH . The
first question always had vL = 20 and vH = 120. For the other 19 questions we chose the two values
{vL , vH } randomly under the following constraints: Both vL and vH had to be even numbers and
furthermore 10 ≤ vL ≤ 30 and 80 ≤ vH ≤ 140.5 We decided to select those values randomly in order
to not bias our results by unknowingly selecting values (or sequences of values) that would favor
specific results (for a discussion on the value of random games see also Fragiadakis, Knoepfle and
Niederle, 2017). While all subjects saw the same 19 sets of values {vL , vH }, the order was randomized
at the subject level. Because subjects could submit any price p ∈ [0, 150] ∩ N, underbidding is always
possible and overbidding (which can lead to losses) is bounded. Since in part 1 all values {vL , vH }
satisfied 2vL < vH , a risk-neutral or risk-averse subject in prob would optimally submit a price of
p = vL in all rounds. In det, submitting a price of p = vL is the dominant strategy.
Part 2 consisted of 5 rounds and values {vL , vH } are selected such that 1.5vL > vH .6 This means
it is optimal to submit p = vH regardless of risk attitudes, in both prob and det. We chose
5

We constrained the selected values to be even numbers so that computations such as 1.5v result in integers.
Subjects are not told the domains from which the values are drawn. They are simply shown the realizations for a
specific round and asked to submit a price. The 19 pairs we implemented in rounds 2-20 were: {10, 86}, {10, 96},
{12, 112}, {12, 136}, {12, 138}, {14, 122}, {14, 140}, {16, 98}, {16, 106}, {18, 80}, {18, 86}, {18, 90}, {18, 122},
{20, 130}, {22, 106}, {24, 126}, {24, 128}, {24, 134}, {28, 126}.
6
In the experiment there was a clear “break” between part 1 and part 2, see the Instructions Appendix for details.
Given that the problems require computations, we were concerned that mixing up problems where (for non risk-seeking

6

random combinations of values with the constraint that both vL and vH had to be even numbers
and furthermore 48 ≤ vL ≤ 54, 54 ≤ vH ≤ 64 and vL 6= vH .7 While once more all subjects saw the
same set of five values {vL , vH }, the order was randomized at the subject level.
In part 3 subjects provided incentivized advice for the case vL = 20 and vH = 120. They recommended what price to submit and why to a future participant, which we refer to as the advisee. We
told them that the advisee will be presented with advice from 5 different participants and that she
will select which of the 5 pieces of advice was the most helpful. We told participants that they would
receive the profits the advisee made in this problem provided that their advice is the one selected
by the advisee. By forcing vL = 20 and vH = 120 to be in the first round of part 1, we ensure that
all subjects have the same amount of experience (none) when they encountered this problem and 24
rounds passed before they provided advice.
In part 4, subjects are presented with 10 rounds that are similar to those in part 1. However, now,
subjects faced the treatment of the problem they did not face so far. This means that subjects who
faced 20 rounds of probabilistic problems in part 1 faced 10 rounds of deterministic problems in part
4 and vice versa. In round 1 of part 4 we fix vL = 20 and vH = 120. For the remaining 9 rounds
we pre-selected 9 pairs with the same criterion as described in part 1, though all were different from
those in part 1.8 Different subjects faced these pairs in a different random order.
Part 5 consists of three questions in which subjects selected one of two lotteries. In all lotteries
there was a 50-50 chance of obtaining a low (π L ) or a high (π H ) payoff. For simplicity we describe

a lottery as L π L , π H . In the three questions agents chose between L (0, 10) and L (−250, 140),
L (0, 20) and L (−180, 120), and L (0, 30) and L (−70, 80), respectively. These lotteries correspond
to prob cases (vL , vH ) = {{10, 140}, {20, 120}, {30, 80}}, where the agent either submits p = vL or
p = vH , respectively.9 All subjects faced these three questions in that same order, though they were
not told how those lotteries were constructed.
Throughout part 1, and in fact throughout all parts of prob and det, and throughout all other
treatments, subjects did not receive any feedback. That is, after each question they answered (e.g.
submitted a price, in part 1), subjects simply received the next question. On the one hand, Charness
and Levin (2009) document that feedback and experience does not remove overbidding (p > vL ) in
the probabilistic case in a problem akin to those of part 1. In addition, while it may be interesting to
address how learning changes the answers, we were mostly concerned that learning would be much
more rapid in det than in prob. The reason is that feedback may be more informative in det than
in prob. In prob, feedback is the outcome of a lottery: Sometimes submitting a high price p = vH ,
while leading to expected losses, may result in large gains. In det, feedback consists of the profit.
agents) p = vL and p = vH was a dominant strategy would increase the fraction of subjects who would not submit
payoff maximizing prices due to computation mistakes.
7
The five pairs we implemented were: {48, 54}, {50, 54}, {52, 58}, {52, 64}, {54, 58}.
8
The 9 pairs we implemented in rounds 2-10 of part 4 were: {10, 82}, {10, 110}, {14, 98}, {16, 80}, {24, 122},
{26, 96}, {28, 92}, {28, 96}, {30, 88}.
9
The {10, 140} and {30, 80} combinations correspond to the most extreme values subjects could have encountered
in part 1 and part 4.

7

In fact, in all our part 1 problems, 3vL < vH , which implies that submitting a price p = vH leads to
a negative profit. Indeed, experiments have shown that learning may be slower when the feedback
is the result of a lottery rather than the expected outcome, for a specific example with such a direct
comparison see Bereby-Meyer and Roth (2006).

2.3

Advisee Treatments

We have a Probabilistic (advprob) and a Deterministic Advisee (advdet) treatment. The subject (advisee) goes through the same instructions and understanding tests as subjects in the main
treatments. A subject then receives the first question of part 1 from the main treatment, where the
values are vL = 20 and vH = 120. Before being given a chance to submit a price for that question,
the subject receives five pieces of advice, one from each of five subjects from the corresponding main
treatment. The subject sees one piece of advice at the time and answers whether the advice is useful
(selecting from: very useful, somewhat useful or not useful at all). Subsequently, the subject sees all
5 pieces of advice at once and indicates which advice is most helpful. Finally, the subject submits a
price for the {20, 120} question and then faces the same 19 questions of part 1 of the main treatment,
in random order. The subject then faces part 2 of the main treatment.

2.4

One-Firm Treatments

In the One-Firm treatments subjects are first confronted with 25 rounds of a simplification of part 1
and part 2 of the main treatments. Specifically, in the first 25 rounds (part 1 and part 2) each subject
submits a price for only a single firm of which they know the value. These 25 rounds correspond
to part 1 and part 2 of the main treatments where, in each round, we randomize at the individual
level which value (which firm) the subject can buy. In part 3, part 4 and part 5 subjects face part
1, part 2 and part 5 of either prob or det.
We implement two One-Firm treatments. In the Probabilistic One-Firm treatment, subjects are
presented with the instructions that correspond to prob. After they have read the instructions,
but before they start part 1 we tell them that they will actually know the value of the firm before
they submit a price. In the Deterministic One-Firm treatment, subjects read the instructions that
correspond to det. Once they finish reading those instructions we tell them that in part 1 there
will actually be only one of the two firms available for sale and that they will know which one of
the two they can buy. That is, in each treatment we reduce the two-contingency problem to a
one-contingency problem, but retain the initial description of the two-contingency problem. Part
3, part 4 and part 5 of the Probabilistic (Deterministic) One-Firm treatment correspond to part 1,
part 2 and part 5 of prob (det).

8

2.5

Hypotheses and Predictions

A common approach to showing the difficulties with conditional reasoning is to show that subjects
are not making profit-maximizing choices, and investigating whether some changes to the problem
reduce those difficulties, see Charness and Levin (2009), Enke and Zimmermann (2017). A few
papers directly compare choices when subjects know the contingency (or as good as) to when they
did not, such as comparing the performance in the the first two parts of the One-Firm treatment
to part 1 of prob, see for example, Esponda and Vespa (2014), Ngangoué and Weizsäcker (2017),
Fragiadakis, Knoepfle and Niederle (2017).10 We expect to replicate that subjects are somewhat
able to compute profit-maximizing prices when there is a single firm of known value in the first parts
of the One-Firm treatment, but that there are fewer cases of profit-maximizing choices in prob.
We start by presenting the corresponding hypothesis, an early discussion of which can be found in
Hamilton (1878), see also Lennie (2003) for a more recent discussion.11

Hypothesis 0: Computational Complexity (Complexity ) One reason why subjects may
find it more difficult to make payoff-maximizing choices in prob compared to onefirm is that the
problem is harder when there are two firms or two possible contingencies to consider. This could be
either because subjects have a hard time thinking about two values, or because they find it difficult
to aggregate this information. This may include that the stakes are simply too small to warrant
expending costly computations needed to achieve payoff maximization. We summarize these issues
as computational complexities.
Computational complexity is one potential explanation why agents who can compute the best response in each contingency fail to do so when there are multiple contingencies. In this paper we
10

Esponda and Vespa (2014) consists of an extreme version of such a result where, while there are multiple contingencies, the agent has a dominant action in one contingency and is indifferent between her actions in all other
contingencies. They show that many subjects who understand what to do in each contingency still fail to behave
optimally when there is uncertainty over which contingency is realized. Specifically, they have a voting experiment
where there is a true state of the world, either red or blue. A committee consisting of 2 robots and a subject vote for
red or blue. If the majority vote corresponds to the true state, the subject receives a strictly positive payoff, otherwise
the subject receives zero. One example of a problem subjects face is that the two computers use the following rule,
which is known to subjects: If the state of the world is red, computers are programmed to vote red; if the state of
the world is blue, computers mix independently between voting red or blue, with each vote being equally likely. The
subject does not know the state of the world and does not know the votes of the computers when submitting her own
vote. Note that the vote of the subject is pivotal only when one computer votes red and the other blue, which only
happens in the blue state, so, the subject has a dominant strategy to vote blue. Put differently, in all contingencies
the vote of the subject is irrelevant, apart from in one case, in which the subject has a strict incentive to vote blue.
Only about 19 percent of subjects eventually always submit the dominant strategy. In a sequential treatment where
subjects are shown the computers’ votes before voting themselves, approximately 70 percent of subjects eventually
always vote blue.
11
Hamilton (1878) poses it in the following way: “Supposing that the mind is not limited to the simultaneous
consideration of a single object, a question arises, How many objects can it embrace at once? You will recollect that
I formerly stated that the greater the number of objects among which the attention of the mind is distributed, the
feebler and less distinct will it be its cognizance of each. ‘Pluribus intentus, minor est ad singula sensus.” ’ A more
recent presentation is in Lennie (2003), who argues that “The need for energy management provides an interesting
physiological perspective on a traditional view of attention as adaptation to the brain’s limited capacity to process
information: energy limitations require that only a small fraction of the machinery can ever be engaged concurrently.”

9

introduce an additional explanation, namely that multiple contingencies are especially hard to handle in the case of uncertainty about the state of the world, rather than when there are simply multiple
certainties.12

Hypothesis 1: The Power of Certainty (PoC)

In prob we have one firm that is equally

likely to have one of two possible values, while in det we have two firms, one of each of the two
potential values in prob. In both prob and det subjects have to submit one price that applies to
all possible values the firm can have. The two treatments differ, however, in one dimension: While
in prob subjects submit a price for one firm with two possible values vL and vH , subjects in det
submit a price that is transmitted separately to the firm vL and the firm vH . The expected profits
in prob are exactly the same as the realized profits in det. That is, in both cases there are two
possible values of the firm to consider. The main difference between the two treatments is that
while in prob there is uncertainty over the contingency (state) that is realized, in det there is no
uncertainty about which contingency is realized. det shares the lack of uncertainty with onefirm,
while, however, keeping the complexity associated with two contingencies from prob.
The Power of Certainty (PoC) hypothesis claims that individuals may have a difficulty to compute
payoff-maximizing actions when there is uncertainty about which state is realized. When both contingencies of the firm exist, however, computing payoff-maximizing actions may be easier. While
there is no active literature on this topic in economics, there are several concepts that hint towards
the fact that it is difficult for individuals to think about uncertainty. In hindsight bias, see Fischhoff
(1975), individuals underestimate the likelihood that other states could have occurred, basically
having a hard time to think about outcomes being uncertain. In fact, there is substantial evidence
that shows that young children tend to view the world in a deterministic manner and often attribute
causal effects to situations of chance, a finding that does not disappear with age (for a survey see
Langrall and Mooney, 2005).13 Believing in superstition or fate are some of the ways by which
individuals reduce the uncertainty inherent in life. In a similar vain, the “control heuristic” (Thompson, Armstrong and Thomas, 1998) provides one possible rationale for the PoC. This heuristic is a
shortcut that involves two elements: the intention of the subject to achieve an outcome (here the
payoff associated with p = vH and v = vH ) and the perceived connection between the subject’s
action and the desired outcome.
In psychology, Hoffrage et al. (2002) find that individuals are better at Bayesian updating when
probabilistic problems are restated using frequencies. One interpretation of our det treatment is
that it presents a frequentist problem (50 percent of the existing firms are of value vL and 50 percent
are of value vH ), and, hence, we too compare a frequentist problem in det to a probabilistic problem
in prob. However, Hoffrage et al. (2002) use their results to dismiss Kahneman and Tversky (1972)
12

Strictly speaking, we have only risk and not Knightian uncertainty in our experiment, but we will use the terms
interchangeably.
13
In fact, Fischbein and Schnarch (1997) suggest that probabilistic reasoning requires the construction and development of new intuitions.

10

by arguing that heuristics and biases may simply be due to subject confusion when faced with an
unfamiliar (probabilistic) frame. We, instead, propose that there is a fundamental difference between
frequencies and probabilities, that there is a Power of Certainty. In our experiment prob and det
are not different frames of the same problem, but are different problems.
To ascertain the role of PoC in accounting for subjects’ difficulties in prob we compare behavior
in parts 1 and 2 between prob and det. Specifically, if subjects find it easier to think through the
problem in the absence of uncertainty, we would expect more subjects to select p = vL in part 1
of det than in prob. However, when we compare the occurrence of prices p = vL across part 1 of
prob and det, we may have underestimated the number of profit maximizing subjects in prob.
Specifically, there is one reason why subjects in prob might submit vL < p = vH : the subject might
be (very) risk seeking.

Hypothesis 2: Risk Seeking (Risk ) While any price p 6= vL is strictly dominated in det, this
is the case in prob only for subjects who are not (very) risk seeking. For subjects who are risk
seeking, submitting a price p = vH can be profit maximizing. Subjects’ lottery choices in part 5
provide a direct test whether they prefer the risky lottery that corresponds to p = vH over the safer
lottery that corresponds to p = vL . We can then correlate their lottery choices with their propensity
to submit p = vL in part 1.14
The lottery choices in part 5 provide a risk-seeking measure that closely mirrors the choices of
submitting a price in part 1 (see Niederle, 2016 for a discussion of the virtues of such an approach).
In addition, we can use the lottery choices to address losses due to presenting the complicated
probabilistic problem rather than the simple lottery choices that result from submitting either p = vL
or p = vH (see also Ambuehl, Bernheim and Lusardi, 2014).
Since we want to establish the importance of the PoC hypothesis, it is not only important that
we do not underestimate the number of profit maximizing subjects in prob, but also that we do
not overestimate the number of profit maximizing subjects in det. Specifically, we do not want to
misclassify subjects who use a rule of thumb as profit maximizers in det. Therefore, we introduce
part 2 of the main treatments where p = vH is the profit maximizing price (in both det and prob).
Hence, subjects who submit p = vL in any deterministic (or probabilistic) problem of two firms
independent of the actual values of vL and vH will reveal themselves as not profit maximizing in
part 2.

Hypothesis 3: Rules of Thumb (Rules of Thumb) There are several rules of thumb that
subjects may use that would make them appear to be profit maximizing in part 1. For example,
subjects could submit p = vL in part 1 independent of the values of vL and vH since it feels “cheaper”
14
While there are many measures addressing how risk averse an individual is, there is no standardized measure for
how risk seeking an individual is. Recall that even a risk-neutral, let alone a risk-averse individual finds p = vL to be
the dominant choice in part 1 of prob.

11

than p = vH . Alternatively, it could be that subjects prefer to think about one firm only. While in
prob the subject can buy at most one firm, she can buy two firms in det if she submits a price
p ≥ vH . A subject in det who wants to avoid buying two firms could submit a price below vH .
Such a “constraint” would vastly simplify the problem, and could lead to fewer prices that can lead
to losses and more selections of p = vL .
We need a simple way to test whether rules of thumb lead to a high occurrence of p = vL (independent
of the values of vL and vH ) especially in det. Therefore, in part 2 of the main treatments, we
change the values {vL , vH } such that 1.5vL > vH and the dominant strategy is to submit a price
p = vH . We use the combination of submitted prices in part 1 and part 2 to better understand the
behavior of subjects and assess whether they consistently use profit maximizing strategies and have
an understanding of the problem.
Note that a rule of thumb that leads subjects to either always submit p = vH or mix between {vL , vH }
is easily recognized as dominated in det, though in prob such subjects may be misclassified as being
very risk seeking albeit profit maximizing in part 1.

2.6

Understanding the Power of Certainty

We aim to not only show the significant role of the PoC hypothesis, but also its relative importance.
We first use the comparison between onefirm and prob to assess the quantitative problem that is
in general associated with Complexity. We then use the relative outcome of det between on the one
hand onefirm, which will account for the difference between a single known certainty and multiple
known certainties, and prob, which will account for the difference between two contingencies that are
certain versus uncertain. Second, we will use the choices in part 5 that correspond to lotteries where
the agent either submits p = vL or p = vH in probabilistic problems. We compare the performance of
subjects who have to compute the four possible outcomes v, p ∈ {vL , vH } themselves in probabilistic
problems rather than receiving those four outcomes directly and only choosing between the lotteries
that correspond to submitting vL or vH , respectively.
To begin to investigate the underlying reasons for the PoC, note that in both prob and det subjects
have to consider all possible four outcomes v, p ∈ {vL , vH } – just as we explicitly do for subjects
choosing between lotteries in part 5. We first use the (incentivized) advice subjects provide for a
different participant to assess whether subjects, in at least some form, mention those four outcomes.
We then assess to what extent this can account for differences between det and prob. Second,
after providing advice, in part 4, subjects submitted prices in part 1-style problems, though now in
the other treatment. That is, subjects in det encountered probabilistic problems and vice versa.
We assess whether mentioning all four outcomes has a significant impact on the performance of
subjects in part 4. Third, we use the Advisee treatments advprob and advdet for some external
verification of the role of thinking about all four outcomes. Finally, we assess the role of forcing
subjects to think about outcomes by having subjects first solve 25 rounds of problems involving a
single firm with known value in onefirm. This may increase the understanding of the problem and
12

help subjects to focus on outcomes when submitting prices. We study whether after that experience,
differences between probabilistic and deterministic problems are still large and present.

2.7

Procedures

Our subjects are Amazon Turk workers located in the US with a rating of 90 percent or higher.
Subjects received a link to a qualtrics survey (see the Instructions Appendix which contains all the
surveys). Participants knew that if they made more than two mistakes on the instructions for part 1
of any treatment, they were not allowed to continue the experiment. Upon finishing the experiment,
we asked survey questions pertaining to the sex, age, ethnicity, education and state of residence of
the participant. We have a total of 880 Amazon Turk workers with unique IP addresses, of which
44.8 percent are female, 74.8 percent are White, 49 percent are at or below the median age of 32
and 38.9 percent have low schooling (‘Some college,’ ‘High School’ or lower education level).15
Probabilistic and Deterministic Treatments We aimed to recruit 200 participants per treatment. In total 425 Amazon Turk workers with a unique IP address started the experiment (211 prob
and 213 in det), but 54 made more than 2 mistakes in the instructions for part 1.16 Eventually, we
were left with 188 and 183 subjects in prob and det, respectively.
Payments in prob and det were determined as follows. A participant received $4 for finishing the
survey, as well as another $4 at the beginning of part 1 to which they can add or subtract depending
on their choices in the experiment. In each of part 1, part 2 and part 4 we randomly select one
round for payment. In part 3 subjects are paid the advisee’s payoff in case their advice was selected.
In part 5 we randomly select one of the three lottery choices and pay subjects based on their chosen
lottery. Payoffs in all questions are expressed in points which are subsequently converted to dollars.
In parts 1, 3 and 5 we paid 3 cents per point. In parts 2 and 4 we paid 1 cent per point.17 On
average a participant received $8.7 (including the $4 for finishing the survey) and on average the
survey took approximately 40 minutes to complete.
We paid special attention to ensure that the instructions of prob and det are as similar as possible.
For example, when describing the problem in prob we use the phrase “transaction of the company,”
while in det we use “transaction for each company.” In Appendix E we provide a brief summary of
how we explained each problem in the instructions and show screenshots for part 1 round 1 of prob
and det (see Figure 9). In the Instructions Appendix we provide the full instructions.
15

The regression tables presented in the results section and the appendix control for these demographic variables.
In particular, we construct a gender dummy, an ethnicity dummy that takes value 1 if the responder selected ‘White’
and 0 otherwise, an age dummy that takes value 1 if the responder is at or below the median age, and a low schooling
dummy that takes value 1 if the responder selected ‘Some college,’ ‘High School’ or lower education level.
16
Of the 54 subjects, 23 and 31 correspond to prob and det, respectively. The difference in the drop rate is not
significant across treatments.
17
Note that in part 2, the average payment in points from submitting the dominant price is higher than in part 1,
and furthermore, there are fewer problems in part 2. We therefore used a lower exchange rate in part 2. However,
just to ensure subjects do not react too strongly to only 1 cent payments, we also used that payment in part 4. We
find no indication that the reduced payments have an effect.

13

Advisee Treatments We recruited 90 Mturkers who had not participated in an earlier treatment,
half of which were assigned to each treatment. We drop 4 subjects in the probabilistic and 5 subjects
in the deterministic version who make more than two mistakes in the instructions. Eventually we
are left with 41 subjects in advprob and 40 in advdet.18
One-Firm Treatments We recruited 468 Mturkers (233 in the probabilistic and 235 to the deterministic version) who had not participated in an earlier treatment. However, 40 subjects made more
than two mistakes in the instructions for part 1 which leaves us with 216 subjects in the Probabilistic
and 212 in the Deterministic One-Firm treatment.19

3

The Power of Certainty

We first classify subjects based on their choices in part 2 where p = vH is a dominant strategy in
both prob and det. The difference between these two treatments provides a first indication of the
PoC hypothesis. We then turn to the classic form of the acquiring-a-company problem in part 1,
which is perhaps more difficult and where p = vL is a dominant strategy in det and in prob for
subjects who are not (very) risk seeking. We assess whether subjects are able to adjust their prices
to the values of the two firms to address the role of Rules of Thumb. We then examine whether
risk-seeking subjects account for prices p = vH in part 1 of prob and hence the Risk hypothesis.
Finally, we assess the relative importance of PoC in the more general problem of Complexity. We
present all our results using classification of subjects into types. This allows us to address both
the role of rules of thumbs as well as of risk-seeking preferences. We show in Appendix A that our
findings are robust to instead considering the distribution of prices.20
18

A participant received $4 for finishing the survey. Participants are then endowed with another $4 at the beginning
of part 1 and can add/subtract the payment from one randomly selected round for part 1 and one randomly selected
round for part 2. Payoffs are expressed in points and transformed to dollars at 3 cents per point in part 1 and 1 cent
per point in part 2. On average a participant received $8.2.
19
Of the 40 subjects, 17 and 23 correspond to the probabilistic and deterministic version, respectively. The difference
in the drop rate is not significant across treatments. A participant received $4 for finishing the survey. Participants
are then endowed with another $4 at the beginning of part 1 and can add/subtract the payment from one randomly
selected round for parts 1, 2, 3 and 4, and from one randomly selected lottery in part 5. In parts 1, 3 and 5 we paid 3
cents per point. In parts 2 and 4 we paid 1 cent per point. On average a participant received $8.6. While it is possible
for subjects to have total earnings below the $4 they were endowed with to potentially lose during the experiment,
of the 880 subjects that are in our final sample, only 24 did so, for those subjects we paid only the $4 they were
guaranteed, and we did not implement their full losses above the $4 they were endowed with initially.
20
The approach we follow in the text, which classifies subjects into types, demands consistent behavior from subjects.
In contrast, the analysis on submitted prices in Appendix A does not rely on consistency of behavior. This means that
there will be a difference in levels. For example, the aggregate frequency of prices p = vL is higher than the frequency
of subjects who submit p = vL in all periods. While there is a difference in levels, our conclusions comparing outcomes
across treatments are not affected. In the appendix we also present classifications of subjects into types allowing for
small deviations, and again, we reach the same conclusions.

14

3.1

Part 2 Behavior

In part 2 (rounds 21-25) the values {vL , vH } are such that 1.5vL > vH which makes submitting
the price p = vH a dominant strategy in both prob and det. Figure 1a shows the proportion
of subjects who placed a price p = vH from round n to 25, for 21 ≤ n ≤ 24. Subjects in det
are about 15 percentage points more likely to do so than subjects in prob. Another price that
may be focal, though dominated, is p = vL . Figures 1b and 1c show the fraction of subjects who
consistently submit p = vL and who strictly mix between p = vL and p = vH from round n onwards,
for 21 ≤ n ≤ 24, respectively. The use of non-optimal focal prices is somewhat more prevalent in
prob than in det. Finally, we show the fraction of subjects who consistently submit a dominated
price that is non-focal, that is p ∈
/ {vL , vH }. Once more this strategy is slightly more prevalent in

80
70
Percentage of subjects
30
40
50
60
20
10

10

20

Percentage of subjects
30
40
50
60

70

80

prob than in det.

21

22

23
Round

24

0

0

Probabilistic
Deterministic

21

25

22

24

25

24

25

70
Percentage of subjects
30
40
50
60
20
10
0

0

10

20

Percentage of subjects
30
40
50
60

70

80

(b) p = vL .

80

(a) p = vH .

23
Round

21

22

23
Round

24

25

21

22

(c) p ∈ {vL , vH } and at least one of each .

23
Round

(d) p ∈
/ {vL , vH }

Figure 1: Evolution of Types in Part 2.
Notes: Percentage of subjects who submit the described price from round n onwards, for 21 ≤ n ≤ 24 by treatment.

Figure 1, in addition, shows that the fraction of subjects classified as one of the aforementioned types
does not change much whether we consider subjects who submit a given price in all five rounds or
15

Part 2 Types

VH

VL

Mix

Dom

Residual

Participants

prob

47.9

10.1

8.5

24.0

9.5

188

det

65.0

2.2

7.1

20.2

5.5

183

Table 1: Part 2 Type Classification [as % of participants]
Notes: Subjects are classified as belonging to a type if their submitted price corresponds to the same type in all 5 rounds 20-25
in part 2. VL :p = vL . VH :p = vH . Mix: p = vL or p = vH (and at least one of each). Dom: p ∈
/ {vL , vH }. Residual: all other
subjects.

only in the last two rounds. Therefore, we classify subjects as a type if they submit the type-specific
price in all five rounds of part 2, see Table 1.
Table 1 shows that in det we have substantially more subjects classified as using the dominant
strategy VH , that is subjects who submit the dominant strategy p = vH in all five rounds, and this
difference is significant.21 In turn, subjects in prob are somewhat more likely to be classified as
using focal strategies that are dominated, such as VL (submitting p = vL in all five rounds) or Mix
(strictly mixing between p = vL and p = vH ).22 There are 9.5 and 5.5 percent of subjects who do
not follow any of these classifications in prob and det, respectively.

3.2

Part 1 & Part 2 Behavior

In part 1, p = vL is a dominant strategy for subjects in det and for subjects in prob who are
not (very) risk seeking. For those latter subjects, dependent on the values of vL and vH , a price of
p = vH might be utility maximizing. Since subjects are mostly risk averse, and in experiments often
very risk averse (see Holt and Laury, 2002), subjects who submit p = vL in part 1 and p = vH in
part 2 should comprise almost all subjects who are submitting prices that maximize their payoffs.
Thus, we first explore how many of the subjects classified as VH in part 2 also submit p = vL in
part 1.
We classify a subject as VL VH if she submitted a price p = vL in the last five rounds of part 1 (VL
in part 1) and p = vH in all five rounds of part 2 (VH in part 2).23 The fraction of VL VH subjects in
det (41.5 percent) is more than twice the corresponding fraction in prob (19.5 percent), confirming
the PoC hypothesis.
The vast majority of subjects in both treatments can be classified as using either the payoffmaximizing VL VH strategy, or one of three other alternatives. The second strategy includes behavior
21

To test for the treatment effect we run a linear regression in which the left-hand side variable takes value 1 if the
subject is classified as type VH and on the right-hand side we include a treatment dummy (1=det) and a constant.
The treatment effect estimate is 0.17 and the p-value 0.001.
22
In part 2, a price p = vL is also dominated, but for expositional purposes we keep that category separate from
Dom.
23
The classification of subjects into types using exclusively the last five periods of part 1 is presented in Table 20
of Appendix B.

16

that is potentially profit-maximizing in prob. It requires that subjects are classified as VH in part
2 and, in addition, either strictly mix vH and vL in the last five rounds of part 1 (M ixVH ), or select
p = vH in all the last five rounds of part 1 (VH VH ). We group subjects who exhibit either behavior
in type {M ixVH , VH VH }.
In prob, the {M ixVH , VH VH } type contains 24.5 percent of all subjects, more than the percent
classified as VL VH . One interpretation is that many of those subjects are risk seeking and profit
maximizing. This seems, however, implausible given results in other experiments which often find
few subjects to be very risk seeking, as well as for two other reasons. First, there are also many such
subjects in det (15.9 percent), where these strategies are dominated independent of risk preferences.
Second, there are other strategies that rely on the subject exclusively submitting prices that are either
vL or vH (focal prices), which are not payoff maximizing in either treatment: These consist of part
1/part 2 strategies: VL VL , VL M ix, M ixVL , M ixM ix, VH VL , and VH M ix, which we group as a
third strategy type labeled ‘Focal.’ In total, 18.7 percent of subjects in prob are classified as using
Focal strategies that cannot be payoff maximizing, compared to 8.3 percent in det. The prevalence
of Focal strategies in prob therefore suggests that the higher incidence of VH VH and M ixVH types
in prob may be due to a general larger incidence of types in prob who use only vL and vH prices,
though not necessarily in a profit-maximizing way.
In addition, among the Focal strategies, there are two that show an insensitivity towards the fact
that values of the companies are changing between part 1 and part 2, namely VL VL and MixMix.
Such subjects are almost exclusively present in prob, where 8.0 and 3.2 percent of participants are
classified in these categories compared to 1.1 and 1.6 percent in det, respectively.
Finally, we also find more subjects in prob systematically submitting dominated prices that are
neither vL or vH . This fourth strategy type, which we label ‘Dominated,’ consists of subjects
submitting p ∈
/ {vL , vH } in rounds 16-25.24 There are 21.8 percent of subjects classified as this type
in prob compared to 15.9 percent in det.
Table 2 presents a summary of the classification of subjects into the four strategy types that use
the last five rounds of part 1 and the five rounds of part 2.25 Figure 2 shows the evolution the
classification if we also include the first fifteen rounds of part 1. For each of the four strategies,
each figure reproduces the proportion of types using data from rounds 16-25, but also shows how
the proportions would change if, in addition, we demand subjects to follow the corresponding part
1 portion of the strategy not only in the last 5 rounds but starting from round n ≤ 15.
In particular, Figure 2a follows the proportion of subjects who are eventually classified as VL VH .
Notice that the fraction of such profit-maximizing subjects in det is almost twice that of prob for
any given n.26 This is, again, strong indication of the significant role of PoC. At the same time,
24

In part 1 of det submitting p = vH is also dominated, but for classification purposes and to ease the comparison
to prob we do not consider subjects who consistently submit p = vH in the dominated category.
25
Subjects classified as ‘Residual’ submit p ∈
/ {vL , vH } at least once and either p = vL or p = vH at least once in
rounds 16-25.
26
In round 1 the values were selected to match the parametrization of Charness and Levin (2009). In the first round

17

Types

VL VH

{M ixVH , VH VH }

Focal

Dom

Residual

Participants

prob

19.7

24.5

18.7

21.8

15.4

188

det

41.5

15.9

8.2

15.9

18.5

183

Table 2: Part 1 and Part 2 Type Classification [as % of participants]
Notes: Types are defined based on the prices p1 submitted in the last five rounds of part 1 and p2 submitted in all five rounds
of part 2. Type VL VH : p1 = vL and p2 = vH . Type {M ixVH , VH VH }: p1 ∈ {vL , vH } and at least one p1 = vH and p2 = vH .
Type ‘Focal’: p1 , p2 ∈ {vL , vH } and at least one p2 = vL (corresponds to VL VL , VL Mix , Mix VL , MixMix , VH VL , or VH Mix ).
Type ‘Dom’: p1 , p2 ∈
/ {vL , vH }. Residual: All remaining subjects.

there seems to be some learning in both treatments (though there is no feedback): the number of
such subjects increases by roughly 50 percent when we move from considering subjects who submit
vL from round 1 onwards compared to those who submit vL in the last 5 rounds (and always submit
vH in part 2).
In Appendix B, we relax the definition of types in two ways and show that we would reach the same
conclusions. First, we classify subjects as a given type even if their prices conform to the type in
only four of the last five rounds in part 1 and four of the five rounds in part 2, see Table 21 and
Figure 7. Second, allow subjects to submit a price that is slightly higher than the price describing
the type of the subject. Although we explicitly asked a question in the instructions, it is possible
that some subjects think that they have to bid slightly above v in order to buy the firm. In the
second robustness exercise a subject is classified as VL VH if p ∈ [vL, , vL + 2] in the last 5 rounds of
part 1 and if p ∈ [vH , vH + 2] in all rounds of part 2. See Table 23 for details.

3.3

Risk Seeking Preferences

While the previous section already presented evidence suggesting that the prevalence of p = vH
prices in prob is not due to subjects being risk seeking, we now test for this hypothesis directly.
In part 5 subjects chose between three sets of lotteries, where there is a 50-50 chance of obtaining

a low (π L ) or a high (π H ) payoff. For simplicity, we describe a lottery as L π L , π H . Participants choose between L (0, 10) and L (−250, 140), L (0, 20) and L (−180, 120), and L (0, 30) and
L (−70, 80), which correspond to the probabilistic problems {vL , vH } of {10, 140}, {20, 120}, and
{30, 80}, if subjects were to submit p = vL and p = vH , respectively.27 The cases of {10, 140} and
{30, 80} present the extreme values subjects subjects could have encountered in part 1, and represent the case where the difference in expected returns between p = vL and p = vH is maximized or
of Charness and Levin’s “two-value treatment” 2VT treatment {0,99}, 29.2 percent of subjects submit a price equal to
the low value, 55.7 percent submit a dominated price and 15.1 percent submit a price equal to the high value. In the
first round of prob, 42.0 percent of subjects submit a price equal to the low value, 40.4 percent submit a dominated
price, and 18.6 percent submit a price equal to the high value. We have therefore no indication that the Amazon Turk
subjects behave differently to the undergraduate students in Charness and Levin (2009).
27
Recall that subjects were not told that these lotteries corresponded to possible cases of part 1. All subjects faced
these three questions in that same order.

18

50

50

Percentage of subjects
20
30

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

40
Percentage of subjects
20
30

40

50

(b) p = vH or strict mix between vL and vH in rds n ≤ 15
and classified as {M ixVH , VH VH }.

50

(a) p = vL in rds n ≤ 15 and classified as VL VH .

Focal: 18.6%

Dominated: 15.9%

0

10

Focal: 8.2%

Dominated: 21.8%

0

Percentage of subjects
20
30

{MixVH, VHVH}: 15.9%

0

0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

10

{MixVH, VHVH}: 24.5%

10

VLVH: 19.7%

10

Percentage of subjects
20
30

40

VLVH: 41.5%

40

Probabilistic
Deterministic

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(c) p ∈ {vL , vH } in rds n ≤ 15 and classified as Focal.

(d) p ∈
/ {vL , vH } in rds n ≤ 15 and classified as Dominated.

Figure 2: Evolution of Types (part 1 & part 2).

19

minimized, respectively. Overall, 70.0 percent of subjects in det and 68.6 percent in prob never
took any of the lotteries that correspond to submitting p = vH .28
To directly address the role of risk-seeking preferences, Figure 3a shows the fraction of subjects in
prob who submit p = vL from round n until 20 and p = vH in rounds 21-25 (part 2), depending on
whether the subject took at least one risky lottery or not. The fact that there is a higher fraction
of such subjects among those who who never took a risky lottery compared to those who took at
least one risky lottery is consistent with risk-seeking subjects using strategies other than VL VH .
However, the simultaneous increase of subjects who from round n until round 25 submit prices that
correspond to a focal strategy that is never profit maximizing as well as the increase of subjects who
consistently use dominated prices, see Figure 3e and 3g, is not consistent with subjects who select
risky lotteries largely being profit maximizing subjects who are risk seeking.
A second indication that risk-seeking preferences may not be the only and perhaps not even the
main driving factor for prices p = vH in part 1 in prob comes from Figures 3b, 3d, 3f and 3h that
capture participants in det. We see changes in the fraction of subjects who use given strategies
from round n onwards between subjects who never took a risky lottery and those who took at least
one risky lottery in det that are similar to changes observed in prob. This is despite the fact that
in det risk preferences have no bearing: submitting a price p = vL in part 1 and p = vH in part 2
is the unique dominant strategy regardless of whether the subject is risk seeking!
(1)
VL VH

(3)
Focal

(4)
Dom

−0.102∗
(0.056)

−0.053
(0.048)

−0.074
(0.053)

−0.062∗∗
(0.031)

−0.008
(0.029)

−0.003
(0.025)

Num Risky × Det

0.043
(0.046)

0.040
(0.043)

−0.011
(0.036)

Constant

0.286∗∗∗
(0.069)

0.223∗∗∗
(0.065)

Det
Num Risky

Observations

0.244∗∗∗
(0.060)

371

(2)
{M ixVH , VH VH }

371

0.203∗∗∗
(0.055)
371

0.073∗∗∗
(0.027)
−0.052
(0.040)
0.147∗∗
(0.061)
371

(5)
Dom or Res
−0.089
(0.063)
0.073∗∗
(0.033)
−0.071
(0.048)
0.287∗∗∗
(0.073)
371

Table 3: Main Treatments: Estimation output using last 5 rounds of part 1 and part 2 for the
classification of types
Notes: Results from a linear regression. The dependent variable takes value 1 if the subject is classified as (1) VL VH , (2)
{M ixVH , VH VH } (3) Focal, (4) Dom - Dominated, (5) Dom or Res - Dominated or Residual type. Det is a treatment dummy
that equals 1 if the subject participated in det. Num Risky is the number of risky lotteries the subject chooses in part 5 (from
0 to 3). The regression also includes demographic controls (Gender, Ethnicity, Age and Schooling) and a control for the number
of errors in the instructions. The full output is presented in Table 28 of the Online Appendix.

Finally, the differences between prob and det described in the previous Sections are robust to
controls for risk preferences. A linear regression on the classification of subjects (Table 3) shows that
28

The risky alternative was selected only once by 15.4 percent (15.9 percent), twice by 4.8 percent (4.9 percent),
and in all three lotteries by 11.2 percent (9.3 percent) of subjects in the prob (det) treatment.

20

50
0

10

Percentage of subjects
20
30

40

50
0

10

Percentage of subjects
20
30

40

No Risk
At least one risky

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

40
Percentage of subjects
20
30
10
0

0

10

Percentage of subjects
20
30

40

50

(b) det: p = vL : n to 15 & VL VH .

50

(a) prob: p = vL : n to 15 & VL VH .

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(d) det:
p ∈
{M ixVH , VH VH }.

n to 15 &

{vL , vH }:

n to 15 &

40
Percentage of subjects
20
30
10
0

0

10

Percentage of subjects
20
30

40

50

{vL , vH }:

50

(c) prob:
p ∈
{M ixVH , VH VH }.

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

40
Percentage of subjects
20
30
10
0

0

10

Percentage of subjects
20
30

40

50

(f) det: p ∈ {vL , vH }: n to 15 & Focal.

50

(e) prob: p ∈ {vL , vH }: n to 15 & Focal.

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(g) prob: p ∈
/ {vL , vH }: n to 15 & Dominated.

(h) det: p ∈
/ {vL , vH }: n to 15 & Dominated.

Figure 3: Evolution of Types (Part 1 & Part 2) by Part 5 lottery choices
21

there are significantly more subjects classified as VL VH in det compared to prob, when controlling
for subjects’ lottery choices in part 5.
When we consider the types that consist of strategies that may be dominant for a risk-seeking
subject in prob (though never in det), namely VH VH and M ixVH , they are indeed less likely in
det. However, a final piece of evidence that the number of risky lotteries is not a sign of subjects
being very risk seeking but otherwise payoff maximizing is the following: While taking more lotteries
significantly reduces the chance for subjects to be classified as VL VH , it increases the chance to be
classified as the Dominated type. Furthermore, the effect of risk is never significantly different in
det compared to prob, though risk preferences have no bearing in det.29
To summarize, we do not find evidence for the Risk hypothesis, that is risk-seeking preferences
playing a role in accounting for the treatment effect between prob and det. This is not surprising
given that individuals are in general classified as risk averse, and a subject in part 1 would have to
be quite risk seeking for p = vL not being the profit-maximizing price in prob as well as in det.

3.4

The Role of the Power of Certainty

While in the previous subsections we provided evidence for PoC, in this subsection we address the
relative importance of PoC in accounting for the difficulties when moving from an environment
involving a decision for a single known contingency to an environment with several contingencies
that are uncertain. That is, what fraction of the problem is due to the fact that the number of
contingencies increases from one to two (Complexity) and what fraction is due to the fact that
the two contingencies are uncertain rather than certain (PoC )? Put differently, when comparing
the behavior in prob to the behavior of subjects in the One-Firm treatment: How much of the
difference between the two treatments can be accounted for by det? If PoC plays a quantitatively
significant role, then det should not be almost identical to prob, but rather be different from prob
and moving towards outcomes in onefirm.
We first describe results of the 25 rounds in the One-Firm treatment, where the problem consists of a
single firm of known value. Note that we find no statistical difference depending on whether subjects
were introduced to the one-firm environment using probabilistic or deterministic instructions.30
Consequently, we pool all data from both treatments and we label the joint treatment as onefirm.
When we classify subjects based on the prices they submit in the last 10 rounds, 62.6 percent
submit only p = v, a strategy type that we refer to as V .31 A total of 13.6 percent of subjects
29

While in this and all other regressions we use the number of risky lotteries subjects chose, our results are robust
to instead using a dummy indicating whether a subject chose at least one risky lottery, see Table 24 in Appendix C.
30
We run a panel regression in which the left-hand side is a dummy variable (1 if p = v) and the right-hand
side includes a treatment dummy (1 if the subject was introduced to the one-firm problem after reading deterministic
instructions), demographic controls and a constant. The estimated treatment dummy is quite small and not significant
in the whole sample (-0.012, p-value = 0.717) or if we constrain the sample to prices in the last 10 rounds (0.005,
p-value = 0.883).
31
Appendix A describes the aggregate distribution of prices that subjects submitted in this treatment when there
is no constraint of consistency within subject.

22

All

VL VH /V

{M ixVH , VH VH }

Focal

Dom

Residual

Participants

prob

19.7

24.5

18.6

21.8

15.4

188

det

41.5

15.9

8.2

15.9

18.6

183

onefirm

62.6

–

–

13.6

23.8

428

No

prob

24.8

25.6

18.6

15.5

15.5

129

Risky

det

43.8

15.6

9.4

11.7

19.5

128

Lottery

onefirm

67.8

–

–

13.4

18.8

276

At least one

prob

8.5

22.0

18.6

35.6

15.3

59

Risky

det

36.4

16.4

5.5

25.5

16.4

55

Lottery

onefirm

53.3

–

–

13.8

32.9

152

Table 4: Type classification using rounds 15-25 [as % of participants]
Notes: Types are defined based on the prices p1 submitted in the last five rounds of part 1 and p2 submitted in all five rounds
of part 2. Type VL VH : p1 = vL and p2 = vH . Type {M ixVH , VH VH }: p1 ∈ {vL , vH } and at least one p1 = vH and p2 = vH .
Type ‘Focal’: p1 , p2 ∈ {vL , vH } and at least one p2 = vL (corresponds to VL VL , VL Mix , Mix VL , MixMix , VH VL , or VH Mix ).
Type ‘Dom’: p1 , p2 ∈
/ {vL , vH }. Residual: All remaining subjects. In onefirm subjects are classified as V if they submit p = v
in rounds 16-25, and as ‘Dom’ if p 6= v in rounds 16-25, with Residual being the remaining subjects.

submit dominated prices: 8.6 percent submit only p > v, 1.2 percent submit only p < v, and 3.8
percent submit either p < v or p > v in the last 10 rounds. Subjects who submit each, a dominated
price and p = v in at least one of the last 10 rounds are classified as the Residual type, which covers
23.8 percent, see Table 4.32
If onefirm, where there is only one firm of known value, is a benchmark for the number of subjects
who consistently make profit-maximizing choices, then 62.6 percent of subjects consistently do so in
rounds 16-25. In contrast, when there is one firm of two possible values, both equally likely, as in
prob, only 19.7 percent of subjects submit the dominant strategy (for not very risk-seeking subjects)
in rounds 16-25, namely p = vL in rounds 16–20 and p = vH in rounds 21-25. One interpretation
is that among the subjects who consistently submit profit-maximizing prices in onefirm, only 31.5
percent are able to do so when the firm has two possible values each with 50 percent chance.33
Put differently the 42.9 percentage points reduction in subjects who submit profit-maximizing strate32

If we relax the definition and allow for two periods (out of the last 10) in which the subject did not submit p = v,
the proportion of subjects classified as V increases from 62.6 to 67.1 percent. Likewise, if we allow for at most two
periods in which the subject did not submit a dominated choice, the proportion of subjects submitting dominated
choices increases from 13.6 to 20.5 percent. Table 21 in Appendix B presents further details. In addition, Table 23 in
Appendix B shows the results if we relax the definition allowing for optimality being defined as the subject submitting
p ∈ [v, v + 2]. That is, we consider a choice to be optimal if the overbidding is at most two units above v. The
proportion of subjects classified as V increases to 66.8 percent.
33
For a qualitative comparison, we summarize the corresponding findings of Esponda and Vespa (2014). In their
sequential voting treatment (in which subjects can perfectly infer the state before making a decision) approximately 76
percent of subjects consistently make the optimal decision. However, when subjects subsequently make a decision in
which the state is unknown (but in which subjects do know the probability distribution over the states) approximately
22 percent of subjects make the optimal decision. Even though the environment in Esponda and Vespa (2014) is quite
different from ours, and they use NYU students rather than Amazon Turk workers, the quantitative effects are
surprisingly similar.

23

gies is an upper bound of the size of the problem created by the computational Complexity hypothesis. We use det to assess the relative impact of PoC. In det, 41.5 percent of subjects consistently
use profit-maximizing strategies. That is 21.8 percentage points of the problem, slightly more than
half of the total effect, is driven by probabilistic difficulties, the lack of PoC, and only the remaining
half can be attributed to pure computational complexity, the fact that subjects have to deal with
multiple (two) contingencies rather than just one.
To ensure that risk-seeking preferences do not distort results, the second set of rows of Table 4 shows
the classification of subjects who never took any risky lottery and hence are not very risk seeking.
Among those subjects, the strategy VL VH is the only dominant strategy. The difference between
onefirm and prob for this type is 43 percentage points. Once more, the proportion of subjects in
det who submit profit-maximizing strategies is roughly in between onefirm and prob. Of the 43
percentage point difference, a simple decomposition suggests that 24 percentage points are related
to computational complexity, and 19 percentage points to the fact that there is uncertainty in prob.
An alternative approach to evaluate the relative role of PoC is to measure differences between
treatments in the payoff space. The first measure we provide is simply the payoffs subjects achieve,
where for prob we compute expected payoffs. Note that to compare the earnings to the onefirm
benchmark in part 1 problems, we restrict attention to onefirm cases where the realized value of
the firm was vL and we simply use the submitted price p to compute earnings as 1.5 × vL − p (since
if we were to use values of vH in part 1 there would be profits in onefirm that are simply not
achievable by the dominant strategy in det or - for risk-neutral subjects - in prob).34 Averaging
earnings over all 25 rounds of part 1 and part 2, of the total payoff gain for the median subject in
onefirm relative to prob, 69.4 percent is achieved by the median subject in the det. That is, for
the median subject, PoC accounts for approximately seventy percent of the payoff loss incurred by
contingent reasoning.35
Instead of using realized prices, we can ask which fraction of gains a subject made when comparing
their payoffs to, on the one hand, random behavior, and, on the other hand, optimal behavior.
Specifically, we compute a relative payoff variable in which the numerator is the subject’s profit
minus the profit that would result from random prices and the denominator is the difference between
the profit of the price-maximizing action for a risk-neutral subject and the profit that results from
34

Specifically, in onefirm subjects saw 20 randomly drawn numbers of the 40 values that correspond to the 20 vL
and 20 vH values in part 1. For each subject we use only values of v that correspond to a value of some vL in part
1. That is, there is a chance that some subjects had two values of vL = 28, while subjects in prob or det only saw
one value of vL = 28. To compute the profits in onefirm of part 2, we consider only values v that correspond to a
value of vH in part 2. For a given value of vH there might be several values of vL that are coupled to it in part 2: For
example, there are two questions for which vH = 54. One with vL = 48 and another with vL = 50. We assign to half
the cases of v = 54 a value vL = 48 and the other half a value of vL = 50. The profits in a given round of part 2 of
the subject in onefirm is computed whenever v = vH and the profit is (1.5 × vL − p) + (1.5 × vH − p), where vL is
one of the values associated with vH in part 2 of prob or det. For each part, we compute average profits by round
given the relevant choices as described above.
35
If we constrain the sample to subjects who never took a risk in part 5 the number is 58.4 percent. This is because,
particularly in prob, subjects who take risks in part 5 are more likely to obtain lower earnings in part 1 than those
who never took a risky lottery.

24

random prices.36 Of the total relative payoff gains for the median subject in onefirm relative to
prob, 71.4 percent is achieved by the median subject in det.37
As a final approach we use the lottery choices in part 5 as a proxy for the subject’s optimal choice
in part 1 (in part 2 the optimal choice is p = vH ). Specifically, the lottery choice is a reduction
or simplification of a problem akin to those in part 1 in prob, as the two lotteries correspond to
the lotteries that result from p = vL and p = vH , respectively. We therefore assume that choices in
this simplified “probabilistic problem” are a better predictor of underlying preferences over outcomes
than the choice of a price in the more complicated probabilistic problem of part 1 (see Ambuehl,
Bernheim and Lusardi, 2014 for a similar approach). That is, we consider that p = vL captures
the welfare-maximizing choice for subjects in det as well as for any subject in prob who always
selected the safer lottery in part 5 (which corresponds to submitting p = vL ). For subjects in prob
who took some risks in part 5, the optimal choice can be either vL or vH , where for p < vH we
use vL and for p ≥ vH we use vH . For subjects in prob who selected the risky lottery in all three
questions of part 5 we assume that vH is the optimal choice. Finally, p = v is considered optimal in
the onefirm treatments.
Note that for some subjects, the imputed welfare maximizing choice in prob does not yield the
highest expected earnings. We therefore cannot simply aggregate earnings. Rather, we ask how
often a subject made the imputed welfare-maximizing choice. We then compute the gain in terms
of the fraction of welfare-maximizing choices for the median subject in onefirm relative to the
median subject in prob. We find that 39.3 percent of the increase in that fraction is achieved by
the median subject in det. If we restrict the sample to subjects who never took a risky lottery in
Part 5, 50 percent of the increase in the fraction of welfare-maximizing choices is achieved by the
median subject in det.38
To summarize, we find that in both the strategy space and the payoff space, there is robust evidence
that a substantial part of the problem of contingent reasoning can be attributed to the lack of PoC
rather than pure computational complexity. More concretely, there is an important improvement,
both in the action and payoff space, when we compare the two contingency environment of det
relative to the two contingency environment of prob, where a major difference is whether the two
contingencies are certainties.
36

To compute the payoff from random choice, we draw, for each question, 1000 uniform values in [0, 150] and for
each draw compute the payoff that would result if the draw were the submitted price. We then average across the 1000
draws. We therefore have for each question an optimal, a random and an actual payoff measure. We then compute,
for each question, (Actual Payoff - Random)/(Optimal - Random). Since Actual Payoff could be smaller than Random
(if the subject submitted, e.g. 150), note that this measure is sometimes negative. After computing the ratio for each
question, we take the average at the subject level.
37
If we constrain the sample to subjects who never took a risky lottery in part 5 the number is 64.7 percent.
38
Note that when we use the measure consisting of the fraction of welfare-maximizing choices, the role of PoC is
increased if we restrict attention to subjects who never took a risky lottery in part 5. This is because using lottery
choices to impute welfare-maximizing choices, allowed for some subjects in prob to submit p = vH in part 1 and
still make a welfare-maximizing choice. This is not the case when we restrict attention to subjects who never took a
risky lottery, thereby reducing the fraction of welfare-maximizing choices of the median subject in prob and as such
increasing the effect of PoC.

25

All Outcomes

Both Explicit
One Explicit
Qualitative
Mention Outcomes Only
Total

prob
1.6
0.0
14.4
10.6
26.6

det
13.7
8.7
30.1
0.0
52.5

Table 5: Reported Payoff in Part 3 Advice for Subjects who mention All Outcomes
Notes: 188 participants in prob and 183 in det. There are four outcomes (v, p) where v, p ∈ {vL , vH }. The four rows
divide subjects who mention all four outcomes dependent on whether subjects explicitly report payoffs associated with
p = vL and (or) p = vH : Both (One) Explicit; report payoffs qualitatively (Qualitative) or only mention all outcomes
without addressing payoffs (Mention Outcomes Only).

4

Understanding the Power of Certainty

In this section we shed light on the underlying cause of PoC. We first analyze the advice subjects
provided to a different participant. Specifically, we assess whether subjects differ in mentioning the
four possible outcomes associated with submitting a price p = vL and p = vH . We then explore to
what extent differences in advice can account for differences in strategies between prob and det. In
a final treatment we explore the robustness of PoC. We ask whether having subjects first encounter
the onefirm problem, and hence be attuned to thinking about the value of the firm, increases the
propensity to use dominant strategies and reduces differences in choices between probabilistic and
deterministic problems.

4.1

Advice

To shed light on the underlying cause for the PoC hypothesis, we analyze the advice subjects provide
to another participant for the vL = 20, vH = 120 problem. In both prob and det the vast majority
of subjects provide a numerical advice (90.4 and 92.4 percent) and/or some explanation (77.7 and
79.2 percent, respectively). Furthermore, the length of the written advice is, while similar between
prob and det, if anything a little longer in prob.39
The overall pattern of numerical recommendations matches our earlier findings. While 63.9 percent
of subjects in det recommend submitting p = vL , 43.1 percent submit the same recommendation in
prob (Table 25 of Appendix D). Meanwhile, 32.5 percent of subjects in prob recommend to either
submit p = vH or mix between vL and vH , compared to 17.5 percent in det.
39

The median (mean) number of words used in the advice equals 42 (54.7) in prob and 34 (45.1) in det. Using a
quantile regression on the median (linear regression) with a treatment dummy on the right-hand side and the number
of words on the left-hand side we find that the difference in the median (mean) is significant at the 10 (5) percent
level (p-values of 0.06 and 0.03, respectively).

26

80
10

20

Percentage of subjects
30
40
50
60

70

80
70
Percentage of subjects
30
40
50
60
20

0

10
0

Probabilistic
Deterministic
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(a) Subjects whose Advice mentions all Four Outcomes

(b) Subjects whose Advice does not mention all Four
Outcomes

Figure 4: Percent of subjects with p = vL in round n ≤ 15 and classified as VL VH in prob and det
dependent on whether their advice mentions all four outcomes.
In general, to determine the payoff-maximizing price, a subject who understood that submitting
prices p ∈
/ {vL , vH } is dominated needs to be aware of the four outcomes (v, p) where v, p ∈
{vL , vH }.40 Table 5 shows that 26.6 percent of subjects in prob mention all four outcomes in
at least some way compared to 52.5 percent in det, a significant difference.41 Only 3 subjects (1.6
percent) in prob explicitly compute payoffs for both p = vL and p = vH , compared to 13.7 percent
of subjects in det. An additional 8.7 percent of subjects in det compute the payoff associated with
either p = vL or p = vH . More subjects in det than in prob qualitatively mention all four outcomes.
There are some subjects in prob who mention all four outcomes without, however, providing any
guidance as to how to use these outcomes to compute payoffs.
Figure 4 shows the fraction of subjects who submit p = vL from round n to 20 for 1 ≤ n ≤ 16 and
submit p = vH in rounds 21-25 (part 5) in prob and det, dependent on whether the subject did
(Figure 4a) or did not (Figure 4b) mention all four outcomes. The Figure suggests that mentioning
all four outcomes increases incidences of submitting the strategy VL VH . It also suggests that within
each of those two groups the treatment effect is smaller.
Table 6 shows the distribution of types in each treatment depending on whether subjects mention
all four outcomes, and suggests that this feature of the advice may account for differences across
treatments. For example, the difference in the fraction of VL VH types is approximately 12 percentage points among subjects who mention all four outcomes, which is close to 50 percent of the
original difference. This suggests that differences across treatments might be driven by differences
in incidences of advice that mentions all four outcomes. The table also suggests that subjects who
do not mention all four outcomes are much more likely to be classified as a Dominated or Residual
40

The advice was classified following the protocol described in Section B of the Online Appendix. The classification
was verified by a research assistant. Examples of each class of advice are provided in Appendix F.
41
To test for significance, we run a linear regression in which the left-hand side variable is a dummy that takes value
1 if the subject submitted advice that mentions all four outcomes and on the right-hand side we include a constant
and a treatment dummy (1=det). The coefficient on the treatment dummy is positive and significant at the 1 percent
level (p-value <0.01).

27

VL VH

{M ixVH , VH VH }

Focal

Dom

Residual

Participants

All four

prob

52.0

20.0

22.0

4.0

2.0

50

Outcomes

det

64.6

7.3

7.3

7.3

13.6

96

Not all four

prob

8.0

26.1

17.4

28.3

20.3

138

Outcomes

det

16.1

25.3

9.2

25.3

24.1

87

Table 6: Participants classified into types using rounds 15-25 (the last 5 rounds in part 1 and in
part 2) [as % of participants]
Notes: Types are defined based on the prices p1 submitted in the last five rounds of part 1 and p2 submitted in all five rounds
of part 2. Type VL VH : p1 = vL and p2 = vH . Type {M ixVH , VH VH }: p1 ∈ {vL , vH } and at least one p1 = vH and p2 = vH .
Type ‘Focal’: p1 , p2 ∈ {vL , vH } and at least one p2 = vL (corresponds to VL VL , VL Mix , Mix VL , MixMix , VH VL , or VH Mix .
Type ‘Dom’: p1 , p2 ∈
/ {vL , vH }. Residual: All remaining subjects.

type.
Table 7 confirms that subjects who mention all four outcomes are more likely to be classified as
VL VH and less likely to be classified as using a strategy dominated in all treatments that is not
focal. This suggests that mentioning all four outcomes (v, p) where v, p ∈ {vL , vH } – and hence
thinking about them – is important to using the dominant strategy and to avoid consistently using
prices that are dominated and not focal (p ∈
/ {vL , vH }). The effect of mentioning all four outcomes
on type classification does not significantly differ by treatment. Furthermore, the differences between
prob and det are no longer significant when we condition on whether subjects mentioned all four
outcomes in the advice.
The results suggests that a big part of PoC stems from helping subjects to envision all four outcomes.
In contrast, when outcomes are hypothetical and not actually realized and certain, subjects seem to
have a much harder time to think of all four outcomes.
To further our understanding of what subjects may think about when they do not mention all four
outcomes, we divide subjects into five categories (shown in Table 8). The first is subjects who do not
mention any of the four outcomes. Another consists of subjects who only mention outcomes (v, p)
associated with p = vL , that is either only {(vL , vL )} or only {(vL , vL ) and (vH , vL )}.42 We classify
subjects as mentioning large gains, if they mention or highlight the gains that can be obtained for
a firm of value v = vH when submitting a price of p = vH .43 We classify subjects as mentioning
large losses, if they mention or highlight the losses that can result for a firm of value v = vL when
submitting a price of p = vH .44 Subjects in prob are more likely not to mention any outcome
42

Some of these latter subjects are subjects who compute the payoff associated with p = vL .
These are subjects who mention only {(vL , vL ) and (vH , vH )}, only {(vH , vH )}, only {(vL , vH ) and explicitly
(vH , vH )} or only {(vL , vL ), (vH , vL ) and (vH , vH )}. We consider an outcome to be mentioned if it is mentioned
explicitly or implicitly. An example in which (vH , vH ) is only implicitly mentioned would be the following: “submitting
120 can lead to a gain, but it’s not worth the risk.” In this case we consider that the subject is implicitly mentioning
that if v = vH , there would be a gain.
44
These are subjects who mention only {(vL , vH )}, only {(vL , vH ) and implicitly (vH , vH )}, only {(vL , vL ), (vH , vL )
and (vL , vH )} or only {(vL , vL ), (vL , vH ) and (vH , vH )}.
43

28

(1)
VL VH

(2)
{M ixVH , VH VH }

(3)
Focal

(4)
Dom

(5)
Dom or Res

0.021
(0.074)

0.004
(0.064)

−0.084
(0.070)

−0.142∗
(0.082)

−0.184∗∗∗
(0.065)

−0.369∗∗∗
(0.076)

Det

0.117
(0.073)

Advice mentions all outcomes

0.395∗∗∗
(0.068)

−0.088
(0.069)

0.062
(0.059)

Advice mentions all outcomes × Det

0.025
(0.092)

−0.148
(0.093)

−0.112
(0.080)

0.090
(0.088)

0.234∗∗
(0.103)

−0.029
(0.029)

−0.015
(0.029)

0.003
(0.025)

0.058∗∗
(0.028)

0.042
(0.032)

Num Risky × Det

0.004
(0.042)

0.050
(0.043)

−0.016
(0.037)

Constant

0.132∗
(0.069)

0.257∗∗∗
(0.070)

Num Risky

Observations

371

371

0.178∗∗∗
(0.060)
371

−0.036
(0.040)
0.220∗∗∗
(0.066)
371

−0.039
(0.047)
0.433∗∗∗
(0.077)
371

Table 7: Main Treatments: Estimation output using last 5 rounds of part 1 and part 2 for the
classification of types
Notes: Results from a linear regression. The dependent variable takes value 1 if the subject is classified as (1) VL VH , (2)
{M ixVH , VH VH } (3) Focal, (4) Dom, (5) Dom or Res. Det is a treatment dummy that equals 1 if the subject participated in
det. Advice mentions all outcomes is a dummy that equals 1 if the advice of the subject mentions all four outcomes (v, p) with
v, p ∈ {vL , vH }. Num Risky is the number of risky lotteries the subject chooses in part 5 (from 0 to 3). The regression also
includes demographic controls (Gender, Ethnicity, Age and Schooling) and a control for the number of errors in the instructions.
The full output is presented in Table 35 of the Online Appendix.

compared to subjects in det (by about thirty percent). However, subjects in prob are much more
likely to only report a subset of the outcomes, either focusing on large gains or large losses: a total
of 29.3 percent of subjects compared to 14.2 percent in det.
The advice subjects provide is a strong indicator of the recommended price and the strategy they
use, see also Appendix D tables 25 and 26 for details. Subjects who mention all four outcomes
are to a large extent recommending p = vL in part 1 (76.0 and 97.9 percent in prob and det,
respectively).45 This is much larger than the 31.2 and 26.4 percent among all other subjects (a
two-sided Fisher’s exact test yields p < 0.01 and p < 0.01, respectively). We have already shown
that subjects who mention all four outcomes are more likely to be classified as VL VH : 52.0 and
64.6 percent in prob and det, respectively, compared to only 8.0 and 16.1 percent among all other
subjects (p < 0.01 and p < 0.01, respectively).
To assess the impact of mentioning only large gains or only large losses, we exclude from our sample
subjects who mention all four outcomes (as we already have a pretty good understanding of their
behavior). Subjects who mention only large gains are very likely to recommend p = vH (83.3 and
81.8 percent) in prob and det, respectively. This is larger than the 20.2 and 30.3 percent among
subjects who do not mention only large gains – and neither all four outcomes (p < 0.01 and p < 0.01,
45

Of subjects who mention all four outcomes in prob, there are 3 subjects (1.6 percent) who recommend p = 120
(for details see Table 25). One of these three subjects explicitly computes the payoffs for the four outcomes, and in
part 5 selects the lottery that corresponds to p = 120 when facing a choice between the lotteries induced by p = 20
and p = 120. The other two subjects who recommend p = 120 only describe the four outcomes qualitatively, and
when they face the lotteries in part 5 they actually select the p = 20 lottery.

29

All Outcomes
Large Gains
Some Outcomes
Large Losses
p = vL outcomes
No Outcome
Mistake
Sum

prob
26.6
12.8
13.3
3.2
40.4
3.7
100.0

det
52.5
6.0
3.8
4.4
29.5
3.8
100.0

Table 8: Reported Outcomes in Part 3 Advice.
Notes: 188 participants in prob and 183 in det. There are four outcomes (v, p) where v, p ∈ {vL , vH }. The first row
shows all subjects who mention all four outcomes in any form. The bottom five rows contain subjects who report on
specific subsets of outcomes: Large Gains: {(vL , vL ) and (vH , vH )}, {(vH , vH )}, {(vL , vH ) and explicitly (vH , vH )},
{(vL , vL ), (vH , vL ) and (vH , vH )}. Large Losses: {(vL , vH )}, {(vL , vH ) and implicitly (vH , vH )}, {(vL , vL ), (vH , vL )
and (vL , vH )}, {(vL , vL ), (vL , vH ) and (vH , vH )}. p = vL outcomes: {(vL , vL )}, {(vL , vL ) and (vH , vL )}. No outcome:
No outcome is reported in any form. Mistake: One of the payoffs is wrongly computed.

respectively). Furthermore, subjects who only mention large gains are quite often classified as Mix
or VH in part 1: 62.5 and 54.6 percent in prob and det, respectively, compared to only 29.8 and
27.6 percent among the other subjects (p < 0.01 and p = 0.08, respectively). However, subjects
who only mention large gains and submit p = vH are probably not doing so because they are very
risk seeking. Indeed, 62.5 and 90.9 percent of subjects who mention large gains in prob and det,
respectively, select the safe alternative (corresponding to p = vL ) when in part 5 they face the
lotteries that corresponds to the problem (vL , vH ) = (20, 120).
Subjects who mention only large losses are very likely to recommend p = vL : 80.0 and 85.7 percent
in prob and det, respectively. This is larger than the 20.3 and 21.3 percent among subjects who do
not mention only large losses – and neither all four outcomes (p < 0.01 and p < 0.01, respectively).
Furthermore, subjects who only mention large losses are quite often classified as VL in part 1: 40
and 100 percent in prob and det, respectively, compared to only 14.2 and 15.0 percent among
the other subjects (p < 0.01 and p < 0.01, respectively). While we do not have sufficiently many
subjects in det for a detailed analysis, subjects in prob who only mention large losses are more
likely to be classified as a Focal type with VL in part 1 (such as VL VL or VL M ix), that is a type who
is definitely not profit maximizing, 21.9 percent compared to only 5.7 percent among other subjects
(p < 0.01).
To summarize, the results in this section point towards subjects in det being much more likely to
think of all four outcomes (v, p) where v, p ∈ {vL , vH }. Subjects who mention all four outcomes
in their advice are significantly more likely to be classified as VL VH . Furthermore, controlling for
whether the advice mentions all four outcomes accounts for the differences between prob and det.
It seems that the fact that the two companies exist in det, rather than being possible states in
prob, makes it easier for subjects to think about the outcomes they receive for each firm for a given
30

(1)
VL4

(2)
4
VH

(3)
M ix4

(4)
Dom4

(5)
Dom4 or Res4

Det

−0.103
(0.063)

0.045
(0.056)

0.084∗
(0.050)

0.010
(0.052)

−0.026
(0.059)

Num Risky

−0.059∗
(0.032)

0.054∗
(0.029)

−0.025
(0.026)

0.036
(0.027)

0.030
(0.030)

Num Risky × Det

−0.061
(0.047)

0.016
(0.042)

0.038
(0.037)

−0.017
(0.039)

0.006
(0.044)

−0.089∗
(0.047)

−0.057
(0.053)

Advice mentions all outcomes

0.134∗∗
(0.057)

−0.106∗∗
(0.051)

0.030
(0.045)

VL VH

0.331∗∗∗
(0.059)

−0.030
(0.053)

−0.049
(0.047)

−0.188∗∗∗
(0.049)

−0.252∗∗∗
(0.055)

Constant

0.312∗∗∗
(0.074)

0.068
(0.059)

0.289∗∗∗
(0.061)

0.409∗∗∗
(0.069)

Observations

371

0.210∗∗∗
(0.066)
371

371

371

371

Table 9: Main Treatments: Estimation output using part 4 types
4 , (3)
Notes: Results from a linear regression. The dependent variable takes value 1 if the subject is classified as (1) VL4 , (2) VH
4
4
4
4
M ix , (4) Dom , or (5) Dom or Res . Det is a treatment dummy that equals 1 if the subject participated in det, which in
this case means that those subjects are facing probabilistic problems. Num Risky is the number of risky lotteries the subject
chooses in part 5 (from 0 to 3). VL VH is a dummy that takes value 1 if the subject was classified as VL VH in parts 1 and
2. Advice mentions all outcomes is a dummy that equals 1 if the advice of the subject mentions all four outcomes (v, p) with
v, p ∈ {vL , vH }. The regression also includes demographic controls (Gender, Ethnicity, Age and Schooling) and a control for
the number of errors in the instructions. The full output is presented in Table 36 of the Online Appendix.

price. This, in turn, seems to be responsible for the Power of Certainty.

4.2

Across Treatment Adaptation and Mentioning all outcomes

We provide a second piece of evidence as to the relevance of thinking about all four outcomes,
or more precisely, of subjects mentioning all four outcomes in the advice. In part 4, subjects
encounter 10 problems that have the same characteristics as those of part 1, including that 2vL < vH .
Subjects now, however, play the opposite treatment than in part 1, that is, subjects in det encounter
probabilistic problems and subjects in prob face deterministic problems in part 4. Note that part
4 comes after part 3, where subjects provided advice. So, instead of connecting their part 3 advice
to the strategies that subjects were classified as before giving advice, we now use the advice as a
control for their future part-4 behavior.
We classify subjects in part 4 based on the prices submitted in the last 5 rounds. Note that in the
deterministic problems (encountered in part 4 of prob), p = vL is the dominant strategy, while in
probabilistic problems very risk-seeking subjects might find that p = vH maximizes their earnings.
All other prices are dominated. Using linear regressions we show that subjects who are classified
as VL VH in part 1 are significantly more likely to be classified as VL4 in part 4 (that is submitting
p = vL in the last 5 rounds of part 4), and less likely to be classified as Dom4 (submitting dominated
prices in part 4). Table 9 also shows that subjects who mention all four outcomes are more likely to
be classified as VL4 and less likely to be classified as Dom4 or VH4 .
31

80
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(a) Probabilistic: vL : n to 15 and VL VH .

0

10

20

Percentage of subjects
30
40
50
60

70

80
70
Percentage of subjects
30
40
50
60
20
10
0

Main Treatment
Advisee Treatment

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(b) Deterministic: vL : n to 15 and VL VH .

Figure 5: Evolution of Types in Main and Advisee Treatments [as % of subjects].
The results from part 4 confirm that mentioning all four outcomes is also relevant in determining a
subject’s ability to submit optimal prices in an environment that is slightly different than the one
they encountered before.

4.3

Advisee Treatment

We have two goals for this section: First, to incentivize the advice, we ran the Advisee treatments
and want to report on whether advice can affect the behavior of subjects who receive advice. Second,
we want to provide some insight as to the role of receiving and recognizing the usefulness of advice
that mentions all four outcomes. Specifically, subjects in the Advisee treatment saw a piece of advice
from five different subjects about what price to submit in round 1 of part 1, which corresponds to
the problem vL = 20 and vH = 120. Subjects in advprob (advdet) received advice from subjects
in prob (det). The subjects then faced part 1 and part 2 from the main treatments.
We present two analyses of the advisee treatments. First, we compare how prices submitted by
advisees compare to prices submitted by subjects in the main treatments who have not received any
piece of advice. Second, we provide some evidence that subjects who receive and understand the
usefulness of advice that mentions all four outcomes behave differently than other subjects.
Figure 5a shows the evolution of strategies that lead to VL VH of subjects in advprob - who received
five pieces of advice - and of subjects in prob - who received no advice, see Figure 8 in Appendix D
for other types. There is almost no difference between these two groups of subjects. Table 10 shows
the distribution of types (based on subject’s choices in the last five rounds of part 1 and the five
rounds of part 2). For instance, 19.7 percent of subjects are classified as type VL VH in prob (see
Table 2), compared to 24.4 percent in advprob. Table 11 confirms that there are no significant
differences in type distributions between advprob and prob.
Figure 5b shows that there is a large difference in the evolution of strategies that lead to VL VH
between advdet and det: For deterministic problems, a subject who received five pieces of advice
from other subjects is almost twice as likely to submit p = vL from round n to 15 and be classified as
32

VL VH

{MixVH , VH VH }

Focal

Dominated

Residual

Participants

All

advprob

24.4

22.0

22.0

14.6

17.1

41

advdet

72.5

5.0

10.0

7.5

5.0

40

Selected Advice

advprob

33.3

16.7

16.7

11.1

22.2

18

that mentions all outcomes

advdet

78.1

3.1

6.3

6.3

6.2

32

Did not Select Advice

advprob

17.4

26.1

26.1

17.4

13.0

23

that mentions all outcomes

advdet

50.0

12.5

25.0

12.5

0.0

8

Table 10: Type Classification in Advisee Treatments [as % of participants].
Notes: Type are defined based on the prices p1 submitted in the last five rounds of part 1 and p2 submitted in all five rounds
of part 2. Type VL VH : p1 = vL and p2 = vH . Type {M ixVH , VH VH }: p1 ∈ {vL , vH } and at least one p1 = vH and p2 = vH .
Type ‘Focal’: p1 , p2 ∈ {vL , vH } and at least one p2 = vL (corresponds to VL VL , VL Mix , Mix VL , MixMix , VH VL , or VH Mix .
Type ‘Dom’: p1 , p2 ∈
/ {vL , vH }. Residual: All remaining subjects.

(1)
VL VH

(2)
{M ixVH , VH VH }

(3)
Focal

(4)
Dom

0.098
(0.083)

−0.056
(0.083)

(5)
Dom or Res

prob
Advisee

0.034
(0.083)

Constant

0.222∗∗∗
(0.076)

Observations

223

−0.046
(0.090)
0.148∗
(0.082)

0.241∗∗∗
(0.076)

223

223

0.256∗∗∗
(0.076)

−0.087
(0.098)
0.388∗∗∗
(0.089)

223

223

0.013
(0.060)

−0.012
(0.071)

−0.125
(0.088)

0.139∗∗
(0.056)

0.001
(0.067)

0.062
(0.082)

det
Advisee

0.249∗∗
(0.100)

Constant

0.596∗∗∗
(0.093)

Observations

229

−0.136∗
(0.074)
0.203∗∗∗
(0.069)
229

229

229

229

Table 11: Main Treatment v. Advisee Treatment: Individual Types Estimations Output
Notes: Results from two linear regressions: prob compares probabilistic treatments (Main and Advice), det compares deterministic treatments (Main and Advice). The dependent variable takes value 1 if the subject is classified as (1) VL VH . (2) M ixVH ,
VH VH (3) Focal, (4) Dom, (5) Dom or Res. Advisee is a dummy variable that takes value 1 if the observation corresponds to
the Advisee treatment. The regression also includes demographic controls (Gender, Ethnicity, Age and Schooling) and a control
for the number of errors in the instructions. The full output is presented in Tables 29 and 30 of the Online Appendix.

33

(1)
VL VH

(2)
{M ixVH , VH VH }

(3)
Focal

(4)
Dom

(5)
Dom or Res

Det

0.373∗∗∗
(0.104)

−0.173∗∗
(0.080)

−0.058
(0.089)

−0.022
(0.076)

−0.141
(0.092)

Selected Advice mentions all outcomes

0.253∗∗
(0.107)

−0.034
(0.082)

−0.146
(0.092)

−0.111
(0.078)

−0.074
(0.095)

Constant

0.331∗∗
(0.165)

0.139
(0.126)

0.438∗∗∗
(0.141)

0.158
(0.121)

0.091
(0.147)

Observations

81

81

81

81

81

Table 12: Advisee Treatments: Individual Types Estimations Output in Parts 1 and 2
Notes: Results from a linear regression. The dependent variable takes value 1 if the subject is classified as (1) VL VH , (2) {M ixVH ,
VH VH } (3) Focal, (4) Dom, (5) Dom or Res. Det is a dummy variable that takes value 1 if the observation corresponds to
the deterministic treatment. The variable ‘Selected Advice mentions all outcomes’ is a dummy variable that takes value 1 if
the advice selected as the ‘most helpful’ by the advisee mentions all four outcomes. The regression also includes demographic
controls (Gender, Ethnicity, Age and Schooling) and a control for the number of errors in the instructions. The full output is
presented in Table 31 of the Online Appendix.

VL VH than a subject who received no advice, see Figure 8 in Appendix D for other types. Table 10
shows the distribution of types in advdet, where we have almost three quarter of all subjects being
classified as the dominant type! The regressions in Table 11 confirm that in deterministic settings,
subjects in the Advisee treatment are significantly more likely to be classified as the dominant
strategy type and less likely to be classified as using the dominated strategy of {VH VH , M ixVH }.
The fact that we have large improvements in terms of subjects classified as using optimal strategies
when moving from subjects in det to subjects in advdet is reminiscent of the common finding
that decisions made with advice are closer to the predictions of economic theory than choices made
without advice.46 However, this is not borne out in probabilistic problems where advice does not
help subjects make better choices. This highlights that explaining and interpreting advice in prob
seems to be relatively difficult.
We now address whether receiving advice that mentions all four outcomes – and understanding that
this is useful advice – is important. The bottom rows of Table 10 show the distribution of types
for subjects who received an advice that mentions all four outcomes and selected one of those as
the most helpful advice. The regression in Table 12 shows that subjects who selected an advice
that mentions all four outcomes as the most helpful advice are more likely to be classified as VL VH
than those that did not. Independent of that, subjects in advdet are more likely to be classified
as VL VH .
While subjects in advdet are more likely to receive an advice that mentions all four outcomes,
recall that subjects received advice from five subjects, that is five pieces of advice.47 We can
thus inquire whether when aggregating across all five pieces of advice, it is the case that subjects
46

For details on choices with and without advice see the survey of Schotter (2003) and references therein.
Table 27 in Appendix D shows the classification into types dependent on whether subjects received an advice that
mentions all four outcomes, whether they selected it or not. All but one subject in advdet received an advice that
mentioned all four outcomes. This is why we could not use the variable “received advice that mentions four outcomes”
when accounting for type classification in the Advisee treatments.
47

34

received information on all four outcomes. We find that 97.5 percent of subjects in advdet received
information on all four outcomes across the five pieces of advice, which is not very different from
the 95.1 percent of subjects in advprob.48
The fact that subjects who received advice that mentions all four outcomes and recognized its
significance are more likely to be classified as VL VH provides some confirmation that thinking about
the four possible outcomes (v, p) with v, p ∈ {vL , vH } is crucial to be able to behave optimally in
the acquiring a company problems.

4.4

Focusing on Certainty: One-Firm treatments

VL VH

{M ixVH , VH VH }

Focal

Dominated

Residual

Participants

27.8

25.5

22.2

12.5

12.0

212

onefirmdet

47.2

18.4

8.0

9.9

16.5

216

onefirmprob

41.7

28.1

27.3

0.0

2.9

139

65.1

21.7

10.9

0.0

2.3

129

All

onefirmprob

V

onefirmdet
V and

onefirmprob

51.1

17.0

28.7

0.0

3.2

94

No risky lottery

onefirmdet

72.0

17.2

10.8

0.0

0.0

93

Table 13: onefirm Treatments: Parts 3 and 4 Type Classification [as % of participants]
Notes: Types are defined based on the prices p1 submitted in the last five rounds of part 3 and p2 submitted in all five rounds
of part 4. Type VL VH : p1 = vL and p2 = vH . Type {M ixVH , VH VH }: p1 ∈ {vL , vH } and at least one p1 = vH and p2 = vH .
Type ‘Focal’: p1 , p2 ∈ {vL , vH } and at least one p2 = vL (corresponds to VL VL , VL Mix , Mix VL , MixMix , VH VL , or VH Mix ).
Type ‘Dom’: p1 , p2 ∈
/ {vL , vH }. Residual: All remaining subjects. In onefirm subjects are classified as V if they submit p = v
in rounds 16-25 (last 5 rounds of part 1 and all rounds of part 2).

We have seen that subjects who think of all four outcomes (v, p) with v, p ∈ {vL , vH } are more
likely to be classified as using a profit-maximizing strategy. In the One-Firm treatments subjects
are first confronted with 25 rounds of a simplification of part 1 and part 2 of the main treatments.
Specifically, when submitting a price, subjects do so only for a single firm of which they know the
value. This may increase the chance that subjects focus on outcomes they receive when submitting
a price. We therefore ask whether this helps to increase the fraction of profit-maximizing strategies
and reduce or even eliminate the role of PoC, that is eliminate differences between probabilistic and
deterministic problems. Specifically, in part 3, part 4 and part 5 of the onefirm treatments subjects
face part 1, part 2 and part 5 of either prob or det, we refer to the treatments as onefirmprob
and onefirmdet , respectively. Therefore, in the onefirm treatments, subjects are trained for 25
periods to think about the value of the company when submitting a price before they encounter part
1 and 2 of the main treatments.
48

A linear regression with a treatment dummy on the right-hand side indicates that the treatment effect is small
(0.024) and not statistically significant (p-value 0.577).

35

(1)
VL VH

(2)
{M ixVH , VH VH }

(3)
Focal

(4)
Dom

−0.064
(0.057)

0.084
(0.054)

−0.092∗
(0.047)

−0.008
(0.031)

−0.008
(0.029)

(5)
Dom or Res

prob
One Firm

0.123∗∗
(0.053)

Num Risky

−0.057∗
(0.029)

Num Risky × One Firm

−0.062
(0.041)

Constant

0.339∗∗∗
(0.062)

0.257∗∗∗
(0.066)

−0.013
(0.041)
0.156∗∗
(0.062)

−0.063∗
(0.036)
0.146∗∗∗
(0.054)

0.073∗∗
(0.031)
−0.034
(0.044)
0.249∗∗∗
(0.066)

404

404

404

404

0.091
(0.067)

−0.018
(0.053)

−0.026
(0.039)

−0.040
(0.045)

−0.047
(0.060)

Num Risky

−0.017
(0.037)

0.033
(0.029)

−0.015
(0.022)

0.020
(0.025)

−0.002
(0.033)

Num Risky × One Firm

−0.077
(0.048)

−0.018
(0.038)

0.015
(0.028)

−0.001
(0.032)

0.080∗
(0.043)

0.049
(0.052)

0.138∗∗
(0.070)

Observations

404

0.109∗∗
(0.044)

0.071∗∗∗
(0.026)

−0.142∗∗
(0.057)

det
One Firm

Constant
Observations

0.518∗∗∗
(0.077)
395

0.218∗∗∗
(0.061)
395

0.126∗∗∗
(0.045)
395

395

395

Table 14: Estimation Output: One Firm Treatments (Parts 3 and 4) compared to Main Treatments
(Parts 1 and 2)
Notes: Summarizes results from two linear regressions: prob compares probabilistic treatments (Main and One Firm), det
compares deterministic treatments (Main and One Firm). The dependent variable takes value 1 if the subject is classified as
(1) VL VH , (2) {M ixVH , VH VH } (3) Focal, (4) Dom, (5) Dom or Res. One Firm=1 is a dummy variable that takes value 1 if
the observation corresponds to the deterministic treatment. Num Risky is the number of risky lotteries the subject chooses in
part 5 (from 0 to 3). The regression also includes demographic controls (Gender, Ethnicity, Age and Schooling) and a control
for the number of errors in the instructions. The full output is presented in Tables 32 and 33 of the Online Appendix.

Table 13 shows the distribution of types in the onefirm treatments. The linear regression in
Table 14 shows that in probabilistic problems, focusing first on one firm significantly increases the
chance with which subjects are classified as the dominant type VL VH and decreases the chance to be
classified as the Dominated type or being unclassified (Residual type). There is no such comparable
change in the deterministic problems.
It seems that focusing one one firm of known value helps subjects to subsequently only submit prices
p ∈ {vL , vH }. However, Table 13 shows that there is still a large difference between the probabilistic and deterministic problems. Indeed, the linear regression in Table 15 shows that subjects in
onefirmdet are significantly more likely to be classified as VL VH than subjects in onefirmprob ,
and less likely to be classified as using a Focal (and hence dominated) strategy.
The coefficients on V in regressions (4) and (5) of Table 15, where V is a dummy controlling for
whether the subject submitted p = v in the last 10 rounds of parts 1 and 2 in which subjects submit
prices for one firm of known value, show that being able to solve the one firm problem significantly

36

reduces instances of prices p ∈
/ {vL , vH }.49 However, subjects classified as V are not only more likely
to be classified as VL VH , but also as using Focal strategies, which are strategies that are dominated
(though only using prices p ∈ {vL , vH }).
Finally, we use subjects in the onefirm treatments to reassess the role of PoC on the general
problem of Computational Complexity. Table 13 shows that 27.8 percent of subjects are classified
as VL VH in parts 3 and 4 in onefirmprob , compared to 47.2 percent in onefirmdet . Overall, 62.6
percent of subjects are classified as V in parts 1 and 2 in the onefirm treatments. So, using only
VL VH as the dominant strategy, PoC accounts for 55.7 percent of what conventionally was labelled
as problems of complexity.
(1)
VL VH
Det
Num Risky

0.255∗∗∗
(0.055)
−0.091∗∗∗
(0.029)

(2)
{M ixVH , VH VH }
−0.045
(0.054)
0.102∗∗∗
(0.029)
−0.082∗∗
(0.039)

(3)
Focal

(4)
Dom

(5)
Dom or Res

−0.178∗∗∗
(0.047)

−0.048
(0.036)

−0.032
(0.041)

−0.015
(0.025)

−0.007
(0.019)

0.004
(0.022)

0.023
(0.034)

0.012
(0.026)

0.041
(0.030)

Num Risky × Det

0.018
(0.040)

V

0.367∗∗∗
(0.044)

0.103∗∗
(0.043)

0.104∗∗∗
(0.037)

−0.258∗∗∗
(0.029)

−0.574∗∗∗
(0.033)

Constant

0.179∗∗∗
(0.067)

0.191∗∗∗
(0.066)

0.144∗∗
(0.057)

0.223∗∗∗
(0.044)

0.487∗∗∗
(0.051)

Observations

428

428

428

428

428

Table 15: onefirm Treatments: Estimation output using last 5 rounds of part 3 and part 4 for the
classification of types.
Notes: Results from a linear regression. The dependent variable takes value 1 if the subject is classified as (1) VL VH , (2)
{M ixVH , VH VH } (3) Focal, (4) Dom, (5) Dom or Res. Det=1 is a dummy variable that takes value 1 if the observation
corresponds to onefirmdet and 0 if it corresponds to onefirmprob . Num Risky is the number of risky lotteries the subject
chooses in part 5 (from 0 to 3). The variable V is a dummy variable that equals 1 if the subject selected p = v in the last 10
periods of problems with one firm (last 5 periods of part 1 and 5 periods of part 2). The regression also includes demographic
controls (Gender, Ethnicity, Age and Schooling) and a control for the number of errors in the instructions. The full output is
presented in Table 34 of the Online Appendix.

Next we restrict attention to subjects who in parts 1 and 2 were classified as V and who never
took a risky lottery in part 5, thereby guaranteeing that we restrict attention to subjects who really
understand the one-firm problem and are not very risk seeking. Among those subjects, by definition, 100 percent are classified as V, though only 51.1 percent of them use VL VH in onefirmprob ,
which should now comprise almost all subjects who use payoff-maximizing choices. Once more, the
Power of Certainty accounts for 42.7 percent of the problem previously attributed to computational
Complexity.
49

This finding is consistent with the effect of experience reported by Charness and Levin (2009).

37

5
5.1

Discussion and Conclusions
Discussion and Additional Related Literature

While most of the recent theoretical literature on biases in decision making cannot directly account
for our results (e.g. Sims, 2003, Bordalo, Gennaioli and Shleifer, 2012, Gabaix, 2014, and Caplin,
Dean and Leahy, 2016), there is one paper which does make predictions that differ between the
probabilistic and the deterministic problems. Li (2017) can account for the main comparative static
result in submitted strategies, though we conjecture perhaps not exactly for the right reasons. The
paper introduces the concept of obvious strategy proofness to understand which strategy proof
mechanisms are obviously so, that is, agents would actually recognize this fact and hence submit
the dominant strategy. Specifically, a dominant strategy is defined as obviously dominant if the best
outcome from a deviation is no better than the worst outcome from following the dominant strategy.
In contrast to the motivating environments in Li (2017), we have no strategic interaction but the
concept of obvious strategy proofness can still be applied. Submitting p = vL is a dominant and
indeed obviously-dominant strategy in part 1 of det (where 2vL < vH ), since the return from p = vL
is 0.5vL , while that from p = vH is 1.5vL − 0.5vH < 0.5vL . However, p = vL is not an obviouslydominant strategy in prob: the best outcome from p = vH is 0.5vH , which is larger than the worst
outcome from p = vL , which is 0. Similarly, for part 2, it is possible to show that p = vH is obviously
dominant only in det but not in prob. If agents are able to recognize dominant strategies only if
these are obviously dominant, then we would expect a comparative static between prob and det
such as the one observed in the paper.
One reason we believe that obvious dominance may not be exactly the right concept in our environment is that we were able to link the failure to play the dominant strategy to the failure to mention
all four outcomes (v, p), where v, p ∈ {vL , vH }. Furthermore, it is not straightforward to link an
agent who can only compare sets, as in the best outcome from deviation to the worst outcome using
the dominant strategy, to an agent who seems to actually not think about all possible outcomes.
One question open for future research is to design an experimental test that assesses if certainty
helps even if profit-maximizing strategies are obviously dominant.
Another strand of the literature studies contingent thinking using Savage’s sure-thing principle
(Savage, 1972). The sure-thing principle asserts that if a person prefers A to B knowing that event
X obtains and also prefers A to B knowing that X does not obtain, then the person prefers A to
B prior to learning whether X results or not. There is literature, however, showing that individuals
often violate the sure-thing principle, a phenomenon referred to as the disjunction effect (see Shafir
and Tversky, 1992, Tversky and Shafir, 1992, Shafir, 1994, Croson and Esponda and Vespa, 2017a).50
50
The psychology model for the disjunction effect is that individuals do not make choices based on consequences
of decisions, but rather based on reasons for making one choice over another. For example, suppose a student would
want to take a vacation in Hawaii when they pass a big test to celebrate and when they fail this big test then in order
to console themselves. Then, if the student has to decide whether to go to Hawaii before knowing the test results, the
student has no reason to go, violating the sure thing principle, see Shafir, Simonson and Tversky (1993).

38

In particular, Esponda and Vespa (2017a) find that failure to satisfy the sure-thing principle can be
related to difficulties in thinking through the state-space. Notice that our experiment does not test
the sure-thing principle because depending on the state of the world, the firm being of low or high
value, the individual would want to submit a different price. However, our findings do suggest a new
mechanism for the violation of the sure-thing principle: Perhaps individuals cannot think about the
different states of the world, and hence fail to envision the returns from various choices.
In this section we also want to discuss some possible alternative designs. Since the main goal of our
paper was to show the PoC, we selected two treatments that truly differ based on that hypothesis.
prob and det are different from each other, and do not just have a different framing. There are,
however, treatments where the difference to prob may be more a difference of framing, though in
which we would expect some of the PoC to play a role. For example, consider the following variation,
which we call the ‘Late Lottery’ treatment. The treatment is almost like det, though, after having
purchased none, one or two firms, we use a randomization device to decide which of the two possible
transactions is actually used for payments. There is an equal chance that either the firm of value
vL or the firm of value vH is chosen. This treatment is a re-framing of prob. It may, however,
nonetheless change behavior in a way that this ‘Late Lottery’ treatment is in between prob and
det. One possible reason could be narrow bracketing, as at the time individuals make choices, the
subject could ignore the final lottery draw, which makes the ‘Late Lottery’ treatment look like det.

5.2

Summary of Results and Conclusion

We provide a new hypothesis, the Power of Certainty, to account for a significant portion of the
difficulties agents experience when they have to engage in contingent reasoning. We propose that
it is not just the fact that agents have to aggregate payoff-relevant information over multiple states
that causes problems, but the fact that the states are uncertain. Specifically, in the ‘Acquiring-aCompany’ problem, an agent purchases a firm of value v whenever p ≥ v, in which case her profits for
that firm are 1.5v − p. The case in which the agent has to submit a price before knowing whether the
value of the firm is low (vL ) or high (vH ), each being equally likely, is notoriously difficult relative to
the environment in which there is one firm of known value. These difficulties have been attributed to
what is perhaps best described as computational complexities, arising from the fact that two values
are more difficult to consider than a single value. We construct a new problem, the deterministic
problem, in which there are two firms, one of value vL and one of value vH . The agent submits
a price, which is then sent to each firm separately. This ensures that for each price p, twice the
expected payoffs in the probabilistic problem equal the realized payoffs in the deterministic problem.
The deterministic problem retains the complexity of two firms but eliminates any uncertainty.
We ran experiments on Amazon Turk to compare subjects’ price choices in a probabilistic and a
deterministic treatment, which contained probabilistic problems (where payoffs are multiplied by
two) and deterministic problems. In part 1, where 2vL < vH , such as vL = 20 and vH = 120, for
subjects in the probabilistic treatment who are not very risk seeking, and for all subjects in the
39

deterministic treatment, the dominant action is to submit a price p = vL . In part 2 the dominant
price for all subjects is to submit p = vH since 1.5vL > vH , such as vL = 50 and vH = 60. By
varying the environment in a way that the dominant action changes, we ensure that subjects who
are classified as payoff maximizing are not using rules of thumb in which the submitted price is
independent of the actual values of the firms.
When we only consider subjects who both submit p = vL in part 1 and p = vH in part 2 as payoff
maximizing, we find that there are twice as many such subjects in the deterministic than in the
probabilistic treatment. Furthermore, we show that this difference cannot be accounted for by many
subjects being risk loving (since risk-loving subjects in the probabilistic treatment may have different
dominant strategies).
Finally, we show that the Power of Certainty is not only significant, but also accounts for a quantitatively large portion of the welfare loss subjects incur when moving from a problem involving one
firm of known value to a problem where the firm could have two possible values, each realized with
fifty percent chance. That is, we show that the Power of Certainty is responsible for a substantial
part of the welfare loss that used to be attributed to computational complexity.
In the last part of the paper we provide some evidence for the causes of the Power of Certainty,
that is why probabilistic problems of one firm with two possible values are harder than deterministic
problems with two firms of known value. Using incentivized advice that subjects provide to a different
participant for the vL = 20 and vH = 120 problem, we find that subjects in the deterministic
treatment are twice as likely to mention the four outcomes that are needed to compute payoffmaximizing actions: For each price p = vL = 20 and p = vH = 120 the outcome that would result if
the firm were of either low or high value. We show that this feature of the advice largely accounts
for differences between the probabilistic and deterministic treatments. We therefore have evidence
that eliminating uncertainty increases the subjects’ tendency to mention all outcomes necessary to
compute payoff-maximizing prices, and hence, presumably, to think about them in the first place.
We expect that the Power of Certainty hypothesis can help explain behavior in other settings. For
example, it has some implications in trying to understand why we might often attribute firms to be
more likely to be profit maximizers than consumers. While a standard argument is the competition
among firms, we provide a new explanation: The Power of Certainty hypothesis suggests that
any individual is much more capable in finding profit-maximizing strategies when playing against a
distribution, than when playing a lottery from that distribution. Since firms may make many choices
in similar environments compared to consumers, Power of Certainty asserts that even individuals
deciding in the role of a firm would be better at profit maximization. There could be a whole range
of environments in which the Power of Certainty could improve subjects’ decision making, such
choices of insurance, annuities, etc. We also suspect that PoC may help agents in strategic settings
such as auctions. Finally, we leave it to future research to provide a model that can account for the
Power of Certainty.

40

Bibliography
Ambuehl, Sandro, B. Douglas Bernheim and Annamaria Lusardi. 2014. “The effect of financial
education on the quality of decision making.” National Bureau of Economic Research .
Ambuehl, Sandro and Shengwu Li. 2017. “Belief updating and the demand for information.” Games
and Economic Behavior (forthcoming) .
Araujo, Felipe, Stephanie Wang and Alistair J. Wilson. 2017. “Cursed through Time: Dynamic
Adverse Selection in the Laboratory.” Working paper .
Barrett, Garry F and Stephen G Donald. 2003. “Consistent tests for stochastic dominance.” Econometrica 71(1):71–104.
Bazerman, Max H. and William F. Samuelson. 1983. “I won the auction but don’t want the prize.”
Journal of Conflict Resolution 27(4):618–634.
Bénabou, Roland and Jean Tirole. 2002. “Self-confidence and personal motivation.” The Quarterly
Journal of Economics 117(3):871–915.
Bereby-Meyer, Yoella and Alvin E Roth. 2006. “The speed of learning in noisy games: Partial
reinforcement and the sustainability of cooperation.” The American Economic Review 96(4):1029–
1042.
Bordalo, Pedro, Nicola Gennaioli and Andrei Shleifer. 2012. “Salience theory of choice under risk.”
The Quarterly Journal of Economics 127(3):1243–1285.
Brunnermeier, Markus K and Jonathan A Parker. 2005. “Optimal expectations.” The American
Economic Review 95(4):1092–1118.
Caplin, Andrew and John Leahy. 2001. “Psychological expected utility theory and anticipatory
feelings.” The Quarterly Journal of Economics 116(1):55–79.
Caplin, Andrew and Mark Dean. 2015. “Revealed preference, rational inattention, and costly information acquisition.” The American Economic Review 105(7):2183–2203.
Caplin, Andrew, Mark Dean and John Leahy. 2016. “Rational inattention, optimal consideration
sets and stochastic choice.” Working paper .
Charness, Gary and Dan Levin. 2009. “The origin of the winner’s curse: a laboratory study.”
American Economic Journal: Microeconomics 1(1):207–236.
Costa-Gomes, Miguel A. and Georg Weizsäcker. 2008. “Stated beliefs and play in normal-form
games.” The Review of Economic Studies 75(3):729–762.
Costa-Gomes, Miguel and Vincent P Crawford. 2006. “Cognition and behavior in two-person guessing
games: An experimental study.” The American Economic Review 96(5):1737–1768.
41

Crawford, Vincent P, Miguel A Costa-Gomes and Nagore Iriberri. 2013. “Structural models of
nonequilibrium strategic thinking: Theory, evidence, and applications.” Journal of Economic Literature 51(1):5–62.
Crawford, Vincent P. and Nagore Iriberri. 2007. “Level-k Auctions: Can a Nonequilibrium Model
of Strategic Thinking Explain the Winner’s Curse and Overbidding in Private-Value Auctions?”
Econometrica 75(6):1721–1770.
Croson, Rachel TA. 1999. “The disjunction effect and reason-based choice in games.” Organizational
Behavior and Human Decision Processes 80(2):118–133.
Enke, Benjamin. 2017. “What You See Is All There Is.” Working paper .
Enke, Benjamin and Florian Zimmermann. 2017. “Correlation neglect in belief formation.” Working
paper .
Esponda, Ignacio. 2008. “Behavioral equilibrium in economies with adverse selection.” The American
Economic Review 98(4):1269–1291.
Esponda, Ignacio and Emanuel Vespa. 2014. “Hypothetical Thinking and Information Extraction in
the Laboratory.” American Economic Journal: Microeconomics 6(4):180–202.
Esponda, Ignacio and Emanuel Vespa. 2017a. “Contingent Preferences and the Sure-Thing Principle:
Revisiting Classic Anomalies in the Laboratory.”.
Esponda, Ignacio and Emanuel Vespa. 2017b. “Endogenous sample selection: A laboratory study.”
Quantitative Economics (forthcoming) .
Eyster, Erik and Georg Weizsäcker. 2010. “Correlation neglect in financial decision-making.” Working
Paper .
Eyster, Erik and Matthew Rabin. 2005. “Cursed equilibrium.” Econometrica 73(5):1623–1672.
Eyster, Erik, Matthew Rabin and Georg Weizsäcker. 2015. “An Experiment on Social Mislearning.”
Working paper .
Fischbein, Efraim and Ditza Schnarch. 1997. “The evolution with age of probabilistic, intuitively
based misconceptions.” Journal for research in mathematics education pp. 96–105.
Fischhoff, Baruch. 1975. “Hindsight 6= foresight: the effect of outcome knowledge on judgment under
uncertainty.” Journal of Experimental Psychology: Human Perception and Performance 1:288–299.
Fragiadakis, Daniel, Daniel Knoepfle and Muriel Niederle. 2017. “Who is Strategic?”.
Gabaix, Xavier. 2014. “A sparsity-based model of bounded rationality.” The Quarterly Journal of
Economics 129(4):1661–1710.

42

Gennaioli, Nicola and Andrei Shleifer. 2010. “What comes to mind.” The Quarterly Journal of
Economics 125(4):1399–1433.
Hamilton, William. 1878. Lectures on metaphysics. Sheldon.
Hoffrage, Ulrich, Gerd Gigerenzer, Stefan Krauss and Laura Martignon. 2002. “Representation
facilitates reasoning: what natural frequencies are and what they are not.” Cognition 84(3):343–
352.
Holt, Charles A and Susan K Laury. 2002. “Risk aversion and incentive effects.” American Economic
Review 92(5):1644–1655.
Ivanov, Asen, Dan Levin and Muriel Niederle. 2010. “Can relaxation of beliefs rationalize the winner’s
curse?: an experimental study.” Econometrica 78(4):1435–1452.
Jehiel, Philippe. 2005.

“Analogy-based expectation equilibrium.” Journal of Economic Theory

123(2):81–104.
Kagel, John H. and Dan Levin. 1986. “The winner’s curse and public information in common value
auctions.” The American Economic Review pp. 894–920.
Kahneman, Daniel and Amos Tversky. 1972. “Subjective probability: A judgment of representativeness.” Cognitive Psychology 3(3):430–454.
Kahneman, Daniel and Amos Tversky. 1979. “Prospect theory: An analysis of decision under risk.”
Econometrica pp. 263–291.
Kőszegi, Botond and Adam Szeidl. 2012. “A model of focusing in economic choice.” The Quarterly
Journal of Economics 128(1):53–104.
Laibson, David. 1997. “Golden eggs and hyperbolic discounting.” The Quarterly Journal of Economics 112(2):443–478.
Langrall, Cynthia W and Edward S Mooney. 2005. “Characteristics of elementary school students’
probabilistic reasoning.” pp. 95–119.
Lennie, Peter. 2003. “The cost of cortical computation.” Current Biology 13(6):493–497.
Levin, Dan, James Peck and Asen Ivanov. 2016.

“Separating Bayesian updating from non-

probabilistic reasoning: An experimental investigation.” American Economic Journal: Microeconomics 8(2):39–60.
Li, Shengwu. 2017. “Obviously strategy-proof mechanisms.” American Economic Review (forthcoming) .
Mobius, Markus M, Muriel Niederle, Paul Niehaus and Tanya S Rosenblat. 2014. “Managing selfconfidence: Theory and experimental evidence.” Working Paper .
43

Nagel, Rosemarie. 1995. “Unraveling in guessing games: An experimental study.” The American
Economic Review 85(5):1313–1326.
Ngangoué, Kathleen and Georg Weizsäcker. 2017. “Learning from unrealized versus realized prices.”.
Niederle, Muriel. 2016. “Gender.” in The Handbook of Experimental Economics, Volume 2, edited
by John H Kagel and Alvin E Roth. .
O’Donoghue, Ted and Matthew Rabin. 1999. “Doing it now or later.” American Economic Review
pp. 103–124.
Rabin, Matthew. 2000. “Risk aversion and expected-utility theory: A calibration theorem.” Econometrica 68(5):1281–1292.
Rabin, Matthew and Joel L Schrag. 1999. “First impressions matter: A model of confirmatory bias.”
The Quarterly Journal of Economics 114(1):37–82.
Savage, Leonard J. 1972. The foundations of statistics. Courier Corporation.
Schotter, Andrew. 2003. “Decision making with naïve advice.” American Economic Review pp. 196–
201.
Shafir, Eldar. 1994. “Uncertainty and the difficulty of thinking through disjunctions.” Cognition
50(1):403–430.
Shafir, Eldar and Amos Tversky. 1992. “Thinking through uncertainty: Nonconsequential reasoning
and choice.” Cognitive Psychology 24(4):449–474.
Shafir, Eldar, Itamar Simonson and Amos Tversky. 1993. “Reason-based choice.” Cognition 49(1):11–
36.
Sims, Christopher A. 2003. “Implications of rational inattention.” Journal of Monetary Economics
50(3):665–690.
Thompson, Suzanne C, Wade Armstrong and Craig Thomas. 1998. “Illusions of control, underestimations, and accuracy: a control heuristic explanation.” Psychological Bulletin 123(2):143.
Tversky, Amos and Daniel Kahneman. 1974. “Judgment under Uncertainty: Heuristics and Biases.”
Science 185(4157):1124–1131.
Tversky, Amos and Eldar Shafir. 1992. “The disjunction effect in choice under uncertainty.” Psychological Science 3(5):305–310.
Vespa, Emanuel and Alistair J. Wilson. 2016. “Communication with multiple senders: An experiment.” Quantitative Economics 7(1):1–36.

44

A

Main Findings Using Submitted Prices

In the main part of the paper we analyze results and provide evidence for the Power of Complexity
by classifying subjects into types. In this appendix we show that we reach similar conclusions if we
use the aggregate distribution of submitted prices in part 1.
Most of our analysis in this section groups prices into five categories: p < vL , p = vL , p ∈ (vL, vH ),
p = vH and p > vH . However, we first provide a more detailed classification in Table 16, which
further disaggregates two categories: p ∈ (vL , vH ) and p > vH , to evaluate whether some prices are
especially common. For example, we observe that 0.48 (0.41) percent of prices submitted in prob
(det) are the average of vL and vH (p =

vL +vH
).
2

We also observe that some prices are equal to

1.5 times one of the values. This is the case for 1.36 and 0.38 percent of prices if v = vL for prob
and det, respectively. We do not, however, observe a large mass of prices at any particular price
p ∈ (vL , vH ) ∪ (vH , 150]. The most common category is prices that are one unit above the value of
the company, which is the focus of one of our robustness checks for the classification of types in the
next section of this appendix.

p < vL
p = vL
p = vL + 1
p = vL + 2
p = 1.5 × vL
H
p = vL +v
2
v
+v
p = 34 L 2 H
p ∈ (vL , vH ) and not yet classified
p = vH
p = vH + 1
p = vH + 2

p = vL + vH
p = 1.5 × vH
p > vH and not yet classified
Total

prob
All Rounds Last 10
1.12
0.85
42.69
43.03
3.59
3.30
1.94
1.60
1.36
0.74
0.48
0.27
0.05
0.05
15.40
14.52
21.33
23.94
1.94
2.13
1.22
1.28
0.48
0.53
0.0
0.0
8.40
7.77
100.0
100.0

det
All Rounds Last 10
4.10
3.39
53.91
55.52
4.07
4.04
1.07
0.82
0.38
0.27
0.41
0.49
0.0
0.0
9.29
8.31
17.16
16.99
1.39
1.64
0.71
0.93
1.61
1.97
0.05
0.11
5.85
5.52
100.0
100.0

Table 16: Distribution of Prices by Treatment (in %)
Notes: 188 and 183 participants in prob and det, respectively. All rounds include data for 3,760 (188 subjects × 20
rounds) and 3,660 (183 subjects × 10 rounds) in prob and det, respectively. Last 10 rounds include data for 1,880
(188 subjects × 10 rounds) and 1,830 (183 subjects × 10 rounds) in prob and det, respectively.

Table 17 shows the distribution of submitted prices for prob and det divided into five categories:
p < vL , p = vL , p ∈ (vL , vH ), p = vH and p > vH for each treatment and a series of subsamples.
45

The first two rows include all 20 rounds of part 1, the second set of two rows constrains the sample
to the last 10 rounds. The third and fourth set consider subjects who never took a risky lottery in
part 5 and subjects who took at least one risky lottery in part 5, respectively.

All rounds
Last 10 rounds
No risky

p < vL

p = vL

p ∈ (vL , vH )

p = vH

p > vH

Participants

prob

1.1

42.7

22.8

21.3

12.1

188

det

4.1

53.9

15.2

17.2

9.6

183

prob

0.8

43.0

20.5

23.9

11.7

188

det

3.4

55.5

13.9

17.0

10.1

183

prob

1.3

50.0

18.8

19.3

10.7

129

Lottery

det

4.8

58.8

12.8

16.1

7.5

128

At least one

prob

0.8

26.7

31.7

25.8

15.1

59

Risky Lottery

det

2.5

42.5

20.9

19.6

14.6

55

Table 17: Main Treatments: Distribution of Prices in Part 1 (in %)
Notes: ‘All rounds’ uses prices submitted for all 20 problems and ‘Last 10 rounds’ for the last 10 problems subjects faced in part
1. ‘No risky lottery’ includes subjects who never selected a risky lottery in part 5. ‘At least one risky lottery’ involves subjects
who selected at least one risky lottery in part 5.

Table 17 shows that, when considering all 20 rounds, there are substantially more prices p = vL in
det (53.9 percent) than in prob (42.7 percent). The numbers are virtually unchanged when we
only consider the last 10 rounds. Column (1) of Table 18 presents a random-effects estimation that
confirms this treatment effect to be significant. Focusing on just the payoff-maximizing price p = vL
we find evidence consistent with the Power of Certainty hypothesis.
Offers of p = vH are 4.1 percentage points higher in prob than in det, a difference that increases
to 6.9 percentage points in the last 10 rounds. The treatment effect, however, is not statistically
significant per Column (2) of Table 18. Consistent with the classification into types, while it may be
tempting to attribute p = vH prices in prob to subjects being risk seeking, the prevalence of such
prices in det, where they are dominated, casts doubt on that interpretation.
Prices that are strictly dominated in both treatments, that is p ∈
/ {vL , vH }, are more prevalent in
prob than in det, with 36.0 (33.0) and 28.9 (27.4) percent such prices in all (the last 10) rounds,
respectively. Column (3) of Table 18 shows that the treatment effect is statistically significant.51
The differences in submitted prices across treatments can be easily summarized by Figure 6. We show
the cumulative distribution of normalized prices. We compute a normalized price pN =

p−vL
vH −vL ,

such

51
Note that there is a class of prices p ∈
/ {vL , vH } that guarantees no losses and could be interpreted as subjects
opting out of participating in the problems, namely p < vL . There are few such prices in either treatment, though
the fraction is a little higher in det. Nonetheless, 1.1 (0.8) percent of prices in prob are such that p < vL in all (the
last 10) rounds, while the numbers are 4.1 (3.8) percent in det. The proportion of subjects who perhaps mistakenly
believe that they have to submit a price of vL + 1 to buy a company of value vL is also small and comparable across
treatments, 3.6 (3.3) percent in prob and 4.1 (4.0) percent in det in all (the last 10) rounds. The instructions
do include a question that checks the understanding (see Instructions Appendix) that submitting p = v guarantees
buying a company of value v.

46

(1)
p=vL
Det
Num Risky
Num Risky × Det

(2)
p=vH

(3)
p∈
/ {vH , vL }

0.151∗∗∗
(0.054)

0.002
(0.043)

−0.153∗∗∗
(0.055)

−0.095∗∗∗
(0.028)

0.034
(0.022)

0.061∗∗
(0.029)
−0.043
(0.042)

0.035
(0.041)

0.008
(0.033)

Num Errors

−0.091∗∗
(0.042)

−0.003
(0.033)

0.094∗∗
(0.042)

Num Errors × Det

−0.149∗∗∗
(0.058)

−0.008
(0.046)

0.158∗∗∗
(0.059)

Last 10 Periods

0.007
(0.009)

0.052∗∗∗
(0.008)

−0.059∗∗∗
(0.007)

Last 10 Periods × Det

0.025∗∗
(0.012)

−0.055∗∗∗
(0.012)

0.030∗∗∗
(0.010)

Female

−0.149∗∗∗
(0.041)

0.052
(0.032)

0.097∗∗
(0.041)

White

0.118∗∗
(0.049)

−0.038
(0.039)

−0.080
(0.050)

Young

−0.026
(0.041)

0.009
(0.032)

0.017
(0.041)

Low Schooling

−0.067
(0.042)

0.061∗
(0.033)

0.006
(0.043)

0.140∗∗∗
(0.050)

0.311∗∗∗
(0.064)

Constant
Observations

0.548∗∗∗
(0.063)
7420

7420

7420

Table 18: Main Treatments: Random Effects Estimations Output using Part-1 Prices
Notes: The dependent variable takes value 1 if: (1) p = vL , (2) p = vH , (3) p ∈
/ {vL , vH }. Det is a dummy variable that
takes value 1 if the observation corresponds to the deterministic treatment. Num Risky and Num Errors are individual-specific.
Num Risky is the number of risky lotteries the subject chooses in part 5 (from 0 to 3). Num Errors is the number of errors the
subject made in the part 1 instructions (from 0 to 2). Last 10 Periods is a dummy variable that takes value 1 if the observation
corresponds to the last 10 periods of part 1. Female, White, Young and Low Schooling are a dummies that take value 1,
respectively, if the subject’s gender is female, the reported ethnicity is white, their age is below the median age of 32, and their
schooling if their education level is ‘Some College’ or lower. There are 371 individuals (188 in prob and 183 in det) and 20
observations in part 1 per individual for a total of 7,420 observations.

47

1
.2

.4

CDF

.6

.8

1
.8
.6
CDF
.4
.2

0

Probabilistic
Deterministic

0

Probabilistic
Deterministic
−.4

−.2

0

.2

.4

.6

.8

1

1.2

1.4

1.6

1.8

2

−.4

−.2

0

.2

.4

Normalized Prices

.6

.8

1

1.2

1.4

1.6

1.8

2

1.6

1.8

2

Normalized Prices

.8
.6
CDF
.4
.2

.2

.4

CDF

.6

.8

1

(b) Last 10 rounds

1

(a) All rounds

0

0

Probabilistic
Deterministic
−.4

−.2

0

.2

.4

.6

.8

1

1.2

1.4

1.6

1.8

2

−.4

Normalized Prices

−.2

0

.2

.4

.6

.8

1

1.2

1.4

Normalized Prices

(c) Subjects who never took a risk

(d) Subjects who took risk once or more

Figure 6: Main Treatments Part 1-Distribution of Normalized Prices by Treatment
Notes: Normalized Prices (pN ) are computed for each question and subject in the following manner: pN =

p−vL
,
vH −vL

where p is

the price submitted by the subject for a question with values {vL , vH }. Notice that if p = vH , then pN = 1 and if p = vL , then
pN = 0. Values of pN ∈ (0, 1) indicate subjects submitting dominated prices in the interval p ∈ (vL , vH ). The case pN < 0,
corresponds with p < vL and, finally, the case pN > 1 takes place if p > vH . ‘All rounds’ uses the answers submitted for all
20 problems and ‘Last 10 rounds’ for the last 10 problems subjects faced in part 1. ‘Subjects who never took a risk’ includes
subjects who never selected a risky lottery in part 5. ‘Subjects who took risk once or more’ involves subjects who selected at
least one risky lottery in part 5.

48

(1)
p=vL

(2)
p=vL

Det

−0.057
(0.036)

0.045
(0.046)

Num Risky

−0.002
(0.019)

0.021
(0.019)

Num Risky × Det

−0.014
(0.028)

−0.036
(0.027)

Num Errors

0.056∗∗
(0.028)

0.079∗∗∗
(0.027)

(3)
p=vH
0.199∗∗∗
(0.059)

(4)
p=vH

(5)
p∈
/ {vH , vL }

(6)
p∈
/ {vH , vL }

0.059
(0.074)

−0.142∗∗
(0.055)

−0.105
(0.066)

−0.069∗∗
(0.031)

−0.054∗
(0.030)

0.072∗∗
(0.029)

0.033
(0.027)

0.048
(0.045)

0.041
(0.044)

−0.034
(0.042)

−0.004
(0.039)

−0.150∗∗∗
(0.046)

−0.137∗∗∗
(0.044)

0.094∗∗
(0.043)

0.059
(0.040)

Num Errors × Det

−0.070∗
(0.039)

−0.095∗∗
(0.039)

−0.092
(0.063)

−0.033
(0.063)

0.162∗∗∗
(0.060)

0.128∗∗
(0.056)

Female

−0.014
(0.027)

−0.001
(0.027)

−0.080∗
(0.044)

−0.042
(0.043)

0.094∗∗
(0.042)

0.043
(0.039)

White

−0.006
(0.033)

−0.024
(0.032)

0.101∗
(0.054)

0.080
(0.052)

−0.095∗
(0.051)

−0.056
(0.047)

Young

0.010
(0.027)

0.004
(0.027)

−0.023
(0.044)

−0.008
(0.043)

0.013
(0.042)

0.004
(0.038)

−0.035
(0.028)

−0.030
(0.027)

−0.001
(0.046)

−0.001
(0.044)

0.036
(0.043)

0.031
(0.039)

Low Schooling
VL (Part 1)
VL (Part 1) × Det
Constant
Observations

0.144∗∗∗
(0.042)
1855

0.223∗∗∗
(0.041)

0.155∗∗
(0.066)

−0.230∗∗∗
(0.057)

0.187∗∗
(0.091)

0.057
(0.044)
1855

0.637∗∗∗
(0.068)
1855

0.561∗∗∗
(0.070)
1855

−0.378∗∗∗
(0.059)
0.043
(0.082)
0.219∗∗∗
(0.064)
1855

0.382∗∗∗
(0.063)
1855

Table 19: Main Treatments: Random Effects Estimations Output using Part-2 Prices
Notes: The dependent variable takes value 1 if the subject selects in (1) and (2) p = vL , in (3) and (4) p = vH and in (5) and
(6) p ∈
/ {vL , vH }. The regression includes all 5 periods of part 2. Columns (2), (4) and (6) control for whether the subject was
classified as submitting p = vL in the last five rounds of part 1 (VL ) and its interaction with Det. Det is a dummy variable that
takes value 1 if the observation corresponds to the deterministic treatment. Num Risky and Num Errors are individual-specific.
Num Risky is the number of risky lotteries the subject chose in part 5 (from 0 to 3). Num Errors is the number of errors the
subject made in the part 1 instructions (from 0 to 2). Female, White, Young and Low Schooling are a dummies that take value
1, respectively, if the subject’s gender is female, the reported ethnicity is white, their age is below the median age of 32, and
their schooling if their education level is ‘Some College’ or lower.

49

Part 1 Types

VH

VL

Mix

Dom

Residual

Participants

prob

12.2

31.9

20.7

27.7

7.5

188

det

9.8

47.5

9.8

19.3

13.7

183

Table 20: Part 1 Type Classification [as % of participants]
Notes: Subjects are classified as belonging to a type if their submitted price corresponds to the same type in the last 5 rounds
of part 1 (rounds 16-20). VL :p = vL . VH :p = vH . Mix: p = vL or p = vH (and at least one of each). Dom: p ∈
/ {vL , vH }.
Residual: all other subjects.

that pN = 0 indicates p = vL , and pN = 1 corresponds to p = vH . The figure shows the cumulative
distribution of normalized prices in part 1 for all rounds (panel a) and the last 10 rounds (panel b).
The distribution of normalized prices in prob first order stochastically dominates the distribution
in the det and the difference is statistically significant.52 Panels c and d confirm that the results
hold if we distinguish between subjects who did and did not select at least one risky lottery in part
5. (The estimates reported in Table 18 also control for part 5 choices).
Finally, Table 19 reports findings in part 2. The left-hand side variables take value 1 if p = vL (the
first two columns) or if p = vH (last two columns) and the unit of observation is each question of
part 2. Columns (1), (3) and (5) present results that use the same controls as in Table 18. There is
no treatment effect in p = vL (1), but there is a significantly higher likelihood of observing p = vH
in det per (3). When we combine findings of part 1 with those of part 2 (see columns (2), (4) and
(6)), we find that being classified as VL in part 1 increases the chances of submitting p = vL in part
2 of prob, but not in det (where the addition of 0.223 and -0.230 is essentially zero). We also
find that being classified as VL in part 1 significantly increases the chances of submitting p = vH
in both treatments (by a positive coefficient 0.155), but there is an additional effect in det that is
significant (a coefficient of 0.187).
In summary, the evidence on prices rather than classification of subjects confirms the results from
Section 3, and mirrors that risk seeking preferences cannot account for the findings. That is, using
prices instead of types in part 1 confirms the evidence for the Power of Certainty.
We can also use prices, rather than types, to assess the relative role of PoC. For that, we first provide
evidence on prices in the first 25 rounds of the onefirm treatment.
When considering all 25 rounds of the onefirm treatment, 73.9 percent of prices are p = v, 8.1
percent are p < v and 18 percent are p > v.53
52

We test for first-order stochastic dominance using the test in Barrett and Donald (2003). The test consists of two
steps. We first test the null hypothesis that the distribution in the deterministic treatment either first order stochastically dominates or is equal to the distribution in the probabilistic treatment. We reject this null hypothesis using all
rounds or the last 10 rounds, the corresponding p−value is 0 in both cases. We then test the null hypothesis that the
distribution in the probabilistic treatment first order stochastically dominates the distribution in the deterministic
treatment. We cannot reject the null in this case, with a corresponding p−value of 0.817 using all rounds and 0.239
using the last 10 rounds.
53
The numbers for the last 10 of the 25 rounds are very similar, 74.3, 7.1 and 18.6 percent, respectively. When

50

prob
All

VL80 VH80 /V 80

{M ix80 VH80 , VH80 VH80 }

Focal80

Dom80

Residual80

Participants

26.6

22.9

15.4

27.1

8.0

188

det

49.2

15.3

4.9

21.3

9.3

183

onefirm

67.1

–

–

20.5

12.4

428

No

prob

32.6

21.7

16.3

20.1

9.3

129

Risky

det

52.4

14.8

6.3

15.6

10.9

128

Lottery

onefirm

71.8

–

–

18.1

10.1

276

At least one

prob

13.5

25.4

13.6

42.4

5.1

59

Risky

det

41.8

16.4

1.8

34.5

5.5

55

Lottery

onefirm

58.6

–

–

25.0

16.4

152

Table 21: Part 1 and Part 2 Type Classification Allowing for Deviations [as % of participants]
Notes: Types are defined based on the prices p1 submitted in at least 80 percent of the last five rounds of part 1 and p2
80 : p = v and p = v . Type {M ix80 V 80 , V 80 V 80 }:
submitted in at least 80 percent of the five rounds of part 2. Type VL80 VH
1
2
L
H
H
H H
80 . Type ‘Focal80 ’: p , p ∈ {v , v } and
p1 ∈ {vL , vH } and at least one p1 = vH and p2 = vH and not classified as VL80 VH
1 2
L H
80 V 80 , or V 80 Mix 80 . Type ‘Dom80 ’:
at least one p2 = vL (corresponds to VL80 VL80 , VL80 Mix 80 , Mix 80 VL80 , Mix80 Mix 80 , VH
L
H
p1 , p 2 ∈
/ {vL , vH }. Residual80 : All remaining subjects. In onefirm subjects are classified as V 80 if they submit p = v in at least
80
80 percent of rounds 16-25, and as ‘Dom ’ if p 6= v in at least 80 percent of rounds 16-25, with Residual80 being the remaining
subjects. In all classifications, since there is a potential for a subject to be classified as two different types (in two different
columns), we break ties such that we classify the subject in the left column.

B

Classification using Part 1 and Robustness of Classifications

First, we show the classification of subjects into types using only part 1, see Table 20.
The first robustness of classification we consider weakens the requirement that subjects submit the
type specific price in each of the last 5 rounds of part 1 and all five rounds of part 2. Subjects only
have to submit the type specific price in 4 of the 5 rounds (that is 80 percent of the prices have to
be type specific). For each subject we first assess whether they can be classified as VL80 VH80 . We then
check for the remainder, whether they can be classified as M ix80 VH80 or VH80 VH80 . And subsequently
for the remainder we assess whether they can be classified as F ocal80 , and then as Dominated80 .
All residual subjects are classified as Residual80 , see Table 21. Figure 7 shows the evolution of types
using this more lenient classification.
Table 22 shows that the main result of Table 3 is robust to using this more lenient classification.
Specifically, subjects in det are significantly more likely to be classified as VL80 VH80 compared to
subjects in prob. While subjects who chose risky lotteries in part 5 are less likely to be so classified,
we distinguish between subjects in the probabilistic and the deterministic version of onefirm, the numbers are very
similar. This is not surprising given that in the first 25 rounds the only difference between the two treatments is
that subjects have seen instructions for either treatment, but have not actually played in those treatments. In the
probabilistic version 74.9 (74.9) percent of prices are p = v, 9.7 (8.5) are p < v and 15.4 (16.6) are p > v when we
consider all 25 (the last 10) rounds. In the deterministic version, the numbers are 72.9 (73.6) percent of prices are
p = v, 6.5 (5.7) are p < v and 20.6 (20.7) are p > v when we consider all 25 (the last 10) rounds. Furthermore,
a random-effects panel regression with a dummy that takes value 1 if p = v on the left-hand side and a treatment
dummy on the right-hand side shows that the treatment dummy is not significant if we look at all rounds (p-value
0.576) or at the last 10 rounds (p-value 0.723).

51

50
10

Percentage of subjects
20
30

40

50
40
Percentage of subjects
20
30
10

0

0

Probabilistic
Deterministic
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(b) p ∈ {vL , vH }: n to 15 & {M ix80 VH80 , VH80 VH80 }.
50
40
Percentage of subjects
20
30
10
0

0

10

Percentage of subjects
20
30

40

50

(a) p = vL : n to 15 & VL80 VH80 .

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(c) p ∈ {vL , vH }: n to 15 & Focal80 .

(d) p ∈
/ {vL , vH }: n to 15 & Dominated80 .

Figure 7: Evolution of Types (Part 1 & Part 2) Allowing for Deviations.
Notes: Percent of subjects who submit the described part-1 price p1 in 80 percent of the rounds from round n to 20 and part-2
price p2 in 80 percent of the rounds in part 2 by treatment.

52

(1)
80
VL80 VH

(3)
Focal80

(4)
Dom80

(5)
Dom80 or Res80

−0.085
(0.055)

−0.072∗
(0.042)

−0.062∗
(0.033)

0.011
(0.029)

−0.013
(0.022)

0.030
(0.048)

0.023
(0.042)

0.004
(0.032)

−0.029
(0.044)

Num Errors

−0.124∗∗
(0.048)

−0.048
(0.043)

0.069∗∗
(0.032)

0.059
(0.044)

0.103∗∗
(0.048)

Num Errors × Det

−0.112∗
(0.067)

0.021
(0.060)

−0.090∗∗
(0.045)

0.157∗∗
(0.061)

0.181∗∗∗
(0.066)

Female

−0.103∗∗
(0.047)

0.033
(0.042)

−0.032
(0.032)

0.073∗
(0.043)

0.103∗∗
(0.046)

White

0.132∗∗
(0.057)

0.001
(0.051)

−0.023
(0.038)

−0.106∗∗
(0.052)

−0.110∗
(0.056)

Det
Num Risky
Num Risky × Det

0.272∗∗∗
(0.063)

(2)
80 , V 80 V 80 }
{M ix80 VH
H H

−0.125∗∗
(0.057)
0.080∗∗∗
(0.030)

−0.115∗
(0.062)
0.064∗∗
(0.032)
−0.057
(0.047)

Young

−0.031
(0.047)

−0.001
(0.042)

0.009
(0.032)

0.046
(0.043)

0.023
(0.046)

Low Schooling

−0.033
(0.049)

0.038
(0.043)

−0.044
(0.033)

0.015
(0.044)

0.039
(0.048)

0.212∗∗∗
(0.066)

0.270∗∗∗
(0.071)

Constant
Observations

0.339∗∗∗
(0.072)
371

0.209∗∗∗
(0.064)
371

0.181∗∗∗
(0.049)
371

371

371

Table 22: Main Treatments: Estimation output using last 5 rounds of part 1 and part 2 for the
classification of types that allows for one deviation
80 , (2)
Notes: Results from a linear regression. The dependent variable takes value 1 if the subject is classified as (1) VL80 VH
80 , V 80 V 80 }, (3) Focal80 , (4) Dom80 , (5) Dom80 or Res80 . A subject is classified as V 80 V 80 if they submitted p = v
{M ix80 VH
L
H H
L
H
in at least 80 percent of the last 5 periods of part 1 and p = vH in at least 80 percent of the periods of part 2. A subject is
80
80
classified as M ix VH if they mix between p = vL and p = vH in the last 5 periods of part 1, selecting at least one of the two
and not being previously classified as VL80 , and if they select p = vH in at least 80 percent of the periods of part 2. The other
definitions of types are adjusted accordingly. Det is a dummy variable that takes value 1 if the observation corresponds to the
deterministic treatment. Num Risky and Num Errors are individual-specific. Num Risky is the number of risky lotteries the
subject chooses in part 5 (from 0 to 3). Num Errors is the number of errors the subject made in the part 1 instructions (from 0
to 2). Female, White, Young and Low Schooling are a dummies that take value 1, respectively, if the subject’s gender is female,
the reported ethnicity is white, their age is below the median age of 32, and their schooling if their education level is ‘Some
College’ or lower.

there is no significant treatment effect even though risk preferences have no bearing in det. In
addition, subjects who took risks in part 5 are more likely to be classified as the Dominated80 type
or as either Dominated80 or Residual80 type, further casting doubt that risk-seeking preferences
are responsible for a correlation between a VL80 VH80 classification and part 5 choices.
Table 21 also shows the classification of subjects in the first two parts of the onefirm treatments.
In onefirm, a subject is classified as V 80 if they submit p = v in at least 80 percent of the rounds
16-25, among the remainder, subjects are classified as Dom80 , if p 6= v in at least 80 percent of
rounds 16-25, and Residual80 contains the remaining subjects.
The Power of Certainty is not only robust to the more lenient classification of types, but so is its
quantitative importance. We can think of the difference in between 67.1 and 26.6 percent as what
used to be attributed to computational complexity. However, 55.8 percent of that total effect can
53

All

VLA VHA /V A

{M ixA VHA , VHA VHA }

FocalA

DomA

ResidualA

Participants

prob

21.8

26.6

19.1

17.6

14.9

188

det

45.9

18.0

8.2

11.5

16.4

183

onefirm

66.8

–

–

12.9

20.3

428

No

prob

27.1

27.1

18.6

12.4

14.7

129

Risky

det

47.7

17.2

9.4

8.6

17.2

128

Lottery

onefirm

72.5

–

–

12.3

15.2

276

At least one

prob

10.2

25.4

20.3

28.8

15.3

59

Risky

det

41.8

20.0

5.5

18.2

14.5

55

Lottery

onefirm

56.6

–

–

13.8

29.6

152

Table 23: Part 1 and Part 2 Type Classification Allowing for Small Overbidding [as % of participants]
Notes: Adjusted Types are defined based on the prices p1 submitted in the last five rounds of part 1 and p2 submitted in all five
A : p ∈ P = [v , v + 2] and p ∈ P
A A
A A
rounds of part 2. Type VLA VH
1
2
L
L L
H = [vH , vH + 2]. Type {M ix VH , VH VH }: p1 ∈ {PL , PH }
A . Type ‘FocalA ’: p , p ∈ {P , P } and at least one
and at least one p1 ∈ PH and p2 ∈ PH and not classified as VLA VH
1 2
L
H
A V A , or V A Mix A . Type ‘DomA ’: p , p ∈
p2 ∈ PL (corresponds to VLA VLA , VLA Mix A , Mix A VLA , MixA Mix A , VH
1 2 / {vL , vH } and
L
H
not classified as another type. ResidualA : All remaining subjects. In onefirm subjects are classified as V A if they submit
p ∈ P = [v, v + 2] in all rounds 16-25, and as ‘DomA ’ if p 6= v in all of rounds 16-25 (and not classified as V A ), with ResidualA
being the remaining subjects.

actually be attributed to the lack of the PoC, and only the remainder to complexity.
In a second robustness exercise we classify a subject as submitting p = vL and p = vH if they
submit p ∈ [vL , vL + 2] and p ∈ [vH , vH + 2]. While we had a question in the instructions testing
the understanding that p = v would purchase a firm of value v (see Instructions Appendix), some
subjects may still be confused and pay a price that is slightly too high. Again, if we used this
second classification, we would reach similar conclusions. In this case, we can think of the difference
between 66.8 and 21.8 percent as what used to be capture complexity, but 53.6 percent of the total
difference can be attributed to PoC.

C

Robustness to Binary Lottery Choice Controls

In this section we show that the main results in Table 3 is robust when instead of linearly controlling
for the number of risky lotteries a subject chose, we use a dummy whether the subject chose at least
one risky lottery in part 5. The results are shown in Table 24.

D

Advice

In Table 25 we provide the full results as to the numerical price recommendation subjects provided
as a function of the outcomes mentioned in the Advice.

54

(1)
VL VH

(3)
Focal

(4)
Dom

−0.098∗
(0.058)

−0.050
(0.049)

−0.073
(0.054)

−0.160∗∗
(0.068)

−0.033
(0.064)

0.004
(0.054)

Risky × Det

0.131
(0.097)

0.048
(0.092)

−0.031
(0.077)

−0.087
(0.085)

Num Errors

−0.101∗∗
(0.046)

−0.063
(0.044)

0.048
(0.037)

0.074∗
(0.040)

0.116∗∗
(0.049)

Num Errors × Det

−0.094
(0.064)

0.025
(0.061)

−0.096∗
(0.051)

0.076
(0.056)

0.164∗∗
(0.068)

Female

−0.133∗∗∗
(0.045)

0.044
(0.043)

−0.005
(0.036)

0.070∗
(0.039)

0.094∗∗
(0.047)

White

0.095∗
(0.055)

0.019
(0.052)

−0.011
(0.044)

−0.092∗
(0.048)

−0.103∗
(0.058)

Young

−0.024
(0.045)

0.004
(0.043)

−0.006
(0.036)

0.025
(0.039)

0.026
(0.047)

Low Schooling

0.009
(0.047)

0.031
(0.044)

−0.053
(0.037)

0.027
(0.041)

0.013
(0.049)

Constant

0.298∗∗∗
(0.070)

0.229∗∗∗
(0.066)

0.134∗∗
(0.061)

0.273∗∗∗
(0.074)

Det
Risky

Observations

0.229∗∗∗
(0.062)

371

(2)
{M ixVH , VH VH }

371

0.200∗∗∗
(0.056)
371

0.192∗∗∗
(0.059)

371

(5)
Dom or Res
−0.081
(0.065)
0.189∗∗∗
(0.071)
−0.148
(0.102)

371

Table 24: Main Treatments: Estimation output using last 5 rounds of part 1 and part 2 for the
classification of types and using a discrete dummy to control for part 5 choices
Notes: Results from a linear regression. The dependent variable takes value 1 if the subject is classified as (1) VL VH , (2) {MixVH ,
VH VH }, (3) Focal, (4) Dom, (5) Dom or Res. Det is a dummy variable that takes value 1 if the observation corresponds to the
deterministic treatment. Num Risky and Num Errors are individual-specific. Risky is a dummy that takes value 1 if the subject
took a risky lottery in part 5 and zero otherwise. Num Errors is the number of errors the subject made in the part 1 instructions
(from 0 to 2). Female, White, Young and Low Schooling are a dummies that take value 1, respectively, if the subject’s gender
is female, the reported ethnicity is white, their age is below the median age of 32, and their schooling if their education level is
‘Some College’ or lower.

55

Part 3 Recommendation
vL
vH
prob
Mix {vL , vH }
Dom
Unclassified
Sum

det

vL
vH
Mix {vL , vH }
Dom
Unclassified
Sum

None
8.5
8.5
1.1
12.8
9.5
40.4

All
20.2
1.6
4.3
0.5
0.0
26.6

5.5
9.8
0.0
8.7
5.4
29.5

51.4
0.0
0.0
0.6
0.5
52.5

Advice: Outcomes Mentioned
Large Gain Large Loss p = vL
0.5
10.7
2.7
10.7
1.6
0.0
1.0
1.1
0.0
0.5
0.0
0.5
0.0
0.0
0.0
12.8
13.3
3.2
0.0
4.9
0.0
0.5
0.6
6.0

3.3
0.0
0.0
0.0
0.6
3.8

3.8
0.0
0.0
0.0
0.5
4.4

Error
0.5
2.1
0.5
0.6
0.0
3.7

Sum
43.1
24.5
8.0
14.9
9.5
100.0

0.0
2.7
0.0
0.5
0.6
3.8

63.9
17.5
0.0
10.4
8.2
100.0

Table 25: Part 3 (Recommendation) v. Part 3 (Advice Outcomes Categories)
Notes: 188 participants in prob and 183 in det. There are four outcomes (v, p) where v, p ∈ {vL , vH }. All (none)
are subjects who mentioned all (none of the) four outcomes in any form. Large Gain: Subjects who mention either
{(vL , vL ) and (vH , vH )}, {(vH , vH )}, {(vL , vH ) and explicitly (vH , vH )}, {(vL , vL ), (vH , vL ) and (vH , vH )}. Large
Loss: Subjects who mention either {(vL , vH )}, {(vL , vH ) and implicitly (vH , vH )}, {(vL , vL ), (vH , vL ) and (vL , vH )},
{(vL , vL ), (vL , vH ) and (vH , vH )}. p = vL : Subjects who mention outcomes: {(vL , vL )}, {(vL , vL ) and (vH , vL )}.
Mistake: Subjects who compute one of the payoffs wrongly.

In Table 26 we provide a detailed description of the classification of subjects based on their submitted
prices in parts 1 and 2 as a function of the outcomes mentioned in the advice.
Table 27 reproduces the classification of subjects presented in Table 10 and adds the rows that show
how many subjects observed at least one advice that mentions all four outcomes and how many
observed no advice that mentions all four outcomes.
In the text we present the evolution of the classification of types in advprob and advdet relative
to the main treatments for subjects who are eventually classified as VL VH (Figure 5), which is
reproduced in the top row of Figure 8. In addition, Figure 8 also shows the evolution for subjects
who are eventually classified as one of the other types.

E

Summary of Instructions

Below we present a summary of how we describe the problem in the instructions that subjects are
asked to read. Full instructions are available in the Instructions Appendix. Below, in upright letters
we present lines that are specific to prob. In italics and between brackets we show the corresponding
line for det. In bold we display lines that are common to both treatments.
• There is a company for sale. You can buy one company or none. The company’s value is
either A or B. [There are two companies for sale. You can buy two companies, one or none.
56

VL VH
{M ixVH , VH VH }

prob

Focal (VL X)
Focal ({M ixX, VH X})
Dom
Residual
Sum
VL VH
{M ixVH , VH VH }

det

Focal (VL X)
Focal ({M ixX, VH X})
Dom
Residual
Sum

None
2.1
9.0
2.1
1.1
17.6
8.5
40.4

All
13.8
5.3
4.3
1.6
1.1
0.5
26.6

1.6
8.2
1.6
0.6
9.8
7.7
29.5

33.9
3.8
2.7
1.1
3.9
7.1
52.5

Advice: Outcomes Mentioned
Large Gain Large Loss p = vL
1.1
1.6
1.1
5.3
4.8
0.0
0.0
3.7
1.1
2.7
0.5
0.0
2.1
0.5
0.5
1.6
2.1
0.5
12.8
13.3
3.2
0.6
2.7
0.0
0.5
1.6
0.5
6.0

3.8
0.0
0.0
0.0
0.0
0.0
3.8

Error
0.0
0.0
0.0
1.6
0.0
2.1
3.7

Sum
19.7
24.4
11.2
7.5
21.8
15.4
100.0

0.5
0.6
0.0
0.0
0.5
2.2
3.8

41.5
15.9
5.4
2.8
15.8
18.6
100.0

1.1
0.6
1.1
0.6
0.0
1.1
4.4

Table 26: Parts 1 and 2 (Type Classification) v. Part 3 (Advice Outcomes Categories)
Notes: 188 participants in prob and 183 in det. There are four outcomes (v, p) where v, p ∈ {vL , vH }. All (none)
are subjects who mentioned all (none of the) four outcomes in any form. Large Gain: Subjects who mention either
{(vL , vL ) and (vH , vH )}, {(vH , vH )}, {(vL , vH ) and explicitly (vH , vH )}, {(vL , vL ), (vH , vL ) and (vH , vH )}. Large
Loss: Subjects who mention either {(vL , vH )}, {(vL , vH ) and implicitly (vH , vH )}, {(vL , vL ), (vH , vL ) and (vL , vH )},
{(vL , vL ), (vL , vH ) and (vH , vH )}. p = vL : Subjects who mention outcomes: {(vL , vL )}, {(vL , vL ) and (vH , vL )}.
Mistake: Subjects who compute one of the payoffs wrongly. X ∈ {VL , M ix}.

VL , VH

{Mix, VH },VH

Focal

Dominated

Residual

Participants

prob

24.4

22.0

22.0

14.6

17.1

41

det

72.5

5.0

10.0

7.5

5.0

40

Observed at least one Advice

prob

27.6

27.6

13.8

6.9

24.1

29

that mentions all outcomes

det

71.8

5.1

10.3

7.7

5.1

39

No Advice

prob

16.7

8.3

41.7

33.3

0.0

12

mentions all outcomes

det

100.0

0.0

0.0

0.0

0.0

1

Selected Advice

prob

33.3

13.3

20.0

6.7

26.7

15

that mentions all outcomes

det

78.1

3.1

6.3

6.3

6.2

32

Did not Select Advice

prob

19.2

26.9

23.1

19.2

11.5

26

that mentions all outcomes

det

50.0

12.5

25.0

12.5

0.0

8

All

Table 27: Advisee Treatments: Parts 1 and 2 Type Classification [as % of participants]
Notes: Type are defined based on the prices p1 submitted in the last five rounds of part 1 and p2 submitted in all five rounds
of part 2. Type VL VH : p1 = vL and p2 = vH . Type {M ixVH , VH VH }: p1 ∈ {vL , vH } and at least one p1 = vH and p2 = vH .
Type ‘Focal’: p1 , p2 ∈ {vL , vH } and at least one p2 = vL (corresponds to VL VL , VL Mix , Mix VL , MixMix , VH VL , or VH Mix .
Type ‘Dom’: p1 , p2 ∈
/ {vL , vH }. Residual: All remaining subjects.

57

80
0

10

20

Percentage of subjects
30
40
50
60

70

80
70
Percentage of subjects
30
40
50
60
20
10
0

Main Treatment
Advisee Treatment
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

10

20

Percentage of subjects
30
40
50
60

70

80

(b) Deterministic: p = vL : n to 15 & VL VH .

0

0

10

20

Percentage of subjects
30
40
50
60

70

80

(a) Probabilistic: p = vL : n to 15 & VL VH .

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

10

20

Percentage of subjects
30
40
50
60

70

80

(d) Deterministic: p ∈ {vL , vH }: n to 15 &
{M ixVH , VH VH }

0

0

10

20

Percentage of subjects
30
40
50
60

70

80

(c) Probabilistic: p ∈ {vL , vH }: n to 15 &
{M ixVH , VH VH }

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

10

20

Percentage of subjects
30
40
50
60

70

80

(f) Deterministic: p ∈ {vL , vH }: n to 15 & Focal.

0

0

10

20

Percentage of subjects
30
40
50
60

70

80

(e) Probabilistic: p ∈ {vL , vH }: n to 15 & Focal.

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(g) Probabilistic: p ∈
/ {vL , vH }: n to 15 & Dominated.

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Round

(h) Deterministic: p ∈
/ {vL , vH }: n to 15 & Dominated.

Figure 8: Evolution of Types: Main Treatments and Advisee Treatments.
58

The first company’s value is A. The second company’s value is B.] A and B represent two
numbers. The numbers change from round to round.
• In each round you will learn the value A and B that the company may have. With equal chance
the company’s value is A or B. In each round, the interface is programmed to toss a coin and
assign value A if heads comes up and value B if tails comes up. You will not know which of
the two values is selected. [In each round, you will learn the value A of the first company and
the value B of the second company.]
• You will submit one price. You can submit any price from 0 to 150. This is the
price that you are willing to pay for the company [a company].
• You do not know if the company for sale is of value A or of value B. [You do know that the
two companies for sale are the first company of value A and the second company of value B.]
• Transaction for the [each] company:
– If the price you submit is higher than or equal to the value of the company,
you buy the company.
– If you buy, you increase the company’s value by 50%. This means that if you
buy the company of value A [the first company of value A], the value to you is 1.5 × A.
If you buy the company of value B [the second company of value B ], the value to you
is 1.5 × B.
– If you buy the company, your profit is: 2 times (1.5 x value of the company - price). [For
each company you buy, your profit is: 1.5 x value of the company - price.]
– [For each company,] If the price you submit is lower than the value of the company, you don’t buy the company and your profit is zero.
• [The price you submit is the same for both transactions. You can make money from both
transactions.]
• Your payoff for the round is the profit from the transaction. [Your payoff for the round is
the sum of the profit from the two transactions, the transaction of the first company and the
transaction of the second company.]
For screenshots of the round 1 part 1 problem with vL = 20 and vH = 120 in prob and det, see
Figure 9.

59

(a) prob

(b) det

Figure 9: Screenshots of Part 1 Round 1

60

F

Examples of Advice

After the examples, we provide comments as to why the advice was classified in the corresponding
category, in cases where it is not obvious. The advices mentioned below are verbatim, and hence
they may contain mistakes or grammatical errors. To make recommendations, subjects use the
terminology used in the instructions. For example, they refer to a company of low value as company
A and to a company of high value as company B.

prob
• Computes both payoffs explicitly: “You should not go with an offer in the middle of the two
values, as going above 20 reduces your profits if A is selected but also does not help your chances
of purchasing B if that is selected. So the two options are bid 20, if A is selected you make a
profit of 20, if B is selected you make 0, making an average value of 10. If you bid 120, if A is
selected your profit is -180, is B is selected your profit is 120, making an average value of -30.
Statistically based on the law of averages you should always bid the value of the lower option”.

• Payoffs mentioned qualitatively: “I would submit 20. Worst case you would sit at 0 profit.
Best case A is selected a you would make profit. I don’t think submitting 120 is worth the
risk.” Comments: Outcome (v, p) = (vL , vH ) mentioned (Worst case you would sit at 0 profit);
Outcome (v, p) = (vL , vL ) qualitatively mentioned (Best case A is selected a you would make
profit). Outcomes (vH , vL ) and (vH , vH ) qualitatively mentioned when the subject states: ‘I
don’t think submitting 120 is worth the risk.’ We code this recommendation as mentioning
the outcomes for the following reason. The phrase ‘worth the risk’ indicates that there is a
potential gain, but that the risk is too high for it to be worth it.
• Mentions outcomes, but does not compute Expected Payoffs: “I would advise you to bid 20 for
the company. This would decrease your risk to "0" but your possible profit to "20". If you bid
120 for the company, you could profit 120 but if the company selected is "A" then your would
lose 180. There is no risk with bidding 20 since if "B" is selected your profit is "0". So bidding
20 is the selection with no chance of losing money.”
• Advice mentions large losses “You should enter the price that is equal to value A, the lower
value. If you pick a higher price to get the higher value, if the lower value is selected instead
you could lose money.”
• Advice mentions large gains “I think you should always go with the possible higher amount of
B since it will result in more of a profit for you if it happens to be the selected company.”
• Advice mentions p = vL outcomes: “I would be willing to pay $20. In my opinion that is the
safest amount. If you are lucky and the value is A, you make money, if the value is B, you
loose nothing.”
61

• Mentions no outcome: “70, provides a good amount.”
• Makes some mistake: “You will make a lot more money if B is chosen but there is still a chance
A will be chosen. If you choose 20 you will definitely make some money. But if you choose
a number higher than 20 is A is chosen then you cannot buy the property and your profit is
zero. If you are a risk taker, choose a value higher to or equal to 120. I hope that helps you.”
Comment: We do not code an advice as including a mistake if the subject is conceptually
correct, but he/she makes a mistake in arithmetic. We only count as mistakes when there is
some conceptual mistake in the advice. In the case of the example recall that if p ≥ vL , then a
company of value v = vL is purchased. The following sentence is conceptually incorrect: ‘But
if you choose a number higher than 20 is A is chosen then you cannot buy the property and
your profit is zero.’

det
• Computes both payoffs explicitly: “My only advice is to do the math for each company purchase.
To purchase both companies you are looking at now will cost 120 for EACH at a minimum.
While the value of company B will be 180 to you, the final value after your purchase cost will
be 60. Seems like an OK prospect until you factor in the loss you will take on the purchase of
company A. You will spend 120 to purchase to company A which only has a value of 30 to you
when you purchase it. Subtract the purchase cost of 120 and you are left with -90. Add -90
(loss from the purchase of company A) and 60 (profit from the purchase of company B) and
you are left with a total loss of -30 for the transaction overall. I would submit no more than
20 for an offer and only purchase company A which would represent a profit of 10 .”
• Computes one payoff explicitly: “To buy you offer 120 for each. Making the profit of A: 30 120 = -90. And The profit of B: 180 -120 = 60. The combined profit of A: -90 + B: 60 = -30.
Creating a loss of -30 on the total deal. It is better to only purchase A. The 120 cost of A when
purchasing both creates a loss.” Comment: Outcomes corresponding to v = vH are explicitly
mentioned and computed. Consider the phrase: ‘It is better to only purchase A.’ With this
phrase the subject implicitly mentions that there is no gain if the company is that of high
value (‘only purchase A’). At the same time, by mentioning that it is better to purchase A the
subject is implicitly recognizing that there is a positive payoff from buying just the low value
company (A). We code this as the subject qualitatively (or implicitly) mentioning outcomes
related to v = vL .
• Payoffs mentioned qualitatively: “Only Bid 20. This way you will purchase company A and
make a small profit in the end. When there is a large difference between the values of company
A and company B, only buy company A. If you spend 120 and buy company B as well, then the
amount you will lose from the purchase from company A having spent 120 for it, will offset any
profits that you make from the purchase of company B, putting you in a negative profit which
62

means you lose money.” Comment: Outcomes related to p = vL are implicitly mentioned in
this sentence: ‘This way you will purchase company A and make a small profit in the end.’ The
subject implicitly recognizes that bidding 20 will not buy both companies and qualitatively
mentions that there is a profit from buying the low value company. Outcomes related to
p = vH are also qualitatively mentioned in the last sentence of the advice.
• Advice mentions large gains: “Buy company B profit will be 180.”
• Advice mentions large losses: “Only buy A, you will lose money if buy both.”
• Advice mentions p = vL outcomes: “buy the first one at 20 for a profit of 10.”
• Mentions no outcome: “Do the math for your equations to ensure to get a good result.”
• Makes some mistake: “I would submit a price of 120. With this price, there is a direct profit
since the value will increase by 50%. If you own both, the sum of 140 x 1.5 - 120 would result
in a net profit of 90 and makes this a profitable purchase. Even if you only purchase B you
would still make a profit of 60 (120 x 1.5 - 120 = 60). ” Comment: We do not code an advice
as including a mistake if the subject is conceptually correct, but he/she makes a mistake in
arithmetic. One mistake in this advice is in the last sentence. It is not possible to offer a price
that only purchases the high-value company.

63

