{
    "id": 30981,
    "citation_title": "Automating Automaticity: How the Context of Human Choice Affects the Extent of Algorithmic Bias",
    "citation_author": [
        "Amanda Y. Agan",
        "Diag Davenport",
        "Jens Ludwig",
        "Sendhil Mullainathan"
    ],
    "citation_publication_date": "2023-02-20",
    "issue_date": "2023-02-16",
    "revision_date": "None",
    "topics": [
        "\n",
        "Other",
        "\n",
        "General, Teaching",
        "\n",
        "Microeconomics",
        "\n",
        "Welfare and Collective Choice",
        "\n",
        "Economics of Information",
        "\n"
    ],
    "program": [
        "\n",
        "Children and Families",
        "\n",
        "Economics of Health",
        "\n",
        "Law and Economics",
        "\n",
        "Labor Studies",
        "\n",
        "Public Economics",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nConsumer choices are increasingly mediated by algorithms, which use data on those past choices to infer consumer preferences and then curate future choice sets. Behavioral economics suggests one reason these algorithms so often fail: choices can systematically deviate from preferences. For example, research shows that prejudice can arise not just from preferences and beliefs, but also from the context in which people choose. When people behave automatically, biases creep in; snap decisions are typically more prejudiced than slow, deliberate ones, and can lead to behaviors that users themselves do not consciously want or intend. As a result, algorithms trained on automatic behaviors can misunderstand the prejudice of users: the more automatic the behavior, the greater the error. We empirically test these ideas in a lab experiment, and find that more automatic behavior does indeed seem to lead to more biased algorithms. We then explore the large-scale consequences of this idea by carrying out algorithmic audits of Facebook in its two biggest markets, the US and India, focusing on two algorithms that differ in how users engage with them: News Feed (people interact with friends' posts fairly automatically) and People You May Know (people choose friends fairly deliberately). We find significant out-group bias in the News Feed algorithm (e.g., whites are less likely to be shown Black friends' posts, and Muslims less likely to be shown Hindu friends' posts), but no detectable bias in the PYMK algorithm. Together, these results suggest a need to rethink how large-scale algorithms use data on human behavior, especially in online contexts where so much of the measured behavior might be quite automatic.\n\n",
    "acknowledgement": "\nFor helpful comments we thank Hunt Alcott, Amanda Coston, Josh Dean, Jonathan Guryan, Reid Hastie, Sam Hirshman, Alex Imas, Erika Kirgios, Jon Kleinberg, Alex Koch, Betsy Levy Paluck, Emma Pierson, Devin Pope, Emma Rackstraw, Manish Raghavan, Ashesh Rambachan, Evan Rose, Jon Roth, Avner Strulov Shlain, Cass Sunstein, Richard Thaler, Alex Todorov, Stefan Uddenberg, Oleg Urminsky, Bernd Wittenbrink, George Wu, attendees at the annual meetings of the American Economic Association and the Society for Judgment and Decision Making, and seminar participants at the University of Chicago and the AI for Behavior Change workshop at the 2022 meetings of the Association for the Advancement of Artificial Intelligence. We are particularly grateful to the Center for Applied Artificial Intelligence at the University of Chicago Booth School of Business for funding. For phenomenal programming assistance we thank Khoa Nguyen and Mani Yatam. For research support we thank Becky White, Bryan Baird, Amy Boonstra, and the team of RAs at CDR, Tirtha Patel, Pavan Mamidi, the team of RAs at CSBC at Ashoka University, and the Harvard Decision Science Lab. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n\nJens Ludwig\n\nI attest that:\n\n\n1.\tAll sources of financial support for the research described in our paper are disclosed as part of the acknowledgement section of the paper itself\n2.\tOver the past three years I have received at least $10,000 in support from two organizations that might be viewed as having some stake in the results of this paper - the U.S. Department of Housing and Urban Development, or HUD (the sponsor of the Moving to Opportunity experiment that we study in our paper), and the MacArthur Foundation (which is heavily invested in efforts to improve housing conditions for low-income families as a way to improve their well-being).\n3.\tI have no other paid or unpaid positions as an officer, director, or board member of any other relevant non-profit organization or profit-making entity.\n4.\tI have no close relatives who have received support or serve in paid or unpaid positions from any interested party or relevant organization.\n5.\tHUD is the only organization that had the right to review our paper prior to circulation HUD's review was only to ensure that the paper did not disclose any confidential information about participants in the MTO experiment.\n6.\tAs indicated in the acknowledgment section of our paper, the research reported on in our study was reviewed and approved by the IRBs at HUD, the NBER, the University of Michigan, Northwestern University, and the University of Chicago.\n\n\nSincerely,\n\n\nJens Ludwig\nMcCormick Foundation Professor of Social Service Administration, Law, & Public Policy\nUniversity of Chicago\nResearch Associate, National Bureau of Economic Research\n\n\n"
}