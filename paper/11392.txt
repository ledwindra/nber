OPTIMAL POLICY PROJECTIONS
Lars E. O. Svensson
Robert J. Tetlow
Working Paper 11392

NBER WORKING PAPER SERIES

OPTIMAL POLICY PROJECTIONS
Lars E. O. Svensson
Robert J. Tetlow
Working Paper 11392
http://www.nber.org/papers/w11392
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
June 2005

We thank Brian Madigan, Dave Reifschneider, John Roberts, Dave Small, Peter Tinsley, and participants
in a seminar at the Federal Reserve Bank of New York for helpful suggestions, and Brian Ironside for
research assistance. All remaining errors are ours. The views expressed in this paper are solely the
responsibility of the authors and should not be interpreted as reflecting the views of the Board of Governors
of the Federal Reserve System or the views of any other person associated with the Federal Reserve System.
The views expressed herein are those of the author(s) and do not necessarily reflect the views of the National
Bureau of Economic Research.
©2005 by Lars E. O. Svensson and Robert J. Tetlow. All rights reserved. Short sections of text, not to
exceed two paragraphs, may be quoted without explicit permission provided that full credit, including ©
notice, is given to the source.

Optimal Policy Projections
Lars E. O. Svensson and Robert J. Tetlow
NBER Working Paper No. 11392
June 2005
JEL No. E52, E58
ABSTRACT
We outline a method to provide advice on optimal monetary policy while taking policymakers'
judgment into account. The method constructs Optimal Policy Projections (OPPs) by extracting the
judgment terms that allow a model, such as the Federal Reserve Board's FRB/US model, to
reproduce a forecast, such as the Greenbook forecast. Given an intertemporal loss function that
represents monetary policy objectives, OPPs are the projections - of target variables, instruments,
and other variables of interest -that minimize that loss function for given judgment terms. The
method is illustrated by revisiting the Greenbook forecasts of February 1997 and November 1999,
in each case using the vintage of the FRB/US model that was in place at that time. These two
particular forecasts were chosen, in part, because they were at the beginning and the peak,
respectively, of the late 1990s boom period. As such, they differ markedly in their implied
judgments of the state of the world, and our OPPs illustrate this difference. For a conventional loss
function, our OPPs provide significantly better performance than Taylor-rule simulations.
Lars O. Svensson
Department of Economics
Princeton University
Princeton, NJ 08544-1021
and NBER
svensson@princeton.edu
Robert J. Tetlow
Division of Research and Statistics
Board of Governors of the Federal Reserve System
Washington, DC 20551
robert.j.tetlow@frb.gov

Policy... depends on forecasts of probabilities developed from large macromodels, numerous submodels, and judgments based on less mathematically precise regimens. Such
judgments, by their nature, are based on bits and pieces of history that cannot formally
be associated with an analysis of variance. Yet there is information in those bits and
pieces. (Greenspan [12, p. 39]).

1. Introduction
There has long been a gulf between advice on monetary policy conduct, as gleaned from the academic literature, and the practice of monetary policy, as captured, for example, in the historical
record of the U.S. Federal Reserve System. Academic treatments of monetary policy have tended
to stress commitment to fixed monetary policy rules and the forbearance of discretion. To many
academics, the conduct of monetary policy is, or should be, a largely mechanical exercise. Descriptions of the practice of monetary policy, on the other hand, have focussed on the accumulation
of experience by policymakers and the application of judgment based on that experience, as the
above quotation from the Chairman of the Federal Reserve Board attests. Meaningful dialogue
between the two camps has been hindered in the past by the absence of a structure within which
the application of judgment can be applied.
The Fed staﬀ recently began reporting to the Federal Reserve Board and the Federal Open
Market Committee (FOMC) optimal policy projections (although they were not referred to by
that name).1 The method of Optimal Policy Projections (OPPs) is a method to present options
on optimal monetary policy while taking into account the judgment of policymakers or, as in the
case of the Federal Reserve Board, that of the staﬀ. It was implemented in June 2001 by Robert
Tetlow using a mostly backward-looking variant of the Federal Reserve Board’s FRB/US model,
although explorations of methods of this nature using other large-scale models at the Board go
back to the 1970s.

2

The procedure has subsequently been extended to versions of the FRB/US

model incorporating rational expectations in asset pricing. This paper explains OPPs in terms of a
generalization of the linear-quadratic model of optimal policy with judgment and forward-looking
variables laid out in Svensson [16] and [17]. It also demonstrates the feasibility of using OPPs
to help inform policy making under the real-world conditions faced by the Federal Reserve. We
1
The staﬀ of the Federal Reserve Board prepare for members of the FOMC an oﬃcial Greenbook forecast (with
a green cover) for each of the eight FOMC meetings per year. It also contains analysis of recent incoming data, an
assessment of the state of the economy, and some alternative scenarios. Alongside the Greenbook, FOMC members
receive the Bluebook (with a blue cover), which adds some analysis of financial and money market conditions and
detailed policy alternatives based in large part on the Greenbook forecast. The Greenbooks and Bluebooks of the
most recent 5 years are kept confidential by the Federal Reserve. In this paper, we will be using Greenbook baselines
from prior to the five-year window to demonstrate the eﬃcacy of OPPs.
2
The early Federal Reserve work was pioneered by Peter Tinsley. See, in particular, Kalchbrenner and Tinsley [13].

1

examine policy options in early 1997 when the U.S. economy appeared to be reaching capacity. We
do this using two vintages of the FRB/US model and two Greenbook baselines: the February 1997
Greenbook, when the state of the world was unclear, and again with the November 1999 vintage
and database, when there was a bit more clarity with the benefit of hindsight.3 Unbeknownst to
the Fed at the time, the economy in 1997 was in the early stages of a productivity boom, a fact
that was evident by 1999. Examining these two baselines and the models that were used at the
time allows us to isolate the influence of judgment on the OPP.
The remainder of the paper proceeds as follows. Following this introduction, section 2 lays out
the method of OPPs. Section 3 provides a real-world example. Section 4 oﬀers some concluding
remarks.

2. The method of Optimal Policy Projections
2.1. A model of the policy problem with judgment
The method of OPPs is for simplicity illustrated in a linear model (the FRB/US is a near-linear
model). Consider the following linear model of an economy, in a form that includes a role of
judgment and allows for both backward- and forward-looking elements,
¸
∙
¸
¸
∙
∙
Xt+1
Xt
z1,t+1
=A
+ Bit +
.
Cxt+1|t
xt
z2,t

(2.1)

Here, Xt is an nX -vector of predetermined variables in period t; xt is an nx -vector of forward-looking
variables; it is an ni -vector of instruments (the forward-looking variables and the instruments are
the nonpredetermined variables); z1t and z2t are exogenous nX - and nx -vector stochastic processes,
0 , z 0 )0 is called the deviation in period t (0 denotes the transpose); A, B, and
respectively; zt ≡ (z1t
2t

C are matrices of the appropriate dimension; and yt+τ |t denotes Et yt+τ for any variable yt , the
rational expectation of yt+τ conditional on information available in period t. The variables can
be measured as diﬀerence from steady-state values, in which case their unconditional means are
zero. Alternatively, one of the components of Xt can be unity, so as to allow the variables to have
nonzero means.
The standard case of this problem is when z2,t ≡ 0 and z1,t is a vector of iid zero-mean shocks.
The new element here is that zt , the deviation, is an arbitrary exogenous stochastic process. As
discussed in more detail in Svensson [17], the deviation represents the diﬀerence between the model
3
Under the Federal Reserve’s information-security rules, the November 1999 Greenbook was the most recent one
that was available to the public at the time section 3 of this paper was prepared. The February 1997 forecast is
extended beyond the regular Greenbook range in a manner to be described.

2

outcomes and the actual realizations of data and includes all extra-model explanations of the actual
data. Below, the central bank’s judgment will be represented as the central bank’s projections of the
future deviations. This allows us to incorporate the fact that a considerable amount of judgment is
always applied to assumptions and projections. Projections and monetary-policy decisions cannot
rely on models and simple observable data alone. All models are drastic simplifications of the
economy, and data give a very imperfect view of the state of the economy. Therefore, judgmental
adjustments in both the use of models and the interpretation of their results–adjustments due to
information, knowledge, and views outside the scope of any particular model–are a necessary and
essential component in modern monetary policy. The only restriction we shall impose below on the
stochastic process of the deviation is that the expected deviation is constant (and, without loss of
generality, zero) beyond a particular horizon.
The upper block of (2.1) provides nX equations determining the nX -vector Xt+1 in period t + 1
for given Xt , xt , it and z1,t+1 ,
Xt+1 = A11 Xt + A12 xt + B1 it + z1,t+1 ,
where A and B are decomposed according to
¸
∙
A11 A12
,
A≡
A21 A22

B=

∙

B1
B2

¸

.

(2.2)

The lower block provides nx equations determining xt in period t for given xt+1|t , Xt , it , and z2t ,
xt = A−1
22 (Cxt+1|t − A21 Xt − B2 it − z2t );
we assume that the nx × nx submatrix A22 is invertible.
Let the Yt be an nY -vector of target variables, measured as the diﬀerence from an nY -vector
Y ∗ of target levels. This is not restrictive, as long as we keep the target levels time invariant.4 If
we would like to examine the consequences of diﬀerent target levels, we can instead interpret Yt as
the absolute level of the target levels and replace Yt by Yt − Y ∗ everywhere below. Assume that
the target variables can be written as a linear function of the predetermined, forward-looking, and
instrument variables,

4

This restriction can be easily relaxed.

⎤
Xt
Yt = D ⎣ xt ⎦ ,
it
⎡

3

(2.3)

where D is an nY × (nX + nx + ni ) matrix. Let the intertemporal loss function in period t be
Et

∞
X

0
δ τ Yt+τ
W Yt+τ ,

(2.4)

τ =0

where 0 < δ < 1 is a discount factor and W is a symmetric positive semidefinite matrix.
Let y t ≡ {yt+τ ,t }∞
τ =0 denote a central-bank projection in period t for any variable yt , a centralbank mean forecast conditional on central-bank information in period t. As mentioned above, the
t
projection of the deviation, z t ≡ {zt+τ ,t }∞
τ =0 , is identified with judgment. For given judgment, z ,

let the projection model of the central bank for the projections (X t , xt , it , Y t ) in period t be
∙
¸
∙
¸
¸
∙
Xt+τ +1,t
Xt+τ ,t
z1,t+τ +1,t
=A
+ Bit+τ ,t +
,
(2.5)
Cxt+τ +1,t
xt+τ ,t
z2,t+τ ,t
⎡
⎤
Xt+τ ,t
Yt+τ ,t = D ⎣ xt+τ ,t ⎦
(2.6)
it+τ ,t
for τ ≥ 0, where

Xt,t = Xt

(2.7)

and Xt is given.
The policy problem in period t is to find the optimal projection (X̂ t , x̂t , ı̂t , Ŷ t ), that is, the
projection that minimizes the intertemporal loss function,
∞
X

δ τ Lt+τ ,t ,

(2.8)

τ =0

where the period loss, Lt+τ ,t , is specified as
Lt+τ ,t = Yt+τ ,t 0 W Yt+τ ,t .

(2.9)

The minimization is subject to given Xt , z t , and (2.5) for τ ≥ 0. For the policy problem in terms
of projections, we can allow 0 < δ ≤ 1, since the above infinite sum will normally converge also for
δ = 1. The optimization is done under commitment in a “timeless perspective;” however, we do
not discuss the details of how the timeless perspective shall be implemented but refer to Svensson
[17] for that.
Ideally, the implementation of the optimal policy in period t would involve announcing the
optimal policy projection, conditional on the judgments of the monetary authority, and setting the
instrument in period t equal to the first element of the instrument projection,
it = ı̂t,t .
4

Announcing the policy would serve to direct the expectations of a possible skeptical public toward
the goals of policy and over time provide a framework for the central bank to discuss the evolution
of its views. In period t + 1, a new optimal policy projection, (X̂ t+1 , x̂t+1 , ı̂t+1 , Ŷ t+1 ), is derived,
conditional, once again, on Xt+1 and z t+1 , and announced together with a new instrument setting,
it+1 = ı̂t+1,t+1 ,
and so on.
2.2. Extracting judgment
Consider a given reference projection (X̃ t , x̃t , ı̃t ), a projection (X̃ t , x̃t ) conditional on ı̃t . This could
be, for instance, a largely judgmental forecast for all relevant variables (X̃ t , x̃t ), conditional on
a particular federal-funds-rate projection ı̃t , or it could be a model-based projection. Define the
corresponding judgment, z̃ t , as the projection (the projection of the future deviations) z̃ t that
fulfills

∙

X̃t+τ +1,t
C x̃t+τ +1,t

¸

=A

∙

X̃t+τ ,t
x̃t+τ ,t

¸

+ Bı̃t+τ ,t +

∙

z̃1,t+τ +1,t
z̃2,t+τ ,t

¸

(2.10)

for τ ≥ 0. This is the judgment that makes the projection model reproduce the reference projection.
What “judgment” represents depends on the context. In purely model-based forecasting, judgment
is the extra-model information that the central bank brings to bear on the forecast. Judgment
obviously depends on the model and on the reference projection. The method of OPPs assumes
that the dynamics of the economy are adequately represented by the coeﬃcients of the matrices A,
B, and C, and that the relevant diﬀerence between the model and the economy can be adequately
captured by the judgment.5 However, this is not as restrictive as it might seem on the surface.
The method of OPPs involves overlaying a “policy round” on top of a baseline forecast, however
produced. Two assumptions are required to make this operational: first, that those aspects of (2.5)
that pertain to the monetary-policy transmission mechanism are consistent with the forecasters’
views; and second, that the judgment itself, z t , can be taken as exogenous with respect to policy
actions. The first of these assumptions means that, in principle, the forecast could be carried out
using a “model” that is very diﬀerent from the model with which the OPP is to be conducted. The
5
An example might be the adjustments of forecasts done in 1999 due to the Y2K phenomenon at the century’s
end. Y2K is a particularly clean-cut example because it was seemingly important, but since it had never happened
before, no model could be expected to encompass it. Notice that judgment can be attributed to either the structural
equations in the upper block of the system, or to beliefs, or expectations, conditional on structure, as is the case in
the lower block. Continuing with our Y2K example, there are the perfect-foresight implications of a “destruction” of
a part of the capital stock that Y2K represented; plus there are the implications of people’s beliefs of a shock that
no one had experienced before.

5

second is a mild restriction, the prospective violation of which would require the modelers to add
the relevant equations to accommodate the case. For example, forecasters could include judgment
about the existence of a stockmarket bubble but could not capture an assumed direct eﬀect of
policy on bursting the bubble without first adding equations to the model to capture this eﬀect.
This specification assumes that the dimensions of the predetermined variables, nonpredetermined variables, and instruments are not less than the corresponding dimensions of the reference
projection. If the dimension of the model is larger than the dimension of the reference projection,
the judgment will not be unique. Since the FRB/US model is near-linear, the particular judgment
chosen will not be of first-order importance for our results; the OPPs can be carried out with any
fixed judgment. Alternatively, the judgment can be chosen so as to minimize the norm (a measure
of the size) of the judgment, for instance. If the dimension of the model is smaller than that of the
reference projection, then it is possible that the model is insuﬃcient to capture all the details of
the forecast.
In the case of the FRB/US model, there are elements of the Greenbook forecast that do not have
direct analogues in the FRB/US model. For example, aircraft production and automobile sales to
consumers are forecast in the staﬀ’s judgmental forecast but do not appear directly in the model;
instead, the model judgment necessary to replicate these are contained within the investment and
consumer durables equations respectively. However, so long as the interest elasticities of these
equations are representative of the beliefs of the policymaker, the OPP experiment will also be
representative.
If the reference forecast includes all the variables in the projection model, the judgment z̃ t is
unique, since z̃1,t+τ +1,t is given by the residuals of the upper block of (2.5),
z̃1,t+τ +1,t = X̃t+τ +1,t − A11 X̃t+τ ,t − A12 x̃t+τ ,t − B1 ı̃t+τ ,t
for τ ≥ 0, and z̃2,t+τ ,t is given by the residuals of the lower block,
z̃2,t+τ ,t = C x̃t+τ +1,t − A21 X̃t+τ ,t − A22 x̃t+τ ,t − B2 ı̃t+τ ,t
for τ ≥ 0.
2.3. A finite-horizon approximation
It is convenient to use a finite-horizon approximation to the above infinite-horizon projection model.
The implementation below with the FRB/US model will also use a finite-horizon approximation.
6

As explained in detail in Svensson [17], under suitable assumptions, there is a convenient finitehorizon approximation of this projection model, an approximation that can be made arbitrarily
accurate by extending the horizon T . The first assumption is that the judgment is constant and,
without loss of generality, zero beyond some horizon T ,
zt+τ ,t = 0

(τ ≥ T ).

(2.11)

The second assumption is that the optimal projection asymptotically approaches a steady state.
Assuming that the optimal projection reaches the steady state in finite time is then an approximation that is arbitrarily accurate if the horizon is suﬃciently long. Svensson [17] also notes that
alternative assumptions can make the finite-horizon projection model exact, also for relatively short
horizons.
Let the (nX + nx + ni )-vector st = (Xt0 , x0t , i0t )0 denote the state of the economy in period t,
and let st+τ ,t denote a projection in period t of the state of the economy in period t + τ . Let
st , the projection of the (current and future) states of the economy, denote the finite-dimensional
(T + 1) × (nX + nx + ni )-vector st ≡ (s0t,t , s0t+1,t , ..., s0t+T,t )0 . Similarly, let all projections y t for
0 , y0
0
0
y = X, x, i, and Y now denote the finite-dimensional vector yt ≡ (yt,t
t+1,t , ..., yt+T,t ) . Svensson

[17] shows that the finite-horizon projection model can be written compactly as
Gst = g t ,

(2.12)

where G is a (T + 1)(nX + nx ) × (T + 1)(nX + nx + ni ) matrix, and g t is a (T + 1)(nX + nx )-vector
0
0
0
0
0
0
0
0
, z2,t,t
, z1,t+2,t
, z2,t+1,t
, ..., z1,t+T,t
, z2,t+T
defined as g t ≡ (Xt , z1,t+1,t
−1,t , z2,t+T,t ) . Here, Xt denotes

the given vector of predetermined variables in period t.
0 ,Y 0
0
0
Since Y t now denotes the finite-dimensional (T + 1)nY -vector Y t ≡ (Yt,t
t+1,t , ..., Yt+T,t ) , we

can write
Y t = D̃st ,

(2.13)

where D̃ denotes a finite-dimensional (T + 1)nY × (T + 1)(nX + nx + ni ) block-diagonal matrix
with the matrix D in each diagonal block.
The intertemporal loss function can be written as a function of st as the finite-dimensional
quadratic form
1 t0 t
s Ωs ,
2

(2.14)

where Ω is a symmetric positive semidefinite block-diagonal (T + 1)(nX + nx + ni ) matrix with its

7

(τ + 1)-th diagonal block being δ τ D0 W D for 0 ≤ τ ≤ T .6
Then, the policy problem is to find the optimal policy projection ŝt that minimizes (2.14) subject
to (2.12). The Lagrangian for this problem is
1 t0 t
s Ωs + Λt 0 (Gst − g t ),
2

(2.15)

where Λt is the (T + 1)(nX + nx )-vector of Lagrange multipliers of (2.12). The first-order condition
is
st 0 Ω + Λt 0 G = 0.
Combining this with (2.12) gives the linear equation system
∙
¸∙ t ¸ ∙ t ¸
G 0
s
g
.
=
Ω G0
Λt
0
The solution to this linear system gives the optimal policy projection ŝt , which in turn determines
the optimal policy projection of the target variables, Ŷ t ≡ D̃ŝt . In particular, the method of OPPs
amounts to finding a whole projection path for the instrument and doing so in one step, as opposed
to deriving an instrument rule.7
A finite-dimensional projection model has several advantages beyond ease of computation. One
is that it is very easy to incorporate any restrictions on the projections. Any equality restriction
on X t , xt , it , or Y t can be written
Rst = s̄t ,

(2.16)

where the number of rows of the matrix R and the dimension of the given vector s̄t equal the number
of restrictions. This makes it easy to incorporate any restriction on the instrument projection, for
instance, that it shall be constant or of a particular shape for some periods. Then it is possible to
compute restricted OPPs, OPPs that are subject to some restrictions, for particular purposes.

3. A real-world demonstration of OPPs
This section provides a real-world demonstation of OPPs, using the FRB/US model.
6

Svensson [17] shows how this loss function shall be modified to incorporate commitment in a timeless perspective;
we abstract from these issues here.
7
Robustness can addressed by looking at “distorted judgment” in a way suggested by, for instance, Hansen and
Sargent [10] and [11] and Tetlow and von zur Muehlen [22], through the addition of worst-case judgment to the
baseline forecast and optimizing conditional on that judgment. One would, of course, consider such a scenario as
part of a suite of scenarios, including the OPP for the best-guess forecast, not as a replacement for the best-guess
forecast.

8

3.1. The world in early 1997
We use the economy in early 1997 as our backdrop. To illustrate the importance of judgment, we
use two diﬀerent views of the state of the economy at that time. The first is the contemporaneous
view from the February 1997 Greenbook forecast and the FRB/US model of that time.8 The
second, is the “backcast” of this period as seen from the November 1999 Greenbook.
The contemporaneous forecast of February 1997 was selected for a number of reasons. First, in
the view of the Federal Reserve Board’s staﬀ, the economy was straining at capacity constraints.
According to the Greenbook (Federal Reserve Board [7, Part 1, p. I-2]):
Labor markets, of course, are already tight, and the latest statistics have confirmed the
uptilt in compensation increases last year. With the unemployment rate projected to
edge down to 5 percent and with the minimum wage jumping again later this year, we
see labor cost inflation continuing to escalate... [O]ur forecast has edged further in the
direction of a more cyclical pattern of inflationary overshooting, which typically has
been followed by a period of weakness.
Real GDP growth in 1996 was measured at a bit over 3 percent per year, well above most
estimates of the growth rate of potential output.9 The unemployment rate, which had started 1996
at 5.6 percent, finished it at 5.3 percent, below most estimates of the NAIRU. Meanwhile, growth
in personal consumption expenditures (PCE) prices was climbing, to 2.5 percent for the twelve
months ending December 1996, up from 2.1 percent a year earlier. Not surprisingly, then, the staﬀ
saw unsustainable growth, given a constant federal funds rate, over the projection period ending in
1998:Q4. Table 1 below summarizes the emerging data of that time and the forecast.
Table 1. February 1997 Greenbook Forecast1
1995 1996 1997 1998
Real GDP
3.3
3.1
2.3
2.1
5.6
5.3
5.1
5.0
Unemployment rate2
Non-farm business productivity −0.1
0.8
0.8
0.8
PCE inflation
2.1
2.5
2.2
2.8
2.6
3.1
3.5
3.7
Employment cost index3
1. 4-quarter or 12-month growth as applicable, except as noted.
2. Monthly average value in the final quarter of the year shown.
3. Private-industry workers, December-December.
8
We follow the convention internal to the Fed of dating the forecast as of the date of the FOMC meeting. The
Greenbook document corresponding to the February decision–Federal Reserve Board [7]–was actually completed
in late January.
9
For example, the Congressional Budget Oﬃce was projecting potential output growth in 1996 of about 2-1/4
percent, measured on a GDP basis (see CBO [4]). See Orphanides and Williams [15] for a detailed examination of
retrospective and real-time estimates of NAIRU measures.

9

The projected wage-price spiral is evident in the sharp acceleration in wage inflation (the employment cost index). And while productivity growth had increased recently, it had only climbed
from dismal rates in 1995 to modest rates in 1996; similar modest rates were projected into the
future. The warning in the Greenbook’s statement that tighter monetary policy was likely to be
necessary was also reflected in the Bluebook, the Fed staﬀ’s main document for analyzing monetary
policy options for the FOMC (Federal Reserve Board [8, pp. 6-7]):
By the end of the Greenbook forecast, the disequilibrium in policy and in the economy
has become quite evident–the economy is producing beyond its sustainable potential
and the stance of monetary policy is too easy to correct the situation and forestall a
continuous rise in core inflation.
The second reason for selecting the February 1997 forecast is that the judgment contained
therein would turn out to be wrong: unbeknownst to the staﬀ, a productivity boom was underway
in the United States that would obviate the need for a tightening of monetary policy, at least for
a while.

The staﬀ and the Committee were aware that productivity had been usually high in

1996, but the staﬀ took the recent data to have been a temporary phenomena.10 Over the next
year, the persistence of productivity growth became evident, and the staﬀ consequently revised
its forecast. Accordingly, the view expressed in the November 1999 Greenbook was quite diﬀerent
(Federal Reserve Board [9, Part 1, p. I-1]):
The key changes in our forecast relate to a revised outlook for labor productivity... [T]he
combination of revisions to the NIPA [National Income and Product Accounts] and a
reassessment of the contribution to potential output from growth of the capital stock
has led us to raise our estimate of trend growth in recent years and... in the period
ahead.
The upward revision to the estimates of past and projected trend growth meant substantially
less incipient inflation pressures than had previously been anticipated. For our purposes, this sets
up an interesting contrast of what the policy prescription would have been in real time with what
it would have been in retrospect, nearly three years later.
3.2. The Greenbook extension
Using the February 1997 Greenbook provides a third advantage: it was the first Greenbook that was
extended beyond its normal forecast period to provide a baseline for policy analysis experiments in
10

Still, there was enough evidence of something going on that the staﬀ included some alternative scenarios in the
Bluebook to illustrate the possibility that higher productivity growth was might persist.

10

the Bluebook. (The extension procedure would become routine somewhat later.) And while the
extension of that time was not stored electronically, the Bluebook document oﬀers guidance on how
to reconstruct the original extension. We do this for this paper. Reproducing the extension, in turn,
has two benefits. First, it provides a reasonably lengthy period in common with the November 1999
Greenbook–the period from 1997:Q1 to 2001:Q4–that we can use to compare OPP experiments,
with and without the benefit of some hindsight. Second, it demonstrates the procedure in use at
the Federal Reserve Board for creating extensions and the judgment encompassed therein.
Good judgment is of obvious benefit for policy design. For us, however, the veracity of the judgment at the time is less important than demonstrating its significance to optimal policy projections
in general, and the diﬀerences from alternative policies, in particular.
The Greenbook forecast is conditioned on an assumed path for the federal funds rate, the Federal
Reserve’s policy instrument. Following the convention of the day, the February 1997 Greenbook
held fixed the funds rate at the prevailing value of 5-1/4 percent until the end of the forecast period
in 1998:Q4. This gave the Committee a sense of what a “no-change policy” would imply.
In all cases, the Greenbook extension maintains all the assumptions of the Greenbook forecast
itself for the forecast period. This is done by computing the judgment–that is, the residuals to the
FRB/US model–that is necessary to replicate the Greenbook forecast. Thereafter, the fundamental
views of the forecast are maintained wherever possible by extending several years into the future
the model residuals as of the end of the regular forecast period.

In principle, any assumptions

regarding the economic outlook could have been incorporated in the extension by adjusting the
model residuals and exogenous variables in an appropriate way over the extension period. But the
staﬀ have tended to focus on determinants of the medium-term outlook, including the stance of
fiscal policy, foreign economic conditions, oil prices, productivity growth, and the exchange rate.
The medium-term outlook also included a view on the general state of the economy and what
that state implies for monetary policy.11 As noted, the view in early 1997 was that the economy
had reached an unsustainable level of output with incipient inflation pressures. For the extension,
this elicited an increase in the funds rate to stabilize the economy and contain inflation, albeit not
necessarily at an inflation rate that FOMC members would find desirable. Risks in the forecast
and its extension can be (and were) handled by reconstructing baseline forecasts with alternative
assumptions and recomputing policy scenarios conditional on the alternative baseline.
11
Except where extra-model information would suggest otherwise, variables that have typically exhibited trend in
history are extrapolated out at trend rates in the extension period. Variables that have been stationary are assumed
to settle on values at or near their forecast ending values except when stabilizing on such values would be inconsistent
with the views incorporated in the forecast.

11

We will have more to say about the extension and its properties in the next section. We
close this subsection by noting that since the funds rate path in the extension is not likely to
be optimal, in the OPP exercises below we should expect to see a markedly diﬀerent paths. To
provide some context, we will also include some scenarios with funds rate settings directed by a
simple Taylor rule, just as in Taylor [20], except that core PCE inflation is used instead of the GDP
price deflator. Besides being simple and familiar, the Taylor rule was and is held up as an example
of an instrument rule that, although not necessarily optimal, should work reasonably well in a wide
variety of circumstances. Moreover, in accordance with the real-time nature of the present analysis,
in early 1997 the rule was novel and was garnering a great deal of attention.
3.3. The FRB/US model
The FRB/US model is the workhorse model of the Federal Reserve Board’s staﬀ. As such it
serves in a variety of capacities: conducting forecasts, carrying out policy experiments, generating
alternative Greenbook simulations, conducting stochastic simulations to measure uncertainty, and
constructing the Greenbook extension, to name a few. And while the model is not used to produce
the oﬃcial Greenbook forecast–that is done judgmentally–the model provides a check on the
Greenbook forecast, both formally through the model forecasts themselves, and informally through
explorations of the model’s properties and examinations of the Greenbook extension.12
Fundamentally, the model is of New Keynesian design. It includes a specific expectations block,
and with it, a fundamental distinction between intrinsic model dynamics (dynamics that are immutable to policy) and expectational dynamics (which policy can aﬀect). In most instances, the
intrinsic dynamics of the model were designed around representative agents choosing optimal paths
for decision variables while facing polynomial adjustment costs. The notion of polynomial adjustment costs, a straightforward generalization of the well-known quadratic adjustment costs, allowed,
for example, the flow of investment to be costly to adjust, and not just the capital stock. This idea,
controversial at the time, has recently been adopted in the broader academic community.13
The model has a neoclassical steady state with growth and rich channels through which monetary policy operates. Monetary impulses originate from the model’s instrument, the federal funds
rate, and then transmit, in large part through expectations, to longer-term interest rates, asset
prices, and wealth, and from there to expenditure decisions of firms and consumers. The model is
12

The Greenbook extension provides a path for the funds rate beyond the Greenbook forecast period that can be
used to inform the path for longer-term bond rates that condition the Greenbook forecast.
13
Christiano, Eichenbaum, and Evans [5], for example, allow the flow of investment to be costly to adjust which
is the same thing as having higher-order adjustment costs for the stock of capital.

12

estimated using NIPA data, with most equations estimated over the period since the early 1960s.
FRB/US is a large model. In 1997, it contained some 300 equations and identities, of which
perhaps 50 were behavioral. About half of the behavioral equations of that vintage of the model
were modeled using formal specifications of optimizing behavior containing explicit estimates of
forward expectations and adjustment costs.14
Two versions of expectations formation were envisioned: rational expectations and VAR-based
expectations. Rational expectations means that agents are assumed to understand and take fully
into account the entire structure of the model, including monetary policy formulation, in arriving
at their decisions. VAR-based expectations follows a parable quite like the Phelps-Lucas “island
paradigm:” the model’s agents live on diﬀerent islands where they have access to a limited set
of core macroeconomic variables, knowledge they share with everyone in the economy. The core
macroeconomic variables are the output gap, the inflation rate, and the federal funds rate, as well
as beliefs on the long-run target rate of inflation and the what the equilibrium real rate of interest
will be in the long run. In addition they have information that is germane to their island, or sector.
Consumers, for example, augment their core VAR model with information about potential output
growth and the ratio of household income to GDP.
There is not the space here for a complete description of the model. Readers interested in
detailed descriptions of the model are invited to consult papers on the subject, including Brayton
and Tinsley [1], Brayton and others [2], Brayton and others [3], and Reifschneider and others [18].
Tetlow and Ironside [21] describes the real-time evolution of the model and the time variation in
optimized Taylor-type rules that are implied.
The FRB/US model is a near-linear model. The illustrative linear (or linearized) framework
used above to explain OPPs can thus be seen as a good linear approximation to the FRB/US
model. The model is solved using a terminal condition that projections of the variables are equal
to their target or steady-state values at a given horizon T . Thus, the FRB/US model is solved
as a finite-horizon problem. This is a practical step, and it is not restrictive for OPP purposes.
Optimal policy in the FRB/US model makes all variables approach their target or steady-state
values at suﬃciently long horizons. Then, the horizon T can be set to a reasonably large number
such that the finite-horizon solution is insensitive to local perturbations of the terminal date.15 The
finite-horizon approximation outlined above can then be seen as a linearization of the finite-horizon
14

In price and volume decisions, polynomial adjustment costs ruled. In financial markets, intrinsic adjustment
costs were assumed to be zero.
15
That is, one need only extend the horizon until such a point that the extension no longer aﬀects the simulated
results over the horizon of interest. This is a “type III iteration” in the parlance of Fair and Taylor [6].

13

problem for the FRB/US model. Indeed, the near-linear FRB/US model can be represented as
near-linear equation system instead of (2.12),
f (st , g t ) = 0,

(3.1)

where the function f (·, ·) is a vector-valued function of dimension (T + 1)(nX + nx ). The OPP is
then the projection st that minimizes (2.14) subject to (3.1).
OPPs can and have been done with both VAR-based- and rational-expectations versions of the
model. An important point to glean from these examples is that in comparison with most models,
the various versions and vintages of the FRB/US model are complicated. It follows that if the
method of OPPs can work for this model under these circumstances it can work for a wide variety
of other applications.
In what follows in this paper, we restrict the analysis to the version with VAR-based expectations, in large part because this is what was used almost exclusively for Greenbook and Bluebook
work in 1997. Today, it is still the VAR-based-expectations version of the model that is used for
forecasting. For policy analysis, when the staﬀ believes the experiment in question does not deviate
too much from what has been typical in the past, so that the average historical experience captured in the VAR can be thought of as representative of the likely response under the experiment,
the VAR-based-expectations version is again used. The rational-expectations version is used for
problems in which agents are likely to have the information and motivation to formulate a detailed
understanding of events, or for policies that are systematic enough that agents could be expected
to learn about their consequences.16
3.4. Two Optimal Policy Projections
In designing OPPs in the present context, one is faced with choices regarding the specification of
the loss function, (2.8), which amounts to fixing weights in the matrices W and Ω and targets for
the inflation rate and the unemployment rate. One could, in principle, choose the weights on the
loss function using a quadratic approximation of the true social welfare function, as described in
Woodford [23]. However, this would be prohibitively diﬃcult to do in a model as large and as
complicated as the FRB/US model. Moreover, as Levin and Williams [14] have argued, leveraging
the microfoundations of a model in this way can make the selected policy even more susceptible to
16
Examples of where foresight is regarded as critical include certain kinds of fiscal-policy interventions, since they
involve legislative commitments to future actions that are costly to undo and for which it pays for agents to make
the eﬀort to learn the implications of the legislation. Another example would be monetary-policy rules that are used
so systematically that agents can be expected to learn them.

14

model uncertainty than would otherwise be the case. For this exercise we choose equal weights on
each of the (squared) deviation of inflation from its target rate (the inflation gap), the deviation
of the unemployment rate from the estimated NAIRU (the unemployment gap), and the change in
the federal funds rate.17 The target for the unemployment rate is set equal to the staﬀ estimate
of NAIRU at the time of 5.6 percent. The choice of a target rate of inflation is more problematic.
The Federal Reserve does not have an oﬃcial target rate of inflation. As we show below, while the
judgmental path for the federal funds rate in the extension period was chosen with some notion
of stabilizing the economy in mind, it was not done so to render “price stability.” Under these
circumstances, we arbitrarily choose a rate of 2 percent, measured in terms of PCE inflation, for
the target rate.18 Hence this corresponds to a periodic loss function,
Lt+τ ,t = (π t+τ ,t − π ∗ )2 + (ut+τ ,t − u∗t+τ ,t )2 + (it+τ ,t − it+τ −1,t )2 ,

(3.2)

where π t denotes annualized quarterly PCE inflation in quarter t, measured in percent per year,
the inflation target π ∗ equals 2 percent per year, ut denotes the unemployment rate measured in
percent, and u∗t denotes the natural unemployment rate. The discount factor in the intertemporal
loss function is set at δ = 0.99 per quarter.
The same exercise is carried out based on the November 1999 Greenbook, using the baseline
of that time, and that model vintage. Now, since part of the period we study has us looking back
at the 1997 to 1999 period, the “judgment” is quite diﬀerent. In the intervening years, the staﬀ
had come to recognize the productivity boom during the mid-1990s. In addition, myriad other
forces had impinged on the economy, including the 1998 Asian crisis and the Russian debt default.
The policy maker’s loss function also diﬀers, albeit only slightly: in the nearly three years between
the two Greenbooks under study, the staﬀ reduces its estimate of the NAIRU, u∗t , to 5.2 percent
instead of the previous 5.6 percent. Accordingly, it is the lower figure that enters into equation
(3.2) for the November 2003 exercise.
The results are best shown graphically, which we do in figure 1. The left column of the figure
shows the results for the February 1997 Greenbook, while the right column the results for November
1999 Greenbook. In each case, the baseline projection is shown by the solid line, the OPP is the
17
The presence in the loss function of the inflation rate less its target rate and the unemployment rate less the
NAIRU (or the output gap) is conventional. The use of the change in the funds rate as an argument to the loss
function is a simple acknowledgement of the empirical observation that central banks the world over seem to smooth
instrument settings over time. This phenomenon may represent eﬀorts to hedge against model uncertainty, an inherent
taste of central bankers, or something else. See Sack and Wieland [19] for a survey on Fed interest-rate smoothing.
18
Besides being a reasonable, mainstream choice, as we shall see, a 2-percent target corresponds with a scenario
called “stable inflation” in the Bluebook.

15

dashed line, and the Taylor-rule projection is the dashed-dotted line. A vertical line marks 1996:Q4,
the last quarter before our projections. Let us focus on the left column for the time being.
Figure 1
Optimal Policy Projections with the FRB/US model
(Selected historical extended Greenbook forecasts)

February 1997
7.0

November 1999

Federal funds rate

7.0
Baseline

6.5

6.5

Taylor

6.0
5.5

6.0
5.5

OPP

5.0
4.5

5.0
4.5

1996 1997 1998 1999 2000 2001

Unemployment Rate
6.25
6.00
5.75
5.50
5.25
5.00
4.75
4.50
4.25
4.00
3.75

1996 1997 1998 1999 2000 2001

Unemployment Rate
6.25
6.00
5.75
5.50
5.25
5.00
4.75
4.50
4.25
4.00
3.75

1996 1997 1998 1999 2000 2001

PCE Inflation

1996 1997 1998 1999 2000 2001

PCE Inflation

3.5

3.5

3.0

3.0

2.5

2.5

2.0

2.0

1.5

1.5

1.0

1.0

0.5

0.5

0.0

Federal funds rate

0.0

1996 1997 1998 1999 2000 2001

1996 1997 1998 1999 2000 2001

As already discussed, the February 1997 Greenbook baseline projection holds the funds rate
at its inherited level until the conclusion of the forecast period in 1998:Q4; thereafter the funds
rate path was adjusted judgmentally to contain excess demand and stabilize the inflation rate.
In particular, the funds rate rises 50 basis points in each of the first three quarters of 1999 to
16

reach 6-3/4 percent where it stays until the end of 2000. Thereafter, the funds rate is reduced
to 6-1/4 percent where it remains for the duration of the scenario. With these increases in the
funds rate coming as late as they do, the middle-left panel shows a near-continuous decline in the
unemployment rate under the baseline policy until mid 1999, after which time it gradually returns
to the staﬀ NAIRU of 5.6 percent. And, the staﬀ warned in the Greenbook of the time, the result
is a steady rise in inflation rate, to about 3.3 percent.
The OPP calls for an immediate increase in the funds rate, by about 125 basis points over
the first three quarters of 1997 and reaches its peak of a little over 6-1/2 percent in early 1998.
Thereafter, it slides slowly back down toward its original level. These increases in the funds rate
are suﬃcient to reverse the decline in unemployment that would otherwise be projected to occur,
bringing the unemployment rate above the NAIRU by the middle of 1998. The prescribed increases
in the funds rate, while not particularly large by historical standards, are timely. By acting early,
the OPP keeps inflation very close to the target rate of 2 percent, as shown in the bottom-left
panel.
The OPP simplifies the construction of, and adds rigor to, the process that was actually carried
out for the February 1997 Bluebook. Appendix A to this paper shows selected pages from that
Bluebook. The dotted line in the appendix chart shows a path for the funds rate, determined by
trial-and-error methods, that brings core PCE inflation to the same 2 percent target level we use in
OPP. The path for the funds rate shown there shows broadly the same characteristics as the OPP
path, although the path is not an optimal one.
Returning to figure 1, in order to provide some context, the dashed-dotted lines show projections
under the Taylor rule.19 By construction, a Taylor-rule policy responds to the big picture of
the economy, but eschews the judgment that is the subject of this paper. Thus, it is useful for
comparative purposes. The dashed-dotted line in the upper-left panel shows that the Taylor rule
calls for an even sharper tightening of policy in the short run than does the OPP policy. Thereafter,
it advocates a more equivocal policy for some time, with oscillations up and down in the funds rate.
This reflects the myopic nature of the rule–picking the funds rate period by period based only
on current conditions–as opposed to the multi-period forward-looking optimal planning of OPPs.
Thus, the Taylor rule must reverse what turn out to be excessive movements in previous settings
of the funds rate. In the end, the Taylor rule’s policy prescription ends up with unemployment and
19
For the Taylor rule simulation, the equilibrium real fed funds rate, r∗ , is set equal to 3.1 percent, the value that
the real rate converges on in the extension shown in the baseline simulation as shown in the upper right panel of the
chart in the appendix A.

17

inflation that are closer to target than the baseline policy, but still well oﬀ target levels.
Now let us turn to the November 1999 baseline shown in the right column. It is obviously not
possible for a policymaker acting in real time to have the scope of information that the November
1999 backcast of the period from 1997:Q1 to 1999:Q3 includes. Our objective here is diagnostic; we
revisit this period in history to see what better judgment–as captured by the historical data and
the backcast of unobservable variables that those data engendered–does to policy prescriptions.
The solid line, once again, shows the forecast (for 1999:Q4 and beyond) as well as the revised
historical data (from 1997:Q1 to 1999:Q3). Notice the dip in the funds rate, in the upper-right
panel, from late 1998 until early 2000, reflecting the FOMC’s response to the Asia crisis and its
eﬀects on global financial markets.20 Beginning as before in 1997:Q1, the actual funds rate rose
25 basis points early in 1997. With the benefit of hindsight, the OPP would have called for a
modest easing in the stance of policy in the early going. The subsequent increases in the funds
rate, although superficially similar to those for the February 1997 scenario, are smaller and shorter
lived. In any event, one of the interesting historical aspects of the two baseline scenarios is the
remarkable diﬀerence in inflation projections that are supported by relatively similar patterns of
excess demand as captured by the unemployment rates. This change in view was a reflection of
the new-found appreciation by the staﬀ of the productivity boom and its eﬀects on marginal costs
and hence on inflation. In the end, this results in a situation–as seen from the perspective of the
November 1999 Greenbook–in which excess demand for labor must be tolerated for a time in order
to bring inflation up toward the target of 2 percent from the low levels seen in 1997 and 1998.
The Taylor rule is oblivious to all this. It responds only to contemporaneous excess demand
(which diﬀers only in small ways in early 1997 between the two scenarios) and inflation (which
diﬀers even less). Consequently, the policy prescription from the Taylor rule is quite similar for the
two baselines, even though they diﬀer in important ways. The myopia of the Taylor rule causes it
to oscillate back and forth between tightening and easing in the November 1999 baseline.
To provide a summing up of the performance of these rules, table 2 below computes the loss as
calculated using (2.14) and (3.2), for the baseline projection and the two counterfactual experiments,
for both the February 1997 and November 1999 cases. In all cases the losses are computed over
the same period of 20 quarters from 1997:Q1 to 2001:Q4 and divided by the number of quarters, so
the losses reported are average loss measured per quarter. To add some perspective, the columns
20
In daily data, the intended funds rate would move in 25 basis points increments, given the FOMC’s practice to
move it in such discrete increments. Historically, however, the Fed was not always able to keep the funds rate at its
intended level. And, in any case, in our figures the funds rate is expressed as a quarterly average of daily observations.

18

marked “Incr.” show the increase in loss compared with the OPP loss, which can be interpreted as
the maximum increase in the average (per quarter) squared inflation gap, unemployment gap, or
federal funds change, that the policymaker would be willing to incur for the privilege of using the
OPP policy instead of the policy shown.21
Table 2. Losses under alternative funds rate paths1
February 1997 November 1999
Loss
Incr.2
Loss Incr.2
OPP policy
0.20
1.69
Taylor rule
0.66
0.46
2.27
0.58
Extended Greenbook
1.06
0.86
2.28
0.59
1. Losses calculated from 1997:Q1 to 2001:Q4, average per quarter.
2. Increase in loss compared with the OPP.

The two left columns show the results for the February 1997 model. They demonstrate that
both the baseline and the Taylor-rule projections produce substantially inferior performance, in
proportionate terms, in comparison with the OPP. In both cases, the policymaker would be willing
to suﬀer an increase in the average squared inflation gap of about a half percentage point per
period or more for the privilege of using the OPP. The result under the baseline projection is hardly
surprising, since the path for the funds rate in that instance is a conditioning assumption for the
forecast, rather than a policy prescription—but the Taylor-rule result requires some explanation.
What it tells us is that the Taylor rule supplies the broad strokes of a stabilizing policy, but in a
large-scale model where there are numerous channels through which shocks are conveyed and policy
operates, it provides insuﬃcient breadth to come close to the optimal policy. Advocates of simple
rules recognize that such rules are suboptimal (except in special cases) to a complexity of a fully
optimal rule. The trade-oﬀ for this suboptimality is said to be that such rules are likely to be more
robust than many alternatives, a point to which we return presently. Our result for the February
1997 model, however, suggests that the Taylor rule leaves substantial room for improvement.
The two right columns show the results for the November 1999 model. In this case, the baseline
projection is littered with the actual shocks borne over the period from 1997 to 1999. Even with
optimal feedback in response to these shocks, significant losses are incurred and so the performance
under the alternative projections are likely to be more similar than in our previous case. The
results here show that the OPP, operating with the advantage of hindsight over the 1997-1999
period, would have outperformed by a significant margin the performance of the Taylor rule or the
baseline funds rate path.
21
It is because each squared term of the loss function carries the same weight that the increase in loss apply to all
terms.

19

The foregoing shows the importance of judgment for the design of policy. It also shows that
diﬀerent judgments can lead to diﬀerent policies. The question of robustness of policy logically
arises. If judgment can be suspect, it stands to reason that diﬀerent OPPs should be conducted
for diﬀerent, plausible sets of judgment.

OPPs for alternative assumptions–including, in the

context of these experiments, increasing productivity growth and falling natural unemployment
rates, as well as for alternative weights in the loss function–can easily be computed. Together,
these alternative OPPs along with the baseline projection can comprise a useful portfolio of policy
alternatives for central bankers.

4. Conclusions
This paper shows in theory and in practice how judgment can be optimally incorporated into
a rigorous process for monetary policy decision making. Optimal Policy Projections have the
advantage of fully incorporating all the knowledge and views that can be formalized of monetary
policy decision makers. This method is already in use by the staﬀ of the Federal Reserve Board for
presenting policy options to the Federal Open Market Committee.
We demonstrate the eﬃcacy of OPPs using two historical baselines and two vintages of the
Federal Reserve Board’s FRB/US model. To us, the results are encouraging. Moreover, we would
argue that the Fed’s continued use of such exercises–complicated as it is by the use of a large-scale
model that is diﬀerent from the “model” with which the forecast is generated–shows that OPPs
are a viable tool for many central banks.
Looking to the future, an important limitation of the procedure is the certainty-equivalence
assumption for the results and the consequent underplaying of model-uncertainty issues other than
additive judgmental adjustments. The paper mentions the possibility of computing multiple OPPs
associated with diﬀering sets of judgments. Also mentioned is the use of min-max procedures
in combination with OPPs to formulate defensive strategies against locally worst-case outcomes.
These should be worthwhile avenues to pursue. Another possible extension is to show how OPPs
can be updated over real time as new data are collected and new judgment is adopted.

20

References
[1] Brayton, Flint, and Peter Tinsley (eds.), “A Guide to FRB/US–A Macroeconomic Model of
the United States,” Finance and Economics Discussion Series Paper No. 1996—42.
[2] Brayton, F., A. Levin, R. Tryon, and J.C. Williams (1997), “The Evolution of Macro Models
at the Federal Reserve Board,” Carnegie-Rochester Conference Series on Public Policy 47,
227—245.
[3] Brayton, F., E. Mauskopf, D. Reifschneider, P. Tinsley, and J. C. Williams (1997), “The Role
of Expectations in the FRB/US Macroeconomic Model,” Federal Reserve Bulletin (April),
227—245.
[4] Congressional Budget Oﬃce (1997) The Budget and Economic Outlook: Fiscal Years 1998—
2007 (January).
[5] Christiano, Laurence, Martin Eichenbaum, and Charles Evans (2004), “Nominal Rigidities and
the Dynamic Eﬀects of a Shock to Monetary Policy,” Journal of Political Economy, forthcoming.
[6] Fair, Ray C., and John B. Taylor (1983), “Solution and Maximum Likelihood Estimation of
Dynamic Rational Expectations Models,” Econometrica 51, 1169—1185.
[7] Federal Reserve Board (1997a), Greenbook, February.
[8] Federal Reserve Board (1997b), Bluebook, February.
[9] Federal Reserve Board (1999), Greenbook, November.
[10] Hansen, Lars P., and Thomas J. Sargent (2003), “Robust Control of Forward-looking Models,”
Journal of Monetary Economics 50, 581—604.
[11] Hansen, Lars P., and Thomas J. Sargent (2004), Misspecification in Recursive Macroeconomic
Theory, unpublished manuscript.+
[12] Greenspan, Alan (2004), “Risk and Uncertainty in Monetary Policy” American Economic
Review Papers and Proceedings 94, 33—39.

21

[13] Kalchbrenner, John, and Peter Tinsley (1976), “On the Use of Feedback Control in the Design
of Aggregate Monetary Policy.” American Economic Review Papers and Proceedings 66, 349—
355.
[14] Levin, Andrew T., and John C. Williams (2003), “Robust Monetary Policy with Competing
Reference Models,” Journal of Monetary Economics 50, 945—975.
[15] Orphanides, Athanasios, and John C. Williams (2002), “Robust Monetary Policy Rules with
Unknown Natural Rates,” Brookings Papers on Economic Activity ,2: 63—118.
[16] Svensson, Lars E.O. (2003), “What Is Wrong with Taylor Rules? Using Judgment in Monetary
Policy through Targeting Rules,” Journal of Economic Literature 41, 426—477.
[17] Svensson, Lars E.O. (2005), “Monetary Policy with Judgment: Forecast Targeting,” International Journal of Central Banking 1, 1—54, www.ijcb.org.
[18] Reifschneider, David, Robert J. Tetlow, and John C. Williams, (1999), “Aggregate Disturbances, Monetary Policy, and the Macroeconomy: The FRB/US Perspective,” Federal Reserve
Bulletin (January), 1—19.
[19] Sack, Brian and Volker Wieland (2000) "Interest-rate smoothing and optimal monetary policy:
a review of recent empirical literature" Journal of Economics and Business,52: 205-228.
[20] Taylor, John B. (1993), “Discretion versus Policy Rules in Practice,” Carnegie-Rochester Conference Series on Public Policy 39, 195—214.
[21] Tetlow, Robert J., and Brian Ironside (2004), “Real-Time Model Uncertainty in the
United States:

the Fed from 1996—2003,” working paper, Federal Reserve Board,

www.members.cox.net/btetlow/default.htm.
[22] Tetlow, Robert J., and Peter von zur Muehlen (2001), “Robust Monetary Policy with Misspecified Models: Does Model Uncertainty Always Call for Attenuated Policy?” Journal of
Economic Dynamics and Control 25, 911—949.
[23] Woodford, Michael (2003), Interest Rates and Prices, Princeton University Press.

22

Appendix
A. Excerpt from the February 1997 Bluebook
Long-Run Scenarios
(6) To provide a longer-run perspective on the strategic issues confronting the Committee, this
section presents econometric model simulations designed to examine alternative monetary policies
as well as the eﬀects of certain shocks to the economy. The three policy scenarios considered first are
built around the Greenbook forecast, using the staﬀ’s new macroeconometric model to extend that
forecast and to derive diﬀerences resulting from alternative policies. These scenarios incorporate
the same assumptions regarding underlying macroeconomic factors; notably, the full-employment
budget for the federal government is on path to balance by early in the next century and the NAIRU
is 5.6 percent. Other sets of scenarios consider: (1) a favorable shock to productivity growth, (2)
an increase in the NAIRU, and (3) a significant decline in the stock market. The model’s dynamic
properties are importantly aﬀected by the level and changes in the public’s expectations about key
economic variables–such as the rate of inflation likely to prevail in the long run. Because these
expectations adapt slowly and nominal wages adjust sluggishly, the sacrifice ratio over a period of
five years is about 2–in line with the historical average for the U.S. economy. That is, reducing
inflation by 1 percentage point requires unemployment to exceed the NAIRU by the equivalent of
1 percentage point for two years.
(7) The baseline strategy, shown by the solid lines in Chart 2, is an extension of the Greenbook
forecast. By the end of the Greenbook forecast, the disequilibrium in policy and the economy has
become quite evident–the economy is producing beyond its sustainable potential and the stance
of policy is too easy to correct the situation and forestall a continuous rise in core inflation.4 Under
the baseline strategy, the Committee caps the rise in inflation by tightening policy after 1998 by
enough to bring the unemployment rate quickly up to the NAIRU. This requires the federal funds
rate to be raised by around 1-1/2 percentage points, so that the real funds rate overshoots its
equilibrium for a time.5 With this strategy, the Committee would accept whatever rate of inflation
that developed while the economy was operating beyond its potential, and, as a consequence, core
PCE inflation would ratchet up from an average of 2 to 2-1/2 percent in recent years to a little
over 3 percent.
(8) Some pickup in core inflation appears to be unavoidable in the near term given the staﬀs
assessment of the cyclical position of the economy, but the stable inflation strategy limits that
rise and ultimately brings inflation back down to around its recently prevailing rate. This entails
a near-term tightening, with the nominal funds rate rising to 6-l/2 percent by the end of 1998.
The eﬀects of the unemployment rate remaining below the NAIRU until early 1999 are tempered
4

In the charts, inflation is measured by the core PCE chain-weight price index, and past movements in this index
are used to proxy for inflation expectations in calculations of the real funds rate. This index has a steeper upward
trajectory over the next few years than do many other broad measures of prices, because it: (1) excludes food and
energy prices, which are moderating; (2) is unaﬀected by the changes in BLS calculations of the CPI; and (3) unlike
a broad GDP price measure, includes import prices, which are damped at first and boosted later by the actual and
assumed gyrations of the dollar. We think it gives a clearer view of the underlying inflation tendencies in the various
scenarios, but its application in calculating the real interest rate may exaggerate the projected drop in real rates
in 1997 and 1998, especially if the public forms its expectations based on a broader set of prices than in this core
measure. The real funds rates shown in the charts are higher than those calculated using the CPI, but would be
higher through history as well because inflation as measured by the PCE index on average has run 12 percentage point
below the CPI.
5
That equilibrium itself is lower toward the end of the simulation than at present owing primarily to additional
fiscal consolidation.

23

in the near term by the sharp slowing in real growth, which keeps inflation expectations damped
in the model (similar in result to a “speed eﬀect” in the Phillips curve), and by the rise in the
dollar associated with higher interest rates. These eﬀects dissipate, however, and ultimately the
real interest rate and the unemployment rate must be kept above their natural levels for some time
to oﬀset the underlying inflationary pressures built up as the economy operated above potential
from 1996 through 1998.
(9) A strategy involving a sharper tightening of policy over the next two years, with the nominal
funds rate rising soon and reaching 7 percent in late 1998, would achieve price stability in seven years
or so. In this scenario, a higher real funds rate is sustained for longer than under the stable inflation
strategy to produce enough slack in the economy to keep downward pressures on wages and prices.
The sizable output loss reflects the slow adaptation of expectations noted above. In the absence
of empirical evidence that the cost of disinflation from moderate levels is reduced by an aggressive
anti-inflation program or by announced inflation targets, we have included no special “credibility”
eﬀects from the Committee embarking on a deliberate strategy to achieve price stability. Credibility
for price stability does develop–but “in the old fashioned way,” by earning it through achieving
stable prices. This simulation also makes no allowance for enhanced productivity as price stability
is approached.

24

1. The real federal funds rate is calculated as the quarterly nominal funds rate minus the four-quarter percent change
in the PCE chain-weight price index excluding food and energy.
Note: Data points are plotted at the midpoint of each period.

25

