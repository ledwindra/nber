NBER WORKING PAPER SERIES

CAN HIGHER-ACHIEVING PEERS EXPLAIN THE BENEFITS TO ATTENDING SELECTIVE SCHOOLS?:
EVIDENCE FROM TRINIDAD AND TOBAGO
C. Kirabo Jackson
Working Paper 16598
http://www.nber.org/papers/w16598
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
December 2010

This paper was previously circulated under the title "Peer Quality or Input Quality?: Evidence from
Trinidad and Tobago." I thank Stephanie Riegg Cellini, David Deming, Ron Ehrenberg, David Figlio,
Caroline Hoxby, Brian Jacob, Jordan Matsudaira, Jonah Rockoff, and Miguel Urquiola for helpful
comments on early drafts of this paper. The views expressed herein are those of the author and do
not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2010 by C. Kirabo Jackson. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.

Can Higher-Achieving Peers Explain the Benefits to Attending Selective Schools?: Evidence
from Trinidad and Tobago
C. Kirabo Jackson
NBER Working Paper No. 16598
December 2010, Revised August 2012
JEL No. H0,I2,J0
ABSTRACT
Using exogenous secondary school assignments to remove self-selection bias to schools and peers
within schools, I credibly estimate both (1) the effect of attending schools with higher-achieving peers,
and (2) the direct effect of short-run peer quality improvements within schools, on the same population.
While students at schools with higher-achieving peers have better academic achievement, within-school
short-run increases in peer achievement improve outcomes only at high-achievement schools. Short-run
(direct) peer quality accounts for only one tenth of school value-added on average, but at least one-third
among the most selective schools. There are large and important differences by gender.
C. Kirabo Jackson
Northwestern University
School of Education and Social Policy
2040 Sheridan Road
Evanston, IL 60208
and NBER
kirabo-jackson@northwestern.edu

Can Higher-Achieving Peers Explain the Benefits to Attending Selective
Schools?: Evidence from Trinidad and Tobago
C. Kirabo Jackson1
Northwestern University
Abstract: Using exogenous secondary school assignments to remove self-selection bias to schools and
peers within schools, I credibly estimate both (1) the effect of attending schools with higher-achieving
peers, and (2) the direct effect of short-run peer quality improvements within schools, on the same
population. While students at schools with higher-achieving peers have better academic achievement,
within-school short-run increases in peer achievement improve outcomes only at high-achievement
schools. Short-run (direct) peer quality accounts for only one tenth of school value-added on average, but
at least one-third among the most selective schools. There are large and important differences by gender.

In many nations, there is fierce competition for scarce slots at selective schools (Hsieh
and Urquiola 2006, Hastings and Weinstein 2008). This is, in part, because students at more
selective schools typically have better outcomes ─ giving the impression of sizable benefits to
attending selective, and often prestigious, schools. However, because motivated and highachieving students tend to select to these schools, these differences may reflect selection rather
than selective schools providing greater value-added. Addressing this selection problem, Jackson
(2010) uses a quasi-experimental design and finds that attending a more selective school in
Trinidad and Tobago has positive effects on exam performance and high-school graduation.
While not all studies find positive selective school effects (e.g. Abdulkadiroglu et. al. 2011),
similar positive effects have been found in Romania (Pop-Eleches and Urquiola 2010), the
United Kingdom (Clark 2008), and the United States (Dobbie and Fryer 2011).
The effect of attending a more selective school reflects both short-run peer effects (that
arise from contemporaneous interactions with higher-achieving classmates in addition to
interactions with parents and teachers) and the effect of inputs that may be endogenous to longrun differences in peer quality across schools (such as teacher quality, superior management
style, or funding)2. As such, if students benefit directly from higher-achieving classmates (Lavy,
Silva, and Weinhardt 2012; Hoxby and Weingarth 2006; Hanushek et. al. 2003), part of the
benefit to attending a selective school may be attributed to the direct benefits of having higher-

1

I thank Stephanie Riegg Cellini, David Deming, Ron Ehrenberg, David Figlio, Caroline Hoxby, Brian Jacob,
Jordan Matsudaira, Jonah Rockoff, and Miguel Urquiola for helpful comments on early drafts of this paper.
2
Jackson (2009) presents evidence that teacher quality is determined in part by student characteristics.

1

achieving peers. If the high concentrations of high-achieving students afforded by selective
schools engenders an environment particularly conducive to learning, then adopting the practices
of selective schools (such as strict discipline and educated teachers) in other schools will not
yield similarly impressive results. Without knowing how much of the benefits associated with
successful selective schools is due to those schools providing higher-achieving peers, we will
have little indication of whether successful school models can be replicated in other settings.
I aim to shed new light on this issue by investigating the extent to which the positive
selective school effects documented in Trinidad and Tobago (Jackson 2010) can be attributed to
selective schools providing higher-achieving contemporaneous peers.3 This poses empirical
difficulties because; (a) school selectivity and peer quality generally move together, (b) students
select to schools, and (c) students select to peers. However, peculiarities of the Trinidad and
Tobago education system provide a rare opportunity to overcome these difficulties.
To address concerns that peer quality and input quality tend to move together, I identify
the effects of attending a selective school (a school with higher-achieving peers) using variation
across schools, and I identify the direct effect of exposure to higher-achieving peers using only
variation in peer achievement across cohorts within schools ─ effectively holding input quality
constant.4 I present an analytical framework that shows under certain conditions, the marginal
effect of higher achieving peers obtained within schools divided by the marginal effect of higher
achieving peers obtained across schools will yield the fraction of the school selectivity effect that
can be directly attributed to that school providing higher achieving peers.
To address concerns that students may self-select to schools and peers, I restrict analysis
to a sub-sample where students are assigned to schools by the Ministry of Education (MOE)
based on observable characteristics that I can control for directly ─ precluding self-selection to
one's assigned school or assigned peers. I use the assignments to construct instruments for (a) the
selectivity of the schools that students attend and (b) changes in the incoming achievement of
peers across cohorts within schools. I present tests indicating that (a) school assignments are
conditionally exogenous, and (b) changes in assigned peer quality across cohorts within schools
are conditionally exogenous. One remaining concern is that changes in peer achievement within
3

This paper does not investigate the broader related question of how much school value-added can be explained by
peer quality. This would be very ambitious research to undertake. The data available and the nature of the
exogenous variation preclude a rigorous treatment of this broader question.
4
Similar approaches to identifying direct peer effects are used in Ammermueller and Pischke (2006), Lavy and
Schlosser (2009), Hoxby and Weingarth (2006), and Hanushek, et al. (2003).

2

schools across cohorts could be correlated with changes in unobserved school inputs. This is
unlikely because the schools used are centrally operated by the MOE that rarely alters school
policies, spending, or inputs on a school-by-school basis. Moreover, I show that changes in
assigned peer quality over time within schools are unrelated to changes a school's desirability (a
measure of perceived long-run school quality) or observed teacher quality.
I use the number of secondary school-leaving exams passed at the end of 10th grade as the
main outcome. This variable is a summary statistic for overall educational attainment because it
is sensitive to dropping out of school, the number of exams attempted, and performance on a
given exam. Echoing findings of Jackson (2010), attending schools with higher incoming peer
achievement increases the number of exams passed. Also, increases in mean peer achievement
within schools increase the number of exams passed. The marginal effect of increases in mean
peer achievement across schools is about 10 times larger than the marginal effect of increases in
mean peer achievement across cohorts within schools ─ implying that approximately 1/10 = 10
percent of the selective schooling effect can be attributed to the achievement level of peers.
However, the direct marginal effect of peers varies considerably across schools such that peer
achievement can account for two-thirds of the school selectivity effect among the most selective
schools, but explains little of the effect for the lower three quartiles of schools. Four additional
empirical facts support the notion that a sizable part of the selective school effect is a peer effect:
(1) among high-achieving schools, both the benefits to attending a more selective school and the
benefits of increased peer achievement within schools are large; (2) among low-achieving
schools both the benefits to attending a school with higher-achieving peers and benefits of
increased peer achievement within schools are small; (3) females, who benefit more from
attending selective schools also enjoy the largest benefits to marginal increases in peer
achievement within schools, and (4) gender differences in the response to peers can completely
explain gender differences in response to schools.
In this paper, I present a framework within which one can determine how much of a
school selectivity effect is directly attributable to a peer achievement effect. Using this
framework, this paper is the first paper to show that, among the most selective schools, much of
the school selectivity effect can be directly attributed to the incoming achievement level of the
peers ─ such that the successes at the most selective schools may not be scalable. Conversely, I
show that incoming peer achievement can only explain a small fraction of the school selectivity
3

effect on average, so that school attributes that generate the large differences in value-added
between low- and middle-achieving schools are potentially scalable.
The remainder of the paper proceeds as follows: Section II describes the Trinidad and
Tobago education system and the data. Section III lays out the analytic framework. Section IV
outlines the empirical strategy. Section V presents results, specification tests, and robustness
checks. Section VI concludes.

II

The Trinidad and Tobago Education System and the Data.
In Trinidad and Tobago secondary school begins in first form (6th grade) and ends at fifth

form (10th grade) when students take the Caribbean Secondary Education Certification (CSEC)
examinations. The exams are given in 31 subjects and are externally graded. Students who pass
five or more subjects including English language and mathematics exams meet the requirements
for secondary school graduation.5 There are eight public school districts, and private schools
account for a small share of student enrollment and tend to serve those who “fall through the
cracks” in the public system.6 There are two types of public secondary schools: Government
schools, and Government Assisted schools (Assisted schools). Government schools provide
instruction from 6th through 10th grade and often continue to 12th grade. These schools teach the
national curriculum and are fully funded and operated by the Government. Assisted schools are
almost identical to government schools but differ along the following key dimensions; (a)
Assisted schools are run by religious boards and are often single-sex schools; (b) all operating
expenses except teacher costs are publicly funded; (c) while the MOE assigns students to fill all
available slots at Government schools, the MOE assigns students to 80 percent of the open slots
at Assisted schools. The remaining 20 percent of school slots at Assisted schools are assigned by
the principal. This last distinction is key because unlike assignment to Assisted schools,
assignment to government schools is not subject to self-selection bias.

II.1.

Data and Summary Statistics: The data used in this study come from two sources: the

official SEA test score data (5th grade) for the 1995 through 2002 cohorts and the official 2000
5

The CSEC examinations are accepted as an entry qualification for higher education in Canada, the United
Kingdom and the United States. Students may continue to take the Caribbean Advanced Proficiency Examinations
at the end of grade 12, which is a prerequisite for more selective colleges and universities in most nations.
6
Students at private secondary schools have SEA scores 0.33 standard deviations below the average.

4

through 2007 CSEC test score data (10th grade). The SEA data contain each of the nation's
student’s SEA test scores, their list of preferred secondary schools, their gender, age, religion
code, primary school district, and the secondary school to which they were assigned by the
MOE. The SEA exam is comprised of five subjects that all students take: math, English, science,
social studies, and an essay. To track these 5th grade students through to secondary school, I link
the SEA data with the CSEC examination data both four and five years later. About 72 percent of
SEA test takers were linked to CSEC exam data.7 The CSEC data contain each student's grades
on each CSEC exam and the secondary school they attended. In the data, there are 123 public
secondary schools, some small test-taking centers and private schools. Among students linked to
CSEC data, under seven percent attended a private institution, were home schooled, or were
unaffiliated with any public education institution. To ensure that I have a sample within which
the school assignments are not subject to any selection, I drop students who are assigned to
Assisted schools and private schools. The resulting analytical dataset contains 150,701 students
across seven cohorts and 158 school assignments.8
Table 1 summarizes the final dataset, broken up by the assigned secondary schools’
rankings in incoming SEA scores (i.e. the school with the highest average incoming total SEA
scores is ranked first). SEA scores are in standard deviation units. Females make up about half of
students in each school group. There is much variation in school and peer quality. The 30 most
selective schools had students with about one standard deviation higher incoming SEA scores
than schools ranked between 31 and 90, which in turn had students with average incoming scores
over half a standard deviation higher than schools ranked below 90. As expected, selective
schools have better outcomes. About 87 percent of students at schools ranked better than 30 took
the CSEC exams compared to 71 percent for schools ranked 31 to 90, and 59 percent for schools
ranked below 90. The average student at a top 30 school passes 4.44 exams, compared to 1.9
exams in schools ranked between 31 and 90, and passing only 1 exam at schools ranked below
90. Some of these differences are due to students not taking the CSEC exams having no passes.
The schools that attract the brightest students typically have the best school resources.
7

Students were matched on name, gender, and date of birth. The match rate is consistent with the national dropout
rate of one third. Students with missing outcomes are coded as having zero passes and included in the sample.
8
There are more assignments than high schools because some when there are more students than available spots, the
government assigns students with the lowest SEA scores to small "temporary" schools or purchases seats in private
schools. Omitting students assigned to such schools does not affect the results.

5

Schools with the highest achieving students are on average smaller with cohort sizes being about
120 students at the top 30 schools and about 440 students in both other groups of schools.
Similarly, about 58 percent of teachers at schools ranked better than 30 have a bachelors degree
compared to 55 percent for schools ranked 31 to 90, and 36 percent for schools ranked below 90.
Given that having a university or college degree likely has an important effect on teaching
ability, this may translate into sizable teacher quality differences across schools. Higher ranked
schools also have fewer inexperienced teachers. Specifically, 14 percent of teachers at schools
ranked better than 30 have between 0 and 3 years of teaching experience compared to 16 percent
for schools ranked 31 to 90, and 24 percent for schools ranked below 90.

III

Econometric Framework
I present a model showing that, under reasonable assumptions, the ratio of the coefficient

on peer quality obtained using variation across schools and the coefficient on peer quality
obtained using variation within-schools across cohorts yields an estimate of the proportion of the
effect of attending a school with higher-achieving peers that can be directly attributed to
contemporaneous exposure to higher-achieving peers. The within-school peer effects will reflect
direct peer interactions, contextual effects, and teacher and parent interactions that may all be
directly affected by the contemporaneous classroom composition and not scalable across schools.
Input quality at school j at time t, denoted Ijt, is a linear function of permanent (long-run)
peer quality P j and idiosyncratic determinants ujt. This is written as [1] below.
[1]

I jt   P j  u jt

where E[u jt | P j ]  E[u jt ]  0 .

This captures the fact that school inputs (such as teacher quality and alumni donations) are
endogenous to persistent characteristics of the student body. Input quality is not a function of
contemporaneous peer quality because input quality changes are likely not sensitive to transitory
shocks to peer quality and changes. I present empirical support of this assumption in section V.2.
Peer quality is comprised of long-run component P j and idiosyncratic component  jt .
[2]

Pjt  P j   jt

where E[ jt | P j , u jt ]  E[ jt ]  0 .

The long-run component plus random error captures the fact that the schools that attract the
highest/lowest achieving students have done so for years. This modeling assumption is akin to
saying that Harvard and Yale (or Oxford and Cambridge) always attract the top students in any
6

given year. I present empirical support of this assumption in section V.2.
Achievement of student i at school j at time t Yijt is a separable function of input quality
and peer quality as in [3] below, where  ijt is an idiosyncratic error term.
[3]

Yijt     Pjt   I jt   ijt where E[ ijt | Pjt , I jt ]  E[ ijt ]  0 .

Substituting [1] into [3], the expected difference in student outcomes across schools j and
school j' conditional on peer quality is given by [4] below.
[4]

E[Yijt  Yi ' j ' t | Pjt , Pj ' t ]   ( Pjt  Pj ' t )   ( I jt  I j ' t )  (    )( Pjt  Pj ' t ) .

Equation [4] illustrates that, under the identifying assumptions in [1],[2], and [3], differences in
student achievement across schools associated with peer quality reflect the direct marginal effect
of short-run variation in peers  plus the marginal effect of input quality differences across
schools that exist in equilibrium as a result of long-run difference in peer quality  .
Because in expectation there are no systematic differences in input quality within schools
over time, the expected difference in student outcomes across cohorts t and t-1, within school j
conditional on peer quality is given by [5] below.
[5]

E[Yijt  Yi ' jt 1 | Pjt , Pjt 1 , J  j ]   ( Pjt  Pjt 1 ) .

Equation 5 illustrates that comparing the outcomes of observationally similar students attending
the same school but exposed to different peers because they attend at different times, yields the
direct effect of peers on outcomes  . As such,  / (    ) , the coefficient on within-school
changes in peer quality divided by the coefficient on peer quality obtained across schools, yields
the fraction of the benefits to attending a selective school that can be directly attributed to
exposure to higher achieving peers. Because the assumptions of linearity and the additive
separability are restrictive, I also present findings where these assumptions are somewhat
relaxed. I now detail strategies to uncover consistent estimates of both  and    .

IV

Empirical Strategy

Estimating the effect of attending a more selective school
To estimate the effect of attending a school with higher-achieving peers,    , the
basic approach is to compare the outcomes of observationally similar students at different
schools. For the naïve baseline specification, I model the outcome of student i at a school j in

7

cohort c with the following equation.
[6]

Yijc  f ( SEAi )  SEAijc  (    )   c   ijc

where f ( SEAi ) is a function if a student's incoming SEA score, SEAijc is the mean incoming SEA
score of all other students at school j in cohort c with student i,  c is a cohort fixed effect and  ijc
is unobserved determinants of student achievement. Because students may select to schools
based on unobserved determinants of achievement, naïve estimation of [6] might not uncover the
parameter (    ) . This motivates an instrumental variables strategy that uses students' initial
school assignments by the MOE to remove bias due to student selection to schools.
Estimating the direct effect of contemporaneous peer achievement
To estimate the effect of contemporaneous exposure to higher-achieving peers,  , I
compare outcomes of observationally similar students at the same school but who are exposed to
peers with different levels of incoming achievement because they are in different cohorts. In the
naïve specification, I model the outcome of student i at a school j in cohort c with [7].
[7]

Yijc  f ( SEAi )  SEAijc     c   j   ijc

.

All variable are defined as in [6]. The difference between [6] and [7] is the inclusion of a school
fixed effect  j , that absorbs time-invariant variation in peer quality ─ so that estimation is based
on only idiosyncratic transitory variation in peer quality within schools across cohorts. This same
strategy has successfully been employed in several papers including Ammermueller and Pischke
(2006), Lavy and Schlosser (2009), and Hoxby and Weingarth (2006). However, because (a)
students may select to schools based on unobserved determinants of achievement and this
selection may change over time, and (b) mean peer achievement could change within schools for
reasons other than random transitory shocks, naïve estimation of [7] by OLS may not uncover 
. This motivates an instrumental variables strategy that uses students' initial school assignments,

and the initial school assignments of students' peers, to remove bias due to student selection to
schools and bias due to endogenous changes in peer achievement over time.

IV.2

Student Assignment Rules:
After 5th grade, all students take the SEA examinations. Each student lists four ordered

secondary school choices. These choices and the SEA scores are used by the MOE to assign
8

students to schools. School slots are assigned in successive rounds such that the most highly
subscribed/ranked school fills its spots first, then the next highly subscribed school fills its slots
in the second round, and so on until all school slots are filled. This is done as follows: (1) The
number of school slots at each school nj is predetermined based on capacity. (2) Students are
tentatively placed in the applicant pools for their first choice schools and are ranked in
descending order by SEA score within each application pool. (3) The school at which the njth
ranked applicant has the highest SEA score is deemed the most highly subscribed/ranked school
j1, this score is the cut-off score for this school, and the top nj1 students in the applicant pool for
top-ranked school j1 are admitted to school j1. (4) The top ranked school's slots and the admitted
students are removed from the pool, and the second choice becomes the new "first choice" for
students who had the top ranked school as their first choice but did not gain admission. (5) This
process is repeated in round two to assign students to the second highest ranked school j2 and
determine the cut-off score for the second ranked school. This is repeated in subsequent rounds
until all slots are filled. This process applies to over 95% of students. However, assisted schools
(16% of school slots) can admit 20% of their incoming class at the principal’s discretion. The
rule is used to assign 80% of the students at these schools, while the remaining 20% can be handpicked by the principal before the next-highest ranked school fills any of its slots.9
The actual cut-off scores for each school are not released to the public. However, because
the rules are known, and I have the same information that the MOE used to assign students, I can
simulate where the cut-offs would have been if Assisted schools could not hand pick students
(see Appendix Note 1). After simulating clean cut-offs, I estimated the likelihood of attending
one's top choice school as a function of one's score relative to the simulated cut-off for one's top
choice school. Figure 1 depicts a sudden increase in the likelihood of assignment to one's top
choice school as one's score goes from below to above the simulated cut off ─ indicating that (a)
the assignments operate as described, and (b) there are meaningful differences in school
assignments associated with scoring above/below a cut-off that are not due to selection. The fact
that assignments to government schools are orthogonal to unobserved student characteristics
(where both choices and SEA scores are observed) plays a crucial role for identification, and
motivates my using only those students with exogenous assignments to government schools.
9

These hand-picked students are chosen based on family alumni connections, being relatives of teachers, or religion.
These students need not list the school as their top choice. Students receive one assignment and are never made
aware of other schools they would have been assigned to had they not been hand-picked.

9

In general, students tend to put schools with higher-achieving peers higher up on their
preference ranking. On average, the difference between the mean incoming SEA scores at a
student’s top choice school and second, third and fourth choice school is 0.277, 0.531, and 0.91
standard deviations, respectively. Also, higher-achieving students tend to have more selective
schools in their list, students request schools with the same religious affiliation as their own, and
students typically list schools geographically close to home. Because school choices are a
summary statistic for student/parental aspirations, preferences, expectations about ability,
religious affiliation, and geographic location makes these choices a powerful set of controls.

Identification Strategy
Because students assigned to government schools cannot self-select into the assignment,
conditional on incoming test scores and student choices, the school assignments within the group
of government schools is exogenous. I detail how I exploit this fact to identify (a) the effect of
attending a selective secondary school, and (b) the effect of marginal increases in incoming peer
achievement within a school.
Using instruments to estimate the effect of attending a more selective school
The problem with merely comparing outcomes of observationally similar students who
attend different schools (as in [6]) is that students may select or transfer into selective schools
based on unobserved characteristics that directly affect student outcomes. As such, one needs
variation in school attendance that is beyond students' control. Conditional on school choices, the
assignment rule creates test score cut-offs above which students are assigned to one school and
below which they are assigned to another. Among students who chose a selective school, the
likelihood of being assigned to (and attending) a more selective school increases in a sudden and
discontinuous manner as one's score goes from below to above the cut-off for that selective
school. If the locations of the cut-offs are orthogonal to student characteristics, and the effect of
test scores on outcomes is smooth through the cut-offs, one can attribute any sudden jumps in
outcomes as one's score goes from below to above the cut-offs to the sudden exogenous
increased likelihood of attending a selective school. While this variation is amenable a fuzzy
regression discontinuity design, relatively small sample sizes preclude precise estimates using
such a design. However, the main findings are robust to using this discontinuity variation.

10

I follow Jackson (2010; 2012) and exploit a different source of exogenous variation
created by the assignment rules. This second source of exogenous variation comes from the fact
that different schools have different cut-offs so that students with the same test scores but
different choices are assigned to different schools. This variation is best illustrated with an
example. Consider a world with two similarly selective schools 1 and 2 (both with mean peer
quality of 0.5) and one average school 3 (with mean peer quality of 0). There are two choice
groups; choice group 1 who list school 1 as their top choice and school 3 as their second; and
choice group 2 who list school 2 as their top choice and school 3 as their second choice.
Applicants to school 1 scoring above 82 on the SEA are admitted, while school 2 has a higher
cut-off such that applicants to school 2 scoring above 92 are admitted. One can put all students
into one of three test score groups: group A with scores of 82 and below; group B with scores
between 83 and 92; and group C with scores of 93 and above. This is illustrated in Figure 2.
Students in test score group A (with scores below the cut-offs for both selective schools)
are never admitted to a selective school whether they are in choice group 1 or choice group 2.
Similarly, students in test score group C (with scores above the cut-offs for both selective
schools) are all admitted to a selective school whether they are in choice group 1 or choice group
2. However, those in test score group B (with scores above the cut-off for school 1 but below the
cutoff for school 2) who are in choice group 1 are admitted to a selective school while those in
choice group 2 are not admitted to a selective school. As such, if the choice group effects are
additively separable from that of test scores, one can use a difference in difference approach to
identify the effect of attending a selective school.
Because the difference in choices do not lead to a difference in selective school
attendance within test score groups A and C, the difference in outcomes between choice groups 1
and 2 within test score groups A and C cannot be due to differences in test scores or differences
in selective school attendance and must therefore be due to differences in choices. However,
because the difference in choices lead to a differences in selective school attendance within test
score range B, the difference in outcomes between choice groups 1 and 2 within test score group
B reflects both differences in selective school attendance (in this example a mean peer
achievement difference of 0.5 standard deviations) and differences in choices. The difference in
outcomes between choice groups 1 and 2 within test score group B (selective effect + choice

11

group effect), minus the difference in outcomes between choice groups 1 and 2 within test score
groups A or C (choice group effect), reflects the effect of attending a more selective school.
To capture the difference in difference (DID) variation due only to the assignment rules, I
use assigned peer quality (i.e. the average incoming test scores of other students assigned to the

 ij*c , as an instrument for actual peer quality while
same school j* as student i in cohort c), SEA
controlling for a full set of choice indicator variables, and a full set of SEA score indicator
variables. Specifically, I estimate the outcome of student i from cohort c, at school j with the
following system of equations by two-stage-least-squares (2SLS).
50

 ijc 
 ij*c    I
SEA
 I SEAi k  k1  SEA
 Pi  p  p1  X i 1   c1   ijc1
1

[8]

k 1

50

 ijc  (    )  I
Yijc   I SEAi  k   k 2  SEA
 Pi  p  p 2  X i  2   c 2   ijc 2

.

k 1

 ijc is the mean total SEA scores for students attending the same school j as student i in
In [8], SEA
cohort c, I SEAi  k is an indicator variable equal to one if the student's SEA score is in test score bin

k (because SEA scores are continuous, SEA scores are put into 50 narrow bins per) so that  k 2 is
effectively a test score fixed effect, I Pi  p is an indicator variable denoting whether a student has a
particular school preference ordering (choices), and  p ,2 is a preference ordering (choices) fixed
effect (i.e. there is an indicator variable denoting each distinct ordered list of schools. For
example there is a dummy variable for all students who list schools A,B,C,D as their first,
second, third and fourth choice schools, and another different indicator variable for all students
who list A,B,D,C as their first, second, third and fourth choice schools.), X i includes student
gender, c is a SEA test taking cohort fixed effect, and ijc is the idiosyncratic error term.
The 2SLS estimate of the coefficient on peer achievement,    , from [8] should be
unbiased since (1) the analytic sample only includes those schools and students for whom the
initial school assignment is exogenous conditional on incoming test scores and choices, (2) the
excluded instrument, mean peer quality of the students assigned school (based on other students
assigned to the school), is not affected by students subsequently transferring to schools they
prefer, and (3) inference is based on comparisons within groups of students who are similar in
important ways but who were assigned to different schools for reasons beyond their control.

12

While there is no way to test for a correlation between the instruments and unobserved
student characteristics, I present evidence consistent with the identifying assumption by showing
that observed covariates are uncorrelated with the instruments. Specifically, I run a regression
with SEA score fixed effects choice fixed effects and the assigned peer quality to predict each
religion indicator variable, each primary school district indicator variable, and the number of
times a student attempted the SEA exams. I present the estimated coefficient on mean total in
Table 2. Assigned peer quality is not associated with student religion, the number of SEA
attempts, or the students' primary school district at the five percent level — suggesting that the
conditional exogeneity assumption is valid. I present further robustness checks in Section V.2.

Using Instruments to Estimate the effect of improved peer quality within schools
The problem with simply comparing the outcomes of observationally similar students at
the same school exposed to different levels peers is that (a) students select to schools and (b)
peers select to schools. To address this problem, I use variation in peer quality that is not subject
to student or peer selection. The fact that students assigned to government schools cannot selfselect into their assignment allows this. I use across-cohort within-school changes in average
incoming test scores of other students assigned to the same assigned school as an instrument for
changes in actual peer quality within students actual schools. To do this, I augment the crosssectional equation [8] to include an assigned school fixed effect  j* .
50

 ijc 
 ij*c    I
SEA
 I SEAi  k  k1  SEA
 Pi  p  p1  X i 1   c1   j*1   ijc1
1

[9]

k 1

50

 ijc    I
Yijc   I SEAi  k   k 2  SEA
 Pi  p  p 2  X i  2   c 2   j*2   ijc 2

.

k 1

The difference between [9] above and the naïve model [7] is that I include fixed effects for the
student’s assigned secondary school (as opposed to the actual school) and I instrument for the
actual peers at the actual school with the assigned peers at the assigned school. As long as the
school assignments are exogenous, conditional on test scores and student choices, equation [9]
will remove bias due to selection of students and peers and will yield an unbiased estimate of  .
I present evidence consistent with the identifying assumptions of no selection by showing
that observed covariates are uncorrelated with changes in assigned peers within assigned schools
across cohorts. Specifically, I run a regression with SEA score fixed effects, choice fixed effects,
assigned school fixed effects, and the assigned peer quality to predict each religion indicator
13

variable, each primary school district indicator variable, and the number of times a student
attempted the SEA exams. I present the estimated coefficient on mean total in Table 2. Assigned
peer quality is not associated with student religion, the number of SEA attempts, or the students'
primary school district at the five percent level — suggesting that the conditional exogeneity
assumption is valid. While this strategy may remove selection bias, there remains the concern
that changes in peer achievement could be correlated with changes in unobserved school inputs.
In section V.2, I present evidence that this is not the case.
There are two sources of plausibly exogenous variation in mean peer achieving within
schools over time. Because there are only about 20 thousand students taking the SEA each year
and schools take small slices (between 100 and 500 students) out of the SEA distribution based
largely on student rank, both small transitory changes in the distribution of test scores and
idiosyncratic variation in the distribution of student preferences over time lead to meaningful
exogenous variation in assigned peer achievement within assigned schools across cohorts.
To illustrate this variation due only to small changes the SEA distribution, I look at mean
SEA scores for students ranked 1 to 100, students ranked 3000 to 3500, and 10000 to 10500 for
each year between 1995 and 1998. Figures 3 and 4 show that due to small changes in the
distribution of test scores, the average incoming test scores of students ranked 1 to 100 decreased
by 0.1σ between 1995 and 1996 and increased by 0.06σ between 1996 and 1997. In comparison,
mean test scores of students ranked 10000 to 10500 increased by 0.05σ between 1995 and 1996
and fell by 0.02σ between 1996 and 1997. Within district changes are even larger. Mean test
scores of the top 100 students in the largest district increased by 0.21σ between 1995 and 1996,
while the mean test scores of students ranked between 2000 and 2500 fell by 0.11σ.10
The second source of variation is due to year to year changes in student preferences.
Because the assignment mechanism fills schools sequentially and a school’s order is based on the
score of the last admitted student, small changes in preferences, demographics, and scores can
cause a school that fills its slots first in one year to be second or third the following year. As
such, small perturbations in the distribution of student choices and test scores play out into
meaningful differences in peer quality within schools over time. This source of variation would
10

Because schools must fill a fixed number of school slots every year there is no correlation between peer quality
changes and cohort size within a school over time. The null hypothesis that within-school changes in assigned mean
peer quality are not correlated with within school changes in cohort size yields a p-value of 0.65.

14

be problematic if changes in student choices were correlated changes in unobserved school
characteristics. I present empirical evidence that this is not the case in Section V.2.

V

Results

Main Results: Table 3 presents the cross-school and within-school estimates for the full analytic
sample. The top panel presents naive OLS results based on students' actual schools attended, the
second panel presents reduced forms (RF) effect of assigned peer quality on students' outcomes,
and the third panel presents the Instrumental Variables (IV) estimates that use assigned peer
quality as an exogenous predictor of actual peer quality. Columns 1 through 3 present the across
school models, while columns 4 through 7 present the within school models.
A parsimonious OLS model of the number of exams passed as a function of the mean
total scores of the students at the actual school, SEA cohort fixed effects, the student's gender,
and a cubic in the total SEA score (column 1 top panel) yields an across school coefficient of
1.044 (se = 0.082). Controlling for student choices (column 2) and including indicator variables
for 50 SEA test score groups (column 3) yield very similar estimates of 1.229 (se = 0.073) and
1.235 (se = 0.073), respectively. These OLS results indicate that a student who attends a school
where peer test scores 0.2 standard deviations higher (roughly the within-school variance in peer
quality) would pass about 0.244 more exams. These findings echo Jackson (2010).
The second panel presents the reduced form effect of being assigned to a school with
higher achieving peers that should be free from self-selection bias. The reduced form estimates
range between 0.437 and 0.574 and are all statistically significant at the 1 percent level. The
instrumental variables results in the third panel (row) are similar to the OLS results yielding
statistically significant coefficient estimates between 0.85 and 1.177 ─ indicating that after
taking self-selection into account, a student who attends a school where peer test scores are 0.2
standard deviations higher would pass between 0.17 and 0.23 more exams.
Columns 4 through 7 present the within-school results that should identify the direct
effect of peer achievement on outcomes. The OLS results are based on actual incoming peer
achievement at students' actual schools attended and include indicator variables for students'
actual school attended. The parsimonious model that includes SEA cohort fixed effects gender, a
cubic of their total SEA score, and fixed effects for the actual school attended (column 4 top
panel) yields a within-school coefficient of 0.14 (se = 0.067). Controlling for score group
15

dummies (column 5), student choices (columns 6) and both score group dummies and choices
(column 7) yield very similar within-school estimates of 0.144 (se = 0.067), 0.135 (se = 0.072)
and 0.141 (se=0.074), respectively. These OLS results indicate that a student would pass about
0.028 more exams than an observationally similar student who attends the same school when
peer achievement was 0.2 standard deviations lower. The ratio of interest, the preferred acrossschool OLS coefficient divided by the preferred within-school OLS coefficient is a statistically
significant 0.114 (se = 0.58) ─ suggesting that roughly 11 percent of school selectivity effect can
be attributed directly to peer quality differences across schools.11
The IV estimates are about half the size of the OLS estimates, suggesting that there is
positive selection on unobservables to high achieving peers. The IV estimates are not statistically
significant, and range between 0.069 and 0.085. Taken literally, the point estimates suggest that
after taking self-selection bias into account, a student would pass between 0.014 and 0.017 more
exams than an observationally similar student who attended the same school when peer
achievement was 0.2 standard deviations lower. Note that the first stage F-statistics are all above
100. The ratio of the preferred within-school and across-school IV estimates is 0.072 (se=0.122)
─ suggesting that on average roughly 7.2 percent of school value-added can be attributed directly
to peer quality differences across schools. The results in Table 3 suggest that most of the school
selectivity effect, on average is not due to contemporaneous exposure to higher achieving peers,
so that much of the differences across schools may be scalable on average. However, these
average effects may mask considerable heterogeneity by gender and school type.

Effects by gender: Recent findings indicate that girls are more likely to benefit from attending
better schools than boys (Hastings, Kane and Staiger 2006; Deming, Hastings, et al. 2012). Also,
using similar data, Jackson (2010) finds that the selectivity effect is larger for girls than for boys.
A psychology literature suggests that females may be more responsive to peers than males (Cross
and Madson 1997, Maccoby and Jacklin 1974, Eagly 1978). As such, a differential gender
response to schools might be due to a differential gender response to peers. Indeed recent papers
find that while females benefit from exposure to higher achieving peers, males may not (e.g.
Lavy, Silva and Weinhardt 2010; Han and Li 2009).
11

The standard error of this ratio of coefficients was computed by estimating both the across school model and the
within school models simultaneously (a two-equation model) and then computing the standard error of the nonlinear
combination of coefficients using the delta method. This computation is dome by STATA's "nlcom" command.

16

To test for whether gender differences in response to peers can explain gender differences
in response to schools, I present the preferred specifications with the inclusion of the interaction
between being female and peer quality in Table 4. As such, the coefficient on mean peer scores
is the effect for males, while the coefficient on the interaction between female and mean peer
scores is the difference in the marginal effect for males and females. Columns 1 and 2 present the
across school estimates. In the OLS model, both males and females benefit from attending
schools with higher achieving peers and females benefit more than males. In the OLS model in
column 1, the coefficient on peer scores is 0.931 (se=0.069) and that for the interaction between
peer scores and female is 0.579 (se=0.068). The IV results tell a similar story. The point
estimates in the preferred IV model (with preference and score group fixed effects) in column 2
suggest that males and females who attend a school with 0.2 standard deviations higher peer test
scores will pass 0.2 and 0.27 more exams, respectively. The effect for females is 38 percent
larger than that for males, and the difference is statistically significant at the one percent level.
Columns 4 through 7 present the within school estimates of the direct effect of peers by
gender. The OLS results suggest no direct peer effect for males and large peer effects for
females. In the within-school OLS model (column 3), the coefficient on peer scores is -0.041
(se=0.079) and that for the interaction between peer scores and female is 0.358 (se=0.073). This
suggests that females who attend a school during a time when peer test scores are 0.2 standard
deviations higher would pass 0.064 more exams while males would be unaffected. The within
school IV results (columns 4) tells a similar story to the within school OLS results, but are
noisier. The coefficient on mean peer quality is negative and not statistically significantly
different from zero, while the coefficient on the interaction with female is positive and
statistically significant at the five percent level. The IV coefficient on peer scores is -0.067
(se=0.138) and that for the interaction between peer scores and female is 0.265 (se=0.134).
While the direct effects are imprecisely estimated, they suggest that females who attend a school
during a period when peer test scores are 0.2 standard deviations higher would pass 0.039 more
exams while males would pass 0.0134 fewer exams.
The similarity between the gender differences across the cross-section and the withinschool models is notable. In the instrumental variables models, the gender difference in the
school effect is 0.365 while that for the direct effect of peers is similar at 0.286. In fact, one
cannot reject the null hypothesis that peer effects account for all of the gender gap in response to
17

schools at the 10 percent level.12 The pattern of results suggest that much of the explanation for
gender differences in response to schools (in this and other studies) has to do with females
responding differently to their peers. Females who benefit more from exposure to higher
achieving peers within the same school benefit more than males from attending schools with
high-achieving peers, and the differential peer response may explain all of the differential
response to schools ─ compelling evidence that between 6.67 and 6.67+14.4=21.1 percent of the
school effect can be directly attributable to peers.13 Also, the similarity of the gender differences
in response to schools    , and the gender differences in response to peers  , suggest that the
assumption of additive separability of inputs and peer quality may be a good first approximation.

Relaxing the Additive Separability and Linearity Assumptions: Because the decomposition for
schools on average may mask considerable heterogeneity by school selectivity, I relax the
assumption that selectivity and peer effects are the same across all schools to a more flexible
model that allows for non-linearity and non-additive separability.14 I do this in two ways. First I
present the instrumental variables results broken up by subsamples of similar schools by the level
of peer incoming achievement. The second approach is to present flexible semi-parametric
reduced form estimates of the effects of being assigned to schools with higher-achieving peers
and the effects of increases in assigned peer achievement within assigned schools over time.
These approaches allow one to see if peer quality plays a more important role for certain schools
than others, and if the global estimates pertain to all schools.
Because peer quality and input quality are both higher at high-achieving schools, nonlinearity in the across-school effect could be due to (a) peer effects being non-linear, (b) the
effect of other inputs being non-linear, or (c) complementarity of peer inputs and other inputs. In
contrast, non-linearity in the within-school effects will reflect only (a) and (c). This implies that
similarities in the non-linearity in across-school models and within-school models can provide
further evidence of the importance of peers in explaining school effects. That is, if the acrossschool effects (    ) are largest among schools for which the within-school effects (  ) are
12

The difference between the cross-school difference and the within-school gender difference is 0.08. The standard
errors on the across school and within school gender differences are 0.12 and 0.07, respectively.
13
These numbers are calculated from the fraction of the school effect attributable to peers by gender from Table 5.
14
Because linearity and additive separability are approximated locally by the first order terms of a Taylor expansion
of a continuously differentiable function, one can represent global non-linearity and non-separability by piecewise
linear functions applied to different regions of the data.

18

largest and vice versa, it would imply that direct peer effects are an important component of the
school selectivity effect. I show this below.
The top panel of Table 5 presents the across-school estimates and the second panel
presents the within school-estimates. Within each panel, the top row shows the reduced form
results and the second row presents the instrumental variables results. In columns 1,2, and 3, I
present linear peer effect estimates for different subsamples of schools based on rank (while
controlling for gender, choices, and SEA score). Both the RF and IV estimates suggest that
attending a school with marginally higher-achieving peers has a larger positive effect among
schools with high-achieving peers. The IV across school coefficient on mean peer achievement is
2.526 for the top 30 schools, 0.826 for schools ranked 31 through 90, and 0.542 for the bottom
68 schools (all effects are significant at the 1 percent level).
The lower panel of Table 5 presents within-school estimates of the direct contribution of
peers for the same groups of schools. All models include assigned school fixed effects, and
controls for choices, gender and SEA score.15 The IV within-school coefficient on mean peer
achievement is 1.959 for the top 30 schools (p-value= 0.015), and is statistically insignificant and
small for schools ranked below 30. In words, while increases in peer quality have little or no
effect in most schools, increases in peer quality have a large positive effect on achievement
among the most selective schools.16 The fact that the non-linearity in the school effects track
closely the non-linearity in the direct peer effect suggests that among the top 30 schools, some of
the increased value-added can be attributed to the direct contribution of peers on outcomes.
Consistent with this interpretation, based on the IV results,  / (    ) , the fraction of the
across school effect explained by direct peer influences is 0.78 (p-value=0.03) in the top 30
schools, and is not statistically distinguishable from zero at other schools.
To ensure that these effects are driven by heterogeneity by school selectivity and not
heterogeneity in student achievement, I estimate the within-school model among students with
different levels of incoming test scores (pooling all schools). Columns 4 through 7 present the
results broken up by quartile of the student in incoming SEA scores. None of the within-school
models yield results that are close to statistical significance and the pattern of point estimates are
15

For increased efficiency, I also include interactions of incoming test scores and cohort indicator variables with
gender ─ this has little effect on the point estimates but does reduce the size of the standard errors.
16
While not present in here results by gender yield a similar pattern, however the marginal benefits of peers are
always higher for females than for males.

19

not consistent with the results in columns 1 through 3 ─ suggesting that response heterogeneity
by student ability does not drive the non-linear peer effects.
To provide visual evidence of the nonlinear school selectivity effects, the left panel of
Figure 5 shows the local polynomial fit of the number of exams passed (after taking out the
effects of own incoming test scores, choices and gender) on the mean assigned peer level of the
assigned school (this is a semi-parametric representation of the reduced form). Between -2 and 0,
there are small increases in the number of examinations passed, however among schools with
assigned peer achievement above 0, there are large benefits to attending a school with higherachieving peers ─ consistent Table 5. On the right panel of Figure 5, I show the relationship
between the within-school effect of increases in assigned peer achievement over time and the
mean peer achievement of the school. Specifically, I estimate the reduced form within-school
model for each school βj, and then fit a local polynomial of the estimated βjs to the mean
assigned peer achievement level of the school. The marginal effect of within school increases in
peer quality is highest among high-achievement schools ─ consistent with the regression
evidence. Figure 5 suggests that non-linearity in the selectivity effects are driven, in part, by nonlinearity in the within-school effects ─ evidence that direct peer effects are responsible for much
of the large selectivity effects among selective schools but do not explain selectivity effects for
middle- and low-achievement schools.
While the objective of this paper is to establish how much of the benefits to attending a
selective school can be directly attributed to the quality of the peers at the school, it is helpful to
discuss the policy implications behind the documented non-linear peer effects. Because peer
quality and input quality are both higher at high-achieving schools, the non-linearity of the direct
peer effects either reflect that marginal increases in peer quality within a school are more
effective when peer achievement is already high, or that marginal increases in peer quality within
a school are most effective when input quality is high. Because I do not observe input quality I
am unable to distinguish these two scenarios. This distinction does not affect the interpretation of
the ratio  / (    ) , but it does have direct implications for how improvements in input quality
(or peer quality) may increase school effectiveness. If the non-linearity in the within-school
effect is driven by non-linearity in the marginal effect of peers, it would imply that school valueadded can be increased by increasing input quality at all schools (and the distribution of inputs
across schools would only have distributional effects). It would also imply that one could
20

increase overall achievement by stratifying students across schools by ability. However, if the
non-linearity in the direct peer effect reflects complementarity between peer quality and other
inputs, it would imply that the marginal effect of improved inputs will be highest at schools with
the highest achieving students. It would also imply that overall education output would be
highest if high ability students attend schools with the best inputs. Despite clear policy
implications, data limitations preclude rigorous investigation into the source of the non-linearity.

Intensive or Extensive Margin?: While the number of exams passed is a good measure of overall
academic achievement, one may wonder if these effects are driven by students being less likely
to drop out at schools that have higher achieving peers or due to improvements in outcomes
conditional on taking the CSEC exams. To get a sense of this, I re-estimate the main preferred
specifications using "taking the CSEC exams" as the dependent variable. In the cross-school
model the IV estimate is small and not statistically significant ─ indicating that most, if not all,
of the cross-school effect was on the intensive margin, as found in Jackson (2010). Similarly, the
within-school model of CSEC taking yields a very small statistically insignificant point estimate.
Even at the top 30 schools where there are large positive effects on the number of exams passed,
the coefficient on taking the CSEC exams is a small and statistically insignificant─ suggesting
that much of the direct peer effect is on intensive margin.

V.2

Robustness Checks
While there are a priori reasons to believe that the results presented reflect true causal

effects, there remain lingering concerns. I present these concerns and address them in turn.
(1)

The difference in difference variation may not be clean: Given that the source of the

exogenous variation exploited in this paper is driven in part by the test score cut-offs, it is
helpful to show that the main cross sectional results are robust to exploiting this discontinuity
only, and not relying on explicit controls for school choices. To do this, I create three variables
that denote whether a student scores above the simulated cut-off for their first second, and third,
choice schools. I then use these three variables as instruments en lieu of the assigned peer
quality. The results are presented in Table 6. In the first stage, scoring above the cut-off for the
first, second and third choice schools are associated with attending school with peers with 0.033,
0.056 and 0.076 standard deviations higher incoming test scores, respectively. The reduced form
21

regression indicates that scoring above the cut-off for the first, second and third choice schools
are associated with passing -0.003 (se=0.44), 0.052 (se=0.36) and 0.072 (0.033) more exams,
respectively. Using these three variables as instruments yields a coefficient on mean peer quality
of 0.867 (se=0.254) ─ similar to the estimates obtained in Table 3. As a formal test that the
assigned peer quality yield similar results to the cut-off instruments, I include the assigned peer
quality as an additional instrument. In such a model (column 5) the coefficient is 1.009
(se=0.074), and the test of overidentifying restrictions yields a p-value of 0.79 ─ indicating that
the instrumental variables results based on the DID variation in assigned peer quality are
consistent with discontinuity based instruments that rely only on variation due to the cut-offs.17
(2)

The estimated peer effects may be spurious: I argue that the gender differences in

response to peers and the differences by the school rank reflect a true causal relationship. As a
further test of the validity of the results, I implement a test similar to Jackson and Bruegmann
(2009) and Lavy and Schlosser (2009) where I include the current peers (for which there should
be a true treatment effect) and the peer quality of the preceding cohort and the following cohort
(for which there should be little to no effect). The estimates that include current peers and peer
quality in the following and preceding cohorts are presented in Table 7. To test for gender
differences I show the coefficient on current peers and peer quality in the following and
preceding cohorts interacted with whether the student is female. For all models, one rejects the
null hypothesis that the subsequent peers and the preceding peers are jointly statistically
significant at the 20 percent level. However, similar to Tables 4 and 5, one can reject the null
hypothesis that current peer quality has zero effect among the top 30 schools (column 1) and the
null hypothesis that females and males have the same response to peers (column 4) at the 10
percent level. While the p-value on the contemporaneous effects for these two models is below
0.1, those for the joint significance of the lag and lead are above 0.7 for both models. If we take
the conservative view that the effect on the lag reflects some underlying spurious association,
then we could subtract that from the contemporaneous effect to obtain a conservative causal
estimate. Doing this for the female interaction results in a conservative reduced form peer effect
estimate of 0.085. This is about two thirds of the reduced form estimate obtained in the preferred
model in Table 4 ─ suggesting that there is a true gender difference. The same calculation for the

17

Note that a test for balance indicates that all observable covariates (including selectivity of choices) are smooth
through the simulated cut-off.

22

top 30 schools yields a conservative reduced form current peer effect estimate of 0.99. This is
about 70 percent of the within school reduced form coefficient in Table 5. This conservative
estimate implies that 37 percent (as opposed to 78 percent) of the across school effect among the
top quartile of schools can be attributed to peers.
(3)

The peer effects may be driven by sampling variation: In difference in difference models

there is always the concern that inference based on estimated effects could be biased by
underlying serial correlation in the data. To asses this problem I follow an approach used by
Bertrand, Duflo and Mullainathan (2004). Specifically I create placebo treatments by taking each
school and rearranging the actual peer achievement values for a given cohort so that the actual
peer achievement are not lined up with the corresponding outcome for that year. I estimate the
placebo treatments based on 100 replications of this reshuffling. I compare the actual estimates
to the distribution of placebo estimates. Since the gender differences and the positive effect of
peers among the top 30 school are the estimates that are statistically significant, I test these two
models. In both cases, none of the 100 replications yielded parameter estimates larger than the
actual estimated coefficients, suggesting that the estimates obtained were not some artifact of the
sample and would not have been obtained merely due to sampling variation.
(4)

Changes in peer quality within schools could be correlated with changes in input

quality within schools: Becuase more desirable schools attract higher-achieving students and
therefore have brighter peers, one may worry that improvements in input quality at a particular
school in a particular year may cause students to rank that school more highly in their preference
lists generating a correlation between changes in input quality and changes in peer quality within
a school over time. While I do not observe input quality directly, all scenarios where changes in
inputs lead to changes in the peer quality and vice versa involve schools moving up or down the
rankings in desirability and therefore peer quality. I can test for this possibility directly. To show
that this is not a source of bias, I show that such changes in school rankings from the assignment
mechanism essentially do not occur in these data. Table 8 shows the correlation between a
school's rank in simulated cut-off scores across years. The correlation between a school's rank
across any two adjacent years in the data is at least 0.98 and the correlation between a school's
rank in 1995 and seven years later in 2002 is 0.96 ─ so that systematic changes in school
rankings are not driving the variation in assigned peer achievement within schools over time.
As a more direct test, I test for a correlation between changes in mean assigned peer
23

achievement and teacher quality (one of the most important school inputs). Specifically, I run
regressions of mean teacher characteristics on the mean SEA score of students assigned to the
school while including both cohort and school fixed effects. The results (Table 9) indicate no
relationship between changes in peer quality and changes in teacher quality. Taken together the
results show that changes in peer quality within schools are due to idiosyncratic shocks around a
long-run mean that are not correlated with changes in other school inputs within schools.

VI

Conclusions
There is a growing body of evidence based on credible research designs indicating that

attending selective schools may improve student outcomes. However, we have little understand
of why. Using a unique dataset from Trinidad and Tobago, I investigating the extent to which the
positive selective school effects can be attributed to selective schools providing higher-achieving
contemporaneous peers. Using a carefully selected group of students where there is no selfselection of students to assigned schools or assigned peers, I attempt to overcome a variety of
econometric obstacles to estimating credible school selectivity effects and direct peer effects on

the same student population.
Using instrumental variables strategies, I find that attending a school with higherachieving peers is associated with substantial improvements in academic outcomes. However, on
average, improvements in incoming peer achievement within a school are associated with small,
improvements. The point estimates suggest that, on average, between 7 and 14 percent of the
school effect can be directly attributed to peer quality differences across schools. Echoing other
studies, the marginal effects of attending a school with higher achieving peers are larger for
females than for males. I find that the gender differences in response to peers can account for all
of the gender differences in response to schools ─ evidence that part of the school effect can be
explained by the direct contribution of peers. I also find substantial non-linearity in the effects.
Similar to Ding and Lehrer (2007) and Pop-Eleches and Urquiola (2008), the marginal effect of
attending a more selective school is greatest among the most selective schools. Looking at the
direct effect of peers, this non-linear school selectivity effect appears to be driven by the fact that
the marginal effect of improvements in peer achievement within a school is largest at selective
schools ─ further evidence that direct peer effects are responsible for some of the effect of
attending schools with higher-achieving peers. The symmetry in the non-linearity leads me to
24

conclude that that while direct peer effects may explain little of the benefits to attending a more
selective school among the bottom three-quarters of schools, at least one-third of the benefits to
attending a more selective school among the top quarter of schools can be attributed directly to
the achievement level of the peers.
The finding that at least one-third of the estimated school selectivity effect can be directly
explained by peer achievement for the top quartile of schools is sobering because it implies that
very little of the large estimated success at these selective schools can be scaled up to all schools.
These findings underscore the fact that identifying highly successful schools may not be
informative about how to improve outcomes for the average school. However, the finding that
peer achievement explains almost none of the benefits to attending a more selective school
among the bottom three-quarters of schools suggest that the relative successes at average schools
may be scalable to low-performing schools.
Owing to the uniqueness of the institutional setup in Trinidad and Tobago, this paper is
the first to rely on independent exogenous variation in schools attended and peer quality on the
same student population ─ allowing one to credibly decompose a school selectivity effect and
estimate the extent to which positive selective school effects can be attributed to selective
schools providing higher-achieving contemporaneous peers. The findings highlight the
importance of understanding the mechanisms through which selective schools may improve
student outcomes.

25

Bibliography
1. Abdulkadiroglu, Atila, Joshua Angrist, Susan Dynarski, Thomas Kane, and Parag Pathak.
Informing the Debate: Comparing Boston's Charter,Pilot, and Traditional Schools". The
Boston Foundation , 2009.
2. Ammermueller, Andreas, and Jörn-Steffen Pischke. "Peer Effects in European Primary
Schools: Evidence from PIRLS." Institute for the Study of Labor (IZA) Discussion Papers
2077., 2006.
3. Bertrand, Marianne, Esther Duflo, and Sendhil Mullainathan. ""How Much Should We Trust
Differences-in-Differences Estimates?"." Quarterly Journal of Economics 119, no. 1 (2004):
249-75.
4. Clark, Damon. "Elite Schools and Academic Performance." mimeo, 2007.
5. Cross, S. E., and L. Madson. "Models of the self: Self-construals and gender." Psychological
Bulletin, no. 122 (1997): 5–37.
6. Deming, David, Justine Hastings, Tom Kane, and Douglas Staiger. "School Choice and
College Attendance: Evidence from Randomized Lotteries." Yale University mimeo, 2010.
7. Ding, Weili, and Steven F. Lehrer. "Do Peers Affect Student Achievement in China's
Secondary Schools?" Review of Economics and Statistics 89, no. 2 (2007): 300-312.
8. Dobbie, W., and R. G. Fryer (2011): "Exam High Schools and Academic Achievement:
Evidence from New York City," NBER Working Paper 17286.
9. Eagly, A. H. "Sex differences in influenceability." Psychological Bulletin, no. 85 (1978): 86116.
10. Han, Li, and Tao Li. "The gender difference of peer influence in higher education."
Economics of Education Review 28, no. 1 (February 2009): 129-143.
11. Hanushek, Eric, John Kain, Jacob Markman, and Steven Rivkin. "Does peer ability affect
student achievement?" Journal of Applied Econometrics 18, no. 5 (2003): 527-544.
12. Hastings, Justine S., Thomas Kane, and Douglas Staiger. "Gender, Performance and
Preferences: Do Girls and Boys Respond Differently to School Environment? Evidence
fromSchool Assignment by Randomized Lottery." American Economic Review Papers and
Proceedings 96, no. 2 (2006): 232-236.
13. Hastings, Justine, and Jeffrey Weinstein. "No Child Left Behind: Estimating the Impact on
Choices and Student Outcomes." National Bureau of Economic Research Working Paper,
2007.
14. Hoxby, Caroline M, and Gretchen Weingarth. "Taking Race Out of the Equation: School
Reassignment and the Structure of Peer Effects." mimeo, 2006.
15. Hsieh, C.-T., and M. Urquiola (2006): "The effects of generalized school choice on
achievement and stratification: Evidence from Chile's voucher program," Journal of Public
Economics, 90, 1477-1503.
16. Jackson, C. Kirabo. (2010), Do Students Benefit from Attending Better Schools? Evidence
from Rule-based Student Assignments in Trinidad and Tobago." The Economic Journal 120
(2010): 1399–1429.
26

17. Jackson, C. Kirabo. "Student Demographics, Teacher Sorting, and Teacher Qualty: Evidence
From the End of School Desegregation." Journal of Labor Economics 27, no. 2 (2009): 213256.
18. Jackson, C. Kirabo, and Elias Bruegmann. "Teaching Students and Teaching Each Other:
The Importance of Peer Learning for Teachers." American Economic Journal: Applied
Economics 1, no. 4 (2009).
19. Jackson, C. Kirabo. "Single-Sex Schools, Student Achievement, and Course Selection:
Evidence from Rule-Based Student Assignments in Trinidad and Tobago" Journal of Public
Economics February 2012, Pages 173-187.
20. Lavy, Victor, and Analía Schlosser. "Mechanisms and Impacts of Gender Peer Effects at
School." working paper, 2009.
21. Lavy, Victor, Olmo Silva, and Felix Weinhardt. "The Good, The Bad and The Average:
Evidence on the Scale and Nature of Ability Peer Effects in School." NBER Working Paper
No. 15600, 2010.
22. Maccoby, E. E., and C. N. Jacklin. The psychology of sex differences. Stanford: Stanford
University Press, 1974.
23. Pop-Eleches, Christian, and Miguel Urquiola. "The Consequences of Going to a Better
School." mimeo, 2008.

27

Tables and Figures
Table 1:

Summary Statistics

Variable
Total SEA score
Female
Take the CSEC Exams
Number of Exams Passed
Certificate a
Cohort size
% Teacher with BA
% Teacher 0 to 3 yrs
% Teachers 4 to 20 yrs
% Teacher 20 plus

Rank of School in Mean incoming SEA scores
1 to 30
31 through 90
Above 90
1.102
0.122
-0.561
(0.333)
(0.693)
(0.654)
0.490
0.499
0.505
(0.500)
(0.500)
(0.500)
0.871
0.706
0.586
(0.335)
(0.456)
(0.493)
4.444
1.939
1.005
(2.628)
(2.308)
(1.746)
0.508
(0.500)
119.1
(63.6)
0.584
(0.232)
0.137
(0.093)
0.399
(0.169)
0.309
(0.153)

0.128
(0.334)
439.7
(218.5)
0.552
(0.197)
0.162
(0.120)
0.418
(0.111)
0.356
(0.188)

0.037
(0.189)
443.9
(241.9)
0.360
(0.248)
0.240
(0.136)
0.402
(0.113)
0.279
(0.145)

Observations
17811
84746
48144
a. Certificate is variable that is equal to 1 if the student passes five CSEC exams including
English and Mathematics.
Note that the teacher variables are only available for 2000, 2001, and 2002.

28

Table 2:

Falsification Tests and Implied Bias
Cross-Section Bias tests

Dependent Variable
Mean Total

Mean Total

Religions
1
2
3
4
0.001
0
-0.005 0.009
[0.004] [0.006] [0.005] [0.006]

5
-0.008
[0.005]

SEA Attempts
attempts
0.013
[0.007]+

Districts
1
2
3
4
5
6
7
8
-0.003 -0.003 0.004 <0.00000 -0.002 -0.001 <0.0000 <0.0000
[0.003] [0.002] [0.003] [0.002] [0.002] [0.005 [0.000] [0.000]

Within-School Bias tests
Dependent Variable
Mean Total

Religions
1
2
3
4
-0.004 <0.00000 0.019 -0.014
[0.005] [0.008] [0.015] [0.012]

5
<0.0000
[0.009]

SEA Attempts
attempts
0.005
[0.012]

Districts
1
2
3
4
5
6
7
8
Mean Total
0.004
0.008 -0.024 -0.007
0.004 -0.021 <0.0000 <0.0000
[0.007] [0.008] [0.018] [0.004]+ [0.013] [0.016 [0.000] [0.000]
Robust standard errors in brackets
+ significant at 10%; * significant at 5%; ** significant at

29

Table 3:

Main Results
Effects on the Number of Exams Passed: Full Sample
2
3
4
5
6
Cross sectional results
Within School results
1.044
1.229
1.235
0.14
0.144
0.135
[0.082]** [0.073]** [0.073]**
[0.067]* [0.067]* [0.072]*

7

Ratiob

0.141
[0.074]*

0.114
[0.058]*

Assigned (RF)

0.437
[0.062]**

0.57
[0.057]**

0.574
[0.057]**

0.039
[0.058]

0.043
[0.059]

0.038
[0.079]

0.047
[0.079]

0.082
[0.077]

Actual (2SLS)

0.851
[0.124]**
578.48

1.171
[0.094]**
390.3

1.177
[0.092]**
396.23

0.07
[0.105]
666.88

0.076
[0.105]
453.67

0.069
[0.144]
513.023

0.085
[0.144]
519.67

0.072
[0.122]

YES

YES
YES

YES
YES
YES

YES

YES

YES
YES

YES

YES
YES

YES
YES
YES
YES

‐
‐
‐
‐
‐
‐
‐

1

Actual (OLS)

First stage F-Statistic
Cohort Fixed Effect?
Preference Effects? a
Score Group Dummies?
School Fixed Effects?

YES

Observations
150701
150695
150695
150701
150701
150695
150695
R-squared
0.39
0.62
0.62
0.41
0.42
Robust standard errors in brackets. Standard errors are adjusted for clustering at the assigned school level.
+ significant at 10%; * significant at 5%; ** significant at 1%
a. note that preferences include gender so that all models with preference fixed effect are within both preference and gender.
b. The estimate of β/(β+πδ) ─ the coefficient in column 7 divided by the coefficient in column 3. The standard error was computed
by stacking the data to estimate both the within and across model simultaneously and then using the delta method.

Table 4:

Testing for Gender Differences

Effects on the Number of Exams Passed: Effects by Gender
Across schools
Within schools
OLS
2SLS
OLS
2SLS
1
2
3
4
Peer SEA scores
0.931
1.005
-0.041
-0.067
[0.069]**
[0.101]**
[0.079]
[0.138]
Female*Peer SEA scores
0.579
0.365
0.358
0.265
[0.068]**
[0.121]**
[0.073]**
[0.134]*
Female Effect
1.509
1.37
0.317
0.198
[se]
[0.086]**
[0.116]**
[0.081]**
[0.128]
Cohort Fixed Effect?
YES
YES
YES
YES
Choice Effects?
YES
YES
YES
YES
Score Group Dummies?
YES
YES
YES
YES
School Fixed Effects?
YES
YES
YES
Observations
150695
150695
150695
150695
Robust standard errors in brackets. Standard errors are adjusted for clustering at the assigned
+ significant at 10%; * significant at 5%; ** significant at 1%

30

Table 5:

Effect by School Selectivity
Dependent Variable is the Number of Exams Passed
Across School Variation

Reduced form

Mean peer scores (2SLS)

1

2

3

4

5

6

Schools
rank 1-30
2.701
[0.366]**

Schools
rank 3190
0.453
[0.081]**

Schools
rank 91+
0.313
[0.075]**

Students
in top
SEA
quartile
1.308
[0.142]**

Students
in third
SEA
quartile
0.466
[0.075]**

Students
in second
SEA
quartile
0.228
[0.088]**

7
Students
in
bottom
SEA
quartile
0.028
[0.032]

2.526
[0.323]**

0.826
[0.161]**

0.543
[0.168]**

2.457
[0.262]**

1.046
[0.171]**

0.526
[0.183]**

0.079
[0.058]

-0.164
[0.121]

-0.06
[0.243]

Reduced form

1.186
[0.563]*

-0.0165
[0.099]

Within School Variation
0.047
<0.001
-0.092
[0.118]
[0.245]
[0.162]

Mean peer scores (2SLS)

1.959
[0.797]*

-0.163
[0.184]

0.078
[0.215]

0.084
[0.490]

-0.166
[0.292]

-0.329
[0.232]

-0.042
[0.087]

Observations

17811

84740

48144

26454

50348

42249

27521

Ratio (2SLS)

0.78

-0.197

0.145

0.034

-0.16

-0.626

-0.537

[se]
[0.33]*
[0.225]
[0.399]
[0.200]
[0.280]
[0.492]
[1.17]
Robust standard errors in brackets. Standard errors are adjusted for clustering at the assigned school level.
+ significant at 10%; * significant at 5%; ** significant at 1%.
All models include preference ordering fixed effects, and control for the total SEA score, its quadratic and its
cubic, and gender.

31

Effects Using Discontinuity Variation Only

Table 6:

2SLS Results using Scoring Above a Simulated Cut-off for a Preferred School as Instruments
1
2
3
4
Depended Variable
Mean Peer Score (actual)
Above first choice cut-off
Above second choice cut-off
Above third choice cut-off

Polynomial order of Total
Cohort fixed Effects
Preference fixed effects

5

Assigned Mean
Peer Scores
OLS
0.049
[0.021]*
0.07
[0.022]**
0.14
[0.023]**

Actual Mean
Peer Scores
OLS
0.033
[0.016]*
0.056
[0.015]**
0.076
[0.016]**

Exams
Passed
OLS
-0.003
[0.044]
0.054
[0.033]+
0.072
[0.033]*

Exams
Passed
2SLS
0.867
[0.254]**
-

Exams
Passed
2SLS
1.009
[0.074]**
-

5
YES
YES

5
YES
YES

5
YES
YES

5
YES
YES

5
YES
YES

-

-

-

0.63

0.79

150695

114062

114062

P-value of J-Stat

Observations
150695
150695
Robust standard errors in brackets
+ significant at 10%; * significant at 5%; ** significant at 1%

Column 4 includes scoring above the cut-offs as excluded instruments. Column 5 includes scoring above the cutoffs and simulated peer quality as excluded instruments.

Table 7:

Placebo Peer Treatments
Dependent Variable is the Number of Exams Passed
1
2
3

Sample:
Peersc-1
Peers
Peersc+1
Current minus lag
Current minus lead

Schools
rank 1-30
0.735
[1.043]
1.733
[0.991]+
0.109
[0.713]

Schools
rank 31-90
-0.06
[0.102]
-0.17
[0.112]
0.027
[0.092]

Schools
rank 91+
-0.231
[0.224]
-0.223
[0.142]
0.319
[0.200]

0.997
1.623

-0.109
-0.143

0.008
-0.542

Female*Peersc-1
Female*Peers
Female*Peersc+1

4
All
0.094
[0.069]
0.1793
[0.010]+
0.057
[0.135]
0.0853
0.1223

P-value on lag and lead
0.7
0.75
0.23
0.73
P-value on current
0.13
0.14
0.08
0.09
Robust standard errors in brackets are adjusted for clustering at the assigned school level. P-values in parentheses.
+ significant at 10%; * significant at 5%; ** significant at 1%.
All models include preference fixed effects, and control for the total SEA score, its quadratic and its cubic, and gender.
Model 6 also includes the first order effect of the lag and lead of peer quality.

32

Table 8:
Rank in
1995
1996
1997
1998
1999
2000
2001
2002

Table 9:

Correlations between schools' ranks across years
1995
1
0.993
0.9855
0.9838
0.9779
0.9724
0.9622
0.9618

1996

1997

1998

1999

2000

2001

2002

1
0.9914
0.9888
0.9835
0.9793
0.971
0.9697

1
0.9929
0.991
0.981
0.9717
0.9701

1
0.9933
0.9877
0.9754
0.9721

1
0.9875
0.9754
0.9736

1
0.9833
0.9824

1
0.9951

1

Relationship between changes in peer quality and changes in teacher quality

Mean total SEA of assigned students

Assigned School fixed Effects
Cohort Fixed Effects

1

2

3

4

% Teachers with
1 -3 years
experience
-0.01
[0.022]

% Teachers
with 1 -3 years
experience
-0.001
[0.041]

% Teachers
with 1 -3 years
experience
-0.08
[0.059]

% Teachers
with a BA
degree
0.004
[0.097]

Y
Y

Y
Y

Y
Y

Y
Y

Observations
25962
25962
25962
Robust standard errors in brackets are adjusted for clustering at the school level.
+ significant at 10%; * significant at 5%; ** significant at 1%

33

25962

Likelihood of being assigned to first choice school
0
.2
.4
.6
.8
1

Discontinuity in Assigned School

-400

-200
0
200
SEA score relative to simulated cut-off of first choice school

Figure 1: Likelihood of being Assigned to One's Top Choice School

Figure 2: Illustration of the Difference in Difference Variation

34

400

0

Kernel Density of Total SEA Score
.1
.2
.3
.4

.5

Distribution of Test Scores Accross Cohorts

-2

-1

0
SEA Score

1995

Figure 3:

1

1996

1997

2
1998

Distribution of SEA Scores Across Cohorts

0

.5

1

1.5

2

Mean SEA Scores Within Student Rank Groups Accross Cohorts

Rank 1 to 100

Rank 3000 to 3500
1995

1996

Rank 10000 to10500
1997

1998

-.1

-.05

0

.05

Change in Mean SEA Scores Within Student Rank Groups Accross Cohorts

Rank 1 to 100

Rank 3000-3500
1995-1996

Rank 10000 to 10500

1996-1997

1997-1998

Figure 4: Hypothetical Changes in Mean Test Scores Driven Only by Changes in SEA Test score
Distribution Across Cohorts.

35

Within-School Coefficients on Peer Achievment
By Mean Peer Achievment at School

-1

-.1

0

-.5

.1

.2

0

.3

.4

.5

.5

Mean Residual of Number of Exams Passed
By Mean Peer Achievment at School

-2

-1
0
1
Mean Peer Achievment at School
90% CI

-2

-1
0
1
Mean Peer Achievment at School

Fit: mean residuals

Local polynomial fit with 90% confidence interval

Figure 5:

2

90% CI

Fit: beta

Local polynomial fit with 90% confidence interval

Graphical evidence of non-linear peer effects

36

2

Appendix

1

Distribution of mean SEA scores of school choices

Empirical cumulative density
.2
.4
.6
.8

Fourth Choice

Third Choice
Top Choice

0

Second Choice

-2

-1

0
1
Mean SEA score of school choice

2

Appendix Figure 1: Distribution of Peer Quality by School Choice Rank

0

Kernel Density of Total SEA Score
.5
1
1.5

Distributions of Total SEA Scores by School Rank

-2

-1

0

1

2

x
Rank 1 to 30
Rank 91 to 158

Rank 31 to 90

Appendix Figure 2: Distribution of total SEA scores by school rank

37

Assigned Peer Quality at Cut-off for First and Second Choice School
200

Second Choice

-200

-200

-100

-100

0

0

100

100

200

First Choice

-400

-200
0
200
Score Relative to Simulated Cut-off

Assigned Peers (residual)

400

-400

Local Polynomial Fit

-200
0
200
Score Relative to Simulated Cut-off

Assigned Peers (residual)

400

Local Polynomial Fit

Appendix Figure 3: Discontinuity in assigned peer quality (raw SEA scores are shown)
through the assigned cut-off for the first and second choice school.

Appendix Note 1: Constructing the Simulated Cut-off
The simulated cut-offs are constructed sequentially as follows: (1) All secondary school sizes are
fixed based on capacity, (2) all students are put in the applicant pool for their top choice school,
(3) the school for which the first rejected student has the highest test score fills all its slots (with
the highest scoring students who listed that school as their first choice), (4) the students who
were rejected from the top choice school are placed back into the applicant pool and their second
choice school becomes their first choice school, (5) Steps 2 through 5 are repeated, after
removing previously assigned students and school slots until the lowest ranked school is filled.
The only difference between how students are actually assigned and the “tweaked” rule-based
assignment is that at step (3) the “tweaked” rule does not allow any students to be hand-picked
while, in fact, some students are hand-picked by principals only at Government assisted schools.
Jackson (2009, 2010) exploits the discontinuities inherent in the assignment mechanisms to
identify the effect of attending schools with higher achieving peers. In this paper, I use the school
assignments (to government schools) that are not driven by any gaming or selection.

38

