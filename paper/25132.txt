                               NBER WORKING PAPER SERIES




       MATRIX COMPLETION METHODS FOR CAUSAL PANEL DATA MODELS

                                         Susan Athey
                                        Mohsen Bayati
                                      Nikolay Doudchenko
                                         Guido Imbens
                                      Khashayar Khosravi

                                       Working Paper 25132
                               http://www.nber.org/papers/w25132


                     NATIONAL BUREAU OF ECONOMIC RESEARCH
                              1050 Massachusetts Avenue
                                Cambridge, MA 02138
                                    October 2018




We are grateful for comments by Alberto Abadie and participants at the NBER Summer Institute
and at seminars at Stockholm University and the California Econometrics Conference. This
research was generously supported by ONR grant N00014-17-1-2131 and NSF grant
CMMI:1554140. The views expressed herein are those of the authors and do not necessarily
reflect the views of the National Bureau of Economic Research.

At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w25132.ack

NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.

Â© 2018 by Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar
Khosravi. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted
without explicit permission provided that full credit, including Â© notice, is given to the source.
Matrix Completion Methods for Causal Panel Data Models
Susan Athey, Mohsen Bayati, Nikolay Doudchenko, Guido Imbens, and Khashayar Khosravi
NBER Working Paper No. 25132
October 2018
JEL No. C01,C21,C23

                                         ABSTRACT

In this paper we study methods for estimating causal effects in settings with panel data, where a
subset of units are exposed to a treatment during a subset of periods, and the goal is estimating
counterfactual (untreated) outcomes for the treated unit/period combinations. We develop a class
of matrix completion estimators that uses the observed elements of the matrix of control
outcomes corresponding to untreated unit/periods to predict the â€œmissingâ€ elements of the matrix,
corresponding to treated units/periods. The approach estimates a matrix that well-approximates
the original (incomplete) matrix, but has lower complexity according to the nuclear norm for
matrices. From a technical perspective, we generalize results from the matrix completion
literature by allowing the patterns of missing data to have a time series dependency structure. We
also present novel insights concerning the connections between the matrix completion literature,
the literature on interactive fixed effects models and the literatures on program evaluation under
unconfoundedness and synthetic control methods.

Susan Athey                                     Guido Imbens
Graduate School of Business                     Graduate School of Business
Stanford University                             Stanford University
655 Knight Way                                  655 Knight Way
Stanford, CA 94305                              Stanford, CA 94305
and NBER                                        and NBER
athey@stanford.edu                              Imbens@stanford.edu

Mohsen Bayati                                   Khashayar Khosravi
Graduate School of Business                     Department of Electrical Engineering
Stanford University                             Stanford University
Stanford, CA 94305                              Stanford, CA 94305
bayati@stanford.edu                             khosravi@stanford.edu

Nikolay Doudchenko
Graduate School of Business
Stanford University
Stanford, CA 94305
nikolayd@stanford.edu
1     Introduction
In this paper we develop new methods for estimating average causal effects in settings with
panel or longitudinal data, where a subset of units is exposed to a binary treatment during
a subset of periods, and we observe the realized outcome for each unit in each time period.
To estimate the (average) effect of the treatment on the treated units in this setting, we
focus on imputing the missing potential outcomes. The statistics and econometrics litera-
tures have taken two general approaches to this problem. The literature on unconfounded-
ness (Rosenbaum and Rubin (1983); Imbens and Rubin (2015)) imputes missing potential
outcomes using observed outcomes for units with similar values for observed outcomes in
previous periods. The synthetic control literature (Abadie and Gardeazabal (2003); Abadie
et al. (2010, 2015); Doudchenko and Imbens (2016)) imputes missing control outcomes for
treated units by finding weighted averages of control units that match the treated units in
terms of lagged outcomes. Although at first sight similar, the two approaches are concep-
tually quite different in terms of the patterns in the data they exploit to impute the missing
potential outcomes. The unconfoundedness approach estimates patterns over time that are
assumed to be stable across units, and the synthetic control approach estimates patterns
across units that are assumed to be stable over time. Both sets of methods also primarily
focus on settings with different structures on the missing data or assignment mechanism.
In the case of the unconfoundedness literature typically the assumption is that the treated
units are all treated in the same periods, typically only the last period, and there are a sub-
stantial number of control units. The synthetic control literature has primarily focused on
the case where one or a small number of treated units are observed prior to the treatment
over a substantial number of periods.
    In this study we also draw on the econometric literature on factor models and interactive
fixed effects, and the computer science and statistics literatures on matrix completion, to


                                              2
take an approach to imputing the missing potential outcomes that is different from the
unconfoundedness and synthetic control approaches. In the literature on factor models
and interactive effects (Bai and Ng (2002); Bai (2003)) researchers model the observed
outcome, in a balanced panel setting, as the sum of a linear function of covariates and
an unobserved component that is a low rank matrix plus noise. Estimates are typically
based on minimizing the sum of squared errors given the rank of the matrix of unobserved
components, sometimes with the rank estimated. Xu (2017) applies this to causal settings
where a subset of units is treated from common period onward, so that the complete data
methods for estimating the factors and factor loadings can be used. The matrix completion
literature (CandeÌ€s and Recht (2009); CandeÌ€s and Plan (2010); Mazumder et al. (2010))
focuses on imputing missing elements in a matrix assuming the complete matrix is the sum
of a low rank matrix plus noise and the missingness is completely at random. The rank of
the matrix is implicitly determined by the regularization through the addition of a penalty
term to the objective function. Especially with complex missing data patterns using the
nuclear norm as the regularizer is attractive for computational reasons.
   In the current paper we make two contributions. First, we generalize the methods from
the matrix completion literature to settings where the missing data patterns are not com-
pletely at random. In particular we allow for the possibility of staggered adoption (Athey
and Imbens (2018)), where units are treated from some initial adoption date onwards,
but the adoption dates vary between units. Compared to the factor model literature the
proposed estimator focuses on nuclear norm regularization to avoid the computational dif-
ficulties that would arise for complex missing data patterns with the fixed-rank methods in
Bai and Ng (2002) and Xu (2017), similar to the way LASSO (`1 regularization, Tibshirani
(1996)) is computationally attractive relative to subset selection (`0 regularization) in linear
regression models. The second contribution is to show that the synthetic control and un-
confoundedness approaches, as well as our proposed method, can all be viewed as matrix

                                               3
completion methods based on matrix factorization, all with the same objective function
based on the FroÌˆbenius norm for the difference between the latent matrix and the observed
matrix. Given this common objective function the unconfoundedness and synthetic control
approaches impose different sets of restrictions on the factors in the matrix factorization,
whereas the proposed method does not impose any restrictions but uses regularization to
define the estimator.



2     Set Up
Consider an N Ã— T matrix Y of outcomes with typical element Yit . We only observe Yit
for some units and some time periods. We define M to be the set of pairs of indices (i, t),
i âˆˆ {1, . . . , N }, t âˆˆ {1, . . . , T }, corresponding to the missing entries and O to be the set
corresponding to the observed entries: Yit is missing if (i, t) âˆˆ M and observed if (i, t) âˆˆ O.
We wish to impute the missing Yit . Our motivation for this problem arises from a causal
potential outcome setting (e.g., Rubin (1974); Imbens and Rubin (2015)), where for each
of N units and T time periods there exists a pair of potential outcomes, Yit (0) and Yit (1),
with unit i exposed in period t to treatment Wit âˆˆ {0, 1}, and the realized outcome equal
to Yit = Yit (Wit ). In that case the primary object of interest may be the average causal
                               P
effect of the treatment, Ï„ = i,t [Yit (1) âˆ’ Yit (0)]/(N T ), or some other average treatment
effect. In order to estimate such average treatment effects, one approach is to impute the
missing potential outcomes. In this paper we focus directly on the problem of imputing
the missing entries in the Y(0) matrix for treated units with Wit = 1.
    In addition to partially observing the matrix Y, we may also observe covariate matrices
X âˆˆ RN Ã—P and Z âˆˆ RT Ã—Q where columns of X are unit-specific covariates, and columns of
Z are time-specific covariates. We may also observe unit/time specific covariates Vit âˆˆ RJ .
    Putting aside the covariates for the time being, the data can be thought of as consisting


                                                4
of two N Ã— T matrices, one incomplete and one complete,
             ï£«                                   ï£¶                         ï£«                         ï£¶
                 Y11   Y12    ?     ...    Y1T                                 0 0 1 ...        0
            ï£¬                                    ï£·                   ï£¬                               ï£·
            ï£¬ ?         ?    Y23    ...     ?                                  1 1 0 ...        1
            ï£¬                                    ï£·                   ï£¬                               ï£·
                                                 ï£·                   ï£¬                               ï£·
            ï£¬                                    ï£·                   ï£¬                               ï£·
        Y = ï£¬ Y31       ?    Y33    ...     ?    ï£·,          and W = ï£¬         0 1 0 ...        1    ï£·,
            ï£¬                                    ï£·                   ï£¬                               ï£·
            ï£¬ ..        ..    ..            ..                                 .. .. .. . .     ..
            ï£¬                                    ï£·                   ï£¬                               ï£·
                                    ..
                                       .                                                    .
                                                 ï£·                   ï£¬                               ï£·
            ï£¬ .          .     .             .   ï£·                   ï£¬          . . .            .   ï£·
            ï£­                                    ï£¸                   ï£­                               ï£¸
              YN 1      ?    YN 3 . . .     ?                                  0 1 0 ...        1

where                                   ï£±
                                        ï£² 1               if (i, t) âˆˆ M,
                                  Wit =
                                        ï£³ 0               if (i, t) âˆˆ O,

is an indicator for Yit being missing.



3    Patterns of Missing Data, Thin and Fat Matrices,
     and Horizontal and Vertical Regression
In this section, we discuss a number of particular configurations of the matrices Y and W
that are the focus of distinct parts of the general literature. This serves to put in context
the problem, and to motivate previously developed methods from the literature on causal
inference under unconfoundedness, the synthetic control literature, and the interactive fixed
effect literature, and subsequently to develop formal connections between all three. First,
we consider patterns of missing data. Second, we consider different shapes of the matrix
Y. Third, we consider a number of specific analyses that focus on particular combinations
of missing data patterns and shapes of the matrices.




                                                      5
3.1     Patterns of Missing Data
In the statistics literature on matrix completion the focus is on settings with randomly miss-
ing values, allowing for general patterns on the matrix of missing data indicators (CandeÌ€s
and Tao (2010); Recht (2011)). In many social science applications, however, there is a
specific structure on the missing data.


3.1.1   Block Structure

A leading example is a block structure, with a subset of the units treated during every
period from a particular point in time T0 onwards.
                                     ï£«                               ï£¶
                                         X X X X ...            X
                                     ï£¬                               ï£·
                                         X X X X ...            X
                                     ï£¬                               ï£·
                                     ï£¬                               ï£·
                                     ï£¬                               ï£·
                                         X X X X ...            X
                                     ï£¬                               ï£·
                                     ï£¬                               ï£·
                                     ï£¬                               ï£·
                            YN Ã—T   =ï£¬   X X X      ?    ...    ?    ï£·.
                                     ï£¬                               ï£·
                                     ï£¬                               ï£·
                                         X X X      ?    ...    ?
                                     ï£¬                               ï£·
                                     ï£¬                               ï£·
                                         .. .. ..   ..          ..
                                     ï£¬                               ï£·
                                                         ..
                                                            .
                                     ï£¬                               ï£·
                                     ï£¬    . . .      .           .   ï£·
                                     ï£­                               ï£¸
                                         X X X      ?    ...    ?

There are two special cases of the block structure. Much of the literature on estimating
average treatment effects under unconfoundedness focuses on the case where T0 = T , so
that the only treated units are in the last period. We will refer to this as the single-treated-
period block structure. In contrast, the synthetic control literature focuses on the case
of with a single treated unit which are treated for a number of periods from period T0




                                               6
onwards, the single-treated-unit block structure:
        ï£«                                 ï£¶
            X X X ...        X X                                ï£«                                            ï£¶
    ï£¬                                     ï£·                          X X X ...         X
            X X X ...        X X
    ï£¬                                     ï£·                ï£¬                                                 ï£·
    ï£¬                                     ï£·
                                                                     X X X ...         X
    ï£¬                                     ï£·                ï£¬                                                 ï£·
                                                           ï£¬                                                 ï£·
            X X X ...        X       ?
    ï£¬                                     ï£·                ï£¬                                                 ï£·
    ï£¬                                     ï£·
                                                                     X X X ...         X
                                                           ï£¬                                                 ï£·
            .. .. .. . .     ..      ..
    ï£¬                                     ï£·                ï£¬                                                 ï£·
  Y=ï£¬                    .                         and Y = ï£¬                                                 ï£·.
    ï£¬                                     ï£·
             . . .            .       .   ï£·                          .. .. .. . .      ..
                                                                                  .
                                                           ï£¬                                                 ï£·
    ï£¬
    ï£¬
                                          ï£·
                                          ï£·                ï£¬          . . .             .                    ï£·
    ï£¬       X X X ...        X       ?    ï£·                ï£¬
                                                           ï£¬
                                                                                                             ï£·
                                                                                                             ï£·
    ï£¬
    ï£¬
                                          ï£·
                                          ï£·                ï£¬         X X X ...         X                     ï£·
    ï£¬                                â†‘    ï£·                ï£­                                                 ï£¸
    ï£­                                     ï£¸                          X X     ?   ...   ?    â† treated unit
                    treated period




3.1.2   Staggered Adoption

Another setting that has received attention is characterized by staggered adoption of the
treatment (Athey and Imbens (2018)). Here units may differ in the time they are first
exposed to the treatment, but once exposed they remain in the treatment group forever
after. This naturally arises in settings where the treatment is some new technology that
units can choose to adopt (e.g., Athey and Stern (2002)). Here:
                             ï£«                                                               ï£¶
                                 X X X X ...                    X          (never adopter)
                          ï£¬                                                               ï£·
                                 X X X X ...                    ?          (late adopter) ï£·
                          ï£¬                                                               ï£·
                          ï£¬
                          ï£¬                                                               ï£·
                                 X X          ?     ?    ...    ?
                          ï£¬                                                               ï£·
                          ï£¬                                                               ï£·
                 YN Ã—T   =ï£¬                                                               ï£·.
                                 X X          ?     ?    ...    ?       (medium adopter) ï£·
                          ï£¬                                                               ï£·
                          ï£¬
                                 .. ..        ..    ..          ..
                          ï£¬                                                               ï£·
                                                         ..
                                                            .
                          ï£¬                                                               ï£·
                          ï£¬       . .          .     .           .                        ï£·
                          ï£­                                                               ï£¸
                                 X        ?   ?     ?    ...    ?         (early adopter)




                                                            7
3.2    Thin and Fat Matrices
A second classification concerns the shape of the matrix Y. Relative to the number of
time periods, we may have many units, few units, or a comparable number. These data
configurations may make particular analyses more attractive. For example, Y may be a
thin matrix, with N  T , or a fat matrix, with N  T , or an approximately square
matrix, with N â‰ˆ T :
            ï£«                      ï£¶
                ?    X        ?
         ï£¬                       ï£·
                X    ?        X ï£·
         ï£¬                       ï£·
         ï£¬
         ï£¬                       ï£·                              ï£«                                 ï£¶
                ?    ?        X ï£·
         ï£¬                       ï£·
         ï£¬                                                             ?   ?   X X X ...      ?
         ï£¬                       ï£·                       ï£¬                                      ï£·
       Y=ï£¬      X    ?        X ï£· (thin)               Y=ï£¬ X X X X                            X ï£· (fat),
         ï£¬                       ï£·                       ï£¬                                      ï£·
                                                                                    ?   ...
         ï£¬                       ï£·                       ï£­                                      ï£¸
                ?    ?        ? ï£·
         ï£¬                       ï£·
         ï£¬                                                 ? X ? X                  ?   ...   X
                ..   ..       .. ï£·
         ï£¬                       ï£·
         ï£¬
         ï£¬       .    .        . ï£·
         ï£­                       ï£¸
                ?    ?        X

or                        ï£«                                        ï£¶
                               ?       ?   X X ...         ?
                  ï£¬                                                ï£·
                              X X X X ...                  X
                  ï£¬                                                ï£·
                  ï£¬                                                ï£·
                  ï£¬                                                ï£·
                               ?       X   ?    X ...      X
                  ï£¬                                                ï£·
                  ï£¬                                                ï£·
                Y=ï£¬                                                ï£· (approximately square).
                              X X          ?    X ...      X
                  ï£¬                                                ï£·
                  ï£¬                                                ï£·
                              .. ..        ..   .. . .     ..
                  ï£¬                                                ï£·
                                                       .
                  ï£¬                                                ï£·
                  ï£¬            . .          .    .          .      ï£·
                  ï£­                                                ï£¸
                               ?       ?   X X ...         X




3.3    Horizontal and Vertical Regressions
Two special combinations of missing data patterns and the shape of the matrices deserve
particular attention because they are the focus of substantial separate literatures.



                                                               8
3.3.1     Horizontal Regression and the Unconfoundedness Literature

The unconfoundedness literature focuses primarily on the single-treated-period block struc-
ture with a thin matrix, and imputes the missing potential outcomes in the last period using
control units with similar lagged outcomes. A simple version of that approach is to regress
the last period outcome on the lagged outcomes and use the estimated regression to pre-
dict the missing potential outcomes. That is, for the units with (i, T ) âˆˆ M, the predicted
outcome is

                       T âˆ’1                                                            T âˆ’1
                                                                                                       !2
                       X                                       X                       X
        YÌ‚iT = Î²Ì‚0 +          Î²Ì‚s Yis , where Î²Ì‚ = arg min                YiT âˆ’ Î²0 âˆ’          Î²s Yis         .   (3.1)
                       s=1                           Î²                                  s=1
                                                             i:(i,T )âˆˆO


We refer to this as a horizontal regression, where the rows of the Y matrix form the units
of observation. A more flexible, nonparametric, version of this estimator would correspond
to matching where we find for each treated unit i a corresponding control unit j with Yjt
approximately equal to Yit for all pre-treatment periods t = 1, . . . , T âˆ’ 1.


3.3.2     Vertical Regression and the Synthetic Control Literature

The synthetic control literature focuses primarily on the single-treated-unit block structure
with a fat or approximately square matrix. Doudchenko and Imbens (2016) discuss how
the Abadie-Diamond-Hainmueller synthetic control method can be interpreted as regressing
the outcomes for the treated unit prior to the treatment on the outcomes for the control
units in the same periods. That is, for the treated unit in period t, for t = T0 , . . . , T , the
predicted outcome is

                       N âˆ’1                                                             N âˆ’1
                                                                                                        !2
                       X                                        X                       X
     YÌ‚N t = Î³Ì‚0 +            Î³Ì‚i Yit , where Î³Ì‚ = arg min                YN t âˆ’ Î³0 âˆ’          Î³i Yit        .   (3.2)
                                                     Î³
                       i=1                                   t:(N,t)âˆˆO                  i=1




                                                         9
We refer to this as a vertical regression, where the columns of the Y matrix form the units
of observation. As shown in Doudchenko and Imbens (2016) this is a special case of the
Abadie et al. (2015) estimator, without imposing their restrictions that the coefficients are
nonnegative and that the intercept is zero.
   Although this does not appear to have been pointed out previously, a matching version
of this estimator would correspond to finding, for each period t where unit N is treated, a
corresponding period s âˆˆ {1, . . . , T0 âˆ’ 1} such that Yis is approximately equal to YN s for
all control units i = 1, . . . , N âˆ’ 1. This matching version of the synthetic control estimator
clarifies the link between the treatment effect literature under unconfoundedness and the
synthetic control literature.
   Suppose that the only missing entry is in the last period for unit N . In that case if we
estimate the horizontal regression in (3.1), it is still the case that imputed YÌ‚N T is linear
in the observed Y1T , . . . , YN âˆ’1,T , just with different weights than those obtained from the
vertical regression. Similarly, if we estimate the vertical regression in (3.2), it is still the
case that YÌ‚N T is linear in YN 1 , . . . , YN,T âˆ’1 , just with different weights from the horizontal
regression.


3.4     Fixed Effects and Factor Models
The horizontal regression focuses on a pattern in the time path of the outcome Yit , specifi-
cally the relation between YiT and the lagged Yit for t = 1, . . . , T âˆ’1, for the units for whom
these values are observed, and assumes that this pattern is the same for units with missing
outcomes. The vertical regression focuses on a pattern between units at times when we
observe all outcomes, and assumes this pattern continues to hold for periods when some
outcomes are missing. However, by focusing on only one of these patterns, cross-section
or time series, these approaches ignore alternative patterns that may help in imputing the


                                                 10
missing values. An alternative is to consider approaches that allow for the exploitation of
both stable patterns over time, and stable patterns accross units. Such methods have a
long history in the panel data literature, including the literature on fixed effects, and more
generally, factor and interactive fixed effect models (e.g., Chamberlain (1984); Arellano and
HonoreÌ (2001); Liang and Zeger (1986); Bai (2003, 2009); Pesaran (2006); Moon and Wei-
dner (2015, 2017)). In the absence of covariates (although in this literature the coefficients
on these covariates are typically the primary focus of the analyses), such models can be
written as
                               R
                               X
                       Yit =         Î³ir Î´tr + Îµit ,        or Y = UV> + Îµ,              (3.3)
                               r=1

where U is N Ã— R and V is T Ã— R. Most of the early literature, Anderson (1958) and
Goldberger (1972)), focused on the thin matrix case, with N  T , where asymptotic
approximations are based on letting the number of units increase with the number of time
periods fixed. In the modern part of this literature (Bai (2003, 2009); Pesaran (2006);
Moon and Weidner (2015, 2017); Bai and Ng (2017)) researchers allow for more complex
asymptotics with both N and T increasing, at rates that allow for consistent estimation of
the factors V and loadings U after imposing normalizations. In this literature it is typically
assumed that the number of factors R is fixed, although not necessarily known. Methods
for estimating the rank R are discussed in Bai and Ng (2002) and Moon and Weidner
(2015).
   Xu (2017) implements this interactive fixed effect approach to the matrix completion
problem in the special case with blocked assignment, with additional applications in Gobil-
lon and Magnac (2013); Kim and Oka (2014) and Hsiao et al. (2012). Suppose the first NC
units are in the control group, and the last NT = N âˆ’ NC units are in the treatment group.
The treatment group is exposed to the control treatment in the first T0 âˆ’ 1 pre-treatment
periods, and exposed to the active treatment in the post-treatment periods T0 , . . . , T . In


                                                       11
that case we can partition U and V accordingly and write
                                         ï£«        ï£¶ï£«             ï£¶>
                                             UC           Vpre
                              UV> = ï£­             ï£¸ï£­             ï£¸ .
                                             UT          Vpost

Using the data from the control group pre and post, and the pre data only for the treatment
group, we have
                        ï£«           ï£¶>
                            Vpre                                        >
             YC = UC ï£­              ï£¸ + ÎµC ,           and YT,pre = UT Vpre + ÎµT,pre
                            Vpost

where the first equation can be used to estimate UC , Vpre , and Vpost , and the second is
used to estimate UT , both by least squares after normalizing U and V. Note that this is
not necessarily efficient, because YT,pre is not used to estimate Vpre .
   Independently, a closely related literature has emerged in machine learning and statis-
tics on matrix completion (Srebro et al. (2005); CandeÌ€s and Recht (2009); CandeÌ€s and
Tao (2010); Keshavan et al. (2010a,b); Gross (2011); Recht (2011); Rohde et al. (2011);
Negahban and Wainwright (2011, 2012); Koltchinskii et al. (2011); Klopp (2014)). In this
literature the starting point is an incompletely observed matrix, and researchers have pro-
posed matrix-factorization approaches to matrix completion, similar to (3.3). The focus
is not on estimating U and V consistently, only on imputing the missing elements of Y.
Instead of fixing the rank of the underlying matrix, estimators rely on regularization, and
in particular nuclear norm regularization.




                                                  12
4      The Nuclear Norm Matrix Completion Estimator
In the absence of covariates we model the N Ã— T matrix of outcomes Y as


                          Y = Lâˆ— + Îµ,          where      E[Îµ|Lâˆ— ] = 0 .                       (4.1)


The Îµit can be thought of as measurement error. The goal is to estimate the matrix Lâˆ— .
     To facilitate the characterization of the estimator, define for any matrix A, and given
a set of pairs of indices O, the two matrices PO (A) and PâŠ¥
                                                          O (A) with typical elements:

                ï£±                                              ï£±
                ï£² A
                    it       if (i, t) âˆˆ O ,         âŠ¥
                                                               ï£² 0               if (i, t) âˆˆ O ,
     PO (A)it =                                 and PO (A)it =
                ï£³ 0          if (i, t) âˆˆ
                                       / O,                    ï£³ Ait             if (i, t) âˆˆ
                                                                                           / O.

A critical role is played by various matrix norms, summarized in Table 1. Some of these
depend on the singular values, where, given the full Singular Value Decomposition (SVD)
LN Ã—T = SN Ã—N Î£N Ã—T R>
                     T Ã—T , the singular values Ïƒi (L) are the ordered diagonal elements of

Î£.
                         Table 1: Matrix Norms for Matrix L



                                                                    ( i Ïƒi (L)p )1/p
                                                                     P
     Schatten Norm                    kLkp
                                                                           P P             1/2
                                                                    1/2        N     T
                                                       ( i Ïƒi (L)2 ) =                    2
                                                        P
     FroÌˆbenius Norm            kLkF                                           i=1      L
                                                                                     t=1 it
                                                                      P
     Rank Norm                   kLk0                                    1
                                                                       Pi Ïƒi (L)>0
     Nuclear Norm                kLkâˆ—                                    i Ïƒi (L)
     Operator Norm              kLkop                           maxi Ïƒi (L) = Ïƒ1 (L)
     Max Norm                  kLkPmax                          max1â‰¤iâ‰¤N,1â‰¤tâ‰¤T |Lit |
     Element Wise `1 Norm kLk1,e = i,t |Lit |



     Now consider the problem of estimating L. Directly minimizing the sum of squared

                                               13
differences,
                          1 X                     1
                   min        (Yit âˆ’ Lit )2 = min    kPO (Y âˆ’ L)k2F ,                    (4.2)
                    L    |O|                   L |O|
                            (i,t)âˆˆO

does not lead to a useful estimator: if (i, t) âˆˆ M the objective function does not depend
on Lit , and for other pairs (i, t) the estimator would simply be Yit . Instead, we regularize
the problem by adding a penalty term Î»kLk, for some choice of the norm k Â· k.


The estimator: The general form of our proposed estimator for Lâˆ— is
                                                                       
                                            1
                        LÌ‚ = arg min           kPO (Y âˆ’ L)k2F + Î»kLkâˆ—       ,            (4.3)
                                  L        |O|

studied by Mazumder et al. (2010), with the penalty factor Î» chosen through cross-
validation that will be described at the end of this section. We will call this the Matrix-
Completion with Nuclear Norm Minimization (MC-NNM) estimator.
   Other commonly used Schatten norms would not work as well for this specific problem.
For example, the FroÌˆbenius norm on the penalty term would not have been suitable for
estimating Lâˆ— in the case with missing entries because the solution for Lit for (i, t) âˆˆ M is
always zero (which follows directly from the representation of kLkF = i,t L2it ). The rank
                                                                        P

norm is not computationally feasible for large N and T if the cardinality and complexity
of the set M are substantial. Formally, the problem is NP-hard. In contrast, a major
advantage of using the nuclear norm is that the resulting estimator can be computed using
fast convex optimization programs, e.g. the SOFT-IMPUTE algorithm by Mazumder et al.
(2010) that will be described next.


Calculating the Estimator: The algorithm for calculating our estimator (in the case
without additional covariates) goes as follows. Given the SVD for A, A = SÎ£R> , with



                                                 14
singular values Ïƒ1 (A), . . . , Ïƒmin(N,T ) (A), define the matrix shrinkage operator


                                     shrinkÎ» (A) = SÎ£ÌƒR> ,                                    (4.4)


where Î£Ìƒ is equal to Î£ with the i-th singular value Ïƒi (A) replaced by max(Ïƒi (A) âˆ’ Î», 0).
Now start with the initial choice L1 (Î», O) = PO (Y). Then for k = 1, 2, . . . , define,

                                               n                    o
                      Lk+1 (Î», O) = shrink Î»|O| PO (Y) + PâŠ¥
                                                          O   L k (Î»)   ,                     (4.5)
                                              2




until the sequence {Lk (Î», O)}kâ‰¥1 converges. The limiting matrix LÌ‚(Î», O) = limkâ†’âˆž Lk (Î», O)
is our estimator given the regularization parameter Î».


Cross-validation: The optimal value of Î» is selected through cross-validation. We choose
K (e.g., K = 5) random subsets Ok âŠ‚ O with cardinality b|O|2 /N T c to ensure that the
fraction of observed data in the cross-validation data sets, |Ok /|O|, is equal to that in
the original sample, |O|/(N T ). We then select a sequence of candidate regularization
parameters Î»1 > Â· Â· Â· > Î»L = 0, with a large enough Î»1 , and for each subset Ok calculate
LÌ‚(Î»1 , Ok ), . . . , LÌ‚(Î»L , Ok ) and evaluate the average squared error on O \ Ok . The value of Î»
that minimizes the average squared error (among the K produced estimators corresponding
to that Î») is the one chosen. It is worth noting that one can expedite the computation by
using LÌ‚(Î»i , Ok ) as a warm-start initialization for calculating LÌ‚(Î»i+1 , Ok ) for each i and k.



5     Theoretical Bounds for the Estimation Error
In this section we focus on the case that there are no covariates and provide theoretical
results for the estimation error. Let Lmax be a positive constant such that kLâˆ— kmax â‰¤ Lmax
(recall that kLâˆ— kmax = maxi,t |Lâˆ—it |). We also assume that Lâˆ— is a deterministic matrix.

                                                  15
Then consider the following estimator for Lâˆ— .
                                                                              
                                                   1
                      LÌ‚ =     arg min                kPO (Y âˆ’ L)k2F + Î»kLkâˆ—       .           (5.1)
                             L:kLkmax â‰¤Lmax       |O|

5.1     Additional Notation
First, we start by introduction some new notation. For each positive integer n let [n] be
the set of integers {1, 2, . . . , n}. In addition, for any pair of integers i, n with i âˆˆ [n] define
ei (n) to be the n dimensional column vector with all of its entries equal to 0 except the
ith entry that is equal to 1. In other words, {e1 (n), e2 (n), . . . , en (n)} forms the standard
basis for Rn . For any two matrices A, B of the same dimensions define the inner product
hA, Bi â‰¡ trace(A> B). Note that with this definition, hA, Ai = kAk2F .
   Next, we describe a random observation process that defines the set O. Consider N
independent random variables t1 , . . . , tN on [T ] with distributions Ï€ (i) . Specifically, for
                                      (i)
each (i, t) âˆˆ [N ] Ã— [T ], define Ï€t        â‰¡ P[ti = t]. We also use the short notation EÏ€ when
taking expectation with respect to all distributions Ï€ (1) , . . . , Ï€ (N ) . Now, O can be written
        S n                                   o
as O = N i=1  (i, 1), (i, 2), . . . , (i, ti ) .
   Also, for each (i, t) âˆˆ O, we use the notation Ait to refer to ei (N )et (T )> which is a N
by T matrix with all entries equal to zero except the (i, t) entry that is equal to 1. The
data generating model can now be written as


                                Yit = hAit , Lâˆ— i + Îµit ,   âˆ€ (i, t) âˆˆ O ,


where noise variables Îµit are independent Ïƒ-sub-Gaussian random variables that are also
independent of Ait . Recall that a random variable Îµ is Ïƒ-sub-Gaussian if for all real numbers
t we have E[exp(tÎµ)] â‰¤ exp(Ïƒ 2 t2 /2).
   Note that the number of control units (Nc ) is equal to the number of rows that have all

                                                      16
                               PN
entries observed (i.e., Nc =     i=1 Iti =T ). Therefore,   the expected number of control units
                               PN     (i)
can be written as EÏ€ [Nc ] =    i=1 Ï€T . Defining


                                                      (i)
                                        pc â‰¡ min Ï€T ,
                                              1â‰¤iâ‰¤N


we expect to have (on average) at least N pc control units. The parameter pc will play an
important role in our main theoretical results. Specifically, assuming N and T are of the
                                                                            âˆš
same order, we will show that the average per entry error (i.e., kLÌ‚âˆ’Lâˆ— kF / N T ) converges
                                         âˆš
to 0 if pc grows larger than log3/2 (N )/ N up to a constant. To provide some intuition
for such assumption on pc , assume Lâˆ— is a matrix that is zero everywhere except in its ith
row. Such Lâˆ— is clearly low-rank. But recovering the entry Lâˆ—iT is impossible when it < T .
            (i)
Therefore, Ï€T cannot be too small. Since i is arbitrary, in general pc cannot be too small.

Remark 5.1. It is worth noting that the sources of randomness in our observation process
O are the random variables {ti }N
                                i=1 that are assumed to be independent of each other. But

we allow that distributions of these random variables to be functions of Lâˆ— . We also assume
that the noise variables {Îµit }itâˆˆ[N ]Ã—[T ] are independent of each other and are independent
of {ti }N
        i=1 . In Â§8 we discuss how our results could generalize to the cases with correlations

among these noise variables.

Remark 5.2. The estimator (5.1) penalizes the error terms (Yit âˆ’ Lit )2 , for (i, t) âˆˆ O,
equally. But the ex ante probability of missing entries in each row, the propensity score, in-
creases as t increases. In Â§8.3, we discuss how the estimator can be modified by considering
a weighted loss function based on propensity scores for the missing entries.




                                               17
5.2    Main Result
The main result of this section is the following theorem (proved in Â§A.1) that provides an
                             âˆš
upper bound for kLâˆ— âˆ’ LÌ‚kF / N T , the root-mean-squared-error (RMSE) of the estimator
LÌ‚. In literature on theoretical analysis of empirical risk minimization this type of upper
bound is called an oracle inequality.

Theorem 1. If the rank of Lâˆ— is R, then there is a constant C such that with probability
greater than 1 âˆ’ 2(N + T )âˆ’2 ,
                      ï£®     s                 s                    s                  ï£¹
                                                                           3
   kLâˆ— âˆ’ LÌ‚kF                 log(N +  T )      R log(N   + T )      R log   (N + T )
     âˆš        â‰¤ C max ï£°Lmax                ,Ïƒ                   ,Ïƒ                    ï£»,   (5.2)
       NT                        N p2c              T p2c                 N p2c

                                                       hp                 âˆš                 i
when the parameter Î» is a constant multiple of Ïƒ max        N log(N + T ), T log3/2 (N + T ) /|O|.


Interpretation of Theorem 1: In order to see when the RMSE of LÌ‚ converges to zero
as N and T grow, we note that the right hand side of (5.2) converges to 0 when Lâˆ— is
                                                  p
low-rank (R is constant) and pc  log3/2 (N + T )/ min(N, T ). A sufficient condition for
the latter, when N and T are of the same order, is that the lower bound for the average
                                                                  âˆš
number of control units (N pc ) grows larger than a constant times N log3/2 (N ). In Â§8 we
will discuss how the estimator LÌ‚ should be modified to obtain a sharper result that would
hold for a smaller number of control units.


Comparison with existing theory on matrix-completion: Our estimator and its
theoretical analysis are motivated by and generalize existing research on matrix-completion
Srebro et al. (2005); Mazumder et al. (2010); CandeÌ€s and Recht (2009); CandeÌ€s and Tao
(2010); Keshavan et al. (2010a,b); Gross (2011); Recht (2011); Rohde et al. (2011); Ne-
gahban and Wainwright (2011, 2012); Koltchinskii et al. (2011); Klopp (2014). The main


                                              18
difference is in our observation model O. Existing papers assume that entries (i, t) âˆˆ O
are independent random variables whereas we allow for a dependency structure including
staggered adoption where if (i, t) âˆˆ O then (i, t0 ) âˆˆ O for all t0 < t.



6     The Relationship with Horizontal and Vertical Re-
      gressions
In the second contribution of this paper we discuss the relation between the matrix com-
pletion estimator and the horizontal (unconfoundedness) and vertical (synthetic control)
approaches. To faciliate the discussion, we focus on the case with M containing a single
pair, unit N in period T , M = {(N, T )}. In that case the various previously proposed
versions of the vertical and horizontal regressions are both directly applicable, although
estimating the coefficients may require regularization.
    The observed data are Y, an N Ã— T matrix that can be partitioned as
                                               ï£«               ï£¶
                                                    YÌƒ    y1
                                        Y=ï£­                    ï£¸,
                                                    y2>   ?


where YÌƒ is (N âˆ’ 1) Ã— (T âˆ’ 1), y1 is (N âˆ’ 1) Ã— 1, and y2 is (T âˆ’ 1) Ã— 1.
    The matrix completion solution to imputing YN T can be characterized, for a given
regularization parameter Î», as
                                                                           
                      mcâˆ’nnm                        1              2
                  L            (Î») = arg min           kPO (Y âˆ’ L)kF + Î»kLkâˆ— .       (6.1)
                                         L         |O|




                                                    19
The predicted value for the missing entry YN T is then


                                          YÌ‚Nmcâˆ’nnm
                                              T     = Lmcâˆ’nnm
                                                       NT     (Î»).                                            (6.2)


   We are interested in comparing this estimator to horizontal regression estimator. Let
us initially assume that the horizontal regression is well defined, without regularization, so
that N > T . First define
                                                       âˆ’1       
                                         Î²Ì‚ hr = YÌƒ> YÌƒ      YÌƒ> y1 .

Then the horizontal regression based prediction is

                                                                          âˆ’1         
                            YÌ‚NhrT   =   y2> Î²Ì‚ hr   =   y2>         >
                                                                   YÌƒ YÌƒ            >
                                                                                   YÌƒ y1 .


For the vertical (synthetic control) regression, initially assuming T > N , we start with

                                                                  âˆ’1       
                                              vt               >
                                         Î³Ì‚        = YÌƒYÌƒ                  YÌƒy2 ,


leading to the horizontal regression based prediction

                                                           âˆ’1      
                             YÌ‚NvtT = y1> Î³Ì‚ hr = y1> YÌƒYÌƒ>       YÌƒy2 .


The original (Abadie et al. (2010)) synthetic control estimator imposes the additional re-
                      PN âˆ’1
strictions Î³i â‰¥ 0, and i=1   Î³i = 1, leading to

                                                                                             N âˆ’1
                                                     2                                       X
          Î³Ì‚ scâˆ’adh = arg min y2 âˆ’ YÌƒ> Î³                 ,           subject to âˆ€i Î³i â‰¥ 0,          Î³i = 1.
                         Î³                           F
                                                                                             i=1




                                                             20
Then the synthetic control based prediction is


                                             YÌ‚Nscâˆ’adh
                                                 T     = y1> Î³Ì‚ scâˆ’adh .


The Doudchenko and Imbens (2016) modification allows for the possibility that N â‰¥ T
and regularizes the estimator for Î³. Focusing here on an elastic net regularization, their
proposed estimator is
                                                                                 
                    vtâˆ’en                           >
                                                         2                1âˆ’Î±    2
               Î³Ì‚           = arg min       y2 âˆ’ YÌƒ Î³        + Î» Î± kÎ³k1 +     kÎ³kF    .
                                  Î³                      F                 2

Then the vertical elastic net prediction is


                                              YÌ‚Nvtâˆ’en
                                                  T    = y1> Î³Ì‚ vtâˆ’en .


We can modify the horizontal regression in the same way to allow for restrictions on the
Î², and regularization, although such methods have not been used in practice.
    The question in this section concerns the relation between the various predictors,
YÌ‚Nmcâˆ’nnm
    T     , YÌ‚NhrT , YÌ‚NvtT , YÌ‚Nscâˆ’adh
                                  T     , and YÌ‚Nvtâˆ’en
                                                  T    . The first result states that all these estimators
can be viewed as particular cases of matrix factorization estimators, with the difference
coming in the way the estimation of the components of the matrix factorization is carried
out.

Theorem 2. All five estimators YÌ‚Nmcâˆ’nnm
                                   T     , YÌ‚NhrT , YÌ‚NvtT , YÌ‚Nscâˆ’adh
                                                                 T     , and YÌ‚Nvtâˆ’en
                                                                                 T    , can be written
in the form YÌ‚NestT = LÌ‚est
                        N T , for est âˆˆ {mc âˆ’ nnm, hr, vt, sc âˆ’ adh, vt âˆ’ en}, where



                                               LÌ‚est = Aest Best> ,


with Lest , Aest , and Best N Ã— T , N Ã— R and T Ã— R dimensional matrices, and Aest and

                                                        21
Best estimated as
                                                                                                      
                                                1                         2
         est        est
                                                   PO Y âˆ’ AB>
                                                             
       A ,B                    = arg min                                  F
                                                                              + penalty terms on (A, B) ,
                                    A,B        |O|

subject to restrictions on A and B, with the penalty terms and the restrictions specific to
the estimator.

   Theorem 2 follows from the following result.

Theorem 3. We have,
(i) (nuclear norm matrix completion)
                                                                                                                             
                                                           1                           2
     (AÎ»mcâˆ’nnm , Bmcâˆ’nnm                                      PO Y âˆ’ AB>                        0
                                                                                                    kAk2F        0
                                                                                                                     kBk2F
                                                                         
                  Î»      )              = arg min                                      F
                                                                                           +Î»               +Î»                    ,
                                                A,B       |O|

for Î»0 = Î»/2.
(ii) (horizontal regression, defined if N > T ), R = T âˆ’ 1
                                                                                                                     
                                                       1                          2
               hr         hr
                                                          PO Y âˆ’ AB>                       Î»kAk2F           Î»kBk2F
                                                                     
         (A , B ) = lim arg min                                                   F
                                                                                      +               +                   ,
                                  Î»â†“0      A,B        |O|
                                                                              ï£«            ï£¶
                                                                                  YÌƒ
                                                       subject to Ahr = ï£­                  ï£¸,
                                                                                  y2>

(iii) (vertical regression, defined if T > N ), R = N âˆ’ 1
                                                                                                                     
                                                       1                          2
               vt         vt
                                                          PO Y âˆ’ AB>                       Î»kAk2F           Î»kBk2F
                                                                     
         (A , B ) = lim arg min                                                   F
                                                                                      +               +                   ,
                                  Î»â†“0      A,B        |O|

subject to                                                   ï£«           ï£¶
                                                                 YÌƒ>
                                                      Bvt = ï£­            ï£¸.
                                                                   y1>

                                                              22
(iv) (synthetic control), R = N âˆ’ 1
                                                                                                                    
                                                              1                        2
          scâˆ’adh        scâˆ’adh
                                                                 PO Y âˆ’ AB>                    Î»kAk2F       Î»kBk2F
                                                                            
     (A            ,B            ) = lim arg min                                       F
                                                                                           +            +                ,
                                     Î»â†“0       A,B           |O|

subject to                                 ï£«             ï£¶
                                                                                N âˆ’1
                                 scâˆ’adh
                                                   YÌƒ>                          X
                            B             =ï£­             ï£¸,     âˆ€ i, AiT â‰¥ 0,          AiT = 1,
                                                   y1>                           i=1


(v) (elastic net), R = N âˆ’ 1
                                 ï£±                                                  ï£®             ï£«         ï£¶   2           ï£¶ ï£¹ï£¼
                                                                                                                              ï£«
                                 ï£² 1
                                                                           2               1 âˆ’ Î± ï£­ a2 ï£¸                  a2    ï£½
(Avtâˆ’en , Bvtâˆ’en ) = lim arg min       PO Y âˆ’ AB>
                                                  
                                                                           F
                                                                                + Î»ï£°                                +Î± ï£­    ï£¸ ï£» ,
                     Î»â†“0     A,B ï£³ |O|                                                       2     a3                    a3    ï£¾
                                                                                                                F                    1


subject to                                ï£«            ï£¶                        ï£«                ï£¶
                                                   >
                                              YÌƒ                                    AÌƒ a1
                          Bvtâˆ’en = ï£­                   ï£¸,        where A = ï£­                     ï£¸.
                                              y1>                                   a>
                                                                                     2 a3

   The proof is straightforward algebra and is omitted.

Comment 1. For nuclear norm matrix completion, if LÌ‚ is the solution to Equation (4.3)
that has rank RÌ‚, then one solution for A and B is given by


                                              A = SÎ£1/2 ,            B = RÎ£1/2                                               (6.3)


where LÌ‚ = SN Ã—RÌ‚ Î£RÌ‚Ã—RÌ‚ R>
                          T Ã—RÌ‚
                                is singular value decomposition of LÌ‚. The proof of this fact is
provided in (Mazumder et al. (2010); Hastie et al. (2015)). 




                                                                23
Comment 2. For the horizontal regression the solution for B is
                                     ï£«                          ï£¶
                                         1    0    ...   0
                                  ï£¬                             ï£·
                                  ï£¬
                                  ï£¬      0    1    ...   0      ï£·
                                                                ï£·
                               hr
                                  ï£¬      ..   ..         ..     ï£·
                              B =ï£¬ï£¬       .    .          .     ï£·,
                                                                ï£·
                                  ï£¬                             ï£·
                                  ï£¬
                                  ï£­      0    0    ...   1      ï£·
                                                                ï£¸
                                         Î²Ì‚1 Î²Ì‚2 . . . Î²Ì‚T âˆ’1

and similarly for the vertical regression the solution for A is
                                     ï£«                          ï£¶
                                         1    0    ...   0
                                  ï£¬                             ï£·
                                  ï£¬
                                  ï£¬      0    1    ...   0      ï£·
                                                                ï£·
                               vt
                                  ï£¬      ..   ..         ..     ï£·
                              A =ï£¬ï£¬       .    .          .     ï£·.
                                                                ï£·
                                  ï£¬                             ï£·
                                  ï£¬
                                  ï£­      0    0    ...   1      ï£·
                                                                ï£¸
                                         Î³Ì‚1 Î³Ì‚2 . . . Î³Ì‚N âˆ’1

The regularization in the elastic net version only affects the last row of this matrix, and
replaces it with a regularized version of the regression coefficients. 

Comment 3. The horizontal and vertical regressions are fundamentally different ap-
proaches, and they cannot easily be nested. Without some form of regularization they
cannot be applied in the same setting, because the non-regularized versions require N > T
or N < T respectively. As a result there is also no direct way to test the two methods
against each other. Given a particular choice for regularization, however, one can use
cross-validation methods to compare the two approaches. 




                                              24
7     Two Illustrations
The objective of this section is to compare the accuracy of imputation for the matrix
completion method with previously used methods. In particular, in a real data matrix Y
where no unit is treated (no entries in the matrix are missing), we choose a subset of units
as hypothetical treated units and aim to predict their values (for time periods following
a randomly selected initial time). Then, we report the average root-mean-squared-error
(RMSE) of each algorithm on values for the pseudo-treated (time, period) pairs. In these
cases there is not necessarily a single right algorithm. Rather, we wish to assess which of
the algorithms generally performs well, and which ones are robust to a variety of settings,
including different adoption regimes and different configurations of the data.
    We compare the following estimators:

    â€¢ DID: Difference-in-differences based on regressing the observed outcomes on unit and
      time fixed effects and a dummy for the treatment.

    â€¢ VT-EN: The vertical regression with elastic net regularization, relaxing the restric-
      tions from the synthetic control estimator.

    â€¢ HR-EN: The horizontal regression with elastic net regularization, similar to uncon-
      foundedness type regressions.

    â€¢ SC-ADH: The original synthetic control approach by Abadie et al. (2010), based on
      the vertical regression with Abadie-Diamond-Hainmueller restrictions.

    â€¢ MC-NNM: Our proposed matrix completion approached via nuclear norm mini-
      mization, explained in Section 2 above.

    The comparison between MC-NNM and the two versions of the elastic net estimator,
HR-EN and VT-EN, is particularly salient. In much of the literature researchers choose

                                            25
ex ante between vertical and horizontal type regressions. The MC-NNM method allows
one to sidestep that choice in a data-driven manner.


7.1    The Abadie-Diamond-Hainmueller California Smoking Data
We use the control units from the California smoking data studied in Abadie et al. (2010)
with N = 38, T = 31. Note that in the original data set there are 39 units but one of them
(state of California) is treated which will be removed in this section since the untreated
values for that unit are not available. We then artificially designate some units and time
periods to be treated, and compare predicted values for those unit/time-periods to the
actual values.
   We consider two settings for the treatment adoption:

   â€¢ Case 1: Simultaneous adoption where Nt units adopt the treatment in period T0 + 1,
      and the remaining units never adopt the treatment.

   â€¢ Case 2: Staggered adoption where Nt units adopt the treatment in some period after
      period T , with the actual adoption date varying among these units.

   In each case, the average RMSE for different ratios T0 /T is reported in Figure 1. For
clarity of the figures, for each T0 /T , while all confidence intervals of various methods are
calculated using the same ratio T0 /T , in the figure they are slightly jittered to the left or
right. In the simultaneous adoption case, DID generally does poorly, suggesting that the
data are rich enough to support more complex models. For small values of T0 /T , SC-ADH
and HR-EN perform poorly while VT-EN is superior. As T0 /T grows closer to one, VT-
EN, HR-EN, SC-ADH and MC-NNM methods all do well. The staggered adoption results
are similar with some notable differences; VT-EN performs poorly (similar to DID) and
MC-NNM is the superior approach. The performance improvement of MC-NNM can be
attributed to its use of additional observations (pre-treatment values of treatment units).

                                              26
      (a) Simultaneous adoption, Nt = 8                   (b) Staggered adoption, Nt = 35


                              Figure 1: California Smoking Data


7.2    Stock Market Data
In the next illustration we use a financial data set â€“ daily returns for 2453 stocks over 10
years (3082 days). Since we only have access to a single instance of the data, in order to
observe statistical fluctuations of the RMSE, for each N and T we create 50 sub-samples
by looking at the first T daily returns of N randomly sampled stocks for a range of pairs of
(N, T ), always with N Ã— T = 4900, ranging from very thin to very fat, (N, T ) = (490, 10),
. . . (N, T ) = (70, 70), . . . (N, T ) = (10, 490), with in each case the second half the entries
missing for a randomly selected half the units (so 25% of the entries missing overall), in
a block design. Here we focus on the comparison between the HR-EN, VT-EN, and
MC-NNM estimators as the shape of the matrix changes. We report the average RMSE.
Figure 2 shows the results.




                                               27
                              Figure 2: Stock Market Data



   In the T  N case the VT-EN estimator does poorly, not surprisingly because it
attempts to do the vertical regression with too few time periods to estimate that well.
When N  T , the HR-EN estimator does poorly. The most interesting finding is that
the proposed MC-NNM method adapts well to both regimes and does as well as the best
estimator in both settings, and better than both in the approximately square setting.
   The bottom graph in Figure 2 shows that MC-NNM approximates the data with a
matrix of rank 4 to 12, where smaller ranks are used as N grows relative to T . This
validates the fact that there is a stronger correlation between daily return of different
stocks than between returns for different time periods of the same stock.



                                            28
8     Generalizations
Here we provide a brief discussion on how our estimator and its analysis should be adapted
to more general settings.


8.1    The Model with Covariates
In Section 2 we described the basic model, and discussed the specification and estimation
for the case without covariates. In this section we extend that to the case with unit-
specific, time-specific, and unit-time specific covariates. For unit i we observe a vector of
unit-specific covariates denoted by Xi , and X denoting the N Ã— P matrix of covariates with
ith row equal to Xi> . Similarly, Zt denotes the time-specific covariates for period t, with Z
denoting the T Ã— Q matrix with tth row equal to Zt> . In addition we allow for a unit-time
specific J by 1 vector of covariates Vit .
    The model we consider is

                                        Q
                                      P X
                                      X
                   Yit =   Lâˆ—it   +                  âˆ—
                                                Xip Hpq Zqt + Î³iâˆ— + Î´tâˆ— + Vit> Î² âˆ— + Îµit .   (8.1)
                                      p=1 q=1


the Îµit is random noise. We are interested in estimating the unknown parameters Lâˆ— , Hâˆ— ,
Î³ âˆ— , Î´ âˆ— and Î² âˆ— . This model allows for traditional econometric fixed effects for the units (the
Î³iâˆ— ) and time effects (the Î´tâˆ— ). It also allows for fixed covariate (these have time varying
coefficients) and time covariates (with individual coefficients) and time varying individual
covariates. Note that although we can subsume the unit and time fixed effects into the
matrix Lâˆ— , we do not do so because we regularize the estimates of Lâˆ— , but do not wish to
regularize the estimates of the fixed effects.




                                                        29
    The model can be rewritten as


                        Y = Lâˆ— + XHâˆ— Z> + Î“âˆ— 1>        âˆ— >
                                                            > âˆ—
                                              T + 1N (âˆ† ) + Vit Î² it + Îµ .                                    (8.2)


Here Lâˆ— is in RN Ã—T , Hâˆ— is in RP Ã—Q , Î“âˆ— is in RN Ã—1 and âˆ†âˆ— is in RT Ã—1 . An slightly richer
version of this model that allows linear terms in covariates can be defined as by


                        Y = Lâˆ— + XÌƒHÌƒâˆ— ZÌƒ> + Î“âˆ— 1>        âˆ— >
                                                               > âˆ—
                                                 T + 1N (âˆ† ) + Vit Î² it + Îµ                                   (8.3)


where XÌƒ = [X|IN Ã—N ], ZÌƒ = [Z|IT Ã—T ], and
                                                          ï£®               ï£¹
                                                              Hâˆ—X,Z Hâˆ—X
                                                 HÌƒâˆ— = ï£°                  ï£»
                                                              Hâˆ—Z     0

where Hâˆ—XZ âˆˆ RP Ã—Q , Hâˆ—Z âˆˆ RN Ã—Q , and Hâˆ—X âˆˆ RP Ã—T . In particular,


       Y = Lâˆ— + XÌƒHÌƒâˆ—X,Z ZÌƒ> + HÌƒâˆ—Z ZÌƒ> + XHÌƒâˆ—X + Î“âˆ— 1>        âˆ— >
                                                                    > âˆ—
                                                      T + 1N (âˆ† ) + Vit Î² it + Îµ                              (8.4)


From now on, we will use the richer model (8.4) but abuse the notation and use notation
X, Hâˆ— , Z instead of XÌƒ, HÌƒâˆ— , ZÌƒ. Therefore, the matrix Hâˆ— will be in R(N +P )Ã—(T +Q) .
    We estimate Hâˆ— , Lâˆ— , Î´ âˆ— , Î³ âˆ— , and Î² âˆ— by solving the following convex program,
            ï£®                                                                             !2                          ï£¹
                                                  Q
                                                P X
                X      1                        X
  min ï£°                           Yit âˆ’ Lit âˆ’             Xip Hpq Zqt âˆ’ Î³i âˆ’ Î´t âˆ’ Vit Î²        + Î»L kLkâˆ— + Î»H kHk1,e ï£» .
H,L,Î´,Î³,Î²             kO|                       p=1 q=1
            (i,t)âˆˆO

                       P
Here kHk1,e =               i,t   |Hit | is the element-wise `1 norm. We choose Î»L and Î»H through
cross-validation.
    Solving this convex program is similar to the covariate-free case. In particular, by using


                                                               30
a similar operator to shrinkÎ» , defined in Â§2, that performs coordinate descent with respect
to H. Then we can apply this operator after each step of using shrinkÎ» . Coordinate descent
with respect to Î³, Î´, and Î² is performed similarly but using a simpler operation since the
function is smooth with respect to them.


8.2    Autocorrelated Errors
One drawback of MC-NNM is that it does not take into account the time series nature of
the observations. It is likely that the columns of Îµ exhibit autocorrelation. We can take this
into account by modifying the objective function. Let us consider this in the case without
covariates, and, for illustrative purposes, let us use an autoregressive model of order one.
Let YiÂ· and LiÂ· be the ith row of Y and L respectively. The original objective function for
O = [N ] Ã— [T ] is

             N       T                                       N
         1 XX                                  1 X
                    (Yit âˆ’ Lit )2 + Î»L kLkâˆ— =         (YiÂ· âˆ’ LiÂ· )(YiÂ· âˆ’ LiÂ· )> + Î»L kLkâˆ— .
        |O| i=1 t=1                           |O| i=1

                          PN                 âˆ’1          >
We can modify this to        i=1 (YiÂ· âˆ’LiÂ· )â„¦ (YiÂ· âˆ’LiÂ· ) /|O|+Î»L kLkâˆ— ,          where the choice for the
T Ã— T matrix â„¦ would reflect the autocorrelation in the Îµit . For example, with a first order
autoregressive process, we would use â„¦ts = Ï|tâˆ’s| , with Ï an estimate of the autoregressive
coefficient. Similarly, for the more general version O âŠ‚ [N ] Ã— [T ], we can use the function

                          1 X      X
                                            (Yit âˆ’ Lit )[â„¦âˆ’1 ]ts (Yis âˆ’ Lis ) + Î»L kLkâˆ— .
                         |O|
                          (i,t)âˆˆO (i,s)âˆˆO


8.3    Weighted Loss Function
Another limitation of MC-NNM is that it puts equal weight on all observed elements of the
difference Y âˆ’ L (ignoring the covariates). Ultimately we care solely about predictions of

                                                        31
the model for the missing elements of Y, and for that reason it is natural to emphasize the
fit of the model for elements of Y that are observed, but that are similar to the elements
that are missing. In the program evaluation literature this is often achieved by weighting
the fit by the propensity score, the probability of outcomes for a unit being missing.
   We can do so in the current setting by modelling this probability in terms of the covari-
ates and a latent factor structure. Let the propensity score be eit = P(Wit = 1|Xi , Zt , Vit ),
and let E be the N Ã— T matrix with typical element eit . Let us again consider the case
without covariates. In that case we may wish to model the assignment W as


                                     WN Ã—T = EN Ã—T + Î· N Ã—T .


We can estimate this using the same matrix completion methods as before, now without
any missing values:

                                           1 X
                         EÌ‚ = arg min          (Wit âˆ’ eit )2 + Î»L kEkâˆ— .
                                      E   NT
                                                (i,t)


Given the estimated propensity score we can then weight the objective function for esti-
mating Lâˆ— :
                                      1 X   eÌ‚it
                      LÌ‚ = arg min                 (Yit âˆ’ Lit )2 + Î»L kLkâˆ— .
                               L     |O|  1 âˆ’ eÌ‚it
                                          (i,t)âˆˆO


8.4    Relaxing the Dependence of Theorem 1 on pc
                                                                     (i)
Recall from Â§5.1 that the average number of control units is N
                                                             P
                                                                i=1 Ï€T . Therefore, the frac-
                               (i)
tion of control units is N
                        P
                          i=1 Ï€T /N . However, the estimation error in Theorem 1 depends
                    (i)                    (i)
on pc = min1â‰¤iâ‰¤N Ï€T rather than N
                                   P
                                      i=1 Ï€T /N . The reason for this, as discussed in Â§5.1 is

due to special classes of matrices Lâˆ— where most of the rows are nearly zero (e.g, when only
one row is non-zero). In order to relax this constraint we would need to restrict the family

                                                        32
of matrices Lâˆ— . An example of such restriction is given by Negahban and Wainwright (2012)
where they assume Lâˆ— is not too spiky. Formally, they assume the ratio kLâˆ— kmax /kLâˆ— kF
                       âˆš
should be of order 1/ N T up to logarithmic terms. To see the intuition for this, in a
                                              âˆš
matrix with all equal entries this ratio is 1/ N T whereas in a matrix where only the (1, 1)
entry is non-zero the ratio is 1. While both matrices have rank 1, in the former matrix the
value of kLâˆ— kF is obtained from most of the entries. In such situations, one can extend our
                                                              (i)
results and obtain an upper bound that depends on N
                                                      P
                                                         i=1 Ï€T /N .



8.5     Nearly Low-rank Matrices
Another possible extension of Theorem 1 is to the cases where Lâˆ— may have high rank, but
most of its singular values are small. More formally, if Ïƒ1 â‰¥ Â· Â· Â· > Ïƒmin(N,T ) are singular
                                                                      Pmin(N,T )
values of Lâˆ— , one can obtain upper bounds that depend on k and r=k+1 Ïƒr for any
k âˆˆ [min(N, T )]. One can then optimize the upper bound by selecting the best k. In the
low-rank case such optimization leads to selecting k equal to R. This type of more general
upper bound has been proved in some of prior matrix completion literature, e.g. Negahban
and Wainwright (2012). We expect their analyses would be generalize-able to our setting
(when entries of O are not independent).


8.6     Additional Missing Entries
In Â§5.1 we assumed that all entries (i, t) of Y for t â‰¤ ti are observed. However, it may be
possible that some such values are missing due to lack of data collection. This does not
mean that any treatment occurred in the pre-treatment period. Rather, such scenario can
occur when measuring outcome values is costly and can be missed. In this case, one can
                                         hS n                                     oi
                                             N
extend Theorem 1 to the setting with O =     i=1 (i, 1), (i, 2), . . . , (i, ti )    \ Omiss , where
each (i, t) âˆˆ âˆªN
               i=1 {(i, 1), (i, 2), . . . , (i, ti )} can be in Omiss , independently, with probability p



                                                   33
for p that is not too large.



9      Conclusions
We present new results for estimation of causal effects in panel or longitudinal data settings.
The proposed estimator, building on the interactive fixed effects and matrix completion lit-
eratures has attractive computational properties in settings with large N and T , and allows
for a relatively large number of factors. We show how this set up relates to the program
evaluation and synthetic control literatures. In illustrations we show that the method
adapts well to different configurations of the data, and find that generally it outperforms
the synthetic control estimators from Abadie et al. (2010) and the elastic net estimators
from Doudchenko and Imbens (2016).



References
Abadie, A., A. Diamond, and J. Hainmueller (2010). Synthetic control methods for compar-
    ative case studies: Estimating the effect of Californiaâ€™s tobacco control program. Journal
    of the American Statistical Association 105 (490), 493â€“505.

Abadie, A., A. Diamond, and J. Hainmueller (2015). Comparative politics and the synthetic
    control method. American Journal of Political Science, 495â€“510.

Abadie, A. and J. Gardeazabal (2003). The economic costs of conflict: A case study of the
    basque country. American Economic Review 93 (-), 113â€“132.

Anderson, T. W. (1958). An introduction to multivariate statistical analysis, Volume 2.
    Wiley New York.



                                              34
Arellano, M. and B. HonoreÌ (2001). Panel data models: some recent developments. Hand-
  book of econometrics 5, 3229â€“3296.

Athey, S. and G. W. Imbens (2018). Design-based analysis in difference-in-differences set-
  tings with staggered adoption. Technical report, National Bureau of Economic Research.

Athey, S. and S. Stern (2002). The impact of information technology on emergency health
  care outcomes. The RAND Journal of Economics 33 (3), 399432.

Bai, J. (2003). Inferential theory for factor models of large dimensions. Econometrica 71 (1),
  135â€“171.

Bai, J. (2009). Panel data models with interactive fixed effects. Econometrica 77 (4),
  1229â€“1279.

Bai, J. and S. Ng (2002). Determining the number of factors in approximate factor models.
  Econometrica 70 (1), 191â€“221.

Bai, J. and S. Ng (2017). Principal components and regularized estimation of factor models.
  arXiv preprint arXiv:1708.08137 .

BuÌˆhlmann, P. and S. Van De Geer (2011). Statistics for high-dimensional data: methods,
  theory and applications. Springer Science & Business Media.

CandeÌ€s, E. J. and Y. Plan (2010). Matrix completion with noise. Proceedings of the
  IEEE 98 (6), 925â€“936.

CandeÌ€s, E. J. and B. Recht (2009). Exact matrix completion via convex optimization.
  Foundations of Computational mathematics 9 (6), 717.

CandeÌ€s, E. J. and T. Tao (2010). The power of convex relaxation: Near-optimal matrix
  completion. IEEE Trans. Inf. Theor. 56 (5), 2053â€“2080.

                                             35
Chamberlain, G. (1984). Panel data. Handbook of econometrics 2, 1247â€“1318.

Doudchenko, N. and G. W. Imbens (2016). Balancing, regression, difference-in-differences
  and synthetic control methods: A synthesis. Technical report, National Bureau of Eco-
  nomic Research.

Gobillon, L. and T. Magnac (2013). Regional policy evaluation: Interactive fixed effects
  and synthetic controls. Review of Economics and Statistics (00).

Goldberger, A. S. (1972). Structural equation methods in the social sciences. Econometrica:
  Journal of the Econometric Society, 979â€“1001.

Gross, D. (2011). Recovering low-rank matrices from few coefficients in any basis. IEEE
  Trans. Information Theory 57 (3), 1548â€“1566.

Hastie, T., R. Mazumder, J. D. Lee, and R. Zadeh (2015). Matrix completion and low-rank
  svd via fast alternating least squares. J. Mach. Learn. Res. 16 (1), 3367â€“3402.

Hsiao, C., H. Steve Ching, and S. Ki Wan (2012). A panel data approach for program
  evaluation: measuring the benefits of political and economic integration of hong kong
  with mainland china. Journal of Applied Econometrics 27 (5), 705â€“740.

Imbens, G. W. and D. B. Rubin (2015). Causal Inference in Statistics, Social, and Biomed-
  ical Sciences. Cambridge University Press.

Keshavan, R. H., A. Montanari, and S. Oh (2010a, June). Matrix completion from a few
  entries. IEEE Trans. Inf. Theor. 56 (6), 2980â€“2998.

Keshavan, R. H., A. Montanari, and S. Oh (2010b, August). Matrix completion from noisy
  entries. J. Mach. Learn. Res. 11, 2057â€“2078.



                                            36
Kim, D. and T. Oka (2014). Divorce law reforms and divorce rates in the usa: An interactive
  fixed-effects approach. Journal of Applied Econometrics 29 (2), 231â€“245.

Klopp, O. (2014). Noisy low-rank matrix completion with general sampling distribution.
  Bernoulli 20 (1), 282â€“303.

Koltchinskii, V., K. Lounici, A. B. Tsybakov, et al. (2011). Nuclear-norm penalization
  and optimal rates for noisy low-rank matrix completion. The Annals of Statistics 39 (5),
  2302â€“2329.

Liang, K.-Y. and S. L. Zeger (1986). Longitudinal data analysis using generalized linear
  models. Biometrika 73 (1), 13â€“22.

Mazumder, R., T. Hastie, and R. Tibshirani (2010). Spectral regularization algorithms
  for learning large incomplete matrices. Journal of machine learning research 11 (Aug),
  2287â€“2322.

Moon, H. R. and M. Weidner (2015). Linear regression for panel with unknown number of
  factors as interactive fixed effects. Econometrica 83 (4), 1543â€“1579.

Moon, H. R. and M. Weidner (2017). Dynamic linear panel regression models with inter-
  active fixed effects. Econometric Theory 33 (1), 158â€“195.

Negahban, S. and M. J. Wainwright (2011). Estimation of (near) low-rank matrices with
  noise and high-dimensional scaling. The Annals of Statistics, 1069â€“1097.

Negahban, S. and M. J. Wainwright (2012). Restricted strong convexity and weighted
  matrix completion: Optimal bounds with noise. Journal of Machine Learning Re-
  search 13 (May), 1665â€“1697.



                                            37
Negahban, S. N., P. Ravikumar, M. J. Wainwright, and B. Yu (2012). A unified framework
  for high-dimensional analysis of M -estimators with decomposable regularizers. Statistical
  Science 27 (4), 538â€“557.

Pesaran, M. H. (2006). Estimation and inference in large heterogeneous panels with a
  multifactor error structure. Econometrica 74 (4), 967â€“1012.

Recht, B. (2011). A simpler approach to matrix completion. Journal of Machine Learning
  Research 12 (Dec), 3413â€“3430.

Rohde, A., A. B. Tsybakov, et al. (2011). Estimation of high-dimensional low-rank matrices.
  The Annals of Statistics 39 (2), 887â€“930.

Rosenbaum, P. R. and D. B. Rubin (1983). The central role of the propensity score in
  observational studies for causal effects. Biometrika 70 (1), 41â€“55.

Rubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonran-
  domized studies. Journal of Educational Psychology 66 (5), 688â€“701.

Srebro, N., N. Alon, and T. S. Jaakkola (2005). Generalization error bounds for collabora-
  tive prediction with low-rank matrices. In L. K. Saul, Y. Weiss, and L. Bottou (Eds.),
  Advances in Neural Information Processing Systems 17, pp. 1321â€“1328.

Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the
  Royal Statistical Society. Series B (Methodological), 267â€“288.

Tropp, J. A. (2012). User-friendly tail bounds for sums of random matrices. Foundations
  of Computational Mathematics 12 (4), 389â€“434.

Xu, Y. (2017). Generalized synthetic control method: Causal inference with interactive
  fixed effects models. Political Analysis 25 (1), 57â€“76.

                                              38
A         Online Appendix for â€œ Matrix Completion Meth-
          ods for Causal Panel Data Modelsâ€: Proofs

A.1       Proof of Theorem 1
First, we will discuss three main steps that are needed for the proof.


Step 1: We show an upper bound for the sum of squared errors for all (i, t) âˆˆ O in terms of
the regularization parameter Î», rank of Lâˆ— , kLâˆ— âˆ’LÌ‚kF , and kEkop where E â‰¡ (i,t)âˆˆO Îµit Ait .
                                                                            P


Lemma 1 (Adapted from Negahban and Wainwright (2011)). Then for all Î» â‰¥ 3kEkop /|O|,

                         X hAit , Lâˆ— âˆ’ LÌ‚i2      âˆš
                                            â‰¤ 10Î» R kLâˆ— âˆ’ LÌ‚kF .                        (A.1)
                                  |O|
                        (i,t)âˆˆO


    This type of result has been shown before by Recht (2011); Negahban and Wainwright
(2011); Koltchinskii et al. (2011); Klopp (2014). For convenience of the reader, we include
its proof in Â§A. Similar results also appear in the analysis of LASSO type estimators (for
example see BuÌˆhlmann and Van De Geer (2011) and references therein).


Step 2: The upper bound provided by Lemma 1 contains Î» and also requires the condition
Î» â‰¥ 3kEkop /|O|. Therefore, in order to have a tight bound, it is important to show an
upper bound for kEkop that holds with high probability. Next lemma provides one such
result.

Lemma 2. There exist a constant C1 such that

                                    hp              âˆš                 i
                 kEkop â‰¤ C1 Ïƒ max     N log(N + T ), T log3/2 (N + T ) ,



                                             39
with probability greater than 1 âˆ’ (N + T )âˆ’2 .

   This result uses a concentration inequality for sum of random matrices to find a bound
for kEkop . We note that previous papers, Recht (2011); Negahban and Wainwright (2011);
Koltchinskii et al. (2011); Klopp (2014), contain a similar step but in their case O is
obtained by independently sampling elements of [N ]Ã—[T ]. However, in our case observations
from each row of the matrix are correlated. Therefore, prior results do not apply. In fact,
the correlation structure deteriorates the type of upper bound that can be obtained for
kEkop .


Step 3: The last main step is to show that, with high probability, the random vari-
able on the left hand side of (A.1) is larger than a constant fraction of kLÌ‚ âˆ’ Lâˆ— k2F . In
high-dimensional statistics literature this property is also referred to as Restricted Strong
Convexity, Negahban et al. (2012); Negahban and Wainwright (2011, 2012). The following
Lemma states this property for our setting and its proof that is similar to the proof of
Theorem 1 in (Negahban and Wainwright, 2012) or Lemma 12 in (Klopp, 2014) is omitted.

Lemma 3. If the estimator LÌ‚ defined above satisfies kLÌ‚ âˆ’ Lâˆ— kF â‰¥ Îº for a positive number
Îº, then,
                ï£±                                                          ï£¼
                                                                                                p2c Îº2
                ï£²p                                                         ï£½                             
                     c
                                               X
           PÏ€            kLÌ‚ âˆ’   Lâˆ— k2F   â‰¤                          âˆ— 2
                                                        hAit , LÌ‚ âˆ’ L i        â‰¥ 1 âˆ’ exp âˆ’                    .
                ï£³2                                                         ï£¾                 32 T L2max
                                              (i,t)âˆˆO


   Now we are equipped to prove the main theorem.

Proof of Theorem 1. Let âˆ† = Lâˆ— âˆ’ LÌ‚. Then using Lemma 2 and selecting Î» equal to




                                                                40
3kEkop /|O| in Lemma 1, with probability greater than 1 âˆ’ (N + T )âˆ’2 , we have

                                  âˆš      hp              âˆš                 i
   X hAit , âˆ†i      2       30C1 Ïƒ R max   N log(N + T ), T log3/2 (N + T )
               â‰¤                                                                                kâˆ†kF .    (A.2)
       |O|                                                     |O|
  (i,t)âˆˆO


Now, we use Lemma 3 to find a lower bound for the left hand side of (A.2). But first note
that if p2c kâˆ†k2F /(32 T L2max ) â‰¤ 2 log(N + T ) then
                                                            s
                                      kâˆ†kF                      log(N + T )
                                      âˆš    â‰¤ 8Lmax
                                        NT                         N p2c

                                                                           p
holds which proves Theorem 1. Otherwise, using Lemma 3 for Îº = (8Lmax /pc ) T log(N + T ),
                            ï£±                                        ï£¼
                            ï£²1                 X                     ï£½              1
                        P        pc kâˆ†k2F â‰¤             hAit , âˆ†i2       â‰¥1âˆ’               .              (A.3)
                            ï£³2                                       ï£¾           (N + T )2
                                              (i,t)âˆˆO


Combining this result, (A.2), and union bound we have, with probability greater than
1 âˆ’ 2(N + T )âˆ’2 ,
                                          s                              s                      !
                          âˆš                     N log(N + T )                T
            kâˆ†k2F â‰¤ 60C1 Ïƒ R max Ïƒ                            ,                  log3/2 (N + T ) kâˆ†kF .
                                                     p2c                     p2c

                                                                                âˆš
The main result now follows after dividing both sides with                       N T kâˆ†kF .


A.2         Proof of Lemma 1
Variants of this Lemma for similar models have been proved before. But for completeness
we include its proof that is adapted from Negahban and Wainwright (2011).




                                                          41
Proof of Lemma 1. Let

                                           X (Yit âˆ’ Lit )2
                                f (L) â‰¡                    + Î»kLkâˆ— .
                                                  |O|
                                          (i,t)âˆˆO



Now, using the definition of LÌ‚,
                                              f (LÌ‚) â‰¤ f (Lâˆ— ) ,

which is equivalent to

              X hLâˆ— âˆ’ LÌ‚, Ait i2    X Îµit hLâˆ— âˆ’ LÌ‚, Ait i
                                 +2                       + Î»kLÌ‚kâˆ— â‰¤ Î»kLâˆ— kâˆ— .      (A.4)
                     |O|                      |O|
            (i,t)âˆˆO                        (i,t)âˆˆO



Now, defining âˆ† â‰¡ Lâˆ— âˆ’ LÌ‚ and using the definition of E, the above equation gives

                       X hâˆ†, Ait i2     2
                                    â‰¤âˆ’     hâˆ†, Ei + Î»kLâˆ— kâˆ— âˆ’ Î»kLÌ‚kâˆ—                (A.5)
                           |O|         |O|
                      (i,t)âˆˆO
                                      (a)   2
                                          â‰¤    kâˆ†kâˆ— kEkop + Î»kLâˆ— kâˆ— âˆ’ Î»kLÌ‚kâˆ—        (A.6)
                                           |O|
                                            2
                                      â‰¤       kâˆ†kâˆ— kEkop + Î»kâˆ†kâˆ—                    (A.7)
                                          |O|
                                      (b) 5
                                      â‰¤ Î»kâˆ†kâˆ— .                                     (A.8)
                                          3

Here, (a) uses inequality |hA, Bi| â‰¤ kAkop kBkmax which is due to the fact that operator
norm is dual norm to nuclear norm, and (b) uses the assumption Î» â‰¥ 3kEkop /|O|. Before
continuing with the proof of Lemma 1 we state the following Lemma that is proved later
in this section.

Lemma 4. Let âˆ† â‰¡ Lâˆ— âˆ’ LÌ‚ for Î» â‰¥ 3kEkop /|O| Then there exist a decomposition âˆ† =
âˆ†1 + âˆ†2 such that


                                                     42
  (i) hâˆ†1 , âˆ†2 i = 0.

 (ii) rank(âˆ†1 ) â‰¤ 2r.

(iii) kâˆ†2 kâˆ— â‰¤ 3kâˆ†1 kâˆ— .

   Now, invoking the decomposition âˆ† = âˆ†1 + âˆ†2 from Lemma 4 and using the triangle
inequality, we obtain

                             (c)       (d) âˆš        (e) âˆš
                        kâˆ†kâˆ— â‰¤ 4kâˆ†1 kâˆ— â‰¤ 4 2rkâˆ†1 kF â‰¤ 4 2rkâˆ†kF .                                 (A.9)


where (c) uses Lemma 4(iii), (d) uses Lemma 4(ii) and Cauchy-Schwarz inequality, and
(e) uses Lemma 4(i). Combining this with (A.8) we obtain

                                  X hâˆ†, Ait i2      âˆš
                                               â‰¤ 10Î» r kâˆ†kF ,                                   (A.10)
                                      |O|
                                 (i,t)âˆˆO


which finishes the proof of Lemma 1.

Proof of Lemma 4. Let Lâˆ— = UN Ã—r SrÃ—r (VT Ã—r )> be the singular value decomposition for
the rank r matrix Lâˆ— . Let PU be the projection operator onto column space of U and let
PU âŠ¥ be the projection operator onto the orthogonal complement of the column space of U.
Let us recall a few linear algebra facts about these projection operators. If columns of U are
denoted by u1 , . . . , u0 , since U is unitary, PU = ri=1 ui u>
                                                     P                               PN         >
                                                               i . Similarly, PU âŠ¥ =  i=r+1 ui ui

where u1 , . . . , u0 , ur+1 , . . . , uN forms an orthonormal basis for RN . In addition, the projector
operators are idempotent (i.e., P2U = PU , P2U âŠ¥ = PU âŠ¥ ), PU + PU âŠ¥ = IN Ã—N .
   Define PV and PV âŠ¥ similarly. Now, we define âˆ†1 and âˆ†2 as follows:


                             âˆ†2 â‰¡ PU âŠ¥ âˆ†PV âŠ¥        ,   âˆ†1 â‰¡ âˆ† âˆ’ âˆ†2 .



                                                  43
It is easy to see that


                         âˆ†1 = (PU + PU âŠ¥ )âˆ†(PV + PV âŠ¥ ) âˆ’ PU âŠ¥ âˆ†PV âŠ¥             (A.11)

                            = PU âˆ† + PU âŠ¥ âˆ†PV .                                  (A.12)


Using this fact we have


              hâˆ†1 , âˆ†2 i = trace âˆ†> PU PU âŠ¥ âˆ†PV âŠ¥ + PV âˆ†> PU âŠ¥ PU âŠ¥ âˆ†PV âŠ¥
                                                                            
                                                                                 (A.13)

                         = trace PV âˆ†> PU âŠ¥ âˆ†PV âŠ¥
                                                  
                                                                                 (A.14)

                         = trace âˆ†> PU âŠ¥ âˆ†PV âŠ¥ PV = 0
                                                  
                                                                                 (A.15)


that gives part (i). Note that we used trace(AB) = trace(BA).
   Looking at (A.12), part (ii) also follows since both PU and PV have rank r and sum of
two rank r matrices has rank at most 2r.
   Before moving to part (iii), we note another property of the above decomposition of
âˆ† that will be needed next. Since the two matrices Lâˆ— and âˆ†2 have orthogonal singular
vectors to each other,


                                kLâˆ— + âˆ†2 kâˆ— = kLâˆ— kâˆ— + kâˆ†2 kâˆ— .                  (A.16)


   On the other hand, using inequality (A.6), for Î» â‰¥ 3kEkop /|O| we have

                            
                                       âˆ—
                                                2
                          Î» kLÌ‚kâˆ— âˆ’ kL kâˆ— â‰¤         kâˆ†kâˆ— kEkop
                                                |O|
                                                2
                                               â‰¤ Î»kâˆ†kâˆ—
                                                3
                                                2
                                               â‰¤ Î» (kâˆ†1 kâˆ— + kâˆ†2 kâˆ— ) .          (A.17)
                                                3


                                                44
Now, we can use the following for the left hand side


                      kLÌ‚kâˆ— âˆ’ kLâˆ— kâˆ— = kLâˆ— + âˆ†1 + âˆ†2 kâˆ— âˆ’ kLâˆ— kâˆ—

                                     â‰¥ kLâˆ— + âˆ†2 kâˆ— âˆ’ kâˆ†1 kâˆ— âˆ’ kLâˆ— kâˆ—
                                     (f )
                                      = kLâˆ— kâˆ— + kâˆ†2 kâˆ— âˆ’ kâˆ†1 kâˆ— âˆ’ kLâˆ— kâˆ—

                                     = kâˆ†2 kâˆ— âˆ’ kâˆ†1 kâˆ— .


Here (f ) follows from (A.16). Now, combining the last inequality with (A.17) we get

                                                    2
                           kâˆ†2 kâˆ— âˆ’ kâˆ†1 kâˆ— â‰¤          (kâˆ†1 kâˆ— + kâˆ†2 kâˆ— ) .
                                                    3

That finishes proof of part (iii).


A.3     Proof of Lemma 2
First we state the matrix version of Bernstein inequality for rectangular matrices (see Tropp
(2012) for a derivation of it).

Proposition 1 (Matrix Bernstein Inequality). Let Z1 , . . . , ZN be independent matrices in
Rd1 Ã—d2 such that E[Zi ] = 0 and kZi kop â‰¤ D almost surely for all i âˆˆ [N ] and a constant R.
Let ÏƒZ be such that
                               ï£±                                                      ï£¼
                               ï£²     N
                                     X                           N
                                                                 X                    ï£½
                   ÏƒZ2 â‰¥ max                E[Zi Z>
                                                  i ]        ,         E[Z>
                                                                          i Zi            .
                               ï£³                                                      ï£¾
                                     i=1                op       i=1             op




                                                   45
Then, for any Î± â‰¥ 0
                     ï£±                      ï£¼
                         N
                                                                    âˆ’Î±2
                     ï£²   X                  ï£½                               
                 P             Zi        â‰¥ Î± â‰¤ (d1 + d2 ) exp                  .                          (A.18)
                     ï£³
                         i=1
                                            ï£¾                 2ÏƒZ2 + (2DÎ±)/3
                                    op


Proof of Lemma 2. Our goal is to use Proposition 1. Define the sequence of independent
random matrices B1 , . . . , BN as follows. For every i âˆˆ [N ], define

                                                      ti
                                                      X
                                              Bi =          Îµit Ait .
                                                      t=1

                     PN
By definition, E =      i=1 Bi and E[Bi ] = 0 for all i âˆˆ [N ]. Define the bound D â‰¡
    p
C2 Ïƒ log(N + T ) for a large enough constant C2 . For each (i, t) âˆˆ O define ÎµÌ„it = Îµit I|Îµit |â‰¤D .
Also define Bi = tt=1
                Pi
                       ÎµÌ„it Ait for all i âˆˆ [N ].
   Using union bound and the fact that for Ïƒ-sub-Gaussian random variables Îµit we have
P(|Îµit | â‰¥ t) â‰¤ 2 exp{âˆ’t2 /(2Ïƒ 2 )} gives, for each Î± â‰¥ 0,
                                          ï£±                           ï£¼
                                          ï£²     N
                                                X                     ï£½        X
               P{ kEkop â‰¥ Î±} â‰¤ P                      Bi         â‰¥Î±       +             P{|Îµit | â‰¥ D}
                                          ï£³                           ï£¾
                                                i=1         op                (i,t)âˆˆO
                                          ï£±                           ï£¼
                                                N
                                                                                               âˆ’D2
                                          ï£²     X                     ï£½                              
                                    â‰¤P                Bi         â‰¥Î±       + 2|O| exp
                                          ï£³
                                                i=1
                                                                      ï£¾                        2Ïƒ 2
                                                            op
                                          ï£±                           ï£¼
                                                N
                                          ï£²     X                     ï£½          1
                                    â‰¤P                Bi         â‰¥Î±       +             .                 (A.19)
                                          ï£³
                                                i=1
                                                                      ï£¾       (N + T )3
                                                            op




                                                       46
Now, for each i âˆˆ [N ], define Zi â‰¡ Bi âˆ’ E[Bi ]. Then,

  N                   N
                                             "                #
  X                   X                          X
        Bi        â‰¤         Zi        + E                Bi
  i=1        op       i=1        op              1â‰¤iâ‰¤N            op
                      N
                                             "                #                 N
                                                                                                         "            #
                      X                          X                              X                âˆš           X
                  â‰¤         Zi        + E                Bi                â‰¤          Zi        + NT E           Bi              .
                      i=1        op              1â‰¤iâ‰¤N            F             i=1        op            1â‰¤iâ‰¤N             max


But since each Îµit has mean zero,
                                                                           q
                  |E[ÎµÌ„it ]| = |E[Îµit I|Îµit |â‰¤D ]| = |E[Îµit I|Îµit |â‰¥D ]| â‰¤ E[Îµ2it ] P(|Îµit | â‰¥ D)
                                                                           p
                                                                         â‰¤ 2Ïƒ 2 exp[âˆ’D2 /(2Ïƒ 2 )]
                                                                              Ïƒ
                                                                         â‰¤            .
                                                                           (N + T )4

Therefore,
                                       "                 #                âˆš
                            âˆš                X                           Ïƒ NT          Ïƒ
                             NT E                   Bi                â‰¤         4
                                                                                  â‰¤           ,
                                           1â‰¤iâ‰¤N
                                                                        (N + T )    (N + T )3
                                                              max


which gives

                                       N                      N
                                       X                      X                        Ïƒ
                                             Bi         â‰¤             Zi        +             .                           (A.20)
                                       i=1                    i=1
                                                                                    (N + T )3
                                                   op                      op

                                 âˆš
   We also note that kZi kop â‰¤ 2D T for all i âˆˆ [N ]. The next step is to calculate ÏƒZ




                                                                  47
defined in the Proposition 1. We have,

      N                                                                           N
                                                                                              "   ti
                                                                                                                          #
      X                                                                           X               X
            E[Zi Z>             â‰¤ max E [(ÎµÌ„it âˆ’ E[ÎµÌ„it ])2 ]                                           ei (N )ei (N )>
                                     
                  i ]                                                                    E                                             (A.21)
                                  (i,t)âˆˆO
      i=1                  op                                                     i=1             t=1                         op
                                                ï£«                ï£¶
                                                    X          (i)
                                â‰¤ 2Ïƒ 2 max ï£­                 tÏ€t ï£¸ â‰¤ 2T Ïƒ 2                                                            (A.22)
                                       iâˆˆ[N ]
                                                    tâˆˆ[T ]


and

                            N                                    N
                                                                             "   ti
                                                                                                           #
                            X                                    X               X
                                  E[Z>
                                     i Zi ]          â‰¤ 2Ïƒ 2              E             et (T )et (T )>                                 (A.23)
                            i=1                 op                i=1            t=1                           op
                                                                         ï£«                         ï£¶
                                                                             X        T
                                                                                      X        (i)
                                                     = 2Ïƒ 2 max ï£­                             Ï€t0 ï£¸ = 2N Ïƒ 2 .                         (A.24)
                                                                tâˆˆ[T ]
                                                                             iâˆˆ[N ]   t0 =t


Note that here we used the fact that random variables ÎµÌ„it âˆ’ E[ÎµÌ„it ] are independent of each
other and centered which means all cross terms of the type E{(ÎµÌ„it âˆ’ E[ÎµÌ„it ])(ÎµÌ„js âˆ’ E[ÎµÌ„js ])}
are zero for (i, t) 6= (j, s). Therefore, ÏƒZ2 = 2Ïƒ 2 max(N, T ) works. Applying Proposition 1,
we obtain
            ï£±                        ï£¼
              N
                                                                    Î±2
            ï£² X                      ï£½                                                                                       
       P              Zi         â‰¥ Î± â‰¤ (N + T ) exp âˆ’                       âˆš
            ï£³
                i=1         op
                                    ï£¾                 4Ïƒ 2 max(N, T ) + (4DÎ± T )/3


                                                                                                  Î±2
                                                                                                                            
                                                            3                                               Î±
                                            â‰¤ (N + T ) exp âˆ’ min                               2
                                                                                                          , âˆš                      .
                                                            16                                Ïƒ max(N, T ) D T )




                                                                   48
Therefore, there is a constant C3 such that with probability greater than 1 âˆ’ exp(âˆ’t),

 N
 X                             p                                                           
                                                             p
        Zi        â‰¤ C3 Ïƒ max     max(N, T )[t + log(N + T )], T log(N + T )[t + log(N + T )] .
  i=1        op


Using this for a t that is a large enough constant times log(N + T ), together with (A.19)
and (A.20), shows with probability larger than 1 âˆ’ 2(N + T )âˆ’3

                                    hp                        âˆš                 i
                   kEkop â‰¤ C1 Ïƒ max    max(N, T ) log(N + T ), T log3/2 (N + T )
                                    hp                âˆš     3/2
                                                                       i
                         = C1 Ïƒ max   N log(N + T ), T log (N + T ) ,


for a constant C1 .




                                                  49
