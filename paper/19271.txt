NBER WORKING PAPER SERIES

DISTRIBUTIONAL EFFECTS OF A SCHOOL VOUCHER PROGRAM:
EVIDENCE FROM NEW YORK CITY
Marianne P. Bitler
Thurston Domina
Emily K. Penner
Hilary W. Hoynes
Working Paper 19271
http://www.nber.org/papers/w19271
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
July 2013

Research reported in this publication was supported by the Eunice Kennedy Shriver National Institute
of Child Health & Human Development of the National Institutes of Health under Award Number
P01HD065704. The content is solely the responsibility of the authors and does not necessarily represent
the official views of the National Institutes of Health. We thank Mathematica Policy Research for
making the restricted use data available. We are grateful to Greg Duncan and other members of the
UC Irvine Network on Interventions in Development and our INID Advisory Board members Jeff
Smith, Susanna Loeb, Sean Reardon, Robert Crosnoe, and Jacquelynne Eccles; as well as to Christina
Tuttle and Steve Glazerman, and seminar and conference participants for helpful comments. We also
thank Kevin Williams for his excellent research assistance. The views expressed herein are those of
the authors and do not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2013 by Marianne P. Bitler, Thurston Domina, Emily K. Penner, and Hilary W. Hoynes. All rights
reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission
provided that full credit, including © notice, is given to the source.

Distributional Effects of a School Voucher Program: Evidence from New York City
Marianne P. Bitler, Thurston Domina, Emily K. Penner, and Hilary W. Hoynes
NBER Working Paper No. 19271
July 2013
JEL No. I2
ABSTRACT
We use quantile treatment effects estimation to examine the consequences of a school voucher experiment
across the distribution of student achievement. In 1997, the School Choice Scholarship Foundation
granted $1,400 private school vouchers to a randomly-selected group of low-income New York City
elementary school students. Prior research indicates that this program had no average effect on student
achievement. If vouchers boost achievement at one part of the distribution and hurt achievement at
another, zero or small mean effects may obscure theoretically important but offsetting program effects.
Drawing upon prior research related to Catholic schools and school choice, we derive three hypotheses
regarding the program’s distributional consequences. Our analyses suggest that the program had no
significant effect at any point in the skill distribution.
Marianne P. Bitler
Department of Economics
University of California, Irvine
3151 Social Science Plaza
Irvine, CA 96297
and NBER
mbitler@uci.edu
Thurston Domina
Department of Education
University of California, Irvine
Irvine, CA 92697
tdomina@uci.edu

Emily K. Penner
Department of Education
University of California, Irvine
Irvine, CA 92697
emily.penner@uci.edu
Hilary W. Hoynes
Richard & Rhoda Goldman School of Public Policy
University of California, Berkeley
2607 Hearst Avenue
Berkeley, CA 94720-7320
and NBER
hoynes@berkeley.edu

Introduction
Excellence and equity goals motivate much of American educational policy.
These two goals are not always mutually reinforcing. Some educational policies and
practices boost average academic achievement even as they broaden educational
inequalities (c.f. Arygs, Rees, & Brewer 1996). Others have little effect on average
achievement but narrow inequalities (c.f. Hong, et al. 2012). The twin goals of excellence
and equity should lead policy-makers to be interested in both the average effects of
educational policies and their distributional consequences. But although developmental
science suggests that many interventions may have heterogeneous effects (Duncan &
Vandell 2012), much educational evaluation research focuses on the estimation of mean
treatment effects either for the population at large or for particular subgroups of interest.
In this paper we use a distributional approach to estimate the effects of a school
voucher experiment, in which low-income elementary school students in New York City
applied for a $1,400 private school voucher. Nearly 80 percent of the students who were
randomly selected from the pool of eligible applicants to receive the voucher used their
vouchers to enroll in private schools (Mayer et al. 2002). However, there is very little
evidence to suggest that this voucher offer influenced mean student achievement.
This paper makes three main contributions to the research on vouchers and school
choice: First, we argue that estimates of the mean effects of vouchers may obscure
theoretically and practically important effects across the distribution of achievement. In
particular, we demonstrate that distributional analyses make it possible to test hypotheses
that are prominent in the literature on school choice regarding the effects of educational

2

vouchers on skills inequality. Second, we estimate the effects of the NYC voucher
experiment across the distribution of student achievement, as measured by the Iowa Test
of Basic Skills (ITBS). Third, we situate NYC voucher experiment participants in the
national distribution of student achievement. Doing so provides important context for
understanding the external validity of the findings reported here and may inform
interpretation of the findings from other voucher experiments.
Our findings are largely consistent with a no-voucher-effects hypothesis. We find
some evidence to suggest that the New York City voucher offer had a small negative
short-term effect on math achievement at the top of the distribution. However, this effect
fades out rapidly and is not precisely estimated. Furthermore, the measured effect of the
New York City voucher offer is close to zero throughout the bulk of the study sample’s
math and reading achievement distribution.
School choice and the distribution of achievement
Arguing that traditional public schools are monopolistic and inefficient, school
voucher proponents aim to create more vibrant educational marketplaces. By broadening
the educational choices available to parents and students and creating incentives for
schools to improve, vouchers and other school choice programs aim to boost educational
outcomes for students who might otherwise have no choice but to enroll in low-quality
public schools (Chubb & Moe 1990; Friedman & Friedman 1980).
School reformers have launched a handful of voucher programs across the U.S. over
the past two decades in an attempt to demonstrate the effectiveness of this approach. In
1997, the School Choice Scholarships Foundation initiated one such program in New
York City, offering three-year scholarships worth $1,400 a year to a randomly selected

3

group of low income children then in grades K–4. This program’s random assignment
design makes it possible to distinguish the effects of a voucher offer from the potentially
confounding characteristics of families who self-select into voucher programs.1
Mathematica Policy Research (MPR) and the Harvard University Program on Education
Policy collected enrollment and achievement data from students in the treatment and
control groups.
Analyses of the New York City voucher experiment data clearly indicate that
vouchers influence school choice. Students randomly selected to receive a voucher were
several times more likely than their peers in the control group to attend private schools.
More than three-fourths of voucher recipients used their vouchers to enroll in private
schools at some point in the program, and more than half enrolled in private schools for
the entire three-year scholarship period. 85 percent of the students who used the voucher
enrolled in Catholic schools, where tuition estimates ranged from $1,200 - $2,500 in 1997
(Hartocollis 1997; Steinberg 1997a; 1997b). Parent surveys clearly indicate that those
who received an offer of a voucher had higher levels of satisfaction with their children’s
schools, compared to those in the control group. Voucher lottery winners – and
particularly those who actually used their vouchers to attend private schools – enrolled in
smaller schools with smaller classrooms, more computer labs, and more after-school

1

Other domestic voucher studies that have used random assignment include the voucher experiments in
Dayton, OH and Washington DC (Howell and Peterson 2000; Howell et al. 2002; Wolf, Howell, and
Peterson 2000). Internationally, experiments were also conducted in Chile (Lara et al. 2011; McEwan and
Carnoy 2000) and Colombia (Angrist, Bettinger, and Kremer 2006). The Milwaukee voucher program also
took advantage of a legally-required lottery policy to assign vouchers, although voucher assignment was
overseen by administrators and not independent evaluators (Greene, Peterson, and Du 1997, 1998; Rouse
1998; Witte 1998). In addition several studies have also examined voucher programs using observational
data. Domestically, these include: Cleveland (Greene, Howell, and Peterson 1997; Peterson, Howell, and
Greene 1999), Florida (Chakrabarti 2013; Greene and Winters 2003; Kupermintz 2002), Milwaukee (Rouse
1998) and San Antonio (Peterson, Myers, and Howell 1999); and internationally, New Zealand (Ladd and
Fiske 2003).
4

programs than their peers in the control group (Mayer et al. 2002).
But to date there is little evidence to suggest that these school resources translated to
higher levels of achievement for voucher recipients. While the New York voucher
experiment has inspired a vigorous debate about appropriate methods for analyzing
experimental data (Barnard et al. 2003; Krueger & Zhu 2004a; Krueger & Zhu 2004b;
Peterson & Howell 2004), the results of various analyses of the mean effect of the
voucher program on student achievement are strikingly consistent. Voucher recipients
score no higher, on average, than students in the control group on standardized measures
of math and reading achievement (Krueger & Zhu 2004a; Mayer et al. 2002; Howell et
al. 2002). Voucher programs implemented in other contexts yield somewhat more mixed
results. Evaluations of voucher offers in Charlotte, NC (Cowen 2008; Greene 2001),
Milwaukee, WI (Rouse 1998), Washington, DC (Howell et al. 2002; Wolf et al. 2013),
and Chile (Lara et al. 2011) provide evidence of modest positive average effects on
student achievement. (Cowen 2012 provides a comprehensive review of the existing
literature on voucher program achievement effects.)
The evidence that the NYC voucher experiment had no average effect does not
mean, however, that it had no effect at all. In fact, some evidence suggests that the
program had positive effects on African American students’ achievement (Barnard et al.
2003; Howell et al. 2002; Mayer et al. 2002; Peterson & Howell 2004). Furthermore,
instrumental variable analyses that use the randomized voucher offer to estimate the
causal effect of private school enrollment suggest that private schools particularly benefit
African-American students (Howell et al. 2002). While these findings are highly sensitive

5

to the measurement of student race (Krueger & Zhu 2004a, Krueger & Zhu 2004b)2, they
may indicate that small average effects of voucher programs mask larger heterogeneous
voucher program effects for particular types of students. Furthermore, recent studies
indicate that both the NYC and the Washington, DC voucher program have larger longrun effects on student attainment than one might expect given their short-term
achievement effects (Chingos & Peterson 2012; Wolf et al. 2013). Distributional analyses
could help make sense of these findings, if voucher receipt helps students at the bottom of
the skills distribution acquire a baseline level of skills and successfully progress through
their educational career to high school graduation.
In this paper, we investigate the possibility that weak average effects of vouchers
disguise larger (and possibly contradictory) voucher program effects for high or low
achieving students. We test three competing hypotheses regarding the effects of voucher
programs on student achievement.
(1) Common School Hypothesis: Vouchers mitigate inequality by boosting
achievement primarily at the bottom of the distribution
This hypothesis is grounded in the literature on the effects of Catholic schools.
Nearly all of the students in the NYC experiment who used a voucher to attend a private
school enrolled in a school with a religious affiliation, and 85 percent enrolled in Catholic
When analysts consider only students with African-American mothers as African-American, voucher
receipt has a positive effect on their achievement. However, this effect is not significantly different from
zero when students with either African-American mothers or fathers are included in the pool of AfricanAmerican students (Krueger and Zhu 2004a). Furthermore, Krueger & Zhu (2004a, 2004b) demonstrate
that positive effects for African-Americans (however defined) hold only when controlling for students’
baseline test scores. Krueger and Zhu argue that controlling for baseline test scores is not required to gain
valid estimates of the effect of voucher receipt on student achievement, since assignment to treatment and
control conditions is independent of student test scores. Furthermore, they maintain that controlling for
baseline test scores while omitting observations without baseline scores may introduce bias, since a sizable
proportion of students are missing these scores and they appear not to be randomly selected from the
student population.

2

6

schools (Howell, Wolf, Campbell, and Peterson 2002). Catholic schools are typically
smaller than competing public schools, their curricula are often relatively
undifferentiated, and they are often situated in social networks that allow parents and
teachers to more closely monitor student achievement and behavior. Additionally,
Catholic schools have greater control than public schools over the composition of their
student body, since they can admit students selectively and expel students at will. There
is some evidence to suggest that enrolling in a Catholic school is particularly beneficial
for poor, minority, low-performing and otherwise at-risk students (Coleman & Hoffer
1987; Evans & Schwab 1995; Greeley 1982; Hoffer, Greeley & Coleman 1995; Neal
1997; Morgan 2001), although there have also been concerns raised about selection
biasing some of these comparisons.
By providing a mechanism for students to opt out of neighborhood public schools
and into Catholic and other private schools, voucher experiments attempt to make the
positive achievement effects associated with Catholic schools more broadly available.
Assuming that estimated Catholic school effects are both causal and generalize to the
schools that voucher recipients chose, the “common school hypothesis” suggests that
voucher school programs will have positive effects at the bottom of the academic
achievement distribution, but not at the middle or the top of the distribution.
(2) Stratifying Hypothesis: Vouchers exacerbate inequality by boosting achievement
primarily at the top of the distribution of applicants
In contrast, the “stratifying hypothesis” suggests that voucher programs magnify
educational inequalities. Voucher program advocates take it for granted that parents use
school choice to maximize their children’s educational success. In practice, however,

7

many parents make school choice decisions based on the convenience of the school’s
location, its disciplinary style, and its religious affiliation (e.g., Elacqua, Schneider, &
Buckley 2006). Hastings, Kane, & Staiger (2005) hypothesize the effects of voucher
programs are contingent on the type of the school choices that families make. For
students whose families make school choices on the basis of academic quality, voucher
programs may have positive effects on achievement. But for students whose families
make school choices based on other factors, vouchers may have zero or negative effects
on achievement. If educational preferences vary with student academic achievement,
voucher programs may boost achievement at the top of the achievement distribution,
even as they have no effect or even hurt achievement at the bottom of the achievement
distribution.
(3) No-Effects Hypothesis: Vouchers have no effect across the distribution
While each of the prior two hypotheses are theoretically viable, perhaps the most
common-sense hypothesis based on the results of earlier analyses of New York City
voucher data is that vouchers simply do not influence the distribution of achievement
because vouchers like these do not have very much impact on student achievement. For
many students, the voucher program may have amounted to a weak treatment. It did little
to change students’ home or neighborhood life. Furthermore, the extent to which it
influenced the quality of schools to which students were exposed is debatable. Although
many voucher recipients used their vouchers to enroll in private schools, the $1,400
stipend that the voucher likely provided few students with access to New York City’s
elite private schools; most of which charge considerably higher tuition levels and have
competitive admissions. Rather, most voucher recipients likely attended inexpensive

8

private schools in their own neighborhoods. If these schools do not differ substantially
from the neighborhood public schools that students would have otherwise attended, or if
family and neighborhood factors trump the effects of schools on achievement for these
students, voucher receipt may have had no effect on either the mean or the distribution of
student achievement.
In this paper, we use quantile treatment effect (QTE) estimation to test these
competing hypotheses. This technique, which is not widely used in educational research,
provides unique insights into the ways in which the treatment influences the distribution
of student achievement, making it possible to explicitly investigate this intervention’s
consequences for educational inequality.
Data: The New York City School Choice Scholarship Program
The New York City School Choice Scholarship Program (NYCSCSP) was a three
year private school choice randomized experiment. Randomization procedures are
described in detail in Hill, Rubin, and Thomas (2000). As noted above, low income
students (students qualified for free school lunch) currently in grades K–4 were eligible
to apply for vouchers of $1,400 to be used towards private school tuition for subsequent
school years. Initial applications were received in the spring of 2007 from over 20,000
students, with roughly 5,000 of them meeting the eligibility requirements. Of these,
approximately 2,600 students were randomized at the family level to treatment and
control using two methods of random assignment from separate lottery rounds.
Students from 1,000 families were randomized using a Propensity Matched Pairs
Design (PMPD) in the first lottery and a Stratified Block design was used for students
from an additional 960 families from a second series of lotteries. The PMPD design was

9

used for randomization in the first lottery because the number of eligible applicants far
exceeded the money available to follow up on all of them. Instead of a randomly selected
group of control students, students were selected based on propensity score matching
relative to the group of students who were randomly assigned to the treatment condition.
Variables used in the estimation of the propensity score model are described in Hill et al.
(2000) and Krueger and Zhu (2004a), and included family size and whether the children
attended above or below-median test score schools as the two most important variables
for matching. Matching was done using a Mahalanobis “nearest neighbor” metric for
selecting control families.
The Stratified Block design was created using screened applicants that were invited
to a series of four baseline data collection sessions. Invitation to these sessions was
weighted such that roughly 85% of the invitees were from schools with below-median
test scores. Families were assigned to treatment and control conditions from each of the
testing sessions, creating four stratified blocks.
As described in Krueger and Zhu (2004a), from these two sampling methods, 30
mutually exclusive “random assignment strata” were created from: 5 lottery blocks (1
PMPD block plus 4 stratified blocks) times 2 school types (above- or below-median test
scores) times 3 family size groups (1, 2, or 3 or more students). Within these original
strata, assignment is random. Krueger and Zhu (2004a) detail the discovery by
Mathematica that some families misreported their family size and were placed in the
wrong strata. While revised strata were created and used by Howell and Peterson (2002)
and Mayer, Peterson, et al. (2002), because assignment was random within the original
strata, we follow Krueger and Zhu’s use of the original, rather than the revised strata.

10

Krueger and Zhu note that differences in results between the two sets of strata are very
minor.
Krueger and Zhu also identified two issues with sample weights that were
subsequently revised by Mathematica in 2003.3 When we attempt to replicate others’
findings, we are constrained to either use these revised sample weights, which adjust for
non-response, or use no weights. The combined effect of using the original strata and
having only the revised weights makes it so that we are unable to exactly replicate any
work published prior to Krueger and Zhu (2004a), including Mayer, Peterson, et al.
(2002). Thus, replication attempts are primarily concentrated on Krueger and Zhu
(2004a) and Jin and Rubin (2009), both of which use the original strata and revised
weights.
Baseline student achievement in reading and math was collected for nearly all
students, except for applicants in kindergarten, using the Iowa Test of Basic Skills
(ITBS). We present results using the National Percentile Rankings (NPR) of these
scores4. Initial examinations of the distribution of these baseline ITBS scores revealed
unexpected differences between treatment and control at the top of the raw ITBS score
distribution. Using percentile scores, meanwhile, we found similarly large differences at
the bottom of the distribution. Taken at face value, these findings if statistically

3

MPR discovered after randomization that some families mis-reported their family size and were placed
into the wrong strata. The initial sample weights corrected for the revised sample sizes in the strata. The
corrected weights return the families to their originally assigned strata from the point of randomization.
Krueger and Zhu (2004a) discovered that the baseline weights did not correctly adjust for the size of the
underlying assignment strata. These weights were revised to include poststratification adjustments, which
eliminated previously identified baseline test score differences between the treatment and control groups
(see p. 663 for a detailed discussion).
4
National Percentile Ranking scores are calculated from raw scores which are then normed based on grade
and quarter of the school year (fall, winter, or spring) and converted into rankings as a percentile of the
national distribution based on the normed sample of the ITBS. This allows for cross age and cross-grade
comparisons of scores.
11

significant would seem to indicate that the randomization procedure failed to generate
balanced treatment and control groups.
However, upon closer examination, it became clear that the problem involved the
coding of missing data, rather than the treatment assignment process. Figure 1 reports a
histogram for baseline scores on the raw ITBS mathematics exam. As it indicates, a large
number of students scored 99 on baseline tests in reading and math with the next highest
score not exceeding 40.5 The distribution of all other baseline and post-tests in reading
and math throughout the voucher study show a similar pattern (not shown). Furthermore,
participants with a raw score of 99 have NPR scores and normal curve equivalent (NCE)
scores of 0. Communications with the ITBS’s publisher indicate that zero is not a valid
ITBS NCE score. While analyses reported below indicate that Krueger and Zhu (2004a)
and Jin and Rubin (2009) do not set these cases to missing, we do so in our analyses.6
[Figure 1 about here]
We create inverse propensity score weights to adjust for nonresponse (including both
nonresponse because the observation is missing test scores in the data and non-response
from treating the invalid 99 raw scores as missing). First, we predict treatment status as a
function of demographics, baseline scores when available, and whether the student has a
missing math or reading test score or an invalid 99 math or reading raw test score, using a
5

According to the ITBS website for the publisher, Riverside Publishing, and confirmed through telephone
communication with customer support, students are given tests of increasing difficulty depending on age
and skill level in timed sessions that do not exceed 30 minutes. Raw scores are calculated from each test
level. Although the total number of questions varies somewhat by level, the highest possible raw score in
reading at any level is 44 and the highest possible raw score in math is 50 (Hoover, Dunbar & Frisbie
2013).
6
Both Krueger and Zhu (2004a) and Mayer et al (2002) identify that many students received an NPR score
of 0. Neither points out that this score corresponds to a raw score of 99 (See Mayer et al. 2002 p. 32
footnote 10 – Students with a score of 0 were included in the generation of composite scores. Page 32 also
suggests that they include NPR scores ranging from 0-100. See also Krueger and Zhu (2004a) endnote 4,
which identifies the large concentration of scores of 0 that are included in the analysis while suggesting
perhaps these are not valid scores.
12

logistic regression. We calculate a predicted probability of being in the treatment group
̂ , and then construct weights of 1/ ̂ for those in the treatment group and 1/(1- ̂ ) for the
control group. These weights balance the treatment and control group on these observable
dimensions.
After accounting for the miscoded nonresponse by treating it as a missing value,
roughly 31 percent of the NYC voucher respondents have missing tests in reading or
math at baseline. Table 1 provides a detailed description of differences between treatment
and control groups in various types of missing data. As the table indicates, there are small
but statistically significant differences between the prevalence of missing data mistakenly
included as valid in the treatment and control groups using the Mathematica weights
(which were constructed while treating the miscoded nonresponse cases as valid data). In
particular, students in the treatment group are nearly one-third or 2.1 percentage points
more likely to have 99 values on the baseline reading test (although this difference is not
statistically significant at the 5 percent level, p=0.089). Students in the treatment group
were less than half as likely or 3.9 percentage points less likely to have 99 values on the
year 1 math test (p<0.01). Finally, treatment group students were 1.9 percentage points
more likely to have invalid 99 reading scores in year 3 (p=0.054). As Panels C and D in
Table 1 make clear, our inverse propensity score weights thoroughly account for these
differences.
[Table 1 about here]
Table 2 indicates that the inverse propensity score weighted data are well balanced
on the child’s gender, race/ethnicity, gifted or special education status, the family’s
annual income being low, whether the family speaks English at home, maternal years of

13

schooling, whether the mother works full time, whether the mother was born in the U.S.,
whether the family receives some form of public assistance, whether the family has lived
in their house for at least one year, and whether the mother is Catholic. Non-response to
these questions was quite low, and the only variable with more than 10 percent of the
observations missing information was whether the mother was U.S. born (in the 50 states
or DC, but not Puerto Rico), with 13.2 percent missing. Checks for whether the share of
observations missing information differed between the treatment and control groups
suggest that the shares were not significantly different.
[Table 2 about here]
There are no significant differences between treatment and control on either the raw
or percentile ITBS scores at baseline, further evidence of balance when the inverse
propensity score weights are utilized.
Methods
The analyses that follow take advantage of randomized assignment into the treatment
and control groups in the New York City voucher experiment to estimate the mean effect
of the voucher offer as well as its effect on the distribution of student achievement. The
potential outcomes model provides a framework for estimation of the effects of a
treatment. Each individual i has two potential outcomes, Y1i and Y0i (for our purposes, a
test score). Person i has outcome Y1i if assigned to the treatment group and outcome Y0i
if assigned to the control group. D(i) denotes the group that i is assigned to in a
randomized experiment. If person i is assigned to the treatment group, then D(i) = 1, and
if person i is assigned to the control group, D(i) = 0; the treatment effect on person i is
defined as di=Y1i – Y0i.

14

Quantiles, Average Treatment Effects, and Quantile Treatment Effects
Let Y be a random variable with a cumulative distribution function (CDF) F(y),
where F(y) = Pr[Y ≤ y]. Then, the qth quantile of the distribution F(y) is defined as the
smallest value yq such that F(yq) is at least as large as q (e.g., y0.5 is the median). Now
consider two (marginal) distributions F1 (the CDF for the potential outcomes if D = 1),
and F0 (the CDF for the potential outcomes if D = 0). We define the difference between
the qth quantiles of these two distributions as yq = yq1 – yq0, where yqd is the qth quantile
of distribution Fd.
The joint distribution of (Y0i,Y1i) is not identified without assumptions. However, if
program assignment is independent of the potential outcomes, the difference in means, or
average treatment effect, d = E[di] = E[Y1] - E[Y0], is identified because each expectation
requires only one of the two marginal distributions. Similarly, identification of the
marginal distributions implies identification of the quantiles yqd, and thus identification of
the differences in their quantiles, yq = yq1 – yq0. In this experimental setting, the quantile
treatment effect (QTE) is the estimate of this difference in the quantiles of the two
marginal distributions. For example, we consistently estimate the QTE at the 0.50
quantile by subtracting the control group’s sample median from the treatment group’s
sample median. Graphically, QTE estimates are the horizontal differences in the CDFs of
the outcome for the treatment and control groups.
As an example, we show the CDFs and QTE for the baseline math NPR scores in
Figures 2 and 3. Figure 2 shows the CDF for the baseline math scores in the treatment
and control groups. The horizontal distance between these CDFs at each point in the
distribution is the quantile treatment effect (QTE) at that point or quantile. Figure 3

15

translates the horizontal differences in the CDFs to a QTE plot, showing the QTE (y-axis)
for baseline math NPR scores at each percentile (x-axis), along with 95% confidence
intervals (dashed lines), calculated by bootstrapping families within strata. Figure 3
shows that the bulk of the QTE point estimates are zero or close to zero for the baseline
scores, and even when they are not, the confidence intervals clearly include zero. These
QTE estimates indicate that the NYC voucher data are well balanced on baseline
achievement after addressing weighting and missing data issues.
[Figures 2 and 3 about here]
Findings/Results
Revisions to previous mean treatment effect estimates
Since our preliminary analyses indicate that previous analyses using data from the
NYC voucher experiment included a substantial amount of miscoded missing data, we
begin by reconsidering the mean effect of the NYC voucher experiment. The first column
of Table 3 summarizes the results of the Krueger & Zhu (2004a) mean effect analyses,
which includes students who scored zero on the ITBS National Percentile Ranking/99 on
the raw test as non-missing cases, taken directly from Table 3b Panel 3. Their analysis
indicated that the NYC voucher offer had no mean effect on student mathematics or
reading achievement in any of the study’s three years. In the second column of Table 3,
we report our replication of the Krueger & Zhu analyses, again including students with
zero on the ITBS as non-missing cases. We are able to replicate Krueger & Zhu
coefficients precisely, with only minor differences in the standard errors that do not affect
the (lack of) significance of the coefficients.

16

In the third column of Table 3, we report our estimates of the mean effects of the
NYC voucher offer, estimated with out of-range values set to missing and using our
inverse propensity-score weights. The results reported in the third column of Table 3 are
substantively similar to the results in the prior two columns, indicating that the NYC
voucher program had no mean effect on math or reading achievement in any of its three
years. This finding suggests that the inclusion of students who were actually missing data
on the ITBS due to 99s but had a National Percentile Ranking score of 0 did not lead to
substantively different conclusions about the lack of a mean effect of receiving a voucher
in the NYC experiment but did in fact change the point estimates. This third column most
accurately captures the true effect of the NYC voucher offer, since these results do not
assume that students who were missing ITBS scores would have scored that the very
bottom of the test’s distribution.
[Table 3 about here]
Previous work by Mayer, et al. (2002) is not replicable with our restricted use data
given that weights have been changed since Krueger and Zhu discovered they were being
calculated incorrectly, but given Krueger and Zhu’s ability to replicate their results and
our ability to replicate the Krueger and Zhu results only while including the zeros, it
seems likely that results from Mayer and colleagues in their 2002 paper and in their 2003
reply to Krueger and Zhu also include respondents with out-of-range zero ITBS scores in
their analyses.
Finally, an additional set of papers, Jin and Rubin (2009) and Jin, Barnard, and
Rubin (2010) also use the NYC voucher data in their analyses. Jin, Barnard, and Rubin
do not present any basic descriptive statistics to reveal how they handled the missing

17

data, but Jin and Rubin do. Their Figure 1 presents box-and-whisker plots of total preand year 3 post-test scores of “complete cases” [their term] using the sum of the normal
curve equivalent (NCE) math and reading scores. Near replication of these plots is only
possible if the NCE scores of zero are treated as valid scores. Once excluded, the mean
NCE score increases from 28.7 to 32.2 in reading and 22.7 to 27.6 in math at baseline and
32.6 to 33.7 in reading and 32.5 to 33.8 in math on the year 3 post-test. This suggests that
results from Jin and Rubin, and possibly also Jin, Barnard, and Rubin, include out-ofrange test scores as valid data.
In sum, while our re-analysis corrects a data problem with earlier analysis of
NYCSCP data, our findings are substantively consistent with earlier findings: the
NYCSCSP had no mean effect on student math achievement overall (Howell et al. 2002;
Krueger & Zhu 2004a, 2004b). In supplementary analyses, we consider the consequences
of our corrections for the debate about whether the NYC voucher experiment has a
disproportionately positive effect for African-American students. While these analyses,
reported in Appendix A, do not resolve this dispute, they do draw attention to the
considerable skills overlap between racial categories in this sample.
Quantile treatment effect estimates
Having established that the voucher program had no mean effect on student
achievement, we next turn to the QTE, which provides an estimate of the effect of
voucher receipt on the distribution of student achievement. Figure 4 shows the QTE for
NPR math scores as of spring of the first year, Figure 5 shows NPR math scores for the
spring of the second year, and Figure 6 shows NPR math scores for the spring of the third
year. In each of these QTE plots, the horizontal differences in the cumulative

18

distributions of math NPR scores (y-axis) are plotted as a function of the percentile of the
distribution at which this difference is calculated (x-axis). Thus, the x-axis ranges from 199, and the y-value at each percentile q from 1-99 is the difference in the qth quantiles
from the treatment and control groups. This difference represents the horizontal distance
between the two CDFs for treatment and control.
Figure 4 shows the QTE for differences in math outcomes in year 1. For most of
the distribution, there are few test score differences between the voucher recipients and
the control students, as the solid line rarely deviates from the zero. This solid line shows
the difference between the math scores of the treatment and control children at each
percentile. For example, the 25th percentile treatment score is 6 and the 25th percentile
control score is 6, leading to a difference of zero NPR points, and the 75th percentile
treatment score is 34 and the 75th percentile control score is 37, leading to a difference of
-3 NPR points. Figure 4 shows that the difference between the treatment and control
students’ scores at each point along the distribution of math scores remains fairly similar
and close to zero. However, at the very top of the distribution, the difference between the
treatment and control students becomes larger and negative. For example, the 91st
percentile treatment score is 56 and the 91st percentile control score is 60, leading to a
difference of -4 NPR points, and the 97th percentile treatment score is 75, the 97th
percentile control score is 84, and the difference is -9 NPR points. This difference is
significant at the 5 percent level at the 97th percentile, where the confidence interval falls
below the zero line, but it is not significant at even the 10% level for any other percentile,
even though the treatment control difference indicated by the solid line is as low as -10 at
the 95th percentile. Thus, for the bulk of the distribution of achievement in math, effects

19

are zero, and we can rule out effects larger than 5 points at the 10% level for all but a
small group of students.
[Figure 4 about here]
Figure 5 shows the QTE for math NPR scores at the end of year 2. As in year 1, in
year 2 across most of the distribution, there are few differences between math scores for
treatment and control students. The solid line showing the treatment and control
differences is at or near zero, or negative but not significant at even the 10% level, for
most of the distribution. At the 93rd percentile, the difference between treatment and
control is the largest, at -9, but it is not significant at the 5% level as the confidence
interval includes zero. Only the difference at the 99th percentile is significantly different
from 0 at the 5% level.
[Figure 5 about here]
Figure 6 shows the QTE for math NPR scores at the end of year 3. Differences in
year 3 math scores are even less pronounced than in years 1 and 2. For most of the
distribution, the solid line displaying these differences is very near to the zero line--the
difference between treatment and control scores is -1, 0, or 1 for most of the distribution.
There are some larger treatment and control differences above the 89th percentile, with
the largest, negative difference of -5 occurring at the 99th percentile. However, none of
these differences are statistically significant, suggesting that whatever negative effect
emerged in the first two years has reverted to zero by the third year.
[Figure 6 about here]
We also estimated QTE using the same approach for reading at baseline and in
years 1 through 3. In each of the three years, the test score differences between treatment

20

and control were at or near zero for the entire distribution. At no point in any year were
these differences larger than three percentage points and at no point were the differences
statistically significant. These results are reported in Appendix Figures 1 through 4.
Discussion
These findings suggest that the NYC voucher experiment had no mean effect as
well as no effect on the distribution of student achievement. At first glance, these findings
seem to align closely with the predictions of the no-effects hypothesis. Before making
this conclusion, however, it is important to consider the extent to which the distribution
of achievement for students in the NYC voucher experiment reflects the distribution of
students who might be eligible for vouchers if a similar school choice policy were
implemented nationwide. Figure 7 places the NYC voucher program participants in the
broader context of elementary school achievement across the United States by comparing
the frequency of scores at various percentiles of the national distribution for baseline
math for students in both the treatment and the control groups of the NYC voucher
experiment with the frequency of math achievement scores for all students in the
nationally representative Early Childhood Longitudinal Study, Kindergarten Cohort
(ECLS-K) who attend Catholic schools as well as all ECLS-K students who come from
low-income homes.7 This comparison illustrates the stark educational disadvantage that
students eligible for the NYC voucher experiment and other poor youth face. While the
achievement distribution for the NYC voucher students at baseline is skewed to the left

For the ECLS‐K, we constructed the low income public school distribution so as to best match the
children in the voucher experiment while still having sufficient sample size. The voucher children are
all eligible for free lunch. Our comparison children are either obtaining free lunch or on welfare or
under poverty (the closest proxy in the public use data to being under 130% of poverty). The ECLS‐K
scores are for spring of first and third grade, about midway between the baseline voucher scores,
which are in grades 1‐4.

7

21

relative to that of poor youth nationwide, it is skewed even more sharply to the left
compared to Catholic school students nationwide. This fact has potentially important
implications for interpreting the results of the NYC voucher experiment. While our
analyses clearly indicate that this treatment had no effect for this set of students at the
bottom of the skill distribution, it provides little grounds for inference regarding the
effects of voucher programs on a broader range of students. Since the NYC voucher
study includes few students above the middle of a broader test score distribution, we
cannot make strong statements about the effects of vouchers on students at the top of a
broader distribution. Distributional analyses of less strictly means-tested voucher
programs, such as the statewide programs operating in Indiana, Florida, Georgia, may
thus produce very different findings higher up in the achievement distribution.
[Figure 7 about here]
Our findings suggest that the NYC voucher experiment had little effect across the
distribution of student achievement, with the possible exception of small negative effects
in math at the top of the distribution of students who sought vouchers which fade out over
time. This may not be so surprising given the size of the intervention, although the offer
had a very large effect on take-up of private school.
These small distributional findings mostly disconfirm both the Common School
and Stratifying hypotheses. To the extent that vouchers are used to attend schools with a
common curriculum, this seems to have had none of the anticipated positive effects for
low-achievers. However, the math findings may be somewhat consistent with the
Common Schools hypothesis because the high-achievers (relative to the average
achievement in the experimental sample) experienced some small penalties from the

22

voucher offer. Similarly, any attempts made of the part of parents of high-achievers to
pursue schools with challenging coursework did not lead to the hypothesized benefits or
the stratification that voucher opponents feared. If anything, the distributional findings
confirm our third hypothesis, that vouchers (at least of this magnitude) have no positive
or negative effect for the majority of students to whom they were offered.
Put in the context of other interventions, such as KIPP or charter schools, perhaps
these null-effects findings are not surprising. These interventions, which involve the
development of entirely new schools, have significant impacts, but at much greater
expense. For example, evaluations of KIPP Lynn in Massachusetts found that a year of
enrollment in KIPP resulted in average effects of 0.35 SD in math and 0.12 SD in
reading, with the students entering KIPP with the lowest baseline scores experiencing the
largest effects (Angrist et al. 2010). Experimental evaluations of New York City and
Boston charter schools found more modest effects on student achievement. In New York,
the average effects were 0.09 SD in math and 0.065 SD in reading (Hoxby, Murarka, and
Kang 2009). In Boston, the average effects were 0.18 SD in math and 0.09 SD in reading
(Abdulkadiroglu et al. 2011). These results, which come from programs that were
considerably more comprehensive than the vouchers we evaluate, likely serve as an upper
bound of the possible achievement impacts that we might have observed. While the
vouchers cost $1,400 per child, a year of enrollment at KIPP costs approximately $13,000
per student at some of the east coast KIPP schools (Angrist, Dynarski, Kane, Pathak, and
Walters 2012). Thus, perhaps it would take a much larger financial investment to see
effects that are comparable to a program like KIPP.

23

Despite the nearly null distributional findings, examining the New York voucher
data with a distributional lens yielded other important information that would not have
otherwise been discovered. We uncovered unusually large concentrations of test score
responses with a raw score of 99 and an NPR or NCE score of 0 when early results
returned unbelievably large group differences at the tails of the distribution. Only when
we included the observations with these missing data codes in our analyses were we able
to replicated previously published analyses of these data. While excluding these codes
does not change the substantive conclusions from previous results, it reduces the
magnitude of even the most favorable previous findings.
In sum, our distributional analysis of the New York City voucher experiment
shows that the offer of a small voucher did little to influence student achievement. The
possible exception is a small negative effect for a small group of high-performing
students after the first two years of the program, but not after the third. The distributional
approach taken here provides additional evidence suggesting that vouchers have a limited
impact on student achievement.

24

References
Abdulkadiroğlu, A., Angrist, J. D., Dynarski, S. M., Kane, T. J., & Pathak, P. A. (2011).
Accountability and flexibility in public schools: Evidence from Boston’s charters
and pilots. The Quarterly Journal of Economics, 126(2), 699–748.
Angrist, J., Bettinger, E., & Kremer, M. (2006). Long-Term Educational Consequences
of Secondary School Vouchers: Evidence from Administrative Records in
Colombia. The American Economic Review, 96(3), 847-862.
Angrist, J. D., Dynarski, S. M., Kane, T. J., Pathak, P. A., & Walters, C. R. (2012). Who
Benefits from KIPP? Journal of Policy Analysis and Management, 31(4), 837–
860.
Angrist, J. D., Dynarski, S. M., Kane, T. J., Pathak, P. A., & Walters, C. R. (2010). Inputs
and impacts in charter schools: KIPP Lynn. The American Economic Review,
100(2), 239–243.
Argys, L. M., Rees, D. I., & Brewer, D. J. (1996). Detracking America's Schools: Equity
at zero cost? Journal of Policy analysis and Management, 15(4), 623-645.
Barnard, J., Frangakis, C.E., Hill, J.L., & Rubin, D.B. (2003). Principal stratification
approach to broken randomized experiments: A case study of school choice
vouchers in New York City. Journal of the American Statistical Association, 98.
Chakrabarti, R. (2013). Vouchers, public school response, and the role of incentives:
Evidence from Florida. Economic Inquiry, 51(1), 500-526.
Chingos, M. M., & Peterson, P. E. (2012). The Effects of School Vouchers on College
Enrollment: Experimental Evidence from New York City. Brookings Institution.
Chubb, J.E. & Moe, T.M. (1990). Politics, Markets, and America’s Schools. Washington,
DC: Brookings Institute Press.
Coleman, J. S., & Hoffer, T. (1987). Public, Catholic, and private schools: The
importance of community. New York: Basic.
Cowen, J. M. (2008). School choice as a latent variable: Estimating the “complier
average causal effect” of vouchers in Charlotte. Policy Studies Journal, 36(2),
301-315.
Cowen, J. M. (2012). Interpreting School Choice Effects: Do Voucher Experiments
Estimate the Impact of Attending Private School?. Journal of Research on
Educational Effectiveness, 5(4), 384-400.
Duncan, G. J., & Vandell, D. L. (2012). A Conceptual Approach to Understanding
Treatment Heterogeneity in Human Capital Interventions. Society for Research on
25

Educational Effectiveness.
Elacqua, G., Schneider, M. & Buckley, J. (2006). School Choice in Chile: Is It Class or
the Classroom? Journal of Policy Analysis and Management, 25, 577–601.
Evans, W. & Schwab, R. (1995). Finishing high school and starting college: Do Catholic
schools make a difference? Quarterly Journal of Economics, 110, 941–974.
Friedman, M. & Friedman, R. (1980). Free to Choose. New York: Harcourt Brace
Jovanovich.
Greeley, A. M. (1982). Catholic high schools and minority students. Transaction
Publishers.
Greene, J. P. (2001). Vouchers in Charlotte. Education matters, 1(2), 55-60.
Greene, J. P., Howell, W. G., & Peterson, P. E. (1997). Lessons from the Cleveland
scholarship program. Education Policy and Governance, Harvard University.
Greene, J. P., Peterson, P. E., & Du, J. (1997). Effectiveness of school choice: The
Milwaukee experiment. Program in Education Policy and Governance and Center
for American Political Studies, Harvard University.
Greene, J., Peterson, P., & Du, J. (1998). School choice in Milwaukee. Learning from
school choice, 335-356.
Greene, J. P., & Winters, M. A. (2003). When schools compete: The effects of vouchers
on Florida public school achievement. Center for Civic Innovation, Manhattan
Institute.
Hartocollis, A. (1997). School Voucher Experiment Will Be Extended and Expanded.
The New York Times, NY Region, November 26.
Hastings, J., Kane, T., & Staiger, D. (2005). Parental preferences and school
competition: evidence from a public school choice program. National Bureau of
Economic Research Working Paper No.11805.
Hill, J.L, Rubin, D.B., & Thomas, N. (2001). The design of the New York School
Choice Scholarships Program evaluation. In L. Bickman (Ed.), Research Design:
Donald Campbell’s Legacy (Vol. II). Thousand Oaks, CA: Sage.
Hoffer, T., Greeley, A. M., & Coleman, J. S. (1985). Achievement growth in public and
Catholic schools. Sociology of Education, 74-97.
Hong, G., Corter, C., Hong, Y., & Pelletier, J. (2012). Differential Effects of Literacy
Instruction Time and Homogeneous Ability Grouping in Kindergarten
Classrooms Who Will Benefit? Who Will Suffer?. Educational Evaluation and
Policy Analysis, 34(1), 69-88.
26

Hoover, H.D., Dunbar, S.B., & Frisbie, D.A. Iowa Tests of Basic Skills (ITBS_ Forms
A, B, and C. Retrieved April 2, 2013 from
http://www.riversidepublishing.com/products/itbs/details.html.
Howell, W. & Peterson, P. (2000). School choice in Dayton, Ohio: An evaluation after 1
year. Paper prepared for the conference on charters, vouchers and public
education sponsored by the program on education policy and governance,
Kennedy School of Government, Harvard University.
Howell, W.G., Wolf, P.J., Campbell, D.E., & Peterson, P.E. (2002). School vouchers
and academic performance: Results from three randomized field trials. Journal of
Policy Analysis and Management, 21, 191–217.
Hoxby, C. M., Murarka, S., & Kang, J. (2009). How New York City’s charter schools
affect achievement (No. 2). Cambridge, MA New York City Charter Schools
Evaluation Project.
Jin, H., Barnard, J., & Rubin, D. B. (2010). A modified general location model for
noncompliance with missing data revisiting the New York City School Choice
Scholarship Program using principal stratification. Journal of Educational and
Behavioral Statistics, 35(2), 154–173.
Jin, H., & Rubin, D. B. (2009). Public schools versus private schools: Causal inference
with partial compliance. Journal of Educational and Behavioral Statistics, 34(1),
24–45.
Krueger, A. & Zhu, P. (2004a). Another look at the New York City school voucher
experiment. American Behavioral Scientist, 47, 658–698.
Krueger, A. & Zhu, P. (2004b). Inefficiency, subsample selection bias, and
nonrobustness. American Behavioral Scientist, 47, 718–728.
Kupermintz, H. (2002). The Effects of Vouchers on School Improvement: Another Look
at the Florida Data. Center for the Study of Evaluation, National Center for
Research on Evaluation, Standards, and Student Testing, Graduate School of
Education & Information Studies, University of California, Los Angeles.
Ladd, H. F., & Fiske, E. B. (2003). Does competition improve teaching and learning?
Evidence from New Zealand. Educational Evaluation and Policy Analysis, 25(1),
97-112.
Lara, B., Mizala, A., Repetto, A. (2011). The effectiveness of private voucher education:
Evidence from structural school switchers. Educational Evaluation and Policy
Analysis 33, 119-137.
Mayer, D.P., Peterson P.E., Myers, D.E., Tuttle, C.C., & Howell, W.G. (2002). School
choice in New York City after three years: An evaluation of the school choice
scholarship program. Mathematica Policy Research, Inc.: Washington, D.C.
27

McEwan, P. J., & Carnoy, M. (2000). The effectiveness and efficiency of private
schools in Chile's voucher system. Educational evaluation and policy analysis,
22(3), 213-239.
Morgan, S.L. (2001). Counterfactuals, causal effect heterogeneity, and the Catholic
school effect on learning. Sociology of Education, 74, 341–374.
Neal, D. A. (1997). The Effects of Catholic Secondary Schooling on Educational
Achievement. Journal of Labor Economics, 15(1), 98-123.
Peterson, P.E., & Howell, W.G. (2004). Efficiency, bias, and classification schemes: A
response to Alan B. Krueger and Pei Zhu. American Behavioral Scientist, 47,
699–717.
Peterson, P.E., Howell, W. G., & Greene, J. P. (1999) An Evaluation of the Cleveland
Voucher Program after Two Years.
Peterson, P. E., Myers, D., & Howell, W. G. (1999). An Evaluation of the Horizon
Scholarship Program in the Edgewood Independent School District, San Antonio,
Texas: The First Year.
Rouse, C. E. (1998). Private school vouchers and student achievement: An evaluation of
the Milwaukee parental choice program. The Quarterly journal of economics,
113(2), 553-602.
Steinberg, J. (1997a). Giuliani Sees Tuition Plan Set for Fall. The New York Times, NY
Region, February 4.
Steinberg, J. (1997b). Students Chosen for Grants To Attend Private Schools. The New
York Times, NY Region, May 13.
Witte, J. F. (1998). The Milwaukee voucher experiment. Educational Evaluation and
Policy Analysis, 20(4), 229-251.
Wolf, P. J., Howell, W. G., & Peterson, P. E. (2000). School choice in Washington, DC:
An evaluation after one year. Program on Educational Policy and Governance,
Harvard University.
Wolf, P. J., Kisida, B., Gutmann, B., Puma, M., Eissa, N., & Rizzo, L. (2013). School
Vouchers and Student Outcomes: Experimental Evidence from Washington, DC.
Journal of Policy Analysis and Management.

28

Appendix A:
One important point of contention in prior analyses of the NYC voucher
experiment involves variation in the effect of the voucher offer by race and ethnicity.
Several studies find that the voucher offer had a small positive effect on the academic
achievement of African-American recipients (Barnard, Frangakis, Hill, & Rubin, 2003;
Howell, Wolf, Campbell, & Peterson 2002; Peterson and Howell, 2004). However,
subsequent analyses suggest that the observed effects for African Americans are sensitive
to the definition of racial and ethnic categories and hold only when controlling for
students’ initial characteristics/omitting students without baseline scores (Krueger & Zhu,
2004a, 2004b).
This debate is potentially consequential in two regards: first, evidence of a unique
positive voucher effect for African-Americans may point toward a strategy to mitigate
persistent and troublesome black-white test score gaps. Second, several analysts have
suggested that evidence of a unique positive voucher effect for African-Americans is
consistent with the idea embedded in the “common school” hypothesis that vouchers may
be particularly beneficial for students at the bottom of the skills distribution.
In this appendix, we reconsider the evidence regarding the extent to which NYC
voucher offer effects vary by student race and ethnicity in light of the missing data and
weighting corrections that we have implemented. In doing so, we note that it is important
to consider several distinctive characteristics of the NYC voucher experiment sample. By
design, all of the students who participated in the NYC voucher experiment were from
low-income families in New York City. As Table 2 makes clear, the vast majority of
these students were black or Hispanic. Within these racial categories, however, lies a

29

great deal of ethnic heterogeneity. 15 percent of students identified as African-American
come from immigrant families, with origins primarily from the Caribbean. Similarly, the
Hispanic category includes Puerto Rican and Dominican students (many of which may be
phenotypically black). This heterogeneity helps to explain the debates concerning the
definition of African Americans in these data. While Howell and Peterson categorize all
students whose mother indicated her race at baseline as African American as AfricanAmericans, Krueger and Zhu additionally categorize children as African American if
their mother indicated her race was African American in a subsequent data collection
wave, if the mother indicated her race was other, but wrote in some combination of
Black/African American and something else as her race (e.g., Black/Hispanic), or if the
father indicated his race was African American in the baseline wave. Our analyses
indicate that these definitions likely yield common racial categorizations for 90 percent of
students in the sample, but disagree for 10 percent of students in the sample.
Appendix Table 1 summarizes the consequences of these questions of racial
categorization for estimating the effect of the NYC voucher experiment on AfricanAmerican students’ mathematics achievement. In first model of Panel 1 (column 1), we
replicate Howell & Peterson’s estimates of the treatment effect for African-Americans
(point estimates are identical, SEs nearly so). This analysis indicates that the voucher
offer significantly improved black student math achievement in the study’s first and third
years. (This analysis yields a positive, but not statistically significant, treatment effect for
black students in Year 2.) Similarly, in the first model of Panel 2 (column 1), we attempt
to replicate Krueger & Zhu’s racial categorization scheme to estimate of the effects of the
voucher offer for African-Americans. While this replication is not perfect (our sample

30

sizes are 1 observation off from their reported ones),8 it returns an estimate of the
African-American treatment effect that is very close to Krueger & Zhu’s published
findings. Using the Krueger & Zhu definition of African-American but treating the 99s as
valid percentile scores of 0, we find a positive significant treatment effect on Math scores
in Year 1, but no effects in subsequent years.
The subsequent models (columns) in Appendix Table 1 consider the extent to
which these findings are sensitive to corrections for out-of-range values on the ITBS and
non-response weighting. Model 2 replicates both analyses with a sample that excludes
students who have out-of-range values on the ITBS with the original MPR weights;
Model 3 replicates the Howell & Peterson and Krueger & Zhu analyses on the original
sample (including students who have out-of-range values on the ITBS as non-missing
zeros) with our inverse propensity score weights; and model 4 replicates both analyses
with a sample that excludes students who have out-of-range values on the ITBS and our
inverse propensity score weights.
We focus particular attention on the results reported in Model 4, since we believe
that this model most thoroughly accounts for missing data and attrition. In most cases,
these analyses return estimates of the effect of the NYC voucher offer for AfricanAmericans that are between 36 and 99% of the Howell & Peterson estimates and between
8

The recoding described in Krueger & Zhu provides some contradictory information about which cases
were recoded. In the text, it suggests that students were recoded if: 1. Their mother listed her race as
African American in a subsequent wave; 2. If the father listed his race as African American in the baseline
wave; and 3. If a parent indicated that their race was “other” and wrote in an entry that included the words
black or African American in combination with something else or abbreviated in an obvious manner. In a
footnote, they suggested this recoding only occurred if the mother used a write in response, but not the
father. To match their sample sizes within one case, we used only the mother’s write-in responses. If the
father’s write-in responses were included, the sample size was too large. Given that it is not possible to
know exactly which write-in cases for either the mother or the father were recoded, our replication of the
coefficients in this table is not exact. Their coefficients and standard errors for the alternative version of
African American subgroup including the full sample and controls for randomization block presented in
Table 5 Panel 2, for reading are 1.36 (1.82) in year 1, 1.57 (1.81) in year 2, and 0.99 (1.84) in year 3, and
for math, are 3.34 (1.63) in year 1, 1.15 (1.93) in year 2, and 3.04 (1.85) in year 3.
31

41 and 74% of the size of our replication of the Krueger and Zhu estimates. Using the
Howell & Peterson definition, the Model 4 analysis returns a significant positive
treatment effect for African-Americans for math in Year 3, but not in other years. Using
the Krueger & Zhu definition, the Models 2, 3, and 4 return no significant treatment
effects for African-Americans for math.
Elsewhere, analysts have viewed this evidence pointing to a unique positive and
significant voucher effect for African-Americans as an indication that vouchers may have
unique positive consequences for students at the bottom of the skill distribution.
However, a distributional analysis suggests that this interpretation may be misleading in
the context of the NYC voucher data. In Appendix Figure 5, we show that the blacks (and
because the sample is nearly entirely blacks and Hispanics, also Hispanics) are relatively
evenly located across the overall baseline test score distribution. The x-axis in Appendix
Figure 5 represents the percentiles of the overall baseline test score distribution for the
control group. The y-axis denotes the share of the observations between the percentiles at
which we calculated the QTE that are black using both the Howell and Peterson and
Krueger and Zhu definitions. So, if these lines were horizontal, it would be equivalent to
the statement that the blacks are uniformly distributed across the baseline score
distribution. As Appendix Figure 5 indicates, black students are distributed
approximately evenly across the overall test score distribution in the NYC voucher data.
This finding may not be particularly surprising, given the fact that all participants in this
study are low-income New York City youth. However, it represents an important piece of
context to consider in interpreting evidence of heterogeneous effects in this experiment.

32

Figure 1:

Histogram of raw ITBS math items correct at base-

line

0

.02

Density
.04

.06

.08

Number correct on ITBS math (raw) score

0

20

40
60
number correct, math total

80

100

Notes: Figure shows histogram of raw number of math items correct on baseline ITBS test as
reported in the public-use version of the data. The large point mass at 99 represents those individuals with ITBS raw math scores of 99 and associated National Percentile Ranking scores of
0, and represents a missing data code. Data from the New York City School Choice Scholarships
Program evaluation conducted by Mathematica Policy Research. Baseline scores unavailable for
kindergarten students.

Cumulative distribution functions (CDFs) of math
National Percentile Rankings for the treatment and control
groups for the baseline year
Figure 2:

0

20

Cumulative Percent
40
60
80

100

CDFs for treatment and control groups
Baseline math percentile scores

0

20

40
60
Math Percentile Score
Treatment Group

80

100

Control Group

Notes: Figure shows cumulative distribution functions for baseline math National Percentile Ranking scores from the Iowa Test of Basic Skills separately for the treatment (voucher offer) and control
(no voucher offer) groups. Estimates are weighted using inverse propensity score weights. Weights
are 1/p̂ for treatment observations and 1/(1 − p̂) for control observations, where p̂ is generated from
a logistic regression of treatment status on baseline demographics, sample design variables, and
baseline test scores. Data from the New York City School Choice Scholarships Program evaluation
conducted by Mathematica Policy Research.

Quantile treatment effect estimates of the impact
of a voucher offer on math National Percentile Rankings at
baseline
Figure 3:

−20 −15 −10 −5

0

5

10

15

20

QTE for baseline math percentile scores

0

20

40

60

80

100

Quantile
Lower end of 95% CI
Upper end of 95% CI

QTE

Notes: Figure shows QTE for the effect of being offered a voucher for private school on baseline
math National Percentile Ranking scores from the Iowa Test of Basic Skills. Estimates are weighted
using inverse propensity score weights. Weights are 1/p̂ for treatment observations and 1/(1 − p̂)
for control observations, where p̂ is generated from a logistic regression of treatment status on
baseline demographics, sample design variables, and baseline test scores. 95% CIs are obtained by
bootstrapping families with replacement. Data from the New York City School Choice Scholarships
Program evaluation conducted by Mathematica Policy Research. Baseline scores unavailable for
kindergarten students.

Quantile treatment effect estimates of the impact
of a voucher offer on math National Percentile Rankings
for the first year after random assignment
Figure 4:

−20 −15 −10 −5

0

5

10

15

20

QTE for Year 1 math percentile scores

0

20

40

60

80

100

Quantile
Lower end of 95% CI
Upper end of 95% CI

QTE

Notes:
Figure shows QTE for the effect of being offered a voucher for private school on math National
Percentile Ranking scores from the Iowa Test of Basic Skills in the spring after the first year of
voucher distribution. Estimates are weighted using inverse propensity score weights. Weights are
1/p̂ for treatment observations and 1/(1 − p̂) for control observations, where p̂ is generated from
a logistic regression of treatment status on baseline demographics, sample design variables, and
baseline test scores. 95% CIs are obtained by bootstrapping families with replacement. Data from
the New York City School Choice Scholarships Program evaluation conducted by Mathematica
Policy Research.

Quantile treatment effect estimates of the impact
of a voucher offer on math National Percentile Rankings
for the second year after random assignment
Figure 5:

−20 −15 −10 −5

0

5

10

15

20

QTE for Year 2 math percentile scores

0

20

40

60

80

100

Quantile
Lower end of 95% CI
Upper end of 95% CI

QTE

Notes:
Figure shows QTE for the effect of being offered a voucher for private school on math National
Percentile Ranking scores from the Iowa Test of Basic Skills in the spring after the second year of
voucher distribution. Estimates are weighted using inverse propensity score weights. Weights are
1/p̂ for treatment observations and 1/(1 − p̂) for control observations, where p̂ is generated from
a logistic regression of treatment status on baseline demographics, sample design variables, and
baseline test scores. 95% CIs are obtained by bootstrapping families with replacement. Data from
the New York City School Choice Scholarships Program evaluation conducted by Mathematica
Policy Research.

Quantile treatment effect estimates of the impact
of a voucher offer on math National Percentile Rankings
for the third year after random assignment
Figure 6:

−20 −15 −10 −5

0

5

10

15

20

QTE for Year 3 math percentile scores

0

20

40

60

80

100

Quantile
Lower end of 95% CI
Upper end of 95% CI

QTE

Notes:
Figure shows QTE for the effect of being offered a voucher for private school on math National
Percentile Ranking scores from the Iowa Test of Basic Skills in the spring after the third year of
voucher distribution. Estimates are weighted using inverse propensity score weights. Weights are
1/p̂ for treatment observations and 1/(1 − p̂) for control observations, where p̂ is generated from
a logistic regression of treatment status on baseline demographics, sample design variables, and
baseline test scores. 95% CIs are obtained by bootstrapping families with replacement. Data from
the New York City School Choice Scholarships Program evaluation conducted by Mathematica
Policy Research.

Histogram of scores falling at various points in the
overall national distribution for a national sample of lowincome children in public schools (ECLS-K Spring 1st and
3rd), all children in Catholic schools (ECLS-K Spring 1st
and 3rd), and New York City School Choice Scholarships
Program children at baseline in 1st, 2nd, 3rd and 4th

Figure 7:

0

1

2

3

Math

0

.2

.4
.6
test score percentile
Poor public school
Voucher students

.8

1

Catholic school

Notes: Figure shows histogram of percentiles of from the ELCS-K overall public school distribution
among poor kids (income low enough for free lunch or on welfare) and for all Catholic school attendees in Spring of first and third grades and well as percentiles of the National Percentile Ranking
from the ITBS for the pre-tests for grades 1, 2, 3, and 4 from the New York City School Choice
Scholarships Program evaluation conducted by Mathematica Policy Research. Baseline scores unavailable for kindergarten students in NYCSCSP and scores for end of second grade unavailable in
the ECLS K. Statistics weighted to reflect non-response and complex sampling.

Imbalance in incidence of “missing data” values
across treatment and control groups
Table 1:

Control mean T-C difference
SE
Panel A: Differences in missing scores, Mathematica weights
Missing score, baseline, math/reading
0.310
-0.015
0.020
Panel B: Differences in invalid (99) scores, Mathematica weights
Invalid (99) score, baseline, math
0.120
0.012
0.015
Invalid (99) score, baseline, reading
0.066
0.021
0.012
Invalid (99) score, year 1, math
0.067
-0.039
0.010
Invalid (99) score, year 1, reading
0.036
0.001
0.008
Invalid (99) score, year 2, math
0.068
0.016
0.015
Invalid (99) score, year 2, reading
0.027
0.008
0.009
Invalid (99) score, year 3, math
0.046
-0.013
0.010
Invalid (99) score, year 3, reading
0.022
0.019
0.010
Panel C: Differences in missing scores, authors’ inverse p-score weights
Missing score, baseline, math/reading
0.309
0.002
0.019
Missing score, year 1, math/reading
0.219
0.002
0.020
Missing score, year 2, math/reading
0.343
0.003
0.024
Missing score, year 3, math/reading
0.329
0.003
0.024
Panel D: Differences in invalid (99) scores, authors’ inverse p-score weights
Invalid (99) score, baseline, math
0.122
-0.001
0.014
Invalid (99) score, baseline, reading
0.075
-0.0004
0.011
Invalid (99) score, year 1, math
0.039
-0.0004
0.008
Invalid (99) score, year 1, reading
0.032
0.0006
0.007
Invalid (99) score, year 2, math
0.050
-0.001
0.009
Invalid (99) score, year 2, reading
0.022
-0.0005
0.006
Invalid (99) score, year 3, math
0.025
-0.001
0.006
Invalid (99) score, year 3, reading
0.021
0.0006
0.007

P-value

N

0.444

2666

0.425
0.089∗
0.000∗∗∗
0.944
0.301
0.379
0.226
0.054∗

2666
2666
2080
2080
1754
1754
1801
1801

0.929
0.915
0.895
0.883

2662
2662
2662
2662

0.936
0.973
0.964
0.937
0.910
0.934
0.867
0.921

2662
2662
2662
2662
2662
2662
2662
2662

Notes: Table reports treatment control differences for baseline and year 1–3 missing test scores and the invalid 99
raw/0 percentile test scores. Panels A and B report the differences with the MPR non-response weights, and Panels
C and D with our inverse propensity score weights. Column 1 reports the control group mean, column 2 the TC difference, column 3 the SE on this difference, and column 4 the p-value. The inverse p-score weights are 1/p̂
for treatment observations and 1/(1 − p̂) for control observations, where p̂ is generated from a logistic regression
of treatment status on baseline demographics, sample design variables, and baseline test scores. SEs clustered by
family. Data from the New York City School Choice Scholarships Program evaluation conducted by Mathematica
Policy Research.

Balance in Xs and test scores at baseline across
treatment and control groups, our inverse propensity-score
weights
Table 2:

Child characteristics
Male
African-American
Hispanic
Labeled gifted
Labeled special education
Speaks English at home
Mother/family characteristics
Mother’s years of schooling
Mother works full time
Mother born in the US (not PR)
Family gets some welfare
Mother in same house 1 year ago
Mother is Catholic
Probability income ≤ $15, 000
Baseline test scores (no K scores)
Math score
Reading score

Control mean

T-C difference

SE

P-value

0.496
0.438
0.489
0.109
0.110
0.764

0.001
-0.002
-0.0001
-0.0007
0.0007
-0.003

0.019
0.019
0.019
0.012
0.012
0.017

0.951
0.932
0.994
0.958
0.958
0.854

12.977
0.215
0.600
0.779
0.914
0.539
0.502

-0.006
0.0002
0.001
0.001
0.0005
-0.00031
-0.001

0.069
0.017
0.020
0.016
0.011
0.020
0.025

0.932
0.989
0.960
0.947
0.965
0.988
0.958

19.79
25.57

0.012
-0.111

1.004
1.067

0.990
0.917

Notes:
Table reports treatment control differences for baseline demographics and baseline test scores, treating the invalid 99
scores as missing. Column 1 reports the control group mean, column 2 the T-C difference, column 3 the SE on this
difference, and column 4 the p-value. The inverse p-score weights are 1/p̂ for treatment observations and 1/(1 − p̂)
for control observations, where p̂ is generated from a logistic regression of treatment status on baseline demographics,
sample design variables, and baseline test scores. SEs clustered by family. Mother born in the US denotes born in
one of the 50 states and Washington DC¡ and not Puerto Rico, and family welfare use denotes use of Food Stamps,
AFDC/public assistance, Social Security, or Medicaid. The probability that income is less than or equal to $15,000
is reported in the table, the specifications (following others) control for the natural log of the midpoint of income
ranges. A small number of observations are missing demographics. Data from the New York City School Choice
Scholarships Program evaluation conducted by Mathematica Policy Research.

Table 3: Effect of excluding “missing data” values on mean
treatment effects reported in Krueger and Zhu (2004a)
Using Mathematica weights Using our inverse p-score weights
K&Z estimates
Our replication
Our estimates
0s included
0s included
0s excluded
Panel A: Effects for full sample, math National Percentile Rankings
Year 1
0.17
0.17
-0.88
(1.34)
(1.38)
(1.05)
Year 2
-0.69
-0.69
-1.29
(1.40
(1.37)
(1.15)
Year 3
0.23
0.23
-0.004
(1.35)
(1.28)
(1.18)
Panel B: Effects for full sample, reading National Percentile Rankings
Year 1
-0.84
-0.84
-1.79
(1.25)
(1.32)
(1.09)
Year 2
0.41
0.41
0.22
(1.30)
(1.26)
(1.16)
Year 3
-0.73
-0.73
0.52
(1.26)
(1.32)
(1.13)
Notes: Table reports original results from panel 3 of Table 3B of Krueger and Zhu (2004a), our replication of these
results, and then shows the impact of excluding the 0 percentile values (which are invalid percentiles corresponding to
the 99 raw scores) and using inverse propensity score weights as an alternative to the non-response adjusted weights
provided with the Mathematica data. Dependent variable is the math (Panel A) or reading (Panel B) National
Percentile Ranking scores from the Iowa Test of Basic Skills in the spring for years 1–3 of voucher distribution.
Regressions also control for dummies for the strata in the initial sampling. Estimates in column 2 use the Mathematica
provided non-response weights, while those in 3 use our inverse propensity score weights. The p-score weights are 1/p̂
for treatment observations and 1/(1 − p̂) for control observations, where p̂ is generated from a logistic regression of
treatment status on baseline demographics, dummies for missing demographics, dummies for invalid scores or missing
scores, dummies for strata (sample design) and grade at baseline, and baseline test scores. 95% CIs are obtained
by bootstrapping families with replacement. Data from the New York City School Choice Scholarships Program
evaluation conducted by Mathematica Policy Research.

Quantile treatment effect estimates of the
impact of a voucher offer on reading National Percentile
Rankings at baseline
Appendix Figure 1:

−20 −15 −10 −5

0

5

10

15

20

QTE for baseline reading percentile scores

0

20

40

60

80

100

Quantile
Lower end of 95% CI
Upper end of 95% CI

QTE

Notes:
Figure shows QTE for the effect of being offered a voucher for private school on baseline reading
National Percentile Ranking scoress from the Iowa Test of Basic Skills. Estimates are weighted
using inverse propensity score weights. Weights are 1/p̂ for treatment observations and 1/(1 − p̂)
for control observations, where p̂ is generated from a logistic regression of treatment status on
baseline demographics, sample design variables, and baseline test scores. 95% CIs are obtained by
bootstrapping families with replacement. Data from the New York City School Choice Scholarships
Program evaluation conducted by Mathematica Policy Research. Baseline scores unavailable for
kindergarten students.

Quantile treatment effect estimates of the
impact of a voucher offer on reading National Percentile
Rankings for the first year after random assignment
Appendix Figure 2:

−20 −15 −10 −5

0

5

10

15

20

QTE for Year 1 reading percentile scores

0

20

40

60

80

100

Quantile
Lower end of 95% CI
Upper end of 95% CI

QTE

Notes:
Figure shows QTE for the effect of being offered a voucher for private school on reading National
Percentile Ranking scores from the Iowa Test of Basic Skills in the spring after the first year.
Estimates are weighted using inverse propensity score weights. Weights are 1/p̂ for treatment
observations and 1/(1 − p̂) for control observations, where p̂ is generated from a logistic regression
of treatment status on baseline demographics, sample design variables, and baseline test scores.
95% CIs are obtained by bootstrapping families with replacement. Data from the New York City
School Choice Scholarships Program evaluation conducted by Mathematica Policy Research.

Quantile treatment effect estimates of the
impact of a voucher offer on reading National Percentile
Rankings for the second year after random assignment
Appendix Figure 3:

−20 −15 −10 −5

0

5

10

15

20

QTE for Year 2 reading percentile scores

0

20

40

60

80

100

Quantile
Lower end of 95% CI
Upper end of 95% CI

QTE

Notes:
Figure shows QTE for the effect of being offered a voucher for private school on reading National
Percentile Ranking scores from the Iowa Test of Basic Skills in the spring after the second year.
Estimates are weighted using inverse propensity score weights. Weights are 1/p̂ for treatment
observations and 1/(1 − p̂) for control observations, where p̂ is generated from a logistic regression
of treatment status on baseline demographics, sample design variables, and baseline test scores.
95% CIs are obtained by bootstrapping families with replacement. Data from the New York City
School Choice Scholarships Program evaluation conducted by Mathematica Policy Research.

Quantile treatment effect estimates of the
impact of a voucher offer on reading National Percentile
Rankings for the third year after random assignment
Appendix Figure 4:

−20 −15 −10 −5

0

5

10

15

20

QTE for Year 3 reading percentile scores

0

20

40

60

80

100

Quantile
Lower end of 95% CI
Upper end of 95% CI

QTE

Notes:
Figure shows QTE for the effect of being offered a voucher for private school on reading National
Percentile Ranking scores from the Iowa Test of Basic Skills in the spring after the third year.
Estimates are weighted using inverse propensity score weights. Weights are 1/p̂ for treatment
observations and 1/(1 − p̂) for control observations, where p̂ is generated from a logistic regression
of treatment status on baseline demographics, sample design variables, and baseline test scores.
95% CIs are obtained by bootstrapping families with replacement. Data from the New York City
School Choice Scholarships Program evaluation conducted by Mathematica Policy Research.

Share of each one percentile range of control
group test distribution that is made up of African American
using either the definition of the Mathematica reports or
the definition in Krueger and Zhu
Appendix Figure 5:

Year 1 Math percentile scores by two definitions of African American
Covariate shares
.6

.5

.4

.3

.2
10

20

30

40

50
Percentile

African American − HP

60

70

80

90

African American − KZ

Notes:
Figure shows the share of each percentile range of the overall control group pre-random assignment
math National Percentile Ranking scores from the Iowa Test of Basic Skills at baseline that is
made up of African Americans using the Krueger and Zhu and Mathematica report definitions. For
example, the value for the 5th percentile is the weighted share of the observations with test scores
larger than the 4th percentile of scores but less than or equal to the 5th percentile of scores that is
made of African Americans. Estimates are weighted using inverse propensity score weights. Weights
are 1/p̂ for treatment observations and 1/(1 − p̂) for control observations, where p̂ is generated from
a logistic regression of treatment status on baseline demographics, sample design variables, and
baseline test scores. Data from the New York City School Choice Scholarships Program evaluation
conducted by Mathematica Policy Research.

Effect of excluding “missing data” values
on mean treatment effects for African Americans reported
in Krueger and Zhu (2004a) using Peterson and Howell or
Krueger and Zhu definition of African American

Appendix Table 1:

Using Mathematica weights
Using our inverse p-score weights
Our replication, K&Z Our estimates Our estimates
Our estimates
0s included
0s excluded
0s included
0s excluded
Panel A: Effects for Peterson & Howell sample of African Americans, math NPRs
Year 1
4.54∗∗∗
3.65∗∗
2.64
2.37
(1.53)
(1.59)
(1.42)
(1.46)
Year 2
2.59
3.03
2.01
2.02
(2.06)
(2.09)
(1.67)
(1.73)
Year 3
4.00∗∗
3.38∗
4.22∗∗
3.94∗∗
(1.86)
(1.92)
(1.73)
(1.78)
Panel B: Effects for Krueger & Zhu sample of African Americans, math NPRs
Year 1
3.18∗∗
2.21
1.41
1.31
(1.53)
(1.57)
(1.38)
(1.41)
Year 2
1.33
1.74
0.48
0.43
(1.97)
(2.01)
(1.65)
(1.72)
Year 3
2.83
2.32
2.33
2.10
(1.76)
(1.81)
(1.66)
(1.70)
Notes: Table reports original results from panel 3 of Table 3B of Krueger and Zhu (2004a), our replication of these
results, and then shows the impact of excluding the 0 percentile values (which are invalid percentiles), as well as
the impact of using inverse propensity score weights as an alternative to the non-response adjusted weights in the
data. Dependent variable is the math (Panel A) or reading (Panel B) National Percentile Ranking scores from the
Iowa Test of Basic Skills in the spring for years 1–3 of voucher distribution. Estimates in columns 1 and 2 use the
MPR provided weights, while those in 3 and 4 use inverse propensity score weights. The p-score weights are 1/p̂
for treatment observations and 1/(1 − p̂) for control observations, where p̂ is generated from a logistic regression of
treatment status on baseline demographics, sample design variables, and baseline test scores. 95% CIs are obtained
by bootstrapping families with replacement. Data from the New York City School Choice Scholarships Program
evaluation conducted by Mathematica Policy Research.

