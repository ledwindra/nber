NBER WORKING PAPER SERIES

WHEN SHOULD YOU ADJUST STANDARD ERRORS FOR CLUSTERING?
Alberto Abadie
Susan Athey
Guido W. Imbens
Jeffrey Wooldridge
Working Paper 24003
http://www.nber.org/papers/w24003

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
November 2017

The questions addressed in this paper partly originated in discussions with Gary Chamberlain.
We are grateful for questions raised by Chris Blattman. We are grateful to seminar audiences at
the 2016 NBER Labor Studies meeting, CEMMAP, Chicago, Brown University, the HarvardMIT Econometrics seminar, Ca' Foscari University of Venice, the California Econometrics
Conference, the Erasmus University Rotterdam, and Stanford University. The views expressed
herein are those of the authors and do not necessarily reflect the views of the National Bureau of
Economic Research.
At least one co-author has disclosed a financial relationship of potential relevance for this
research. Further information is available online at http://www.nber.org/papers/w24003.ack
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
Â© 2017 by Alberto Abadie, Susan Athey, Guido W. Imbens, and Jeffrey Wooldridge. All rights
reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit
permission provided that full credit, including Â© notice, is given to the source.

When Should You Adjust Standard Errors for Clustering?
Alberto Abadie, Susan Athey, Guido W. Imbens, and Jeffrey Wooldridge
NBER Working Paper No. 24003
November 2017
JEL No. C21
ABSTRACT
In empirical work in economics it is common to report standard errors that account for clustering
of units. Typically, the motivation given for the clustering adjustments is that unobserved
components in outcomes for units within clusters are correlated. However, because correlation
may occur across more than one dimension, this motivation makes it difficult to justify why
researchers use clustering in some dimensions, such as geographic, but not others, such as age
cohorts or gender. This motivation also makes it difficult to explain why one should not cluster
with data from a randomized experiment. In this paper, we argue that clustering is in essence a
design problem, either a sampling design or an experimental design issue. It is a sampling design
issue if sampling follows a two stage process where in the first stage, a subset of clusters were
sampled randomly from a population of clusters, and in the second stage, units were sampled
randomly from the sampled clusters. In this case the clustering adjustment is justified by the fact
that there are clusters in the population that we do not see in the sample. Clustering is an
experimental design issue if the assignment is correlated within the clusters. We take the view
that this second perspective best fits the typical setting in economics where clustering adjustments
are used. This perspective allows us to shed new light on three questions: (i) when should one
adjust the standard errors for clustering, (ii) when is the conventional adjustment for clustering
appropriate, and (iii) when does the conventional adjustment of the standard errors matter.
Alberto Abadie
Department of Economics, E52-546
MIT
77 Massachusetts Avenue
Cambridge, MA 02139
and NBER
abadie@mit.edu

Guido W. Imbens
Graduate School of Business
Stanford University
655 Knight Way
Stanford, CA 94305
and NBER
Imbens@stanford.edu

Susan Athey
Graduate School of Business
Stanford University
655 Knight Way
Stanford, CA 94305
and NBER
athey@stanford.edu

Jeffrey Wooldridge
Department of Economics
Michigan State University
wooldri1@msu.edu

1

Introduction

In empirical work in economics, it is common to report standard errors that account for clustering of units. The first issue we address in this manuscript is the motivation for this adjustment.
Typically the stated motivation is that unobserved components of outcomes for units within
clusters are correlated (Moulton [1986, 1990], Moulton and Randolph [1989], Kloek [1981],
Hansen [2007], Cameron and Miller [2015]). For example, Hansen [2007] writes: â€œThe clustering problem is caused by the presence of a common unobserved random shock at the group
level that will lead to correlation between all observations within each groupâ€ (Hansen [2007],
p. 671). Similarly Cameron and Miller [2015] write: â€œThe key assumption is that the errors are
uncorrelated across clusters while errors for individuals belonging to the same cluster may be
correlatedâ€ (Cameron and Miller [2015], p. 320). This motivation for clustering adjustments
in terms of within-group correlations of the errors makes it difficult to justify clustering by
some partitioning of the population, but not by others. For example, in a regression of wages
on years of education, this argument could be used to justify clustering by age cohorts just as
easily as clustering by state. Similarly, this motivation makes it difficult to explain why, in a
randomized experiment, researchers typically do not cluster by groups. It also makes it difficult
to motivate clustering if the regression function already includes fixed effects. The second issue
we address concerns the appropriate level of clustering. The typical answer is to go for the most
aggregate level feasible. For example, in a recent survey Cameron and Miller [2015] write: â€œThe
consensus is to be conservative and avoid bias and to use bigger and more aggregate clusters
when possible, up to and including the point at which there is concern about having too few
clusters.â€ (Cameron and Miller [2015], p. 333). We argue in this paper that there is in fact
harm in clustering at too aggregate a level, We also make the case that the confusion regarding
both issues arises from the dominant model-based perspective on clustering.
We take the view that clustering is in essence a design problem, either a sampling design
or an experimental design issue. It is a sampling design issue when the sampling follows a
two stage process, where in the first stage, a subset of clusters is sampled randomly from a
population of clusters, and in the second stage, units are sampled randomly from the sampled
clusters. Although this clustered sampling approach is the perspective taken most often when a
formal justification is given for clustering adjustments to standard errors, it actually rarely fits
applications in economics. Angrist and Pischke [2008] write: â€œMost of the samples that we work
with are close enough to random that we typically worry more about the dependence due to a
group structure than clustering due to stratification.â€ (Angrist and Pischke [2008], footnote 10,
p. 309). Instead of a sampling issue, clustering can also be an experimental design issue, when
clusters of units, rather than units, are assigned to a treatment. In the view developed in this
manuscript, this perspective fits best the typical application in economics, but surprisingly it
is rarely explicitly presented as the motivation for cluster adjustments to the standard errors.
We argue that the design perspective on clustering, related to randomization inference
(e.g., Rosenbaum [2002], Athey and Imbens [2017]), clarifies the role of clustering adjustments
to standard errors and aids in the decision whether to, and at what level to, cluster, both
in standard clustering settings and in more general spatial correlation settings (Bester et al.

[1]

[2009], Conley [1999], Barrios et al. [2012], Cressie [2015]). For example, we show that, contrary
to common wisdom, correlations between residuals within clusters are neither necessary, nor
sufficient, for cluster adjustments to matter. Similarly, correlations between regressors within
clusters are neither necessary, not sufficient, for cluster adjustments to matter or to justify
clustering. In fact, we show that cluster adjustments can matter, and substantially so, even
when both residuals and regressors are uncorrelated within clusters. Moreover, we show that
the question whether, and at what level, to adjust standard errors for clustering is a substantive
question that cannot be informed solely by the data. In other words, although the data are
informative about whether clustering matters for the standard errors, but they are only partially
informative about whether one should adjust the standard errors for clustering. A consequence
is that in general clustering at too aggregate a level is not innocuous, and can lead to standard
errors that are unnecessarily conservative, even in large samples.
One important theme of the paper, building on Abadie et al. [2017], is that it is critical
to define estimands carefully, and to articulate precisely the relation between the sample and
the population. In this setting that means one should define the estimand in terms of a finite
population, with a finite number of clusters and a finite number of units per clusters. This is
important even if asymptotic approximations to finite sample distributions involve sequences
of experiments with an increasing number of clusters and/or an increasing number of units per
cluster. In addition, researchers need to be explicit about the way the sample is generated from
this population, addressing two issues: (i) how units in the sample were selected and, most
importantly whether there are clusters in the population of interest that are not represented
in the sample, and (ii) how units were assigned to the various treatments, and whether this
assignment was clustered. If either the sampling or assignment varies systematically with
groups in the sample, clustering will in general be justified. We show that the conventional
adjustments, often implicitly, assume that the clusters in the sample are only a small fraction of
the clusters in the population of interest. To make the conceptual points as clear as possible, we
focus in the current manuscript on the cross-section setting. In the panel case (e.g., Bertrand
et al. [2004]), the same issues arise, but there are additional complications because of the time
series correlation of the treatment assignment. Analyzing the uncertainty from the experimental
design perspective would require modeling the time series pattern of the assignments, and we
leave that to future work.
The practical implications from the results in this paper are as follows. First, the researcher
should assess whether the sampling process is clustered or not, and whether the assignment
mechanism is clustered. If the answer to both is no, one should not adjust the standard errors
for clustering, irrespective of whether such an adjustment would change the standard errors.
Second, in general, the standard Liang-Zeger clustering adjustment is conservative unless one
of three conditions holds: (i) there is no heterogeneity in treatment effects; (ii) we observe only
a few clusters from a large population of clusters; or (iii) a vanishing fraction of units in each
cluster is sampled, e.g. at most one unit is sampled per cluster. Third, the (positive) bias from
standard clustering adjustments can be corrected if all clusters are included in the sample and
further, there is variation in treatment assignment within each cluster. For this case we propose
a new variance estimator. Fourth, if one estimates a fixed effects regression (with fixed effects
[2]

at the level of the relevant clusters), the analysis changes. Then, heterogeneity in the treatment
effects is a requirement for a clustering adjustment to be necessary.

2

A Simple Example and Two Misconceptions

In this section we discuss two misconceptions about clustering that appear common in the
literature. The first misconception is about when clustering matters, and the second about
whether one ought to cluster. Both misconceptions are related to the common model-based
perspective of clustering which we outline briefly below. We argue that this perspective obscures
the justification for clustering that is relevant for most empirical work.

2.1

The Model-based Approach to Clustering

First let us briefly review the textbook, model-based approach to clustering (e.g., Cameron and
Miller [2015], Wooldridge [2003, 2010], Angrist and Pischke [2008]). Later, we contrast this
with the design-based approach starting from clustered randomized experiments (Donner and
Klar [2000], Murray [1998], Fisher [1937]). Consider a setting where we wish to model a scalar
outcome Yi in terms of a binary covariate Wi âˆˆ {0, 1}, with the units belonging to clusters,
with the cluster for unit i denoted by Ci âˆˆ {1, . . . , C}. We estimate the linear model
Yi = Î± + Ï„ Wi + Îµi = Î² > Xi + Îµi ,
where Î² > = (Î±, Ï„ ) and Xi> = (1, Wi), using least squares, leading to
N 
2 
âˆ’1 

X
Î²Ì‚ = arg min
Yi âˆ’ Î² > Xi = X> X
X> Y .
Î²

i=1

In the model-based perspective, the N -vector Îµ with ith element equal to Îµi , is viewed as the
stochastic component. The N Ã— 2 matrix X with ith row equal to (1, Wi) and the N -vector C
with ith element equal to Ci are viewed as non-stochastic. Thus the repeated sampling thought
experiment is redrawing the vectors Îµ, keeping fixed C and W.
Often the following structure is imposed on the first two moments of Îµ,
h
i
E[Îµ|X, C] = 0,
E ÎµÎµ> X, C = â„¦,

leading to the following expression for the variance of the ordinary least squares (OLS) estimator:

âˆ’1 

âˆ’1
V(Î²Ì‚) = X> X
X> â„¦X X> X
.

In the setting without clustering, the key assumption is that â„¦ is diagonal. If one is also willing
to assume homoskedasticity the variance reduces to the standard OLS variance:

âˆ’1
VOLS = Ïƒ 2 X> X
,
[3]

where Ïƒ = â„¦ii = V(Îµi ) for all i. Often researchers allow for general heteroskedasticity and use
the robust Eicker-Huber-White (EHW) variance (White [2014], Eicker [1967], Huber [1967])
!
N

âˆ’1 X

âˆ’1
>
>
VEHW (Î²Ì‚) = X X
â„¦ii Xi Xi
X> X
.
i=1

In settings with clusters of units, the assumption that â„¦ is diagonal is often viewed as not
credible. Instead, Kloek [1981], Moulton and Randolph [1989], Moulton [1990] use the (homoskedastic) structure
ï£±
if Ci 6= Cj ,
ï£² 0
2
â„¦ij =
ÏÏƒ
if Ci = Cj , i 6= j,
ï£³ 2
Ïƒ
if i = j.

Assuming the clusters are equal size this leads to the following variance for the slope coefficient
Ï„Ì‚ :


N
VKLOEK (Ï„Ì‚ ) = VOLS Ã— 1 + ÏÎµÏW
,
(2.1)
C
where ÏÎµ and ÏW are the within-cluster correlation of the errors and covariates respectively.
Often researchers (e.g., Liang and Zeger [1986], Diggle et al. [2013], Bertrand et al. [2004],
Stock and Watson [2008], William [1998]) further relax this model by allowing the â„¦ij for pairs
(i, j) with Ci = Cj to be unrestricted. Let the units be ordered by cluster, and let the Nc Ã— Nc
submatrix of â„¦ corresponding to the units from cluster c be denoted by â„¦c , and the submatrix
of X corresponding to cluster c by Xc . Then:
!
C

âˆ’1 X

âˆ’1
>
>
VLZ (Î²Ì‚) = X X
X c â„¦c X c
X> X
.
c=1

This can be viewed as the extension to robust variance estimator from the least squares variance,
applied in the case with clustering.
The estimated version of the EHW variance is
!
N

âˆ’1

âˆ’1 X
>
>
2
>
.
(2.2)
VÌ‚EHW (Î²Ì‚) = X X
(Yi âˆ’ Î²Ì‚ Xi ) Xi Xi
X> X
i=1

The estimated version of the LZ variance is
ï£«
ï£«
ï£¶ï£«
ï£¶> ï£¶
C

âˆ’1 X

âˆ’1
X
X
ï£·
ï£¬
ï£­
(Yi âˆ’ Î²Ì‚ > Xi )Xiï£¸ ï£­
(Yi âˆ’ Î²Ì‚ > Xi )Xi ï£¸ ï£¸ X> X
.
VÌ‚LZ (Î²Ì‚) = X> X
ï£­
c=1

i:Ci =c

i:Ci =c

(2.3)

[4]

2.2

Clustering Matters Only if the Residuals and the Regressors are both
Correlated Within Clusters

There appears to be a view, captured by the expression in equation (2.1), that whether the
cluster correction to the standard errors matters depends on two objects. First, it depends
on the within-cluster correlation of the residuals, ÏÎµ , and second, it depends on the withincluster correlation of the regressors of interest, ÏW . It has been argued that clustering does not
matter if either of the two within-cluster correlations are zero. If this were true, an implication
would be that in large samples the cluster adjustment makes no difference in a randomized
experiment with completely randomly assigned treatments. This would follow because in that
case the within-cluster correlation of the regressor of interest is zero by virtue of the random
assignment. A second implication would be that, in a cross-sectional data context, if one
includes fixed effects in the regression function to account for the clusters, there is no reason
to cluster standard errors, because the fixed effects completely eliminate the within-cluster
correlation of the residuals. Although the latter implication is known to be false (e.g., Arellano
[1987]), the perception has lingered.
To illustrate the fallacy of this view, we simulated a single data set with N = 100, 323
units, partitioned into C = 100 clusters or strata with an average approximately 1,000 units
per cluster, where the actual number of units per cluster ranges from 950 to 1063. Both the
number of clusters and the number of units per cluster are substantial to avoid small sample
problems of the type analyzed in Donald and Lang [2007] and Ibragimov and MuÌˆller [2010,
2016]. Below we discuss exactly how the sample was generated, here we wish to make the basic
point that whether the clustering adjustment matters in a given sample is not simply a matter
of inspecting the within-cluster correlation of the errors and covariates. For each unit in our
sample we observe an outcome Yi , a single binary regressor Wi âˆˆ {0, 1}, and the cluster label
Ci âˆˆ {1, . . ., C}. We estimate a linear regression function,
Yi = Î± + Ï„ Wi + Îµi ,
by OLS.
For our sample set we first calculate the within-cluster correlation of the residuals and the
within-cluster correlation of the regressors. We estimate these by first calculating the sample
variance of the residuals (regressors) with and without demeaning by cluster, and then taking
the ratio of the difference of these two to the overall variance of the residuals (regressors),
leading to:
ÏÌ‚ÎµÌ‚ = 0.001,

ÏÌ‚W = 0.001.

Both within-cluster correlations are close to zero, and because there is only modest variation
in cluster sizes, the standard Moulton-Kloek (Kloek [1981], Moulton [1986], Moulton and Randolph [1989], Moulton [1990]) adjustment given in (2.1) would essentially be zero. However,
when we calculate the least squares estimator for Ï„ and both the EHW and LZ standard errors,
we find that the clustering does matter substantially:
Ï„Ì‚ ls = âˆ’0.120

(seEHW = 0.004)

[seLZ = 0.100].
[5]

This demonstrates that inspecting the within-cluster correlation of the residuals and the withincluster correlation of the regressors is not necessarily informative about the question whether
clustering the standard errors using the Liang-Zeger variance estimator matters.
Instead, what is relevant for whether the Liang-Zeger variance adjustment matters is the
within-cluster correlation of the product of the residuals and the regressors. Calculating that
correlation, we find
ÏÎµÌ‚W = 0.500.
This correlation is substantial, and it explains why the clustering matter. Note that this does
not mean one should adjust the standard errors, merely that doing so will matter.
If we use a fixed effects regression instead of OLS, the same conclusion arises, not surprisingly
given the Arellano [1987] results. We estimate the fixed effects regression
Yi = Ï„ Wi +

C
X

Î±c Cic + Îµi ,

c=1

where Cic = 1Ci =c is a binary indicator equal to one if unit i belongs to cluster c, and zero
otherwise. We run this regression and estimate both the regular and the clustered standard
errors (without degrees of freedom corrections, which do not matter here given the design),
leading to:
Ï„Ì‚ fe = âˆ’0.120

(seEHW = 0.003)

[seLZ = 0.243].

Again, the clustering of the standard errors makes a substantial difference, despite the fact that
the within-cluster correlation of the residuals is now exactly equal to zero.

2.3

If Clustering Matters, One Should Cluster

There is also a common view that there is no harm, at least in large samples, to adjusting the
standard errors for clustering. Therefore, one should cluster at the highest level of aggregation
possible, subject to finite sample issues: if clustering matters, it should be done, and if it does
not matter, clustering the standard errors does no harm, at least in large samples. Based
on this perception, many discussions of clustering adjustments to standard errors recommend
researchers to calculate diagnostics on the sample to inform the decision whether or not one
should cluster. These diagnostics often amount to simply comparing standard errors with and
without clustering adjustments. We argue that such attempts are futile, and that a researcher
should decide whether to cluster the standard errors based on substantive information, not
solely based on whether it makes a difference.
To discuss whether one ought to cluster, we step back from the previously analyzed sample
and consider both the population this sample was drawn from, and the manner in which it was
drawn. We had generated a population with 10,000,000 units, partitioned C = 100 clusters, each
cluster with exactly 100,000 units. Units were assigned a value Wi âˆˆ {0, 1}, with probability
1/2 for each value, independent of everything else. The outcome for unit i was generated as
Yi = Ï„Ci Wi + Î½i .
[6]

where Î½i was drawn from a normal distribution with mean zero and unit variance independent
across all units. The slope coefficients Ï„c are cluster-specific coefficients, equal to Ï„c = âˆ’1 for
exactly half the clusters and equal to Ï„c = 1 for the other half, so that the average treatment
in the population is exactly zero. We sample units from this population completely randomly,
with the probability for each unit of being sampled equal to 0.01.
In this example, the EHW standard errors are the appropriate ones, even though the LZ
standard errors are substantially larger. We first demonstrate this informally, and present some
formal results that cover this case in the next section. For the informal argument, let us 10,000
times draw our sample, and calculate the least squares estimator and both the EHW and LZ
standard errors. Table 1 gives the coverage rates for the associated 95% confidence intervals
for the true average effect of zero. The LZ standard errors are systematically substantially
larger than the EHW standard errors, and the LZ-based confidence intervals have substantial
over-coverage, whereas the EHW confidence intervals are accurate. This holds for the simple
regressions and for the fixed effect regressions.
Table 1: Standard Errors and Coverage Rates Random Sampling, Random Assignment (10,000 replications)

No Fixed Effects
âˆš EHW variance
âˆš LZ variance
VEHW cov rate
VLZ cov rate
0.007

0.950

0.051

Fixed Effects
âˆšEHW variance
âˆš LZ variance
VEHW cov rate
VLZ cov rate

1.000

0.007

0.950

0.131

0.986

The reason for the difference between the EHW and LZ standard errors is simple, but
reflects the fundamental source of confusion in this literature. Given the random assignment
both standard errors are correct, but for different estimands. The LZ standard errors are based
on the presumption that there are clusters in the population of interest beyond the 100 clusters
that are seen in the sample. The EHW standard errors assume the sample is drawn randomly
from the population of interest. It is this presumption underlying the LZ standard errors of
existence of clusters that are not observed in the sample, but that are part of the population
of interest, that is critical, and often implicit, in the model-based motivation for clustering the
standard errors. It is of course explicit in the sampling design literature (e.g., Kish [1965]). If
we changed the set up to one where the population of 10,000,000 consisted of say 1,000 clusters,
with 100 clusters drawn at random, and then sampling units randomly from those sampled
clusters, the LZ standard errors would be correct, and the EHW standard errors would be
incorrect. Obviously one cannot tell from the sample itself whether there exist such clusters
that are part of the population of interest that are not in the sample, and therefore one needs
to choose between the two standard errors on the basis of substantive knowledge of the study
design.

[7]

3

A Formal Result

In this section we consider a special case with a single binary covariate to formalize the ideas
from the previous subsection. We derive the exact variance to an approximation of the least
squares estimator, taking into account both sampling variation and variation induced by the
experimental design, that is, by the assignment mechanism. This will allow us to demonstrate
exactly when the EHW and LZ variances are appropriate, and why they fail when they do so.
To make the arguments rigorous, we do need large sample approximations. To do so, we build a
sequence of finite populations where the sample size and the number of clusters goes to infinity.
However, the estimands are defined for finite populations.
We start with the existence of a pair of potential outcomes for each unit. This implicitly
makes the stable-unit-treatment-value assumption (sutva, Rubin [1980]) that rules out peer
effects and versions of the treatment. There is a part of the clustering literature that is concerned with clusters of units receiving different treatments (e.g., clusters of individuals receiving
services from the same health care provider, where the exact set of services provided may vary
by provider), see for example Lee and Thompson [2005], Roberts and Roberts [2005], Weiss
et al. [2016]. Our analysis can be thought of applying to that case keeping fixed the health
care provider associated with each individual, rather than focusing on the average effect over
all possible health care providers that an individual might receive care from.

3.1

The Sequence of Populations

We have a sequence of populations indexed by n. The n-th population has Mn units, indexed
by i = 1, . . ., Mn , with Mn strictly increasing in n. The population is partitioned into Cn
strata or clusters, with Cn weakly increasing in n. Cin âˆˆ {1, . . ., Cn } denotes the stratum that
unit i belongs to. Cicn = 1Cin =c is a binary indicator, equal to 1 if unit i in population n
belongs to cluster c and zero otherwise. The number of units in cluster c in population n is
P
P n
Mcn = ni=1 Cicn , with Mn = n = C
c=1 Mcn . For each unit there are two potential outcomes,
Yin (0) and Yin (1) for unit i, corresponding to a control and treated outcome (e.g., Imbens and
Rubin [2015]) . We are interested in the population average effect of the treatment in population
n,
Ï„n =

Mn 

1 X
Yin (1) âˆ’ Yin (0) = Y n (1) âˆ’ Y n (0),
Mn
i=1

where, for w = 0, 1
Y n (w) =

Mn
1 X
Yin (w).
Mn
i=1

It is also useful to define the population average treatment effect by cluster,
Ï„cn =


X 
1
Yin (1) âˆ’ Yin (0) ,
Mcn

so that Ï„n =

Cn
X
Mcn
c=1

i:Cin =c

[8]

Mn

Ï„cn .

Define also the treatment-specific residuals and their cluster averages, for w = 0, 1,
Mn
1 X
Yjn (w),
Îµin (w) = Yin (w) âˆ’
Mn

Îµcn (w) =

j=1

1 X
Cinc Îµin (w).
Mcn
i=1

All these objects, Yin (w), Îµin (w), Îµcn (w), and functions thereof are non-stochastic.
There are some restrictions on the sequence of populations. These are mild regularity
conditions, and most can be weakened. As n increases, the number of clusters increases without
limit, the relative cluster sizes are bounded, and the potential outcomes do not become too large
in absolute value.
Assumption 1. The sequence of populations satisfies (i)
lim Cnâˆ’1 = 0,

nâ†’âˆ

(ii) for some finite K,
maxc Mcn
â‰¤ K,
minc Mcn
and (iii) for some L,
max |Yin (w)| â‰¤ L, and
i,w

Mn
1 X
|Yin (w)|k âˆ’â†’ Âµkw ,
Mn
i=1

with Âµkw finite for k â‰¤ 2.

3.2

The Sampling Process and the Assignment Mechanism

We do not observe Yin (0) and Yin (1) for all units in the population, and so we cannot directly
infer the value of Ï„n . In this section we describe precisely the two components of the stochastic
nature of the sample. There is a stochastic binary treatment for each unit in each population,
Win âˆˆ {0, 1}. The realized outcome for unit i in population n is Yin = Yin (Win ), with Îµin =
Îµin (Win ) the realized residual. We observe for a subset of the Mn units in the n-th population
the triple (Yin , Win , Cin ), with stochastic sampling indicator Rin âˆˆ {0, 1} describing whether
(Yin , Win , Cin ) is observed (Rin = 1), or not (Rin = 0). The number of sampled units is
P n
Nn = M
i=1 Rin .
Table 2 illustrates the set up. We observe Yin (0) or Yin (1) for a subset of units in the
population: we observe Yin (0) if Rin = 1 and Win = 0, we observe Yin (1) if Rin = 1 and
Win = 1, and we observe neither Yin (0) nor Yin (1) if Rin = 0, irrespective of the value of Win .
Uncertainty reflects the fact that our sample could have been different. In the table under
the columns â€œAlternative Sample Iâ€ a different sample is given. This sample differs from the
actual sample in two ways: different units are sampled, and different units are assigned to the
treatment. Given an estimand, e.g., the average effect of the treatment Ï„n , standard errors are
intended to capture both sources of variation.
[9]

Table 2: Random Sampling, Random Assignment (X is observed, ? is missing)

Actual Sample

Unit

Alternative Sample I

1
2
3
..
.

Rin
1
0
0
..
.

Yin (0)
X
?
?
..
.

Yin (1)
?
?
?
..
.

Win
0
?
?
..
.

Cin
1
1
1

M1n

1

?

X

1

M1n +1
M1n +2
M1n +3
..
.

1
0
0
..
.

X
?
?
..
.

?
?
?
..
.

0
?
?
..
.

M1n + M2n

1

?

X

1

M1n + M2n +0
M1n + M2n +2
M1n + M2n +3
..
.

0
0
0

?
?
?
..
.

?
?
?
..
.

?
?
?
..
.

?

?

?

3
3

1
1
2
2
2
2
2
3
3
3

Rin
0
1
0
..
.

Yin (0)
?
X
?
..
.

Yin (1)
?
?
?
..
.

Win
?
0
?
..
.

0

?

?

?

0
0
0
..
.

?
?
?
..
.

?
?
?
..
.

?
?
?
..
.

0

?

?

?

0
1
1
..
.

?
X
?
..
.

?
?
X
..
.

?
0
0
..
.

0

?

?

...
Cin
1
1
1

...
...
...
...

1
1

...
...

2
2
2

...
...
...

2
2

...
...

3
3
3

...
...
...

?

3
3

...
...

M1n + M2n + M3n

0
0

..
.

..
.

..
.

..
.

..
.

..
.

..
.

..
.

..
.

..
.

..
.

...

PCn âˆ’1
M +1
c=1
PCn âˆ’1 cn
+2
Pc=1
Cn âˆ’1
c=1 +3
..
.

1
0
0
..
.

?
?
?
..
.

X
?
?
..
.

1
?
?
..
.

Cn
Cn
Cn

1
1
1
..
.

X
X
?
..
.

?
?
X
..
.

0
0
1
..
.

Cn
Cn
Cn

...
...
...

Mn

1

?

X

1

0

?

?

?

Cn
Cn

...
...

Cn
Cn

The sampling process that determines the values of Rin is independent of the potential
outcomes and the assignment. It consists of two stages. First clusters are sampled with cluster
sampling probability PCn . Second, we randomly sample units from the subpopulation consisting
of all the sampled clusters, with unit sampling probability PU n . Both PCn and PU n may be
equal to 1, or close to zero. If PCn = 1, we have completeley random sampling. If PU n = 1, we
sample all units from the sampled clusters. If both PCn = PU n = 1, all units in the population
are sampled. PCn close to zero is the case that is covered by the LZ standard errors: we
only observe units from a few clusters randomly drawn from a population consisting of a large

[10]

number of clusters.
The assignment process that determines the values of Win for all i and n, also consists of
two stages. In the first stage, for cluster c in population n, an assignment probability qcn âˆˆ [0, 1]
is drawn randomly from a distribution f (Â·) with mean Âµn and variance Ïƒn2 . To simplify the
algebra, we focus here on the case with Âµn = 1/2. The variance Ïƒn2 â‰¥ 0 is key. If it is zero, we
have random assignment. For positive values of Ïƒn2 we have correlated assignment within the
clusters, and if Ïƒn2 = 1/4 then qcn âˆˆ {0, 1}, all units with a cluster have the same assignments.
In the second stage, each unit in cluster c is assigned to the treatment independently, with
cluster-specific probability qcn .
The parameters Ïƒn2 , PCn and PU n are indexed by the population n to stress that they can be
population specific. The sequences are assumed to converge to some limits, which may include
zero and one for pCn and pU n to capture random sampling from a large population.
Formally we can summarize the conditions on the sampling and assignment processes as
follows.
Assumption 2. The vector of assigments Wn is independent of the vector of sampling indicators Rn .
Assumption 3. (Sampling)
pr(Rin = 1) = PCn PU n ,
pr(Rin = 1|Rjn = 1, Cin 6= Cjn ) = PCn PU n ,
pr(Rin = 1|Rjn = 1, Cin = Cjn ) = PU n .
Assumption 4. (Assignment)
pr(Win = 1) = Âµn = 1/2.
pr(Win = 1|Wjn = 1, Cin 6= Cjn ) = Âµn = 1/2.
pr(Win = 1|Wjn = 1, Cin = Cjn ) = Âµn + Ïƒn2 /Âµn = 1/2 + 2Ïƒn2 .
Assumption 5. (Population Sequences) The sequences Ïƒn2 , PCn and PU n satisfy
Ïƒn2 âˆˆ [0, 1/4], and Ïƒn2 â†’ Ïƒ 2 âˆˆ [0, 1/4],
PCn > 0, and PCn â†’ PC âˆˆ [0, 1],
PU n > 0, and PU n â†’ PU âˆˆ [0, 1],
(nPCn PU n )âˆ’1 â†’ 0.

[11]

Table 3: First Two Moments and Within-Cluster Covariances for Selected Random Variables

Variable

Expected Value

Variance

Within Cluster Covariance

Rin
Win
Rin Win

PCn PU n
1/2
PCn PU n /2

PCn PU n (1 âˆ’ PCn PU n )
1/4
PCn PU n (2 âˆ’ PCn PU n )/4

PCn (1 âˆ’ PCn )PU2 n
Ïƒn2
2
PCn PU n (1 âˆ’ PCn )/4 + Ïƒn2 PCn PU2 n

3.3

First and Second Moments of the Assignment and Sampling Indicators

We are interested in the distribution of the least squares estimator for Ï„n and in particular in
its approximate mean and variance. The estimator is stochastic through its dependence on two
stochastic components, the sampling indicators Rn and the assignment indicators Wn . The
approximate mean and variance depend on the first and second (cross) moments of Rin and
Win . The first two moments, and the within-cluster covariance of Rin , Win , and the product
Rin Win are presented for reference in Table 3. Note that the covariance between any of these
variables not in the same cluster is zero.
The within-cluster covariance of Rin is zero if PCn = 0 or PCn = 1, that is, if either all
clusters are sampled or a vanishing number is sampled. The within-cluster covariance of Win
is zero if the assignment probability is constant across clusters (Ïƒn2 = 0).

3.4

The Estimator

We are interested in the least squares estimator for Ï„ in the regression
Yin = Î± + Ï„ Win + Îµin .
Define the averages
Mn
1 X
Rn =
Rin ,
Mn
i=1

Mn
1 X
Wn =
Rin Win ,
Nn
i=1

Mn

Yn

1 X
=
Rin Yin .
Nn
i=1

Note that except for Rn these averages are defined over the units in the sample, not the units
in the population. Now we can write the least squares estimator Ï„Ì‚ as
Pn
Rin (Win âˆ’ W n )Yin
Ï„Ì‚ = Pi=1
= Y n1 âˆ’ Y n0 ,
n
2
i=1 Rin (Win âˆ’ W n )
where

Y n1

Mn
1 X
=
Rin Win Yin ,
Nn1
i=1

Nn1 =

Mn
X
i=1

[12]

Rin Win ,

Y n0 =

Mn
1 X
Rin (1 âˆ’ Win )Yin ,
Nn0

Nn0 =

i=1

Mn
X
i=1

Rin (1 âˆ’ Win ).

We are interested in the variance of Ï„Ì‚ , and how it compares to the two standard variance
estimators, the Eicker-Huber-White (EHW) variance estimator given in (2.2) and the LiangZeger (LZ) variance estimator given in (2.3).
The first step is to approximate the estimator by a sample average. This is where the large
sample approximation is important.
Lemma 1. Suppose Assumptions 1-5 hold. Then:
p

Mn
X
2
Nn (Ï„Ì‚n âˆ’ Ï„n ) âˆ’ âˆš
Rin (2Win âˆ’ 1)Îµin = op (1).
Mn PCn PU n i=1

Lemma 1 implies we can focus on properties of the Î·n , the linear approximation to
Ï„n ), defined as:
Î·n = âˆš

2
nPCn PU n

Mn
X

Î·in ,

âˆš

Nn (Ï„Ì‚n âˆ’

where Î·in = Rin (2Win âˆ’ 1)Îµin .

i=1

We can calculate the exact (finite sample) variance of Î·n for various values of the parameters
and the corresponding normalized EHW and LZ variance estimators, in order to analyze the
implications of the two types of clustering and the importance (or not) of adjusting the standard
errors for clustering.
Proposition 1. Suppose Assumptions 1-5 hold. Then (i), the exact variance of Î·n is
V (Î·n ) =

Mn n
o

1 X
2 Îµin (1)2 + Îµin (0)2 âˆ’ PU n (Îµin (1) âˆ’ Îµin (0))2 + 4PU n Ïƒn2 (Îµin (1) âˆ’ Îµin (0))2
Mn
i=1

+

Cn
n
o
PU n X
2
Mcn
(1 âˆ’ PCn )(Îµcn (1) âˆ’ Îµcn (0))2 + 4Ïƒn2 (Îµcn (1) + Îµcn (0))2 ,
Mn
c=1

(ii) the difference between the limit of the normalized LZ variance estimator and the correct
variance is
Cn
PCn PU n X 2
VLZ âˆ’ V(Î·n ) =
M (Îµcn (1) âˆ’ Îµcn (0))2 â‰¥ 0,
Mn c=1 cn

(3.1)

and (iii), the difference between the limit of the normalized LZ and EHW variance estimators
is
VLZ âˆ’ VEHW = âˆ’

Mn
2PU n X 
(Îµin (1) âˆ’ Îµin (0))2 + 4Ïƒ 2 (Îµin (1) + Îµin (0))2
Mn
i=1

+

Cn
n
o
PU n X
2
Mcn
(Îµcn (1) âˆ’ Îµcn (0))2 + 4Ïƒ 2 (Îµcn (1) + Îµcn (0))2 .
Mn
c=1

[13]

This result follows from Lemma 1 and Appendix Lemmas A.1-A.3. Part (i) gives the
âˆš
exact variance for the linear approximation of Nn (Ï„Ì‚n âˆ’ Ï„n ), which is the correct variance
of interest. The first sum in V(Î·n ) is approximately the EHW variance. If the sample is
small relative to the population, so that PU n is close to zero, this first term simplifies to
P
2
2
VEHW = N
i=1 (Îµin (1) +Îµin (0) )/Mn . The second sum in V(Î·n ) captures the effects of clustered
sampling and assignment on the variance. There are two components to that sum. The first
set of terms has a factor 1 âˆ’ PCn . The presence of this 1 âˆ’ PCn factor captures the fact that
these terms disappear if we have a random (non-clustered) sample (in which case PCn = 1).
The second set of terms has a factor Ïƒ 2 , which implies they vanish if there is no clustering in
the assignment.
Part (ii) of the proposition compares the LZ variance to the correct variance. It highlights
the fact that the LZ variance estimator captures correctly the component of the clustering
due to clustered assignment (the component that depends on Ïƒ 2 ). However, the LZ variance
does not capture component due to clustered sampling correctly unless PCn is close to zero:
implicitly the LZ variance estimator relies on the assumption that the sampled clusters are a
small proportion of the population of clusters of interest. This leads to the difference between
the LZ variance and the true variance being proportional to PCn .
Part (iii) of the proposition compares the LZ variance to the EHW variance, highlighting
the conditions under which using the LZ variance makes a difference relative to using the EHW
variance. Note that this is different from the question whether one should cluster, which is
captured by part (ii) of the proposition. The first sum in the difference VLZ âˆ’ VEHW is small
relative to the second term when there is a substantial number of units per cluster relative
to the number of clusters. For example, if the number of units per cluster Mcn = M/C is
constant across clusters and large relative to the number of clusters, then the second sum is
proportional to Mn /Cn2 , and large relative to the first sum. In that case, the clustering matters
if there is heterogeneity in the treatment effects (Îµcn (1) âˆ’ Îµcn (0) differs from zero) or there is
clustering in the assignment. Note that the difference does not depend on whether the sampling
is clustered: this follows directly from the fact that one cannot tell from the data whether or
not the sampling was clustered.
The following corollary describes two special cases under which clustering is not necessary.
Corollary 1. Standard errors need to account for clustering unless one of the following two
pairs of conditions hold: (i) there is no clustering in the sampling (PCn = 1 for all n) and there
is no clustering in the assignment (Ïƒ 2 = 0); or (ii) there is no heterogeneity in the treatment
effects (Yi (1) âˆ’ Yi (0) = Ï„ for all units) and there is no clustering in the assignment (Ïƒ 2 = 0).
Our next result highlights three special cases where the LZ clustering is correct.
Corollary 2. The LZ variance is approximately correct if one of three conditions hold: (i) there
is no heterogeneity in the treatment effects, Yi (1) âˆ’ Yi (0) = Ï„ for all units; (ii) PCn is close
to zero for all n, so that we observe only few clusters in the population of clusters; (iii) PU n
is close to zero so that there is at most one sampled unit per cluster (in which case clustering
adjustments do not matter).

[14]

The first of these three conditions (no heterogeneity in the treatment effects) is unlikely to
hold in practice. The third condition is easily verifiable by assessing the distribution of the
number of sampled units per cluster, or by comparing the standard errors with and without
clustering adjustments. The second condition cannot be assessed using the actual data. To
assess this condition one needs to consider the facts about the sampling process and investigate
whether there are clusters in the population of interest that are not included in the sample.
If one were to conclude that all the clusters in the population are included in the sample,
the LZ variance is in general conservative. Then, there are two possibilities. If the assignment
is perfectly correlated within the clusters, there is no general improvement over the LZ variance
available. However, if there is variation in the treatment within the clusters, one can estimate
VLZ âˆ’ V(Î·n) and subtract that from VÌ‚LZ . Define
Ï„Ì‚c = Y c1 âˆ’ Y c0
to be the difference in average outcomes by treatment status in cluster c, an estimator for
the average treatment effect within the cluster. Then our proposed cluster-adjusted variance
estimator is
VÌ‚CA (Ï„Ì‚ ) = VÌ‚LZ (Ï„Ì‚ ) âˆ’

4

C
1 X 2
Nc (Ï„Ì‚c âˆ’ Ï„Ì‚ )2 .
N2
c=1

The Fixed Effects Case

The importance of clustering adjustments to standard errors in settings where the regression
includes fixed effects is also a source of confusion. Arellano [1987] shows clearly that even
with fixed effects included in the regression, the clustering adjustment may matter. Here we
extend the results from the previous section to the case with fixed effects. In the fixed effect
case the assignment within clusters cannot be perfectly correlated, so we focus on the case with
Ïƒn2 < 1/4. We consider the regression of the outcome on the cluster dummies and the treatment
indicator:
Yi = Î±Cin + Ï„ Win + ÎµÌ‡in .
First we strengthen the assumptions on the sequence of populations. The main difference is
that we require the number of units per cluster to go to infinity so that we can estimate the
fixed effects consistently.
Assumption 6. The sequence of populations satisfies (i)
lim Cnâˆ’1 = 0,

nâ†’âˆ

(ii) for some finite K,
maxc Mcn
â‰¤ K,
minc Mcn
and (iii)
âˆ’1
max Mcn
â†’ 0.
c

[15]

Assumption 7. The sequences Ïƒn2 , PCn and PU n satisfy
Ïƒn2 âˆˆ [0, 1/4), and Ïƒn2 â†’ Ïƒ 2 âˆˆ [0, 1/4),
PCn > 0, and PCn â†’ PC âˆˆ [0, 1],
PU n > 0, and PU n â†’ PU âˆˆ [0, 1],

min (Mcn PCn PU n )âˆ’1 â†’ 0.
c

Define the cluster specific treatment rate:
qcn = E[Win |Cin = c].
Also define
j
and Îºj,k = E[qC
(1 âˆ’ qCin n )k ],
in n

Îº = V(qCin (1 âˆ’ qCin )),
Note that

1 âˆ’ 4Ïƒ 2
,
4
and note that Îº can only be positive if Ïƒ 2 > 0.
Define the adjusted residual as
E[qCin (1 âˆ’ qCin )] =

ÎµÌ‡in = Îµin âˆ’ qcn ÎµCin n (1) âˆ’ (1 âˆ’ qcn )ÎµCin n (0).
Lemma 2. Suppose Assumptions 1-7 hold. Then:
p

Nn (Ï„Ì‚nfe âˆ’ Ï„n ) âˆ’

Mn
X
4
âˆš
Rin (Win âˆ’ qCin )ÎµÌ‡in = op (1).
(1 âˆ’ 4Ïƒ 2 ) Mn PCn PU n i=1

Analogous to our analysis of the case without fixed effects, we can now focus on the propâˆš
erties of the linear approximation to Nn (Ï„Ì‚ fe âˆ’ Ï„n ), where
Mn

Î·nfe

X
4
âˆš
=
Rin (Win âˆ’ qCin )ÎµÌ‡in .
(1 âˆ’ 4Ïƒ 2 ) Mn PCn PU n i=1

Proposition 2. Suppose Assumptions 1-7 hold. Then (i), the exact variance of Î·nfe is
(


Mn
 
X
16
1
fe
V Î·n =
(1 âˆ’ PU n ) 1 + Îº
(Îµin (1) âˆ’ Îµin (0))2
Mn
(1 âˆ’ 4Ïƒ 2 )2
i=1
)
16Îº3,1
16Îº
1,3
+
(Îµin (1) âˆ’ ÎµCin n (1))2 +
(Îµin (0) âˆ’ ÎµCin n (0))2
(1 âˆ’ 4Ïƒ 2 )2
(1 âˆ’ 4Ïƒ 2 )2


Cn
PU n X
16Îº
2
+
Mcn (1 âˆ’ PCn ) +
(Îµcn (1) âˆ’ Îµcn (0))2,
Mn
(1 âˆ’ 4Ïƒ 2 )2
c=1

and (ii) the difference between the limit of the normalized LZ variance estimator and the correct
variance is
C

VLZ âˆ’ V(Î·nfe) =

PCn PU n X 2
Mcn (Îµcn (1) âˆ’ Îµcn (0))2.
Mn
c=1

[16]

(4.1)

Compared to the case without fixed effects given in (3.1), there is no difference in the relation
between the LZ variance estimator and the true variance, given in (4.1).
Compared to the case without fixed effects, however, there is a difference in when one should
adjust the standard errors for clustering. Without fixed effects, one should cluster if either (i)
both PCn < 1 (clustering in the sampling) and there is heterogeneity in the treatment effects,
or (ii) Ïƒ 2 > 0 (clustering in the assignment). With fixed effects, one should cluster if either
(i) both PCn < 1 (clustering in the sampling) and there is heterogeneity in the treatment
effects, or (ii) Ïƒ 2 > 0 (clustering in the assignment) and there is heterogeneity in the treatment
effects. In other words, heterogeneity in the treatment effects is now a requirement for clustering
adjustments to be necessary, and beyond that, either clustering in sampling or assignment makes
the adjustments important.

5

Conclusion

We develop a new perspective on clustering adjustments to standard errors. We argue that
there are two potential motivations for such adjustments, one based on clustered sampling, and
one based on clustered assignment. Although when researchers look for formal justification
for clustering, they typically rely on clustered sampling justifications, we argue that clustered
assignment is more commonly the setting of interest. This leads to new conclusions about when
to adjust standard errors for clustering, and at what level to do the adjustment.
The practical implications from the results in this paper are as follows. The researcher
should assess whether the sampling process is clustered or not, and whether the assignment
mechanism is clustered. If the answer to both is no, one should not adjust the standard errors
for clustering, irrespective of whether such an adjustment would change the standard errors.
We show that the standard Liang-Zeger cluster adjustment is conservative, and further, we
derive an estimator for the correct variance that can be used if there is variation in treatment
assignment within clusters and the fraction of clusters that is observed is known. This analysis
extends to the case where fixed effects are included in the regression at the level of a cluster,
with the provision that if there is no heterogeneity in the treatment effects, one need not adjust
standard errors for clustering once fixed effects are included.

[17]

References
Alberto Abadie, Susan Athey, Guido W Imbens, and Jeffrey M Wooldridge. Sampling-based
vs. design-based uncertainty in regression analysis. arXiv preprint arXiv:1706.01778, 2017.
Joshua Angrist and Steve Pischke. Mostly Harmless Econometrics: An Empiricistsâ€™ Companion.
Princeton University Press, 2008.
Manuel Arellano. Computing robust standard errors for within group estimators. Oxford
bulletin of Economics and Statistics, 49(4):431â€“434, 1987.
Susan Athey and Guido W Imbens. The econometrics of randomized experiments. Handbook
of Economic Field Experiments, 1:73â€“140, 2017.
Thomas Barrios, Rebecca Diamond, Guido W Imbens, and Michal KolesaÌr. Clustering, spatial
correlations, and randomization inference. Journal of the American Statistical Association,
107(498):578â€“591, 2012.
Marianne Bertrand, Esther Duflo, and Sendhil Mullainathan. How much should we trust
differences-in-differences estimates? The Quarterly Journal of Economics, 119(1):249â€“275,
2004.
C Bester, T Conley, and C Hansen. Inference with dependent data using clustering covariance
matrix estimators. Unpublished Manuscript, University of Chicago Business School, 2009.
A Colin Cameron and Douglas L Miller. A practitioners guide to cluster-robust inference.
Journal of Human Resources, 50(2):317â€“372, 2015.
Timothy G Conley. Gmm estimation with cross sectional dependence. Journal of econometrics,
92(1):1â€“45, 1999.
Noel Cressie. Statistics for spatial data. John Wiley & Sons, 2015.
P. Diggle, P. Heagerty, K.Y. Liang, and S. Zeger. Analysis of Longitudinal Data. Oxford
Statistical Science Series. OUP Oxford, 2013. ISBN 9780191664335. URL https://books.
google.com/books?id=zAiK-gWUqDUC.
Stephen G Donald and Kevin Lang. Inference with difference-in-differences and other panel
data. The review of Economics and Statistics, 89(2):221â€“233, 2007.
Allan Donner and Neil Klar. Design and analysis of cluster randomization trials in health
research. 2000.
Friedhelm Eicker. Limit theorems for regressions with unequal and dependent errors. In
Proceedings of the fifth Berkeley symposium on mathematical statistics and probability,
volume 1, pages 59â€“82, 1967.
Ronald Aylmer Fisher. The design of experiments. Oliver And Boyd; Edinburgh; London,
1937.
[18]

Christian B Hansen. Generalized least squares inference in panel and multilevel models with
serial correlation and fixed effects. Journal of Econometrics, 140(2):670â€“694, 2007.
Peter J Huber. The behavior of maximum likelihood estimates under nonstandard conditions.
In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability,
volume 1, pages 221â€“233, 1967.
Rustam Ibragimov and Ulrich K MuÌˆller. t-statistic based correlation and heterogeneity robust
inference. Journal of Business & Economic Statistics, 28(4):453â€“468, 2010.
Rustam Ibragimov and Ulrich K MuÌˆller. Inference with few heterogeneous clusters. Review of
Economics and Statistics, 98(1):83â€“96, 2016.
Guido W Imbens and Donald B Rubin. Causal Inference in Statistics, Social, and Biomedical
Sciences. Cambridge University Press, 2015.
Leslie Kish. Survey sampling. 1965.
Teunis Kloek. Ols estimation in a model where a microvariable is explained by aggregates and
contemporaneous disturbances are equicorrelated. Econometrica: Journal of the Econometric
Society, pages 205â€“207, 1981.
Katherine J Lee and Simon G Thompson. The use of random effects models to allow for
clustering in individually randomized trials. Clinical Trials, 2(2):163â€“173, 2005.
Kung-Yee Liang and Scott L Zeger. Longitudinal data analysis using generalized linear models.
Biometrika, 73(1):13â€“22, 1986.
Brent R Moulton. Random group effects and the precision of regression estimates. Journal of
econometrics, 32(3):385â€“397, 1986.
Brent R Moulton. An illustration of a pitfall in estimating the effects of aggregate variables on
micro units. The review of Economics and Statistics, pages 334â€“338, 1990.
Brent R Moulton and William C Randolph. Alternative tests of the error components model.
Econometrica: Journal of the Econometric Society, pages 685â€“693, 1989.
David M Murray. Design and analysis of group-randomized trials, volume 29. Monographs in
Epidemiology & B, 1998.
Chris Roberts and Stephen A Roberts. Design and analysis of clinical trials with clustering
effects due to treatment. Clinical Trials, 2(2):152â€“162, 2005.
Paul R Rosenbaum. Observational studies. In Observational Studies. Springer, 2002.
Donald B Rubin. Randomization analysis of experimental data: The fisher randomization test
comment. Journal of the American Statistical Association, 75(371):591â€“593, 1980.

[19]

James H Stock and Mark W Watson. Heteroskedasticity-robust standard errors for fixed effects
panel data regression. Econometrica, 76(1):155â€“174, 2008.
Michael J Weiss, JR Lockwood, and Daniel F McCaffrey. Estimating the standard error of the
impact estimator in individually randomized trials with clustering. Journal of Research on
Educational Effectiveness, 9(3):421â€“444, 2016.
Halbert White. Asymptotic theory for econometricians. Academic press, 2014.
S. William. Comparison of standard errors for robust, cluster, and standard estimators. 1998.
URL http://www.stata.com/support/faqs/stat/cluster.html.
Jeffrey M Wooldridge. Cluster-sample methods in applied econometrics.
Economic Review, 93(2):133â€“138, 2003.

The American

Jeffrey M Wooldridge. Econometric analysis of cross section and panel data. MIT press, 2010.

[20]

Appendix
It is useful to work with a transformation of Win :
Tin = 2Win âˆ’ 1

so that Win =

Tin + 1
1 âˆ’ Tin
, 1 âˆ’ Win =
.
2
2

Note that in terms of Tin we can write
Yin = Win Yi (1) + (1 âˆ’ Win )Yi (0) = Tin

Yin (1) âˆ’ Yin (0) Yin (1) + Yin (0)
+
,
2
2

Îµin = Tin

Îµin (1) âˆ’ Îµin (0) Îµin (1) + Îµin (0)
+
,
2
2

Tin Îµin =

Îµin (1) âˆ’ Îµin (0)
Îµin (1) + Îµin (0)
+ Tin
.
2
2

and

Proof of Lemma 1: First,
p

n
X
2
Nn (Ï„Ì‚n âˆ’ Ï„n ) âˆ’ âˆš
Rin (2Win âˆ’ 1)Îµin
nPCn PU n i=1

=

p

n
X
2
Nn (Ï„Ì‚n âˆ’ Ï„n ) âˆ’ âˆš
Rin Tin Îµin
nPCn PU n i=1

!
Pn
n
n
X
1X
2
i=1 Rin Yin (Tin âˆ’ T n )
âˆ’
2 1 Pn
(Yin (1) âˆ’ Yin (0)) âˆ’ âˆš
Rin Tin Îµin .
2
n i=1
nPCn PU n i=1
i=1 Rin (Tin âˆ’ T n )
n
1
n

p
= Nn

(A.1)

Substituting
Yin = Tin

Yin (1) âˆ’ Yin (0) Yin (1) + Yin (0)
+
2
2

= Tin

Îµin (1) âˆ’ Îµin (0)
Y n (1) âˆ’ Y n (0) Îµin (1) + Îµin (0) Y n (1) + Y n (0)
+ Tin
+
+
2
2
2
2

= Îµin + Tin

Ï„n
Y n (1) + Y n (0)
+
,
2
2

into (A.1) leads to
p

Nn 2

Pn

1
n
1
n

Rin Îµin (Tin âˆ’ T n )
Pi=1
n
2
i=1 Rin(Tin âˆ’ T n )
+(Y n (1) + Y n (0))

=

p

+ Ï„n

1
n

Pn

1
n

Rin Tin (Tin âˆ’ T n )
âˆ’ Ï„n
2
i=1 Rin (Tin âˆ’ T n )

Pi=1
n

1 Pn
Rin (Tin âˆ’ T n )
n
Pni=1
1
2
i=1 Rin(Tin âˆ’ T n )
n

!

n
X
2
âˆ’âˆš
Rin Tin Îµin
nPCn PU n i=1

Pn
n
X
2 n1 i=1 Rin Tin Îµin
2
âˆš
Nn 1 Pn
âˆ’
Rin Tin Îµin
2
nPCn PU n i=1
i=1 Rin (Tin âˆ’ T n )
n
Pn
1
p
Rin Îµin
n
âˆ’ Nn 2T n 1 Pn i=1
2
i=1 Rin (Tin âˆ’ T n )
n

[21]

1
n

Pn

Rin
Ï„n 1 Pn
âˆ’ Ï„n
2
R
(T
i=1 in in âˆ’ T n )
n
Pn
1
p
Rin Tin
âˆ’ Nn T n 1 Pnn i=1
.
2
i=1 Rin (Tin âˆ’ T n )
n
p
+ Nn

i=1

!

To prove that this is op (1), it is sufficient to prove the following four claims,
(i)
Pn
n
X
p
Rin Tin Îµin
2
âˆš
Nn Pn i=1
Rin Tin Îµin = op (1),
âˆ’
2
nPCn PU n i=1
i=1 Rin (Tin âˆ’ T n )

(A.2)

(ii)

p
(iii)
p

Pn
1
Rin Îµin
n
Nn T n 1 Pn i=1
2
R
(T
i=1 in in âˆ’ T n )
n
Nn

(iv)
p

= op (1),

PN

Rin
Ï„n Pn
âˆ’ Ï„n
2
R
(T
i=1 in in âˆ’ T n )
i=1

1 Pn
Rin Tin
n
Nn T n 1 Pn i=1
2
i=1 Rin (Tin âˆ’ T n )
n

!

(A.3)

= op (1),

= op (1).

(A.4)

(A.5)

First a couple of preliminary observations. By the assumptions it follows that
n

1X
p
(Rin âˆ’ PCn PU n ) âˆ’â†’ 0
n i=1

(A.6)

and so that
Nn
p
âˆ’â†’ 1.
nPCn PU n

(A.7)

In addition,
p
Nn T n = Op(1),

(A.8)

and

n

p

Nn

1X
Rin Îµin = Op (1).
n i=1

(A.9)


Lemma A.1. Suppose Assumptions 1 and 5 hold. Then (i)
Nn VÌ‚EHW â†’ AVEHW =

4
nPCn PU n

n
X
i=1

n

=

2 X
Îµin (1)2 + Îµin (0)2 ,
n
i=1

and
Nn VÌ‚LZ â†’ AVLZ

[22]

=

n


2 X
Îµin (1)2 1 âˆ’ PU n (1 + 4Ïƒ 2 ) + Îµin (0)2 1 âˆ’ PU n (1 + 4Ïƒ 2 ) + Îµin (0)Îµin (1)PU n (2 âˆ’ 8Ïƒ 2 )
n i=1

+

Cn
o
PU n X 2 n
2
2
ncn (Îµcn (1) âˆ’ Îµcn (0)) + 4Ïƒ 2 (Îµcn (1) + Îµcn (0)) .
n c=1

Proof of Lemma A.1: First (i):
AVEHW =

4
nPCn PU n

n
X

2
E[Î·in
].

i=1

Because
2
2 2
E[Î·in
] = E[R2inTin
Îµin ] = E[RinÎµ2in ] = PCn PU n E[Îµ2in ]


2
Îµin (1) âˆ’ Îµin (0) Îµin (1) + Îµin (0)
+
= PCn PU n Tin
2
2

1
= PCn PU n Îµin (1)2 + Îµin (0)2 âˆ’ 2Îµin (1)Îµin (0) + Îµin (1)2 + Îµin (0)2 + 2Îµin (1)Îµin (0)
4

1
= PCn PU n Îµin (1)2 + Îµin (0)2
2
it follows that
n

AVEHW

2 X
=
Îµin (1)2 + Îµin (0)2 ,
n
i=1

finishing the proof for part (i).
Next, consider (ii). The normalized LZ variance estimator is
AVLZ =

=

4
nPCn PU n
4
nPCn PU n

n X
n
X

E [Rin Tin Îµin Rjn Tjn Îµjn ]

i=1 j=1

Cn X
n X
n
X
c=1 i=1 j=1

Cin CjnE [Rin Tin Îµin RjnTjn Îµjn |Cin = Cjn] .

Consider the expectations:
E [Rin Tin Îµin Rjn Tjn Îµjn |Cin = Cjn ]






Îµin (1) âˆ’ Îµin (0) Îµin (1) + Îµin (0)
Îµjn (1) âˆ’ Îµjn (0) Îµjn (1) + Îµjn (0)
= E Rin Tin Tin
+
Rjn Tjn Tjn
+
Cin = Cjn
2
2
2
2






Îµin (1) âˆ’ Îµin (0)
Îµin (1) + Îµin (0)
Îµjn (1) âˆ’ Îµjn (0)
Îµjn (1) + Îµjn (0)
+ Tin
Rjn
+ Tjn
Cin = Cjn .
= E Rin
2
2
2
2
If i = j, the expectation is, per the earlier calculation for AVEHW , equal to
E [Rin Tin Îµin Rjn Tjn Îµjn |Cin = Cjn , i = j] =
If the i 6= j, the expectation is


1
PCn PU n Îµin (1)2 + Îµin (0)2 .
2

E [Rin Tin Îµin Rjn Tjn Îµjn |Cin = Cjn , i 6= j]

[23]

=

1
{(Îµin (1) âˆ’ Îµin (0)) (Îµjn (1) âˆ’ Îµjn (0)) E[RinRjn|Cin = Cjn , i 6= j]
4
+ (Îµin (1) âˆ’ Îµin (0)) (Îµjn (1) + Îµjn (0)) E[RinRjnTjn |Cin = Cjn , i 6= j]
+ (Îµin (1) + Îµin (0)) (Îµjn (1) âˆ’ Îµjn (0)) E[RinRjnTin |Cin = Cjn , i 6= j]
+ (Îµin (1) + Îµin (0)) (Îµjn (1) + Îµjn (0)) E[RinRjnTin Tjn |Cin = Cjn , i 6= j]}

=

1
(Îµin (1) âˆ’ Îµin (0)) (Îµjn (1) âˆ’ Îµjn (0)) PCn PU2 n
4

+ (Îµin (1) + Îµin (0)) (Îµjn (1) + Îµjn (0)) 4PCn PU2 n Ïƒ 2 .

Hence
n X
n
X
i=1 j=1

=

CincCjncE [Rin Tin Îµin RjnTjn Îµjn |Cin = Cjn, i 6= j]

n
n

PCn PU2 n X X
CincCjnc (Îµin (1) âˆ’ Îµin (0)) (Îµjn (1) âˆ’ Îµjn (0)) + (Îµin (1) + Îµin (0)) (Îµjn (1) + Îµjn (0)) 4Ïƒ 2
4
i=1 j=1

âˆ’
=

i=1

PCn PU2 n
4
âˆ’

Thus
AVLZ =

n
n
o
PCn PU2 n X
2
2
Cin (Îµin (1) âˆ’ Îµin (0)) + (Îµin (1) + Îµin (0)) 4Ïƒ 2
4
n X
n
X
i=1 j=1

o
n
2
2
n2cn (Îµcn (1) âˆ’ Îµcn (0)) + (Îµcn (1) + Îµcn (0)) 4Ïƒ 2

n
n
o
PCn PU2 n X
2
2
Cin (Îµin (1) âˆ’ Îµin (0)) + (Îµin (1) + Îµin (0)) 4Ïƒ 2 .
4
i=1

4
nPCn PU n

Cn X
n X
n
X
c=1 i=1 j=1

Cin CjnE [Rin Tin Îµin RjnTjn Îµjn |Cin = Cjn]

n

=

2 X
Îµin (1)2 + Îµin (0)2
n i=1
âˆ’

n
o
PU n X n
2
2
(Îµin (1) âˆ’ Îµin (0)) + 4Ïƒ 2 (Îµin (1) + Îµin (0))
n
i=1

Cn
n
o
PU n X
+
n2cn (Îµcn (1) âˆ’ Îµcn (0))2 + 4Ïƒ 2 (Îµcn (1) + Îµcn (0))2
n c=1

n


1 X
=
Îµin (1)2 2 âˆ’ PU n (1 + 4Ïƒ 2 ) + Îµin (0)2 2 âˆ’ PU n (1 + 4Ïƒ 2 ) + Îµin (0)Îµin (1)PU n (2 âˆ’ 8Ïƒ 2 )
n
i=1

+

Cn
n
o
PU n X
2
2
n2cn (Îµcn (1) âˆ’ Îµcn (0)) + 4Ïƒ 2 (Îµcn (1) + Îµcn (0)) .
n c=1


Next we split Î·n into two uncorrelated sums.

[24]

Lemma A.2.
n
X
2
Î·n = âˆš
Rin Tin Îµin = Sn + Dn ,
npC pU i=1

where
n
X
1
Sn = âˆš
(Rin âˆ’ PCn PU n )(Îµin (1) âˆ’ Îµin (0)),
nPCn PU n i=1

and
Dn = âˆš

1
nPCn PU n

n
X

Rin Tin (Îµin (1) + Îµin (0)).

i=1

Proof of Lemma A.2: Substituting Îµin = Tin (Îµin (1) âˆ’ Îµin (0))/2 + (Îµin (1) + Îµin (0))/2, we have

n
n 
X
X
2
Îµin (1) âˆ’ Îµin (0)
Îµin (1) + Îµin (0)
2
âˆš
+ Ri Tin
.
Rin Tin Îµin = âˆš
Rin
2
2
nPCn PU n
nPCn PU n
i=1

Because
âˆš

Pn

i=1 Îµin (0)

1

nPCn PU n

=

n
X

i=1

Pn

i=1 Îµin (1)

= 0, this is equal to

n
X
1
(Rin âˆ’ pC pU )(Îµin (1) âˆ’ Îµin (0)) + âˆš
Rin Tin (Îµin (1) + Îµin (0)) = Sn + Dn .
nPCn PU n i=1
i=1


Comment: The S here refers to sampling, because Sn captures the sampling part of the clustering, and
D refers to design, as Dn captures the design part of the clustering. For Sn only the clustering in the
sampling (in Rin ) matters, and the clustering in the assignment (in Tin ) does not matter. For Dn it is
the other way around. Even if Rin is clustered, if Tin is not, the covariance terms in the variance of Dn
vanish. 
Lemma A.3. The first two moments of Sn and Dn are
E[Sn ] = 0,
E[Sn2 ] =
E[Dn2 ] =
and

E[Dn ] = 0,

n
C
1 âˆ’ PU n X
PU n (1 âˆ’ PCn ) X 2
(Îµin (1) âˆ’ Îµin (0))2 +
nc (Îµcn (1) âˆ’ Îµcn (0))2 ,
n
n
i=1
c=1
n
C
1 âˆ’ 4Ïƒn2 PU n X
4Ïƒ 2 PU n X 2
(Îµin (1) + Îµin (0))2 + n
ns (Îµcn (1) + Îµcn (0))2 ,
n
n
i=1
c=1

E[Sn Dn ] = 0
so that
ï£®

Eï£° âˆš
=

2
nPCn PU n
n

n
X
i=1

!2 ï£¹
RinTin Îµin ï£»

1 X
(2 âˆ’ PU n (1 + 4Ïƒn2 ))Îµin (1)2 + (2 âˆ’ PU n (1 + 4Ïƒn2 ))Îµin (0)2 + PU n (2 âˆ’ 8Ïƒn2 )Îµin (1)Îµin (0)
n
i=1

C
PU n X 2 
+
n (1 âˆ’ PCn )(Îµcn (1) âˆ’ Îµcn (0))2 + 4Ïƒn2 (Îµcn (1) + Îµcn (0))2
n c=1 c

[25]

Proof of Lemma A.3: Because E[Rin] = PCn PU n , it follows immediately that E[Sn ] = 0. Because E[RinTin ] = 0, it follows that E[Dn ] = 0. Because E[(Rin âˆ’ PCn PU n )Rin Tin ] = E[(Rin âˆ’
PCn PU n )Rin ]E[Tin] = 0, it follows that E[Sn Dn ] = 0. Next, consider E[Sn2 ]:
n X
n
X

1

E[Sn2 ] =

nPCn PU n

=

n
X

1
nPCn PU n
+

=

i=1 j=1

i=1


PCn PU n (1 âˆ’ PCn PU n ) âˆ’ PU2 n PCn (1 âˆ’ PCn ) (Îµin (1) âˆ’ Îµin (0))2
Cn X
n X
n
X

1
nPCn PU n

1 âˆ’ PU n
n

n
X
i=1

E [(Rin âˆ’ PCn PU n )(Îµin (1) âˆ’ Îµin (0))(Rjn âˆ’ PCn PU n )(Îµjn (1) âˆ’ Îµjn (0))]

c=1 i=1 j=1


Cin Cjn PU2 n PCn (1 âˆ’ PCn ) (Îµin (1) âˆ’ Îµin (0))(Îµjn (1) âˆ’ Îµjn (0))

(Îµin (1) âˆ’ Îµin (0))2
Cn

n

n

PU n (1 âˆ’ PCn ) X X X
+
Cin Cjn (Îµin (1) âˆ’ Îµin (0))(Îµjn (1) âˆ’ Îµjn (0))
n
c=1
i=1 j=1

=

1 âˆ’ PU n
n
E[Dn2 ].

Next, consider
E[Dn2 ] =

=



i=1

1
nPCn PU n
1
nPCn PU n
+

=

n
X

C

(Îµin (1) âˆ’ Îµin (0))2 +

n X
n
X

n
PU n (1 âˆ’ PCn ) X
n2cn (Îµcn (1) âˆ’ Îµcn (0))2 .
n
c=1

E [Rin Tin (Îµin (1) + Îµin (0))RjnTjn (Îµjn (1) + Îµjn (0))]

i=1 j=1

n
X
i=1


PCn PU n âˆ’ 4Ïƒn2 PCn PU2 n (Îµin (1) + Îµin (0))2

1
nPCn PU n

Cn X
n X
n
X

Cin Cjn 4Ïƒn2 PCn PU2 n (Îµin (1) + Îµin (0))(Îµjn (1) + Îµjn (0))

c=1 i=1 j=1

n
Cn
1 âˆ’ 4Ïƒn2 PU n X
4Ïƒ 2 PU n X
(Îµin (1) + Îµin (0))2 + n
n2cn (Îµcn (1) + Îµcn (0))2 .
n
n
i=1
c=1

Lemma A.4.
n
X
2
Rin (Tin âˆ’ qCin )ÎµÌ‡in = Snfe + Dnfe ,
Î·nfe = âˆš
npC pU
i=1

where
Snfe = âˆš

1
nPCn PU n

n
X
i=1

2
(Rin âˆ’ PCn PU n )(1 âˆ’ qC
)(ÎµÌ‡in (1) âˆ’ ÎµÌ‡in (0)),
in

and
Dnfe = âˆš

1
nPCn PU n

n
X
i=1

Rin (Tin âˆ’ qCin {(ÎµÌ‡in (1) + ÎµÌ‡in (0)) âˆ’ qCin (ÎµÌ‡in (1) âˆ’ ÎµÌ‡in (0))} .

[26]

The proof follows the same argument as the proof for Lemma A.2 and is omitted.
Proof of Proposition 2: By definition
ÎµÌ‡in = Îµin âˆ’ qcn ÎµCin n (1) âˆ’ (1 âˆ’ qcn )ÎµCin n (0)
= Win Îµin (1) + (1 âˆ’ Win )Îµin (0) âˆ’ qcn ÎµCin n (1) âˆ’ (1 âˆ’ qcn )ÎµCin n (0)
= (Win âˆ’ qcn )(Îµin (1) âˆ’ Îµin (0)) + qcn (Îµin (1) âˆ’ ÎµCin n (1)) + (1 âˆ’ qcn )(Îµin (0) âˆ’ ÎµCin n (0)).
Hence
âˆš

Mn
Mn
Mn
Mn
1 X
1 X
1 X
1 X
Rin (Win âˆ’ qcn )ÎµÌ‡in = âˆš
Î·i1n + âˆš
Î·i2n + âˆš
Î·i3n
Mn i=1
Mn i=1
Mn i=1
Mn i=1

where
Î·i1n = Rin (Win âˆ’ qcn )2 (Îµin (1) âˆ’ Îµin (0))
Î·i2n = Rin (Win âˆ’ qcn )qcn (Îµin (1) âˆ’ ÎµCin n (1))
Î·i3n = Rin (Win âˆ’ qcn )(1 âˆ’ qcn )(Îµin (0) âˆ’ ÎµCin n (0)).
Note that for the covariance terms the we can look at the three terms separately because
E [Î·i1n Î·j2n|i 6= j, Cin = Cjn ] = 0,
E [Î·i1n Î·j3n|i 6= j, Cin = Cjn ] = 0,
and
E [Î·i2n Î·j3n|i 6= j, Cin = Cjn ] = 0.
In addition,
E [Î·i2n Î·j2n|i 6= j, Cin = Cjn ] = 0,
and
E [Î·i3n Î·j3n|i 6= j, Cin = Cjn ] = 0,
so that we only need to consider the covariance terms from the first term. For this first term note that
because
Mn
X
i=1

(Îµin (1) âˆ’ Îµin (0)) = 0,

it follows that
âˆš

Mn
Mn
1 X
1 X
Î·i1n = âˆš
Rin (Win âˆ’ qcn )2 (Îµin (1) âˆ’ Îµin (0))
Mn i=1
Mn i=1


Mn 
1 X
1 âˆ’ 4Ïƒ 2
2
= âˆš
Rin (Win âˆ’ qcn ) âˆ’ PCn PU n
(Îµin (1) âˆ’ Îµin (0)).
4
Mn i=1

Let us first look at the covariance terms:
E [ Î·i1n Î·j1n| Cin = Cjn, i 6= j]

[27]



1 âˆ’ 4Ïƒ 2
2
= E Rin (Win âˆ’ qCin n ) âˆ’ PCn PU n
(Îµin (1) âˆ’ Îµin (0))
4



1 âˆ’ 4Ïƒ 2
2
(Îµjn (1) âˆ’ Îµjn (0)) Cin = Cjn, i 6= j
Rjn(Win âˆ’ qCjn n ) âˆ’ PCn PU n
4
(
!



 )
2 2
2 2
1
âˆ’
4Ïƒ
1
âˆ’
4Ïƒ
2
= PCn PU2 n Îº +
âˆ’ PCn
PU2 n
(Îµin (1) âˆ’ Îµin (0))(Îµjn (1) âˆ’ Îµjn (0))
4
4
(
)

2
1 âˆ’ 4Ïƒ 2
2
2
= PCn PU n (1 âˆ’ PCn )
+ ÎºPCn PU n (Îµin (1) âˆ’ Îµin (0))(Îµjn (1) âˆ’ Îµjn (0)).
4
Hence
ï£®
ï£¹
Mn X
Mn
X
Eï£°
Î·i1n Î·j1nï£»
i=1 j=1,j6=i

=

Cn
X
c=1

)

2 2
1
âˆ’
4Ïƒ
2
PCn PU2 n (1 âˆ’ PCn )
+ ÎºPCn PU2 n Mcn
(Îµcn (1) âˆ’ Îµcn (0))2
4
(
)

2
Mn
X
1 âˆ’ 4Ïƒ 2
2
2
+ ÎºPCn PU n (Îµin (1) âˆ’ Îµin (0))2 .
âˆ’
PCn PU n (1 âˆ’ PCn )
4

(



i=1

In addition we need to collect the variance terms:
"
#
2
1 âˆ’ 4Ïƒ 2
2
2
2
E[Î·i1n] = E Rin (Win âˆ’ qcn ) âˆ’ PCn PU n
(Îµin (1) âˆ’ Îµin (0))
4
(

2 !

2 )
1 âˆ’ 4Ïƒ 2
1 âˆ’ 4Ïƒ 2
2
2
= PCn PU n Îº +
âˆ’ PCn PU n
(Îµin (1) âˆ’ Îµin (0))2
4
4
)
(
2

1 âˆ’ 4Ïƒ 2
+ PCn PU n Îº (Îµin (1) âˆ’ Îµin (0))2
= PCn PU n (1 âˆ’ PCn PU n )
4
h
i
2
2
E[Î·i2n
] = E {Rin (Win âˆ’ qcn )qcn (Îµin (1) âˆ’ ÎµCin n (1))} = PCn PU n Îº3,1 (Îµin (1) âˆ’ ÎµCin n (1))2 ,
and

Thus

h
i
2
2
E[Î·i3n
] = E {Rin (Win âˆ’ qcn )(1 âˆ’ qcn )(Îµin (0) âˆ’ ÎµCin n (0))} = PCn PU n Îº1,3 (Îµin (0) âˆ’ ÎµCin n (0))2 .



!2 ï£¹
Mn
X
4
âˆš
E[(Î·nfe )2 ] = E ï£°
Rin (Win âˆ’ qCin )ÎµÌ‡in ï£»
(1 âˆ’ 4Ïƒ 2 ) Mn PCn PU n i=1
ï£±
ï£¼
Mn
Mn X
Mn
Mn
Mn
ï£²X
ï£½
X
X
X
16
2
2
2
=
E[Î·
]
+
E[Î·
Î·
]
+
E[Î·
]
+
E[Î·
]
i1n j1n
i1n
i2n
i3n
ï£¾
(1 âˆ’ 4Ïƒ 2 )2 Mn PCn PU n ï£³ i=1
i=1 j=1,j6=i
i=1
i=1
(


Mn
1 X
16
=
(1 âˆ’ PU n ) 1 + Îº
(Îµin (1) âˆ’ Îµin (0))2
Mn
(1 âˆ’ 4Ïƒ 2 )2
i=1
)
16Îº1,3
16Îº3,1
2
2
+
(Îµin (1) âˆ’ ÎµCin n (1)) +
(Îµin (0) âˆ’ ÎµCin n (0))
(1 âˆ’ 4Ïƒ 2 )2
(1 âˆ’ 4Ïƒ 2 )2

Cn 
1 X
16
2
+
PU n (1 âˆ’ PCn ) + ÎºPU n
Mcn
(Îµcn (1) âˆ’ Îµcn (0))2 .
Mn c=1
(1 âˆ’ 4Ïƒ 2 )2
ï£®

[28]

