{
    "id": 28948,
    "citation_title": "A Seven-College Experiment Using Algorithms to Track Students: Impacts and Implications for Equity and Fairness",
    "citation_author": [
        "Peter Bergman",
        "Elizabeth Kopko",
        "Julio E. Rodriguez"
    ],
    "citation_publication_date": "2021-06-28",
    "issue_date": "2021-06-24",
    "revision_date": "2023-03-03",
    "topics": [
        "\n",
        "Health, Education, and Welfare",
        "\n",
        "Education",
        "\n"
    ],
    "program": [
        "\n",
        "Economics of Education",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nTracking is widespread in education. In U.S. post-secondary education alone, at least 71% of colleges use a test to track students into various courses. However, there are concerns that placement tests lack validity and unnecessarily reduce education opportunities for students from under-represented groups. While research has shown that tracking can improve student learning, inaccurate placement has consequences: students face misaligned curricula and must pay tuition for remedial courses that do not bear credits toward graduation. We develop an alternative system that uses algorithms to predict college readiness and track students into courses. Compared to the most widely-used placement tests in the country, the algorithms are more predictive of future performance. We conduct an experiment across seven colleges to evaluate the effects of algorithmic placement. Placement rates into college-level courses increase substantially without reducing pass rates. Algorithmic placement generally, though not always, narrows differences in college placement rates and remedial course taking across demographic groups. We use the experimental design and variation in placement rates to assess the disparate impact of each system. Test scores exhibit substantially more discrimination than algorithms; a significant share of test-score disparities between Hispanic or Black students and white students is explained by discrimination. We also show that the selective labels problem nearly doubles the prediction error for college English performance but has almost no impact on the prediction error for college math performance. A detailed cost analysis shows that algorithmic placement is socially efficient: it increases college credits earned while saving costs for students and the government.\n\n",
    "acknowledgement": "\nPreviously circulated as \u201cUsing Predictive Analytics to Track Students: Evidence from a Seven-College Experiment.\u201d The research reported here was undertaken through the Center for the Analysis of Postsecondary Readiness and supported by the Institute of Education Sciences, U.S. Department of Education, through Grant R305C140007 to Teachers College, Columbia University. The opinions expressed are those of the authors and do not represent views of the Institute or the U.S. Department of Education. Numerous people\u2014including several from the Community College Research Center at Teachers College, Columbia University\u2014have helped make this work happen. We particularly thank Elisabeth Barnett for her leadership, as well as Clive Belfield, Magdalena Bennett, Dan Cullinan, Vikash Reddy, and Susha Roy for their contributions. We also thank Will Dobbie, Avi Feller, Nathan Hendren, Peter Hull, Judy Scott-Clayton, Sarah Turner, and various seminar participants for their comments. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}