{
    "id": 24462,
    "citation_title": "Social Media Networks, Fake News, and Polarization",
    "citation_author": [
        "Marina Azzimonti",
        "Marcos Fernandes"
    ],
    "citation_publication_date": "2018-04-02",
    "issue_date": "2018-03-29",
    "revision_date": "2021-08-27",
    "topics": [
        "\n",
        "Econometrics",
        "\n",
        "Estimation Methods",
        "\n",
        "Microeconomics",
        "\n",
        "Mathematical Tools",
        "\n",
        "Welfare and Collective Choice",
        "\n",
        "Economics of Information",
        "\n",
        "Behavioral Economics",
        "\n"
    ],
    "program": [
        "\n",
        "Economic Fluctuations and Growth",
        "\n",
        "Political Economy",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nWe study how the structure of social media networks and the presence of fake news affects the degree of misinformation and polarization in a society. For that, we analyze a dynamic model of opinion exchange in which individuals have imperfect information about the true state of the world and exhibit bounded rationality. Key to the analysis is the presence of internet bots: agents in the network that spread fake news (e.g., a constant flow of biased information). We characterize how agents' opinions evolve over time and evaluate the determinants of long-run misinformation and polarization in the network. To that end, we construct a synthetic network calibrated to Twitter and simulate the information exchange process over a long horizon to quantify the bots' ability to spread fake news. A key insight is that significant misinformation and polarization arise in networks in which only 15% of agents believe fake news to be true, indicating that network externality effects are quantitatively important. Higher bot centrality typically increases polarization and lowers misinformation. When one bot is more influential than the other (asymmetric centrality), polarization is reduced but misinformation grows, as opinions become closer the more influential bot's preferred point. Finally, we show that threshold rules tend to reduce polarization and misinformation. This is because, as long as agents also have access to unbiased sources of information, threshold rules actually limit the influence of bots.\n\n",
    "acknowledgement": "\nWe would like to thank the participants of the 2017 NBER Summer Institute on Political Economy (Boston), 22nd Coalition Theory NetworkWorkshop (Glasgow), 27th International Conference on Game Theory (Stony Brook), 43rd Eastern Economic Association Conference (New York) and seminar series at London Business School/Government (London), Warwick, Harris Public School (Chicago). In particular we are grateful for the valuable comments from Jesse Shapiro, Daron Acemoglu, Ernesto Dal Bo, Matthew Jackson, Yair Tauman and Helios Herrera. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}