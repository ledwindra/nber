{
    "id": 31413,
    "citation_title": "How Good Are Privacy Guarantees? Platform Architecture and Violation of User Privacy",
    "citation_author": [
        "Daron Acemoglu",
        "Alireza Fallah",
        "Ali Makhdoumi",
        "Azarakhsh Malekian",
        "Asuman Ozdaglar"
    ],
    "citation_publication_date": "2023-07-03",
    "issue_date": "2023-06-29",
    "revision_date": "None",
    "topics": [
        "\n",
        "Microeconomics",
        "\n",
        "Welfare and Collective Choice",
        "\n",
        "Economics of Information",
        "\n",
        "Industrial Organization",
        "\n",
        "Industry Studies",
        "\n"
    ],
    "program": [
        "\n",
        "Economic Fluctuations and Growth",
        "\n",
        "Industrial Organization",
        "\n",
        "Law and Economics",
        "\n",
        "Public Economics",
        "\n"
    ],
    "projects": null,
    "working_groups": null,
    "abstract": "\n\nMany platforms deploy data collected from users for a multitude of purposes. While some are beneficial to users, others are costly to their privacy. The presence of these privacy costs means that platforms may need to provide guarantees about how and to what extent user data will be harvested for activities such as targeted ads, individualized pricing, and sales to third parties. In this paper, we build a multi-stage model in which users decide whether to share their data based on privacy guarantees. We first introduce a novel mask-shuffle mechanism and prove it is Pareto optimal\u2014meaning that it leaks the least about the users\u2019 data for any given leakage about the underlying common parameter. We then show that under any mask-shuffle mechanism, there exists a unique equilibrium in which privacy guarantees balance privacy costs and utility gains from the pooling of user data for purposes such as assessment of health risks or product development. Paradoxically, we show that as users\u2019 value of pooled data increases, the equilibrium of the game leads to lower user welfare. This is because platforms take advantage of this change to reduce privacy guarantees so much that user utility declines (whereas it would have increased with a given mechanism). Even more strikingly, we show that platforms have incentives to choose data architectures that systematically differ from those that are optimal from the user\u2019s point of view. In particular, we identify a class of pivot mechanisms, linking individual privacy to choices by others, which platforms prefer to implement and which make users significantly worse off.\n\n",
    "acknowledgement": "\nWe thank participants at various seminars and conferences for comments and feedback. We gratefully acknowledge financial support from the Hewlett Foundation and the Apple Scholars in AI/ML Ph.D. fellowship. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research.\n\n\n"
}