NBER WORKING PAPER SERIES

MACROECONOMICS AFTER THE CRISIS:
TIME TO DEAL WITH THE PRETENSE-OF-KNOWLEDGE SYNDROME
Ricardo J. Caballero
Working Paper 16429
http://www.nber.org/papers/w16429

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
October 2010

I thank Daron Acemoglu, David Autor, Abhijit Banerjee, Olivier Blanchard, Peter Diamond, Francesco
Giavazzi, Jonathan Goldberg, Chad Jones, Bengt Holmström, Arvind Krishnamurthy, John List, Guido
Lorenzoni, James Poterba, Alp Simsek, Timothy Taylor, and Robert Solow for their comments. Of
course they are not responsible for my tirade. The views expressed herein are those of the author and
do not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2010 by Ricardo J. Caballero. All rights reserved. Short sections of text, not to exceed two paragraphs,
may be quoted without explicit permission provided that full credit, including © notice, is given to
the source.

Macroeconomics after the Crisis: Time to Deal with the Pretense-of-Knowledge Syndrome
Ricardo J. Caballero
NBER Working Paper No. 16429
October 2010
JEL No. A1,B4,E01,G01
ABSTRACT
In this paper I argue that the current core of macroeconomics—by which I mainly mean the so-called
dynamic stochastic general equilibrium approach—has become so mesmerized with its own internal
logic that it has begun to confuse the precision it has achieved about its own world with the precision
that it has about the real one. This is dangerous for both methodological and policy reasons. On the
methodology front, macroeconomic research has been in “fine-tuning” mode within the local-maximum
of the dynamic stochastic general equilibrium world, when we should be in “broad-exploration” mode.
We are too far from absolute truth to be so specialized and to make the kind of confident quantitative
claims that often emerge from the core. On the policy front, this confused precision creates the illusion
that a minor adjustment in the standard policy framework will prevent future crises, and by doing so
it leaves us overly exposed to the new and unexpected.
Ricardo J. Caballero
MIT
Department of Economics
Room E52-373a
Cambridge, MA 02142-1347
and NBER
caball@mit.edu

The recent financial crisis has damaged the reputation of macroeconomics, largely for its
inability to predict the impending financial and economic crisis. To be honest, this inability to
predict does not concern me much. It is almost tautological that severe crises are essentially
unpredictable, for otherwise they would not cause such a high degree of distress. Of course, it is
well-known that certain elements can increase the fragility of a financial system, such as high
levels of leverage or mismatches between short-term liabilities and long-term assets, and that
these issues may justify policy intervention. But knowing these mechanisms is quite different
from arguing that a severe crisis can be predicted. Modern Cassandras will always claim to have
seen the crisis coming. What they will not say is how many times they saw things coming that
never materialized, or how the specific mechanisms behind the crisis are different from those on
which their predictions were based. In my view, the conviction that one can foretell a severe
crisis in advance is mostly a manifestation of pareidolia—the psychological phenomenon that
makes people see faces and animals in clouds and the like.
What does concern me of my discipline, however, is that its current core—by which I
mainly mean the so-called dynamic stochastic general equilibrium approach—has become so
mesmerized with its own internal logic that it has began to confuse the precision it has achieved
about its own world with the precision that it has about the real one. This is dangerous for both
methodological and policy reasons. On the methodology front, macroeconomic research has
been in “fine-tuning” mode within the local-maximum of the dynamic stochastic general
equilibrium world, when we should be in “broad-exploration” mode. We are too far from
absolute truth to be so specialized and to make the kind of confident quantitative claims that
often emerge from the core. On the policy front, this confused precision creates the illusion that a
minor adjustment in the standard policy framework will prevent future crises, and by doing so it
leaves us overly exposed to the new and unexpected.
To be fair to our field, an enormous amount of work at the intersection of macroeconomics
and corporate finance has been chasing many of the issues that played a central role during the
current crisis, including liquidity evaporation, collateral shortages, bubbles, crises, panics, fire
sales, risk-shifting, contagion, and the like.1 However, much of this literature belongs to the
periphery of macroeconomics rather than to its core. Is the solution then to replace the current
core for the periphery? I am tempted—but I think this would address only some of our
problems. The dynamic stochastic general equilibrium strategy is so attractive, and even plain
addictive, because it allows one to generate impulse responses that can be fully described in
terms of seemingly scientific statements. The model is an irresistible snake-charmer. In contrast,
the periphery is not nearly as ambitious, and it provides mostly qualitative insights. So we are
1

(In fact, at MIT we divide the first‐year Ph.D. macroeconomics sequence into four parts: methods,
growth, fluctuations, and crises.) I will include specific references of this work in the main text, but see
Part VI of Tirole (2006) for a nice survey and unified explanation of several of these mechanisms.
2

left with the tension between a type of answer to which we aspire but that has limited connection
with reality (the core) and more sensible but incomplete answers (the periphery).
This distinction between core and periphery is not a matter of freshwater versus saltwater
economics. Both the real business cycle approach and its New Keynesian counterpart belong to
the core. Moreover, there was a time when Keynesian economics was more like the current core,
in the sense of trying to build quantitative aggregative models starting from micro-founded
consumption functions and the like. At that time, it was the “rational-expectations”
representatives that were in the insight-building mode, identifying key concepts for
macroeconomic policy such as time-inconsistency and endogenous expectations, without any
pretense of being realistic in all dimensions of modeling in order to obtain quantitative answers.
Moreover, this tension is not new to macroeconomics or even to economics more broadly. In
his Nobel-prize acceptance lecture, Hayek writes: “Of course, compared with the precise
predictions we have learnt to expect in the physical sciences, this sort of mere pattern predictions
is a second best with which one does not like to have to be content. Yet the danger of which I
want to warn is precisely the belief that in order to have a claim to be accepted as scientific it is
necessary to achieve more. This way lies charlatanism and worse. To act on the belief that we
possess the knowledge and the power which enable us to shape the process of society entirely to
our liking, knowledge which in fact we do not possess, is likely to make us do much harm” (von
Hayek, 1974).
One reading of Hayek's comment is as a reminder of the dangers of presuming a precision
and degree of knowledge we do not have. I suspect that if Hayek was confronted with the limited
choice between the core or the periphery of macroeconomics, his vote today would be cast for
the periphery. This is the starting point of the theme I will develop in the first part of the paper.
There I will discuss the distinction between the core and the periphery of macroeconomics in
greater detail, as well as the futile nature of the integrationist movement—that is, the process of
gradually bringing the insights of the periphery into the dynamic stochastic general equilibrium
structure.
However, when we consider Hayek’s comment, we find a silver lining: a contemporary
version of his paragraph, which would involve a discussion of the core and periphery, would
confront one modeling approach against other modeling approaches, not models against
narrative. This is good news. There is no doubt that the formalization of macroeconomics over
recent decades has increased its potential. We just need to be careful to not let this formalization
gain its own life and distract us from the ultimate goal, which is to understand the mechanisms
that drive the real economy. This progress also offers hope that we may find ways to explore
formally and explicitly the limits of our and economic agents' knowledge. This is the second
theme I develop in this paper. The idea is to place at the center of the analysis the fact that the
complexity of macroeconomic interactions limits the knowledge we can ever attain. In thinking
about analytical tools and macroeconomic policies, we should seek those that are robust to the
3

enormous uncertainty to which we are confined, and we should consider what this complexity
does to the actions and reactions of the economic agents whose behavior we are supposed to be
capturing.
I cannot be sure that shifting resources from the current core to the periphery and
focusing on the effects of (very) limited knowledge on our modeling strategy and on the actions
of the economic agents we are supposed to model is the best next step. However, I am almost
certain that if the goal of macroeconomics is to provide formal frameworks to address real
economic problems rather than purely literature-driven ones, we better start trying something
new rather soon. The alternative of segmenting, with academic macroeconomics playing its
internal games and leaving the real world problems mostly to informal commentators and
“policy” discussions, is not very attractive either, for the latter often suffer from an even deeper
pretense-of-knowledge syndrome than do academic macroeconomists.

Core and Periphery
The ultimate goal of macroeconomics is to explain and model the (simultaneous)
aggregate outcomes that arise from the decisions made by multiple and heterogeneous economic
agents interacting through complex relationships and markets. Neither the core nor the periphery
is able to address this incredibly ambitious goal very satisfactorily. The periphery has focused on
the details of the subproblems and mechanisms but has downplayed distant and complex general
equilibrium interactions. The core has focused on (extremely stylized) versions of the general
equilibrium interactions and has downplayed the subproblems.
The natural next step for the core, many would argue, is to add gradually the insights of the
periphery into its dynamic stochastic general equilibrium structure. I am much less optimistic
about this strategy, as I think it is plagued by internal inconsistencies and pretense-of-knowledge
problems.

The Periphery
I believe that up to now the insight-building mode (both past and present) of the
periphery of macroeconomics has proven to be more useful than the macro-machine-building
mode of the core to help our understanding of significant macroeconomic events. For example,
in the context of the current financial and economic crisis, the periphery gave us frameworks to
understand phenomena such as speculative bubbles, leverage cycles, fire sales, flight to quality,
margin- and collateral-constraint spirals, liquidity runs, and so on—phenomena that played a
central role in bringing the world economy to the brink of a severe depression. This literature
also provided the basis for the policy framework that was used to contain the crisis. All in all, I
believe it would be good for macroeconomics to (re)orient a larger share of its human capital in
this direction, not just for the study of crises but also for its broader concerns.
4

Yet the periphery of macroeconomics is defined not only by its subjects, but also, and
perhaps even more so, by a methodological decision which makes its goals narrower than those
of the core. The methodology of the periphery is designed to isolate insights (as micro-theory
does), and research on these topics does not typically display the aspiration to provide
comprehensive answers―let alone quantitative answers―to the overall effects on the
macroeconomy. It is only natural for macroeconomists to want more, but it is the rushed process
to fulfill this ambition that I believe has led the core right into Hayek’s pretense-of-knowledge
syndrome.

The Core
The core approach to macroeconomics, as it is taught in most graduate programs and as it
appears in leading journals, begins with a neoclassical growth model. This model is then
developed into a stochastic form. The early versions were called “real” business cycles, because
the key shocks occurred to technology. In the basic real business cycle approach, households
make optimizing decisions in equating their marginal rate of substitution between consumption
and leisure to the real wage, which in the basic model is determined by the marginal product of
labor. Households also make optimizing decisions in the choice between consumption and
saving, where in this case the maximizing condition involves setting the household’s marginal
rate of substitution between present and future consumption equal to the rate of return, which in
the basic model is determined by the rate of return that firms receive on investment. Firms
optimize their use of labor and capital according to a production function. The standard approach
in macroeconomics is then to add to this core model a few ingredients. For example, in this
journal, Galí and Gertler (2007) build up a model of this sort and then add money, monopolistic
competition (and price mark-ups), and nominal price rigidities. Variants of this model have
become the workhorse model in research departments of central banks. In this symposium, the
papers by Ohanian and Hall also take this general approach of starting with a real business cycle
model and then discussing how it might be adapted to capture the key elements of the financial
crisis.
If we were to stop there, and simply use these stylized structures as just one more tool to
understand a piece of the complex problem, and to explore some potentially perverse general
equilibrium effect which could affect the insights isolated in the periphery, then I would be fine
with it. My problems start when these structures are given life on their own, and researchers
choose to "take the model seriously" (a statement that signals the time to leave a seminar, for it is
always followed by a sequence of naive and surreal claims).
The quantitative implications of this core approach, which are built on supposedly “microfounded” calibrations of key parameters, are definitely on the surreal side. Take for example the
preferred “microfoundation” of the supply of capital in the workhorse models of the core
5

approach. A key parameter to calibrate in these models is the intertemporal substitution elasticity
of a representative agent, which is to be estimated from micro-data. A whole literature develops
around this estimation, which narrows the parameter to certain values, which are then to be used
and honored by anyone wanting to say something about “modern” macroeconomics. This
parameter may be a reasonable estimate for an individual agent facing a specific micro decision,
but what does it have to do with the aggregate? What happened with the role of Chinese
bureaucrats, Gulf autocrats, and the like, in the supply of capital? A typical answer is not to
worry about it, because this is all “as if.” But then, why do we call this strategy microfoundations
rather than reduced-form?
My point is that by some strange herding process the core of macroeconomics seems to
transform things that may have been useful modeling short-cuts into a part of a new and artificial
“reality,” and now suddenly everyone uses the same language, which in the next iteration gets
confused with, and eventually replaces, reality. Along the way, this process of make-believe
substitution raises our presumption of knowledge about the workings of a complex economy, and
increases the risks of a “pretense of knowledge” about which Hayek warned us.
After much trial and error, these core models have managed to generate reasonable
numbers for quantities during plain-vanilla, second-order business cycle fluctuations. However,
the structural interpretation attributed to these results is often naïve at best, and more often is
worse than that. For example, while these models have been successful in matching some
aggregate quantities, they have done much more poorly on prices. But in what sense is it a good
general equilibrium fit if the quantities are right but not the prices?
Incidentally, this process of selective measures of success also weakens the initial
motivation for building the microfoundations of macroeconomics, which is to make the theory
testable. A theory is no longer testable when rejection is used not to discard the theory, but to
select the data moments under which the core model is to be judged. This practice means that
well-known major failures just become “puzzles,” which are soon presumed to be orthogonal to
the output from the quantitative model that is to be taken “seriously.”2
But isn’t abstraction what good economic models are about, for only then can we isolate
the essence of our concerns? Yes, but with certain requirements to which I think the core has
failed to adhere, yet the periphery has gotten mostly right.

2

In a similar spirit, but applied to ad‐hoc dynamics models, Lucas (2003) writes: “There’s an interesting
footnote in Patinkin’s book. Milton Friedman had told him that the rate of change of price in any one
market ought to depend on excess demand and supply in all markets in the system. Patinkin is happy
about this suggestion because he loves more generality, but if you think about Friedman’s review of
Lange, of Lange’s book, what Friedman must have been trying to tell Patinkin is that he thinks the theory
is empty, that anything can happen in this model. And I think he’s got a point. ”
6

The periphery uses abstraction to remove the inessential, but a typical periphery paper is
very careful that the main object of study is anchored by sensible assumptions. It is fine to be as
“goofy” as needed to make things simpler along inessential dimensions, but it is important not to
sound “funny” on the specific issue that is to be addressed.
Instead, core macroeconomics often has aimed not for a realistic anchor and a
simplification of the rest, but for being only half-“goofy” on everything: preferences and
production functions that do not represent anyone but that could be found in an introductory
microeconomics textbook, the same for markets, and so on. By now, there are a whole set of
conventions and magic parameter values resulting in an artificial world that can be analyzed with
the rigor of micro-theory but that speaks of no particular real-world issue with any reliability.

Integration?
One possible reaction to my sarcastic remarks is that I am too impatient; that with enough
time, we will arrive at an El Dorado of macroeconomics where the key insights of the periphery
are incorporated into a massive dynamic stochastic general equilibrium model. After all, there
has been an enormous collective effort in recent decades in building such models, with an
increasing number of bells and whistles representing various microeconomic frictions. The
research departments of central banks around the world have become even more obsessed than
academics with this agenda.
However, I think this incremental strategy may well have overshot its peak and may lead
us to a minimum rather than a maximum in terms of capturing realistic macroeconomic
phenomena. We are digging ourselves, one step at a time, deeper and deeper into a Fantasyland,
with economic agents who can solve richer and richer stochastic general equilibrium problems
containing all sorts of frictions. Because the “progress” is gradual, we do not seem to notice as
we accept what are increasingly absurd behavioral conventions and stretch the intelligence and
information of underlying economic agents to levels that render them unrecognizable.
The beauty of the simplest barebones real business cycle model is, in fact, in its
simplicity. It is a coherent description of equilibrium in a frictionless world, where it is
reasonable to expect that humans can deal with its simplicity. I would rather stop there (perhaps
with space for adding one nominal rigidity) and simply acknowledge that it is a benchmark, not a
shell or a steppingstone for everything we study in macroeconomics, which is unfortunately the
way the core treats it today.3

3

An extreme form of this view of the real business cycle model as the steppingstone for everything else is the so‐
called “gap approach,” which essentially views and constrains the research agenda of macroeconomics to studying
the failures of the maximization conditions of this very particular model (Chari, Kehoe, and McGrattan, 2007).

7

Since the periphery is about isolating specific mechanisms, it surrounds the sources of
these mechanisms with assumptions designed to kill unwanted effects that would pollute the
message. It might seem as if the natural process to build a quantitative answer for the whole
would start with bringing back some of the realistic unwanted effects that were removed for
analytic convenience, and to model the interactions and complexities that arise from the
simultaneous presence of all these parts. But instead, the current core approach of
macroeconomics preserves many of the original convenience-assumptions from the research on
the periphery and then obsesses with “closing” the model by adding artificial factor supply
constraints (note that the emphasis is on the word artificial, not on the word constraints). All that
we learn from this exercise is what these artificial constraints do to the stylized mechanisms, not
what these mechanisms can do to the whole in a realistic setting, which should be our goal.4 We
need to stop this practice, at least as a norm, even if the cost is that we can’t make welfare
statements with the same degree of confidence that a fully structural model would allow us to
do—it would be a false pretense of knowledge anyway.
Moreover, the process of bringing the periphery into the core raises an obvious tension
about the role of rational expectations. Rational expectations is a central ingredient of the current
core; however, this assumption becomes increasingly untenable as we continue to add the
realism of the periphery into the core.5 While it often makes sense to assume rational
expectations for a limited application to isolate a particular mechanism that is distinct from the
role of expectations formation, this assumption no longer makes sense once we assemble the
whole model. Agents could be fully rational with respect to their local environments and
everyday activities, but they are most probably nearly clueless with respect to the statistics about
which current macroeconomic models expect them to have full information and rational
information.
This issue is not one that can be addressed by adding a parameter capturing a little bit
more risk aversion about macroeconomic, rather than local, phenomena. The reaction of human
beings to the truly unknown is fundamentally different from the way they deal with the risks
associated with a known situation and environment (Knight, 1921; Ellsberg, 1961). In realistic,

Again, there is nothing wrong with this approach as a test of the workings of that particular model; the problem
arises when it becomes a benchmark for something beyond that specific goal.
4

It is not rare to find in the literature that some mechanism is called irrelevant because it is killed by the artificial
constraints of the core. However, in many instances that can be corroborated by data, such results are really
indictments of the artificial constraints, not of the mechanisms.

5

Of course, part of the realism added to the core could come from reducing the IQ of its agents. An important line
of recent work started by Sims (2003) attempts to acknowledge human beings’ computational capacity constraints
by building on Shannon’s (1948) information theory work. However, much of this work has taken place within the
context of the current core models of macroeconomics, and hence it still trivializes the environment economic
agents have to confront.

8

real-time settings, both economic agents and researchers have a very limited understanding of the
mechanisms at work. This is an order-of-magnitude less knowledge than our core
macroeconomic models currently assume, and hence it is highly likely that the optimal
approximation paradigm is quite different from current workhorses, both for academic and
policy work. In trying to add a degree of complexity to the current core models, by bringing in
aspects of the periphery, we are simultaneously making the rationality assumptions behind that
core approach less plausible.
Moreover, this integrationist strategy does not come with an assurance that it will take us
to the right place, as there is an enormous amount of path dependence in the process by which
elements are incorporated into the core; and the baggage we are already carrying has the
potential to distort the selection of the mechanisms of the periphery that are incorporated. Given
the enormous complexity of the task at hand, we can spend an unacceptably long time wandering
in surrealistic worlds before gaining any traction into reality.
We ultimately need to revisit the ambitious goal of the core, of having a framework for
understanding the whole, from shocks to transmission channels, all of them interacting with each
other. The issue is how to do this without over-trivializing the workings of the economy (in the
fundamental sense of overestimating the power of our approximations) to a degree that makes
the framework useless as a tool for understanding significant events and dangerous for policy
guidance. I don't have the answer to this fundamental dilemma, but it does point in the direction
of much more diversification of research and methodology than we currently accept. It also
points in the direction of embracing, rather than sweeping under the rug, the complexity of the
macroeconomic environment. I turn to the latter theme next.

Facing and Embracing Economic Complexity
I suspect that embracing rather than fighting complexity and what it does to our modeling
would help us make progress in understanding macroeconomic events. One of the weaknesses
of the core stems from going too directly from statements about individuals to statements about
the aggregate, where the main difference between the two comes from stylized aggregate
constraints and trivial interactions, rather than from the richness and unpredictability of the
linkages among the parts. We need to spend much more time modeling and understanding the
topology of linkages among agents, markets, institutions, and countries.
By embracing complexity I do not mean the direct importation of the models from the
formal complexity literature in the physical sciences, as economics is, and is likely to remain,
fundamentally reductionist (that is, it seeks to understand the behavior of the whole from that of
the parts). The nodes of economic models are special, for they contain agents with frontal lobes

9

who can both strategize and panic, and it is these features that introduce much of the
unpredictability in the linkages I mentioned earlier.6
Having said this, some of the motivations for the econophysics literature do strike a chord
with the task ahead for macroeconomists. For example, Albert and Barabási (2002), in
advocating for the use of statistical mechanics tools for complex networks, write:
Physics, a major beneficiary of reductionism, has developed an arsenal of successful tools
for predicting the behavior of a system as a whole from the properties of its constituents. We
now understand how magnetism emerges from the collective behavior of millions of spins . .
. The success of these modeling efforts is based on the simplicity of the interactions between
the elements: there is no ambiguity as to what interacts with what, and the interaction
strength is uniquely determined by the physical distance. We are at a loss, however, to
describe systems for which physical distance is irrelevant or for which there is ambiguity as
to whether two components interact . . . there is an increasingly voiced need to move beyond
reductionist approaches and try to understand the behavior of the system as a whole. Along
this route, understanding the topology of the interactions between the components, i.e.,
networks, is unavoidable . . .
In any event, I will not review this literature here and instead will focus on arguments about
cumbersome linkages and agents' confusion about these linkages that are closer to mainstream
macroeconomics.

Dominoes and Avalanches
Allen and Gale’s (2000) model of financial networks and the inherent fragility of some of
these structures provided an early and elegant example of how linkages can cause substantial
instability with respect to shocks different from those the network was designed to handle.
Recently, Shin (2009) shows how fire sales can greatly magnify the domino mechanism
highlighted by Allen and Gale (2000). Another example of promising research in this style is
Rotemberg (2009), which uses graph theory to study how interconnectedness affects firms’
ability to make use of an exogenous amount of liquidity. Rotemberg studies a situation in which
all firms are solvent; that is, “the payments that any particular firm is expected to make do not
6

Durlauf (2004) offers a thoughtful survey and discussion of the econophysics literature and its limitations for
economic policy analysis, precisely because it “does not use models that adequately respect the purposefulness of
individual behavior.” This of course is a statement of the current state of affairs in the literature, not of its
potential, which is probably substantial. See Bak, Chen, Scheinkman, and Woodford (1993), Sheinkman and
Woodford (1994), Arthur, Durlauf, and Lane (1997), Durlauf (1993, 1997), Brock (1993), Brock and Durlauf (2001),
Acemoglu, Ozdaglar, and Tahbaz‐Zalehi (2010) for early steps in this direction.

10

exceed the payments it is entitled to receive.” He finds that interconnectedness can exacerbate
“the difficulties that firms have in meeting their obligations in periods where liquidity is more
difficult to obtain.”
Of course, the complex-systems literature itself offers fascinating examples of the power
of interconnectedness. Bak, Chen, Scheinkman, and Woodford. (1992) and Sheinkman and
Woodford (1994) bring methods and metaphors from statistical mechanics to macroeconomics.
They argue that local, nonlinear interactions can allow small idiosyncratic shocks to generate
large aggregate fluctuations, rather than washing out via the law of large numbers. They discuss
a kind of macroeconomic instability called “self-organized criticality,” comparing the economy
to a sand hill: at first, a tiny grain of sand dropped on the hill causes no aggregate effect, but as
the slope of the hill increases, eventually one grain of sand can be sufficient to cause an
avalanche. In the limit, aggregate fluctuations may emerge from hard-to-detect and purely
idiosyncratic shocks.

Panics
In a complex environment, agents need to make decisions based on information that is
astonishingly limited relative to all the things that are going on and that have the potential to
percolate through the system. This degree of ignorance is not something agents with frontal lobes
like to face. Anxiety is also part of the frontal lobe domain! Reactions that include anxiety and
even panic are key ingredients for macroeconomic crises.
Put differently, a complex environment has an enormous potential to generate truly
confusing surprises. This fact of life needs to be made an integral part of macroeconomic
modeling and policymaking. Reality is immensely more complex than models, with millions of
potential weak links. After a crisis has occurred, it is relatively easy to highlight the link that
blew up, but before the crisis, it is a different matter. All market participants and policymakers
know their own local world, but understanding all the possible linkages across these different
worlds is too complex. The extent to which the lack of understanding of the full network matters
to economic agents varies over the cycle. The importance of this lack of understanding is at its
most extreme level during financial crises, when seemingly irrelevant and distant linkages are
perceived to be relevant. Moreover, this change in paradigm, from irrelevant to critical linkages,
can trigger massive uncertainty, which can unleash destructive flights to quality.
As Benoit Mandelbrot, the mathematician perhaps best known for his work on fractal
geometry, once drew a parallel from economics to storms, which can only be predicted after they
form. Mandelbrot (2008, in a PBS NewsHour interview with Paul Solman on October 21, 2008)
said: “[T]he basis of weather forecasting is looking from a satellite and seeing a storm coming,

11

but not predicting that the storm will form. The behavior of economic phenomena is far more
complicated than the behavior of liquids or gases.”
Financial crises represent an extreme manifestation of complexity in macroeconomics,
but this element probably permeates the entire business cycle, probably in part through
fluctuations in the perceived probability that complexity and its consequences will be unleashed
in the near future. These fluctuations could be endogenous and arising from local phenomena as
highlighted by the formal complexity literature in physical sciences, but it also could be in
response to a more conventional macroeconomic shock, such as an oil or aggregate demand
shock, especially once these interact with more conventional financial-accelerator-type
mechanisms (like those in Kiyotaki and Moore, 1997; Bernanke and Gertler, 1989).
The periphery has made some progress on parts of these mechanisms. In Caballero and
Simsek (2009a, b), we capture the idea of a sudden rise in complexity followed by widespread
panic in the financial sector. In our model, banks normally collect basic information about their
direct trading partners, which serves to assure them of the soundness of these relationships.
However, when acute financial distress emerges in parts of the financial network, it is not
enough to be informed about these direct trading partners, but it also becomes important for the
banks to learn about the health of the partners of their trading partners to assess the chances of an
indirect hit. As conditions continue to deteriorate, banks must learn about the health of the
trading partners of the trading partners of their trading partners, and so on. At some point, the
cost of information gathering becomes too large and the banks, now facing enormous
uncertainty, choose to withdraw from loan commitments and illiquid positions. Haldane (2009)
masterfully captures the essence of the counterparty uncertainty problem that can arise in a
complex modern financial network: “Knowing your ultimate counterparty’s risk then becomes
akin to solving a high-dimension Sudoku puzzle.” A flight-to-quality ensues, and the financial
crisis spreads.
Taking a very different agency approach, and thinking about the specifics of the highfrequency repo market, Dang, Gorton, and Holmström (2009) show how a negative aggregate
shock can cause debt to become information sensitive, impeding the efficient trading of assets.
They point out that a security that pays off the same in all states of the world would be truly
information insensitive. Given limited liability, debt is the security that best approximates this
information-insensitive security in the real world and provides the least incentive for the creation
of private information. However, when a bad shock concentrates agents’ beliefs on states of the
world where debt does not pay off in full, agents would generate information before trading,
which raises possibilities of adverse selection and impedes trade. In this model, unexpectedly
enough, opacity would reduce the extent of adverse selection and thus would encourage trade.
In Caballero and Krishnamurthy (2008a), we illustrate with a model and examples of the
amplification role of Knightian uncertainty, which refers to risk that cannot be measured and
thus cannot be hedged. We pointed out that most flight-to-quality episodes are triggered by
12

unusual or unexpected events. In 1970, the default by Penn Central Railroad’s prime-rated
commercial paper caught the market by surprise. In October 1987, the speed of the stock
market’s decline led investors to question their models. In the fall of 1998, the co-movement of
Russian, Brazilian, and U.S. bond spreads surprised even sophisticated market participants. In
the recent financial crisis, another default on commercial paper—this time, by Lehman
Brothers—created tremendous uncertainty. The Lehman bankruptcy also caused profound
disruption in the markets for credit default swaps and interbank loans. The common aspects of
investor behavior across these episodes―re-evaluation of models, conservatism, and
disengagement from risky activities―indicate that these episodes involved Knightian uncertainty
and not merely an increase in risk exposure. The extreme emphasis on tail outcomes and worstcase scenarios in agents’ decision rules suggests aversion to this kind of uncertainty.7
Novelty and uncertainty play an important role in these episodes; for example, the
collapse of Amaranth hedge fund in 2006 caused little disruption to financial markets, whereas
the losses at Long-Term Capital Management in 1998 contributed to worldwide crisis despite the
rescue that was ultimately organized. Some observers similarly argued that the oil price spikes
of the 1970s were associated with much worse macroeconomic outcomes than those in the 2000s
because agents came to expect volatility and hence the recent shocks were not as “earth-shaking”
(Nordhaus, 2007).
Haldane (2009) compares the recent financial crisis to the Severe Acute Respiratory
System (SARS) outbreak earlier in the decade. Morbidity and mortality rates from SARS were,
“by epidemiological standards, modest.” Yet SARS triggered a worldwide panic, reducing
growth rates across Asia by 1–4 percentage points. Parents kept their children home from school
in Toronto, and Chinese restaurants in the United States were the targets of boycotts. Faced with
Knightian uncertainty, people conflated the possibility of catastrophe with catastrophe itself.

7

In Caballero and Krishnamurthy (2008b), we place the origins of the current crisis in this framework. We argue
that perhaps the single largest change in the financial landscape over the last five years was in complex credit
products: collateralized debt obligations, collateralized loan obligations, and the like. Market participants had no
historical record to measure how these financial structures would behave during a time of stress. These two
factors, complexity and lack of history, are the preconditions for rampant uncertainty. When the AAA subprime
tranches began to experience losses, investors became uncertain about their investments. Had the uncertainty
remained confined to subprime mortgage investments, the financial system could have absorbed the losses
without too much dislocation. However, investors started to question the valuation of the other credit
products―not just mortgages―that had been structured in much the same way as subprime investments. The
result was a freezing up across the entire credit market. The policy response to this initial freezing was timid,
which kept the stress on the financial system alive until a full blown “sudden financial arrest” episode developed
(after Lehman’s demise). See Caballero (2009) for an analogy between sudden cardiac arrest and sudden financial
arrest.

13

Some Policy Implications of a Confusing Environment
The centrality of surprises in financial and economic crises seems discouraging since it
would seem that it is difficult to fight something that is essentially impossible to predict, that
keeps changing, and that is not understood until after it happens.
However, some insights and systematic patterns are still possible. Certainly, it remains
useful to think about policies or institutions that, by affecting factors like capital requirements,
leverage, and maturity mismatches, can reduce the risk of crises. But financial and economic
crises are likely to happen anyway, and so it would be useful to consider in advance how policy
might respond, rather than needing to improvise.
For example, one common pattern across all episodes of this kind is that the confusion
triggers panics, and panics trigger spikes in the demand for explicit and implicit insurance. This
observation immediately hints at the core of the required policy response: When a large systemic
crisis of uncertainty strikes, the government must quickly provide access to reasonably priced
balance-sheet insurance to fragile and systemically important institutions.
In Caballero and Krishnamurthy (2008a), we showed that in an episode of Knightian
uncertainty, a government or central bank concerned with the aggregate will want to provide
insurance against extreme events, even if it has no informational advantage over the private
sector. The reason is that during a panic of this kind, each individual bank and investor fears
being in a situation worse than the average, an event that cannot be true for the collective
(individual agents know this aggregate constraint, but they assume they will be on the short end
of things). By providing a broad guarantee, the government gets the private sector to react more
than one-for-one with this guarantee, because it also closes the gap between the true average and
the average of panic-driven expectations. Many of the actual programs implemented during the
crisis had elements of guarantees, although they could probably have gone further.8 For example,
Ross (2009) argues convincingly that the government should supply impairment guarantees
along the lines of those used by the government-sponsored enterprises Fannie Mae and Freddie
Mac to improve the liquidity of the pool of legacy assets in banks balance sheets. Similarly, in
Caballero and Kurlat (2009), we proposed a policy framework which would not only guarantee
access to insurance in the event of a panic, but it would do so in a flexible manner that integrates
the role of the government as an insurer of last resort with private sector information on the

8

Insurance programs during the crisis included: a temporary program created by the U.S. Treasury
Department to insure money‐market funds; nonrecourse funding for the purchase of asset‐backed
securities through the Term Asset‐Backed Securities Loan Facility (TALF); and a temporary increase in
deposit insurance from $100,000 to $250,000. A notable example from outside the United States was
the U.K. Asset Protection Scheme, which backed over half a trillion pounds in post‐haircut assets for two
British banks. See Caballero (2009), Madigan (2009), and IMF (2009).
14

optimal allocation of contingent insurance.9 For examples of related public guarantee programs
and proposals, see Caballero (2009a, b); Mehrling and Milne (2008); and Milne (2009).
A related argument is developed by Geanakoplos in a series of papers. Geanakoplos
(2003, 2009) and Geanakoplos and Fostel (2008) highlight the role of margins in a theory of
“leverage cycles.” In these models, the supply and demand for loans determines not only the
interest rate, but also equilibrium leverage. Geanakoplos (2010) suggests a three-pronged
approach for government policy in the aftermath of a leverage cycle. First, the government
should address the precipitating cause of the crisis: the “scary bad news” and “massive
uncertainty,” which in the particular case of the recent crisis, affected the housing market.
Second, the government should create a lending facility to restore “reasonable” levels of
leverage. Third, the government should restore “optimistic” capital, for example, through
bailouts (although Geanakoplos is more sanguine than I am about the efficacy of “bailouts with
punishment,” as discussed in Caballero, 2010).
For the purpose of this essay, more important than the specific proposals is the
observation that the very acceptance of the key role played by complexity in significant
macroeconomic events should be enough to point us in the direction of the kind of policies that
can help to limit macroeconomic turbulence.

Robustness
A number of researchers have sought to design policy frameworks that are robust to small
modeling mistakes by the policymaker. For example, Hansen, Sargent, and their co-authors have
made substantial progress in incorporating robust control techniques to economic policy analysis
(for example, Hansen, Sargent, and Tallarini, 1999; Hansen and Sargent, 2007; Cogley, Colacito,
Hansen, and Sargent, 2008; Karantounias, Hansen, and Sargent, 2009). Woodford (2010) has
explored the same broad issue in the context of the standard New-Keynesian model used in
central banks’ research departments. This strategy is clearly a step in the right direction,
although I suspect the deviations they consider from the core models are still too local to capture
the enormous uncertainties and confusion that policymakers face in realistic nontrivial scenarios.
But this literature has many of the right words in it.

9

Under our proposal, the government would issue tradable insurance credits (TICs) which would be
purchased by financial institutions, some of which would have minimum holding requirements. During a
systemic crisis, each TIC would entitle its holder to attach a government guarantee to some of its assets.
All regulated financial institutions would be allowed to hold and use TICs, and possibly hedge funds,
private equity funds, and corporations as well. In principle, TICs could be used as a flexible and readily
available substitute for many of the facilities that were created by the Federal Reserve during the crisis.
15

The natural next step for this robustness literature is to incorporate massive uncertainty.
This step may also harbor some of the answers on how to deal with quantitative statements in a
highly uncertain environment. As a starting point, we probably need to relax the artificial microfoundation constraints imposed just for the sake of being able to generate “structural” general
equilibrium simulations. When closing a quantitative model, there should be an explicit
correspondence between the knowledge we assume in such closure and the state of our
knowledge about such closure. This means replacing artificial structural equations for looser
ones, or even for reduced-form data-relationships if this all that we really know. It is not nearly
enough (although it is progress) to do Bayesian estimation of the dynamic stochastic general
equilibrium model, for the absence of knowledge is far more fundamental than such an approach
admits (for example, Fernandez-Villaverde, 2009, provides a good survey of this literature).
We need to rework the mechanism the core currently uses to go from insights derived in
the periphery to quantitative general equilibrium ones. There has been considerable progress in
formalizing arguments for how dynamic adjustments happen, but while this is clearly progress, it
is not a substitute for the kind of progress I advocate here, which is to acknowledge explicitly our
degree of ignorance.10 There are many instances in which our knowledge of the true structural
relationship is extremely limited. In such cases, the main problem is not in how to formalize an
intuition but in the assumption that the structural relationship is known with precision.
Superimposing a specific optimization paradigm is not the solution to this pervasive problem, as
much of the difficulty lies precisely in not knowing which, and whose, optimization problem is
to be solved. For this reason the solution is not simply to explore a wide range of parameters for
a specific mechanism. The problem is that we do not know the mechanism, not just that we
don’t know its strength.
But how do we go about doing policy analysis in models with some loosely specified
blocks not pinned down by specific first-order conditions? Welcome to the real world! This task
is what actual policymakers face. Academic models often provide precise policy prescriptions
because the structure, states, and mechanisms are sharply defined. In contrast, policymakers do
not have these luxuries. Thoughtful policymakers use academic insights to think about the type
of policies they may want to consider, but then try to understand the implications of such policies
when some (or most) of the assumptions of the underlying theoretical model do not hold.
10

As Lucas (2003), following on a Friedman (1946) lead, points out: “[T]he theory is never really solved. What are
the predictions of Patinkin’s model? The model is too complicated to work them out. All the dynamics are the
mechanical auctioneer dynamics that Samuelson introduced, where anything can happen. . . . You can see from his
verbal discussion that he’s reading a lot of economics into these dynamics. What are people thinking? What are
they expecting? . . . He’s really thinking about intertemporal substitution. He doesn’t know how to think about it,
but he is trying to.” In the specific example used here by Lucas, the Lucas–Rapping (1969) model, for example, did
represent significant progress over the Patinkin modeling of labor dynamics. It solved the “how” part of the formal
underpinning of Patinkin’s dynamics. But as I point out in the text, this kind of solution is insufficient.

16

However, this kind of robustness analysis is nearly absent in our modeling. In this sense, and as I
mentioned earlier, the work of Hansen and Sargent (2007) and others on robust control in
policymaking points in the right direction, although I think we need to go much, much further in
reducing the amount and type of knowledge policymakers and economic agents are assumed to
possess.
One primary driving force behind modern macroeconomics (both core and periphery)
was an attempt to circumvent the Lucas critique—the argument that market participants take the
policy regime into account and so estimates of economic parameters for one policy regime may
well not be valid if the policy regime changes. If we now replace some first-order conditions by
empirical relationships and their distributions, doesn’t this critique return to haunt us? The
answer must be “yes,” at least to some extent. But if we do not have true knowledge about the
relationship and its source, then assuming the wrong specific first-order condition can also be a
source of misguided policy prescription. Both the ad-hoc model and the particular structural
model make unwarranted specific assumptions about agents’ adaptation to the new policy
environment. The Lucas critique is clearly valid, but for many (most?) policy questions we
haven’t yet found the solution—we only have the pretense of a solution.
Ultimately, for policy prescriptions, it is important to assign different weights to those
that follow from blocks over which we have true knowledge, and those that follow from very
limited knowledge. Some of this has already been done in the asset pricing literature: for
example, Ang, Dong, and Piazzesi (2007) use arbitrage theory to constrain an otherwise
nonstructural econometric study of the yield curve and Taylor’s rule. Perhaps a similar route can
be followed in macroeconomics to gauge the order of magnitude of some key effects and
mechanisms, which can then be combined with periphery insights to generate back-of-theenvelope-type calculations. For now, we shouldn't pretend that we know more than this, although
this is no reason to give up hope. We have made enormous progress over the last few decades in
the formalization of macroeconomics. We just got a little carried away with the beautiful
structures that emerged from this process.

The Pretense of Knowledge
The root cause of the poor state of affairs in the field of macroeconomics lies in a
fundamental tension in academic macroeconomics between the enormous complexity of its
subject and the micro-theory-like precision to which we aspire.
This tension is not new. The old institutional school concluded that the task was
impossible and hence not worth formalizing in mathematical terms (for example, Samuels, 1987,
and references therein). Narrative was the chosen tool, as no mathematical model could capture
17

the richness of the world that is to be explained. However, this approach did not solve the
conundrum; it merely postponed it. The modern core of macroeconomics swung the pendulum
to the other extreme, and has specialized in quantitative mathematical formalizations of a precise
but largely irrelevant world. This approach has not solved the conundrum either. I wish the
solution was to be found somewhere in between these polar opposites, but it is not clear what “in
between” means for a range that has a framework based on verbal discussions of the real world
on one end and one based on quantitative analysis of an “alternative” world, on the other.
The periphery of macroeconomics has much to offer in terms of specific insights and
mechanisms, but to fulfill the ambition of the core we need to change the paradigm to go from
these insights on the parts to the behavior of the whole. It is not about embedding these into
some version of the canonical real business cycle model. It is, among other things, about
capturing complex interactions and the confusion that they can generate.
From a policy perspective, the specifics of a crisis are only known once the crisis starts.
For this reason, my sense is that, contrary to the hope of policymakers and regulators, there is
limited scope for policy that can in advance eliminate the risk or costs of financial crisis, beyond
some common-sense measures (like capital requirements for financial institutions) and very
general public–private insurance arrangements (like deposit insurance). By the time a true
financial crisis is underway, the immediately relevant policy issues are no longer about whether
intervention might breed moral hazard, but about a socially wasteful reluctance to invest and to
hire, and the extent to which predatory trading or fire sales can be minimized.
Going back to our macroeconomic models, we need to spend much more effort in
understanding the topology of interactions in real economies. The financial sector and its recent
struggles have made this need vividly clear, but this issue is certainly not exclusive to this sector.
The challenges are big, but macroeconomists can no longer continue playing internal
games. The alternative of leaving all the important stuff to the “policy”-types and informal
commentators cannot be the right approach. I do not have the answer. But I suspect that
whatever the solution ultimately is, we will accelerate our convergence to it, and reduce the
damage we do along the transition, if we focus on reducing the extent of our pretense-ofknowledge syndrome.

18

References
Acemoglu, Daron, Asuman Ozdaglar, and Alireza Tahbaz-Zalehi. 2010. “Cascades in
Networks and Aggregate Volatility.” Available at: http://econwww.mit.edu/faculty/acemoglu/paper.
Albert, Réka, and Albert-László Barabási. 2002. “Statistical Mechanics of Complex
Networks.” Review of Modern Physics, 74(1): 47–97.
Allen, Franklin, and Douglas Gale. 2000. “Financial Contagion.” Journal of Political
Economy, 108(1): 1–33.
Ang, Andrew, Sen Dong, and Monika Piazzesi. 2007. “No-Arbitrage Taylor Rules.” Available
at SSRN: http://ssrn.com/abstract=621126.
Arthur, William B., Steven N. Durlauf, and David A. Lane, eds. 1997. The Economy as an
Evolving Complex System II. Reedwood City: Addison-Wesley.
Bak, Per, Kan Chen, Jose Scheinkman, and Michael Woodford. 1993. “Aggregate
Fluctuations from Independent Sectoral Shocks: Self-Organized Criticality in a Model of
Production and Inventory Dynamics.” Richerche Economiche, 47(1): 3–30.
Bernanke, Ben, and Mark Gertler. 1989. “Agency Costs, Net Worth and Business
Fluctuations.” The American Economic Review, 79(1): 14–31.
Brock, William A. 1993. “Pathways to Randomness in the Economy: Emergent Nonlinearity
and Chaos in Economics and Finance.” Estudios Económicos, 8(1): 3–55.
Brock, William, and Steven N. Durlauf. 2001. “Discrete Choice with Social Interactions.”
Review of Economic Studies, 68(2): 235–60.
Caballero, Ricardo J. 2009. “Sudden Financial Arrest.” Prepared for the Mundell‐Fleming
Lecture delivered at the Tenth Jacques Polak Annual Research Conference, IMF, November 8.
Caballero, Ricardo J. 2010. “Crisis and Reform: Managing Systemic Risk.” Prepared for the
XI Angelo Costa Lecture delivered in Rome on March 23.
Caballero, Ricardo J., and Arvind Krishnamurthy. 2008a. “Collective Risk Management in a
Flight to Quality Episode.” Journal of Finance, 63(5): 2195–2229.
Caballero, Ricardo J., and Arvind Krishnamurthy. 2008b. “Knightian Uncertainty and Its
Implications for the TARP.” Financial Times Economists’ Forum, November 24.

19

Caballero, Ricardo J., and Pablo Kurlat. 2009. “The “Surprising” Origin and Nature of
Financial Crises: A Macroeconomic Policy Proposal.” Prepared for the Jackson Hole
Symposium on Financial Stability and Macroeconomic Policy, August.
http://www.kansascityfed.org/publicat/sympos/2009/papers/caballeroKurlat.07.29.09.pdf.
Caballero, Ricardo J., and Alp Simsek. 2009a. “Complexity and Financial Panics.” NBER
Working Paper 14997.
Caballero, Ricardo J., and Alp Simsek. 2009b. “Fire Sales in a Model of Complexity.”
http://econ-www.mit.edu/files/4736.
Chari, V. V., Patrick J. Kehoe, and Ellen R. McGrattan. 2007. "Business Cycle Accounting."
Econometrica, 75(3): 781–836.
Cogley, Timothy, Riccardo Colacito, Lars Peter Hansen, and Thomas J. Sargent. 2008.
“Robustness and U.S. Monetary Policy Experimentation.” Available at SSRN:
http://ssrn.com/abstract=1267033. (Forthcoming in Journal of Money, Credit, and Banking).
Dang, Tri Vi, Gary Gorton, and Bengt Holmström. 2009. “Ignorance and the Optimality of
Debt for the Provision of Liquidity.”
http://www4.gsb.columbia.edu/null/download?&exclusive=filemgr.download&file_id=7213758.
Durlauf, Steven N. 1993. “Nonergodic Economic Growth.” Review of Economic Studies, 60(2):
349–66.
Durlauf, Steven N. 1997. “Statistical Mechanics Approaches to Socioeconomic Behavior.” In
The Economy as a Complex Evolving System II, ed. W. Brian Arthur, Steven N. Durlauf, and
David A. Lane, 81-104. Redwood City, CA: Addison-Wesley.
Durlauf, Steven N. 2004. “Complexity and Empirical Economics.” Economic Journal,
115(504): F225–F243.
Ellsberg, Daniel. 1961. “Risk, Ambiguity, and the Savage Axioms.” Quarterly Journal of
Economics, 75(4): 643–69.
Fernández-Villaverde, Jesús. 2009. “The Econometrics of DSGE Models.” NBER Working
Paper 14677.
Friedman, Milton. 1946. “Review of ‘Price Flexibility and Employment.’” American Economic
Review, September.
Galí, Jordi, and Mark Gertler. 2007. “Macroeconomic Modeling for Monetary Policy
Evaluation.” Journal of Economic Perspectives, 21(4): 25–45.

20

Geanakoplos, John. 2003. “Liquidity, Default, and Crashes: Endogenous Contracts in General
Equilibrium.” In Advances in Economics and Econometrics: Theory and Applications, Eighth
World Conference, Vol. 2, 170–205. Econometric Society Monographs.
Geanakoplos, John. 2009. “The Leverage Cycle.” Cowles Foundation Discussion Paper 1715.
Available at SSRN: http://ssrn.com/abstract=1441943
Geanakoplos, John. 2010. “Solving the Present Crisis and Managing the Leverage Cycle.”
Cowles Foundation Discussion Paper No. 1751.
http://cowles.econ.yale.edu/P/cd/d17b/d1751.pdf mimeo.
Geanakoplos, John, and Ana Fostel. 2008. “Leverage Cycles and the Anxious Economy.”
American Economic Review, 98(4): 1211–44.
Haldane, Andrew G. 2009. “Rethinking the Financial Network.” Speech delivered at the
Financial Student Association in Amsterdam on April 28.
Hansen, Lars Peter, Thomas J. Sargent, and Thomas D. Tallarini, Jr. 1999. “Robust
Permanent Income and Pricing.” The Review of Economic Studies, 66(4): 873–907.
Hansen, Lars Peter, and Thomas J. Sargent. 2007. Robustness. Princeton University Press.
International Monetary Fund. 2009. Global Financial Stability Report: Navigating the
Financial Challenges Ahead, October. Preliminary version downloaded October 15, 2009.
Jackson, Matthew O. 2008. Social and Economic Networks. Princeton University Press.
Karantounias, Anastasios G., Lars Peter Hansen, and Thomas J. Sargent. 2009. “Managing
Expectations and Fiscal Policy.” Federal Reserve Bank of Atlanta Working Paper 2009-29.
October.
Kiyotaki, Nobuhiro, and John Moore. 1997. “Credit Cycles.” Journal of Political Economy,
105(2): 211–48.
Knight, Frank H. 1921. Risk, Uncertainty and Profit. Boston: Houghton Mifflin.
Lucas, Robert E. 2003. “My Keynesian Education.” Keynote Address to the 2003 HOPE
Conference.
Lucas, Robert E., and Leonard A. Rapping. 1969. “Real Wages, Employment, and Inflation.”
Journal of Political Economy, 77(5): 721–54.
Madigan, Brian F. 2009. “Bagehot's Dictum in Practice: Formulating and Implementing
Policies to Combat the Financial Crisis.” Speech for Federal Reserve Bank of Kansas City's
Annual Economic Symposium, Jackson Hole, Wyoming, August 21.
21

Mandelbrot, Benoit. 2008. PBS NewsHour interview with Paul Solman, October 21.
Mehrling, Perry, and Alistair Milne. 2008. “Government’s Role as Credit Insurer of Last
Resort and How It Can Be Fulfilled.” Unpublished paper.
http://www.econ.barnard.columbia.edu/faculty/mehrling/creditinsureroflastresortfinal09Oct2008.
pdf.
Milne, Alistair. 2009. The Fall of the House of Credit. Cambridge University Press,
Cambridge and New York.
Nordhaus, William. 2007. “Who’s Afraid of a Big Bad Oil Shock?” Prepared for Brookings
Panel on Economic Activity.
http://nordhaus.econ.yale.edu/Big_Bad_Oil_Shock_Meeting.pdfSeptember.
Reinhart, Carmen M., and Kenneth S. Rogoff. 2009. This Time Is Different: Eight Centuries
of Financial Folly. Princeton University Press.
Ross, Stephen A. 2009. “A Modest Proposal.” Unpublished paper, MIT.
Rotemberg, Julio J. 2009. “Liquidity Needs in Economies with Interconnected
Financial Obligations,” Unpublished paper, May 14. (Also an August, 2008, NBER Working
Paper, No. 14222.)
Samuels, Warren J. 1987. "Institutional Economics." In The New Palgrave: A Dictionary of
Economics, vol. 2, ed. by Murray Mitgate, Peter Newman, and John Eatwell. Macmillan.
Scheinkman, Jose, and Michael Woodford. 1994. “Self-Organized Criticality and Economic
Fluctuations.” American Economic Review, 84(2): 417–21.
Shannon, Claude E. 1948. “A Mathematical Theory of Communication.” Bell System
Technical Journal, Vol. 27, July, pp. 379–423 and Vol. 27, October, 623–56.
Shin, Hyun Song. 2009. “Financial Intermediation and the Post-Crisis Financial System.” Paper
presented at the 8th BIS Annual Conference, June.
Sims, Christopher A. 2003. “Implications of Rational Inattention.” Journal of Monetary
Economics, 50(3): 665–90, April.
Tirole, Jean. 2006. The Theory of Corporate Finance. Princeton University Press.

22

von Hayek, Friedrich A. 1974. “The Pretence of Knowledge.” Prize Lecture, The Sveriges
Riksbank Prize in Economic Sciences in Memory of Alfred Nobel.
Woodford, Michael. 2010. “Robustly Optimal Monetary Policy with Near-Rational
Expectations.” American Economic Review, 100(1): 274–303.

23

