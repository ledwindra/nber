NBER WORKING PAPER SERIES

ADMINISTRATIVE DISCRETION IN SCIENTIFIC FUNDING:
EVIDENCE FROM A PRESTIGIOUS POSTDOCTORAL TRAINING PROGRAM
Donna K. Ginther
Misty L. Heggeness
Working Paper 26841
http://www.nber.org/papers/w26841

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
March 2020
Corresponding Author: Misty L. Heggeness, U.S. Census Bureau, 4600 Silver Hill Road,
Washington, DC 20746 (e-mail: misty.heggeness@gmail.com). This paper reports the results of
research and analysis undertaken by the National Institutes of Health (NIH) staff, consultants, and
contractors. We acknowledge generous support and feedback from Lisa Evans, Henry
Khachaturian, Kay Lund, Sherry Mills, Matthew Notowidigdo, Walter T. Schaffer, and Jennifer
Sutton. This work would not have been possible without the amazing skills and assistance of
Frances D. Carter-Johnson, Maria I. Larenas, Ryan Pfirrmann-Powell, and Ying Zeng
within the NIH Division of Biomedical Research Workforce Programs, as well as
participants from the 2018 Association for Public Policy Analysis and Management (APPAM)
fall research conference and seminar participants at Iowa State University. We appreciate the
time and candor given to us by various NIH institute program officers and training directors
who so graciously agreed to be interviewed in an effort to help us truly understand the NRSA
funding process on the ground. We also give very special thanks to Pierre Azoulay, Hyungjo
Hur, Ming Lei, Jon Lorsch, Julie Mason, Michael Spittel, and Jonathan Wiest for taking the time
to provide critical feedback. This analysis began while Misty L. Heggeness was employed at the
National Institutes of Health. Donna Ginther acknowledges financial support from NIH Division
of Biomedical Research Workforce (DBRW) (Contract No. HHSN276201300089U) to NETE
Solutions. Any errors in this paper are the sole responsibility of the authors. The views
presented here do not necessarily reflect any official position of the National Institutes of
Health, U.S. Census Bureau, the Federal Reserve System, the Federal Reserve Bank of
Minneapolis, or the University of Kansas. The views expressed herein are those of the authors
and do not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
¬© 2020 by Donna K. Ginther and Misty L. Heggeness. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including ¬© notice, is given to the source.

Administrative Discretion in Scientific Funding: Evidence from a Prestigious Postdoctoral
Training Program
Donna K. Ginther and Misty L. Heggeness
NBER Working Paper No. 26841
March 2020
JEL No. J24,O3,O38
ABSTRACT
The scientific community is engaged in an active debate on the value of its peer-review system.
Does peer review actually serve the role we envision for it‚Äîthat of helping government agencies
predict what ideas have the best chance of contributing to scientific advancement? Many federal
agencies use a two-step review process that includes programmatic discretion in selecting awards.
This process allows us to determine whether success in a future independent scientific-research
career is more accurately predicted by peer-review recommendations or discretion by program
staff and institute leaders. Using data from a prestigious training program at the National Institute
of Health (NIH), the Ruth L. Kirschstein National Research Service Award (NRSA), we provide
evidence on the efficacy of peer review. We find that, despite all current claims to the contrary,
the existing peer-review system works as intended. It more closely predicts high-quality science
and future research independence than discretion. We discover also that regression discontinuity,
the econometric method typically used to examine the effect of scientific funding, does not fit
many scientific-funding models and should only be used with caution when studying federal
awards for science.

Donna K. Ginther
Department of Economics
University of Kansas
333 Snow Hall
1460 Jayhawk Boulevard
Lawrence, KS 66045
and NBER
dginther@ku.edu
Misty L. Heggeness
U.S. Census Bureau
4600 Silver Hill Road
Washington, DC 20233
and Federal Reserve Bank of Minneapolis
misty.l.heggeness@census.gov

I. Introduction
For most, peer review has always been a pillar of the scientific enterprise.
Recent attacks on the U.S. federal-funding system, however, have questioned
whether peer review identifies the best science (Stahel and Moore 2014; Fang and
Casadevall 2016; Fang, Bowen, and Casadevall 2016; Gallo, Sullivan, and Glisson
2016; Li and Agha 2015; Li 2017). This is not the first time the peer-review system
has come into question (Gustafson 1975; Roy 1985; McNutt et al. 1990; Travis and
Collins 1991; Godlee, Gale, and Martyn 1998; Wessely 1998; Smith 2006; Costello
2010). In fact, in 1975 Congress conducted a public debate during the National
Science Foundation Peer Review Special Oversight Hearings to evaluate the role
of peer review in federal scientific funding. For the most part, it concluded that peer
review, compared with all other options (including agency discretion), worked as
expected (Baldwin 2018). Gustafson (1975, 1065) argued that ‚Äúfor most types of
fundamental research the traditional project grant, selected by peer review, with
overall priority among fields and subfields determined at least in part by proposal
pressure, appears to provide the best available guarantee of scientific merit and
accurate information.‚Äù In this paper, we study the funding decisions at the National
Institutes of Health (NIH) and examine whether discretion or peer review is more
effective at identifying future engagement in science. Our results show that peer
review identifies future success more often than program officer discretion.
Whether today or in the past, questions continue as to whether the peer review
system functions like an old boys‚Äô club. On the one hand, we are lucky that in recent
decades the number of scientists in the U.S. has been greater than at any other time

in history, as more and more get trained and fewer leave the larger ecosystem of
science when they reach retirement age (Heggeness et al. 2016, 2017; Blau and
Weinberg 2017). More scientists means more intellect being poured into scientific
advancements and the betterment of humankind‚Äînot only in academia but in
industry and government as well. However, more scientists also suggests increased
competition‚Äîespecially for academic science positions (Tilghman and Rockey
2012)‚Äîand competition has been heating up for scientific grant funding. Even
those with low percentile scores on grant applications are finding themselves out of
the loop and unfunded. Frustrated with a system that is not able to fund as many
high-quality science projects as are out there, peer review continues to be debated
(Scarpa 2006; Marsh, Upali, and Bond 2008; Fang and Casadevall 2016; Fang,
Bowen, and Casadevall 2016).
While it may seem like the peer-review system has always been an integral part
of the scientific machinery, throughout history journal editors and program
managers have exercised varying levels of discretion. In fact, scientific funding and
journal review have a much longer history of relying on funding managers and
journal editors to make discretionary decisions about what constitutes good science.
These discretionary decisions made by leaders within the scientific community
drove much of what we knew as scientific advancements well into the 1970s (Smith
2006; Benos et al. 2007; Baldwin 2018). The question remains: Do scientific
leaders in control of grant funding (and, more broadly, journal publications)
identify quality science and future superstar scientists more precisely than the
current peer-review system?

2

While the issue of peer review in journal publications is equally relevant, its
role is different. Journal editors aim to identify whether an already-completed
scientific manuscript is novel enough to count as a solid contribution to their
journal. In most cases, scientific funding agencies do not know what the results of
the proposed project will be and whether it will, in fact, produce results. Funding
agencies rely on the scientific track record of the principal investigator and
educated opinions about whether the idea is robust and worthy enough to fund.
Since scientific funding decisions occur at the first stage of an innovation and are,
by nature, a riskier proposition, there is more nuance in determining what will turn
out to be novel science. For this reason, we focus on identifying the impact of peer
review at an innovative endeavor‚Äôs first stage of development.
The U.S. federal government is a major contributor to the advancement of the
scientific enterprise. Federal agencies that fund scientific research include the
National Institutes of Health (NIH), National Science Foundation (NSF), Defense
Advanced Research Projects Agency (DARPA), Advanced Research Projects
Agency-Energy (ARPA-E), United States Department of Agriculture (USDA), and
a handful of other agencies. NIH, the largest scientific agency in the world, is by
far the primary federal funder of biomedical research. Each year it provides over
$39.2 billion in funding.1 The NIH budget has increased since 2013 and more than
doubled since 2000.2 While NIH is the primary funder, other agencies also
distribute funds for biomedical and scientific research, and while each agency has

1

For more information, see https://www.nih.gov/about-nih/what-we-do/budget
For more information, see
https://officeofbudget.od.nih.gov/pdfs/FY19/Approp%20History%20by%20IC%20FY%202000%
20-%20FY%202019%20(V3).pdf
2

3

its own system, the systems have similarities. For example, most agencies send a
proportion of proposals out for external review, and all have program officers who
manage the grant portfolios and help ensure the research funded meets the priorities
of the organization. Nevertheless, given the immense size of the NIH funding
portfolio, and the fact that the review process is the same across all institutes at the
NIH, it is critically important to understand how discretion operates in conjunction
with the peer review process.
Peer review is often seen as a singular and independent method to produce highquality evaluations of scientific projects potentially worth funding. In reality, no
peer-review system mechanically follows rules all the time. Owing to the nature of
science and the federal managers‚Äô responsibility to hold scientists accountable,
there is and always will be some element of discretion within the grant-awarding
system. All federal agencies have some sort of two-step decision-making process
where expert opinion is sought through peer review, then internal deliberations are
undertaken with agency leaders and scientific-program staff, and finally budgets
are explored. All these steps occur before any final decisions are made. The relevant
questions to ask are, (1) How much discretion exists? (2) How is it documented?
And (3) what implications does the level of discretion have on the future of science?
We take advantage of this type of two-stage review to examine the effectiveness
of peer review versus that of discretion in identifying successful scientists. Using
data from administrative records on applicants and awardees of a prestigious
fellowship program at the NIH between 1996 and 2008, we compare the outcomes
of applicants chosen with discretion with those of applicants chosen through peer

4

review. In many cases, the NIH uses discretion in awarding grants, and this is
particularly true when awarding the Ruth L. Kirschstein National Research Service
Award (NRSA) F32 fellowships. As a result, we can categorize individuals who
were ‚Äúskipped over‚Äù and those ‚Äúreached for‚Äù in discretionary decisions made by
program and institute staff and identify how the peer review system performs
compared with discretion. We use the scoring of fellowship applications and
matching techniques that allow us to analyze a limited set of outcomes for
individuals who received the fellowship and for those whose application scores and
observable characteristics are similar, but who did not receive an award.
Our study makes two contributions. First, by comparing the applications of
individuals who are deemed high quality by peer review but ‚Äúskipped‚Äù in the
funding process with those of individuals who are ‚Äúreached for‚Äù through
discretionary decisions by program officers and institute leadership, we show that
peer review predicts future NIH research awards at a higher rate than discretion.
Second, although in theory the method of awarding funding appears consistent with
a regression discontinuity design (RDD), we demonstrate that in practice RDD is
not an appropriate method to study scientific awards where high levels of discretion
exist in the decision-making process. RDD is not an appropriate method in this
environment because discretion drives the second stage of decision making. In
addition, NIH institute budgets are semi-fluid. Within agencies, funds shift between
first-, second-, and third-round funding opportunities and, in some instances,
between programs as funders examine the quality and depth of projects in each
round.

5

The paper proceeds as follows. Section II summarizes the historical use of
discretion and peer review and describes why the federal grant-selection process
does not always fit the mold of a discontinuous kink at an agency budget line for
funding along some peer-evaluated score. Section III describes the methods we
employ and data we use. Section IV analyzes the results comparing the career
outcomes of those who were chosen with programmatic discretion versus those who
were chosen through peer review. Section V is a discussion, and Section VI
concludes.

II. Background
A. Scientific Funding, Discretion, and Peer Review
Scientific peer-review systems began to appear in the United States in the
1940s, driven by federal scientific agencies (Azoulay, Graff Zivin, and Manso
2013; Farrell, Farrell, and Farrell 2017). At its core, peer review provides a
systematic process for scientific evaluation driven by the expertise and experience
of an extramural community of experts. Its main goal is to provide guidance,
direction, and leadership to federal agencies in selecting the most promising
scientific research proposals for funding. Today, it is embedded within the scientific
enterprise as a strong institutional norm and has been driving scientific-funding
decisions since the 1970s (Baldwin 2018). While it is a foundation of much
scientific activity, peer review ‚Äúin theory‚Äù often looks different than peer review
‚Äúin practice.‚Äù Throughout history, scientific institutions have continually made

6

discretionary decisions regarding which scientific proposals to fund and where to
allocate scarce resources.
Discretion, however, may come at a cost. Past studies have shown that actuarial
judgement outperforms clinical assessments (Dawes, Faust, and Meehl 1989).
More recent studies have highlighted how human discretion in hiring can lead to
less-than-average hiring outcomes (Hoffman, Kahn, and Li 2018) and how judicial
discretion in criminal courts may lead to increased crime or increased jailing
(Kleinberg et al. 2018).
Baldwin documents large amounts of discretion at the initiation of scientific
peer review in the 1940s on the part of federal agencies like the National Institutes
of Health:
When the U.S. government formed the National Institutes of Health
(NIH) in 1948, its Division of Research Grants initially evaluated
grant applications with little or no consultation from outside
referees. Instead, each application went first to a small ‚Äústudy
section‚Äù composed of NIH-affiliated scientific experts in a particular
field. From there, the study sections‚Äô recommendations were
forwarded to an NIH council of scientists and laymen, which added
its own recommendations. Final decision-making power rested in
the hands of the institute directors, heads of NIH member
institutions such as the National Cancer Institute and the National
Eye Institute. While the directors took the earlier evaluations into
account, they were not obligated to follow the recommendations of
the study sections or the council. Furthermore, NIH applicants
would receive little information about why their grants had been
accepted or rejected. Deliberations about the grants were considered
confidential and internal to the NIH. (2018, 545)
With time and criticism against agencies mounting, peer review gained momentum
and played an increasingly dominant role in selection of awards, but always within
the bounds of discretion and the agency‚Äôs ability and desire to fund promising, highpriority research. Baldwin notes that some continued to endorse ‚Äúthe older system
7

of using . . . peer review reports as advisory documents, saying . . . peer review has
its uses as a first round of proposal screening, but it does not absolve the
Government program manager from full responsibility for the decision to fund or
reject a proposal. . . . There are some things that we should not ask of peer review.
We should not ask it to take Government agencies off the hook on the question of
protecting the public purse‚Äù (556).
The back-and-forth discussion of the peer review‚Äôs role in federal funding ebbs
and flows. Peer review‚Äôs relevance continued to increase from the 1940s to the
1970s‚Äîwhen most, if not all, agencies established a formalized peer review
system. While congressional hearings in 1975 concluded that peer review was the
fairest and best way to allocate scientific funding (compared with agency
discretion), some scientists were seeing a challenge to the full use of peer review in
resource allocation. Gustafson (1975) writes, ‚ÄúEven in the programs in which
external peer review panels have the greatest sway, there appears to be the
opportunity for the agency staff to influence the process of proposal evaluation by
shaping the agenda, channeling the flow of information to and from the outside
advisers, or actually altering or overriding their decisions. In the NIH this influence
is discreet and informal, while in the NSF it is usually more important than that of
the external advisers. In both agencies, the importance of the professional staff
appears to be growing‚Äù (1064). In fact, within the NIH, a wide range of variation
in discretion exists depending on both the grant mechanism and the institute
funding the research.

8

Smith (2006), decades later, argued against any reality of peer review being
independently operational, highlighting challenges even in the existence of an
agreed-upon operational definition of peer review. If, as Smith states, peer review
is challenging to operationally define, how might we expect it to be the primary and
sole driver of a grant-award system in practice? Regardless of the challenges, the
NIH began enhancing the peer review program in 2007. The measures it took
included incorporating process and change phases and shortened, restructured
applications in an effort to ‚Äúfund the best science, by the best scientists, with the
least amount of administrative burden‚Äù (NIH 2011). Below, we further highlight
how federal agencies handle peer review and the allocation of resources, which we
argue always includes a peer-review process mixed with the discretionary priorities
of the agency and program staff.

B. The Value of Peer Review
Studies have attempted to detangle the value of peer review in selecting the
best, most innovative science. Li and Agha (2015) studied risk aversion in peer
review. Specifically, they examined whether peer review selects projects already
demonstrating success or chooses big-name scientists for continued funding
regardless of the novelty of their current grant proposal idea. They established that
proposals with better scores have higher numbers of publications and citations, a
finding that supports the idea that peer review is working. This result was also
confirmed by Gallo et al. (2014) in another funding context. However, when Fang,
Bowen, and Casadevall (2016) compared percentile scores of 20 or better, they

9

found that peer-review scoring had limited predictive power for future publications
and citations.
In somewhat related research, Pier et al. (2018) uncovered little agreement on
proposal quality in an experiment designed to mimic the NIH peer-review process.
When Li (2017) examined expertise versus bias in NIH peer review, she discovered
that expertise slightly outweighed the cost of bias. She also demonstrated that when
reviewers and researchers shared expertise, they more harshly judged proposals.
Gallo, Sullivan, and Glisson (2016) obtained a similar result in another funding
context. Furthermore, Ayoubi, Pezzoni, and Visentin (2019) suggest that the very
process of applying for research funding improves publications, regardless of
whether the researcher receives an award.
Both a handful of smaller studies that randomized peer review and senior
journal editors discussing their experience have suggested that peer review is little
better than chance at selecting which science to fund or publish (Smith 2006).
Taken together, prior work has come to different conclusions on the validity and
efficiency of the peer-review process. These studies assess questions and concerns
associated with the quality of the peer-review system and peer review‚Äôs ability to
predict bold and innovative science, but they do not evaluate specific differences
between discretion and a systematic peer-review process. While Goldstein and
Kearney (2018) show how federal-program directors use discretion to allocate
funds in alignment with an agency mission, to our knowledge our study is one of
the first within the context of federal funding for biomedical science to answer the

10

comparative question of whether peer review or discretionary decisions more
accurately identify scientists who will develop into independent researchers.

C. Two-Stage Institutional Behavior and Award Decisions
Most federal agencies have some style of two-stage review. At the NIH, there
are 27 institutes and centers, and each one has complete independence from the
others in allocating awards. Grant awards are generally decided based on some
weighted mix of peer-review scores, the research priorities of institute leadership,
and the discretionary behavior of staff. In terms of award type, these organizations
greatly vary in their process for awarding a grant, with smaller grants like
fellowships exhibiting much larger discretionary influence than large independentresearch awards. For this study, we interviewed staff at four institutes and centers.
These people provided contextual knowledge into the process of selection.
Together, they encompassed a range of variation in institute size, disease focus, and
training programs.
Our interviews with program staff indicate that the funding process for
fellowships is indeed complex. Most institutes receive application-review scores as
defined by a study section coordinated through the NIH‚Äôs Center for Scientific
Review (CSR). Once the institute receives the scores, program officers and staff
assess the full application, including a summary statement from peer review, the
quality of the applicant and his or her institution, and the alignment of the research
proposal with the institute‚Äôs priorities. Members of the institute‚Äôs staff‚Äî
specifically, program officers‚Äîthen participate in a team meeting in which they

11

defend the proposals that best match their defined priorities. Together, the program
officers, the training director, and other institute staff make a joint decision for
recommendations to the institute director. Either the institute director or his or her
delegate makes the final decision and signs off on which proposals to fund. Institute
directors vary in terms of their direct involvement in the consideration and final
approval of proposals. Before making a final decision and informing the candidates,
the budget office reviews and signs off on the final list of candidates, primarily
making sure sufficient funds are available for the recommended awards.

D. Scientific Funding‚ÄîType and Evaluative Technique
i. Research Training‚ÄîA Case Study
Since 1974, the U.S. government has formally committed to training highpotential, early-career scientists to carry out the nation‚Äôs biomedical research
agenda through congressionally mandated programs like the Ruth L. Kirschstein
National Research Service Award (NRSA). Subject to periodic review (National
Research Council 2011), large federally contracted studies have monitored the
outcomes of those who received the award (Pion 2001; Mantovani, Look, and
Wuerker 2006). Nevertheless, few studies have used more rigorous methods to
estimate the award‚Äôs unbiased impact on future career outcomes (Levitt 2010; Jacob
and Lefgren 2011).
In this study, we focus on the NRSA F32 postdoctoral training award for two
reasons. First, it allows us to capture scientists at the beginning of their career, when
evidence of their potential success is not yet fully developed, thereby providing the

12

strongest raw evidence of whether peer review can identify future successful
scientists. Second, fellowship awards are relatively inexpensive compared with
other major funding awards,3 and as individual (non-institutional) training awards,
they represent the federal government‚Äôs best method for directly shaping the future
generation of scientists. Both these facts drive NIH institutes and their leaders to
impose even more discretion than is used with other awards. They use a higher level
of discretion because the risks associated with making a selection error are
relatively low, and the gains from influencing the future direction of science are
potentially high. Leaving a lasting legacy influencing the next generation of
researchers and future direction of science is an admirable goal that most institute
leaders take seriously and are interested in pursuing. Altogether, this program gives
our analysis a perfect mix of grants awarded based on both discretion and peer
review.

ii. Evaluating the Validity of Regression Discontinuity
Numerous studies have used regression discontinuity design (RDD) to evaluate
the impact of scientific R&D funding (Jacob and Lefgren 2011a, 2011b; Grilli and
Murtinu 2011; Benavente et al. 2012; Li 2017; Howell 2017; Bol, de Vaan, and de
Rijt 2018; Azoulay et al. 2019). An RDD works best when the level of discretion
is minimal (as is the case with major independent-research grants like R01 awards)
and budgets are fixed ahead of time. However, with awards like fellowships, high

3

Individual training awards are generally around $60,000 each, whereas a standard R01
independent-research grant can run anywhere from five to ten times as much.

13

levels of staff discretion and congressionally mandated annual budgets that
distribute awards in multiple annual cycles dilute the appropriateness of an RDD.
With fellowships, a budget line is generated showing how many grants the
organization can fund in a particular council round within a particular year.
Theoretically, the organization then funds the ‚Äúbest‚Äù proposals in each council
round up to the point where it exhausts its budget. This scenario is seemingly
appropriate for an RDD, with which one can compare applicants who ‚Äújust‚Äù got
funding with those who ‚Äújust did not‚Äù get funding solely because of exogenous
factors (e.g., the money was exhausted through no fault or manipulation of the
applicant). These applicants would otherwise appear similar; therefore, following
the logic of RDD, one could reasonably conclude that any difference observed
between those funded and those not funded around a maximum budget level (called
a ‚Äúpay line‚Äù) is due solely to the effect of the award.
The reality, however, is that funding and award-making decisions are
complicated. What funding is available depends on the number of applicants in
each council round and previous rounds and whether everyone who was previously
offered a grant accepts. Institutions receive an annual budget but make grantfunding decisions by council round. Depending on the institute, there are anywhere
from two to four council rounds in a given year. If enough high-quality applicants
do not apply in council round one, the budget (and pay line) can be reduced
allowing for more applications to be funded in future council rounds within the
same year. The reverse is also true; budget offices may increase their budget (and
pay line) if there is a large pool of candidates and funds are available.

14

Another complication is the fluidity of funds across programs and the ability to
move allocations across scientific divisions. An institute must spend all the money
appropriated to it by Congress, so if applications in other programs are light in a
particular year, this could provide additional funds to increase a budget line in
another program. The fluidity of a budget line and the fact that it could be
influenced by the quality and quantity of applications violates the assumptions
required for a valid RDD and even those required for a fuzzy RDD where limited
discretion around the budget line takes place.

III. Data and Methods
A. The Data
Our analytical data include administrative records from the NIH‚Äôs Information
for Management, Planning, Analysis, and Coordination (IMPAC II) system from
1996 to 2008.4 The NIH matches its administrative records to data from the
National Science Foundation‚Äôs Survey of Earned Doctorates (SED), an annual
census of doctoral recipients from U.S. institutions. The NSF SED contains
information on individual demographics, characteristics of graduate study, and
future career plans. By linking these data sets, we are able to obtain missing data
and add additional individual-level covariates on our sample. We use demographic
variables before or at the point of PhD completion. These variables are extracted

4

NIH administrative data is part of the IMPAC II grants data system National Institutes of
Health, IMPAC II, http://era.nih.gov/. The data is restricted-use. Researchers interested in
replicating our study or accessing the data for research can submit a request to the National
Institutes of Health‚Äôs Office of Extramural Research.
.

15

from the SED and include age at PhD completion, gender, race and ethnicity,
marital status at PhD completion, PhD field of study, and type of doctorateeducation funding. We use the two data sources to construct one large panel data
set for analysis, limiting our data to those who applied for NRSA F32 funding
between 1996 and 2008, and then observe these individuals‚Äô future NIH awardapplication and funding patterns through 2015.
We include application-review score, funded or non-funded status, time frame,
the institute or center receiving applications or funding the award, and previous
grant-funding or training affiliations. We further queried IMPAC II for subsequent
applications for NIH funding and awards from these individuals. Similar to Jacob
and Lefgren (2011a), we define our outcome variables to identify research-award
application or receipt four or more years out from the individual‚Äôs application year.
Our control variables mimic those used in the Ginther et al. (2011) paper on
research awards and race. In particular, we include controls for race and ethnicity,
gender, marital status, age, degree, scientific field, and previous NIH training
experience.
Our analytical sample is a subset of all applicants. We drop applications that
are higher than the 60th percentile in each council round, because the NIH does not
consistently save scores for these applications in the reporting database, and
practically none of them get funding. Some institutes and centers have too few
applicants for our matching method, so for this reason, we drop applicants from
seven institutes and centers. Our final analytical sample contains 14,276
individuals.

16

B. Descriptive Statistics
We report descriptive statistics in Table 1. In our analytical sample, awardees
and non-awardees do not differ in terms of age at application, marital status, or
likelihood of having a prior T32 traineeship. Table 1 shows that awardees and nonawardees do differ across a number of observable characteristics. Awardees are
significantly less likely to be black or Hispanic. Individuals with MD degrees are
less likely to receive fellowship awards, whereas PhDs are more likely. Individuals
with biomedical or social-science degrees are more likely to receive fellowship
awards compared with those without a reported PhD field. Awardees are
significantly more likely to aspire to and receive subsequent NIH funding, as
measured by the number of Research Proposal Grant (RPG) applications and
awards, the probability of an RPG award, and the probability of an R01 award. As
expected, awardees have significantly lower (better) scores on their last observed
application.
In Table A1 in the Appendix, we estimate the probability of receiving an NIH
F32 award as a function of observable characteristics for the full sample and the
analysis sample.

C.

The NRSA and Discretionary Decision-Making

Using the NRSA F32 postdoctoral training fellowship program, we demonstrate
the problems with RDD for federal-grant awards where discretion takes place.
Figure 1 is an illustration using all applicants from 1996 to 2008. Panels A‚ÄìC

17

provide three typical institute-level examples of variation in processing the
awarding of fellowships around a fictitious budget line similar to what is generated
by a budget office each council round. Each panel shows a point for each F32
applicant. The points at the top signify applicants who received an award. The
points along the bottom represent applicants who applied but did not receive an
award. The x-axis ranks the applicants by peer-reviewed priority score from best to
worst. In each panel, a vertical line represents a pseudo‚Äìpay line5 imposed by a
budget office suggesting to program officers and staff how many awards can be
funded in that particular council round. Points at the top and left of the vertical line
represent awards funded in order of peer-review scores. Points at the bottom and
right of the vertical line are non-funded applicants in order of peer-review scores,
meaning that given budget constraints, their scores fell outside of the range of
feasible acceptance. Points at the top and to the right of the vertical line represent
awards that were funded out of order. For each of these points, at the bottom and
left of the vertical line, there is an equal point representing an applicant who had a
priority score low (good) enough to be funded, but whom staff and institute
discretion skipped over in order to fund an applicant in the top and to the right of
the budget line.
If all funding were awarded based solely on the ranked order of priority scores,
we would observe Panel A, and no discretion would creep into the award process.
Panel A distributes awards based solely on the peer-reviewed score, allocated from

5

A pay line is the end point of the budget, where all resources have been exhausted. If there are
10 applicants with scores ranging from 1 to 10, and the budget allows for the funding of 3 applicants,
then the pay line is a score of 3. Anyone with a score lower than or equal to 3 would get funded if
only peer review was followed in decision-making.

18

best score to worst score until the institute budget allocations for the fellowship are
exhausted. If this were the case (and budgets were not fluid or exchangeable), a
sharp RDD would be valid.
Panel B of Figure 1 illustrates an institute following the guidance of the peerreviewed score for the best (lowest) scores. However, once the institute has funded
a majority of applicants with meritorious scores, it uses discretion to distribute
awards near the pay line. In this case, the institute is comfortable using discretion
near the pay line to fund applications that best fit within its scientific priorities and
where the institute staff believes the applicant has the best-case scenario for future
success‚Äîperhaps because they consider the applicants to be more or less similar in
quality. If this were the case (and budget lines were not fluid), a fuzzy RDD would
be appropriate.
A third, more complicated case is Panel C. It demonstrates the most complex
case of selection for fellowship awards. About two-thirds of proposals funded
would be below the expected pay line if the institute were to fund based solely on
peer-reviewed scores. Institutes represented here use a significant amount of
discretion when selecting proposals for funding and, because of this, no real cutoff
exists. For our purposes, a very valid question exists as to the frequency with which
institutes engage in this third scenario and, to some extent, the second scenario as
well.
We examine this question in Figure 1, Panel D. Since no real data are available
for pay lines, we construct pseudo‚Äìpay lines by counting the number of awards
funded and assuming that for each institute this number is equivalent to the pay line

19

for that council round.6 Each diamond represents the percent of applications funded
within the pseudo-constructed pay line (e.g., the top and left of the budget line;
funded in order of peer reviewed score) by year, institute, and council round. If an
institute has three council rounds in one year, they will have three diamonds
represented vertically for that year. In Panel D, the black, horizontal curved line
represents an average yearly rate of funding strictly by peer-reviewed score among
all institute-council rounds in that year. Between 1996 and 2002, overall NIH
institute-council rounds funded approximately 40 percent of proposals in order
based solely on the rank of scores. In other words, over half the time, institutes were
reaching for applicants below the pay line within a council round and, equivalently,
skipping a proportion of applicants above said pay line.
After the NIH doubling,7 the rate of institute-council rounds funding solely in
order dropped to a low of 28 percent in 2006 and increased to 35 percent by 2008.
This finding suggests that when institutes have fewer resources, they use less
discretion.
Between two-thirds to three-fourths of all institute-council rounds used
discretion in award allocation in recent years. Although review scores assigned
during study section are an important criterion in the selection of awardees, they
are clearly not the only criteria. Over the entire period, only 37.5 percent of year,
institute, and council round (YIC) units (N=701) complied with a sharp RDD

6
This in and of itself is a false presumption because it does not take into account budget-office
discretion in adding or reducing slots based on the applicant pool. However, we hold judgement on
that piece in order to demonstrate our general point here.
7
The NIH doubling occurred between 1998 and 2002. It was a period of 5 years during which
the total federal budget allocated to the NIH doubled in size.

20

framework (ignoring the issue of budget fluidity). Not only were there few YIC
units in compliance, but those that did comply had few applicants. Only 10.7
percent of individual applications were in YIC units that complied (data not shown).
This result is worth emphasizing. Only one in ten applicants experienced a review
process where peer review scores were strictly followed‚Äîthe rest experience a twostage process infused with institutional discretion. Extending to a fuzzy RDD,8
around 61.5 percent of YIC units complied, translating into 47.9 percent of
applicants in our sample (data not shown). Even under a fuzzy RDD, less than half
of all applicants experienced a council round that met the criteria for some form of
RDD.
In fact, institutes vary widely in how they implement the two-stage process. At
one extreme, take the National Institute of General Medical Sciences, which
explicitly delineates its two-stage process and discretionary actions. In a recent
report on application and funding trends, the institute stated that ‚Äúwe do not use a
strict percentile cut off (‚Äòpay line‚Äô) to make funding decisions. Instead, we take a
variety of factors into account, including peer review scores, summary statements,
Institute priorities, overall portfolio diversity, and an applicant‚Äôs other research
support‚Äù (Hechtman and Lorsch 2019).
We examine whether discretion varies by council round and year. If discretion
depends solely on budgets, we would expect it to depend on the budget remaining
in a given fiscal year. Thus, we examine whether the probability of reaching for or

8

Here we assume that a YIC unit fits a fuzzy RDD if less than 10 percent of cases either skip a
good review score or reach for a worse review score.

21

skipping a proposal depends on the timing of the council round (three per fiscal
year) after control for institute and fiscal-year dummies. It could be that if fewer
funds were systematically spent in the first council, then the amount of discretion
would increase in the following two rounds. In Table 2, we show that there is not
a statistically significant difference in the probability of discretion as a function of
council round measured by reaching or skipping proposals. However, in times of
tight budgets (after the NIH doubling ended in 2002), the probability of a proposal‚Äôs
being funded by discretion (reached) falls. As funding got tighter in the late 2000s,
the probability of skipping proposals with good scores also fell. As with Figure 1,
discretion falls when budgets become tight.
Figure 1 and Table 2 indicate that discretion is widely used in the allocation of
the NRSA F32 fellowship. Since other factors besides review score drive the
decision-making process (and pay line) in fellowship awards, the often-used RDD
approach is not a valid methodology. We use more appropriate matching methods
for our analysis. More importantly, however, since all applications in our analytical
subsample have a review score (details described below), we can take advantage of
the heavy use of discretion in the NRSA F32 postdoctoral training awards to
evaluate whether discretion or peer review more often predicts future scientific
success.

D. Using Matching to Identify a Causal Effect
As described, the multi-step selection process for grants first generates a review
score via a systematic peer review process. Given this step and the fact that the

22

groups of individuals applying to the awards are relatively homogenous, we use
matching techniques. While any unobserved characteristics differing between
funded and unfunded applicants could confound our results, we argue that matching
is a feasible approach for the following reasons. First, we can account for
unobserved differences by institute and council round by controlling for these
factors. Additionally, selection is made at the institute level. Any unobserved
differences among applicants are unobserved by the institute also and therefore not
a driving component of the selection process. There is no self-selection of
fellowship award offerings. Finally, the groups of individuals that apply for funding
are relatively homogeneous within each institute. All of these people clearly excel
in academics, have been encouraged to apply by their mentors (which means their
mentors believe they have a chance of getting the award), and are typically
intensely interested in biomedical research. If this unobservable variation does
exist, we argue that it is minimal within this select group of applicants.
To estimate the causal effect of fellowship awards on subsequent NIH funding
outcomes, we use the potential-outcomes framework employed in econometric
analysis (Rubin 2004). To fix ideas, let ùëáùëñ = 1

be the treatment when an

individual‚Äôs fellowship application is funded, and let ùëáùëñ = 0 if the application is not
funded. Each individual has two potential outcomes of subsequent NIH funding:
ùëåùëñ (1) if the individual receives the award treatment and ùëåùëñ (0) if the individual is
not treated. For each individual, the causal effect of the award on subsequent NIH
funding is defined as the difference in potential outcomes ùëåùëñ (1) ‚àí ùëåùëñ (0). However,

23

each individual is observed only when they do or do not receive the award, and, in
this case, we must estimate the counterfactual outcome using matching methods.
In order to implement matching methods, we assume that treatment is
independent of the outcome conditional on covariates ùëáùëñ ‚î¥(ùëåùëñ (0), ùëåùëñ (1))|ùëãùëñ . This is
the unconfoundedness assumption, which means that the treatment is conditionally
independent of the outcome after conditioning on observable characteristics. Given
unconfoundedness, we can define the average treatment effect in terms of potential
outcomes as the expected value of potential outcomes:
ùê¥ùëáùê∏ = ùê∏[ùëåùëñ (1) ‚àí ùëåùëñ (0)].
We use two matching methods to identify the ATE. First, we employ
propensity-score matching, defining the propensity score as the probability of
receiving treatment conditional on observed characteristics ùëí(ùëã) = Pr‚Å°(ùëáùëñ =
1|ùëãùëñ = ùë•). In order to implement propensity-score methods, the propensity scores
for the treated and untreated in our sample must overlap such that 0 < ùëí(ùë•) < 1.
Although the unconfoundedness assumption cannot be directly tested, we can
examine whether the propensity score has a causal effect on a pseudo-outcome that
was determined prior to the treatment. If the estimated effect of the treatment on
the pseudo-outcome is significant, then unconfoundedness has likely been violated
(Imbens 2015).
Propensity-score matching has been widely used in economics and other social
sciences (Imbens 2015). However, King and Nielson (2019) and Imbens (2015)
note that propensity-score estimates break down if the propensity-score model fits
to the data too well. As a result, we cannot use the review score to estimate the

24

propensity score related to fellowship funding. Thus, we use the coarsened exact
matching (CEM) algorithm (Blackwell et al. 2009) to improve the balance of the
data and nearest-neighbor methods to facilitate matching on the review score. We
use propensity-score matching, and as a robustness check, we also use nearestneighbor matching after reducing the data using the CEM algorithm (results in the
Appendix).
In addition, given that we have information on a pseudo-constructed pay line
(the total number of funded applicants in each council round), review score, and
award, we construct four indicator categories: those funded and within a pseudoconstructed pay line based on review score (as expected), those not funded and
outside a pseudo-constructed pay line based on review score (as expected), those
not funded but within a pseudo-constructed pay line based on review score
(skipped), and those funded outside a pseudo-constructed pay line based on review
score (reached). We then use our matching methods to examine treatment effects
by comparing outcomes. In particular, we compare outcomes for those who were
(1) reached compared to not funded as expected; (2) funded as expected compared
to reached; (3) reached compared to skipped; (4) skipped compared to not funded
as expected; (5) skipped compared to those funded in order; and (6) funded as
expected compared to those not funded as expected.

IV. Results of Peer Review versus Discretion
Table 3 reports the Average Treatment Effect (ATE) Propensity Score
Matching (PSM) estimates for the analytical sample. If we include the review score

25

in the propensity-score estimates, the propensity score becomes too precise, and the
matching algorithm breaks down (King and Nielson 2019; Imbens 2015). Thus, our
propensity-score estimates include institute and council round fixed effects and the
covariates listed in column 1 of Table A1. Each column of Table 3 shows outcome
variables: the number of research program grant (RPG) awards, the number of RPG
applications, the probability of an RPG, the probability of an R01 and the
probability of never applying for additional funding.
We identify four major categories of funding status based on scores and institute
behavior: funded in order, skipped, reached, and not funded in order. Using the
review score as a measure of the proposal‚Äôs scientific merit, those proposals funded
in order were applications judged as the most meritorious by the reviewers and
institute staff. Some proposals with favorable review scores within the pseudoconstructed pay line (budget) were skipped in favor of proposals with worse review
scores. In this case, the institute reached to fund a proposal out of the review score
order. Proposals that were not funded in order had review scores in excess of the
budget pay line and worse scores than those that were funded in order or skipped.
In Table 3, the first row compares the treated proposals that were reached
compared with those that were not funded. Compared with those not funded,
reached scientists secured around 0.17 more independent-research awards and had
around 0.80 more applications in future years, more than the full sample (results
not shown; see Heggeness et al. 2018 for a detailed analysis of the full sample).
More importantly, they were between 7 to 8 percent more likely to receive
independent awards conditional on applying, and they were 11.9 percent less likely

26

to never apply for future independent awards. These estimates were slightly higher
than those for the full analysis sample (see Heggeness et al. 2018).
Next, we compare proposals where the treatment was reached and had relatively
higher (worse) review scores compared with those that were funded in order. The
ATE estimates indicate that the subsequent NIH funding outcomes for reached
proposals were significantly worse than those funded in order. Reached proposals
received 0.096 fewer RPG awards, submitted 0.5 fewer RPG applications, had a
5.0 ppt lower probability of receiving an RPG award, had a 3.8 ppt lower
probability of receiving an R01 award, and had a 6.3 ppt probability of never
applying for additional NIH funding. These results suggest that those reached
individuals do not perform as well as those who were funded in order.
We then compare the reached proposals that received fellowship funding with
the skipped proposals that had better scores but were not funded. Reached proposals
received .18 fewer RPG awards than skipped proposals, had a 5.1 ppt lower
probability of receiving an RPG award, and had a 4.7 ppt lower probability of
receiving an NIH award. Both skipped and not funded in order proposals did not
receive the fellowship award. However, the skipped proposals were judged to have
better scientific merit than the not funded in order proposals. On average, the
skipped investigators have much better outcomes than the not funded in order
proposals for all outcomes. In other words, discretion results in the selection of
lesser-quality, less-productive scientists.
What is the opportunity cost of skipping meritorious proposals relative to
comparable proposals that were found to be the best during the review process?

27

We find that skipped proposals submitted .342 fewer RPG applications and, as a
result, had a 5.1 ppt lower probability of receiving an RPG compared with proposals
funded in order. Skipped proposals were 6.9 ppt more likely to never submit a
subsequent NIH application. Not getting the award hindered the future success of
the skipped applicants, making them less productive. Even though skipped did not
do as well as funded in order, they still did better than not funded in order. In other
words, those skipped still thrived.
The last row of Table 3 compares the best proposals funded in order with those
not funded in order. The estimated ATEs are between 10 to 41 percent larger for
the reached compared with not funded in order. Compared with those not funded
in order, four years out reached, skipped, and funded in order all do relatively better
in achieving an independent R01 award: 7.8 ppt, 8.3 ppt, and 8.6 ppt, respectively.
As a robustness check, we estimated these models using the coarsened exact
matching (CEM) algorithm and nearest-neighbor matching in Appendix Table A2.
The signs, magnitudes, and statistical significance of the estimated effects are
similar to those found in Table 3.
Although we cannot test the unconfoundedness assumption directly, Imbens
(2015) recommends using propensity score matching (PSM) on pseudo-outcomes
that occur before the award treatment. Given the SED data, we evaluate whether
the fellowship award predicts the probability that an applicant has a PhD degree,
the applicant‚Äôs highest degree is in biomedicine, and the applicant‚Äôs doctoral
funding was from a fellowship or scholarship. Table 4 presents these results and
finds that the fellowship award has no significant impact on these pseudo-

28

outcomes. These results indicate that the unconfoundedness assumption is not
violated.

V. Discussion
Our findings provide evidence that within a pool of young, ambitious scientists,
peer review more accurately identifies scientific ‚Äúdiamonds in the rough‚Äù than NIHinstitute discretion. Our results indicate that scientific leaders and policymakers do
in fact have a choice to make. If they want to fund in the most efficient way possible
the expansion of the scientific frontier, they should encourage peer review and fund
awards without discretion until funding is exhausted. These results echo findings
by Li and Agha (2015) and Gallo et al. (2014), who found that evaluation scores
are correlated with subsequent research publications, indicating that the peerreview process is efficient. While scientific staff members of federal agencies do
their best to keep scientific innovation moving, our results hint that the priorities of
program officers and institutes may come at a cost to the scientific enterprise in
terms of advancing the best science.
Interestingly, our findings also indicate that for those with competitive
applications but no funding (skipped), applying for the award favors future research
success even if they do not receive the award. These results are consistent with
those found by Ayoubi, Pezzoni, and Visentin (2019), who demonstrate that
applying for research funding in a Swiss grant competition increased publications
regardless of whether funding was obtained. For those on the margin (reached),
however, our results show that the fellowship award can have an impact on keeping

29

these young scientists engaged in science. For scientific organizations looking to
retain talent of a particular nature, reaching for that talent does increase their ability
to stay engaged.

VI. Conclusion
Our results have implications for the debate on the validity of the peer review
process. Fang, Bowen and Casadevall (2016) and Pier et al. (2018) argued that peer
review cannot distinguish between proposals of comparable quality. However, the
fact that applicants with skipped proposals are more likely than those with reached
proposals to receive subsequent NIH funding suggests that the peer-review process
can identify small differences in research-proposal quality. Our results also indicate
that review scores are a good predictor of subsequent NIH applications and awards
and an efficient way to allocate research funding given an alternative option of
discretion.
We described in detail why regression discontinuity design (RDD) is not
appropriate and should not be used for studying the impact of scientific funding
when discretionary decisions overriding peer-review rankings are common. The
method of evaluation must fit with the idiosyncrasies of the setting. With matching
methods, we found that the NRSA F32 award keeps postdoctoral researchers
engaged in NIH-funded science at higher rates than they would have otherwise
experienced. Regardless of the restrictions or matching methods we used, our
estimates are robust. Overall, we have demonstrated the value of a peer-review
system in selecting the best talent compared with discretionary decisions. We

30

conclude by noting that even though it has been under intense scrutiny throughout
its existence and has rarely been used in its purest form, peer review is an efficient
option compared with institutional discretion if the goal is to maximize the
advancement of the frontier of science.
What about other methods? Fang and Casadevall (2016) and others have argued
for a two-step lottery, one where a subset of highly qualified applications are
selected by peer review and then funded applicants are selected by lottery. Smith
(2006) argues that peer review is basically equivalent to chance because reviewers
differ, and selection decisions are random based on who one happens to get as a
reviewer and study section.
Perhaps there is an argument for randomization after a preliminary peer review.
If it produces results similar to peer review, cost efficiencies could be realized by
randomizing funding decisions. Greenberg (2008) highlights this point and states
that ‚Äúreliance on chance wouldn‚Äôt be inferior to what‚Äôs happening now, which, as
it turns out, is a game of chance in the guise of informed selection. Moreover, the
[cost] savings from a lottery could be recirculated to research, providing many
millions of dollars for projects that would otherwise go unsupported.‚Äù
There is one additional reason why allocating funding via a lottery after initial
peer review is a potentially wise managerial decision of federal scientific leaders.
It would actually allow policy researchers to rigorously test the true effect of federal
funding on both the system and scientific discovery. Randomized control trials are
common in science. Everyone understands why they are needed, yet scientists still
struggle to accept this simple, highly valued method as a mechanism to study their

31

own productivity and innovation. As Smith (2006), who also noted little difference
between peer review and randomization, stated when discussing challenges to
incorporate randomization, ‚ÄúPeer review is . . . likely to remain central to science
and journals because there is no obvious alternative, and scientists and editors have
a continuing belief in peer review . . . how odd that science should be rooted in
belief.‚Äù
While scientific leaders (and scientists) may be concerned by the random
assignment of funding beyond a certain threshold, perhaps they would be open to
running an experiment along those lines. Such an experiment would begin with
traditional peer review, with study sections then randomly divided into three
groups. Group one would allow unconstrained discretion by the part of staff. Group
two would allow constrained discretion‚Äîwhere rules governing discretion would
be documented and include an audit trail detailing deviations. The third group
would randomize funding selection of those applications receiving the best peerreview scores that meet a certain threshold. This kind of experiment is already
taking place in New Zealand. The Health Research Council allocates two percent
of its budget for ‚Äúexplorer grants‚Äù that provide approximately $100,000 in funding.
Short proposals are screened for eligibility and then funded at random until the
budget is exhausted. A recent study reported that those who were funded by the
lottery supported this system, however, it did not evaluate the impact of receiving
funding on scientific output (Liu et al. 2020).
Understanding what we are trying to optimize with federal scientific funding is
key. This type of experiment would allow us to understand two important factors.

32

First, when discretionary decisions are made specifically to achieve alternative
institutional goals, are those goals met? Second, if we automate decision-making to
a lottery, do outcomes such as publications and subsequent grants increase,
decrease, or stay the same? If outcomes stay the same or increase, then perhaps
using a lottery mechanism could, in fact, be more efficient, in the sense that
resources currently allocated to discretionary decisions could be reallocated to other
priorities. Regardless, it is clear that if we really care about figuring out the best
way to allocate the limited funding available from the federal government for
scientific advancements, we need to have access to application data from federal
funding agencies and, with a clear understanding of the role discretion plays in
research awards, evaluate the outcomes from funding.

33

REFERENCES

Alberts, Bruce, Marc W. Kirschner, Shirley Tilghman, and Harold Varmus. 2015.
‚ÄúOpinion: Addressing Systemic Problems in the Biomedical Research
Enterprise.‚Äù PNAS 112 (7): 1912‚Äì1913. doi: 10.1073/pnas.1500969112.
Ayoubi, Charles, Michele Pezzoni, and Fabiana Visentin. 2019. ‚ÄúThe Important
Thing Is Not to Win, It Is to Take Part: What If Scientists Benefit from
Participating in Research Grant Competitions?‚Äù Research Policy 48 (1):
84‚Äì97. doi: 10.1016/j.respol.2018.07.021.
Azoulay, Pierre, Joshua S. Graff Zivin, and Gustavo Manso. 2013. ‚ÄúNational
Institutes of Health Peer Review: Challenges and Avenues for Reform.‚Äù
Innovation Policy and the Economy 13: 1‚Äì22.
Azoulay, Pierre, Joshua S. Graff Zivin, Danielle Li, and Bhaven N. Sampat. 2019.
‚ÄúPublic R&D Investments and Private-Sector Patenting: Evidence from
NIH Funding Rules.‚Äù Review of Economic Studies 86 (1): 117‚Äì152.
Benos, Dale J., Edlira Bashari, Jose M. Chaves, Amit Gaggar, Niren Kapoor,
Martin LaFrance, Robert Mans, David Mayhew, Sara McGowan, Abigail
Polter, Yawar Qadri, Shanta Safare, Kevin Schultz, Ryan Splittgerber,
Jason Stephenson, Cristy Tower, R. Grace Walton, and Alexander Zotov.
2007. ‚ÄúThe Ups and Downs of Peer Review.‚Äù Advances in Physiology
Education 31 (2): 145‚Äì152.
Baldwin, Melinda. 2018. ‚ÄúScientific Autonomy, Public Accountability, and the
Rise of ‚ÄòPeer Review‚Äô in the Cold War United States.‚Äù Isis 109 (3): 538‚Äì
558.
Benavente, Jos√© Miguel, Gustavo Crespi, Lucas Figal Garone, and Alessandro
Maffioli. 2012. ‚ÄúThe Impact of National Research Funds: A Regression
Discontinuity Approach to the Chilean FONDECYT.‚Äù Research Policy 41
(8): 1461‚Äì1475.
Blackwell, Matthew, Stefano Iacus, Gary King, and Giuseppe Porro. 2009. ‚ÄúCEM:
Coarsened Exact Matching in Stata.‚Äù The Stata Journal 9 (4): 524‚Äì546.

34

Blau, David M., and Bruce A. Weinberg. 2017. ‚ÄúWhy the US Science and
Engineering Workforce is Aging Rapidly.‚Äù PNAS 114 (15): 3879‚Äì3884.
Bol, Thijs, Mathijs de Vaan, and Arnout van de Rijt. 2018. ‚ÄúThe Matthew Effect in
Science Funding.‚Äù PNAS 115 (19): 4887‚Äì4890.
Costello, Leslie C. 2010. ‚ÄúPerspective: Is NIH Funding the ‚ÄòBest Science by the
Best Scientists?‚Äô A Critique of the NIH R01 Research Grant Review
Policies. Academic Medicine 85 (5): 775‚Äì779.
Dawes, Robyn M., David Faust, and Paul E. Meehl. 1989. ‚ÄúClinical versus
Actuarial Judgment.‚Äù Science 243: 1668‚Äì1674.
Fang, Ferric C., and Arturo Casadevall. 2016. ‚ÄúResearch Funding: The Case for a
Modified Lottery.‚Äù mBio 7 (2): e00422-16.
Fang, Ferric C., Anthony Bowen, and Arturo Casadevall. 2016. ‚ÄúNIH Peer Review
Percentile Scores Are Poorly Predictive of Grant Productivity.‚Äù eLife 5:
e13323.
Farrell, P.R., Magida L. Farrell, and M.K. Farrell. 2017. ‚ÄúAncient Texts to PubMed:
A Brief History of the Peer-Review Process.‚Äù Journal of Perinatology, 37:
13‚Äì15.
Gallo, Stephen A., Afton S. Carpenter, David Irwin, Caitlin D. McPartland, Joseph
Travis, Sofie Reynders, Lisa A. Thompson, and Scott R. Glisson. 2014.
‚ÄúThe Validation of Peer Review through Research Impact Measures and the
Implications for Funding Strategies.‚Äù PLoS One 9 (9): e106474.
Gallo, Stephen A., Joanne H. Sullivan, and Scott R. Glisson. 2016. ‚ÄúThe Influence
of Peer Reviewer Expertise on the Evaluation of Research Funding
Applications.‚Äù PLoS One 11 (10): e0165147.
Ginther, Donna K., Walter T. Schaffer, Joshua Schnell, Beth Masimore, Faye Liu,
Laurel L. Haak, and Raynard Kington. 2011. ‚ÄúRace, Ethnicity, and NIH
Research Awards.‚Äù Science 333 (6045): 1015‚Äì1019.
Godlee, Fiona, Catharine R. Gale, Christopher N. Martyn. 1998. ‚ÄúEffect on the
Quality of Peer Review of Blinding Reviewers and Asking Them to Sign
Their Reports: A Randomized Control Trial.‚Äù JAMA 280 (3): 237‚Äì240.

35

Goldstein, Anna P., and Michael Kearney. 2018. ‚ÄúUncertainty and Individual
Discretion in Allocating Research Funds.‚Äù (February 28). Available at
SSRN:

https://ssrn.com/abstract=3012169

or

http://dx.doi.org/10.2139/ssrn.3012169.
Greenberg, Dan. 2008. ‚ÄúPeer Review at NIH: A Lottery Would Be Better.‚Äù
Brainstorm

(blog),

Chronicle

of

Higher

Education.

http://chronicle.com/blogs/brainstorm/peer-review-at-nih-a-lottery-wouldbe-better/5696.
Grilli, Luca, and Samuele Murtinu. 2011. ‚ÄúEconometric Evaluation of Public
Policies for Science and Innovation: A Brief Guide to Practice.‚Äù In Science
and Innovation Policy for the New Knowledge Economy, edited by
Massimo G. Colombo, Luca Grilli, Lucia Piscitello, and Cristina RossiLamastra. Northampton, MA: Edward Elgar Publishing.
Gustafson, Thane. 1975. ‚ÄúThe Controversy over Peer Review.‚Äù Science 190 (4219):
1060-1066.
Hechtman, Lisa, and Jon Lorsch. 2019. ‚ÄúApplication and Funding Trends in Fiscal
Year 2018.‚Äù National Institute of General Medical Sciences Feedback Loop
Blog,

National

Institute

of

General

Medical

Sciences

.

https://loop.nigms.nih.gov/2019/04/application-and-funding-trends-infiscal-year-2018/.
Heggeness, Misty L., Kearney T. Gunsalus, Jose Pacas, and Gary S. McDowell.
2016. ‚ÄúPreparing for the 21st Century Biomedical Research Job Market:
Using Census Data to Inform Policy and Career Decision-Making.‚Äù SJS
Working Paper, http://www.sjscience.org/article?id=570.
Heggeness, Misty L., Kearney T. Gunsalus, Jose Pacas, and Gary S. McDowell.
2017. ‚ÄúThe New Face of Science.‚Äù Nature 541: 21‚Äì23.
Heggeness, Misty L., Donna K. Ginther, Maria I. Larenas, and Frances D. CarterJohnson. 2018. ‚ÄúThe Impact of Postdoctoral Fellowships on a Future
Independent Career in Federally Funded Biomedical Research.‚Äù NBER
Working Paper No. 24508.

36

Hoffman, Mitchell, Lisa B. Kahn, and Danielle Li. 2018. ‚ÄúDiscretion in Hiring.‚Äù
The Quarterly Journal of Economics 133 (2): 765‚Äì800.
Howell, Sabrina T. 2017. ‚ÄúFinancing Innovation: Evidence from R&D Grants.‚Äù
American Economic Review 107 (4): 1136‚Äì1164.
Imbens, Guido W. 2015. ‚ÄúMatching Methods in Practice: Three Examples.‚Äù
Journal of Human Resources 50 (2): 373‚Äì419.
Jacob, Brian A., and Lars Lefgren. 2011a. ‚ÄúThe Impact of NIH Postdoctoral
Training Grants on Scientific Productivity.‚Äù Research Policy 40 (6): 864‚Äì
874. doi: 10.1016/j.respol.2011.04.003.
Jacob, Brian A., and Lars Lefgren. 2011b. ‚ÄúThe Impact of Research Grant Funding
on Scientific Productivity.‚Äù Journal of Public Economics 95: 1168‚Äì1177.
King, Gary, and Richard Nielson. 2019. ‚ÄúWhy Propensity Scores Should Not Be
Used

for

Matching.‚Äù

Political

Analysis

27

(4).

Copy

at

http://j.mp/2ovYGsW. Accessed on January 11, 2019.
Kleinberg, Jon, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil
Mullainathan. 2018. ‚ÄúHuman Decisions and Machine Predictions.‚Äù The
Quarterly Journal of Economics 133 (1): 237‚Äì293.
Levitt, David G. 2010. ‚ÄúCareers of An Elite Cohort of U.S. Basic Life Science
Postdoctoral Fellows and the Influence of Their Mentor‚Äôs Citation Record.‚Äù
BMC Medical Education 10, article number 80. doi: 10.1186/1472-692010-80.
Li, Danielle, and Leila Agha. 2015. ‚ÄúBig Names or Big Ideas: Do Peer-Review
Panels Select the Best Science Proposals?‚Äù Science 348 (6233): 434‚Äì438.
doi: 10.1126/science.aaa0185.
Li, Danielle. 2017. ‚ÄúExpertise versus Bias in Evaluation: Evidence from the
NIH.‚Äù American Economic Journal: Applied Economics 9 (2): 60‚Äì-92.
doi: 10.1257/app.20150421.
Liu, M., Choy, V., Clarke, P., Barnett, A., Blakely T., and Pomeroy L. 2020.
‚ÄúThe Acceptability of Using a Lottery to Allocate Research Funding: A
Survey of Applicants.‚Äù Research Integrity and Peer Review (5):3
https://doi.org/10.1186/s41073-019-0089-z.
37

Mantovani, Richard, Mary V. Look, and Emily Wuerker. 2006. The Career
Achievements of National Research Service Award Postdoctoral Trainees
and Fellows: 1975‚Äì2004. Bethesda, MD: ORC Macro.
Marsh, Herbert W., Upali W. Jayasinghe, and Nigel W. Bond. 2008. ‚ÄúImproving
the Peer-Review Process for Grant Applications: Reliability, Validity, Bias,
and Generalizability.‚Äù American Psychologist 63 (3): 160‚Äì168.
McNutt, Robert A., Arthur T. Evans, Robert H. Fletcher, and Suzanne W. Fletcher.
1990. ‚ÄúThe Effects of Blinding on the Quality of Peer Review.‚Äù JAMA 263
(10): 1371‚Äì1376.
National Institutes of Health. 2011. ‚ÄúEnhancing peer review.‚Äù Retrieved May 9,
2017. https://enhancing-peer-review.nih.gov/index.html.
National Research Council. 2011. Research Training in the Biomedical,
Behavioral, and Clinical Research Sciences. Committee to Study the
National Needs for Biomedical, Behavioral, and Clinical Research.
Washington, DC: National Academies Press.
Pier, Elizabeth L., Markus Brauer, Amarette Filut, Anna Kaatz, Joshua Raclaw,
Michell J. Nathan, Cecilia E. Ford, and Molly Carnes. 2018. ‚ÄúLow
Agreement among Reviewers Evaluating the Same NIH Grant
Applications.‚Äù PNAS 115 (12): 2952‚Äì2957.
Pion, Georgine M. 2001. The Early Career Progress of NRSA Predoctoral Trainees
and Fellows. Prepared for U.S. Department of Health and Human Services,
National Institutes of Health, NIH Publication No. 00-4900.
Roy, Rustum. 1985. ‚ÄúFunding Science: The Real Defects of Peer Review and an
Alternative to It.‚Äù Science, Technology, & Human Values 10 (3): 73‚Äì81.
Scarpa, Toni. 2006. ‚ÄúPeer Review at NIH.‚Äù Science 311: 41.
Smith, Jeffrey, and Petra Todd. 2005. ‚ÄúDoes Matching Overcome LaLonde‚Äôs
Critique of Nonexperimental Estimators?‚Äù Journal of Econometrics 125 (1‚Äì
2): 305‚Äì353.
Smith, Richard. 2006. ‚ÄúPeer Review: A Flawed Process at the Heart of Science and
Journals.‚Äù Journal of the Royal Society of Medicine 99: 178‚Äì182.

38

Stahel, Philip, and Ernest E. Moore. 2014. ‚ÄúPeer Review for Biomedical
Publications: We Can Improve the System.‚Äù BMC Medicine 12: 179‚Äì182.
Tilghman, Shirley, and Sally Rockey. 2012. ‚ÄúReport of the Biomedical Research
Workforce Working Group of the Advisory Committee to the NIH
Director.‚Äù

Washington,

DC:

National

Institutes

of

Health.

http://acd.od.nih.gov/bwf.htm. Accessed 15 August 2014.
Travis, G. D. L., and H. M. Collins. 1991. ‚ÄúNew Light on Old Boys: Cognitive and
Institutional Particularism in the Peer Review System.‚Äù Science,
Technology, & Human Values 16 (3): 322‚Äì-341.
Wessely, Simon. 1998. ‚ÄúPeer Review of Grant Applications: What Do We Know?‚Äù
Lancet 352: 301‚Äì305. doi: 10.1016/S0140-6736(97)11129-1.

39

Panel B: Fuzzy RD

Pay Line

Pay Line

Panel A.

Pay Line

Priority Score (Lowest to Highest)

Priority Score (Lowest to Highest)
Note: For Institute A and Council Round B. Source: NIH IMPACII administrative records

Panel C: None RD

NRSA F32 Award Status

NRSA F32 Award Status

NRSA F32 Award Status

Panel A: Sharp RD

Priority Score (Lowest to Highest)

Note: For Institute C and Council Round D. Source: NIH IMPACII administrative records

Note: For Institute E and Council Round F. Source: NIH IMPACII administrative records

Panel B.

Panel C.

100
90
80
70
60
50
40
30
20
10
0
1996

1998

2000

2002

2004

2006

2008

Panel D. Percent of NRSA F32 Applications Funded within Pay Line by Year, Institute, and Council Round, 1996 to 2008

FIGURE 1. THE NRSA F32 SELECTION PROCESS
Note: Figure excludes council rounds with an N<20.
Source: Authors‚Äô calculations. National Institutes of Health IMPACII administrative records.

40

TABLE 1. DESCRIPTIVE STATISTICS OF APPLICANTS BY FUNDING STATUS, ANALYTICAL SAMPLE, 1996‚Äì
2008
All
F32 awarded
No F32 awarded
t-test
p-value
Review score
220.67
162.244
193.68
60.22
0.000
(75.291)
(26.899)
(34.432)
DEMOGRAPHICS
Age at application
31.201
30.942
31.078
1.12
0.261
(7.629)
(6.775)
(7.115)
Age at application missing
0.042
0.034
0.037
0.98
0.328
(0.200)
(0.181)
(0.189)
Married at application
0.381
0.396
0.38
-1.84
0.066
(0.486)
(0.489)
(0.485)
Married at application missing
0.205
0.182
0.186
0.68
0.494
(0.404)
(0.386)
(0.389)
Female
0.417
0.412
0.424
1.30
0.194
(0.493)
(0.492)
(0.494)
Sex missing
0.051
0.038
0.055
4.90
0.000
(0.219)
(0.190)
(0.228)
White, non-Hispanic
0.344
0.386
0.319
-7.93
0.000
(0.475)
(0.487)
(0.466)
Black, non-Hispanic
0.009
0.006
0.01
2.64
0.008
(0.095)
(0.075)
(0.098)
Asian, non-Hispanic
0.086
0.085
0.079
-1.29
0.196
(0.280)
(0.279)
(0.269)
Other, non-Hispanic
0.002
0.002
0.002
-0.33
0.744
(0.048)
(0.050)
(0.047)
Hispanic
0.032
0.029
0.029
-0.07
0.946
(0.175)
(0.168)
(0.167)
Race missing
0.544
0.509
0.579
7.97
0.000
(0.498)
(0.500)
(0.494)
EDUCATION and TRAINING
MD
0.086
0.084
0.08
-0.74
0.459
(0.280)
(0.277)
(0.272)
MD/PhD
0.032
0.035
0.03
-1.80
0.073
(0.176)
(0.185)
(0.170)
PhD
0.867
0.87
0.874
0.67
0.506
(0.340)
(0.336)
(0.332)
Other Degree
0.016
0.01
0.016
2.76
0.006
(0.125)
(0.102)
(0.125)
Biomedical degree
0.594
0.614
0.617
0.33
0.739
(0.491)
(0.487)
(0.486)
Physical Science degree
0.129
0.129
0.124
-0.72
0.471
(0.335)
(0.335)
(0.330)
Social Science degree
0.069
0.074
0.071
-0.67
0.506
(0.253)
(0.261)
(0.256)
Prior T32 Predoc Award
0.021
0.024
0.027
0.96
0.339
(0.144)
(0.154)
(0.162)
Prior T32 Postdoc Award
0.019
0.016
0.02
1.65
0.099
(0.136)
(0.127)
(0.141)
Prior NRSA Predoctoral Fellowship
0.001
0.001
0
-1.64
0.101
(0.023)
(0.023)
0.000
OUTCOME VARIABLES
Number of RPG Awards
0.387
0.586
0.367
-10.24
0.000
(1.071)
(1.281)
(1.095)
Number of RPG Applications
1.94
2.752
1.698
-12.51
0.000
(4.360)
(5.158)
(4.066)
Probability of RPG
0.183
0.266
0.165
-13.69
0.000
(0.386)
(0.442)
(0.372)
Probability of R01
0.133
0.204
0.122
-12.25
0.000
(0.339)
(0.403)
(0.328)
Probability of Never Receiving an RPG
0.678
0.579
0.713
16.02
0.000
(0.467)
(0.494)
(0.452)
N
14,276
9,276
5,000
Source: Authors‚Äô calculations. National Institutes of Health IMPACII and NIH/NSF Survey of Earned Doctorates.

41

TABLE 2. PROBABILITY OF NIH F32 PROPOSAL BEING FUNDED BY DISCRETION (REACHED) OR NOT
FUNDED BY DISCRETION (SKIPPED)

(1)

(2)

VARIABLES

Reached

Skipped

Second Council Round

-0.005

-0.006

[0.006]

[0.006]

-0.005

-0.004

[0.005]

[0.005]

Third Council Round

FY 1997

FY 1998

FY 1999

FY 2000

FY 2001

FY 2002

FY 2003

-0.001

-0.011

[0.011]

[0.009]

0.012

0.007

[0.012]

[0.011]

-0.006

-0.003

[0.011]

[0.011]

-0.009

0.005

[0.011]

[0.011]

-0.008

0.002

[0.011]

[0.011]

-0.008

-0.015

[0.012]

[0.010]

-0.012

-0.029***

[0.011]

[0.009]

-0.012

-0.027**

[0.010]

[0.009]

FY 2005

-0.015

-0.034***

[0.010]

[0.008]

FY 2006

-0.025**

-0.038***

[0.009]

[0.008]

-0.026**

-0.030***

[0.009]

[0.008]

-0.030***

-0.035***

[0.009]

[0.008]

FY 2004

FY 2007

FY 2008

N
14,276
14,276
Note: Robust Standard errors in brackets. *** p<0.001, ** p<0.01, * p<0.05
Source: Authors‚Äô calculations. IMPACII and NIH/NSF Survey of Earned Doctorates, 1996 to 2008.

42

TABLE 3. PROPENSITY SCORE MATCHING (PSM) ESTIMATES OF THE IMPACT ON OUTCOMES BY
COMPARATIVE FUNDING-STATUS TYPES

VARIABLES
Reach vs. Not Funded
N = 5,215
Reach vs. In Order
N = 9,062
Reach vs. Skip
N=2,538
Skip vs. Not Funded
N=5,211
Skip vs. In Order
N=9,058
In Order vs. Not Funded
N = 11,735

Number

Number

Probability

Probability

Never

RPG Awards

RPG Applications

RPG

R01

RPG

0.174***

0.801***

0.071***

0.078***

-0.119***

[0.043]

[0.167]

[0.017]

[0.015]

[0.018]

-0.096*

-0.495**

-0.050**

-0.038*

0.063**

[0.048]

[0.183]

[0.018]

[0.016]

[0.019]

-0.176**

-0.333

-0.051**

-0.047**

0.025

[0.063]

[0.220]

[0.019]

[0.017]

[0.022]

0.243***

0.655***

0.085***

0.083***

-0.083***

[0.053]

[0.181]

[0.019]

[0.016]

[0.021]

-0.047

-0.342*

-0.051**

-0.025

0.069***

[0.048]

[0.165]

[0.016]

[0.014]

[0.019]

0.246***

0.983***

0.106***

0.086***

-0.140***

[0.034]

[0.153]

[0.012]

[0.010]

[0.014]

Note: Robust Standard errors in brackets. *** p<0.001, ** p<0.01, * p<0.05
Source: Authors‚Äô calculations. IMPACII and NIH/NSF Survey of Earned Doctorates, 1996 to 2008.

43

TABLE 4. COUNTERFACTUAL TREATMENT EFFECTS WITH PSEUDO-TREATMENTS

VARIABLES
ATE

ATT

Observations

(1)

(3)

PhD degree

Biomedical
degree

(5)
Fellowship or
scholarship PhD
funding

0.007

0.001

0.005

(0.007)

(0.010)

(0.010)

0.005

0.007

0.004

(0.008)

(0.012)

(0.012)

14,273

14,273

14,273

44

Appendix.
APPENDIX TABLE A1. PROBIT REGRESSIONS ON EVER RECEIVING AN AWARD, 1996-2008
(1)
(2)
(3)
Full
Full
Analysis
sample
sample
sample
Review score
-0.006
(0.000)
Age (missing = <26)
Age = 27
-0.107
-0.033
-0.053
(0.024)
(0.030)
(0.036)
Age = 28
-0.040
-0.031
-0.034
(0.022)
(0.027)
(0.030)
Age = 29
-0.056
-0.022
-0.050
(0.021)
(0.026)
(0.029)
Age = 30
-0.061
-0.013
-0.041
(0.021)
(0.026)
(0.029)
Age = 31
-0.072
-0.031
-0.035
(0.020)
(0.026)
(0.029)
Age = 32
-0.088
-0.039
-0.060
(0.020)
(0.026)
(0.030)
Age = 33
-0.086
-0.017
-0.045
(0.021)
(0.027)
(0.030)
Age = 34
-0.105
-0.031
-0.055
(0.021)
(0.027)
(0.031)
Age = 35 or 36
-0.149
-0.035
-0.057
(0.020)
(0.026)
(0.031)
Age = 37 or 38
-0.181
-0.068
-0.085
(0.020)
(0.027)
(0.034)
Age > 38
-0.248
-0.104
-0.137
(0.017)
(0.026)
(0.034)
Marital status (missing = not married)
Married
0.009
0.014
0.016
(0.007)
(0.008)
(0.009)
Marital status missing
0.093
0.057
0.044
(0.062)
(0.090)
(0.085)
Sex (missing = male)
Female
-0.010
-0.008
-0.009
(0.006)
(0.007)
(0.009)
Sex missing
-0.120
-0.098
-0.115
(0.014)
(0.017)
(0.023)
Race and ethnicity (missing = White, non-Hispanic)
Black, non-Hispanic
-0.156
-0.099
-0.146
(0.030)
(0.036)
(0.055)
Asian, non-Hispanic
-0.038
-0.021
-0.025
(0.011)
(0.013)
(0.016)
Other race, non-Hispanic
0.135
0.167
0.064
(0.063)
(0.078)
(0.074)
Hispanic
-0.022
0.001
0.014
(0.018)
(0.021)
(0.025)
Race missing
0.005
0.013
0.011
(0.008)
(0.009)
(0.010)
MD
0.088
0.033
-0.009
(0.030)
(0.033)
(0.042)
MD/PhD
0.189
0.098
0.076
(0.031)
(0.037)
(0.039)
PhD
0.126
0.093
0.060
(0.024)
(0.027)
(0.039)
Biomedical science degree
0.145
0.065
0.041
(0.059)
(0.086)
(0.088)
Physical science degree
0.107
0.054
0.052
(0.062)
(0.090)
(0.084)
Social science degree
0.133
0.022
0.017
(0.062)
(0.089)
(0.087)
Prior T32 Predoc Award
0.067
-0.004
-0.024
(0.022)
(0.025)
(0.026)
Prior T32 Postdoc Award
-0.061
-0.056
-0.048

45

(4)
Analysis
sample
-0.008
(0.000)
0.021
(0.039)
-0.028
(0.036)
-0.016
(0.035)
0.002
(0.034)
0.024
(0.033)
-0.005
(0.034)
0.027
(0.033)
0.016
(0.035)
0.028
(0.034)
0.014
(0.036)
-0.019
(0.037)
0.018
(0.010)
0.086
(0.092)
-0.006
(0.009)
-0.103
(0.026)
-0.103
(0.055)
-0.016
(0.017)
0.048
(0.074)
0.033
(0.025)
0.016
(0.011)
-0.019
(0.045)
0.016
(0.046)
0.049
(0.041)
0.073
(0.101)
0.094
(0.089)
0.031
(0.097)
-0.045
(0.028)
-0.033

Prior NRSA Predoc Award
Observations

(0.022)
-0.002
(0.140)
27,504

(0.025)
0.128
(0.260)
25,719

(0.031)

(0.033)

14,268

14,268

Notes: Robust standard errors in parentheses. All specifications include controls for IC and council rounds.
Source: Authors‚Äô calculations. IMPACII and NIH/NSF Survey of Earned Doctorates.

46

APPENDIX TABLE A2. Nearest Neighbor Estimates of Discretion versus Scientific Review on Career Outcomes
Number
VARIABLES
Reach vs. Not Funded
N = 5,160
Reach vs. In Order
N = 8,502
Reach vs. Skip
N=2,471
Skip vs. Not Funded
N=5,151
Skip vs. In Order
N=8,493
In Order vs. Not Funded
N = 11,182

RPG Awards

Number
RPG
Applications

Probability

Probability

Never

RPG

R01

RPG
-0.093***

0.168**

0.709***

0.067***

0.071***

[0.052]

[0.200]

[0.020]

[0.017]

[0.024]

-0.172**

-0.624**

-0.095***

-0.061**

0.099***

[0.055]

[0.229]

[0.022]

[0.019]

[0.029]

-0.138

-0.101

-0.035

-0.031

0.003

[0.079]

[0.286]

[0.025]

[0.022]

[0.031]

0.166*

0.612**

0.058*

0.062**

-0.091**

[0.071]

[0.231]

[0.025]

[0.023]

[0.030]

0.040

-0.258

-0.017

-0.003

0.060**

[0.054]

[0.186]

[0.017]

[0.016]

[0.019]

0.201***

0.739**

0.090***

0.075***

-0.134***

[0.049]
[0.238]
[0.019]
[0.016]
Source: IMPACII and NIH/NSF Survey of Earned Doctorates, 1996 to 2008
Note: Robust Standard errors in brackets. *** p<0.001, ** p<0.01, * p<0.05.

47

[0.023]

