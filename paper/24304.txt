NBER WORKING PAPER SERIES

WHO PAYS IN PAY FOR PERFORMANCE? EVIDENCE FROM HOSPITAL PRICING
Michael Darden
Ian McCarthy
Eric Barrette
Working Paper 24304
http://www.nber.org/papers/w24304

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
February 2018, Revised August 2019

This paper was previously circulated under the title ‚ÄúHospital Prices and Public Payments.‚Äù We
received no funding for this work. We report no conflicts of interest. We thank Jonathan
Ketcham, Michael Richards, Jason Hockenberry, Bryan Dowd, David Dranove, Craig
Garthwaite, and seminar participants at Johns Hopkins University, The Southeastern Health
Economics Study Group, The Midwest Health Eco- nomics Conference, The National Bureau of
Economic Research Summer Institute, George Washington University, The Carey School of
Business at Johns Hopkins University, and the Federal Trade Commis-sion. Finally, we
acknowledge the Health Care Cost Institute (HCCI), along with companies providing data
(Aetna, Humana, and UnitedHealthcare) used in this analysis. All data analyses were conducted
while Eric Barrette was Director of Research for HCCI. The views expressed herein are those of
the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
¬© 2018 by Michael Darden, Ian McCarthy, and Eric Barrette. All rights reserved. Short sections
of text, not to exceed two paragraphs, may be quoted without explicit permission provided that
full credit, including ¬© notice, is given to the source.

Who Pays in Pay for Performance? Evidence from Hospital Pricing
Michael Darden, Ian McCarthy, and Eric Barrette
NBER Working Paper No. 24304
February 2018, Revised August 2019
JEL No. I11,I18,L2
ABSTRACT
The Hospital Readmission Reduction Program (HRRP) and the Hospital Value Based Purchasing
Program (HVBP), two components of the Affordable Care Act's cost containment measures,
introduced potentially sizeable penalties to underperforming hospitals across a variety of metrics.
To the extent that penalized hospitals subsequently changed their processes of care, such changes
may translate into higher payments from commercial insurance patients. In this paper, we
estimate the effects of these pay-for-performance programs on private hospital payments using
data on commercial insurance payments from a large, multi-payer database. We find that nearly
70% of the costs of the HRRP and HVBP penalties are borne by private insurance patients in the
form of higher private insurance payments to hospitals. Specifically, we show that HRRP and
HVBP led to increases in private payments of 1.4%, or approximately $183,700 per hospital
based on an average relative penalty of $271,000. We find very limited evidence that these effects
are driven by quality improvements, changes in treatment intensity, or changes in service mix.

Michael Darden
Carey School of Business
Johns Hopkins University
100 International Dr.
Baltimore, MD 21202
and NBER
mdarden4@jhu.edu

Eric Barrette
Health Care Cost Institute
1100 G Street NW
Suite 600
Washington, DC 20005
eric.barrette@gmail.com

Ian McCarthy
Department of Economics
Emory University
Rich Memorial Building, Room 319
Atlanta, GA 30322
and NBER
immccar@emory.edu

An online appendix is available at http://www.nber.org/data-appendix/w24304

1

Introduction

Public pay-for-performance (P4P) programs tie public payments to a predetermined set of
measures, which allow policy makers to encourage or discourage certain outcomes. While
the potential advantages of such programs are clear, P4P may also introduce unintended
consequences depending on the design of the program, the relevance of the outcomes, and
the precision with which relevant outcomes can be measured. The United States health
care system has historically operated in the absence of any large scale public P4P programs;
however, this changed with the introduction of the Hospital Readmission Reduction Program
(HRRP) and the Hospital Value Based Purchasing Program (HVBP), both of which were
introduced in 2012 as part of the cost containment provisions of the Patient Protection and
Affordable Care Act (ACA). The HRRP and HVBP were designed to penalize hospitals with
lower-than-expected quality, and an active literature has emerged that attempts to measure
the effects of these programs on hospital quality outcomes (Ryan et al., 2015; Mellor et al.,
2016; Gupta, 2016; Ryan et al., 2017; Gupta et al., 2018; Wilcock et al., 2018). While
the empirical results remain mixed, implicit in this important literature is the assumption
that hospitals pursued some costly investments in an attempt to improve their performance.
Any such costly investments may then have the unintended consequence of increasing a
hospital‚Äôs negotiated payments with private insurers, particularly in the highly concentrated
U.S. hospital market.1
In this paper, we use a compelling data set on actual payments from private insurance
firms to hospitals to quantify the effects of public P4P programs on hospital payments from
private insurers. Our data, maintained by the Health Care Cost Institute (HCCI), contain all
hospital inpatient claims to three national commercial insurers.2 These unique data include
payments for every claim, which capture the negotiated payments between hospitals and
insurers and which may differ substantially from charge-based estimates of payments often
used in the literature (Dafny, 2009; Dranove et al., 2017). Our data cover approximately
28% of individuals under the age of 65 who have employer-sponsored insurance (ESI). When
merged with several other datasets on hospital and county characteristics, our final analytic
data constitute a balanced panel of 50% of all inpatient prospective payment hospitals in
the U.S. between 2010 and 2015.
Under the HRRP and HVBP programs, hospitals were penalized (or potentially rewarded
1
Throughout, rather than use the term ‚Äúprice,‚Äù we refer to the financial transfer for a given procedure as
the ‚Äúpayment‚Äù from a private insurance firm to a hospital. A payment is distinctly different than a hospital
‚Äúcharge,‚Äù which effectively represents a hospital‚Äôs list price for a give procedure. Private insurance firms
negotiate substantial discounts from charges.
2
Cooper et al. (2017) also use HCCI data to examine broad trends in hospital pricing from 2007 through
2011.

2

under the HVBP) by up to 3% of the hospital‚Äôs total Medicare revenues based on observed
quality metrics.3 Since penalty amounts vary from year to year, and because not all hospitals are penalized, the HRRP/HVBP generate both cross-sectional and temporal variation
in P4P penalties. Exploiting this variation, our baseline empirical specification is a hospital
fixed effects estimator in which we estimate the difference in average payments between those
hospitals with a net penalty under the HRRP/HVBP relative to those not penalized, discussed in detail in Section 3.2. Our baseline results reveal an increase in average payments of
1.4% for penalized hospitals, equivalent to a $167 increase in the average private payer payment from 2013 through 2015. We also find evidence that penalty size matters with respect
to payment changes, with a 2.4% increase in payments for the most heavily penalized hospitals relative to those hospitals receiving no penalty or a bonus. As a back-of-the-envelope
calculation, our estimated increase of 1.4% equates to a total increase in private payments
of $183,700 per hospital, based on an average relative reduction in Medicare payments of
$271,000.4
At least four factors support a causal interpretation of our findings. First, a central
feature of HRRP/HVBP is that penalized hospitals had little, if any, opportunity to adjust
their penalties ex ante. This is because the HRRP/HVBP penalties were calculated using
data from several years prior to the start of the programs. For example, penalties incurred
in Fiscal Year (FY) 2013 were based on Medicare claims from July 2008 through June 2011.
The set of quality metrics underlying the penalty formulas also changed over time, further
limiting a hospital‚Äôs ability to predict their penalty status in advance. For example, the set
of conditions covered by the HRRP/HVBP expanded in FY 2015, but the new conditions
were not announced until FY 2014, at which point the data underlying the new conditions
were already collected.
Second, there is evidence that the formulas used to assign HRRP/HVBP penalties have
not sufficiently identified marginally low- versus high-performing hospitals. For example, the
HRRP penalizes hospitals for under-performance in any of the relevant conditions, even if
hospitals significantly over-perform in other areas. As a result, nearly 80% of hospitals in our
sample are ultimately penalized under the HRRP at some point in our panel. Recent studies
also document substantial noise in HVBP penalties or rewards and suggest that a hospital‚Äôs
performance under the HVBP is largely due to chance (Friedson et al., 2016; Wilcock et al.,
2018).
3

Some private insurer contracts explicitly tie payments to Medicare reimbursement rates (Cooper et al.,
2017), but in our context, there is no change in the prospective payment but rather a downward adjustment
of some percentage.
4
The total relative reduction in Medicare payments incorporates bonus payments made to some hospitals,
such that the relative reduction is larger than the average penalty amount.

3

Third, our data offer a compelling advantage relative to most other studies of hospital
payments.5 In particular, the correlation between actual payments and a charge-based proxy
for payments from the Healthcare Cost Report Information System (HCRIS) is 0.435, suggesting that charge-based estimates of payments may contain significant measurement error.
Since we observe actual payments made to hospitals from private insurers, we avoid this
source of measurement error.
Fourth, from an econometric perspective, time-varying unobserved heterogeneity in payments that is correlated with HRRP/HVBP penalties would tend to produce differential
trends and biased results. We consider several additional analyses to test for the potential
presence of such differential trends and any subsequent effects on our estimates, including
a series of event studies for each treatment group, alternative specifications testing for and
allowing for differential trends by penalty status, and an instrumental variables strategy
that exploits the timing of treatment as instruments. Ultimately, we fail to reject a test of
differential trends by penalty status, and we demonstrate that allowing for ever-penalized
differential trends does not change our conclusions. With a series of alternative specifications
and robustness checks, we further show that our results are not driven by regional differences, the ACA Medicaid expansion, or patient severity mix. Collectively, we find strong
empirical evidence that penalties incurred under the HRRP and HVBP led to increases in
private insurance payments to hospitals.
In the remainder of the paper, we examine heterogeneity in and mechanisms behind the
effects of the HRRP and HVBP. We find substantial variation in payment increases across
different service lines, with increases in average hospital payments for circulatory system
(1.9%) and nervous system (2.1%) claims, but with economically small and insignificant
effects for respiratory system, musculoskeletal system, and labor and delivery claims.6 In
addition, we estimate larger effects among hospitals that are likely to be in a better relative
bargaining position with insurers, as proxied by the hospital‚Äôs share of private insurance patients (Wu, 2010). Finally, we find significant heterogeneity in the effects of HRRP/HVBP by
a hospital‚Äôs financial relationship with its physicians, with a 2.3% increase in mean payment
for vertically integrated hospitals and physician groups.
To investigate the mechanisms behind these effects, we study investments that hospitals
may make following the HRRP/HVBP for which private insurers would plausibly be willing to pay. With our data, we can directly test changes in hospital quality and changes
5

Notable exceptions include Clemens & Gottlieb (2017), who study the market for physician services and
find that private payments decreased following a reduction in Medicare payment rates, and Dranove et al.
(2017), who find little evidence of changes in private payments to hospitals following the 2008 stock market
collapse.
6
We identify ‚Äúadmission categories‚Äù based on the major diagnostic category classifications.

4

in hospital services and costs. First, because the HRRP penalizes hospitals based on riskadjusted 30-day readmission rates for Medicare patients, hospitals may have undertaken
costly investments in improving these metrics which spilled over to readmission rates for
the privately insured. Thus, we estimate our preferred model on over 3 million individual
acute care claims, and we find a statistically insignificant 0.1 percentage point decline in the
probability of readmission. We find similar null effects across several other outcomes, including: 1) a measure of profitable services offered by the hospital (Horwitz & Nichols, 2009),
suggesting that hospitals did not simply internalize the loss from HRRP/HVBP penalties;
2) the hospital‚Äôs average DRG weight, suggesting little change in patient selection; and 3)
average hospital length of stay and costs per discharge, suggesting no change in treatment
intensity. Therefore, across a range of outcomes intended to capture mechanisms related to
quality improvements, treatment intensity, and patient selection, we find economically small
and statistically insignificant effects of HRRP/HVBP penalties.
One possible explanation for our estimated payment increases, which is consistent with
anecdotal evidence from physicians, is that the HRRP/HVBP penalties encouraged hospitals to change their processes of care (e.g., introducing more checklists and more oversight).
For example, Tanguturi et al. (2016) describes these introduction of case managers to identify high risk patients and the use of a discharge checklist designed to reduce preventable
readmissions following a hospitalization for percutaneous coronary intervention. Such investments would not necessarily be captured in our cost data but would intuitively be more
salient for hospitals that are financially integrated with their physicians, which is consistent
with our estimates of larger effects among vertically integrated hospitals. Another explanation may be that private insurers are willing to pay for investments that improved outcomes
for Medicare patients (even without an improvement in private insurance patients) if such
investments improved overall hospital reputation and thus increased willingness-to-pay for
private insurance patients. Finally, we acknowledge that hospitals may have pursued some
costly investments in response to P4P penalties, but that the effects of P4P penalties on such
costs may not be precisely estimated in our analysis to the well-documented measurement
error in the HCRIS.
Ultimately, our analysis offers two central contributions to the literature. First, we extend
the literature on P4P in health care. Much of this existing literature studies other areas of
care delivery, such as skilled nursing facilities and home health agencies, and studies of P4P
in the hospital setting focus almost exclusively on quality outcomes. To our knowledge, we
are the first to examine the effects of P4P on private insurance payments to hospitals. The
potential unintended consequence of an increase in hospital prices due to P4P programs is
an important issue as we further refine existing P4P programs and expand P4P into other
5

areas.
Second, and more generally, our analysis introduces another important factor in our
understanding of variation in health care pricing. As clearly documented in Cooper et al.
(2017), hospital market power explains a large amount of geographic variation in hospital
prices. A large literature also considers the role of public payments on hospital prices, often
examined in the context of hospital cost-shifting (Dranove, 1988; Cutler et al., 2000; Frakt,
2011). Cost-shifting may play some role in our estimates given that we do not identify a
clear change in costs or quality to explain our estimated payment increases; however, given
the stated goals of the HRRP/HVBP and anecdotal evidence regarding hospital responses
to these programs, we do not interpret the HRRP/HVBP penalties are a pure reduction
in public payments. We instead take as given that hospitals responded in some costly way
to the P4P penalties. In this context, our results show that changes in public policy also
meaningfully contribute to variation in health care prices, even after adjusting for market
power, hospital fixed effects, and other observable hospital and market characteristics.

2

Policy Background: The HRRP and HVBP

The adoption of the Medicare prospective payment system (PPS) in 1983, in which Medicare
payments changed from pure fee-for-service to a capitated amount for each inpatient stay
depending on diagnosis, generated incentives for hospitals to cut ‚Äúexcessive‚Äù procedures.
PPS also created incentives for hospitals to discharge patients quickly. By 2011, Medicare
paid $24 billion per year for 1.8 million hospital readmissions ‚Äì admissions to any hospital
within 30-days of discharge for the same condition. While some readmissions are unavoidable,
the HRRP was a cost containment in the ACA designed to levy penalties on hospitals with
‚Äúexcessive‚Äù readmissions.
Starting in FY 2013 (October 2012-September 2013), the HRRP penalized hospitals for
which 30-day readmissions for acute myocardial infarction (AMI), heart failure (HF), and
pneumonia (PN) exceeded risk-adjusted thresholds constructed as a function of national
averages. Recall that this assessment was based on data collected from July 2008 through
June 2011. In this first year of the program, hospitals faced a maximum cut in Medicare
payments of 1% across all DRGs. In FY 2015, the maximum penalty increased to 3%, total
penalties rose to $420m (Rau, 2015), and applicable conditions were expanded to include
chronic obstructive pulmonary disease (COPD) and total hip and knee replacements. The
Congressional Budget Office (2010) estimates that HRRP would reduce hospital payments
from Medicare by $113 billion through 2019. There is also strong evidence suggesting that
hospitals were aware of the potential impact of the HRRP. For example, a national survey
6

of hospital leaders found that nearly two-thirds of respondents reported that the HRRP had
a substantial impact on their hospital‚Äôs efforts to reduce readmissions compared to prior
readmission policies (Joynt et al., 2017).
By contrast, the HVBP program is rooted in a standard principal-agent model in which
the principal (CMS in this case) contracts with agents (hospitals) to provide quality care
to Medicare enrollees. The HVBP program scores hospitals based on their achievement
(comparison to other hospitals) as well as their improvement (comparison to their own previous performance). Similar to the HRRP, the HVBP bases changes in payments on past
quality, with data collected over the same lagged time period as in the HRRP. However,
unlike the HRRP, the HVBP program is funded by reducing all hospitals‚Äô base operating
Medicare severity diagnosis-related group (MS-DRG) payments and creating rebate incentives depending on defined quality metrics. The percentage reduction increased annually
up to 2%. The program defines several quality domains and converts measures of quality
within each domain to points, which are aggregated and mapped to a total point score. The
total point score determines the magnitude of the payment change, which may be positive or
negative depending on if a hospital generates a rebate large enough to offset the reduction.
Since the goal of both the HRRP and HVBP is to improve hospital quality, a recent
literature examines the effects of the HRRP/HVBP on hospital readmission rates and other
quality metrics. The existing literature in this area remains mixed. Gupta et al. (2018) find
that the HRRP was associated with a 1.6 percentage-point reduction in 30-day Medicare
readmissions for heart failure but a 1.4 percentage-point increase in 30-day mortality. Gupta
(2016), however, finds evidence of a reduction in Medicare hospital mortality rates (a decrease
of about 3%, significant at the 10% level) from the HRRP, which may account for as much as
60% of the reduction in readmissions. Mellor et al. (2016) similarly find that the HRRP led
to a decline in Medicare AMI 30-day readmission rates; however, new evidence from Ibrahim
et al. (2017) suggests that observed decreases in readmissions may have been driven by
hospitals coding patients more severely and not by ‚Äúreal‚Äù quality improvements. Consistent
with this result, Wilcock et al. (2018) find that the majority of HRRP penalties are a
reflection of poor risk adjustment in the penalty calculation and not of true, underlying
hospital quality.
Regarding the HVBP, the literature generally finds little or no effect on hospital quality
(Ryan et al., 2015; Doran et al., 2017; Norton et al., 2017; Ryan et al., 2017). Examining data
from 2015 to 2016, Norton et al. (2017) did find some hospital response to the HVBP, but
this response was in specific areas with the greatest marginal revenue rather than those areas
with larger quality benefits. Conversely, based on quality data from 2005 through 2014, the
Government Accountability Office (2015) found no effect of HVBP on quality. This study
7

also interviewed a handful of hospital officials and concluded ‚Äúthe HVBP program generally
reinforced ongoing quality improvement efforts, but did not lead to major changes in focus.‚Äù
Friedson et al. (2016) offer an explanation for these findings, where the authors find that
the HVBP does not sufficiently discriminate between hospitals, and whether hospitals are
penalized or rewarded by the HVBP program is largely a matter of chance rather than a
reflection of true underlying quality.

3

Empirical Analysis

3.1

Data

Our primary data come from three large health insurance firms and account for roughly 28%
of all individuals under the age of 65 with employer sponsored health insurance over the
period of 2010 through 2015. To these data, we merge information on HRRP and HVBP
penalties/rewards and other cost information from the Healthcare Cost Report Information
System (HCRIS); hospital-level characteristics such as bed count, for-profit status, and system membership from the American Hospital Association (AHA) annual surveys; data on a
hospital‚Äôs payer mix (i.e., the number and share of Medicare, Medicaid, or private insurance
patients) also from HCRIS; and county-level demographic characteristics from the American
Community Survey (ACS). We restrict our sample to community hospitals in urban areas
and in the contiguous United States, with at least 30 staffed beds, at least 25 admissions in
a given year in the HCCI data, and observed HRRP/HVBP from HCRIS. Our final sample
consists of 1,386 hospitals and 8,316 hospital/year observations.7
Because hospital payments are often bundled across services, we follow Gowrisankaran
et al. (2015), who use similar payment data from Northern Virginia, and aggregate payments
to the hospital level by dividing the total payment for each claim by the appropriate DRG
weight and regressing this amount on individual (claimant) characteristics and hospital fixed
effects. Using the estimated regression results, we predict the risk-adjusted mean hospital
payment for a given year, which reflects the mean bargained payment. Table 1 presents
mean payments across hospitals over time. While average risk-adjusted payments received
by hospitals increase roughly 5% annually between 2010 to 2015, shares of public (Medicare &
Medicaid) and private patients remain relatively stable over time. Importantly, while shares
remain stable, within-hospital patient mix may vary considerably over time as a function of
7

We also consider alternative samples in which we allow for missing net penalty values from HCRIS or
where we arbitrarily set missing HRRP/HVBP values to 0 (e.g., under the assumption that missing values
indicate that the hospital was excluded for the program in that year). Results from these samples are similar
to those presented in the text and available upon request.

8

public payments, which is why we treat payer-specific discharges as a separate dependent
variable. The last column of Table 1 shows the fraction of hospitals subject to a net Medicare
payment reduction. Note that the CMS fiscal year runs from October through the following
September. Because of discrepancies between the fiscal year of the hospital and that of
CMS, 32% of hospitals faced a penalty in their 2012 FY. By FY 2015, 79% of hospitals faced
some payment reduction. Beginning FY 2013, the average penalty amount among hospitals
ever penalized was $204,711, which increased from $171,279 in 2013 to $272,438 in FY 2015.
With non-penalized hospitals receiving an average bonus of just over $66,000, the average
relative payment reduction among penalized hospitals was around $271,000.
Since our baseline empirical specification exploits within-hospital variation, we split our
sample by whether a hospital ever faced a payment reduction under the HRRP and HVBP
during our sample period. Table 2 presents summary statistics of our main dependent
variable and selected independent variables by ever-penalized status. Payments to neverpenalized hospitals are marginally higher than those to penalized hospitals over the 2010‚Äì
2015 period. Non-profit hospitals (public and private) constituted a much larger share of
never-penalized hospitals, suggesting that non-profit hospitals may be of higher quality, at
least in terms of the HRRP and HVBP. However, case mix is significantly more severe in
the ever-penalized hospitals, which suggests that CMS risk-adjustment in the HRRP and
HVBP may not perfectly adjust penalty thresholds (consistent with Wilcock et al. (2018)).
Ever-penalized hospitals tend to be in more competitive markets, have lower Medicare share,
and come from more heavily populated counties. Evidence from Table 2 therefore suggests
that controlling for hospital fixed effects is important in models of hospital payments because
of persistent differences between ever-penalized and never-penalized hospitals.
The log of the annual, within-hospital mean of private insurance payments constitutes our
primary dependent variable of interest. For brevity, we refer to this variable simply as the log
mean payment. For comparison with the literature, we also follow Dafny (2009) in estimating
hospital payments using the average net revenue for non-Medicare inpatient discharges. Since
Medicaid revenues are not provided in HCRIS, the measure is a weighted average of net
revenue per discharge for commercially insured and Medicaid patients where the weights
equal the share of inpatient discharges belonging to each payer. This same measure has
been used in recent studies examining hospital pricing behavior, including Schmitt (2018)
and Lewis & Pflum (2015). To eliminate outliers, we trim the lower and upper tails at
the 5th and 95th percentile of the resulting payment distribution, and we normalize this
estimated payment based on the hospital‚Äôs observed case mix index (CMI) from the inpatient
prospective payment system (IPPS) final rule files. To differentiate this measure of payments
from our observed payments from the HCCI data, we refer to this measure as the log mean
9

net charge.
Finally, since a natural way to reduce exposure to HRRP/HVBP penalties is to avoid
treating Medicare patients, we include measures of payor mix as an additional set of outcomes. These measures include the log number of Medicare discharges, the log number
of Medicaid discharges, and the log number of other discharges (non-Medicare and nonMedicaid). We also considered the Medicare, Medicaid, and other insurer shares (rather
than log counts). Those results are excluded for brevity but qualitatively similar to the
analysis of log counts.

3.2

Regression Analysis

Our preferred empirical specification isolates within-hospital variation in private payments
over time by whether a hospital faced a net penalty from the HRRP and HVBP. This
analysis therefore focuses on the extensive margin of penalties. Equation 1 presents our
main empirical model:
0

yht = Œ±h + xht Œ≤ + Œ¥1[P enaltyht ] +

2015
X

Œ∏t 1[t = j] + ht ,

(1)

j=2011

where outcome yht at hospital h in fiscal year t is a function of a hospital specific intercept, Œ±h ;
a vector of time-varying hospital and market-level exogenous characteristics, xht ; an indicator
for a net penalty under the combination of HRRP/HVBP policies; controls for year effects,
Œ∏t ; and an i.i.d. error term ht . Because the penalty indicator is zero for all hospitals in 2010
and 2011, and because we include hospital fixed effects, Equation 1 represents an unscaled
difference-in-differences estimator, which constitutes a weighted average of four difference-indifferences estimates corresponding to the four years in which a hospital may have first been
penalized. Our parameter of interest, Œ¥, captures the extent to which hospitals penalized
under the HRRP/HVBP receive differential private payments relative to hospitals with no
penalty (which includes hospitals that received a bonus).
Table 3 presents estimated effects of HRRP/HVBP penalties on the log of mean payments, the log of mean net charges, and payer-specific (log) discharges. The first column
of Table 3 demonstrates that hospitals that faced payment reductions increased payments
by 1.4% over the period of 2012-2015. This represents a roughly $167 increase in payments
among penalized hospitals, on average.8 Column 2 presents estimates from a similar model
8

This interpretation is based on the average private insurance payment of $12,100 among penalized
hospitals after FY 2012. Assuming this average payment reflects a 1.4% increase in the average payment in
$12,000
the absence of the penalty, we calculate the effect in dollar terms as $12, 100 ‚àí 1+0.014
.

10

in which we replace negotiated payments with the log of mean net charges as discussed previously (Dafny, 2009; Lewis & Pflum, 2015; Schmitt, 2018; Dranove et al., 2017). Results
in column 2 suggest a smaller and statistically insignificant change in log mean net charges
for penalized hospitals, which we argue demonstrates the importance of using actual payment data. Columns 3 and 4 of Table 3 show movement away from Medicaid and Medicare
patients for penalized hospitals, with discharges decreasing by 4.5% and 2.7%, respectively.
While Table 3 demonstrates higher payments on the extensive margin, we investigate
the intensive margin effect of penalties on payments by breaking the distribution of penalty
size into quartiles and replacing the indicator for net penalty in Equation 1 with indicators
for each of the four penalty quartiles, where the omitted category represents those hospitals
which either saw no penalty or a net bonus in Medicare reimbursements. Results are presented in Table 4. Consistent with our results in Table 3, we find that average payments are
significantly higher in penalized hospitals relative to those receiving no change or a small
bonus. We find no effect on payments for hospitals in the first (smallest) quartile of penalties,
defined as a per Medicare discharge penalty of between $0.01 and $12.59; however, we find
a 2.4% increase in mean payments for hospitals in the highest quartile of penalties (between
$57.10 and $291.60 per Medicare discharge). Results in Table 4 therefore suggest that private payment increases are larger as the HRRP/HVBP penalty increases. Furthermore, we
find monotonically more negative effects of a penalty on Medicaid and Medicare discharges
in the size of the penalty.

3.3

Sensitivity and Robustness

For a causal interpretation of Œ¥, the underlying assumption in Equation 1 is that there are
no time-varying unobserved characteristics that differentially affect payments in penalized
hospitals relative to non-penalized hospitals. While we cannot directly test this assumption
empirically, we can examine the presence of pre-trends as suggestive evidence for or against
the assumption of parallel counterfactual trends. Note that in our data, we have four different treatment groups defined by hospitals first penalized in 2012, 2013, 2014, or 2015,
respectively. There is also substantial persistence in treatment, such that most hospitals
penalized in year t are also penalized in years t + 1, t + 2, etc. Therefore, we test for evidence of differential trends in two ways. First, we present an event study for each treatment
group, in which we interact the treatment dummy with year dummies and estimate separate
treatment coefficients in each year (relative to the year prior to the penalty). Second, we
plot the mean residual from a regression of average payments, analogous to Equation 1 but
where we exclude the net penalty variable. Based on the regression results, we then form

11

the mean residual separately for penalized versus non-penalized hospitals in each year and
add to this the average observed payment among penalized/non-penalized hospitals by year.
Results are presented in Figures 1 and 2, which demonstrate important differences in
trends across treatment groups. While our pre-period data are limited in the early treatment
groups, we fail to reject the null hypothesis of parallel pre-trends among hospitals penalized
in 2012 or 2013 (Figure 1). This is not the case for hospitals treated in 2014 or 2015 (Figure
2), where we do find evidence of differential pre-trends. Allowing for differential trends as
reflected in the mean residual payment graphs, we see clear graphical evidence of parallel
movement in average payments with a discontinuity at the treatment period in the 2012 and
2013 treatment groups. Figures 1 and 2 also demonstrate that payment results in Table 3
are likely driven by those hospitals initially penalized earlier in our sample.
To proceed, we consider three additional estimators. First, we re-estimate Equation 1
when setting Œ±h = Œ± in order to gauge the sensitivity of our results to the presence of unobserved and time-invariant hospital factors. Second, we include in Equation 1 a set of time
dummies interacted with a dummy variable that equals one if the hospital is ever observed
to be penalized. Differential trends conditional on penalty status and other controls would
be suggestive of time-varying unobserved heterogeneity, which may bias our estimate of Œ¥
toward zero. Finally, we estimate a fixed-effects instrumental variables regression that scales
the four difference-in-differences estimates by the probability a hospital is first penalized in
each respective year. Based on graphical evidence in Figure 2 suggesting differential trends
among hospitals penalized in 2014 or 2015, we also present estimates from these additional
estimators for both the full sample and a sample that restricts the treatment group to only
those hospitals first penalized in 2012 or 2013.
The top panel of Table 5 presents results for the full sample while the bottom panel
presents results for the sample restricted to never penalized hospitals and those penalized
initially in 2012 or 2013. The first row in Table 5 shows that estimates for log mean payments
and log net charges without hospital fixed effects are negative, large, and significant. Relative
to our initial results, these findings suggest that: 1) persistent and unobserved hospital-level
heterogeneity is an important driver of outcomes in our setting; and 2) hospital fixed effects
may in fact go a long way toward controlling for mean differences between charges and
payments. Many studies of hospital pricing proxy for payments with hospital charges and
argue that hospital fixed effects control for mean differences between charges and payments
(Cutler et al., 2000). Our results offer some assurance that findings of a significant effect
using charge-based estimates of prices may indeed be reflective of a true price increase;
however, we also emphasize the importance of payment data with respect to the precision
and measurement of private insurance payments, noting the lack of statistical significance
12

in our model of log mean net charges presented in Table 3. Findings of an insignificant
effect using charge-based proxies for private payments may therefore be driven by incorrect
inference (e.g., due to measurement error) or due to a true underlying null effect.
The second row in Table 5 repeats our initial results in Table 3, and the third row presents
results when allowing for differential trends by whether a hospital is ever penalized. Here,
the estimate for log mean payments decreases from 1.4% to 1.0% (or from 1.8% to 1.2%
in the 2012/2013 treatment group) but remains economically meaningful and statistically
significant in the full sample. We also present the p-value of a joint test of the null that
the time trend dummies interacted with ever-penalized are jointly zero. For our log mean
payment outcome, we fail to reject the null of common trends between the ever-penalized
and never-penalized hospitals, with a p-value of 0.497 in the full sample and 0.903 for the
2012/2013 treatment group.
The final row of Table 5 presents results from the fixed effects instrumental variables
estimator. Specifically, we estimate the following with two-stage least squares (2SLS):
0

ÀÜ ht ] +
yht = Œ±h + xht Œ≤ + Œ¥1[P enalty

2015
X

Œ∏t 1[t = j] + ht

(2)

j=2011
0

P enaltyht = Œ±h + xht Œ≤ +

2015
X

Œ∏t 1[t = j] +

j=2011

2015
X

Œªt 1[t = j]Eh + œÑht ,

(3)

j=2011

where Equation 2 is the main equation that estimates the effect of penalty status predicted
from the first-stage in Equation 3. Here, interactions between year dummies and Eh , a timeinvariant dummy for ever-penalized status, are highly predictive instruments for penalty
status, which remove the temporal component of hospital penalties. The Œª parameters then
measure the adjusted proportion of ever-penalized hospitals that were penalized in year t.9
Estimates in the final row of Table 5 find a larger effect of penalties on payments, with a
3.7% and 2.9% increase in mean payments in the final and restricted samples, respectively.
Furthermore, an over-identification test is available. Because no hospitals were penalized in
2011 and because there is only one endogenous variable in Equation 2, we can include the
interaction Eh 1[t = 2011] in the main equation. Table 5 presents the p-value on the t-test
that the parameter on Eh 1[t = 2011] is zero in the main equation. For both samples, we fail
to reject the null hypothesis, which suggests that trends in payments are similar between
those hospitals that will eventually be penalized versus those that will not.
The results in Tables 3-5, coupled with the graphical evidence in Figures 1 and 2 and
9

Kaestner et al. (2014) and Carton et al. (2016) use a similar estimator as an alternative to two-way fixed
effects estimators.

13

the institutional details of the HRRP/HVBP, offer compelling evidence of a causal effect
of HRRP/HVBP penalties on private insurance payments to hospitals; however, we remain
concerned about several institutional confounding factors that may bias this result. Thus,
we consider different specifications for Equation 1 intended to assess the sensitivity of our
estimates to such potential confounders. All results are presented in Table 6.
First, we are concerned that unobserved differences across markets (e.g., with regard to
insurer market power) may influence our estimates. We therefore include a set of countylevel fixed effects, with results summarized in panel 1 of Table 6. Here, we continue to find
positive and significant effects on private insurance payments, as well as significant reductions
in the log number of Medicare discharges. These results suggest that local area variation in
provider or insurer markets is not driving our results.
Second, we remain concerned that other changes in the hospital-insurer relationship may
drive our estimated increase in payments, particularly with respect to the implementation of
the ACA. We therefore consider an alternative specification in which we include an indicator
for whether the hospital was in a Medicaid expansion state as of 2014. These results are
presented in panel 2 of Table 6 and are largely unchanged from our initial estimates. We
also note that this concern is partially alleviated in the bottom panel of Table 5 where we
restrict the treatment group only to those hospitals treated before 2014.
Third, since the HRRP and HVBP are intended to reward and/or punish hospitals based
in-part on quality of care, hospitals may respond to HRRP/HVBP penalties by improving
along a broad set of quality metrics. These metrics need not directly align with quality
underlying the HRRP/HVBP. Indeed, an optimal hospital response may be to focus on
patient satisfaction or other non-clinical measures to potentially offset the financial effects
of the penalties. One requirement for such a response to exist is that the HRRP/HVBP
penalties reveal new quality information to the market. The distribution of readmission
rates across hospitals before the HRRP/HVBP suggest this is not the case, as penalized
hospitals already displayed higher readmission rates relative to other hospitals in the years
prior to 2012 (see Figure 3). Another requirement for this type of response is that patients are
responsive to any hospital improvements. Studies from Dranove & Sfekas (2008) and others
suggest that this is unlikely given the relatively small estimated effects of quality reporting
on hospital choice. We nonetheless examine this issue with an alternative specification in
which we control for a hospital‚Äôs quality as measured by patients‚Äô overall hospital rating
from the Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS).
Panel 3 of Table 6 reports results from this model, with estimates almost identical to those
in Table 3.
Fourth, it may be that other changes introduced through the ACA (e.g., expansion of
14

insurance on the individual market) changed the ‚Äútypical‚Äù patient being admitted to the
hospital. In panel 4 of Table 6, we demonstrate that our results are again unchanged when
conditioning on the hospital‚Äôs average case mix.

3.4

Heterogeneous Effects

If HRRP/HVBP penalties induce some hospital behaviors that ultimately affect private
insurance payments, then there are several dimensions by which we would expect effect sizes
to vary. Perhaps the most natural source of variation is across service lines, particularly
since only selected conditions are included as part of the HRRP/HVBP penalty calculations.
Therefore, we examine heterogeneity across service lines by estimating effects of net penalty
status on the log of mean payments within selected acute care admission service categories.
Estimates for Œ¥ are presented in Table 7 for several specific categories, where we find the
largest increases in payments for nervous and circulatory admissions.10
Because hospitals cannot unilaterally translate costly investments into higher payments
from commercial insurers, an important source of heterogeneity is the relative bargaining
power of hospitals as payments are the result of bilateral negotiation with insurers, and
a hospital‚Äôs ability to negotiate higher payments will depend on the hospital‚Äôs bargaining
position (Dor et al., 2004; Gowrisankaran et al., 2015; Lewis & Pflum, 2015; Ho & Lee, 2017).
To investigate, we attempt to proxy for a hospital‚Äôs bargaining position by constructing the
quartile of the hospital‚Äôs share of public patients relative to total patients, and we interact
our penalty variable with indicators for each quartile.11 This analysis is similar to that of
Wu (2010), who intuits that a hospital with a large share of private payers represents a more
important client for the insurance market.12 Results are presented in Table 8 and suggest that
our initial estimate is driven by hospitals with the smallest share of public patients. Indeed,
the first column of Table 8 demonstrates that payments increased by 3.9% for hospitals with
the smallest share of public patients. This increase was nullified for hospitals in the third
and fourth quartile of public patient shares.
10

For each admission category, we restrict our sample to hospitals with at least 25 admissions in that
category in each year of our sample.
11
We also tested for differential effects of the penalty among hospitals operating as a monopoly, duopoly, or
triopoly. Here, we find a relatively large and positive effect of the interaction between a monopoly indicator
and the penalty indicator, with a point estimate of 0.013; however, the effect is statistically insignificant
with a p-value of 0.23. We estimate smaller and statistically insignificant effects on other interaction terms
between penalty status and duopoly or triopoly indicators. This pattern of results persists for different
measures of the hospital market. For brevity, the full results from these specifications are excluded from the
paper but are available upon request.
12
Applying this intuition to a study of hospital cost-shifting following the Balanced Budget Act of 1997,
Wu (2010) finds that hospitals with larger shares of private patients were more able to pass Medicare payment
reductions on to private payers.

15

Another proxy for bargaining position is whether a hospital is aligned with its network
of physicians. Lewis & Pflum (2015), for example, find that hospitals that are affiliated with
a physician group are able to negotiate a larger share of surplus. Vertical integration with
physicians may therefore put some hospitals in a more favorable bargaining position, and
thus facilitate a larger increase in private payments. To investigate, we estimate our preferred
empirical model on data from only those hospitals that already owned a physician group or
physician practice prior to 2012.13 We also estimate our model on hospitals never observed
to be vertically integrated. As shown in Table 9, among those hospitals already vertically
integrated, the effect of a net penalty on payments is 2.3% and significant. Meanwhile,
penalties are associated with a small and statistically insignificant effect on payments among
those hospitals never observed to be vertically integrated.14

4

Mechanisms for Payment Increases

The results in Section 3 provide strong empirical evidence that penalized hospitals were able
to increase private insurance payments. The effect size varies along several dimensions, but
on average, we estimate an increase in private insurance payments to hospitals by 1.4%.
It is unclear, however, exactly how a hospital could translate a penalty into higher private
insurance payments. In this section, we therefore consider different mechanisms that may
have facilitated such an increase.

4.1

Changes in Hospital Quality

Since the HRRP/HVBP are designed to improve hospital quality, it may be that hospital
quality improvements ultimately led to our estimated payment increases. As discussed in
Section 2, most of the existing studies of the HRRP/HVBP on quality tend to focus on the
Medicare population, but to explain increases in private insurance payments, we need to
consider the effects of the HRRP and HVBP on quality among private insurance payments.
We are not aware of any evidence in the literature suggesting that quality in the private
insurance market improved due to the HRRP or HVBP programs. Indeed, in a study of
13

The AHA surveys provide information at the hospital-level on whether a hospital currently has an
‚Äúintegrated salary model.‚Äù This measure unfortunately does not capture how many physicians are employed
by a hospital, but instead only captures if there is any integrated model reported between the hospital and
any of its physicians.
14
We also considered whether the penalty itself led to more integrated salary models by treating the binary
integration measure as an additional outcome. Here, we estimate a very small and insignificant negative
effect of being penalized on the probability of reporting an integrated salary model, suggesting that penalized
hospitals were not integrating with physicians due to the penalty. These results are limited by the nature of
our vertical integration data and are therefore excluded from the paper but available upon request.

16

private insurance patients in Florida and California, Demiralp et al. (2017) find no evidence
that the HRRP reduced the readmission rate among the non-Medicare population. To test
this in our data, we directly investigate whether penalized hospitals improved quality (as
measured by readmissions) in the commercial insurance market.15 We estimate the effect of
hospital penalty status on the probability of readmission using a linear probability model with
data at the individual admission level. Following the Agency for Healthcare Research and
Quality definition, we classify a readmission to be any admission to any inpatient prospective
payment hospital within 30 days of a discharge.16
Our linear probability model includes all controls from our main specification plus patient
controls such as age range, gender, length of stay, DRG weight, insurance product type
(HMO, PPO, POS, EPO), and DRG fixed effects. As summarized in column 1 of Table
10, the results demonstrate that, even with a sample of over 3 million observations, we
find an economically and statistically insignificant effect of penalty status on the probability
of readmission.17 To the extent that penalized hospitals are investing in quality to lower
Medicare readmissions among the indicated areas, we find no evidence that such quality
improvements are changing readmissions on average for the commercially insured population.

4.2

Changes in Services or Treatment Intensity

Since our outcome is calculated as an average payment per patient, our results could simply
reflect increases in the intensity of treatment rather than an increase in the payment received
for an otherwise identical service. Using our data on private payments, we therefore consider
the extent to which hospitals respond to public penalties by changing treatment patterns
or reallocating resources towards more profitable services. We first estimate the effects of
Medicare payment reductions on charges among the commercial insurance population. This
analysis uses within-hospital variation in charges as a general proxy for changes in intensity
of treatment, with results presented in column 2 of Table 10. Here, we find no economically
or statistically significant increase in charges among penalized hospitals.
We also follow Horwitz & Nichols (2009) in constructing a set of indicators for ‚Äúprofitable‚Äù
(e.g., angioplasty or neonatal intensive care) versus ‚Äúunprofitable‚Äù (e.g., alcohol dependency
15

Our data do not have a reliable measure of mortality. We therefore focus the analysis on readmissions.
We also note that our data include inpatient stays in which the patient may have died in the hospital or
soon after; however, given the age composition of the commercial sample, death is likely to be less frequent
than in the Medicare population.
16
See 2017 AHRQ Statistical Brief #230 for additional details on the readmission calculations. Our sample
excludes newborns and transfers, and we limit the analysis to all patients with 12 months of private insurance
coverage in a calendar year.
17
We also estimated the model using the lagged net penalty, where we again find an economically and
statistically insignificant effect of penalty status on the probability of readmission.

17

services or hospice care) hospital services.18 We then constructed a ‚Äúprofitable services
index‚Äù calculated as the ratio of profitable services to all profitable and unprofitable services
identified by Horwitz & Nichols (2009). For example, if the hospital offered 2 profitable
services and 2 unprofitable services, then the ratio for this hospital would be 50%. Treating
this profitable services index as an additional outcome and repeating our analysis from
Section 3, column 3 of Table 10 demonstrates that we find small and insignificant effects of
being penalized. These insignificant effects also persist across all robustness checks presented
in Table 6. A similar pattern emerges in Table 10 when we consider average DRG weights and
average length of stay (among our commercial insurance population) as separate outcomes,
with insignificant effects of HRRP/HVBP penalties on these outcomes in all specifications
considered.
Finally, it may be that penalized hospitals incurred some costly investments, perhaps
with the aim of improving quality of care. While our data are limited in these areas, we
also estimated the effect of hospital penalty status on the log of cost per discharge (hospitalwide).19 Here, we again find no significant or economically meaningful effects of being
penalized on hospitals‚Äô average costs per discharge.

5

Conclusion

This paper uses novel payment data from a large, multi-payer database to investigate how
hospital payments from private insurers change under a large scale pay-for-performance program. We use variation in pay-for-performance incentives generated by two cost-containment
policies within the ACA ‚Äî the hospital readmissions reduction program and the hospital
value based purchasing program ‚Äî to estimate the effect of pay-for-performance penalties on
average hospital payments. Our initial analysis estimates a 1.4% increase in average private
insurance payments to hospitals that were penalized under the HRRP/HVBP programs.
Subsequent analysis finds that this estimate is robust to a variety of alternative specifications, including differential trends among penalized and non-penalized hospitals. We also
find little empirical evidence that HRRP/HVBP penalties induced hospitals to increase quality in the commercial insurance population, increase intensity of treatment, adjust service
offerings toward more profitable areas, or otherwise increase overall costs per discharge.
Our results therefore suggest that hospitals were able to negotiate higher private insurance
payments without any clear quality improvements among private insurance patients. While
18

A full list of relatively profitable and relatively unprofitable services is provided in Table 2 of Horwitz
& Nichols (2009). Following their analysis, we identify whether a hospital offers these services based on
responses from the AHA annual surveys.
19
We calculate costs per discharge based on data available in HCRIS.

18

we do not find evidence of a change in average costs per discharge (based on hospital cost
reports), we also acknowledge that granular data on hospital cost structures is notoriously
difficult to obtain. Indeed, the stated goal of the HRRP/HVBP was to improve hospital
quality, and a large body of anecdotal evidence suggests that hospitals actively attempted to
improve their performance under these programs. We suspect that such efforts are potentially
valuable to private insurers and should therefore translate into higher private insurance
payments; however, to the extent that these efforts involve non-monetary investments such
as changes to processes in care delivery or specific administrative oversight, we likely cannot
measure these investments using existing data from hospital cost reports or claims data.
That said, we examine heterogeneities in the effects of HRRP/HVBP penalties and find that
effects are largest among hospitals that also appear to be in a better bargaining position with
commercial insurers, which is consistent with the bargaining mechanism by which hospitals
might translate investments into higher private insurance payments.
Another theory that is potentially consistent with these results is that of hospital costshifting. This is plausible in our setting if we assume that the HRRP/HVBP did not induce
any costly investments among penalized hospitals. Since the HRRP/HVBP were specifically
designed as quality improvement programs, we acknowledge the likelihood that penalized
hospitals would incur some additional costs (not necessarily direct monetary costs) as they
attempt to improve their performance. Given these details, as well as the limited theoretical
basis for cost-shifting to occur in practice, we do not claim definitive evidence of cost-shifting
based on our results. Instead, we interpret our findings as an unintended consequence of the
HRRP/HVBP, in which penalized hospitals were able to pass on any additional investments
to private insurers in the form of higher payments.
Collectively, our analysis offers three central findings: 1) private insurance payments increased among hospitals penalized by the HRRP and HVBP; 2) effects were largest among
hospitals with larger penalties and among hospitals better positioned in a bilateral negotiation with insurers; and 3) the payment increases do not appear to be explained by changes
in hospital services or quality of care. To quantify this effect, note that our estimated 1.4%
increase in payments implies an increase of $167 per inpatient stay based on an average
private insurance payment of approximately $12,100 among penalized hospitals. As a backof-the-envelope calculation, if one assumes that this payment increase applies to around 1,100
inpatient stays per year, then we estimate a total increase in private insurance payments of
up to $183,700 per hospital per year.20 To put this in context, penalized hospitals saw an
20

Our price data are based on just over 550 inpatient stays per year per hospital and reflect nearly 30% of
all commercial insurance claims. Extrapolating to 1,100 assumes that some but not all commercial insurers
captured in our data would have experienced the same price increase as estimated in our analysis.

19

average penalty of around $205,000, while non-penalized hospitals received an average bonus
of just over $66,000. This yields a differential payment between penalized and non-penalized
hospitals of approximately $271,000. An estimated increase of $183,700 in private insurance
payments therefore suggests that 68% of the cost of HRRP/HVBP penalties is passed on to
private insurers in the form of higher payments.
We stress that these results should not be interpreted to suggest that pay-for-performance
in health care is inherently bad. Instead, we interpret our results as highlighting the importance of how the pay-for-performance program is designed. In the case of the HRRP,
hospitals need only be below average in one area in order to incur some percent penalty
levied on all Medicare payments. Most hospitals are not better than average in every dimension, and indeed, as the number of conditions in the HRRP has grown, so too has the
percentage of hospitals penalized in a given year. In practice, the HRRP is a relatively blunt
instrument that penalizes most hospitals in a given year. Subsequently, HRRP penalties
may serve as a poor quality signal. The HVBP may similarly suffer from some basic design
problems. For example, in tracking a hospital‚Äôs performance across 20-plus metrics, it becomes difficult to discern a true quality signal from each hospital. When applied to a highly
concentrated private industry, our results suggest that such pay-for-performance programs
may have important unintended consequences.

20

References
Carton, Thomas, Darden, Michael, Levendis, John, Lee, Sang, and Ricket, Iben 2016. In the
shadow of a giant: Medicares influence on private physician payments. Journal of Political
Economy, 125(1), 1‚Äì39.
Clemens, Jeffrey, & Gottlieb, Joshua D. 2017. Comprehensive Indoor Smoking Bans and
Smoking Prevalence: Evidence at the State Level. American Journal of Health Economics,
2(4), 535‚Äì556.
Congressional Budget Office. 2010. Distribution among Types of Providers of Savings from
the Changes to Updates in Section 1105 of Reconciliation Legislation and Sections 3401
and 3131 of H.R. 3590 as Passed by the Sentate. Report. Congressional Budget Office.
Cooper, Zack, Craig, Stuart V, Gaynor, Martin, & Van Reenen, John. 2017. The price
aint right? Hospital prices and health spending on the privately insured. Working Paper.
National Bureau of Economic Research.
Cutler, David M, McClellan, Mark, & Newhouse, Joseph P. 2000. How does managed care
do it? The Rand journal of economics, 526‚Äì548.
Dafny, Leemore. 2009. Estimation and Identification of Merger Effects: An Application to
Hospital Mergers. Journal of Law and Economics, 52(3), 523‚Äì550.
Demiralp, Berna, He, Fang, & Koenig, Lane. 2017. Further Evidence on the System-Wide
Effects of the Hospital Readmissions Reduction Program. Health services research.
Dor, Avi, Grossman, Michael, & Koroukian, Siran M. 2004. Hospital transaction prices and
managed-care discounting for selected medical technologies. American Economic Review,
352‚Äì356.
Doran, Tim, Maurer, Kristin A, & Ryan, Andrew M. 2017. Impact of provider incentives on
quality and value of health care. Annual review of public health, 38, 449‚Äì465.
Dranove, David. 1988. Pricing by non-profit institutions: the case of hospital cost-shifting.
Journal of Health Economics, 7(1), 47‚Äì57.
Dranove, David, & Sfekas, Andrew. 2008. Start spreading the news: a structural estimate
of the effects of New York hospital report cards. Journal of Health Economics, 27(5),
1201‚Äì1207.

21

Dranove, David, Garthwaite, Craig, & Ody, Christopher. 2017. How do hospitals respond
to negative financial shocks? The impact of the 2008 stock market crash. RAND Journal
of Economics, forthcoming.
Frakt, Austin B. 2011. How much do hospitals cost shift? A review of the evidence. Milbank
Quarterly, 89(1), 90‚Äì130.
Friedson, Andrew I, Horrace, WC, & Marier, AF. 2016. So many hospitals, so little information: How hospital value based purchasing is a game of chance. Working Paper 194.
Center for Policy Research working paper.
Government Accountability Office. 2015. Hospital Value-Based Purchasing: Iniial Results
Show Modest Effects on Medicare Payments and No Apparent Change in Quality-of-Care
Trends. Tech. rept. GAO-16-9.
Gowrisankaran, Gautam, Nevo, Aviv, & Town, Robert. 2015. Mergers When Prices Are
Negotiated: Evidence from the Hospital Industry. American Economic Review, 105(1),
172‚Äì203.
Gupta, Ankur, Allen, Larry A, Bhatt, Deepak L, Cox, Margueritte, DeVore, Adam D,
Heidenreich, Paul A, Hernandez, Adrian F, Peterson, Eric D, Matsouaka, Roland A,
Yancy, Clyde W, et al. 2018. Association of the Hospital Readmissions Reduction Program
Implementation With Readmission and Mortality Outcomes in Heart Failure. JAMA
cardiology, 3, 44‚Äì53.
Gupta, Atul. 2016. Impacts of performance pay for hospitals: The Readmissions Reduction
Program. Working Paper. University of Michigan.
Ho, Kate, & Lee, Robin S. 2017. Insurer competition in health care markets. Econometrica,
85(2), 379‚Äì417.
Horwitz, Jill R, & Nichols, Austin. 2009. Hospital ownership and medical services: market
mix, spillover effects, and nonprofit objectives. Journal of health economics, 28(5), 924‚Äì
937.
Ibrahim, Andrew, Dimick, Justin, Sinha, Shashank, Hollingsworth, John, Nuliyalu,
Ushapoorna, & Ryan, Andrew. 2017. Association of Coded Severity With Readmission
Reduction After the Hospital Readmissions Reduction Program. JAMA Internal Medicine,
178(2), 84‚Äì95.

22

Joynt, Karen E, Figueroa, Jose F, Orav, John E, & Jha, Ashish K 2016. Opinions on
the Hospital Readmission Reduction Program: Results of a National Survey of Hospital
Leaders. The American Jouranal of Managed Care, 22(8), e287‚Äìe294.
Lewis, Matthew, & Pflum, Kevin. 2015. Diagnosing Hospital System Bargaining Power in
Managed Care Networks. American Economic Journal: Economic Policy, 7(1), 243‚Äì274.
Kaestner, Robert, Darden, Michael, Lakdawalla, Darius . 2014. Are investments in disease
prevention complements? The case of statins and health behaviors. Journal of Health
Economics, 36, 151‚Äì163.
Mellor, Jennifer, Daly, Michael, & Smith, Molly. 2016. Does It Pay to Penalize Hospitals
for Excess Readmissions? Intended and Unintended Consequences of Medicare‚Äôs Hospital
Readmissions Reductions Program. Health Economics.
Norton, Edward C, Li, Jun, Das, Anup, & Chen, Lena M. 2017. Moneyball in Medicare.
Journal of health economics.
Rau, Jordan. 2015. Half Of Nation‚Äôs Hospitals Fail Again To Escape Medicare‚Äôs Readmission
Penalties. KHN Report. Kaiser Health News.
Ryan, Andrew M, Burgess, James F, Pesko, Michael F, Borden, William B, & Dimick,
Justin B. 2015. The Early Effects of Medicare‚Äôs Mandatory Hospital Pay-for-Performance
Program. Health services research, 50(1), 81‚Äì97.
Ryan, Andrew M, Krinsky, Sam, Maurer, Kristin A, & Dimick, Justin B. 2017. Changes in
hospital quality associated with hospital value-based purchasing. New England Journal of
Medicine, 376(24), 2358‚Äì2366.
Schmitt, Matt. 2018. Multimarket Contact in the Hospital Industry. American Economic
Journal: Economic Policy, 10(3), 361‚Äì87.
Tanguturi, Varsha K, Temin, Elizabeth, Yeh, Robert W, Thompson, Ryan W, Rao, Sandhya K, Mallick, Aditi, Cavallo, Elena, Ferris, Timothy G, & Wasfy, Jason H. 2016. Clinical
Interventions to Reduce Preventable Hospital Readmissions After Percutaneous Coronary
Intervention. Circulation: Cardiovascular Quality and Outcomes, 9, 600‚Äì604.
Wilcock, Andrew, Escarce, Jose J., Huckfeldt, Peter J., Sood, Neeraj, Popsescu, Ioana, &
Nuckols, Teryl. 2018. Luck of the Draw: the Role of Chance in the Assignment of Medicare
Readmission Penalties. Conference Presentation. American Society of Health Economists.

23

Wu, Vivian Y. 2010. Hospital cost shifting revisited: new evidence from the balanced budget
act of 1997. International journal of health care finance and economics, 10(1), 61‚Äì83.

24

Figures

25

Figure 1. Evidence of Parallel or Differential Trends, 2012 and 2013
(a) Event Study: 2012 Treatment

-.02

11,000

0

Coefficient
.02
.04

.06

Mean Price ($)
12,000
13,000
14,000

.08

15,000

(b) Mean Residual Payments: 2012 Treatment

2010
-1

First

+1
Period

+2

2011

+3

2012

Year

Penalty

2014

2015

No Penalty

(d) Mean Residual Payments: 2013 Treatment

-.02

11,000

0

Coefficient
.02

Mean Price ($)
12,000 13,000 14,000

.04

15,000

(c) Event Study: 2013 Treatment

2013

2010
-2

-1

First
Period

+1

+2

2011

2012
Penalty

Year

2013

2014

2015

No Penalty

Event studies and mean residual payments as discussed in the main text. Each event study reflects estimated coefficients on
the interaction between treatment and year dummies. The excluded year in all cases is the year before treatment. Mean
residual prices reflect the mean residual from a regression of mean hospital payments similar to Equation 1 but excluding
the net penalty variable. The residual payment in the figure is the mean residual by treatment group by year plus the mean
observed payment by treatment group by year.

26

Figure 2. Evidence of Parallel or Differential Trends, 2014 and 2015
(a) Event Study: 2014 Treatment

10,000

-.05

11,000

Coefficient

0

Mean Price ($)
12,000 13,000

14,000

.05

(b) Mean Residual Payments: 2014 Treatment

-.1

2010
-3

-2

-1
Period

First

2011

+1

2012

Year

Penalty

2014

2015

No Penalty

(d) Mean Residual Payments: 2015 Treatment

-.1

11,000

-.05

12,000

Coefficient

0

Mean Price ($)
13,000
14,000

.05

15,000

(c) Event Study: 2015 Treatment

2013

-4

-3

-2
Period

-1

First

2010

2011

2012
Penalty

Year

2013

2014

2015

No Penalty

Event studies and mean residual payments as discussed in the main text. Each event study reflects estimated coefficients on
the interaction between treatment and year dummies. The excluded year in all cases is the year before treatment. Mean
residual prices reflect the mean residual from a regression of mean hospital payments similar to Equation 1 but excluding
the net penalty variable. The residual payment in the figure is the mean residual by treatment group by year plus the mean
observed payment by treatment group by year.

27

Density
0 .05.1.15.2.25

Figure 3. Pre-HRRP/HVBP Readmission Rates

Penalized
Never Penalized

15

20
25
Pneumonia Readmission Rates

30

35

10

15

20
25
Heart Failure Readmission Rates

30

35

10

15

20
25
AMI Readmission Rates

30

35

Density
0 .1 .2 .3 .4

Density
0 .05.1.15.2.25

10

Notes: Kernel density estimates for readmission rates prior to HRRP/HVBP among hospitals
ultimately penalized versus those not penalized. Readmission rates reflect reported rates in
2010 and 2011, which are constructed from rates in 2006-2009 and 2007-2010, respectively.

28

Tables
Table 1. Characterization of Research Sample over Time
Fiscal
Year

Sample
Size

2010
2011
2012
2013
2014
2015

1,386
1,386
1,386
1,386
1,386
1,386

Payment $
Mean (St. Dev.)
10,729.22
11,602.74
12,079.46
12,668.44
12,795.83
13,397.63

(4,936.50)
(5,076.45)
(5,477.37)
(5,567.76)
(5,444.21)
(5,921.74)

Medicare
Discharges

Medicaid
Discharges

Other
Discharges

Percent
Penalized

4,614.62
4,618.93
4,493.31
4,396.32
4,260.43
4,311.41

2,010.11
1,960.05
1,810.27
1,783.81
1,726.25
1,578.86

7,898.18
7,892.21
8,019.04
7,996.10
7,852.71
8,261.74

0.00
0.00
0.32
0.74
0.76
0.79

Total
8,316
12,212.22 (5,481.55)
4,449.17
1,811.56
7,986.67
0.43
Notes: Balanced panel of hospitals over time between 2010 and 2015. Payment represents the
mean dollar amount paid to a hospital in a year over all acute care admissions. Penalty is a
binary variable for whether the combination of HRRP and HVBP resulted in a net payment
reduction. Other discharges denotes all discharges other than Medicare and Medicaid.

29

Table 2. Hospital Characteristics by
Penalties
Variable

Never
Penalized

Log(Payment)
9.423
Log(Charge)
8.843
System Membership
0.768
Non-profit
0.790
Log(Case Mix Index)
0.437
Local Hospital
Monopoly
0.133
Duopoly
0.282
Triopoly
0.139
Market Share
Medicare
0.338
Medicaid
0.110
Medicare+Medicaid
0.447
Other
0.553
Total Pop. (1000s)
714
County Age Distribution
[18, 34]
0.240
[35, 64]
0.393
>65
0.133
County Race Distribution
White
0.795
Black
0.096
County Income Distribution
< $50k
0.185
[$50k, 75k]
0.126
[$100k, 150k]
0.132
> $150k
0.095
County Education Distribution
High School Only
0.270
Bachelor‚Äôs Only
0.197

Ever
Penalized

p-value

9.300
8.726
0.784
0.692
0.447

0.000
0.000
0.352
0.000
0.090

0.113
0.156
0.108

0.110
0.000
0.012

0.330
0.125
0.455
0.545
1,190

0.056
0.000
0.086
0.086
0.000

0.239
0.393
0.130

0.504
0.947
0.101

0.734
0.134

0.000
0.000

0.180
0.123
0.132
0.101

0.000
0.000
0.820
0.007

0.270
0.191

0.925
0.005

Notes: n = 8, 316 Summary statistics are split by
whether a hospital is ever observed to receive a net
penalty in 2012-2015. Payment represents the mean
dollar amount paid to a hospital in a year over all
acute care admissions. County level characteristics
are from the American Community Survey.

30

Table 3. Baseline Results
Log Mean
Payment
Net Penalty

0.014***
(0.005)
Hospital Characteristics
Monopoly
-0.008
(0.012)
Duopoly
-0.005
(0.010)
Triopoly
0.000
(0.009)
Large Market
-0.041
(0.028)
Any Teaching
-0.018
(0.012)
Major Teaching 0.003
(0.006)
System
0.019
(0.015)
Nonprofit
0.020
(0.026)
County Age Share
[18,34]
-1.132*
(0.681)
[35,64]
-0.402
(0.910)
>64
-0.488
(0.797)
County Share in Income Group
50k-75k
-0.288
(0.386)
75k-100k
-0.279
(0.479)
100k-150k
-0.736
(0.457)
>150k
0.891**
(0.402)

Log Mean
Net Charge

Log Medicaid
Discharges

Log Medicare
Discharges

Log Other
Discharges

0.008
(0.008)

-0.045**
(0.021)

-0.027***
(0.007)

-0.004
(0.011)

0.004
(0.011)
0.010
-(0.010)
0.003
(0.008)
0.001
(0.013)
-0.022
(0.014)
-0.001
(0.004)
-0.002
(0.011)
-0.009
(0.016)

-0.025
(0.055)
0.036
(0.044)
-0.000
(0.039)
-0.063
(0.050)
-0.047
(0.039)
0.008
(0.026)
-0.091**
(0.041)
0.073
(0.058)

0.003
(0.025)
0.030
(0.019)
0.002
(0.015)
0.049**
(0.020)
-0.021
(0.016)
0.009
(0.010)
-0.066***
(0.019)
0.036
(0.028)

-0.012
(0.029)
0.013
(0.023)
0.006
(0.019)
0.179***
(0.043)
-0.013
(0.022)
0.011
(0.012)
-0.083***
(0.020)
0.016
(0.032)

-0.896*
(0.543)
-1.182*
(0.656)
0.281
(0.671)

2.902
(2.327)
2.923
(2.781)
-1.440
(2.765)

-3.163***
(0.853)
-3.428***
(1.171)
0.361
(1.245)

-1.418
(0.880)
-0.044
(1.295)
-0.838
(1.359)

-0.034
(0.286)
0.649*
(0.352)
0.290
(0.313)
-0.139
(0.314)

1.518
(1.439)
0.281
(1.736)
-1.847
(1.533)
0.814
(1.375)

-0.173
(0.548)
-0.319
(0.623)
-0.017
(0.625)
0.997*
(0.511)

0.420
(0.790)
-0.286
(0.791)
0.072
(0.776)
-1.767***
(0.671)

Notes: n = 8, 316. All regressions include hospital and year fixed effects, and other
hospital level controls include bed count, number of nurses, and number of other
non-medical staff. Market power variables are constructed using the overall hospital
service area. Large market is a binary variable for a hospital in the top half of
the market size distribution. In cases in which independent variables are missing,
we recode them and control for missing variable indicators to ensure a balanced
panel. Standard errors are clustered at the hospital level. *** p-value<0.01, **
p-value<0.05, * p-value<0.1.

31

Table 4. Intensive Margin Results
Penalty
Quartile
1
2
3
4

Mean Penalty Per Medicare
Discharge [Range]
$6.00
[$0.01, $12.59]
$20.21
[$12.59, $29.08]
$41.77
[$29.15, $57.06]
$94.25
[$57.10, $291.60]

Log Mean
Payment

Reference Category
0.004
(0.006)
0.020***
(0.006)
0.014**
(0.006)
0.024***
(0.008)

Log Mean
Net Charge

Log Medicaid
Discharges

Log Medicare
Discharges

Log Other
Discharges

= No Penalty
0.01
(0.009)
0.007
(0.009)
0.001
(0.011)
0.016
(0.013)

or Bonus
-0.007
(0.025)
-0.053**
(0.024)
-0.061**
(0.027)
-0.085***
(0.030)

0.001
(0.008)
-0.018**
(0.008)
-0.035***
(0.009)
-0.085***
(0.012)

0.006
(0.012)
0.005
(0.013)
-0.006
(0.013)
-0.036**
(0.015)

Notes: n = 8, 316. Results derived from breaking the size of the per Medicare discharge penalty into
quartiles, with the omitted category as those hospitals receiving no penalty or a bonus. All regressions
include hospital and year fixed effects, and other hospital level controls include bed count, number
of nurses, and number of other non-medical staff. Market power variables are constructed using the
overall hospital service area. In cases in which independent variables are missing, we recode them and
control for missing variable indicators to ensure a balanced panel. Standard errors are clustered at the
hospital level. *** p-value<0.01, ** p-value<0.05, * p-value<0.1.

32

Table 5. Alternative Specifications
Log Mean
Payment

Log Mean
Net Charge

Log Medicaid
Discharges

Log Medicare
Discharges

Log Other
Discharges

-0.061***
(0.015)
0.014***
(0.005)
0.010*
(0.005)
[0.498]
0.037**
(0.014)
[0.899]

-0.049***
(0.018)
0.008
(0.008)
0.019**
(0.008)
[0.041]
-0.048**
(0.024)
[0.511]

0.220***
(0.045)
-0.045**
(0.021)
-0.038
(0.023)
[0.250]
-0.087
(0.058)
[0.875]

0.094***
(0.026)
-0.027***
(0.007)
-0.026***
(0.007)
[0.005]
-0.031
(0.020)
[0.990]

0.069***
(0.022)
-0.004
(0.011)
-0.011
(0.012)
[0.446]
0.033
(0.028)
[0.924]

Restricted Sample: n = 6, 954
OLS
-0.073***
(0.021)
OLS + Hospital FE
0.018***
(0.007)
OLS + Hospital FE +
0.012
+ Ever Penalized Trends
(0.008)
Differential Trend p-value
[0.903]
OLS + Hospital FE + IV
0.029**
(0.013)
OverID p-value
[0.941]

-0.077***
(0.025)
0.002
(0.011)
0.025**
(0.012)
[0.026]
-0.049**
(0.022)
[0.414]

0.305***
(0.062)
-0.029
(0.029)
0.001
(0.035)
[0.142]
-0.095*
(0.051)
[0.949]

0.150***
(0.036)
-0.024***
(0.009)
-0.020**
(0.010)
[0.002]
-0.032*
(0.018)
[0.877]

0.113***
(0.030)
0.001
(0.012)
-0.012
(0.014)
[0.469]
0.031
(0.024)
[0.717]

Full Sample: n = 8, 316
OLS
OLS + Hospital FE
OLS + Hospital FE +
+ Ever Penalized Trends
Differential Trend p-value
OLS + Hospital FE + IV
OverID p-value

Notes: Each point estimate represents the estimated coefficient on a binary variable for
whether or not a hospital received a net penalty in a given year. All regressions include
year fixed effects and other hospital level controls include bed count, number of nurses, and
number of other non-medical staff. Market power variables are constructed using the overall
hospital service area. Large market is a binary variable for a hospital in the top half of the
market size distribution. In cases in which independent variables are missing, we recode them
and control for missing variable indicators to ensure a balanced panel. The restricted sample
includes only hospitals that were never penalized and those first penalized prior to 2014. Differential trend p-value is from the F-test that all year dummy interactions with ever penalized
are zero. OverID p-value is from the test that the interaction between a 2011 dummy and
ever-penalized is zero in the second stage. Standard errors are clustered at the hospital level.
*** p-value<0.01, ** p-value<0.05, * p-value<0.1.

33

Table 6. Robustness Checks
Log Mean
Payment

Log Mean
Net Charge

Log Medicaid
Discharges

Log Medicare
Discharges

Log Other
Discharges

1. Hospital, Year, and County Fixed Effects
Net Penalty

0.015***
(0.005)

0.009
(0.008)

-0.048**
(0.022)

-0.027***
(0.007)

-0.003
(0.011)

2. Controlling for Medicaid Expansion States
Net Penalty

0.014***
(0.005)

0.008
(0.008)

-0.044**
(0.021)

-0.027***
(0.007)

-0.005
(0.010)

3. Controlling for Overall HCAHPS Hospital Rating
Net Penalty

0.014***
(0.005)

0.008
(0.008)

-0.045**
(0.021)

-0.026***
(0.007)

-0.003
(0.010)

4. Controlling for Case Mix
Net Penalty

0.014***
0.004
-0.044**
-0.026***
-0.005
(0.005)
(0.008)
(0.021)
(0.007)
(0.011)
Notes: Further controls include those in our baseline specification for mean payments.
The p-value in the first row of results is in reference to the null hypothesis that trends in
the outcome of interest are the same between ever-penalized and never-penalized hospitals
conditional on the model covariates. In cases in which independent variables are missing,
we recode them and control for missing variable indicators to ensure a balanced panel.
Standard errors are clustered at the hospital level. *** p-value<0.01, ** p-value<0.05, *
p-value<0.1.

34

Table 7. Log Payments for Condition Specific Admissions

Net Penalty

Nervous
System

Respiratory
System

Circulatory
System

Musculoskeletal
System

Labor and
Delivery

Neonatal

0.021***
(0.010)

0.001
(0.011)

0.019**
(0.008)

0.004
(0.007)

-0.001
(0.005)

0.016
(0.010)

n
1,410
1,758
2,754
3,060
5,226
3,204
Mean
13,762.86
12,015.13
13,071.17
12,981.58
11,308.56
8,911.19
Notes: All regressions include hospital and year fixed effects. The dependent variable is the log of
average payments for each associated acute care admission. Further controls include those in our
baseline specification for mean payments. In cases in which independent variables are missing, we
recode them and control for missing variable indicators to ensure a balanced panel. Standard errors
are clustered at the hospital level. We restrict the sample to include at least 25 admissions per
hospital per year. *** p-value<0.01, ** p-value<0.05, * p-value<0.1.

35

Table 8. Triple Differences by Public
Share
Log Mean
Payment
Net Penalty

Log Mean
Net Charge

0.039***
0.043***
(0.010)
(0.013)
* Public Share 2 -0.020*
-0.014
(0.012)
(0.014)
* Public Share 3 -0.033**
-0.043***
(0.013)
(0.015)
* Public Share 4 -0.044*** -0.070***
(0.013)
(0.016)
Public Share 2
0.007
0.049***
(0.010)
(0.013)
Public Share 3
0.016
0.087***
(0.011)
(0.016)
Public Share 4
0.023*
0.157***
(0.012)
(0.018)
Notes: All regressions include hospital and year
fixed effects. Further controls include those in
our baseline specification for mean payments. The
share of a hospital‚Äôs patients insured by the public
sector is broken into quartiles and interacted with
penalty variables. In cases in which independent
variables are missing, we recode them and control for missing variable indicators to ensure a balanced panel. Standard errors are clustered at the
hospital level. *** p-value<0.01, ** p-value<0.05,
* p-value<0.1.

36

Table 9. Vertical Integration and Penalties
Log Mean
Payment

Log Mean
Net Charge

Log Medicaid
Discharges

Log Medicare
Discharges

Log Other
Discharges

Hospitals Integrated Vertically with Physician Groups Prior to 2012
Net Penalty

0.023***
(0.008)

0.017***
(0.006)

-0.036
(0.032)

-0.026**
(0.009)

0.008
(0.016)

Hospitals Never Observed to be Vertically Integrated with a Physician Group
Net Penalty

0.008
0.021***
-0.063**
(0.007)
(0.012)
(0.031)
Notes: Empirical models are identical to those in Table 3.
at the hospital level. *** p-value<0.01, ** p-value<0.05, *

37

-0.024**
-0.005
(0.010)
(0.015)
Standard errors are clustered
p-value<0.1.

Table 10. Changes in Quality or Treatment Intensity
Patient-Level
Readmission

Log
Charge

Profit Index

Net Penalty

Average DRG
Weight

Average
LOS

Log Cost per
Discharge

-0.001
0.004
0.002
0.004
0.015
-0.001
(0.001)
(0.004) (0.001)
(0.004)
(0.012)
(0.001)
n
3,345,641
8,316
8,316
8,316
8,316
8,238
Notes: All regressions include hospital and year fixed effects, and other hospital level controls include
bed count, number of nurses, and number of other non-medical staff. In cases in which independent
variables are missing, we recode them and control for missing variable indicators to ensure a balanced
panel. Standard errors are clustered at the hospital level. *** p-value<0.01, ** p-value<0.05, *
p-value<0.1.

38

