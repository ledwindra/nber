{
    "id": 32934,
    "citation_title": "Patent Text and Long-Run Innovation Dynamics: The Critical Role of Model Selection",
    "citation_author": [
        "Ina Ganguli",
        "Jeffrey Lin",
        "Vitaly Meursault",
        "Nicholas F. Reynolds"
    ],
    "citation_publication_date": "2024-09-16",
    "issue_date": "2024-09-12",
    "revision_date": "None",
    "topics": [
        "\n",
        "Econometrics",
        "\n",
        "Data Collection",
        "\n",
        "Industrial Organization",
        "\n",
        "Market Structure and Firm Performance",
        "\n",
        "Development and Growth",
        "\n",
        "Innovation and R&D",
        "\n"
    ],
    "program": [
        "\n",
        "Development of the American Economy",
        "\n",
        "Productivity, Innovation, and Entrepreneurship",
        "\n"
    ],
    "projects": null,
    "working_groups": [
        "\n",
        "Entrepreneurship",
        "\n",
        "Innovation Policy",
        "\n"
    ],
    "abstract": "\n\nAs distorted maps may mislead, Natural Language Processing (NLP) models may misrepresent. How do we know which NLP model to trust? We provide comprehensive guidance for selecting and applying NLP representations of patent text. We develop novel validation tasks to evaluate several leading NLP models. These tasks assess how well candidate models align with both expert and non-expert judgments of patent similarity. State-of-the-art language models significantly outperform traditional approaches such as TF-IDF. Using our validated representations, we measure a secular decline in contemporaneous patent similarity: inventors are \u201cspreading out\u201d over an expanding knowledge frontier. This finding is corroborated by declining rates of multiple invention from newly-digitized historical patent interference records. In contrast, selecting another single representation without validating alternatives yields an ambiguous or even opposing trend. Thus, our framework addresses a fundamental challenge of selecting among different black-box NLP models that produce varying economic measurements. To facilitate future research, we plan to provide our validation task data and embeddings for all US patents from 1836\u20132023.\n\n",
    "acknowledgement": "\nThe views expressed in this paper are solely those of the authors and do not necessarily reflect the views of the Federal Reserve Bank of Philadelphia, the Federal Reserve System, or the National Bureau of Economic Research. No statements here should be treated as legal advice. Any errors or omissions are the responsibility of the authors. Acknowledgements: We gratefully acknowledge support from an NBER Innovation Policy Grant. We also received excellent RA support from Aaron Rosenbaum, Joseph Huang, Cameron Fen, Annette Gailliot, Jake Moore, and Isaac Rand. Finally, we received useful feedback from Matt Clancy, Darya Davydova, Ga\u00e9tan de Rassenfosse, Luise Eisfeld, Deanna James, Semyon Malamud, Roxana Mihet, participants of the seminar at EPFL, and participants of the NBER Innovation Information Initiative Technical Working Group Meeting and TADA 2023. First version: December 21, 2023.\n\n\n"
}