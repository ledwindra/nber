NEER WORKING PAPER SERIES

P4ULTIPLE TIME SERIES MODELS
APPLIED TO PANEL DATA

Thomas B. MaCurdy

Working Paper No. 646

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge MA 02138
March 1981

This research was supported by NSF grant SOC 77—27136. Portions of
this paper were presented at the Econometric Society Meetings held
in Montreal, Canada, June, 1979. I am grateful for comments from
Bronvyn Ball and Thm Mroz. The research reported here is part of
the NBER's research program in labor Studies. Any opinions
expressed are those of the author and not those of the National
Bureau of Economic Research.

NEER Working Paper ff646
March 1981

Multiple Tine Series Models Applied to Panel Data

ABSTRACT

This

study presents a general methodology for fitting multiple time

series models to panel data. The basic statistical framework considered
here consists of a dynamic simultaneous equation model where disturbances

a permanent—transitory scheme with transitory components generated
by a multivariate autoregressive—moving average process. This error scheme
follow

admits

a wide variety of autocovariance patterns and provides a flexible

for describing the dynamic characteristics of longitudinal data
with a minimal number of parameters. It is possible within this framework
framework

to

consider generally specified rational distributed lag structures involving

both exogenous and endogenous variables which includes infinite order lag

relationships. This paper outlines the generalizations of standard time
series models that are possible when using panel data, and it identifies
those instances in which procedures found in the time series literature

cannot be directly applied to analyze longitudinal data. Data analysis
techniques in the tine series literature are adapted for panel data analysis.
These

techniques aid in the choice of a time series

model and prevent one

from choosing a specification that is broadly inconsistent with the data.
Several estimation procedures are proposed that can be used to estimate all
the parameters of a multiple tine series model including both regression

coefficients and parameters of the covariance matrix. The techniques
developed here are robust in the sense that they do not rely on any specific
distributional assumptions for their asymptotic properties, and in many
cases their implementation requires only standard computer packages.

Thomas E. MaCurdy

Professor

Department of Economics

Stanford

Stanford,

University

California 94305

(415) 497—9694

Introduction

This paper applies the apparatus of stationary time series analysis
to the analysis of panel data. Multiple time series models, such, as those
studied by Zellner and Palm (1974, 1975) and Wallis (1977), are combined
with the components of variance models of Balestra and Nerlove (1966) and
Flussain and Wallace (1969). Grafting these two distinct models together
offers a natural framework for pooling cross section and time series data.
The statistical model considered in this paper is based on what is
known in the time series literature as a dynamic simultaneous equation model
(OSEN).

This model merges multiple time series analysis with the analysis

of simultaneous equations. It consists of a system of structural equations
where endogenous variables are related not only to one another and to exogenous

variables, but also to lagged values of these variables. Generalized variants
of this 'model offer a rich statistical framework for the analysis of panel data.

Virtually any empirical specification that is linear in measured variables can
be analyzed within this framework, including generally specified distributed
lag relationships which may be of infinite order involving either endogenous
or exogenous variables.

To model the correlation properties of disturbances over time, this study
recognizes a broader class of error processes than has been considered in

existing work on panel data. Disturbances are assumed to consist of permanent
components and time varying components that follow generally specified auto-

regressive—moving averages (APXA) processes. Previous studies typically impose
specific autocorrelation schemes on the data, and they do not perform systematic

tests among competing specifications. This study, on the other hand, approaches
the problem of choosing an error structure for panel data the same way that
1

2

a time series analyst decides on a particular time series specification.
Given the class of models considered in this paper, an important part
of the statistical problem is to find those specifications that are consistent with the data.

The statistical framework considered below goes beyond the single
equation case and proposes the use of vector APJIA processes as a method for

pooling cross section and time series data in the multi—equation case.
Multiple time series models offer a robust scheme for combining systems of
equations like those found in seemingly unrelated or simultaneous equation

models. These models allow one to use time series techniques to estimate
simultaneously several structural distributed lag relationships involving
both endogenous and exogenous variables and still allow for a general autocorrelation pattern for disturbances.

This study provides a general method for fitting multiple time series

models to panel data. It is especially tailored for analyzing a panel data
set that has a large number of individuals and a relatively short time series.
Special problems arise when one uses panel data to estimate time series models,
and some procedures found in the time series literature are not directly

applicable. On the other hand, panel data permits the consideration of more
general parameterization than is possible in standard time series analysis.
This study proposes solutions to the special problems, and it identifies the
generalizations of empirical specifications possible in the analysis of panel
data.

To aid in the choice of model specifications, techniques found in the
time series litnrature for identifying the forms of distributed lag relationships and the orders of ARMA processes are adapted for application in a panel

data setting. These techniques prevent one from choosing specifications for

3

the DSEM and for the error process that are broadly inconsistent with the

data. Treating residuals as dependent variables in a seemingly unrelated
regression analysis, it is possible to compute estimates of the sample
covariogram, correlogram, and partial correlation function associated with

disturbances and the standard errors of these estimates. As in the case of
standard tine series analysis, these estimates provide information to choose
among competing ARMA specifications for the error process and they provide
the basis for simple tests for several forms of nonstationarity and heteroscedasticity.

This study also develops general methods for estimating the models and

the error processes described above. Specifically, these methods provide
for the estimation of the parameters of a system of seemingly unrelated
regression or simultaneous equations including parameters of the covariance
matrix when these parameters are subject to an arbitrary set of nonlinear

constraints. These constraints may simultaneously involve regression coefficients and parameters of the covariance matrix. Estimators based on least
squares and on quasi—maximum likelihood procedures are proposed that do not
rely on any specific distributional assumptions for their asymptotic properties.
Both limited information and full information estimation procedures arc

developed. These procedures are computationally efficient and simple to
implement.

Section I presents a general statistical framework and considers a

wide range of issues associated with model specification. Section II develops
data analysis techniques, and Section lIT describes estimation procedures.

4

I. A General Statistical Framework

Panel data offers observations on a sample of "individuals" in more

than one time period. An individual here refers to an observational unit
such as a household or a firm. The models developed below are designed to
estimate the structures relating an individual's variables both within and
across time periods using all the available data.
Dynamic simultaneous equations offer a flexible framework for describ-

ing the relationships between an individual's measured variables. A set of
structural equations from a DSEN, suitably modified for a panel data analysis,
may be written as

(1)

•

rY11(t_i)

=

j=O

j2i

+

B.X.(t-j) + Ujt), t =

i=

1,...

wh e r e

Y1.(t) = g x 1 vector of endogenous variables for individual
i in time period t,

Y2.(t)

=

Xjt) =

r., j =

h

x 1 vector of endogenous variables,
x 1 vector of exogenous variables,

0,... ,n,

f., j

0,.. .,r, and B., j =

0,...,s,

are

coefficient matrices of order g x g, g x h, and
g x in,

respectively,

and
U1(t) = g x 1 disturbance vector.

S

In terms of matrix lag operators, this system of equations may alternatively
be written as

(2)

r(LYY,(t) =

L)Y2jt)

+ B(L)x(t) + u.(t),

where L is the lag operator (i.e., LX.(t) =
and B(L) E

Xjt-j)),

and r(L)

j03

B.L3 are finite order matrix lag operators.

j=O -'
j=O
Period 1 (i.e., t = 1) in this model refers to the first period of the panel

in which one observes all of the current and lagged values of both the endo—

genous and the exogenous variables. Thus, T = T* — max (n, r, s) where T*
total number of time periods supplied by the panel data source and n, r, and
s are the orders of the lag polynomials F(L), 'Y(L) and B(L). The following
analysis assumes that time dummies or polynomials in time are included among
the exogenous variables to capture period effects that are common across

individuals. Thus, we have N independent sets of T time series observations

with which to estimate the parameters of model (2). The above specification
assumes that a researcher wishes to analyze g equations at once, and the pro-

cedures developed below apply to the analysis of this case. A researcher may
desire to consider more than one equation at a time in order to achieve parameter
identification, to impose restrictions across equations, or to obtain more
efficient estimates.

Sepcification (2) includes virtually all econometric models that are

linear in measured variables as special cases. If a researcher chooses to
analyze a single structural equation (or one equation at a time), then in (2)
one sets g =

1, h = the number of separate endoganous variables on the right—

hand side of the equation, and n

the number of separate exogenous variables.1

'Notice that a variable is said to enter the equation if either its
current or its lagged value appears on the right—hand side of equation (2).

6

If r(L)

=

and

L) =

where

and

are coefficient matrices of

orders g x g and g x h, respectively, the system of equations given by
(2) reduces to a standard simultaneous equation model (i.e., there are

no lagged endogenous variables) with g equations per period. If, in
addition, '0 =

tg

and 'F0 =

0, we obtain a seemingly unrelated or a multi—

variate regression model with g equations per period.1 If, still further,
g =

1, we have a standard multiple regression model where there is a single

equation per period for each individual.
The statistical framework given by (2) permits the consideration of
a wide variety, of distributed lag relationships between the elements of

J Y2, and X, including infinite order schemes. The assumption that the
lag polynomials r(L), 'F(L), and B(L) are of finite order is not as restric-

tive as it may first appear. It is possible to estimate any infinite order
distributed lag relationship which can be written as a ratio of finite order

lag polgnomials using model (2). Such lag schemes, known as rati?nal distributed lags, admit flexible weight patterns on past variables and contain
many well known schemes as special cases.2
To see how it is possible to analyze this type of lag structure within
the framework of (2), consider a single equation version of the model where
is related to

and X each represent a single variable. Assuming that
and to X through rational distributed lag schemes, we have

1"Seemingly unrelated regression model" in this paper refers to any
system of regression equations whose disturbances are not assumed to be
uncorrelated. These equations may or may not contain the same set of
explanatory variables, and there may be parameter constraints across equations.
Also, there may exist some for-tu of covariance restrictions for disturbances
included in a system, and there is no requirement that disturbances are un—
correlated across systems.
2See Griliches (1967) for a survey of these distributed lag schemes.

7

c (L)

b1(L)
(3)

Y1•(t)

Y2.(t) + c12(L) Xjt) + U(t)
b2(L)

where b1(L), b2(L), c1(L), and c2(L) are lag polynomials of finite (and

typically low) order, and Ur(t) is a disturbance.' Multiplying both sides
of this equation by b2(L)c2(L) converts it into the form of equation (2)

with F(L) =

b2(L)c2(L),

b2(L)c2(L)tJ(t)

'V(L) =

b1(L)c2(L), EL) = c1(L)b2(L),

and !Jjt) =

where F(L), 'Y(L) and B(L) are all finite order. There is,

then, an equivalent relationship between equations (2) and (3). Analyzing
rational distributed lag schemes using specification (2) will, in general,
imply nonlinear restrictions relating the coefficients of the polynomials

r(L), P(L), and B(L). This, however, introduces no significant complications
in the following rliscussion. Both the data analysis and the estimation pro-

cedures developed below permit the imposition of such restrictions. While
imposing these constraints will, in general, yield more efficient parameter
estimates, it is important to recognize that one can estimate all the parameters
needed to construct rational distributed lag structures without imposing any

restrictions in the estimation of equation (2). One can always convert the
estimates of equation (2) into those of equation (3) by observing that
w(L) —
r(L)

b1(L)
b2(L)

—
and B(L) -

c1(L)

(L)

The following analysis assumes that the error terms, TJi(t).. are dis-

tributed independently across individuals (i.e., across the index 1), but
that these disturbances are autocorrelated over time (i.e., over the index t)

for the same individual. It is often useful or necessary to restirct the
1The polynomials b2(L) and c2(L) are assumed to have roots that lie
strictly outside the unit circle.

8

form of this autocorrelation. Imposing such restrictions reduces the
number of parameters in a model; it can create a statistical model that
may be used for prediction outside the sample period; or, in the case of
a simultaneous equation model, it can aid in securing the identification

of structural parameters. A natural specification for the error process
in a pandel data setting is one that merges linear multiple time series
models with variance component schemes.
This study considers such an error structure. In particular, 131(t)
is assumed to follow a permanent-transitory scheme of the form

(4)

+

U1(t) =

where
= g x 1 vector of permanent components with

if i=j
=
0

otherwise,

and the u(t) is a g x 1 vector of transitory components uncorrelated
with 4. and generated by the multiple time series process

(5)

v(t) =

-

Af(t-i) + e(t)

which may be equivalently written as

(6)

where

A(L)u(t) =

M(L)ci(t)

+
j=l

9

p
A(L) E

q

.

I

A.L3 and M(L) E

j=O

N.L3 are g x g matrix lag

j=0

and the roots of IM(L) = 0
operators with A0 = N0 =
18
are assumed to lie on or outside the unit circle,1
and

ajt) =

g x 1 vector of white noise with

t*and ij

1

if t

0

otherwise,

E(E.(t)c"jt*)) =

Thus, U,(t) is the sum of a vector of correlated permanent components,
4., and a vector of individual specific variates, v.(t), which follows a
multivariate AJUIA process. There are two sources of autocorrelation

accounted for in this error specification: one is due to the presence
of permanent components which capture the effects of unmeasured characteristics unique to the individual that remain constant over the sample
period; and the other source is the time series components which account
for the existence of unobserved variables that vary systematically from

one period to the next. This error process admits a wide variety of
autocorrelation patterns and provides a flexible scheme for describing the
time series aspect of panel data with a minimal number of parameters.
Previous studies on longitudinal analysis have considered special
cases of the error specification proposed above for a single equation
(which implies g = 1 in (4) and (6)). The most popular specification is
one that combines a permanent component with a pure autoregressive scheme
1The restriction on the coefficients of M(L) is the usual one imposed
in the time series literature to guarantee that these coefficients are
identified.

10

(i.e., p ' 0 and q = 0 in (5))1 A few studies consider a permanent

component and a low order moving average process (i.e., g
and q >

1, p =

0,

O).2 No study considers a mixture of an autoregressive and a

moving average process.

There are several other error specifications found in the literature
that can be analyzed using the statistical model for disturbances given

by (4) and (6). One such specification attempts to generalize the above
model following the suggestions of Nerlove (1967) in his work on "unobserved
components.'1 Disturbances are assumed to depend on more than one transitory

component. In particular, instead of (4), it is assumed that
J
u,(t) =
1

+
SL=1

v2.(t)
1

where the transitory components 'i9(t),

i
=

1,...

=

1,... ,J,

are mutually independent

and each follows a restricted ABMA process of the form

=

where A(L) and N(L) are matrix lag operators with the same properties as
A(L) and M(L) defined above, and the ci(t)'s are white noise vectors.
According to this specification, the disturbance vector of the DSEM depends
on permanent components and a sum of J time varying components that are each

generated by a unique multivariate ARMA process. Since this new error process
includes the simpler process proposed above as a special case with J =

1,

1David (1971), ilause (1977), Lillard—Willis (1978) and Lillard—Weiss
(1979) are examples of studies that estimate first order autoregressive
schemes (i.e., p = 1). Ashenfelter (1978) considers higher order processes.

2.
Friedman

(1954, p. 353) and Hause (1977) are examples of such studies.

11

some authors have offered this new process as a way of providing f or a

wider class of autocorrelation structures for disturbances.' This more
complex error process, however, does not admit more general autocorrelation

structures. This result is a direct consequence of the fact that summing
disturbances generated by ARNA processes yields a nev disturbance that
also follows an ARt4A process.2 Thus, any autocorrelation pattern produced
by the complex error process can be duplicated by the simpler process given
by (4) and
'Hause (1977), Lillard—Weiss (1978) and Lillard—Weiss (1979) estimate
2. The
elementary specifications of this error process for the case J
most widely estimated specification assumes that i1.(t) follows a first order
autoregressive process and 'o2.(t) is white noise.

J
2flefine the disturbance vector n(t) =

v(t) where each of the

follows an ARNA process of the form assumed in the text (i.e., A(L)v2(t) =

MjL)c1(t)

£ =

1,... ,J).

It is always possible to represent the ARNA process

for v(t) as A(L)k(t) =

A(L)MjL)c(t)

E N(L)c(t) where [A(L) and

A(L) are the determinant and the adjoint matrix associated with AL(L), and

M*(L)is a fine order matrix defined as At(L)N(L). Thus, premultiplying

(t) by p(L)

J

fl A(L) yields p(L)(t) =

LI

e(L)M*(L)cjt) where e(L) =

1=1
P(L)/fA(L)1 is a finite order polynomial. The right—hand side summation
expression is known to have a vector moving average representation since its
autocorrelation function is zero after finite order (see Hannan (1970, p. 66)).
Thus, we see that (t) follows a vector APIIA process.

3mere is, then, a fundamental identification problem associated with
the use of the complex specification of the error process. To estimate
this specification, one requires a priori restrictions on each of the ARNA
processes generating transitory components.

12

One can also analyze error specifications in which permanent
components are not uncorrelated with itexogenotisti variables or in which

disturbances depend on individual specific growth rate terms as well as

permanent and time series components. For those specifications where
is correlated with the X.(t)'s, first—differencing equations (i.e., multiplying both sides of each equation by (l—L)) eliminates permanent components
and creates a new model that satisfies the assumptions of the DSEM and error

process proposed above. Similarly, if disturbances depend on individual
specific growth rate terms and, instead of (4), Ujt) =

+ bt + v.(t)

where b. is a g x 1 vector of permanent components distributed randomly across
individuals,1 then first—differencing can once again be used to transform this

error specification into a model like those proposed above.2 In the case of
2

an individual specific quadratic trend (i.e., b,t ),

second—differencing

puts

the model into the appropriate form. Differencing, then, offers a way of
collapsing more general DSEM's and error processes into a specification which

is nested within the framework considered in this paper. It is important to
recognize that differencing changes the specification of the DSEM and the
ARMA process for transitory disturbances in a known way and introduces no

new parameters. Thus, the effects of differencing can easily be undone in
the sense that one can construct the model associated with levels using only
the parameter estimates of the differenced equations.
One can estimate rational distributed lag relationships using the
strategy outlined above when disturbances are assumed to follow the error
1such error specifications are common in the empirical literature
on earnings (see, for example, Hause, 1977 and Lillard and Weiss, 1979).
2First differencing in this case reduces random trends to permanent
components.

13

scheme proposed above. If the disturbance U(t) appearing in (3)
follows a permanent—transitory scheme of the sort given by (4) and (6),

then the transformed disturbance U(t) in specification (2) (which equals

b2(L)c2(L)U(t)) follows an error scheme of the same form. Thus, using
model (2) along with error processes (4) and (6), one can fully estimate
rational distributed lag structures while imposing covariance restrictions.
Translating from equation (3) to specification (2) will, in general, imply
nonlinear restrictions relating the coefficients of the lag polynomials
r(L), T(L) and 5(L) associated with measured variables and the polynomials

A(L) and M(L) determining the autocorrelation of disturbances. The data
analysis techniques and the estimation procedures developed below permit

one to consider and to incorporate these types of constraints. Introducing
the possibility of covariance restrictions in the analysis of rational lags
can lead to an increase in the efficiency of estimation, and it can provide
a source of parameter identification which relaxes the need for exclusion
restrictions and exogeneity assumptions.

A Familiar Representation
Combining all the structural equations for an individual into one
model creates an alternative representation of the above DSEM that is partic-

ularly useful for the analysis of panel data. Stacking the equations given
by (1) for individual i in descending order starting with the last period
flelds

14

j=O

(7)

j=O

I

I

I

Z

Fv ('.-j)

jlI
j=O

'

I

1

I

I

[

S

E x (T-j)

j=d

'ii
I

(T)

I

I

S

I

B X (1—i)

I

j=o

U1(1)

j

In

matrix notations this system of equations may be written as

BX
'ft -4-—i
-ii= -21

(8)

with

lj

E(U.U')

where V

ii

21

=

+ U

=
I

o

i=j

o

otherwise

=

(Y'.(T),
lx

VII (Tj)

0'

X = (x:(T),...,x:(l—s)),
1

TY =
1

U(lfl, and the coefficient matrices I, !, and B are block diagonal band
matrices with the matrices

and JBQ,...,B5 ]

running

15

down the diagonal of 1', !, and B, respectively.1 Written in the form of (8),
we have constructed a system of simultaneous equations in which disturbance
vectors are independently distributed over observations so that it is possible to estimate the unknown parameters of the coefficient matrices 1', 1', and

B using standard simultaneous equation estimation procedures.2
The main consequence of assuming a DSEM of the sort presented in
equations (1) or (2) when analyzing panel data is the imposition of constraints

across equations in different time periods for a given individual. Inspection
of equation (7) reveals that the specification assumed in (1) implies equality
constraints across the rows of r, w, and B.
One obvious generalization of the above DSEN that is possible when this
model is used to analyze panel data rather than time series data concerns the
constancy of matrix lag operators over time. In specifying the 08124 given

has

matrix Q
the form

is

a diagonal band or a block diagonal hand matrix if t
-

abcd
abc d

....
0

0

abcd
abcd

If a, b, c, and d are constants, then Q is a diagonal band matrix. If a, b,
c, d are matrices, then Q is a block diagonal band matrix with the matrix
Ia, b, c, d] running down the diagonal.
2

These procedures include two stage and three stage least squares methods
that assume disturbances are correlated across equations but not across observations. When a nonsimultaneous specification of the 05111 consits of g multiple
and 'Y(L) = 0 in (2)), we have
regression equations per period (i.e., r(L) =
]:g
1Tg and V = 0, and (8) becomes a seemingly unrelated regression model which

can be estimated using standard joint generalized least squares procedures. See
Section III for further discussion.

16

by (2), it is not required in a panel data analysis to assume that the
matrix lag operators r(Ij, 49(L) and B(L) are the same across time periods.
Instead, one can add a tttel subscript to these matrices indicating that there

is a new

set

of coefficients for each period and, thus, a different distributed

lag relationship. The consequence of this generalization in (8) is the relaxation of equality constraints relating the rows of F, 49, and B.

A Specification for the Covariance Natrix
The consequence of assuming that disturbances appearing in (1) follow
the error specification given by (4) and (6) is the imposition of restrictions
on the covariance matrix 8 =
of the DSEN

given

E(L71U)

associated

with the stacked representation

by (8). The following analysis derives the exact restrictions

on autocovariances implied by the combined variance component—multiple time
series process assumed above, and it formulates an explicit parameterization
foi the covériance matrix 0.

According to (4), each component of the disturbance vector U. is generated
by an error model of the form tJ.(t) =

4, and o1(t) follows the

E(q1q)

M(L)c(t)

(9)

+ vjt), t =

multivariate

ARMA

l,...,T,

process,

where

A(L)v1(t) =

which is distributed independently of 4). with E(c:jt)c!(t))

Defining =
= (1

4).

(v!(T),... v(l).)

&J.) +
0 =

and

as a T x 1 vector of ones, we have

u..; so,

E(U.U!) =

(u.'

®)

+ E(v.v).

To specify e, we need the implied parameterization for E(vv).
According to (5), v. is determined by the system of equations

E.

17

p

q

E M.e.(T—j)

E A.v.(T-J)

v.(T)

-

j=0

j=l

:i

+

(10)

:

I.

=

where

j=1

covariance matrix for

v.(l—p)

q

E M.c(1—j)

This system does not represent a one—to—one transfor-

mation from the cjt)'s,

for c.,(T),. ..

'

A.v(1—j)

v.(l)
1

,(l).

=

l,...,T

to v.• One cannot, then, derive the

from (10) if given only distributional assumptions
Also appearing in (12) are the variables v.(O),...,

and c.(O)...(l—q) which are kno.-n in the time series litera-

ture as initial conditions or starting values for the error process. To
derive a parameterization for E(.u!), one requires a treatment for initial
conditions.

This paper treats initial conditions for disturbances as random

variables. Conventional time series techniques that treat starting values
as known constants (usually chosen to be zero) result in inconsistent
estimates for the parameters of the error process if applied in a panel data
analysis where T is fixed because, in contrast to time series analysis) initial

conditions do not become "irrelevant" as the sample size increases. Similarly,
time series procedures that "backforecast° or treat initial conditions as
parameters introduce an incidental parameters problem in a panel data analysis
which under most circumstances also leads to inconsistent estimates for all

parameters of the error proeess) Treating initial conditions as random
variables avoids problems with inconsistency by introducing only a finite

number of new parameters: those determining the distribution of the starting
1This problem of incidental parameters and inconsistent estimation is
examined by Andcrson—flsiao (1981)

18

values, and those relating the distribution of the starting values to the
distribution of disturbances realized in periods 1 through T.
There are several complications associated with choosing a distri-

bution for the two sets of initial conditions specified above. If we assume
that the stochastic process generating disturbances during the sample period
is also operative prior to this period, then one would expect the v.(t)'s,

,0,

to be not only correlated with one another and with the c.(j)'s,

2. =

l—p,.

j =

l—q,.... ,0,

..

but also with all the v(t)'s realized after period 0.

Further-

more, the correlations relating these variables will, in general, depend
directly on parameters of the ARNA process given by (6), and one must account

for these restrictions to achieve efficient estimation. It is possible to
minimize these complications by specifying the system of equations given by (10)
and considering an alternative expression for
A moving average representation of an ARNA process provides the basis
for this new expression for v.,. Assuming the multiple time series process

given by (7) started sometime in the finite past, say between the periods

0,

-u—b and r with b >

after period t

as

it is possible to write each of the '.(t)'s realized

a moving average scheme of the form
t--+b

t—i—1
(11)

v1(t)

'

j0

Kci(t_i)

+

where

K0 I g
K1 = N1 - A1
K2 = N2

-

-

A2

A1K1

jl A3_

K.f.(tj)

19
and the fjt)'s, z

= rb,. ..,r,

are error vectors distributed independently

of ci(t*) for all t* > t. Formally, one can derive a relation like (11)
p
by starting with the ARMA representation j1(t) = — E A.u(t—i) +
q
jl
Z M.c(t—j) and successively substituting out for past v1(t—j)1s using
j=O
their ARMA representations until t—j = r• One can readily verify that (11)
is indeed a valid expression for vi(t) since premultiplying this equation

by A(L) yields A(L)vjt) =

M(L)ci(t)

for t > T+q.

The .(z)'s in (11) may

be interpreted as the true initial conditions of the ARMA process. Specifying
the distribution of these variables determines exactly how and when the ARNA
process generating v1(t)'s began.

Using (11), it is possible to reformulate the system of equations

given by (10). To avoid the need for dealing with several possible cases, it
is convenient to introduce the notation K. = 0 for j < 0 (for j > 0, K. is
c

defined in (11)) and the definition that a summation of the form Z is equal
to zero whenever c < 0. Using this notation, equations (10) and (11) imply
-

p

E A.i(T_i)

-q
E M.c(T—j)

j=O

j=O

p

q

Z A.v.(p+l—j)
j=0

E M.E.(p+1—j)
1

0

0

j=0
q— 1

(12)

v.(p)
1

Z K.1(p—j)

+ n.(p)
1

j=O
q—l

n(p—l)

j=o

q—l

vi(l)

jO

K. 41c1(p—j)

nJl)

20

where
t—r+b

t—-r—l

K4c..(t—j) +

ru(t) =

jt—p+q

t =

K.f.(t—j),

I,.. .,p.

j=t—t

The first set of T—p sets of equations in (12) are simply the standard

process generating

representation of the ARMA

,v(T), and the

second p sets of equations are the moving average representations of the

ARNA process for vjl),.. .,v(p) with the n,(LYs, i =

1,... ,p,

defined

to include all disturbances realized prior to period p—q+l (i.e., the
c.(t)''s and the f.(t)'s for t c p—q). The formulation of (12) assumes
that T < p—q.

(13)

F .

1

In matrix notation, (12) may be written as

= [ii]
lii

with
cJT)

v.(T)

i

rijp)

TL=
i

1
v..(l)

c1(p—q+1)

and F is a gT x gT matrix and C is gT x g(T+q) matrix defined a
Fl

=

F =

F

21

F

22

I
0

I

gp

1This assumption concerning the starting time of the ARNA process
generating the v.(t)'s is a weak restriction and follows inmiediately from
the assumption that v1(pf 1) can be represented by the specification given

by (5). This restriction on T ensures that no fjt)'s appear in the moving
average component of (5) for t = p+l.

21

1l

N

012

0

where: F1 =

A is a g(T—p) x g'

K
—

0

23

0

I

gp

block diagonal band matrix with

the matrix [A0,... ,A] running down the diagonal; F21 = 0 is a gp x g(T—p)
matrix; F22 =

'gp

is an identity matrix of dimension gp; 011 = M is a g(T—p)

x g(T—p+q) block diagonal band matrix with the matrix [M0,.. Nq] running
down the diagonal; 012 = 0 is a g(T—p) x gp matrix; 021 = 0 is a gp x g(T—p)

matrix, 023= 'gp; and 022 =

Kis

a gp x gqwith

K

...

1

K

—l

K
1—p

K

0

K

K

K
q—l

q—2

...

K

K
q—2

q—3

...

K
K
q—p—l q—p

2—p

When forming the partioned matrices associated with F and C, the above
analysis assumes that any matrix with an implied dimension equal to zero is

deleted from the specification. Thus, when p = 0, F = [F )
1
and when q = 0, K is eliminated and G HC"

C

L21

23j

ro

for

K is written

process

in

)0

C =

[011 1;

The above specification

general terms to handle all possible orders of the AR}IA

including p c q, p =

# 0 for j

C

and

q, and p > q.

Recognizing

= 0 for .j < 0 and

reveals that K has a fairly simple form with nonzero elements

in those K's on and above the positions CL, 1), (2, 2),. ..,(min(q,p), min(q,p)),

and zero elements in those K's below and left of these positionsJ
'Formally, the matrix K has [K0,

K1,..I,K

1

as its first set of

22

Given the expression for v. implied by (13), the problem of parame-

terizing E(v.v) becomes one of specifying a correlation structure for the
disturbance vectors c. and

Since each of the components of

follow

a white noise error process, we have

E(c.c) =

(14)

where E =

(IT_p+q

E(c.(t)c(t))

® E)

for t =

p—q+l,... ,T and * is

defined by the

Kronecker product. Inspection of the formula for the ri.(i)'s reveals three

facts: (i) the n.(Z)'s depend on a common set of disturbances; (ii) all of
these disturbances are realized prior to period p-I-q—l; (iii) included among

these disturbances are the initial conditions for the ARNA process (i.e.,

the f.(t)'s). Since each of the components of c. are realized during and
after period p+q—l, fact (ii) implies E(ri.c!)

the components of

0. Fact (i) implies that

are mutually correlated, so E(n.ri') contains no zero

elements in general. In addition, without imposing rigorous restrictions on
both the number and the correlation structure of initial conditions, fact

(iii) indicates that no restrictions will exist on the form of E(O.T1). In
the general case, the

will possess an arbitrary covariance structure which

we may formally express as

(15)

E(n.n!)

A

where A is any positive definite symmetric matrix.
Combining the above results, we obtain the following specification
for 0

g rows,jO, K0,.. .,K —2 as its second set of g rows, and so on until the
q
th
set of g rows is reached
set of g rows is reached, or if q > p, until the q
after which the rows of K contain zeros.

23
0
o =

(16)

E(v1)

=

F1

C

Jo' F_i'

This parameterization imposes all of the restrictions implied by the
ARMA process unless one is willing to introduce precise information about
how and when this process started.
There are two modifications of the above parameterization for A that

may be useful in applied work. First, to simplify the construction of the
matrix K, one can replace each of the nonzero elements of K (i.e., all the
IC's,

j

>

0) by arbitrary parameters rather than using the coefficients of

the ARRA process and the formulas specified above to form these elements.
This modification avoids the need for imposing nonlinear restrictions, but
it introduces new parameters and reduces the efficiency of estimation.
The second modification concerns the parameterization of A defined

by (15). This matrix is purely a theoretical construct and represents
nuisance parameters. An unattractive feature of this parameterization is
that one cannot easily infer an approximate value for A using preliminary
data analysis techniques or estimation methods that do not require the full

estimation of a. The availability of approximate values greatly reduces
computational burden when used as starting values for parameters in a nonlinear

computer program which is invoked to estimate 0. A way around this problem

is to replace A by A* E(ninj) + (Iq®E) K' which is also only restricted to be
positive definite and symmetric. Substituting this new parameterization into (16)
implies

M
-

(17)

0=F

N'
—

[o'

NE*l,

[i- —1'
F

1

[0 K] y*

H'

24

According to this new specification for 0, A* E(v') where the vector
v'= ('uj(p),... ,v(l)) includes the last p components of u. In contrast
to the previous paranieterization, Mis directly interpretable and can be
easily estimated prior to the full estimation of 0.
There are several ways in which the above specification of i3 can

be generalized in the analysis of panel data that are not possible in
'standard time series analysis. Each of these generalizations involves a
form of nonstationarity.

First, there is no requirement for the roots of the autoregressive
matrix lag operator (i.e., the roots of (A(L)) = 0) to lie outside the unit

circle. Thus, it is possible to consider such error processes as random
walks when using panel data. Whereas in a time series analysis the existence
of such nonstationarityhas significant consequences on the asymptotic
porperties of estimators, it has no such effects in the case of panel
data where asymptotic results rely on a large number of individuals rather
thai a large number of time periods.

Second, it is possible in the analysis of panel data to permit
the white noise vectors, c.(t), to be heteroscedastic over time, which

introduces yet another form of nonstationarity. To account for this
heteroscedasticity in the above analysis, one only needs to define E* appearing

in (14) as a block diagonal matrix of the form Dia(ET. 'T—p—q

E(c1(t)c(t)).

=

In standard time series analysis this sort of nonstationarity

does not necessarily create any conceptional difficulties, but it does require
an explicit paratneterization of the suspected form of the heteroscedasticity that

avoids an incidental parameters problem. In the case of panel data, however, it
is possible to allow for arbitrary forms of heteroscedasticity of white noise
disturbance vectors over time.

25

A third form of nonstationarity permitted in panel data analysis
concerns the constancy of the autoregressive and the moving average matrix
lag operators appearing in the multiple time series error process given

by (6). The matrices A(L) and N(L) can be allowed to vary arbitrarily
across periods so that there is a new set of autoregressive and moving

average parameters for each t. To modify the above analysis to account
for A(L) and N(L) being period specific1 one must subscript the A., the
M., and the K. matrices appearing in the specifications of F and G defined

by (13) to indicate the time period each matrix is associated with. This
subscripting has the effect of relaxing the equality constraints across
the rows of the matrices A and N, and it essentially voids any constraints
relating the nonzero elements of K to one another or to the coefficients
of the ARMA process.

A Reduced Form
In the following analysis on estimation we require a reduced form

specification for the simultaneous equation model given by (B). Write
this specification as

=
11X1

i=j

(18)

c

E(V4V') =
s

where

+ VI

otherwise

is a vector that includes all the endogenous variables appearing

in (8), the vector

contains all exogenous variables for each period

including lags, II is a coefficient matrix, and V. is a vector of distur—
1

bances.

26

The various specifications of the DSEM considered above imply

different restrictions on the II
simultaneous

and

the 2 matrices. If considering a non—

specification (i.e., when r(L) =

I

and 14(L)

0 in (2)),

then (8) is obviously its own reduced form which implies TI =

If

B and £2 =

S.

considering the special simultaneous specification where there are no
and no lagged endogenous variables

right—hand—side endogenous variables
(i.e., when F(L) =

F0

and ?(L) = 0 in (2)), then t

is

a nonsingular matrix,

and (8) can be solved in the usual way for the reduced form by premulti—
plying through by Li which implies TI =

and

£2

£1 OFl

If analyzing the general specification of the TISEM, however, (8)

does not constitute a complete system of equations in the sense that there
are more endogenous variables than there are equations; so, it is not possible to solve (g) for reduced form specification and determine the restrictions on II and £2 without introducing additional equations. The strategy

followed here to add the needed equations is the one normally used in
limited information analysis of simultaneous equations; namely, a prediction
ecluation is introduced for each endogenous variable that isnot determined

by the structural model under consideration. There are two sets of endogenous
variables that are not directly determined by the DSEM considered above.
The elements of the vector

defined 'by (B) constitute the first set,

ralized prior to period 1 (i.e., the initial

and the elements of the

conditions for the Yii(t)'s) make up the second set which we group into the

vector

=

(Yj1(o). .. ,Y'j(l

are determined by the equations

ii

=

(19)

Y.
2x

2xi

+

—

n)).

This study assumes that these variables

27

where H is an unrestricted coefficient matrix, and V 2i is an error vector
2
that is uncorrelated with all the elements of X1. These specifications
for prediction equations are not restrictive. It is always possible to
define the disturbance vector V

2i

so that it has zero mean, and it is

uncorrelated with all the exogenous variables of the model. There is no
guarantee that the covariance matrix of this error vector will be independent of X.,, but most of the procedures described below do not require the

assumption that V2. is homoscedastic across individuals. We maintain this
assumption only to simplify the exposition.
Combining equations (19) with the structural model given by (B)
implies a complete system of equations that can be solved for a reduced

form like (18) with Y =

(Y.,

are defined by (8)J

Y.) where Y. and

The parameter constraints implicit in (8) translate into restrictions on

the Ji and the 2 matrices. These restrictions can be shown to take the

and

following form. Partitioning 11 =

12 =

2

ll 122 where
21

the unrestricted set of coefficients and E(V .V' .) = 12

2i 2i

covariance matrix associated with (19), we have ll =

r1or'1 —

— 12i2!t

—

F_ly12!tr?l

112 is

22

22

is the unrestricted

r1(I —

and 1212 =

1221

!2' ll

=

is an unconstrained

matrix. These restrictions, of course, collapse to those presented above
for the special cases of the PSEM.

1The complete system of equations can be written as II?. = CX. +

f

The reduced form is obtained by'
J, C = [J and
{
[iJ.
premultiplying by H which implies the restrictions H =
=

where H =

H

B

(

I )H

1

C and 12 =

28

It. Techniques for Data Analysis

This section develops simple procedures that provide the basis for
choosing the orders of the lag polynomials appearing in the above specif 1—

cations and for determining whether or not it is reasonable to assume that

the coefficients of these polynomials are constant over time. These procedures prevent a researcher from choosing a model specification that is

broadly inconsistent with the data. Methods for choosing an appropriate
specification for the lag polynomials determining distributed lag relationships (i.e., F(L), t'(L) and B(L) in (2)) are a by—product of results contained

in the next section on estimation and will be discussed

there. This section focuses on the more complex problem of correctly speci—
fying the form of the autoregressive and the moving average lag polynomials
generating the ARNA component of the error process given by (6).

The two principle items used in the time series literature for
choosing the specification of an ARMA model are the sample covariogram (or

correlogram) and the sample partial correlation function) To provide formal
definitions of these concepts, let 13(t) denote a random vector which is

generated by some time series process. Given a sequence of realizations of
13(t), the covariogram is a plot of the covariance or autocovariance between

any two elements of this sequence as a function of the number of time periods

between realizations. The kth order matrix of autocovariances is.E(U(t)ut(t_k))
Plotting this matrix as a function of k creates the covariogram. The (j, 2-)
element of this plot, E(U.(t)U2-(t—k)), is called an own covariogram when

j =

Ic

and cross covariogram when j # k. The partial correlation function,

1Granger—Newbold (1977, Chs. 3 and 7) provide an extensive discussion
of how to use the covariogram and the partial correlation functions to build
time series models. Nelson (1973) provides a more elementary discussion.

29

on the other hand, is a plot of the partial correlation coefficients against

the length of the lag between random variables. The kth order matrix of
partial correlation coefficients denoted as A.K is defined by the regression

k

equation U(t) =

—I

A.U(t—j) + c(t) where E(c(t)U'(t—j)) = 0 for j =

j=i.
Plotting Ak as a function of k produces the partial correlation function.
The covariogram is particularly useful for identifying the presence of a

moving average process. The partial correlation function greatly aids in
identifying an autoregressive process.
This section formulates simple procedures for estimating the

covariogram and the partial correlation function using panel data. The
discussion develops these procedures for the analysis of the time series
properties of the distrubances 13(t) appearing in the specification of the

DSEM given by (2) and (8).2 Thus, it provides information that is useful
when choosing a specification for the error process given by (4) and (6)

To simplify the following exposition, the data analysis procedures are
formulated for the situation in which a researcher is investigating the

properties of a distrubance from a single equation (i.e., g = 1 in (1)),

which implies that 13(t) is a scalar. These procedures, however, immediately
generalize to the multi—equation case.
1Pormally, A cannot be interpreted as partial correlation coefficients
unless the stochastic process generating U(t) is stationary.

2When considering a simultaneous equation specification, it is
matrices in (B) are
and
Implicitly assumed that the parameters of the £
identified and can be estimated without using any covariance restrictions. Under
these circumstances, it is possible to obtain consistent estimates for each U(t)
using standard two or three stage least squares procedures which neglect covariance
constraints. If this assumption is violated, then it is not possible to directly
analyze the time series properties of the structural disturbances, and one must
apply the following data analysis techniques to the reduced form disturbances
given by (17).

30

Procedures for Estimating the Covariograin and the Partial Correlation Function

Suppose, for the moment, that one knows the true values of the

disturbances. The discussion below shows how these disturbances can be used
to estimate and test hypotheses concerning the form of the covariogram and
the partial correlation function using a standard seemingly unrelated
regression framework.
To construct the sample covariograrn, we require estimates of auto—

covariances for each order or length of lag. To estimate the kth order
autocovariance, consider the following set of regression equations

Ujt) Ujt — k

(20)

where Ujt) U.(t—k)

)

= 0kt

t =

+ iLjt)

is a dependent variable,

is a parameter, and .(t) is

an error term distributed independently across individuals. Stacking these
equations for a given individual yields a seemingly unrelated regression

model of the form

U(T) U.(T—k)
6k +

Ujk+1)Ujl)

(21)

E(4i)
where 6' =

is

(ekT...ek(k÷l))

C

i:=j

0

otherwise

=

is a vector of intercepts, ip'!

=

a disturbance vector, and we have implicitly assumed that the fourth moments

of iJi(t) exist and are constant across individuals. The intercepts of these

31

equations ek = E(U1(t) iJ(t-k))
associated with periods t, t —

represent the kth order autocovariances

k+l,,.

.

,T.

Thus, estimating equations like (20) or (21) by least squares or
joint generalized least squares using data on individuals provides all the
information needed to construct the sample covariogram and to test hypotheses

concerning its form. Unconstrained estimation yields multiple estimates of

ktF order autocovariances (ie one for each period t =

k+l,...,T)

which

reflects the fact that in a panel data analysis one can permit the parameters

of time series processes to be different in each period.1 Constrained
estimation, on the other hand, of seemingly unrelated regression model given
by (21) restricting the intercepts to be constant across equations (i.e.,
constraining the elements of ek to be the same) produces a unique estimate

of the kth order autocovariance which uses all available data. Estimating
models like (21) for each k, then, with equality constraints ott intercepts

yields unique autocovariances for each order. Plotting these constrained
estimates against k creates the sample covariogram.
Combining the system of equations given by (21) for all values of

k yields a model of the form
(22)

St(U1U) =

6 + C1

i

T

i=j

0

otherwise

1,...

'As discussed in the previous section, in a panel data analysis where
asynptotics rely on a large number of individuals, it is possible to allow the
autoregressive and/or the moving average lag polynomials (i.e., A(L) and 11(L)
in (6)) to differ in each period, or allow for heteroscedastic white noise
over time. Permitting variation in parameters of this sort generally implies

that the 8kt are different for each t.

32

where St(•) denotes an operator that stacks the rows of a matrix
and deletes

St(t(t31U1))

all the elements that lie below the diagonal, 6 =
is a vector of intercepts, and

a

St(s)

is an error vector that

contains the J1(t) disturbances appearing in (20) for all values of k.
This expanded seemingly unrelated regression model provides a framework
in which one can simultaneously estimate or test constraints involving auto-

covariances of different orders. An especially interesting hypothesis in
this regard is stationarity of the error process which implies that Q is a

Topletz matrix.) This hypothesis translates into simple equality constraints
relating the elements of a which are easily tested jointly using standard
generalized least squares procedures applied to (22).

To construct the sample partial correlation function we require

estimates of partial correlation coefficients for each order. To estimate
the kth order partial correlation coefficient, consider the following set
of regression equations

(23)

U1(t) =

matrix Q is

U(t—l) +

+ kt U.jtk) + ejt)

t =

a Topletz or a block Topletz matrix if it has the form

ab cd e

babcd
cbabc
d cb ab

edcba
If a, b, c, d, and e are constants, then Q is a Topletz matrix. If a, b, c,
d, and e are matrices, then Q is a block Topletz matrix with a, b, c, d, and
e as its submatrices. When lJi(t) represents a univariate time series, which
is the case considered here (e.g., g = 1), stationarity implies that autocovarlances of a given order are constant, or equivalently that 0 is a Topletz matrix.
In the multivariate case (i.e., g > 1), stationarity of Ui(t) implies that 0
is a block Topletz matrix.

33

where the p's are parameters and ei(t) is a disturbance distributed
independently across individuals. Stacking these equations for a given
individual yields an equation system of the form

13(T)

U.(k+l)
(24)

p.+e.1

•

•

•

0

U (T—j)

k

jl

i=l,...,N,

.

0

R

i=j

0

otherwise

U1(k—j+l)

E(e.e)

where

p

JT""J(k+l)' j =

l,...,k,

are parameter vectors and

(ejT),. ..e,(k+l)) is a disturbance vector. The parameters kt represent
the kth order partial correlation coefficients associated with periods t,
t =

k÷l,... .,T.
Estimating the parameters of the seemingly unrelated regression

model given by (24) for the different values of k, then, allows one to form
the partial correlation function and to test hypothese relating to its structure.
Unconstrained estimation yields estimates of partial correlation coefficients

that are period specific. Estimating the parameter vector

constraining its

elements to be equal creates a unique estimate of the kth order partial correla-

tion coefficient. Graphing these constrained estimates for k against k produces
the sample partial correlation function.

34

Using Residuals in Place of Disturbances

An apparent difficulty with the preceding discussion is that one
does not have the true values of the disturbances available for data analy-

sis. This turns out, however, not to be a problem. All of the estimation
procedures and properties of estimators described above remain valid if
one uses consistent estimates of the disturbances in place of the true

values. Thus, one can use residuals and standard seemingly unrelated
regression packages to estimate and to test hypotheses concerning the forms
of the covariogram and the partial correlation function.
Verifying this proposition requires two theoretical results.
Letting Ti. denote the vector of residuals associated with the stacked repre-

sentation of the PSEM given by (B),1 the needed theorems are

-plim [

(25)

N

N
—

E

i=l

E

U.U!) = 0

i/Ni=]. -'

and
N

N

(26)

A

1

i=l

i=l

AA

A

vec(U1IY) vec(IJ.U!)'] = 0

vec(TJ.U!) vec(U.U!)' —

plim [

11

where vec(•) denotes an operator that stacks the rows of a matrix into a
column vector. Proofs of (25) and (26) are presented in Appendix A.2

1

A

A

1', v and

A

A

A

BX,. The estimators
—2i——1
i
—ii
B are assumed to converge in probability at a rate so that U. — U.
A

The

residuals are defined as U. = F?

—

is

(Nh) for h c 1/2 which is satisfied for familiar estimators (e.g., least
squares, generalized least squares, two— and three—stage least squares, maximum
likelihood, etc.).

2Similar results are proved by Hannan (1970, Ch. 7) who considers the
use of least squares residuals from a time series regression to estimate auto—
covariances and the spectrum.

35

These theoretical results imply that replacing U1 by U in regression
models (20) — (24) yields estimators with the same asymptotic properties as
those computed using the true disturbances. Proof of this proposition is
presented in Appendix B. The central fact used in this proof is that all
unconstrained and constrained joint least squares or generalized least
squares estimators of models (20) — (24) are linear functions of the matrix
N

I JiM!, and their asymptotic normality depends on the large sample behavior
i 1

of this matrix multiplied by the normalizing constant .

Similarly,

when

residuals are used in place of true disturbances, the asymptotic normality
of the new estimators depends on the behavior of the matrix

-

N,.,.
z

,/Ni=l

U.U!.
1

Condition (25) guarantees that the asymptotic distributions of

these new estimators is the same as the estimators computed using true disturbances. Condition (26) guarantees that use of standard techniques to
compute the covariance matrix of estimators based on residuals and on true
disturbances yield equivalent results.
Therefore, when residuals are used to estimate either model (21), (22)

or (24), all parameter estimates, standard errors, and test statistics
reported by a standard seemingly unrelated regression package are asymptoti-

cally valid! Constrained estimation of models like (21), (22) and (24) using

1Notice that these results do not require any special distributional assumptions other than the existence of fourth moments and the constancy of moments across individuals. More precisely, the application of
the central limit theorems requires the existence of any absolute moment
greater than fourth order.

36

residuals, then, offers a simple way to estimate the covariogram and the
partial correlation function and to test hypotheses concerning their

structure. It ia also possible to construct estimates of the correlogram
which is another data analysis tool found in the time series literature.
The correlogram is like the covariogran except that it is a plot of the

autocorrelations instead of the autocovariances. It is often used instead
of the covariogram in time series analysis because correlations are unit
free and normalized to lie between —l and 1, and, so, they are more easily

interpreted. Using the estimated values of ek from equations (21) or (22),
th

an estimate of the k order autocorrelation coefficient equals Sk =
where a is a vector defined by ci' =
C

°k'

h()

0o and h is a function defined

Qs. A

standard application of stochastic limit theorems implies
0
is approximately normally distributed with the true value of the

by h(a) =
that

kth order autocorrelation coefficient as its mean, and a variance given

o o where S is the covariance matrix of the estimates contained

by -

S

in c. Thus no further estimation is required to compute estimates and
standard errors for the correlogram.

Using standard seemingly unrelated regression packages and residuals,

then, one can test between completing specifications of the time series process

generating the structural disturbances. If, for example, the estimated covar—
iogram is not significantly different from zero after a short lag, then a

pure moving average process is implied. In this instance, if we further
test to see whether autocovariances are constant across years, we can deter-

mine if the white noise error process is homoscedastic over tine. By
testing further to see whether autocovariances are constant across samples

composed of individuals of a given characteristic, we can determine if the

moving average process is the same across individuals. If, instead, the

37

covariograri converges to a positive constant after a short lag, then the

error terms may be generated by a moving average process and a permanent
component.

In a more general context, testing among various specifications of

Ok'S in (21) or (22) and k' in (24) allows one to test for a pure moving
average process, a pure autoregressive process, and many kinds of mixed

processes. One can also test for several forms of nonstationarity and
heteroscedasticity. One can distinguish between a fairly wide class of
alternative specifications of the error process if one analyzes the data
in first and second differences along with analyzing the data in levels.
As in standard time series analysis, identifying the specifications of the
Ok'S and the Pk'S that are consistent with the data only narrows the class

of models one needs to consider. Rarely does this type of data analysis identify a unique specification. Several models will often explain the data just
as well. -

In

the analysis of panel data, this is likely to be even more of

a problem because there is typically available only a short time series.

38

Ifl. Estimation Methods

This section presents methods for estimating time series models
that are especially tailored for panel data where T is fixed and asymptotic
results depend on large N = number of individuals. These estimation

methods are very general; given all equations are linear in the variables,
they can be applied to estimate any simultaneous equations model that
involves any set of nonlinear restrictions between parameters including

covariance restrictions. Two kinds of estimation procedures are considered:
the first is "least squares methods" which includes generalized, and two

and three stage least squares procedures; and the second is "quasi—maximum

likelihood methods. For each set of procedures, we consider both limited
and full information methods.

The following analysis does not present any formal identification

conditions. For the standard multivariate AENA model, Kashyap—Nasburg
(1974) develop necessary and sufficient conditions for identification.

liannan (1969) presents sufficient conditions. These conditions are not

easily applied in practice. Panel data introduces additional complications.
The length of the time series becomes a crucial factor. The treatment of
initial conditions reduces the effective length of the panel and at the

same time introduces new parameters. Notice, on the other hand, that
adding permanent components to a multiple time series model does not

complicate the identification conditions. First differencing equations
eliminates permanent components, and it does not introduce any new
parameters.

The standard identification criteria can be applied directly

to the first—differenced specification of the model. Introducing

39

permanent components, then1 has the effect of reducing the length of the

time series by one period. Identification will be lost only in those
cases in which the orders of the autoregressive and the moving average
components are sufficiently high to make the length of the time series
a crucial factor.

The statistical models proposed in Section I involve two sets of

parameters: the first set——hereafter called the structural coefficients——
consist of all those coefficients appearing in the stacked representation
of the DSEN by (9) (i.e., the elements of 1',! and B);1 and the second set——

hereafter called the covariance parameters—-includes those parameters
involved in the specification of the covariance matrix of the disturbance
vector in (8), ® =

E(U.U!)

(i.e., the coefficients of the autoregressive

and moving average lag polynomials and the elements of the covariance
matrices of the white noise error vectors and initial conditions).

Least Squares Methods

Three estimation procedures based on "least squares methods" are

proposed below: one to estimate the set of structural coefficients,
another to estimate the set of covariance parameters, and a third to
estimate both sets of parameters simultaneously.

Standard procedures can be employed to estimate the set of struc-

tural coefficients. If one is analyzing a nonsimultaneous specification

11iopefully this terminology will not result in any confusion. When
and
analyzing a nonsimultaneous specification of the DSEM (i.e., I' =
= 0), the set of structural coefficients obviously just Includes regression
coefficients.

I

40

of the OSEM (i.e., F =

I

and

0 in (8)), then joint generalized least

squares procedures that permit the
imposition of equality constraints across equations can be applied to (8)

to estimate these coefficients (i.e., the elements of B). If, on the
other hand, one is analyzing a simultaneous equation specification of the

DSEM, then two

or three stage least squares methods that allow for

linear restrictiors across equations can be applied to estimate the struc-

tural coefficients of (8)) Recall that the need for imposing equality
constraints across equations is a direct implication of the assumption

that distributed lag relationships are constant over time. All of the
above procedures may be classified as limited information in the sense
that they do not simultaneously estimate the parameters of the covariance

matrix 0. They require a consistent estimate of 0, but this obviously can
be constructed without directly estimating any parameters of the time

series process generating disturbances. In those cases where one chooses
not to introduce any assumptions regarding error processes or covariance
restrictions, the above procedures yield estimates of the structural
coefficients that use all available information and restrictions.
These estimation procedures provide a natural framework for performing preliminary data analysis to determine the length of the lag poly-

nomials associated with distributed lags (i.e., r(L), (L) and B(L) in (2)),
and to test whether or not the coefficients of these polynomials are constant

1The asymptotic properties of estimators obtained from these procedures do not require the assumption that the disturbances in the prediction equations given by (19) are homoscedastic. Thus, to employ these
simultaneous equation estimation procedures, one requires no assumptions
in addition to those presented in (8),

41

across periods. This form of data analysis involves standard tests of
linear hypotheses. Identifying the orders of lag polynomials involves
tests of whether coefficients on lagged endogenous and exogenous variables

are significant or not. Checking for the constancy of distributed lag
relationships over time involves tests of equality of coefficients across

equations. An attractive feature of the data analysis techniques is that
they can be implemented in complete ignorance of the stochastic process

generating error terms. An unconstrained estimate of the covariance matrix
can be used in the computation of coefficient estimates and test statistics.

The results developed in the previous section offer a general
method for estimating the set of covariance parameters.

The seemingly

unrelated regression model given by (22) is particularly well suited for
estimating parameters of the covariance matrix 0 =

E(U.U!) associated with

the stacked representation of the PSEN given by (8). If one assumes that
the components of U. are generated by the combined variance component—ARNA
error process given by (4) and (5), then as showr

in

Section I, the elements of 0 are functions of the parameters of this error

process where the exact functions are implied by the relations (16) or (17).

Substituting these functions for the elements of 0 =

St(G),

one can estimate

the entire set of covariance parameters using a standard nonlinear joint

generalized least squares procedure applied to (22). As noted in the
previous section, conditions (25) and (26) imply that residuals can be used in place

of the true disturbances as dependent variables in the estimation of a
model like (22) without any need for adjusting the output reported by the

computer package; all reported standard errors and test statistics are
asymptotically valid.

42

This, then, provides a simple method for estimating the parameters of any covariance matrix which must satisfy an arbitrary set of
nonlinear constraints. This procedure may be classified as limited inf or—
mation in the sense that it does not simultaneously estimate the set of

structural coefficients. One requires consistent estimates of these
coefficients to compute residuals, but these can be obtained from the methods

described above for estimating structural coefficients. The fact that one
is not required to estimate all parameters simultaneously is an attractive

feature of this procedure since it means that a researcher can concentrate
on correctly specifying and estimating the error structure for any
given

specification of DSEM.
Thus,

while the above methods for estimating structural coeff i—

dents permit ohe to neglect specifying the precise form of the covariance

structure,

this covariance estimation procedure allows one to

specification

ignore the

of the relationships between measured variables once it

has been chosen. This, of course, does not mean that the results of, one
procedure when used as input for the other will not lead to a different set

of parameter estimates. Using, for example, estimates from the second
procedure to construct a consistent estimate of S needed in the first
will in general produce different estimates of the structural coefficients;

and, in turn, using these estimates to form new residuals is likely to

imply different estimates for the covariance parameters. In theory,
however, these differences should not be statistically significant, and
there should be no changes in inferences as a result.

The two limited information procedures outlined above can be
combined into one that simultaneously estimates both structural coefficients

and covariance parameters. Stacking the system of equations given by (8)
on top of those given by (22) yields

43

F?.
—11

'YY .+BX

2' -i

=

i=l...,N

-I-a.
St(O)

st(U1U)
(27)

E(a.a)

H

i=j

0

otherwise

=

where a = (U!, C) is a disturbance vector. Substituting St(U.u!) for
St (U1Ji) (i.e., residuals for disturbances), it is possible to estimate

this expanded model by three stage least squares techniques that permit

the imposition of nonlinear constraints on parameters. In those cases
where one is not considering a simultaneous specification of the DSEM
(i.e., £

=I

and j' =

0),

joint generalized least squares procedures can be

applied instead to estimate (27). In Appendix B, it
that

is

shown once again

the treating of residuals as if they were the disturbances is

appropriate in the sense that all the output reported by the standard estimation
procedures

applied to (27) is asymptotically valid.

Simultaneously estimating structural coeffièients and covariance
parameters yields estimates that are more efficient than those obtained

from the above limited information methods. There are two sources of
increased efficiency. First, in those instances where third moments of
are nonzero (which implies that E(UiC) # 0 so Ii

E(a.a) is not block

diagonal), the estimates based on (27) will be more efficient than those
obtained from the above procedures for the sane reason that generalized
least squares estimates are more efficient than ordinary least squares

estimates. The second source of efficiency gain arises if there are any
constraints involving both structural coefficients and covariance parameters;
it is possible to impose these restrictions when estimating (27).

44

An important assumption maintained in the above discussion of
"least squares methods" is that all structural coefficients are identif led

without the use of any covariance restrictions. If this assumption is not
true, it is obviously not possible to estimate structural coefficients using
the first procedure, and without these estimates one cannot compute residuals

to serve as dependent variables for the second and third procedures. In those
cases where covariance restrictions are required for identification, one must

work instead with the reduced form specification of the DSEM given by Q8)
and apply the above procedures to estimate its parameters. The third full
information procedure then can be used to obtain a complete set of parameter
estimates.

Quasi—Maximum Likelihood Methods
The technique usually applied to estimate models where one is
interested in estimating parameters of a covariance matrix is the method

of maximum likelihood. Typically, a researcher assumes that disturbances
are normally distributed and, then, computes estimates by maximizing the

kernel of a multivariate normal density function. Such a procedure is
computationally efficient, and it can incorporate nonlinear constraints

involving both regression coefficients and covariance parameters. Below
we consider the application of these techniques to estimate structural
and covariance parameters.

The reduced form specification of the DSEM given by (18) provides
the appropriate formulation for application of maximum likelihood methods.
As outlined in the discussion following (18), the DSEM implies restrictions
on both the elements of the matrix of regression coefficients, 11, and the

parameters of the covariance matrix .

Denote these restricions by

45

writing 11 and 12 as functions of the form 11(y) and 12(w) where y and to are

vectors of parameters.1
The maximum likelihood estimates of these parameters are defined
as those values of y and w that maximize the function

N
(28)

12(w), Y, X) =

E

1=1

I

(-in

=

— in

12(w)l -

=

- in

j12(w)J -

=

12(wfl -

E

—

fl(y)X.)'(w)(Y.

(Y. -

n(y)Xj'c2(w)(Y.

(Y1

-

ll(y)x)
fl(x)x.)

tr[121(w)S(fi(y))

where

-

S(n(y)) =

=

(

E
iti

- E(y)X.)'
—

E

i1

N
Y1X)fl(y — neY)(I z x.Y!)
i=l

N
+

fl(y)(4

z
i= 1

and q. is the function of y, w, '1. and X. defined by the second expression
for

Under the assumption that reduced fort' disturbances are normally

distributed, it is well known that maximum likelihood estimates of the
parameter vector cx'

(y', w) in large samples are approximately distributed

according to a normal distribution of the form
1The following analysis does not rule out the possibility that y and
w contain conon elements.

is proportional to the
2The reader will inniediately recognize that
kernel of a multivariate normal density function. N' then, is the distance
function one would use to obtain maximum likelihood estimates assuming that
the reduced form disturbance vectors are identically and independently
distributed according to a multivariate normal distribution.

46
a 't

(29)

N(a0,

.i.fl)

a

where a denotes the true value of the parameters and C
0

1

Ba3a

is an

a
estimate of the matrix of second partials which is also known as the

information matrix. All maximum likelihood computer packages report standard
errors and test statistics based on (29), and it is these results that many
researchers use in their empirical analysis.

An apparent disadvantage of this method of estimation composed with
the least squares methods described above is that it relies on specific

distributional assumptions. The assumption that reduced form disturbances
are normally distributed, however, is not needed in order to use the
estimates produced by maximum likelhood procedures to make statistical

inferences. Below we briefly describe the properties of these "quasi—
maximum likelihood estimates" in absence of the normality assumption
and indicate how the output reported by standard maximum likelihood
computer packages must be modified to avoid specific distributional

assumptions.' The proofs of the results summarized below are contained
in MaCurdy (l9BOa), and the reader should consult this reference for
further details.

Under fairly weak conditions, it is possible to show that the
estimate of ci obtained by maximizing N defined by (28) is consistent
and asymptotically normally distributed even if reduced form disturbances

are not distributed according to a multivariate normal distribution. In
particular, it can be shown that the "quasi—maximum likelihood estimate"
of a in large samples approximately possesses a normal distribution of the
form

'In the absence of the normality assumption, estimates obtained by
maximizing N defined by (28) are commonly called "quasi—maximum likelihood
estimates" (see Malinvaud, 1966).

47

(30)

ci

N(ci0,

4 ç' c2

C;')

N q. q,
where C2 =

L

1=1

a

1
is the matrix of outer partials.

This result

ci

depends on exactly the same assumptions as those required to prove consistency
and asymptotic normality of the estimators based on least squares methods
described above.

According to the above results, a researcher may make incorrect
inferences if he uses output from a standard maximum likelihood computer

program and disturbances are in fact not normally distributed. Many of
the reported standard errors and test statistics are invalid without

normality.2 The correct asymptotic distribution is given by (30). If
the reduced form disturbance vectors are distributed according to a
multivariate normal distribution, then C2 =
and the covariance matrix

C1

in the probability limit

C2 C1' reduces to Cj which is the one

reported and used by most computer packages. Using instead cj C2 C11 —-

which is readily computable —— as the covariance matrix of a avoids the
requirement of any special distributional assumptions.

There also exists quasi—maximum likelihood techniques that can be
applied to estimate subsets of parameters analogous to the limited informa-

tion least squares methods proposed above. These estimation procedures are
1In addition to the assumptions implicit in the specification of the
reduced form given by(18), two conditions are required to prove this result:
(1) the absolute moments of the disturbance vector exists for any order greater

than fourth; and (2) pun

L X.X! exists and is positive definite. See
1=1

1 1

)laCurdy (1980a) for details.

2The standard errors and test statistics associated with regression
coefficients remain valid without the normality assumption. The standard
errors and test statistics associated with covariance parameters, however,
are all invalid. See MaCurdy (1960a) for details.

48

particularly useful if the aim of an analysis is to estimate only covariance
parameters (i.e., only the elements of 2 or w) or only regression coefficients
(i.e., only the elements of TI or y).

To estimate only covariance paramerers, it can be shown that
evaluating the distance function

defined by (28) at a consistent estimate

of TI and maximizing the resulting function with respect to w yields estimates
of the covariance parameters that are consistent and asymptotically normally

distributed. Evaluating the function N at TI, which
Is

consistent estimate for the true value of the regression coefficients,1

creates a new function

that looks like N except that the matrix S
I.

— 1 V.V is replaced by the matrix —

N.1 ii

N.1

V V where V. = '1.—
1

ii

1

TIX

i is a

vector of reduced form residuals. The function Q may be interpreted as a
"likehood function" that treats residuals as if they were the true values

of the disturbances. Given the same conditions assumed for each method
of estimatiob proposed above, one can prove that the estimates of w obtained

by maximizing Q in large samples are approximately distributed according
to a normal probability law of the form

(3l

w '

r
NW0,

where

1 -

32Q
awa'

_ll N

aq

E; iJ;

- 2Q —9

:J

is the true value of w, and qt is the function q. defined by (28)

evaluated at II.

Notice that the estimate of

derived from this computa—

tionally simpler procedure has the same asymptotic distribution as the full
Information quasi—maximum likelihood estimate for u proposed above, Thus,
there is no efficiency gain in simultaneously estimating either TI and cs or y and

1By "consistent" I mean that convergence in probability is o (Nh)
p

where h <

1

49

tii

when using quasi—maximum likelihood methods. These results are important

because they imply that a researcher can use maximum likelihood computer
routines to compute estimates of covariance parameters using only the matrix

of the outer product of residuals as input. Modifying the standard error
and test statistic output reported by this routine along the lines
described above avoids the requirement that disturbances are normally
distributed.

A quasi—maximum likelihood procedure analogous to the one
described above for estimating covariance parameters also exists for only

estimating regression coefficients. Evaluating the function N at a

consistent estimate of ,

rather

function with respect to -r

yields

than TI, and maximizing the resulting

estimates of the regression coefficients

that are consistent and asymptotically normally distributed, This proposi—
tion is obvious once one recognizes that this is completely equivalent to

a joint generalized least squares procedure. It is well known that the
estimates produced by such a procedure have the same asymptotic properties
as the full information estimates proposed above where one simultaneously
estimates all parameters.

Quasi—maximum likelihood techniques, then, offer an attractive

method of estimating the parameters of the OSEM and the error processes

considered in Section I. They are not only competitive with the above
least squares methods in terms of their computational efficiency, they
are also as robust in the sense that they rely on the same assumptions
as the least squares methods to product consistent parameter estimates

and to test hypotheses. For the nonsimultaneous specification of the DSEM,
the computationally simpler procedures that condition on consistent estimates

50

of subsets of parameters can be used without loss of estimation efficiency.

For the simultaneous specification of the model, one can apply the full
information method in which one simultaneously estimates all parameters and

imposes all constraints. This full information method not only allows one
to estimate all structural coefficients directly, it also produces more
efficient estimates if there are constraints involving both regression coefficients and elements of the reduced form covariartce matrix.

IV, Summary

This paper presents specifications of a dynamic simultaneous

equations model that can be applied to analyze panel data. This model
allows for generally specified error structures and rational distributed

lag relationships involving both endogenous and exogenous variables. One
has a 'wider choice of specifications in the analysis of panel data than

in standard time series analysis: one can permit parameters to vary freely
over time in a panel data setting; permanent components can be combined
with multiple time series error processes; and it is possible to relax
many stationarity and homoscedasticity assumptions maintained in time

series analysis. To derive explicit parameterizations for the covariance
matrix associated with disturbances, this study presents a general treatment for initial conditions in a panel data framework.
For purposes of data analysis, simple procedures for estimating the

covariogram and the partial correlation function are developed. These procedures use residuals as dependent variables in a seemingly unrelated regres-

sion framework. It is shown that the estimates based on residuals have the
same asymptotic properties as estimates based on the true disturbances.
Thus, using standard computer packages, it is possible to narrow the choice
of time series models and test among competing specifications.

51

General nonlinear estimation procedures are formulated to
estimate the full set of parameters of the dynamic simultaneous

equations model and the error process, Both "least squares" and
"quasi—maximum likelihood" methods of estimation are discussed.

These procedures permit any form of nonlinear relationship between
parameters in a simultaneous equation model, including restrictions
involving both regression coefficients and parameters of the covariance

matrix. Simple limited information estimators are proposed to estimate
only regression coefficients or only parameters of the covariance

matrix. All of these estimation methods generate estimators that are
consistent and asymptotically normally distributed without any specific
assumptions regarding the distribution of the disturbances.

52

APPENDIX A

The purpose of this appendix is to verify propositions (25)
and (26).

The disturbance associated with the stacked specification of
the DSEN given by (8) can be written as

Y.
ii
U. =
1

(r,
- —,
- —B)

1 .

2i

ER Z.

1

X.

1

where This defined as a matrix of coefficients and Z. is defined as the
1

vector of.observed variables for individual i. Let U. =
1

it

1. denote the
1

vector of residuals for individual i where R is a consistent estimator
for R so that R — R is o(Nk) for k c

plim(Nk(R —

R)) = 0).

(i.e., it— R is o(Nk) if

Defining vec(') as an operator that stacks the

columns of a matrix into a column vector, it can be shown that vec(P C Q) =

Q'®

vec(C). Thus,

vec(U.U) =

11

vec(RZ.Z
11

R')

(R®R) vec(Z.Z!)
E H

vec(Z.Z)
11

where the matrix H is defined as the Kronecker product between R and it.

The analogous expression for the residuals is vec(UT3!) = H vec(ZZj)
where H =

(R®R).

that H — H is

Since R —

it is o(Nk) for k <

4-, it

is easy to verify

53

Proposition (23) follows from the observation that

(A.l)

Plim{1 E (vee(U1U) -

=

plimj

(Ii —

H)

:ui}

E vec(Z.V)
1=1

plim

(B -

E vec(Z.Z)J

R)j iimJi
0 •

0

Plim{

vec(Zic)} =

1

where the last line uses the fact that H — B is a (N'42) for k c

which

k
implies VN (H — H) is a (N ),and the assumption that fourth

N
moments of observed variables exist which implies plim(- E z.z.) <

i=l

-

.

Noting that vec(vec(U.U!)vec(U.U!)') vec(B vec(Z.Z)vec(Z.Zfl'H')
=

(H&JH) vec(vee(Z1Z)vec(Z.Z)'), proposition (26) follows from the
-

observation that

(A.2)

vec(vee(U1V1)vec(U.U) ') ) }

plim{ E [vee(vee(UU) vec(U.UT)

= plim{i [(H®H) — (BØH)]
i=l

vec(vec(Z.Z!)vec(zz!)'))
ii

=0
A

where the last line uses the fact that ((H GB) —

3.
(BQH)) is o(N2')

and fourth moments of observed variables exist.

In Appendix B we require a generalization of (A.l). Let B. i —
denote a set of matrices satisfying the condition

1,..

54

(A.3)

plim

INE B1 vec(Z1Z). .c .
I.

We

1=1

J

have
N

N

B.(vec(U.1J) —
1=1

11

1

vec(U.U))
11

=

BJH — H)vee(Z.Z)
11
1

LI

i=1
N
=

vec(BJH — H)vec(Z.Z.))

LI

1=1

=

[l

(vec(Z.Z)' ØB.)jvec(H - H)

where the last line uses the matrix algebra theorem stated above. Thus,

(A.4)

plim

IN
LI

= plim

B.(vec(U.U) —
1

11

vec(UU))
11

E (vec(Z.Z)'

plim {

vec(

=0
where the last line follows from (A.3) and the fact that (H — H) is 001

55

APPENDIX B

The purpose of this appendix is to prove that when residuals are
used in place of true disturbances to estimate models (22), (24), or (27),
the output reported by a standard application of estimation procedures based

on "least squares methods" is asymptotically valid. To simplify the exposition, proofs are only given for the case in which there are no constraints

on parameters. Proofs of the following propositions when constraints are
present (including nonlinear constraints) involve more algebra, but they are
conceptually equivalent to those presented below.

Consider first the use of residuals in estimating model (22) and the

parameters of the covariance matrix. Since the seemingly unrelated regression
models proposed for estimating the covariogran given by (21) are nested in (22),

the following results apply to these models as well. Replacing disturbances
by residuals in (22) yields
(B.l)

St(U.U!) = a +

+ (St(UU) — St(U.TJ!))
T

i=j

0

i#j

=

where the .'s are independently distributed error vectors.
Estimating (B.l) by a joint generalized least squares computer program

yields an estimate of U equal to

(B.?)

6GLS = ULS =

st(uiU)

where we have used the fact that the generalized and the ordinary least squares
estimators are equivalent since all equations contain the same exogenous variables.

56

This procedure prints standard errors assuming the covariance matrix of
0GLS is

(8.3)

V(OCLS) =

T

1

N

-

E (St(U.U) — e5)(St(UjIfl)

—

and, it reports test statistics assuming that 6GLS is approximately normally
distributed with mean B and covariance matrix V(@GLS) or, equivalently,
(8.4)

BGLS

N(B,

Using (Li), (A.2) and (8.1), standard applications of asymptotic
theory yields
plim{T} = T

and

dlim {v (BGLS

.

= dim

I

= dim

J

N
E

{61i:ri

= N(O,

+

Ci

(St(U.U) -

E

St(U.U))}

1N

+ plim J

'J

Z

ii

(StOJ lID —

St(U.IJ.)')
11

T)

where dlim denotes convergence in distribution as opposed to pun which is
convergence in probability. So, in large samples, we have

N(e, 4
This result verifies that the estimator

'r).
given

by (3,2) is consistent and

the reported output of the generalized least squares procedure given by (8,3)
and (8.4) is valid asymptotically.

57

Consider next the use of residuals to estimate the seemingly unrelated
regression model given by (24) proposed for estimating the partial correlation

function. A convenient representation of (24) is

(B.5)

(IGIJ)J =

(I®U)Q

+

R

ij

0

i#j

E(e.e) =

where I is the identity matrix, J is a vector and Q is a matrix of known
constants, p

is!

a vector of parameters, and e. is an independently distrib-

uted error vector. With the appropriate choice of J and Q and dimensioning
of I, p contains the

partial correlation coefficients for a prespecified

order. Suppose, for example, we are interested in estimating the kth order
partial correlation coefficient. To do this, we set the dimension of I equal

(U.(T),..1,U.(k+l))';

to (T—k) define 5 so that (I(Tk)

=

treat Q as a block matrix of the form Q

dia(Q1,.. 'T—k where the T x k

matrices Q., j

l,...,T—k, are defined so that UQ.

and,

(U(T—j),...UJT—j—k+l)).

The implied parameter vector for this specification is p' =

where

=

lt'- 'kt' =

k+l,.

.

- ,T, with kt being interpreted as the

kth order partial correlation coefficients associated with the period t.

As discussed in the text, constraining

to be constant for all t

produces

a unique estimate for the kth order partial correlation coefficient, but such
constraints are not explicitly considered here.

Using residuals to estimate model (B.5), the estimator for R and the
generalized least squares estimator for p are

58

=

E

+

(I®U1)(J +

CLS =

(I®U.)R(I®Uj)]Q1

IN

(IØui;(IØUj)}

where

LS
Using

=

['®! UiU}QJ

(aLl), we see that

[i®

is consistent for p. The properties of R and

N
(I®U.)C(I®U!) where C is a

CLS depend on matrices of the form
i=l

N

consistent estimate of some positive definite matrix C. The (g, h) block of this
-

quadratic form is
Since Plim(Cgh) =

c

Cgh

A

E UU where C h
h
i=l

is the (g, ii) element of C.

we know from (A.l) that this (g, h) block divided by

N or V has the same asymptotic properties as the (g, h) block of
N

E (I ®U.)C(I®U) divided by the sane normalizing factor. It directly
i=l

A

follows,

A

rN

then, that the quantities of R, GLS' LS' and Q'I

E

-

(I®U.)R

[i=l

I®UjQ have the same asymptotic properties as the analogous quantities
computed using the true values of the disturbances. Thus, standard applications of asymptotic theory yield the conclusion

(B.6)

CLS N[ F[1 (IUi)R1(I®Uj)]Q1].

The result given by (B.6) is exactly the one assumed by a joint generalized
least squares computer package when it reports output.
Finally, consider the use of residuals in estimating model (27)
which includes both structural coefficients and covariance parameters.
Replacing disturbances by residuals in (27), one may write this system of
equations as

59

0

XlI

0

y

Zi

+a.+

=

(B.7)

0

St(U.1J)
11

I

(St(U.U) -

e

11

St(U.U))
11

ij
E(a.a)
1J

=

ij

0

where Yj. =

(YJT)..

is a matrix

,Yj.(l)), y is a parameter vector1

containing both endogenous and exogenous variables (i.e. the elements of
Y1±, Y2±, and X.), and the disturbance vectors a1 are independently distributed.
-

= (!I' St(0.U1Y)
1

Defining I

L

0

0

I

1

WI =

,

and

6'

(i', 6')

a three stage least squares procedure applied to (B,7) yields an estimate of
6 equal to

N=
3LS

Wi

1=1

a.
1

W.

1

1

where W. is the niatrix WI with all endogenous variables replaced by their
predicted values,
—

N .E1

W62LS)(aI

—

W.6zs)'

and

-l N

N
6

=
2LS

E

1=1

W'W.

E W'.ct..

i=l

This procedure prints standard errors and test statistics assuming that

(B.8)

63LS '

j-l
N[65

c]l]

60

and 63Ls' observe

To determine the asymptotic properties of
that using (B.7)

=

62LS — 6)

N1

N
N
L Wa + 1(1 Z B (St(TJ ii
U )

1=111 11=1 ii

St(Uii
U'))

and

N
63LS — 6) = N1
2

where M =
1

Z
.

i=1

ii

E

WW., N =
2

WH a. + M E B .(St(U.U'.)
ii — St(U.U))
11
1
2 1=1 2i
1

N

1=1

ii

W'.N1 w.' B

E

.

1=1

1

1

.

= W'.

1

,

I

2i

Assuming the existence of fourth moments, it can be shown that
B1. satisfy

condition (A.3) of Appendix A and 0 <

PlimfS2LS) = 6 and plim{H} = H.
(e.g.,

l0

and B . = W!H

p1im{

N1) <

1

the

matrices

.

Thus,

Using standard stochastic limit theorems

Theorem 2 of Nann—Wald), it can be further shown that the matrices

B2. also satisfy condition (A.3) and that P = Plim{i N2) exists and is
positive definite. Therefore,
dlIm{61(63LS

(B.9)

6))

=

P dlim I

=

p1

(
dlim 1

Z

i

WH1
a +
1

. EN 1

P' plim

E B2i.stu.u:
ii

-

i

N(0, P1)

where the last lines use asymptotic results typically applied in deriving
the properties of three stage least squares estimators. Since the matrix

St(U.U)
11

61

L
N.i=1

i

W is a consistent estimate for P, (B9) implies that (B.S)is valid
1

as an approximation in large samples; and, so, the output reported by
standard simultaneous equation procedures is valid asymptotically.

62

REFERENCES

Anderson,

T. W. The Statistical Analysis of Time Series. New York:
John Wiley & Sons, 1971.

Anderson, T. W. and Hsiao, C. ttpormulation and Estimation of Dynamic
Models Using Panel Data." Journal of Econometrics, forthcoming, 1981.

Ashenfelter, 0. "Estimating the Effects of Training Programs on Earnings."
Review of Economies and Statistics 40, no. 1 (February 1978): 47—57.
Balestra, P. and Nerlove, N. "Pooling Cross Section and Time Series Data
in the Estimation of a Dynamic Model." Econometrim34 (July 1966):
585—612.

David, N. "Lifetime Income Variability and Income Profiles." Proceedings
of the Annual Meeting of the American Statistical Association,
August, 1971, pp. 285—92.
Friedman, N. and Kuznets, S. Income from Independent Professional Practice.
New York; National Bureau of Economic Research, 1945.

Granger, C.W.J. and Newbold, P. Forecasting Economic Time Series. New
York: Academic Press, 1977.

Griliches,Z. "Distributed Lags: A Survey." Econometrica (1967): 16—49.
Hannan, E. Multiple Time Series. New York: John Wiley & Sons, 1970.

"The Identification of Vector Auto—regressive—Moving Average
__________
Systems." Biometrika (1969): 223—25.
Hause, J. "The Covariance Structure of Earnings and the On—the—Job
Training Hypothesis." Annals of Economic and Social Measurement
(Fall 1977): 335—66.
Hussian, A. and Wallace, T. "The Use of Error Components in Combining
Cross Section with Time Series Data." Econometrica 37 (1969):
52—72.

Kashyap,

and Nasburg,
"Parameter Estimation in Multivariate
Stochastic Difference Equations.t' IEEE Transactions on Automatic
Control AC—l9 (1974): 784—97.
.

.

Lillard, L. and Weiss, Y. "Components of Variation in Panel Earnings Data:
American Scientists 1960—1970." Econometrica (1979); 437—54.

Lillard, L. and Willis, IL "Dynamic Aspects of Earnings Mobility."
Econometrica (1978): 985—1012.

