NBER WORKING PAPER SERIES

THE DETERMINANTS OF MISMATCH BETWEEN STUDENTS AND COLLEGES
Eleanor Wiske Dillon
Jeffrey Andrew Smith
Working Paper 19286
http://www.nber.org/papers/w19286

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
August 2013

We thank Dan Black, Sue Dynarski, Caroline Hoxby, Bill Johnson, Mike McPherson, Sarah Turner,
and Ophira Vishkin for valuable comments. We also benefitted from comments received at seminars
at Arizona, Carlos III / CEMFI, CESifo, Cornell, Georgia State, Miami, Michigan (CIERS), Michigan
State, Rochester, St. Gallen, Syracuse, Toronto, UC-Irvine, Virginia, and ZEW, and from participants
at the NBER Summer Institute and a session at the PAA meetings. This research was supported by
NSF #0915467. The views expressed herein are those of the authors and do not necessarily reflect
the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2013 by Eleanor Wiske Dillon and Jeffrey Andrew Smith. All rights reserved. Short sections of
text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.

The Determinants of Mismatch Between Students and Colleges
Eleanor Wiske Dillon and Jeffrey Andrew Smith
NBER Working Paper No. 19286
August 2013
JEL No. I23,I24
ABSTRACT
We use the National Longitudinal Survey of Youth 1997 cohort to examine mismatch between student
ability and college quality. Mismatch has implications for the design of state higher education systems
and for student aid policy. The data indicate substantial amounts of both undermatch (high ability
students at low quality colleges) and overmatch (low ability students at high quality colleges). Student
application and enrollment decisions, rather than college admission decisions, drive most mismatch.
Financial constraints, information, and the public college options facing each student all affect the
probability of mismatch. More informed students attend higher quality colleges, even when doing
so involves overmatching.

Eleanor Wiske Dillon
Department of Economics
W.P. Carey School of Business
Arizona State University
501 E. Orange Street
Tempe, AZ 85287-9801
ewdillon@asu.edu
Jeffrey Andrew Smith
Department of Economics
University of Michigan
238 Lorch Hall
611 Tappan Street
Ann Arbor, MI 48109-1220
and NBER
econjeff@umich.edu

An online appendix is available at:
http://www.nber.org/data-appendix/w19286

I. Introduction
Students graduating high school in the U.S. can choose to apply to and enroll in a
wide variety of colleges. In this paper, we investigate how students of varying
abilities sort into colleges of different qualities, with a particular focus on high
ability students who choose to attend relatively low quality colleges and low
ability students who attend relatively high quality colleges. The literature refers to
these choices as “undermatch” (think “underachiever”) and “overmatch” (think
“overachiever”), respectively. While we follow the literature in using the
normatively loaded language of mismatch, we do not take a stand here on the
causal effects of mismatch. Instead, we empirically investigate the determinants
of undermatch and overmatch using data on a recent cohort of college-goers,
interpreting our results within the context of an informal economic model of
college choice.
In our informal model, students face real tradeoffs between college quality
and cost due to state tuition policies at public universities and price discrimination
by private ones. They also worry about the potential benefits and costs of being a
relatively weak, or relatively strong, student at their college. A strong student at a
weak school may stand out and garner extra faculty attention, or she may exert
less effort due to the bad study habits of her peers. A relatively weak student at a
strong school may benefit from the extra resources and the strong peers, but may
also find herself overwhelmed by the pace of instruction and the level of
performance expected. Students also care about dimensions of the college
experience other than academics, such as following their high school friends,
attending the same school as their parents, or having a religious environment.
We study the sorting of students to colleges that results from students
making these tradeoffs, given the information and financial resources available to
them and to their families. These choices have important implications not only for

2

the students themselves but for the taxpayers who subsidize state universities and
pay for federal and state student aid programs.
Our work builds on existing research on the extent and determinants of
mismatch. Light and Strayer (2000, Table 4) and Black and Smith (2004, Table 4)
document the empirical importance of both overmatching and undermatching in
the earlier 1979 cohort of the National Longitudinal Survey of Youth. One line of
work considers overmatching resulting from affirmative action, e.g. Arcidiacono,
Aucejo and Spenner (2012), Bowen and Bok (1998) and Sander and Taylor
(2012). Another line of work considers undermatching, focusing specifically on
application behavior, as in Avery (2010), Griffith and Rothstein (2009), Howell
(2011) and Pallais (2012), or the recruiting efforts of elite colleges, as in Hill and
Winston (2010) and Hoxby and Avery (2012). Roderick et al. (2008) and Bowen,
Chingos and McPherson (2009) focus on the entire process, including college
completion. The literature broadly agrees on the empirical importance of both
overmatching and undermatching, and that most mismatch results from where
students apply and where they attend conditional on acceptance, rather than from
rejection decisions by colleges. The literature does not agree on the effects, if any,
of mismatch on academic and/or labor market outcomes.
Relative to the existing literature, we make several contributions. First, we
study a nationally representative sample of college-goers from a relatively recent
cohort using the data from the National Longitudinal Survey of Youth 1997
cohort (hereinafter NLSY97). The NLSY data have many advantages for our
purposes, including a moderately large sample and a variety of useful covariates
including student demographics, family background information and the Armed
Services Vocational Aptitude Battery (ASVAB). The restricted use data allow us
to match in contextual information on student’s high schools, on the census tract
in which they reside when making the college choice and on the state college
system they face, as well as giving us data on what colleges they choose to attend.

3

Second, we use a different and arguably superior definition of mismatch
relative to other studies. Our measure focuses on the difference between a
student’s percentile in the ability distribution, with ability defined based on her
performance on the ASVAB tests, and the percentile of her college in the studentweighted distribution of our college quality index. Third, we separately analyze
the determinants of undermatching and overmatching. Fourth, we look at both
application choices (though in less detail) and at the first college attended. Fifth,
we consider three distinct measures of academic mismatch.
Our approach yields several important findings. We find that substantial
fractions of students are both undermatched and overmatched. Perhaps most
surprising to us, student decisions drive mismatch in almost all cases. Most
students who mismatch either do not apply to a well-matched school or apply and
are admitted, but do not enroll. Some students appear to undermatch due to
financial constraints, as the probability of undermatch depends on parental wealth.
However, many of the factors we examine affect the quality of the college a
student attends regardless of her ability, rather than affecting mismatch. Students
from the wealthiest families, from neighborhoods where many adults have college
degrees, and from high schools where many students go on to college are less
likely to be undermatched but also more likely to be overmatched. This suggests
that more informed students think that the benefits of improved college quality
associated with overmatch overshadow any negative effects. Finally, features of
the state university system facing the student affect the probability of mismatch.
In particular, having a well-matched public university within 50 miles decreases
the probability of both types of mismatch.
The remainder of the paper unfolds as follows: In the next section we
outline an informal model of how students and their families decide which
colleges to attend. Section III describes our data and Section IV describes our

4

measures of student ability, college quality and mismatch. Sections V and VI
present our empirical findings and Section VII concludes.

II. College choice and college mismatch
This section provides the informal theoretical framework within which we
interpret our results. Our informal model draws most heavily on the formal
models in Light and Strayer (2000), Arcidiacono (2004) and Stinebrickner and
Stinebrickner (2012).
In reality, the process by which students are sorted into schools has several
stages and involves choices by both the student and the school. The student first
decides which colleges to apply to, then the colleges decide which students to
admit, and finally the student chooses among her offers of admission. Students
then enroll in their chosen college, learn more about their fit with that college, and
may decide to transfer to a different college or to drop out altogether. Our
discussion and empirical work implicitly collapse the first three stages into a
single choice by the student; we argue in Section V that the data support this
simplification.
We assume rational and forward-looking college applicants. Even among
such applicants, we expect some students to end up at colleges that do not match
their abilities for a variety of reasons, including information constraints, financial
constraints, and social considerations, such as where their high school friends
choose to attend. Lack of information on the part of either the student or the
school could increase the probability of both types of mismatch. The student may
not have complete information about the quality of different colleges, or about
how her abilities compare with other college applicants. We expect that, on
average, students with more educated parents, from better educated and better off
neighborhoods, and who attend high schools where more students enroll in a 4year college will have better information to guide their college choices.

5

If informed students prefer colleges at which they are well-matched then
access to information should lower the probability of mismatching in either
direction. However, students may believe, perhaps correctly, that the positive
effects of a higher quality college outweigh any effects of overmatching or that
the positive effects of being a big fish in a small pond outweigh any negative
effects of undermatching. In this case, students with better information about
college may be more likely to mismatch in whichever direction they perceive as
optimal. In contrast, in the spirit of Manski (1989), relatively uninformed students
might try out a college for which they are undermatched or overmatched partly in
order to learn about their optimal match.
A student’s application may be a poor signal of her actual ability for two
reasons.

It may randomly misstate her ability if, for example, she had a

particularly good or bad day on the SAT. Students may also attempt to
strategically misstate their ability with the help of SAT tutors and pricey
admissions consultants. If a college misinterprets the student’s ability it may
admit her to a school for which she is ill-prepared or reject her from a school that
would suit her.
In a basic framework where students make (what they perceive to be) the
best college match they can subject to their (and their family’s) budget, financial
constraints will tend to push students toward schools for which they are
undermatched, because more elite schools tend to be more expensive. In practice,
for strong students from low-income families the extra cost of a top school is
largely offset by financial aid, but students do not know their aid offers with
certainty when they are applying for schools (e.g. Avery and Turner, 2009).
Financially constrained students may also choose a nearby college to reduce
travel costs or avoid the cost of living away from home. Again, this will tend to
increase undermatching more than overmatching because the students have an
incentive to attend a closer school even if they are undermatched for it, but

6

schools generally have no incentive to accept weaker students just because they
live nearby.
Features of the state university system can also generate mismatch in
either direction. Most state colleges offer discounted tuition to state residents,
making them (often quite substantially) less expensive than other options. We
expect students to trade off lower price for match quality at the margin, increasing
the probability of mismatch for students who have no well-matched college
within their state university system. Because some state colleges are required to
have different admission thresholds for in-state students, students who constrain
themselves to the state system may end up either overmatched or undermatched.
Finally, students may appear mismatched with their college because they
based their choice on other factors. Students may choose a college that is good for
their major, for example engineering or art, even if it appears to be a poor match
on overall quality. Students may be recruited to colleges based on skills, such as
athletics or music, not included in our measure of ability. Students may choose to
go to the college that their friends plan to attend, or their parents attended, or
whose football team they like, or that provides a desired religious environment.
We do not observe these types of skills and preferences in our data, so we will
code the student as mismatched, even though she may have matched well in a
broader sense.

III. Data
We use the National Longitudinal Survey of Youth 1997 Cohort (NLSY97) data,
which samples the population of Americans born between 1980 and 1984. The
first interview was in 1997 with follow-up interviews each year since. The
majority of the sample graduated high school and made their college choice
between 1999 and 2002. 87 percent of the un-weighted sample graduated high
school or got a GED. Of these high school graduates, 38 percent started at a four-

7

year college after high school. We focus on students who start at a 4-year college
but also present analyses pooling 2-year and 4-year college starters. Appendix
Table 1 lists our sample restrictions and the associated sample losses. The
NLSY97 sample includes both a representative cross-section and an over-sample
of black and Hispanic youths. We combine these samples in our analyses. We use
probability of inclusion (in the overall NLSY97 sample) weights, constructed by
the NLSY, to combine the two samples, and also to control for differing sampling
and response rates in different regions of the U.S. and by age, gender, raceethnicity groups.
One of the main strengths of the NLSY97 data lies in the rich set of
individual and family covariates it provides. Using the restricted access geocode
data provides additional information on the identities of colleges attended and
allows the use of contextual information based on the respondent’s residential
location. Appendix Table 2 defines the variables we use in our analysis, which
also include some variables from the double-secret high school survey data, which
can be accessed only at the Bureau of Labor Statistics offices in Washington, DC.
Many of the variables we use have modest amounts of item non-response.
Rather than do listwise deletion of observations when an independent variable is
missing, which would cumulatively result in massive sample loss, we recode
missing values to zero and include an indicator variable for missing values in our
multivariate analyses.
We mostly use standard variables and variable definitions that do not
require additional discussion here. Exceptions are the constructed ability, college
quality and mismatch variables considered in detail in the next section, and the
NLSY97 measures of family income and wealth. The NLSY measures these
variables at a single point in time, namely the 1997 interview. As a result, they get
measured at different ages for different respondents. In addition, they include only
income and wealth for the household in which the respondent resides. Thus, they

8

will miss parental income and wealth entirely for older respondents with their
own households as well as the income and wealth of the non-custodial spouse in
the case of parental divorce. Even without these issues, our ideal measures would
include the stock of wealth available at the time of the college choice as well as
expected future income and wealth. The available measures fall well short of this
ideal, which has implications for how we interpret the estimates from these
variables in our multivariate analysis.

IV. Student ability, college quality, and mismatch
1. Ability
Our primary measure of student ability is the Armed Forces Vocational Aptitude
Battery (ASVAB). The ASVAB is designed for applicants to the U.S. military
and was taken by most of the NLSY97 respondents as part of a norming exercise.
NLSY respondents took the ASVAB during the first wave of the survey in 1997
and those who took the test were paid $75 for their time. 78% of the sample, and
84% of respondents who started at a 4-year college, completed all portions of the
test.
The ASVAB has twelve components, covering both the sorts of skills
measured by the SAT such as algebra, geometry, vocabulary, and reading
comprehension and other skills such as electronics knowledge and spatial
reasoning. The ASVAB is a computer adaptive test, meaning that the difficulty of
the questions asked in the latter part of each section of the test depends on how
well respondents do on the initial questions in the section. The score for each
section reported by the NLSY depends on both the number of questions answered
correctly and the difficulty of those questions as estimated from an earlier sample
of test takers. The ASVAB offers a somewhat richer measure of ability than the
SAT or ACT score, and should be less influenced by variation in preparation

9

because there was nothing riding on this test for the NLSY participants. 1 The
ASVAB score also has the useful feature that colleges do not observe it. We can
therefore capture some of the college mismatch generated by colleges having
incomplete information.
When survey participants took the ASVAB, they ranged in age from 12 to
18, younger than most of the larger population taking the test. We adjust the
scores for age at testing and then take the first principal component of the 12
section scores as our primary measure of ability, which we call ASVAB1. We
calculate each respondent’s percentile within the sample distribution of collegebound NLSY97 respondents, weighted by their probability of inclusion in the
survey.
As shown in Appendix Table 3, the first principal component explains
60% of the total variance in test scores across the 12 sections. The first
component places the highest weight on subjects like those on the SAT (or ACT):
arithmetic, word knowledge, and paragraph comprehension. Not surprisingly
giving the loadings, the correlation between ASVAB1 and the respondent’s SAT
or ACT score equals 0.81.
The second component, which we call ASVAB2, explains a further 11%
of the variance. It places the most weight on the two timed sections of the test:
numerical operations and coding speed. Cawley, Heckman, and Vytacil (2001)
find that the first two principal components of the ASVAB score both predict later
earnings in the NLSY 1979 sample. To construct our measure of mismatch, for
which we need a single measure of ability, we use only ASVAB1. However, we
include ASVAB2 as an additional variable in our multivariate analyses.
1

The ASVAB test is not a straightforward measure of “innate” ability because it includes the
influences and training that the student has had up to the point she takes the test. See Neal and
Johnson (1996) for a more thorough discussion of what the ASVAB test is measuring. We do not
mind if the ASVAB also measures intrinsic motivation, as argued by Segal (2012). More broadly,
we use the term “ability” quite agnostically to mean the set of skills, innate or otherwise, that
students possess around the time of the college choice.

10

While we prefer our ASVAB-based ability measure to the SAT or ACT
scores commonly relied on in the literature, it remains an imperfect measure of
ability. Although the ASVAB tests a richer variety of skills than most
standardized tests it still does not capture all the abilities that make for a strong
college student. Even if it did attempt to measure all relevant abilities, the score
from a single ASVAB test would be an imperfect measure of ability because
some students will perform above or below their usual level on any given day.
2. College quality
We construct a multifaceted index of college quality by combining measures
related to selectivity and college resources. In particular, we combine data from
the U.S. Department of Education’s Integrated Post-Secondary Education Data
System (IPEDS) and U.S. News and World Report, both from 2008. 2 The
components of our college quality index are mean SAT score (or mean ACT score
converted to the SAT scale) of entering students, percent of applicants rejected,
the average salary of all faculty engaged in instruction, and the faculty-student
ratio. Our faculty-student ratio includes only undergraduate students and faculty
who do not teach exclusively in graduate or professional schools within
universities. Most of the NLSY97 respondents started college between 1999 and
2002, somewhat earlier than our college quality measures. 2008 is the earliest
year for which we could obtain US News data and the first year that IPEDS
reported faculty-student ratios focused only on undergraduates. The other
components of our college quality measure are quite stable between 2000 and

2

US News and IPEDS collect many of the same statistics and for the same college in the same
year the numbers are often identical. US News has average SAT or ACT scores for the students at
a number of schools that do not report test scores to IPEDS. However, US News focuses on
selective schools and excludes 2-year colleges altogether. Combining data from the two sources
gives us the most complete sample of colleges. We use US News data to fill in average SAT and
ACT scores and faculty/student ratios when these statistics are missing from IPEDS. Rejection
rates and faculty salaries come only from IPEDS.

11

2008, so we feel the improved data available in 2008 outweigh the measurement
error from observing college quality in a later year.
Following Black and Smith (2004), we use the first principal component
across these four measures of quality as our quality index. Like Black and Smith
(2006), we view our index as providing an estimate of latent college quality,
which we view as continuous and one-dimensional. Within this framework,
combining multiple proxies for college quality into a single index measures latent
quality with less error than using a single proxy (such as the average SAT score of
the entering class) or the categorical quality ratings (e.g. from Barron’s) used in
much of the literature. Our index corresponds well to a priori notions of relative
quality. For example, taking one state at random, the University of Michigan lies
at the 93rd percentile, Michigan State at the 74th, Wayne State at the 36th, and
Eastern Michigan at the 28th. Appendix Table 4 presents the loadings. At the same
time, our measure does not capture differences in the quality that different
students experience within the same university due to, for example, quality
difference across fields of study or participation in honors programs.
This 4-factor quality index is a good measure of the quality of at least
somewhat selective 4-year colleges. However, some 4-year colleges and many 2year colleges do not report the average SAT or ACT scores for their entering
classes, often because they do not require these tests as part of their applications.
Our baseline measure of college quality, which we only construct for colleges
with all four quality measures, disproportionally misses less selective schools. To
address this problem, we also construct an alternative 6-factor measure of college
quality that includes an indicator for colleges that do not report SAT or ACT
scores (setting the average SAT scores to zero for those schools). This alternative
index also includes an indicator for admitting all applicants; that is, for having a
rejection rate equal to zero. This 6-factor college quality measure is our baseline
measure for our analysis combining 2-year and 4-year college starters. We

12

designed this measure to better capture college quality across both 2-year and 4year colleges, but it also allows us to include students starting at 4-year schools
that do not report SAT scores. Failure to report SAT scores and open admission
policies both have negative weights in our college quality factor analysis, so these
new schools are mostly in the lower part of the quality distribution.
3. Measuring mismatch
We employ three alternative measures of mismatch. Our primary measure of
mismatch combines the student ability and college quality measures just
described. We calculate the college’s quality percentile across all four-year
institutions in the United States included in the IPEDS, weighted by student body
size. 3 Because we weight the quality percentile by student body size, a college in
the nth percentile is the college that a student in the nth percentile of the ability
distribution would attend if there were perfect assortative matching of students
and colleges. We consider students mismatched when they deviate substantially
from this type of matching. When considering both 2-year and 4-year college
starters we calculate student ability percentiles across all 2- and 4-year starters in
the NLSY97 sample and calculate weighted college quality percentiles using all
2-year and 4-year colleges in IPEDS and the 6-factor college quality measure.
In practice, substantial gaps between a student’s ability percentile and her
college’s quality percentile are quite common. Table 1A gives the joint
distribution of student ability and college quality, including only 4-year college
starters. Students concentrate along the diagonal, which indicates a good match,
but there are also many mismatched students. The three upper right cells,
corresponding to high ability students at low quality colleges, account for 12.5%
of the sample, while the three lower left cells, corresponding to low ability
students at high quality colleges, account for 12.9%. A comparison of Table 1A

3

Our measure of student body size is full-time equivalent undergraduates.

13

to Table 4 of Black and Smith (2004) reveals (perhaps surprisingly given the
recent policy focus on mismatch) no dramatic changes in the joint distribution
between the NLSY79 and NLSY97 cohorts.
In much of the following analysis we categorize students as overmatched,
well-matched, or undermatched for their college based on the difference between
their ability percentile and their college quality percentile. Figure 1 reveals an
approximately normal distribution for this difference. We consider students to be
undermatched or overmatched, as appropriate, if their percentile difference
exceeds 20. These cutoffs assign about a quarter of the sample to each mismatch
category.

Using binary indicators for mismatch simplifies the analysis and

presentation, but loses some information relative to directly studying the
differences in the ability and college quality measures. Later on, we examine the
sensitivity of our results to changes in the cutoff used to define the binary
mismatch indicators.
We construct our second mismatch measure in the same way as the first,
but using student SAT score as the measure of ability and the average SAT score
of the entering class as the measure of college quality. This measure links us
somewhat to the wider literature, which tends to focus on these specific variables
(or on discretized versions of them). Table 1B presents the joint distribution using
the SAT-based variables. This table reveals less extensive mismatch, as measured
by the fraction in the six corner cells, presumably because colleges observe the
student’s SAT score directly but only observe proxies for ASVAB1.
Our third mismatch measure compares the student’s SAT score to the
inter-quartile range of SAT scores at the student’s college. This measure captures,
in a crude but important way, the notion that being a bit different from the average
means something different at a college with a very heterogenous (in terms of
ability) student body than it means at a college with a very homogenous student

14

body. To our knowledge, we are the first in this literature to consider variance in
student ability in defining mismatch.
Other important studies in the literature, such as Roderick et al. (2008),
Bowen et al. (2009), and Smith et al. (2012) create their measures of mismatch by
making tables with student test score bins on one axis and college quality bins on
the other. For each student test score bin, they then determine the highest quality
bin with a high probability of admission. Students in the highest bin get labeled
well-matched, with undermatch then defined by the distance (measured in bins)
between the bin of the college the student actually enrolled in and the wellmatched bin. Relative to these measures, our primary measure employs better (in
the sense of less measurement error) measures of both college quality and ability.
Our first two measures also have the feature that it is possible for everyone to be
well-matched without violating institutional enrollment constraints. This is not the
case with the other measures in the literature; for every student to be wellmatched by those measures would require a vast expansion in the enrollment
capacity of more selective schools. We think this is an unattractive feature. House
(2013) surveys the literature on mismatch measures in (much) greater detail, and
demonstrates by applying multiple measures to a common data set that the
amount of mismatch varies widely depending on the particular measure adopted.

V. Understanding the college choice
1. Application and admission
The youngest members of the NLSY97 cohort, those born in 1983 and 1984, were
asked an additional battery of questions around the time they finished high school
about the set of colleges to which they applied and the admission decision from
each school. Table 2 presents statistics based on these questions.
The top panel of Table 2 shows that just over 30% of students who ended
up mismatched with their college had applied to at least one college with which

15

they would have been well-matched by our definition. Most of those students who
applied were also accepted to one of those well-matched schools. The bottom
panel reveals that, among students who ended up undermatched, 69% did not
apply to any colleges with which they were well-matched. Only 8% applied to at
least one well-matched school and were rejected. The remaining 23% of
undermatched students were accepted to at least one school with which they were
well-matched but chose to attend a college for which they were undermatched.
Note that 8% represents an upper bound on the percentage of all students who end
up undermatched due to college admissions decisions because even if these
students had been admitted to a well-matched college some would still have
chosen to go elsewhere. More broadly, these students could have applied to more
colleges; the average undermatched student sends only two applications, slightly
below the overall average in our sample. Overmatching is equally a consequence
of student choices rather than college choices; only 4% of overmatched students
applied to a well-matched school and were rejected.
In sum, mismatch overwhelmingly results from choices made by students
and their families, not choices made by college admissions offices. This
conclusion, though perhaps surprising, represents the standard view in the
academic literature; see, e.g. Hoxby and Avery (2012), Griffith and Rothstein
(2010), Avery and Turner (2009), and Roderick et al. (2008). It also justifies our
framing of the choice as primarily one made by students in the informal model in
Section II.
2. Univariate patterns
Tables 3 and 4 describe the characteristics of students and their families by the
quality of college they attend and by their match category, respectively. We
highlight only a few of the most important univariate patterns, saving most of our
attention for the multivariate results to follow.

16

If unconstrained students (and college admissions officers) prefer to avoid
either undermatch or overmatch, then we would expect Table 3 to reveal that
variables correlated with student ability correlate positively with college quality.
In the imperfect information version of that same world, we would expect to see
that variables positively correlated with information quality have higher levels
among well-matched students than among mismatched students (i.e. a hill-shaped
pattern) in Table 4.
In fact, we find common patterns in the two tables for nearly every
variable. Variables that positively correlate with college quality in Table 3 predict
more overmatch and less undermatch in Table 4. For example, in Table 3,
students attending the highest college quality quartile have more educated parents
on average than those attending lower quality colleges. In Table 4, students who
are overmatched for their college have more educated parents on average than
students who are well-matched to their college, who in turn have more educated
parents than students who end up undermatched for their college. Family wealth
has a similarly monotone effect. Monotone patterns in Table 4 such as these
indicate that these characteristics influence college quality rather than mismatch, a
theme that will recur in the multivariate analysis in the next section.
We draw additional measures related to information and guidance from
surveys of the high schools attended by the NLSY97 respondents. These measures
have the advantage (when viewed as proxies for the quality of the student’s
information set) of a weaker (but not zero) correlation with family resources than
parental education. We consider the share of teachers at the student’s high school
with advanced degrees and the shares of graduates from their high school (in the
class of 1999) who went on to attend a 2-year college and who went on to attend a

17

4-year college. 4 All three variables have weak positive relationships with college
quality.

VI. Multivariate analysis
We estimate separate probit models of undermatching and overmatching
conditioning on demographics, multiple measures of ability, family background
variables including parental education and family wealth, contextual variables
related to the census region or tract in which the student finished high school,
variables related to the state university system and variables related to the
student’s high school. Although we estimate reduced form specifications, we use
the informal theory presented above to interpret our findings. For ease of
interpretation, we present mean marginal effects (a.k.a. average derivatives of the
conditional probability of mismatch) rather than probit coefficients.
1. Baseline specification
Table 5A presents estimates from our baseline specification, which defines
mismatch as a difference of more than 20 between the student’s percentile in the
distribution of ASVAB first principal components and the percentile of their
college obtained using the four-factor college quality index. We estimate the
baseline specification using the sample of students who start at a four-year
college.
In general, the results from our multivariate analysis parallel the
unconditional differences presented in Table 4. Consider first the demographic
4

Inspired by Avery (2010) we looked at whether the student’s high school offered college
counseling as well. Virtually every high school answered “yes” to this question. The survey did
not ask further questions about guidance quantity or quality. We also considered high school
teacher experience and salaries, high school graduation rates, the share of the graduating class that
took the SAT or ACT, and the availability of Advanced Placement (AP) classes. The three
variables included in the tables were selected because of their clear conceptual link to information
about college and relatively strong relationship to observed college choices. Frequent item nonresponse thwarted our attempts to create an index combining multiple variables from the high
school survey.

18

variables at the top of the table. We find a lower probability of undermatching for
male students, but little difference in overmatching. Race-based affirmative action
programs should increase the probability of overmatch for minority students,
conditional on their measured ability. We do not find evidence of this effect for
either blacks or Hispanics. In contrast, students in the “other” category, mostly
Asians, have a substantially higher probability (0.08) of overmatching and a
correspondingly lower probability (-0.11) of undermatching. We also went
looking for “quality-quantity” tradeoffs as in Becker and Lewis (1973) by
including the number of household members 18 years old or younger, but the data
indicate they do not matter much in this context.
Ability has a mechanical effect on the probability of mismatch. Very able
students will have few schools for which they are overmatched and many schools
for which they are undermatched. The first principal component of the ASVAB
scores, the measure of ability we use to define mismatch, demonstrates this
mechanical effect. Increasing a student’s ASVAB1 percentile by 10 points
decreases her probability of overmatch by about 9 percentage points and increases
her probability of undermatch by about 7 percentage points.
Once we control for this first ability measure, however, the other ability
measures have the opposite effect: higher high school grades, a higher percentile
on the second principal component of the ASVAB scores and a higher SAT
percentile all raise a student’s probability of ending up overmatched, as defined
by her ASVAB1 score, and lower her probability of being undermatched.
Thinking about the ASVAB1 variable as an error-ridden measure of each
student’s latent ability provides one way to think about these results. Under that
interpretation, the other ability variables represent three additional error-ridden
measures. Conditional on one, a higher value of each of the others suggests higher
latent ability. Students and colleges observe two of these other measures, namely
SAT scores and grades (and perhaps things that proxy for the third, ASVAB2),

19

which suggests that they should also affect application and acceptance decisions,
just as we find here. Put differently, a student with good grades and SAT scores
may truly be a good match for a high-quality school, but we will consider her
overmatched if she scored poorly on the ASVAB.
Now consider our family background variables: household wealth in 1997,
starting college late, parental education, classes outside of regular school, and
having a computer at home. We include wealth in the form of indicators for
quartiles, with the lowest quartile as the reference group. To our surprise, the
wealth variables do not generate much explanatory action. Students from the
wealthiest households have a statistically significantly lower probability of
undermatching of about 0.03, presumably reflecting their parents’ ability and
interest in buying their way into a higher quality college. We find some evidence,
significant at the ten percent level, of a lower probability of overmatching at the
third wealth quartile, which may represent parents too poor for full out-of-state or
private tuition but too well off for much financial aid. Alternatively, it may
represent selection: students from the bottom wealth quartile are less likely to
attend college at all – in Table 3 the average college attendee is in the 3rd quartile
– but those who do may be particularly motivated or subject to some affirmative
action by higher quality schools. Starting college more than 12 months after
graduating from high school raises the undermatch probability by five percentage
points. We think of starting late as (among other things) another indicator of
financial constraints.
Not at all surprisingly, parental education plays a key role in driving
college choices. We find a U-shaped pattern in regard to parental education and
the probability of overmatching. Those with the least educated parents and those
with the most educated parents have the highest conditional probabilities of
overmatching; both groups exceed the omitted group – the highest parental
education is completed high school – by over five percentage points. The opposite

20

pattern holds for undermatching, with students with the most and least educated
parents having substantially lower probabilities of undermatching. These patterns
suggest a combination of disadvantage-based affirmative action at the lower end
of the parental education distribution and the pursuit of college quality at the
upper end. The effects of having less-educated parents may again reflect
selection: students from households where no parent completed high school are
unlikely to attend college, but those who do may have strong unobserved qualities
like motivation. Our concerns about measurement error in the family wealth
variables lead us to interpret the education variables partly as proxies for wealth,
but they also surely capture differences in tastes for education among households
as well as differences in information related to college application and choice.
Our final family background variables measure whether the student took
courses outside of school and/or had a computer in the home. We interpret these
variables as rough proxies for parental enthusiasm about, and willingness to invest
in, education. Other than having had courses outside of school having a negative
effect on undermatching, we do not find much here.
Our measures of context include indicators for three of the four census
regions (the Midwest is the omitted region), a rural residence indicator and
variables measuring log median income and the percent of adults with a four-year
college degree in the student’s census tract. The region variables matter. Students
in the northeast have a 16 percentage point higher probability of overmatching
and an 11 percentage point lower probability of undermatching than students in
the Midwest. Perhaps more surprisingly, students in the south and west also have
lower probabilities of undermatching than those in the Midwest. We suspect that
some of the regional differences in our estimates spring from regional differences
in the relative importance of state and private colleges. In contrast, we find little
effect of living in a rural area, though we might expect one if colleges devote less
recruiting effort to rural high schools, as in Hoxby and Avery (2012).

21

The variables measuring income and education at the census tract level
both positively affect overmatching and negatively affect undermatching, though
only the education effects are precisely estimated. A standard deviation increase
in the share of adults with a BA, a change of nine percentage points on an average
of 21%, increases the probability of overmatch by two percentage points and
lowers the probability of undermatch by three percentage points. These variables
capture a mix of primary and secondary school quality (via residential sorting as
well as voting behavior), information about college, and social pressure directed
toward higher college quality. Given the wealth of variables we condition on at
the student level, the importance of the census tract level education variable
surprised us.
Among the variables drawn from the high school survey, the fraction of
teachers with an advanced degree has no clear effect (and a zero point estimate for
overmatching). This finding comports with a large literature – e.g. Rivkin,
Hanushek and Kain (2005) – that finds that teacher advanced degrees have little
effect on student outcomes. High school student characteristics do matter in our
analysis. The probability of overmatching increases in the fraction of students
going on to either two-year colleges or four-year colleges. Both variables also
decrease the probability of undermatching, though the estimates have smaller
magnitudes and less precision than for overmatching. The fraction going to a
four-year variable likely reflects better information and guidance, as well as
students following their friends. The fraction going to a two-year variable we find
more puzzling, though it may reflect a more select group of students, and thus a
group of students more likely to overmatch, going on to four-year college within
the high school. Taken together, the positive effects of parental education along
with measures of information about college on the probability of overmatch
suggest that more informed students (and their families) prefer to overmatch. That
is, they view the benefits of attending a higher quality college as outweighing any

22

possible costs of mismatch. Manski and Wise (1983) also find that students
prefer colleges where the average SAT score is slightly higher than their own.
The final set of covariates summarizes state higher education policy.
Average four year in-state tuition at public colleges (entered in log form to allow
for a non-linear relationship) decreases the probability of both overmatching and
undermatching, though the latter effect does not attain statistical significance.
This pattern may indicate that states with relatively high four-year tuition do a
better job of matching students to colleges, perhaps because the system offers
more choices of quality and location. Or it may be that lower in-state tuition
induces students at the margin to remain in-state and in the public sector rather
than seeking better matches in the private sector or in other states. The negative
effect on undermatching may result from high four-year college prices pushing
some students to be undermatched at a two-year school rather than at a four-year
school.
A more obvious interpretation follows our finding that having a public
four-year that is a good match within 50 miles (and within the state) leads to
almost a five percentage point decrease in the probability of undermatching. This
suggests that a desire to live nearby, whether to save money by living at home or
to stay near family and high school friends plays a key role in driving
undermatching. The effect on overmatching is small and not statistically
significant, but in the expected sign. Similarly straightforward to understand is
that having a matched private within 50 miles reduces the probability of
undermatching as well, by nearly five percentage points. Students make tradeoffs
between tuition, travel and room and board costs, and quality at the margin in
reasonable ways. Less easy to interpret is the strong positive effect of having a
well-matched private college within 50 miles on the probability of overmatching,
which it increases by 0.106. Taken together, our findings on the substantively
important role of distance in college application and enrollment generally parallel

23

those in the broader literature: see e.g. Griffith and Rothstein (2009) and Turley
(2009).
2. Including students who start at two-year colleges
In addition to attending a lower-quality 4-year college, students can also end up
undermatched by starting at a 2-year college. Reynolds (2012) and Long and
Kurlaender (2009) show that students who start at a two-year college with the
goal of obtaining a four-year degree represent a substantively important group
(albeit one with a low probability of ever attaining a four-year degree). This
section reports on what happens when we expand our analysis to include two-year
starters using the 6-factor college quality index. For this analysis, we follow
Reynolds (2012) and include only 2-year college starters who indicate an
intention to complete a 4-year college degree at the time they start college. 5 When
we use the 6-factor measure of college quality and construct percentiles of college
quality across a pooled sample of 2- and 4-year schools, 70% of the 2-year
schools are in the lowest quality quartile and almost none are in the top half of the
quality distribution. This broadly comports with Stange’s (2012) analysis of
community college quality; see e.g. his Table 1.
Table 5B presents our analysis of mismatch among all college starters.
The percentiles of ability and college quality, and thus our definitions of
mismatch, are now constructed using the set of all 2-year and 4-year colleges in
IPEDS and all 2-year and 4-year college starters in the NLSY97. In general, the
qualitative results parallel those in Table 5A, and even the average derivatives
themselves often do not change by much. We highlight the most interesting
changes in our discussion here.
First, note that adding in students who start at a 2-year college increases
the sample size substantially from 2,125 to 3,805. Second, both black and

5

We provide details on how we construct this filter in the on-line appendix.

24

Hispanic students are now more likely to be overmatched for their college, not
less, and less likely to be undermatched, a pattern consistent with affirmative
action. Third, students from families in the top half of the wealth distribution are
now more likely to be overmatched than students from less wealthy families as
well as less likely to be undermatched. Fourth, the probability of overmatching is
now (roughly) monotone in parental education, but quite non-linear, with all of
the action at the margin between high school completion and some college. Since
this analysis includes many more less-selective colleges, this shift supports the
sample selection interpretation of the positive relationship between the lowest
parental education group and the probability of overmatching in our baseline
analysis.

Fifth, 4-year in-state tuition at public colleges now has a (much)

stronger negative effect on undermatching than on overmatching. Higher 2-year
state tuition, which we include in this model for the first time, decreases
overmatching and increases undermatching. We expected the reverse, with lower
2-year tuition pushing people to the 2-year system and thus toward undermatching
in some cases. In-state 2-year and 4-year tuition are tightly correlated across states
and may partially reflect the breadth and quality of in-state college options.
Finally, the percentage of the student’s high school class going on to a 2-year
college switches from imprecisely decreasing undermatching to strongly
increasing it and decreasing overmatch. This results from the fact that what in
Table 5A was pushing people out of the sample now pushes them into
undermatching when we include the 2-year schools. For a similar reason, the
percentage of the high school class going on to a 4-year school now has a much
larger deterrent effect on undermatching.
Re-estimating the percentiles including the 2-year group increases the
amount of underlying quality spanned by a given percentile difference. This in
turn means that re-estimating the percentiles will change the coding of the
mismatch variables even in the top part of the distribution, as some high ability

25

students who were more than 20 percentile points away from their college before
will not be after the re-estimation. Online Appendix Table OA-1 presents our
multivariate analysis using the 6-factor college quality measure, but constructing
the college quality and ability percentiles from only the sample of 4-year colleges
and starters. The first set of results includes only 4-year starters, although the
sample still increases somewhat because we can include students who start at 4year colleges that do not report SAT scores, while the second includes all starters.
The results parallel those obtained when including all college starters and
recalculating the percentiles, suggesting that the differences between this
specification and our baseline have more to do with the sample expansion than
with the redefinition of mismatch.
3. Alternative measures of mismatch
Table 5C presents our findings using our two alternative definitions of mismatch.
Consider first the two left columns of Table 5C, which display estimates from the
definition based solely on the student’s SAT score relative to the average SAT
score of the incoming class at her college. This multivariate analysis corresponds
to the joint distribution in Table 1B, discussed above. This definition of mismatch
relies on an ability measure observed by colleges, so it does not capture the
mismatch that arises because colleges have imperfect information about the true
ability of applicants or because students misestimate their own abilities relative to
other college applicants. Additionally, the SAT score embodies some of the
guidance students have about applying for college if this information leads them
to put extra effort into preparing for (or re-taking) the SAT or ACT exams. On the
other hand, SAT scores measure ability closer to the time of college application.
Because of the high stakes of the SAT, there is less risk than in the ASVAB of
under-measuring ability because students have not taken the test seriously.
Perhaps most importantly, this analysis requires that we limit the sample
to students reporting an SAT score, which they do by allowing the NLSY to view

26

a high school transcript that includes standardized test scores. As a result, the
sample size falls from 2,125 to 1,279. 6 The loss comes from two different missing
data processes: about half comes from students not releasing their transcripts to
the NLSY and about half comes from schools not providing SAT scores on
transcripts that do get released.
Many of the estimates in the SAT analysis in Table 5C differ substantially
in magnitude and/or precision from the corresponding estimates in Table 5A. But
in only a handful of cases does the average derivative estimate attain statistical
significance in the two analyses, but with different signs. We confine our remarks
here to these cases. First, as expected, the mechanical effect of ability on
mismatch moves from the ASVAB1 percentile, which now has a positive effect
on overmatching, to the SAT percentile, which now has a negative effect on
overmatching and a positive effect on undermatching. Being in the south census
region goes from having a positive effect on overmatching in Table 5A to a
negative effect in Table 5B, possibly due to issues with the ACT to SAT score
translation, though the South is a mixed SAT / ACT region. Finally, the patterns
related to parental education change around a bit. In the SAT analysis, unlike the
analysis in Table 5A, the probability of overmatching increases monotonically in
parental education, while the probability of undermatching continues to have a
hill shape with the maximum for students whose most educated parent is a high
school completer. The change may reflect different investments in preparation for
the SAT or ACT.
As described in Section IV, our third measure of mismatch exploits data
from the IPEDS on the inter-quartile range (IQR) of the SAT scores in the
6

The survey also includes self-reported SAT/ACT scores. We prefer the transcript measures for
two reasons. The self-reported SAT scores are given in 100-point bins, which would add
substantial measurement error. The SAT scores reported on transcripts by the high schools fall
within these self-reported bins for only 70% of respondents and below the bin for 25% of
respondents, suggesting a pattern of score inflation in the self reports.

27

entering class at different colleges. In this analysis, we continue to use SAT
scores as the measure of student ability but define mismatch for each student as
having an own SAT score outside the inter-quartile range for the college. This
measure captures the notion that having an SAT score different from the mean by
some absolute amount means something substantively different at a college whose
students have highly varying SAT scores than it does at a college where students’
SAT scores cluster in a narrow band around the mean. The right-hand side of
Table 5C presents the estimates using this definition of mismatch; note that we
lose some observations due to item non-response for the SAT quartiles in the
IPEDS data. Contrary to our expectations, the big picture of the results does not
change much relative to the estimates on the left-hand side of Table 5C, though
particular estimates do move around, sometimes non-trivially, and become more
or less precise. The most surprising change concerns the parental education
variables, which have much less effect on the probability of overmatching in this
specification, essentially zero for students whose best educated parent has
completed at least high school. We conjecture that the underlying mechanism has
to do with changes in who gets coded as mismatched in states with more
heterogeneous flagships.
4. Match quality versus college quality
We tested the null hypotheses that, for each variable in our empirical model, the
sum of the average derivative in the probit for overmatching and the negative of
the average derivative in the probit for undermatching equals zero. In substance,
this null corresponds to symmetric effects, meaning that analyses that impose
symmetry do not miss much. Online appendix Table OA-9 presents these results.
We reject the null for only two of the 30 coefficients at the five percent level:
living in the south census region and having a well-matched public college within
50 miles. The latter we expected, the former corresponds to a monotone but nonlinear relationship. As we would expect to reject one or two by chance, and

28

because underlying non-linearities in the relationship between our constructed
ability and college quality indices may cloud the interpretation of the tests, we do
not want to over-emphasize these findings. Still, we were surprised. In an
important sense, students and their families care about college quality, not match
quality.
5. Additional sensitivity analyses
We conducted a wide variety of sensitivity analyses related to our baseline
specification, a handful of which merit explicit mention here. Appendix Table
OA-3 presents results from defining mismatch as in Table 5A, but with 10 and 30
percentile point differences, rather than 20. Changing the cutoff used to define
mismatch does not change the qualitative findings.
To test the sensitivity of the results to removing students who could not be
undermatched under the 20 percentile point definition because their ASVAB
percentile was too low, or who could not be overmatched because it was too high,
we repeated the analysis using only students with ASVAB percentiles in (20,80).
Restricting the sample in this way does not change the qualitative results as
shown in Table OA-4.
Inspired by Das and Imberman (2012), who find higher returns to
attending private colleges conditioning on college quality as measured by average
SAT score, we repeated the analysis excluding students at private universities.
Restricting our sample to public universities, which often have simple and binding
admission cut-off rules, also makes our selection on observed variables
assumption particularly plausible. However, Table OA-5 reveals that this, too,
does not change the qualitative findings.
Noting that Black and Smith (2004, 2006) perform their analyses
separately for men and women, we thought we should too. Table OA-6 presents
those results; once again, the qualitative patterns, much to our surprise in this
case, do not change. We also looked at subgroups defined by race/ethnicity and

29

by parental education and including interactions between race/ethnicity and
gender and other key variables. In all cases, the results (not reported) paralleled
the results for the full sample in Table 5A.
Concerned about interpreting the results from multiple measures of ability
all conditional on one another (i.e. ASVAB1, ASVAB2, high school grades and
SAT score in the baseline model), we estimated a model including only one
ability measure, namely ASVAB1. As revealed in Table OA-7, this does not
change the qualitative results. Concerned about the NLSY97 wealth measure, in
Table OA-8 we estimated specifications including wealth in log form, rather than
as indicators for quartiles, and including income, also in log form, in place of
wealth. The qualitative results remain unmoved.

VII. Conclusion
Our analysis of college application and attendance using the NLSY-97 sample of
recent college entrants yielded five main findings. First, using our definition of
academic mismatch between students and colleges, we find substantively
important amounts of both undermatching and overmatching, though not
noticeably more than was present in the earlier NLSY-79 cohort. Second, this
mismatch largely results from choices made by students and their families, not by
college admissions offices. The vast majority of students who end up mismatched
either did not apply to any well-matched schools or were accepted to at least one
well-matched school but attended a mismatched school instead. Third, we find
some evidence that financial constraints lead some students to undermatch, as
students from the wealthiest families undermatch less often.
Fourth, information matters, though not in the way we expected it to. We
thought more informed students would have a lower probability of both types of
mismatch. Instead we found that our proxies for information lower the probability
of undermatching but raise the probability of overmatching. We interpret this as

30

evidence that informed students and their families believe that the benefits of
college quality more than compensate for any possible costs of overmatch.
Fifth, we find that students with a well-matched public college within 50
miles are less likely to mismatch in either direction. In-state tuition policies often
make attending a home state college much less expensive than other options; and
a nearby college allows living at home or, at least, lower travel costs to visit. At
the margin, students trade off these costs against match and quality in reasonable
ways. This supports our view of the reasonableness of looking at mismatch from
the viewpoint of rational, but possibly ill-informed, students and parents.
We close with two big picture points. First, just because we find evidence
that more informed students and their families think college quality trumps
concerns about overmatch does not make it so. Students and parents believe lots
of things contrary to the evidence; this particular belief might belong to that set.
In fact, given the mixed findings in the small existing (academic, rather than
anecdotal) literature – see, e.g. Alon and Tienda (2005), Arcidiacono et al. (2012),
Arcidiacono et al. (2013), Black, Daniel and Smith (2005, Table A.7), Bowen et
al. (2009), Light and Strayer (2000), and Sander and Taylor (2012) – students
deciding where to attend college will have to wait a while to get a clear signal
regarding the evidence. We have our own paper, Dillon and Smith (2013)
underway on this topic, building on the data and findings in this paper.
Second, the optimal amount of mismatch does not equal zero. Sallee,
Resch and Courant (2008) show that in a simple model of university systems with
a fixed cost of establishing each university and complementarity in production
between expenditures per student and student ability, the optimal system consists
of exact matching as we have defined it: i.e. a set of colleges ordered by quality
where the top quality college serves the most able students, the second best
college serves the next most able students and so on. Specific, and not
unreasonable, assumptions about peer effects among students yield a similar

31

optimal system design. These models play an important role as a conceptual
benchmark but they, like our own academically oriented definition of mismatch,
miss important features of the real world. As emphasized in e.g. Smith (2008)
both students and colleges have many other dimensions besides the academic on
which they might care to match. Models and empirical studies that treat these
other dimensions seriously await future work.

32

References
Alon, Sigal and Marta Tienda (2005): “Assessing the `Mismatch’ Hypothesis:
Differences in College Graduation Rates by Institutional Selectivity“, Sociology
of Education, 78, pp. 294-315.
Arcidiacono, Peter (2004): “Ability Sorting and the Returns to College Major,”
Journal of Econometrics, 121, pp. 343-375.
Arcidiacono, Peter, Esteban Aucejo and V. Joseph Hotz (2013): “University
Diﬀerences in the Graduation of Minorities in STEM Fields: Evidence from
California,” Unpublished manuscript, Duke University.
Arcidiacono, Peter, Esteban Aucejo and Ken Spenner (2012): “What Happens
After Enrollment? An Analysis of the Time Path of Racial Differences in GPA
and Major Choice,” IZA Journal of Labor Economics, 1:5.
Avery, Chrisotpher (2010): “The Effects of College Counseling on HighAchieving, Low-Income Students,” NBER Working Paper No. 16359.
Avery, Christopher and Sarah Turner (2009): “Playing the College Application
Game: Critical Moves and the Link to Socio-Economic Circumstances,”
University of Virginia Working Paper.
Becker, Gary and H. Gregg Lewis (1973): “On the Interaction between the
Quantity and Quality of Children,” Journal of Political Economy 81(2, part 2), pp.
S279-S288.
Black, Dan, Kermit Daniel and Jeffrey Smith (2005): “College Quality and
Wages in the United States,” German Economic Review, 6(3), pp. 415-443.
Black, Dan and Jeffrey Smith (2004): “How Robust is the Evidence on the Effects
of College Quality? Evidence from Matching,” Journal of Econometrics, 121(1),
pp. 99-124.
Black, Dan and Jeffrey Smith (2006): “Evaluating the Returns to College Quality
with Multiple Proxies for Quality,” Journal of Labor Economics, 24(3), pp. 701728.

33

Bowen William and Derek Bok (1998): The Shape of the River: Long-Term
Consequences of Considering Race in College and University Admissions,
Princeton, NJ: Princeton University Press.
Bowen, William, Matthew Chingos, and Michael McPherson (2009): Crossing
the Finish Line: Completing College at America’s Public Universities, Princeton,
NJ: Princeton University Press.
Cawley, John, James Heckman, and Edward Vytlacil (2001): “Three Observations
on Wages and Measured Cognitive Ability,” Labor Economics, 8(4), pp. 419-442.
Das, Shreyasee and Scott Imberman (2012): “What is the Return to Attending a
Non-Elite Private College,” Unpublished manuscript, Michigan State University.
Dillon, Eleanor and Jeffrey Smith (2013): “The Consequences of Mismatch
between Student Ability and College Quality,” Unpublished manuscript,
University of Michigan.
Griffith, Amanda and Donna Rothstein (2009): "Can't Get Here from There: The
Decision to Apply to a Selective Institution," Economics of Education Review,
28(5), pp. 620-628.
Hill, Catharine and Gordon Winston (2012): “Low-Income Students and Highly
Selective Private Colleges: Geography, Searching, and Recruiting,” Economics of
Education Review, 29, pp. 495-503.
House, Emily (2013): “Exploring the Undermatch Phenomenon in Michigan,”
Unpublished manuscript, University of Michigan.
Howell, Jessica (2011): “Assessing the Impact of Eliminating Affirmative Action
in Higher Education,” Journal of Labor Economics, 28(1), pp. 113-166.
Hoxby, Caroline and Christopher Avery (2012): “The Missing `One-Offs’: The
Hidden Supply of High-Achieving, Low Income Students,” NBER Working
Paper No. 18586.
Light, Audrey and Wayne Strayer (2000): “Determinants of College Completion:
School Quality or Student Ability?” Journal of Human Resources, 35(2), pp. 299332.
Long, Bridget Terry and Michal Kurlaender (2009): “Do Community Colleges

34

Provide a Viable Pathway to a Baccalaureate Degree?” Educational Evaluation
and Policy Analysis, 31(1), pp. 30-53.
Manski, Charles (1989): “Schooling as Experimentation: A Re-Appraisal of the
Postsecondary Dropout Phenomenon,” Economics of Education Review 8(4), pp.
305-312.
Manski, Charles and David Wise (1983): College Choice in America, Cambridge:
Harvard University Press.
Neal, Derek and William Johnson (1996): “The Role of Premarket Factors in
Black-White Wage Differences,” Journal of Political Economy, 104(5), pp. 869895.
Pallais, Amanda (2012): “Small Differences that Matter: Mistakes in Applying to
College,” Unpublished Manuscript, MIT.
Reynolds, C. Lockwood (2012): “Where to Attend? Estimates of the Effects of
Beginning College at a Two-Year Institution,” Economics of Education Review,
31(4), pp. 345-362.
Rivkin, Steven, Eric Hanushek, and John Kain (2005): “Teachers, Schools and
Academic Achievement,” Econometrica, 73(2), pp. 417-458.
Roderick, Melissa, Jenny Nagaoka, Vanessa Coca, and Eliza Moeller with Karen
Roddie, Jamiliyah Gilliam, and Desmond Patton (2008): From High School to the
Future: Potholes on the Road to the Future. Chicago, IL: Consortium on Chicago
School Research.
Sallee, James, Alexandra Resch and Paul Courant (2008): “On the Optimal
Allocation of Students and Resources in a System of Higher Education,” The B.E.
Journal of Economic Analysis & Policy (Advances), 8(1), Article 11.
Sander, Richard and Stuart Taylor (2012): Mismatch: How Affirmative Action
Hurts Students It’s Intended to Help, and Why Universities Won’t Admit It, New
York: Basic Books.
Segal, Carmit (2012): “Working When No One is Watching: Motivation, Test
Scores, and Economic Success,” Management Science, 58(8), pp. 1438-1457.
Smith, Jeffrey (2008): “Heterogeneity and Higher Education.” In Michael

35

McPherson and Morton Schapiro, eds. Succeeding in College: What it Means and
How to Make it Happen. New York: College Board, pp. 131-144.
Smith, Jonathan, Matea Pender and Jessica Howell (2012): “The Full Extent of
Student-College Academic Undermatch,” Unpublished manuscript, the College
Board.
Stange, Kevin (2012): “Ability Sorting and the Importance of College Quality to
Student Achievement: Evidence from Community Colleges,” Education Finance
and Policy, 7(1), pp. 74-105.
Stinebrickner, Todd and Ralph Stinebrickner (2012): “Learning about Academic
Ability and the College Dropout Decision,” Journal of Labor Economics, 30(4),
pp. 707-748.
Turley, Ruth (2009): “College Proximity: Mapping Access to Opportunity,”
Sociology of Education, 82, pp. 126-146.

36

Table 1A: Joint distribution of college quality and ability—NLSY97, fouryear starters

Ability
Quartiles
1st
Quartile
(lowest)
2nd
Quartile
3rd
Quartile
4th
Quartile
(highest)
Total

1st Quartile
(lowest)
11.7
(42.2)
[44.7]
6.5
(26.3)
[25.0]
5.4
(22.2)
[20.7]
2.5
(10.9)
[9.7]
[100.0]
[N=517]

College Quality Quartiles
2nd Quartile 3rd Quartile
8.0
(29.1)
[30.6]
7.2
(28.9)
[27.3]
6.1
(24.9)
[23.1]
5.0
(21.7)
[19.0]
[100.0]
[N=520]

5.2
(18.6)
[20.4]
6.7
(26.9)
[26.5]
7.3
(30.1)
[29.1]
6.1
(26.3)
[24.0]
[100.0]
[N=499]

4th Quartile
(highest)
2.8
(10.1)
[12.5]
4.5
(17.9)
[20.0]
5.6
(22.8)
[24.9]
9.5
(41.1)
[42.6]
[100.0]
[N=441]

Total
(100.0)
(N=547)
(100.0)
(N=491)
(100.0)
(N=482)
(100.0)
(N=457)
100.0
N=1,977

Each cell contains the overall percentage, (the row percentage), and [the column percentage].
College quality is measured by the 4-factor index. Ability is measured by the first principal
component of the ASVAB scores. All results are weighted as described in the text.

37

Table 1B: Joint Distribution of SAT college quality and ability—NLSY97,
four-year starters
College Quality Quartiles
Ability
Quartiles
1st
Quartile
2nd
Quartile
3rd
Quartile
4th
Quartile
Total

1st Quartile
10.2
(51.9)
[45.0]
6.9
(24.8)
[30.6]
3.7
(14.5)
[16.2]
1.8
(6.7)
[8.2]
[100.0]
[N=385.4]

2nd Quartile
5.2
(26.3)
[23.5]
8.4
(30.2)
[38.5]
5.0
(19.8)
[22.8]
3.3
(12.2)
[15.2]
[100.0]
[N=374.2]

3rd Quartile
2.8
(14.5)
[10.7]
8.1
(29.0)
[30.6]
8.5
(33.7)
[32.1]
7.0
(25.7)
[26.6]
[100.0]
[N=451.9]

4th Quartile
1.4
(7.2)
[4.9]
4.5
(16.0)
[15.3]
8.1
(32.1)
[27.8]
15.1
(55.3)
[52.0]
[100.0]
[N=496.6]

Total
(100.0)
(N=334.3)
(100.0)
(N=476)
(100.0)
(N=431.2)
(100.0)
(N=466.7)
100.0
N=1708.1

Each cell contains the overall percentage, (the row percentage), and [the column percentage].
College quality is measured by the average SAT score of the entering class. Ability is measured
by the student’s SAT score. All results are weighted as described in the text.

38

Table 2: College applications and mismatch

N
Mean number of applications
% applied to over
% applied to well
% applied to under
% accepted to over
% accepted to well
% accepted to under
Share of mismatched
who:
Didn’t apply to a good
match
Applied to a good match
but didn’t get in
Were accepted to a good
match but didn’t attend

Ended up
overmatched
207
2.9
100.0%
32.3%
3.2%
100.0%
27.9%
3.2%

Ended up
well-matched
374
2.5
17.3%
100.0%
16.6%
11.5%
100.0%
16.6%

Ended up
undermatched
208
2.1
9.8%
30.6%
100.0%
4.7%
22.5%
100.0%

Overmatched

Undermatched

67.7%

69.4%

4.4%

8.0%

27.9%

22.5%

Note: Only the younger NLSY97 respondents were asked questions about college applications.
Of the 2,125 respondents who started at a 4-year college and for whom we have a measure of
match with their college, 789 are included in this table. Of the remainder, 1,275 (95% of the
missing) are excluded because they were born in 1980, 1981, or 1982. Another 41 (3% of the
missing) are ineligible for the application section for other reasons. The remaining 21 are missing
because they were eligible but did not answer any application questions. Both panels use weights
as described in the text.

39

Table 3: Average characteristics of students by college choice, four-year
starters
College
Attendees 1, lowest
2,125
591
45%
42%
11%
17%
6%
7%
6%
2%

College quality quartile
2
3
564
513
44%
45%
13%
8%
7%
4%
5%
6%

4, highest
N
457
Male
49%
Black
5%
Hispanic
7%
Other (not white)
11%
Household members age 18 or
2.2
2.3
2.2
2.1
2.3
under
ASVAB 1 percentile
52%
40%
47%
58%
66%
ASVAB 2 percentile
51%
46%
49%
50%
57%
High school GPA percentile
53%
44%
51%
58%
63%
SAT percentile
53%
36%
47%
59%
70%
Household wealth in 1997
$183,185 $127,025 $163,161 $206,554 $249,978
Wealth quartile 1 (lowest)
10%
14%
8%
8%
9%
Wealth quartile 2
18%
24%
20%
14%
12%
Wealth quartile 3
28%
29%
28%
30%
21%
Wealth quartile 4 (highest)
45%
33%
44%
49%
58%
Started college late
9%
16%
9%
6%
5%
No parent completed high school
3%
4%
3%
1%
2%
At least one parent grad. high sch.
18%
24%
23%
14%
8%
At least one parent has some
26%
31%
26%
23%
23%
college
At least one parent completed
54%
41%
48%
62%
67%
college
Took classes outside of school
39%
32%
34%
45%
47%
Had computer at home
80%
72%
80%
83%
86%
Northeast region
21%
12%
15%
23%
36%
South region
30%
35%
25%
31%
30%
Midwest region
32%
35%
36%
35%
19%
West region
17%
17%
24%
11%
16%
Rural
18%
30%
14%
17%
9%
Median income in census tract
$35,867
$31,991
$35,423
$36,984
$40,153
% Adults w/college deg. in tract
21%
18%
20%
22%
24%
% of HS teachers with adv degr
56%
52%
57%
56%
61%
% of HS class to 2-year
18%
16%
19%
18%
18%
% of HS class to 4-year
56%
51%
54%
57%
61%
Avg. 4-year in-state tuition
$3,017
$2,906
$2,880
$3,126
$3,192
Matched public 4-year in 50 mi
52%
45%
49%
53%
61%
Matched private 4-year in 50 mi
66%
53%
64%
68%
81%
Notes: This table describes the characteristics of students at each college quality quartile. For
example, the third row shows the percent of students attending each college type who are male.
All results are weighted as described in the text. Ability percentiles are among 4-year college
starters, with the ASVAB measures adjusted by age when taking the test. In-state tuition is
measured in the year each student graduated from high school, deflated to 1997 dollars.

40

Table 4: Average characteristics of students by match quality, four-year
starters
College
Attendees
2,125
45%
11%
6%
6%
2.2
52%
51%
53%
53%
$183,185
10%
18%
28%
45%
9%
3%

Very
Overmatched
531
36%
18%
8%
9%
2.2
30%
56%
47%
42%
$174,149
15%
20%
24%
41%
9%
5%

Wellmatched
1009
44%
12%
6%
6%
2.2
51%
52%
53%
52%
$193,628
9%
17%
28%
46%
9%
2%

Very
Undermatched
585
53%
5%
4%
3%
2.2
70%
44%
59%
63%
$173,298
8%
18%
29%
46%
9%
1%

N
Male
Black
Hispanic
Other (not white)
Household members age 18 or under
ASVAB 1 percentile
ASVAB 2 percentile
High school GPA percentile
SAT percentile
Household wealth in 1997
Wealth quartile 1 (lowest)
Wealth quartile 2
Wealth quartile 3
Wealth quartile 4 (highest)
Started college late
No parent completed high school
At least one parent graduated high
18%
17%
16%
20%
school
At least one parent has some college
26%
27%
25%
26%
At least one parent completed
54%
51%
56%
53%
college
Took classes outside of school
39%
34%
42%
39%
Had computer at home
80%
76%
81%
83%
Northeast region
21%
32%
21%
12%
South region
30%
33%
33%
25%
Midwest region
32%
21%
29%
44%
West region
17%
15%
17%
19%
Rural
18%
12%
17%
24%
Median income in census tract
$35,867
$37,982
$36,345
$33,674
% Adults w/college deg. in tract
21%
23%
22%
19%
% of HS teachers with adv degr
56%
58%
57%
54%
% of HS class to 2-year
18%
19%
18%
17%
% of HS class to 4-year
56%
57%
56%
54%
Avg. 4-year in-state tuition
$3,017
$3,069
$3,031
$2,958
Matched public 4-year in 50 mi
52%
63%
58%
33%
Matched private 4-year in 50 mi
66%
83%
69%
49%
Notes: This table describes the characteristics of all college attendees (in the first column) and of
students in each mismatch category. For example, the third row shows the percent of all students
and of students in each match category who are male. All results are weighted as described in the
text. Ability percentiles are among 4-year college starters, with the ASVAB measures adjusted by
age when taking the test. In-state tuition is measured in the year each student graduated from high
school, deflated to 1997 dollars.

41

Table 5A: Determinants of mismatch, 4-factor CQ index and ASVAB ability,
four-year starters
Overmatched
-0.005 (0.011)
-0.016 (0.014)
0.009 (0.017)
0.084 (0.025)
-0.003 (0.004)
-0.874 (0.097)
0.065 (0.021)
0.141 (0.028)
0.234 (0.039)
-0.009 (0.020)
-0.032 (0.019)
0.004 (0.020)
-0.045 (0.017)
0.056 (0.028)
0.037 (0.016)
0.054 (0.016)
0.006 (0.013)
-0.013 (0.015)
0.164 (0.028)
0.041 (0.015)
-0.009 (0.019)
-0.002 (0.016)
0.050 (0.029)
0.225 (0.100)

Undermatched
-0.021 (0.009)
0.003 (0.015)
-0.019 (0.017)
-0.110 (0.017)
-0.005 (0.004)
0.715 (0.020)
-0.091 (0.017)
-0.110 (0.021)
-0.121 (0.026)
0.010 (0.020)
-0.012 (0.018)
-0.034 (0.017)
0.053 (0.017)
-0.060 (0.026)
-0.041 (0.012)
-0.080 (0.011)
-0.024 (0.011)
0.018 (0.015)
-0.110 (0.010)
-0.082 (0.011)
-0.037 (0.014)
0.003 (0.013)
-0.027 (0.032)
-0.374 (0.082)

Male
Black
Hispanic
Other (not white)
Household members 18 or under
ASVAB1 percentile
ASVAB2 percentile
High school GPA percentile
SAT percentile
Wealth quartile 2
Wealth quartile 3
Wealth quartile 4
Started college late
No parent completed high school
At least one parent has some col.
At least one par. completed col.
Took classes outside of school
Had computer at home
Northeast region
South region
West region
Rural
Log median income in tract
% adults w/college deg. in tract
% of HS teachers with adv
-0.000 (0.026)
-0.027 (0.024)
degree
% of HS class to 2-year
-0.066 (0.046)
0.155 (0.058)
% of HS class to 4-year
-0.047 (0.028)
0.121 (0.036)
Log avg. 4-year in-state tuition
-0.034 (0.022)
-0.064 (0.027)
Matched public 4-year in 50 mi
-0.015 (0.010)
-0.052 (0.008)
Matched private 4-year in 50 mi
0.106 (0.022)
-0.047 (0.010)
N
2,125
2,125
Pseudo R2
0.279
0.284
Notes: Mean marginal effects (a.k.a. average derivatives) reported. Estimates statistically
different from zero at the five percent level appear in bold. Having a well-matched public and
private school nearby is determined based on the dependent variable’s definition of match for each
pair of regressions. The omitted parental education category is at least one parent completed high
school. Estimates are weighted as described in the text.

42

Table 5B: Determinants of mismatch, CQ index and ASVAB ability, all
starters
Male
Black
Hispanic
Other (not white)
Household members 18 or under
ASVAB 1 percentile
ASVAB 2 percentile
High school GPA percentile
SAT percentile
Wealth quartile 2
Wealth quartile 3
Wealth quartile 4
Started college late
No parent completed high school
At least one parent has some col.
At least one par. completed col.
Took classes outside of school
Had computer at home
Northeast region
South region
West region
Rural
Log median income in tract
% adults w/college deg. in tract
% of HS teachers with adv degree
% of HS class to 2-year
% of HS class to 4-year
Log avg. 4-year in-state tuition
Log avg. 2-year in-state tuition
Matched public college in 50 mi
Matched private college in 50 mi
N
Pseudo R2

Overmatched
0.007 (0.007)
0.084 (0.012)
0.026 (0.011)
0.049 (0.016)
-0.005 (0.003)
-0.742 (0.075)
0.080 (0.015)
0.175 (0.023)
0.062 (0.022)
-0.012 (0.011)
0.031 (0.013)
0.024 (0.013)
-0.090 (0.011)
0.017 (0.013)
0.011 (0.009)
0.027 (0.010)
-0.006 (0.009)
0.042 (0.011)
0.148 (0.024)
0.021 (0.010)
-0.009 (0.013)
-0.000 (0.011)
0.011 (0.023)
0.310 (0.082)
0.063 (0.019)
-0.202 (0.039)
0.018 (0.021)
-0.003 (0.020)
-0.043 (0.012)
-0.000 (0.009)
0.075 (0.015)
3,805
0.278

Undermatched
-0.034 (0.008)
-0.110 (0.010)
-0.051 (0.011)
-0.088 (0.014)
0.009 (0.003)
0.797 (0.016)
-0.120 (0.014)
-0.205 (0.017)
-0.187 (0.022)
0.020 (0.014)
-0.000 (0.013)
-0.045 (0.013)
0.115 (0.011)
-0.040 (0.018)
0.006 (0.010)
-0.057 (0.009)
-0.030 (0.009)
-0.020 (0.011)
-0.067 (0.010)
0.029 (0.011)
0.009 (0.014)
-0.012 (0.010)
-0.143 (0.026)
-0.104 (0.064)
0.036 (0.019)
0.271 (0.033)
-0.078 (0.023)
-0.071 (0.021)
0.032 (0.011)
-0.053 (0.008)
0.023 (0.010)
3,805
0.284

Notes: Mean marginal effects (a.k.a. average derivatives) reported. Estimates statistically
different from zero at the five percent level appear in bold. Having a well-matched public and
private school nearby is determined based on the dependent variable’s definition of match for each
pair of regressions. Estimates are weighted as described in the text.

43

Table 5C: Determinants of mismatch, SAT mismatch, four-year starters
> 20 percentage point gap
Not in college’s inter-quartile range
Overmatched
Undermatched
Overmatched
Undermatched
-0.006 (0.004)
0.056 (0.007)
-0.037 (0.006)
0.094 (0.014)
0.055 (0.011)
-0.083 (0.010)
0.073 (0.010)
-0.050 (0.021)
0.002 (0.011)
-0.031 (0.027)
-0.014 (0.009)
-0.044 (0.010)
0.017 (0.011)
0.006 (0.008)
0.085 (0.013)
0.089 (0.024)
-0.001 (0.003)
-0.021 (0.003)
-0.016 (0.007)
-0.006 (0.002)
-0.060
(0.039)
0.084 (0.017)
0.071 (0.017)
0.029 (0.013)
0.004 (0.007)
0.082 (0.011)
-0.079 (0.011)
0.127 (0.026)
0.241 (0.017)
-0.074 (0.014)
0.248 (0.030)
-0.019 (0.009)
-0.923 (0.051)
0.465 (0.043)
-1.152 (0.042)
0.280 (0.054)
0.002 (0.012)
0.012 (0.010)
-0.102 (0.011)
-0.098 (0.030)
0.011
(0.012)
-0.147 (0.012)
-0.125 (0.030)
0.020 (0.010)
-0.056 (0.029)
0.004 (0.008)
-0.050 (0.010)
-0.043 (0.011)
-0.011 (0.011)
0.006 (0.008)
-0.161 (0.013)
-0.119 (0.034)
-0.077 (0.042)
-0.076 (0.014)
-0.133 (0.020)
-0.046 (0.016)
0.001 (0.018)
0.039 (0.008)
-0.048 (0.007)
-0.021 (0.006)
0.007
(0.019)
0.042 (0.009)
-0.044 (0.007)
-0.018 (0.006)
0.017 (0.019)
0.044 (0.008)
-0.014 (0.007)
-0.010 (0.005)
-0.068 (0.009)
0.053 (0.011)
-0.044 (0.022)
0.017 (0.007)
0.004 (0.006)
0.023 (0.009)
0.052 (0.010)
0.050 (0.020)
-0.006 (0.007)
-0.009 (0.005)
-0.044 (0.008)
-0.058 (0.019)
0.014
(0.009)
-0.012
(0.007)
-0.064 (0.010)
-0.116 (0.028)
0.005 (0.008)
-0.008 (0.020)
0.001 (0.005)
0.056 (0.008)
0.005 (0.018)
0.008 (0.012)
0.041 (0.015)
0.109 (0.042)
0.508 (0.062)
-0.246 (0.038)
0.296 (0.115)
-0.096 (0.027)

Male
Black
Hispanic
Other (not white)
Household members 18 or under
ASVAB 1 percentile
ASVAB 2 percentile
High school GPA percentile
SAT percentile
Wealth quartile 2
Wealth quartile 3
Wealth quartile 4
Started college late
No parent completed high school
At least one parent has some col.
At least one par. completed col.
Took classes outside of school
Had computer at home
Northeast region
South region
West region
Rural
Log median income in tract
% adults w/college deg. in tract
% of HS teachers with adv
0.001 (0.013)
-0.063 (0.034)
-0.012 (0.010)
-0.090 (0.014)
degree
% of HS class to 4-year
-0.018 (0.010)
0.039 (0.015)
-0.067 (0.015)
0.114 (0.038)
Log avg. 4-year in-state tuition
-0.007 (0.013)
0.014 (0.033)
-0.043 (0.014)
-0.022 (0.011)
Matched public 4-year in 50 mi
-0.078 (0.006)
-0.041 (0.006)
-0.126 (0.015)
-0.011 (0.004)
Matched private 4-year in 50 mi
-0.007 (0.016)
0.097 (0.010)
-0.024 (0.005)
-0.014 (0.004)
N
1,279
1,279
1,245
1,246
Pseudo R2
0.267
0.162
0.379
0.409
Notes: Mean marginal effects (a.k.a. average derivatives) reported. Estimates statistically
different from zero at the five percent level appear in bold. Having a well-matched public and
private school nearby is determined based on the dependent variable’s definition of match for each
pair of regressions. Estimates are weighted as described in the text.

44

Figure 1: Distribution of estimated college mismatch, four-year starters

Mismatch defined as student ability percentile minus college quality percentile. Histogram
includes estimated kernel density distribution. The distribution is weighted as described in the
text.

45

Appendix Table 1: Sample
Total Observations
Graduated HS
Did not graduate HS but got GED
Started at a 2-year college*
Started at a 4-year college*
Starting college qualities
Of quality quartile 1
Of quality quartile 2
Of quality quartile 3
Of quality quartile 4
Missing quality (6-factor index)
Has quality, but missing ability
All starters analysis sample**
Starting college qualities, 4-year only
Of quality quartile 1
Of quality quartile 2
Of quality quartile 3
Of quality quartile 4
Missing quality (4-factor index)
Has quality, but missing ability
4-year starters analysis sample

8,984
7,143
701
2,646
2,942
1,699
1,461
1,212
977
239
946
3,805
710
646
609
534
443
374
2,125

* The 2-year starters include 152 respondents who got a GED and 50 respondents with no
recorded high school graduation date or GED. The 4-year starters include 40 respondents who got
a GED and 8 respondents with no recorded high school graduation date or GED.
**Analysis sample excludes 598 2-year college starters who, before starting college, reported less
than 50% probability that they would eventually obtain a 4-year college degree.
College quality is for the first college attended. For the 4-year starter sample the figures are based
on the 4-factor college quality index. For the all starters sample the figures are based on the 6factor college quality index.

46

Appendix Table 2: Description of independent variables
Variable
Male
Black
Hispanic
Other (not white)
Household
members under 18
Started college late
ASVAB percentile

High School GPA

SAT score

Region of the U.S.
Household wealth

Parents’ education

Description
Indicator variable that the respondent is male
Equal to 1 if the respondent lists black as a racial category
Equal to 1 if the respondent lists Hispanic as an ethnic
category and doesn’t list black as a racial category
Equal to one if the respondent doesn’t list black or white as
racial categories or Hispanic as an ethnic category
Number of children age 18 and under living at the
respondent’s address in 1997 (including the respondent)
Equal to one if the respondent started college more than 12
months after finishing high school.
Percentile over 4-year (or all) college starters in the
NLSY97 of the first (ASVAB1) and second (ASVAB2)
principal components of the 12 sections of the ASVAB test,
taken by NLSY97 respondents in 1997.
From respondent’s high school transcript and standardized
to a 4-point scale weighted by Carnegie credits. GPA
percentile is calculated within our [weighted] sample of
college-goers in the same way as the ASVAB percentile.
Combined math and verbal SAT scores (max 1600) or the
composite score on the ACT converted to the SAT scale
from the respondent’s high school transcript. SAT
percentile is calculated within our [weighted] sample of
college-goers in the same way as the ASVAB percentile.
Where the respondent lived in last year of high school.
Total 1997 net worth for the household where the
respondent lived in 1997. Taken from the parent survey
where available or from the youth survey (98.6% from
parent survey). We use total wealth across everyone living
in the same household as the respondent (respondent may
live separately from parents in 1997). 1997 wealth quartiles
are calculated within the (weighted) sample.
Highest educational attainment of either of the respondent’s
resident parents (or only parent in single parent households)
as reported in the fall before the respondent finished high
school (or earlier if that year is unavailable). We include at
most one resident mother and father figure using the
following prioritization: biological, adopted, step, or foster.

47

Log median
income in tract
% in census tract
with BA

Log median income (from 1990 census) in the census tract
where the respondent lived in last year of high school.
The share of the over-25 population that has a 4-year
college degree (from 1990 census) in the census tract where
the respondent lived during his last year of high school.
Took classes
From 1997. Equal to one if he or she answered yes to “In a
outside of school
typical week, did you spend any time taking extra classes or
lessons for example, music, dance, or foreign language
lessons?”
Had computer at
From the 1997 youth survey. Equal to one if he or she
home
answered yes to “In the past month, has your home usually
had a computer?”
Log average 4-year Average in-state tuition, by year, for public four-year and
or 2-year in-state
two-year schools is from the State of Washington Higher
tuition.
Education Coordinating Board. “In-state” tuition for
District of Columbia residents is calculated as max(national
average in-state tuition, national average out-of-state tuition
- $10,000) in accordance with DC Tuition Assistance Grant
Program. For each respondent, in-state tuition is the in-state
tuition in the fall before he finished high school in the state
where he lived that fall. All tuition is CPI-deflated to 1997
dollars.
Well-matched
Well-matched is defined as having a college of the relevant
public or private
category whose weighted quality percentile is within 20
college nearby
percentage points of the student’s ASVAB ability percentile
(as detailed in the text). Distance is calculated from the
zipcode of the respondent’s residence in the fall before he
finished high school. In the 352 cases where the zipcode
that fall was missing, the zipcode from the last available
year prior to graduation is used.
% of HS teachers
From the restricted NLSY97 School Survey, taken from the
with advanced
respondent’s last high school. to the survey question “what
degrees
percent of your teachers have more than a BA?”
% of HS class to
From the restricted NLSY97 School Survey. The response
four-year or twofrom the respondent’s last high school to the survey
year college
question “by the fall following graduation, about what
percent of your 1999 graduating class enrolled in a four-year
(two-year) college?”
Rural
Indicates that the respondent did not live within a
Metropolitan Statistical Area (MSA) in the fall before she
finished high school.

48

Appendix Table 3: Principal components of the 12 test sections of the
ASVAB

Eigenvalue
Total variance explained
Eigenvectors:
General Science
Arithmetic Reasoning
Word Knowledge
Paragraph Comprehension
Mathematics Knowledge
Mechanical Comprehension
Electronics Information
Assembling Objects
Shop Information
Numerical Operations
Auto Information
Coding Speed

1st Component
7.18
59.8%

2nd Component
1.36
11.3%

Unexplained variance

0.326
0.325
0.322
0.320
0.318
0.310
0.304
0.273
0.245
0.240
0.225
0.223

-0.114
0.117
-0.038
0.114
0.239
-0.162
-0.228
0.107
-0.462
0.444
-0.456
0.441

21.9%
22.2%
25.4%
24.8%
19.7%
27.4%
26.8%
45.1%
27.9%
31.8%
35.6%
37.8%

Note: scores on each test component are adjusted for the age of the respondent
when they took the test by regressing the score on age dummies and using the
residuals for the principal components analysis. The first two principal
components combined explain 71.1% of the total variance of the 12 test section
scores.

49

Appendix Table 4: Principal components of the college quality indices
4-factor college quality index among 4-year colleges

Eigenvalue
Total variance explained
Eigenvectors:
Mean SAT
Rejection rate
Faculty/Student ratio
Average faculty salaries

1st Component
2.09
52.2%

Unexplained variance

0.588
0.479
0.359
0.544

27.8%
52.1%
73.1%
38.2%

6-factor college quality index among 2- and 4-year colleges

Eigenvalue
Total variance explained
Eigenvectors:
Mean SAT
Rejection rate
Faculty/Student ratio
Average faculty salaries
Open admissions
Does not report SAT

1st Component
3.47
57.9%

Unexplained variance

0.500
0.422
0.145
0.310
-0.460
-0.492

13.0%
38.1%
92.7%
66.6%
26.4%
15.8%

50

