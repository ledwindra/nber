{
    "id": 30017,
    "citation_title": "Aligned with Whom? Direct and Social Goals for AI Systems",
    "citation_author": [
        "Anton Korinek",
        "Avital Balwit"
    ],
    "citation_publication_date": "2022-05-09",
    "issue_date": "2022-05-05",
    "revision_date": "None",
    "topics": [
        "\n",
        "Microeconomics",
        "\n",
        "Welfare and Collective Choice",
        "\n",
        "Development and Growth",
        "\n",
        "Innovation and R&D",
        "\n"
    ],
    "program": [
        "\n",
        "Public Economics",
        "\n",
        "Productivity, Innovation, and Entrepreneurship",
        "\n"
    ],
    "projects": null,
    "working_groups": [
        "\n",
        "Entrepreneurship",
        "\n",
        "Innovation Policy",
        "\n"
    ],
    "abstract": "\n\nAs artificial intelligence (AI) becomes more powerful and widespread, the AI alignment problem\u2014how to ensure that AI systems pursue the goals that we want them to pursue\u2014has garnered growing attention. This article distinguishes two types of alignment problems depending on whose goals we consider, and analyzes the different solutions necessitated by each. The direct alignment problem considers whether an AI system accomplishes the goals of the entity operating it. In contrast, the social alignment problem considers the effects of an AI system on larger groups or on society more broadly. In particular, it also considers whether the system imposes externalities on others. Whereas solutions to the direct alignment problem center around more robust implementation, social alignment problems typically arise because of conflicts between individual and group-level goals, elevating the importance of AI governance to mediate such conflicts. Addressing the social alignment problem requires both enforcing existing norms on their developers and operators and designing new norms that apply directly to AI systems.\n\n",
    "acknowledgement": "\nThis paper was prepared for the Oxford Handbook of AI Governance. We would like to thank Ond\u0159ej Bajgar, Damon Binder, Justin Bullock, Alexis Carlier, Carla Zoe Cremer, Allan Dafoe, Ben Garfinkel, Lewis Hammond, Fin Moorhouse, Luca Righetti, Toby Shevlane, Joseph Stiglitz, and participants at the Spring 2021 Handbook of AI Governance conference for helpful comments and discussions. Any remaining misalignment is our own. Korinek acknowledges financial support from the David M. Rubenstein Fellowship program at the Brookings Institution.  The views expressed herein are those of the authors and do not necessarily reflect the views of the Brookings Institution or the National Bureau of Economic Research.\n\n\n"
}