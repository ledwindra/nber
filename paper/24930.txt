NBER WORKING PAPER SERIES

TAKING PISA SERIOUSLY:
HOW ACCURATE ARE LOW STAKES EXAMS?
Ş.Pelin Akyol
Kala Krishna
Jinwen Wang
Working Paper 24930
http://www.nber.org/papers/w24930

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
August 2018

We would like to thank Joris Pinkse, Keisuke Hirano and Kim Ruhl for their comments and suggestions
and Meghna Bramhachari for help in proof reading. We owe special thanks to colleagues at the OECD
for answering our numerous questions about the data. Huacong Liu was instrumental in our working
on this project and we thank her for all her help. We are responsible for all errors. The views expressed
herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic
Research.
NBER working papers are circulated for discussion and comment purposes. They have not been peerreviewed or been subject to the review by the NBER Board of Directors that accompanies official
NBER publications.
© 2018 by Ş. Pelin Akyol, Kala Krishna, and Jinwen Wang. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,
including © notice, is given to the source.

Taking PISA Seriously: How Accurate are Low Stakes Exams?
Ş. Pelin Akyol, Kala Krishna, and Jinwen Wang
NBER Working Paper No. 24930
August 2018, Revised August 2019
JEL No. C53,I20,I21
ABSTRACT
PISA is seen as the gold standard for evaluating educational outcomes worldwide. Yet, being a low-stakes
exam, students may not take it seriously resulting in downward biased scores and inaccurate rankings.
This paper provides a method to identify and account for non-serious behavior in low-stakes exams
by leveraging information in computer-based assessments in PISA 2015. We compare the score/rankings
with no corrections to those generated using the PISA approach as well as our method which fully
corrects for the bias. We show that the total bias is large and that the PISA approach corrects for only
about half of it.
Ş. Pelin Akyol
Bilkent University
Department of Economics
06800 Ankara / TURKEY
pelina@bilkent.edu.tr
Kala Krishna
Department of Economics
523 Kern Graduate Building
The Pennsylvania State University
University Park, PA 16802
and NBER
kmk4@psu.edu

Jinwen Wang
jxw490@psu.edu

1

Introduction

Standardized tests are widely used to evaluate students, to rank countries in terms of
educational outcomes, and to certify achievement. If the outcome of the test matters for
the student taking it, the test is regarded as a high-stakes one, otherwise it is a low-stakes
test. High-stakes exams motivate effort on the part of the student. However, to the extent
that students have differential access to inputs that affect outcomes on the test, the resulting
rankings may provide a biased picture of achievement. For example, well-off students tend
to prepare for the SATs, often going to tutoring centers that show them how to raise their
scores, while poor students may be less informed and less able to do so. For this reason,
if the aim is to obtain a snapshot of where students are, then a low-stakes exam may be
preferable to a high-stakes one.
However, the disadvantage of low-stake exams is that students may not take them seriously, so their performance on the exam may not reflect their true ability. As a result,
scores from low-stake exams may be inaccurate. Correcting for this bias can be difficult. It
is less of a problem if being non-serious is totally random and can be identified, as then one
can confine oneself to the serious sub-sample. However, if effort during the test is related
to ability, socioeconomic status, and other characteristics, it is not obvious how one might
correct for such bias. For example, if high-ability students are more likely to be non-serious
in low-stake tests, then test scores could considerably underestimate the average ability and
underestimate the gap between low-ability and high-ability students.
The most well known and best executed low stakes exam is PISA (The Programme for
International Student Assessment) 1 . This is a worldwide study organized by the Organization for Economic Cooperation and Development (OECD) in member and non-member
countries. It is a low-stakes exam as the performance on the exam has no consequences for
those taking the exam. The aim of the exam is to have a common yardstick by which to
measure students’ performance in mathematics, science, and reading at age 15.
We use the computer-based assessment (CBA) in PISA 2015 to investigate the existence
and extent of bias due to non seriousness 2 . As PISA is computer-based, it has data on item
response, response time for each question3 as well as the order of items. We first provide
evidence that some students are not taking the exam seriously so that scores and rankings
1

Other well known low-stakes tests include Trends in International Mathematics and Science Study
(TIMSS) and Progress in International Reading Literacy Study (PIRLS). PISA assesses whether students
can apply what they have learned to solve “real world” problems. PIRLS and TIMSS are grade-based (4th
and 8th graders) and curriculum oriented.
2
The previous work in this field (Zamarro et al. (2016), Huang et al. (2012)) have used the term "careless
answering/responding" instead of "non seriousness".
3
One item is one question. We use the word “item” or “question” interchangeably in the paper.

2

could be biased. We then show how we can adjust for these biases to obtain a reliable
snapshot of student skills.
It is worth noting that though PISA is a low stakes exam for students, there is much at
stake for countries. Governments look at PISA scores to see where weaknesses lie in their
educational systems. What is even more important, in some ways, is the role of PISA in
providing the public with an objective view of how well their government is doing in this
area. Every three years when the new PISA results come out, they are cited authoritatively
in countless newspapers and policy reports. In many countries, they even start to influence
educational practices deeply. In 2014, more than one hundred academics around the world
wrote a letter to the director of PISA to express their deep concern about the impact of
PISA results4 . They wrote:
“As a result of PISA, countries are overhauling their education systems in the
hopes of improving their rankings. Lack of progress on PISA has led to declarations of crisis and "PISA shock" in many countries, followed by calls for
resignations, and far-reaching reforms according to PISA precepts. ”
There is also evidence that a few countries are trying to get their students to take the
exam seriously with a view to gaming the system. For example, Abu Dhabi gave mock PISA
exams to prepare students for the PISA exams in 2018. Each school was sent a student report
as well as a school report comparing them to other schools and the international averages.
For each student, the areas that teachers need to work on were highlighted5 . Canadian school
teachers are given a handbook on how to prepare students for the PISA exam.(See Prince
Edward Island (2002)) In the handbook, teachers are urged to “encourage them (students)
to take the assessment seriously and strive for excellence.”
The PISA exam is composed of four clusters of items with a short break after the first
two clusters. In this study, we restrict attention to the Science component of the test as
in 2015 all students had to take two clusters in this area. The remaining two clusters are
for the reading, math and CPS (collaborative problem solving) components. As a result,
some students may take only reading, while others may only take math or CPS which would
reduce the sample size6 . In the PISA exam, there is no penalty for guessing (wrong answers
4

See the article in The Guardian,
May 6,
2014,
entitled "OECD and Pisa
tests are damaging education worldwide-academics",
Retrieved from the following link:
https://www.theguardian.com/education/2014/may/06/oecd-pisa-tests-damaging-education-academics
5
See the article in the National, Sept 25, 2017, entitled “Abu Dhabi pupils prepare for Pisa 2018”,
Retrieved from the following link: https://www.thenational.ae/uae/abu-dhabi-pupils-prepare-for-pisa-20181.661627
6
In the appendix, we provide some data that suggests that the results we see in the science section
will likely be correlated to and magnified in the reading and math sections as non-seriousness seems more
prevalent in math and reading than in science.

3

are not penalized); therefore there is no reason to skip a multiple-choice question: students
should guess even if they have no idea of the correct answer7 .
The skipping and timing data allows us to identify non-serious students as those who
skip too many questions or spend too little time on too many questions, i.e., seem not to
put reasonable effort into the exam. By definition, non-serious students on average spend
less time than serious students, but we find that this is especially so on items which they get
wrong, suggesting that their inaccuracy is due to their spending less time on them. We note
a marked fall in response time and accuracy with both position within a cluster and position
of the cluster, and this is much more pronounced for non-serious students. These patterns
suggest that we are truly identifying students who are not engaged in taking the exam.
We quantify the effects of non-serious behavior on country performance. We account for
the bias of being non-serious by imputing the scores for skipped questions and for questions
on which too little time is spent using multiple imputation techniques8 . PISA documents
are clear that their scores and rankings come with confidence intervals, see Figure 1.2.14 in
OECD (2015a). We follow what PISA does in generating plausible values for the imputed
data, then calculate the 95% confidence intervals for the imputed scores, and finally use these
intervals to calculate the rank intervals for countries using the computer-based assessments.
We make a number of comparisons. First, we compare the fully imputed score (FIS) to
the original score (OS) where skipped items are given zero points, as is the case in most
tests. One could also compare the fully imputed score to the score when skipped items at
the end of the exam are treated as if they were not administered (i.e. as if they did not
appear in the exam), SENA for skipped at the end not administered, which is the procedure
followed by PISA. The fully imputed score is what would be obtained by a country if its
students took the test seriously.
We show that a country can improve its ranking by up to 15 places by encouraging its
own students to take the exam seriously when we compare the FIS rankings to rankings
using the OS. Of the 58 countries, 24 have rank confidence intervals that do not overlap with
OS rank intervals. Using the FIS versus the SENA score, a country can improve its ranking
by less, up to 7 places. Only 1 of 58 countries have rank confidence intervals that do not
7

One might argue that students do not understand that it is better to guess than to skip. However, if
this was the only reason for skipping, then skipping behavior should not be related to the position of the
item, which clearly is as shown below. One might also argue that as this is a computer based test, students
cannot go back to answer skipped item as they might in a paper test. If students do not realize this, they
may skip inadvertently. Again, since they will quickly learn that they cannot go back even if they do not
know this to begin with, skipping should be less prevalent in the second cluster than the first. Again, the
opposite is true.
8
This procedure uses other responses of the same agent as well as those of agents like him to do the
imputation.

4

overlap. If all countries become serious, then the rankings change by little. But this is to be
expected: if everyone tries to game the system, their efforts cancel out.
What matters for countries is not only their rankings but also their students’ performance.
The difference in the scores (FIS versus OS) is significant at the 5% level in 50 of 58 countries,
and at the 1% level in 46 of 58 countries. The difference in the FIS and SENA is significant
for only 7 countries at the 5% level and 2 countries at the 1% level. This shows that the
PISA approach of treating skipped items at the end as not administered goes part of the
way in correcting for the effect of non seriousness. It also suggests that PISA is aware of the
problem. In effect, imputing data for skipped items at the end versus treating them as not
administered gives roughly the same number for the fraction correct obtained as the only
difference becomes the number of questions administered. Of course, just dropping skipped
items at the end is not enough to fully account for non seriousness. This is why the FIS
also imputes data for skipped items in the middle of the test as well as items on which too
little time was spent. In addition, countries can take advantage of PISA’s approach if they
are aware it. For example, they can instruct their students to spend as much time as they
need on earlier questions because even if they do not have time to reach the latter questions,
those questions will be dropped in score calculation.
Countries vary substantially in terms of the change in score and ranking if their students
took the exam seriously. The change is not driven solely by the proportion of non-serious
students, but also by these students’ ability and the extent of their non-seriousness. There
are countries with a large fraction of non-serious students (such as the Dominican Republic)
who move up very little in their ranking because their non-serious students are of low ability.
There are also countries with a medium fraction of non-serious students (such as Russia)
whose students’ performance improved by a large extent as their non-serious students have
high ability.
We decompose the increase in the fraction correct of questions due to the imputation
into its component parts for each country. Countries vary considerably in terms of the
importance of these components. Across countries, 68% of the variation comes from the
proportion of non-serious students, while 26% comes from the extent of non-seriousness,
with the remaining coming from their ability.

1.1

Relation to the Literature

There is a literature that uses PISA data to study the role of institutional differences
such as effects of instruction time, school autonomy and tracking on students’ performance
(Lavy (2015), Hanushek et al. (2013), Hanushek and W ößmann (2006) ) or to analyze

5

how students’ performance differs according to their background characteristics (Lounkaew
(2013)). If non-serious behavior is correlated with the variables used in these studies, then
their findings may be biased. This is another reason to account for non-serious behavior. We
are not the first to point out that low-stakes exams might be inaccurate because they are
not taken seriously. It has been recognized in the literature that low student motivation is
associated with low performance (Pintrich and De Groot (1990), Wise and DeMars (2005),
Cole et al. (2008), Penk and Richter (2017), and Jalava et al. (2015)), and students may not
put their best effort in low-stakes exams (Wolf and Smith (1995), Duckworth et al. (2011),
see Finn (2015) for a recent review). Attali et al. (2011) show that the stakes of an exam
affect performance of students differentially according to socioeconomic status, gender and
race. The difference between high and low-stakes exams is larger for males, whites and higher
SES students. Similarly Azmat et al. (2016) find that women perform better than men in
low-stakes exams, but as the stakes increase, this performance gap disappears.
Eklöf (2010) points out that it is important to take into account students’ test-taking
motivation especially on exams where the stake is low for the test-taker but high for the
other stakeholders. Jacob (2005) documents that when the Iowa Test of Basic Skills was
low-stakes, a large proportion of students left some questions blank despite there being
no penalty for guessing. After it became a high-stakes exam, the percentage of questions
answered increased by 1 − 1.5 percentage points, and the fraction correct of those answered
also rose by 4 − 5 percentage points. This suggests that effort plays an important role in
the performance of students. The critical role of effort is also noted in designing surveys
and experiments. Early questions in a survey are more likely to be filled out carefully
and experiments that ask for excessive inputs from the subjects may experience a decline
in response quality (see Krosnick et al. (1996)). Huang et al. (2012) summarize existing
approaches to detect careless responding in low stake surveys. Our approach to detect and
to deal with the careless answering can be potentially used in the low-stakes computer-based
surveys and experiments as well.
Although the literature provides ample evidence on the relationship between effort, motivation and performance, there is little work that quantifies the effect of differential effort
on the cross country rankings. Zamarro et al. (2016) attempt to explain the effects of differences in students’ effort on the observed differences in country scores in the 2009 PISA
exam. However, as this was not a computer-based assessment, they can only use the random
ordering of questions, responses to student survey questions and the consistency of these re-

6

sponses to tease out effort differences.9 They then regress the score on their measures of effort
and country fixed effects and argue that their measures of effort explain 32 to 38 percent of
the observed variation in test scores across countries. Borghans and Schils (2012) document
the same fall in test performance over question order as does Zamarro et al. (2016), but
in addition they use two other datasets and show that this decline is related to personality
traits, like agreeableness, and to motivational attitudes towards learning.
Butler and Adams (2007) use self reported expenditure of effort by students and argue
that because it is fairly stable across countries, and is unlikely that systematic differences in
the effort expended by students invalidates the comparison across countries in PISA results.
Baumert and Demmrich (2001) conduct an experiment on German students to see if different
ways of increasing the stakes involved affect performance. They offer monetary incentives
tied to performance, feedback on performance, or the test mattering for school grades as well
as the standard PISA setup as a control. They find no significant effects on performance
or self reported effort from any of these treatment arms. Our work using much richer
keystroke data suggests differently. The extent of non seriousness and its consequences vary
considerably across countries and it would be a mistake to project results from one country
to other countries.
Gneezy et al. (2017) is the paper most closely related to ours. In an experimental environment, they incentivize U.S. and Shanghai students to increase their effort level and
explore the effects of doing so on student performance. Their experiment has less than 500
students in the U.S. and less than 300 in China. The assumption is that student response
in the experiment is what it would be if they had taken the PISA exam seriously. They
show that incentives increased U.S. students’ effort and performance, but did not affect the
Shanghai students’ performance. They then carefully project their experimental results on
PISA data and estimate that the increased effort of U.S. students is equivalent to improving
U.S. mathematics ranking in the 2012 PISA exam from 36 to 19. However, they are unable
to do this for each country as their experiment is limited to two countries.10
Our work extends the findings of Gneezy et al. (2017) to all countries by using some
unique features of the PISA 2015 data. Computer based assessments allow us to better see
how students respond to questions in terms of time spent and response content, which allows
us to correct for non-seriousness without having to do an experiment for each country. It
analyzes the effects on scores and ranking if non-serious students behaved like serious ones
9

One of their measures of effort is the extent to which performance falls when the question occurs later
in the exam. Another is the extent to which questions are skipped in the survey that students have to fill
out and a third is the extent of carelessness in filling the survey.
10
Our estimates below also show that China seems to be less affected by non serious behavior than the
US.

7

for the 58 countries and areas that participated in the computer-based PISA exam in 2015.
As a result we can do “partial equilibrium” analysis (one country is serious at a time), which
is the most relevant since most countries do not intervene actively so as to raise PISA scores,
or general equilibrium analysis (all countries are serious together) and analyze the effect of
being the left out one (all other countries are serious). In the Appendix, we also investigate
what correlates with low student effort across countries. We find large differences across
countries and suggest some possible reasons for these differences.
The organization of the paper is as follows: The next section gives the necessary background about PISA exams. Section 3 presents the data patterns that indicate non serious
behavior is present. Section 4 presents and discusses the effects of non-seriousness on scores
and rankings of countries. Section 5 decomposes the change in the fraction correct of each
country after becoming serious and Section 6 concludes.

2

The PISA Exams

The PISA exams have been given every three years since 2000. In 2015 over half a
million students participated in PISA exams, representing 28 million 15-year-olds in 72
countries and economies. For the first time in 2015, PISA was conducted as a computerbased exam, however the paper-based version was also available for countries that did not
have the technical infrastructure needed.11 As a result, 58 countries and economies took
PISA 2015 in computer-based-assessment mode (CBA), accounting for 86.1% of the whole
sample. In this paper, we will focus on these countries as only CBA items have data on the
response time and the order of the questions, which we use below.
PISA is a two-hour exam.12 It includes four 30-minute clusters, and students have 60
minutes for the first two clusters and 60 minutes for the last two with a 5-minute break in
between (OECD (2015b)). Each student gets different clusters based on a random number
assigned to students.13 Each cycle of PISA emphasizes one domain. While the emphasis was
on reading in PISA 2009 and mathematics in PISA 2012 exam, the 2015 exam focused on
science. Therefore, each student had two consecutive science clusters in the test, and they
took these clusters either in the first hour or in the second hour of testing. According to
OECD (2015b), time is not a binding constraint for most students. On average students
11

In the 2012 PISA exam, 32 countries/regions were invited to complete both a paper and a computer
version of mathematics test. However, by 2015, 58 moved to a computer based assessment. Jerrim (2016) and
Jerrim et al. (2018) find that taking the PISA exam in a computer-based mode affects students’ performance
negatively in many countries.
12
For countries that choose to implement the assessment of financial literacy, it requires an additional 60
minutes.
13
For more detail see PISA 2015 Technical Report Chapter 2. (OECD (2015b))

8

completed a cluster in around 18 minutes and 75% of students completed a cluster in less
than 22 minutes. The PISA exam includes three types of questions: simple multiple choices,
complex multiple choice 14 and open response. Each type accounts for approximately one
third of all questions.
PISA 2015 also asked students and school principals to fill in questionnaires. The responses to the questionnaires, combined with the assessment results, can provide a broader
and more nuanced picture of student, school and system performance. The student questionnaire seeks information about students and their family backgrounds, and aspects of
students’ lives such as their attitudes towards learning, their habits and life in and outside
of school, and their family environment. The school questionnaire provides information on
aspects of schools such as institutional structure, class size, learning activities in class, type
and frequency of students’ assessments.15 Table A.1 in the Appendix contains descriptive
statistics for the data used below.

3

Identifying Non-serious Questions and Behavior

Our approach to correcting for bias involves imputing questions that are not taken seriously. In this section, we explain how we identify such questions. It is natural to expect
serious students to try and answer the questions to the best of their ability. There is no
negative marking for wrong answers in PISA. For this reason, guessing is a dominant strategy for multiple-choice questions. Even if a student does not know the answer, and there
is time remaining, the student is better off choosing some answer than leaving the answer
blank. For open response questions, there may be no point in guessing as a continuum of
answers exists. For this reason, one might want to treat open response and multiple-choice
questions differently. We do not do so. Skipped open response questions (i.e. not responded
to whether or not time is spent on them ) and skipped multiple-choice questions are both
imputed independent of whether there is time left or not. Our logic is that if the student
did not answer the question because he did not know the answer, the imputation procedure
is likely to give a score of zero for the question. We could have only imputed the answers
only if there was a significant amount of time left, but we chose not to do so. This choice
is unlikely to make a difference as only 0.7% students have less than 1 minute left, and 3%
have less than 5 minutes left. It is worth noting that skipping questions is more likely to
happen later in a cluster and in later clusters, see Figure A.2. This would not be the case if
the reason for skipping is complete ignorance of the answer, because there is no correlation
14
15

One complex multiple choice question includes several yes-or-no questions.
Some countries also have parent and teacher questionnaire.

9

between items’ difficulty levels and positions. The above position effect would also not exist
if students skipped because they were naive and did not realize that guessing was always
better than skipping.
Another requirement for a question to have been taken seriously is that it be read and
the answer given after due consideration. If too little time is spent on a question to have
been read, let alone answered after thought, the question is seen as being taken non seriously.
This holds for both multiple choice and open response questions.
Response time data has been used as a measure of test-taking motivation in the education
literature. Schnipke and Scrams (1997) and Wise (2006) use methods based on the frequency
distribution of the time spent on each item under the assumption that serious and nonserious students’ response time distributions are different. Wise and Kong (2005) proposed
a threshold selection method based on the item characteristics such as total length of item’s
stem and options.
However, these methods do not take into account the ability of individuals. By using
the same threshold for all test-takers, high-ability test-takers may mistakenly be labeled
as non-serious. We identify non-serious questions taking this issue into account. We first
drop the 1181 students whose total time spent on the science part of the exam is 0 as
there is no information in their responses.16 Then we remove outliers for each country
in terms of total response time, following Chapter 9 in the technical report (see OECD
(2015b), Leys et al. (2013)). Outliers are defined as those whose total response time on the
science part of the exam is too large: i.e., if student i’s total response time, Ri , exceeds
[mean + 3 ∗ median(k(Ri − median)k)] . The median and mean are country specific. The
purpose of this step is to remove students whose total time is too large, possibly due to
technical issues. This cutoff is typically larger than the total time allowed for this part of
the exam. In this step, we drop 5034 students. In total, these 6215 students account for
1.39% of the sample.
Following this, we mark the item for a student in a country as a too-little-time item if

the response time of item j, rj , is less than the maximum of mean − 2.5 ∗ median( (rj −

median) ) and 5 seconds. The median and mean are again country specific. This method is
similar to that used in setting thresholds in Computerized Adaptive Tests (CAT) suggested
in Wise and Ma (2012). The identified too-little-time items will only be treated as nonserious items if they are answered by a student who has at least three too-little-time items
and the fraction correct for too-little-time items is lower than that for normal-time ones.

16

There may have been technical issues that prevented them from taking the exam. In any case, there is
no way for their responses to be imputed as there is no information.

10

3.1

Behavior Patterns of Serious Versus Non-Serious Students

In this section, we want to compare the behavior patterns of non-serious students to
serious ones. We want to do so to assure ourselves that the behavior we are identifying as
non-serious shows patterns that we might expect to see if students were truly not engaged
in taking the exam. To make this comparison we need to identify non serious students. We
could have treated any student for whom an item is imputed as non serious. This would be
an overly broad definition. We choose to use a more conservative one. We first need a few
definitions in order to proceed.
According to PISA terminology, if a student spends some time on an item but does not
answer it, this item is marked as no response (if this item is in the middle of the cluster)
or non reached (if the item is in the end). Table 1 shows a particular student’s answering
pattern. This students spent some time on item 3 and 6 but answered neither of them. At
the same time student answered questions before and after item 3, so item 3 is marked as
no response. This student did not answer any questions after item 6, so item 6 is labeled
as non reached. If a student does not spend any time on an item, this item is marked as
missing. Since it is impossible to spend no time (time is in milliseconds) on items in the
middle of the test, this basically means missing items are only at the end of the test. In this
example, item 7, 8, 9 are all marked as missing as student did not come to these questions
at all. Item 5 is a too-little-time item as the student only spent 3.1 seconds on it. Note that
we follow the (confusing) PISA terminology and label the items which are at the end of the
cluster and for which the student did spend some time on as “non reached”. If the student
did not reach the item at all, it is labeled as missing.
Table 1: No Response, Non Reached and Missing Definition
Ques. Order
Response
Time spent (s)

1

2

3

4

5

6

7

8

9

C
20.4

I
70.3

.
50.3

C
80.4

I
3.1

.
15.5

.
.

.
.

.
.

Table 2 gives the fraction of non reached, no response, missing and too little time items
for each country in columns 1 to 4 for the science component. Note that countries differ in
the way their students are non serious. Brazil and Peru, for example, have the highest share
of missing items, 20% and 12% respectively. On the other hand, the Dominican Republic
has the highest number of non reached items at 15%. Recall that PISA treats both non
reached and missing items as not administered (i.e. it is as if these items were never in the
exam). In contrast, Montenegro has 10% no response items (skipped in the middle of the
11

exam) which are counted as a zero.
We also look at the fraction of no-response and non-reached items in reading and math
subjects as a robustness check. The fraction of no response items for the reading and math
tests are a bit higher on average as shown in Table A.7 in the appendix. It is also highly
correlated with the numbers in science. For example, the correlation between the fraction of
no-response items for science and for reading is 0.98, showing that non seriousness is common
across subjects of the test as might be expected.
3.1.1

Defining Non Serious Students

We implement the definition of non-serious students as follows. A student is non-serious
if too many items are unanswered (non reached, missing or no response) while there is ample
time remaining (more than 5 minutes) to attempt an additional question17 , or if this student
spends too little time on too many questions. In each of the criteria below we set the cutoff
so that no more than 10% of the students meet it.18
Criterion 1. A student is non-serious if more than 5 minutes are left on the exam and
there are K or more multiple choice questions not reached where K is set so that no more
than 10% of the students meet this criteria. In the data K = 1. This criterion covers 4.2%
of the students. Note that we are using only multiple choice questions here, not the open
response ones in order to be more conservative in defining non serious students.
Criterion 2. A student is non-serious if more than 5 minutes are left and at least 2 or
more multiple choice questions are marked as no response. This criterion covers 6.95% of
students.19
Criterion 3. A student is non-serious if more than 5 minutes are left on the exam and
3 or more questions (both multiple choice and open response) are missing. In other words,
there is time left and there are questions they chose not to get to. This identifies 9.33% of
students as being non-serious.
A student spends too little time on an item either because he is randomly guessing an
answer or because he easily gets the true answer. If the latter is the case, then we would
be mislabeling smart students as non-serious.20 We make sure we avoid such mislabeling as
17

There are roughly 60 minutes allocated for the two science clusters which have in total an average of
31 questions.
18
We also did a robustness check by setting the cutoff at a different level, and found similar patterns. (see
Table A.4 and A.3)
19
Note that students who skipped open response questions in the middle of the exam, even if they spent
very little time on them, were not seen as non serious. They could have equally well been labeled as nonserious. However, such open questions, which are both not answered and spent too little time on, only
account for 0.7% of the total questions, so we are not worried this will affect our results.
20
This is indeed an issue as high-ability students (those with high scores) have a higher fraction correct
for too-little-time items than that for normal-time ones, while the opposite is true for low-ability students.

12

follows.
Criterion 4: A student is non-serious if he spends too little time on at least 3 answered
items and the fraction correct for too-little-time items is lower than that for normal-time
ones. This identifies 8.93% of students as being non-serious.
We use the union of these four criteria, and identify 25.69% of the students in the sample
as non-serious students. There is considerable variation in the fraction of non-serious students
across countries with Brazil and the Dominican Republic having over 50% non-serious. The
fraction of non-serious students by country can be found in the last column of Table 6.
It is worth reiterating that time is not a constraint in this exam. Less than 3% of students
have less than 5 minutes left out of 60 minutes allocated for 2 clusters. Table A.8 shows
time per science cluster across positions for serious and non-serious students. As it is clear
from the table, students on average have more than 15 minutes left out of the 60 minutes
allocated for the two clusters 21 .
3.1.2

Data Patterns Suggesting the Presence of Non Seriousness

A strong feature of the data across all countries is that both time spent and accuracy
fall with item order and jump back up after a break. In addition, this seems to be more
so for non-serious students. This pattern is consistent with student “fatigue”. This pattern
is depicted in Figure 1 and 2 where we depict the median time spent and mean accuracy
respectively per item as a function of item order. Time spent on each question (by all
students who are faced with the question and who spend some time on it, whether or not
they answer it) is standardized so it has mean zero and variance 1. If a student spends no
time on an item, it is “missing” as described earlier and is dropped from this calculation.
This standardization removes the impact of question characteristics, such as difficulty and
question type, on time spent. For each position in a cluster, we depict the median of
the standardized time for all questions present in that position for serious and non-serious
students. We further decompose the non-serious student group by plotting the median time
by each of the four criteria separately.
The standardized score for each question is constructed in a similar manner as follows.
Each person either gets the question correct, partially correct or wrong, getting a score of 1,
0.5 or 0 respectively. The standardized score for the question is then normalized with mean
zero and variance 1 to account for differences in, for example difficulty, between questions.
We follow the PISA approach here and drop all questions that are not reached or are missing
and put a score of 0 for questions marked as no response. For each position in a cluster, the
21

To calculate time spent on two clusters we should add time spent on position 1 and 2 or add time spent
on position 3 and 4.

13

average standardized score of the questions in that position is calculated. A lower average
standardized score means the student’s response is less accurate.
Time spent by serious students increases slightly within the first cluster. Then it falls
sharply coming to the second cluster and remains stable in the rest of second cluster. The
same pattern repeats for the third and fourth cluster. Time spent by non-serious students
falls more sharply upon reaching the second and fourth clusters and continues to fall with
item order within a cluster. The cost of skimping on time is accuracy since accuracy closely
tracks time spent as is evident in Figure 2.
The heterogeneity among non-serious students according to the criterion used for classification is also apparent.22 In particular, non-serious students according to criterion 3 (missing
items) spend even more time than serious ones when they answer a question. But looking
at the total time spent on each cluster as in Table A.8, it becomes clear that they spend
the most time of any group on the first cluster, but then spend the least time of any group
on the second cluster. Moreover, this pattern is repeated in the third and fourth cluster.
In other words, they are skipping most of the questions in the second and fourth clusters
despite having plenty of time left.23 Also note that as is evident in Table A.9, these students
are more likely to answer correctly when they attempt a question than other non-serious students. All of this is consistent with their getting tired more quickly as the exam progresses,
and getting reinvigorated during the break. Non-serious students according to criterion 2
and 4 (no response and little time) spend less time and have lower accuracy than non-serious
students overall but the same pattern over item order is present.
Next, in Figure 3 we look at the time spent on correct and incorrect answers24 by serious
and non-serious students as the difficulty level (as measured by the fraction who got the
question correct) rises. In contrast to Figure 1, here time spent is conditional on having
answered the question. We argue below that the patterns here are consistent with serious
students trying to figure out questions when they are not sure of the answer (even if they
get them wrong) while non-serious ones (other than those with missing items) just take their
guess.
Time spent does not rise with difficulty for wrong answers for both serious and nonserious students, but does rise with difficulty for correct answers. Moreover, non-serious
22

We did not plot time spent on the last 3 items for missing-item students because they miss these items
by definition
23
Note that students satisfying criterion 3 have on average 15 more minutes left.
24
To do so we regress time spent on each item on type of question (multiple choice or open ended), position
within a cluster and position of the cluster. We then remove the effect of question type, position and cluster
to get the residual for each student and question. We plot the residuals for correct and incorrect answers for
serious and non-serious students. We do not include individual fixed effects in the regression as we wish to
see how serious and non-serious students differ in their responses.

14

students spend about the same time as serious ones for incorrect answers but spend more
time for correct answers as shown in Figure 3. Though non-serious students spend more
time per question, overall, they spend less time per cluster25 as they answer fewer questions.
Figure 4 shows that students with missing items drive this result as they spend more time
on all questions they attempt.
Removing these students from the non-serious group as in Figure 5 shows that non-serious
students spend roughly the same time as serious ones when they get the answer correct (top
panel), but spend less time when they get it wrong (bottom panel). Serious students spend
roughly the same time on a question independent of whether they get it right or wrong,
while non-serious ones spend less time on questions they get wrong.
In the next section, we investigate the effects of non-seriousness on country rankings in
PISA.

4

Effect on Scores and Rankings

It is clear that students taking PISA non-seriously will tend to reduce the average country
score and adversely affect countries’ rankings. In this section, we explain how we adjust scores
to account for non-seriousness. We then present results that quantify the effect of non-serious
behavior on country scores and rankings. We also decompose the change in score into its
component parts.
To correct the potential bias of being non-serious, we use Multiple Imputation by Chained
Equations (MICE) to impute scores for all non-serious questions. Recall these were questions
that were not reached, for which there was no response, were missing, or on which too little
time was spent.26 All of these are treated as missing data. Non-reached and no-response
items were looked at by the student who then chose not to answer the item despite having
time left. Had he taken the exam seriously, he would have answered to the best of his ability
which is exactly what the imputation does. Note that in Section 3.1.1 we did not include
open response items in criterion 1 (non reached) and criterion 2 (no response) to define
non-serious students 27 . We did so as we wanted to be conservative in terms of defining who
was non serious. After all, skipping open response items could well be due to not knowing
the answer and guessing being a waste of time with open response items. Since we want
to estimate performance had all questions been taken seriously, in this section we always
impute open response questions as long as they are taken non seriously. Missing items are
25

Serious students spend 19.5 minutes per cluster while non-serious ones spend 17.8 minutes per cluster.
We only impute too little time items for students who satisfy Criterion 4.
27
We did include both multiple choice and open response items in criterion 3 (missing) and criterion 4
(too little time items)
26

15

not even looked at by students despite having time left. Not even bothering to look at the
question again is an indication of non seriousness, and this is why we impute the answers.
We also impute too little time items but only for people who seem to be paying a price in
terms of accuracy for greater speed. Again, these people are not serious.
Multiple imputation involves filling in all the missing data multiple times, creating multiple complete datasets which are then averaged over for the final imputation. The missing
values are imputed based on the observed values for the given individual and the relations
observed in the data for other participants (Schafer and Graham (2002)). The variables
used for imputation for a given individual are laid out in Table A.10. They include the
individual’s scores for other science questions in the test, other participants’ scores for all
science questions, the individual’s characteristics, school characteristics and country fixed
effects. The same individual and school characteristics are used by PISA in generating their
plausible values. We also use a dummy indicating whether the student is non serious or not.
If non serious students are more alike in their responses than serious ones, it makes sense to
include this variable in the imputation.
Since imputation attempts to assign values for missing data based on the responses for
similar individuals/questions/schools, one needs to assume that the probability of being
non-serious is random after controlling for all the observables.28 In the MICE procedure
a series of regression models are run whereby each variable with missing data is modeled
conditional upon the other variables in the data. This means that each variable can be
modeled according to its distribution (Azur et al. (2011)). In our model, whether a question
is right or wrong and school type are binary variables29 , therefore they are modeled using a
logistic regression and all other continuous variables are modeled using linear regressions.
One might be concerned that if students spend more time on a question they had skipped
or spent too little time on, their behavior may change on the questions that they actually
had answered. There are at least two possible channels here. First, they may have less time
to spend on other questions. Second, they may be more fatigued after answering/spending
more time. Since students have almost two more minutes they can use for each non-serious
item in addition to the time they had already spent, the time constraint is unlikely to
be binding, so the first channel seems irrelevant. As far as the second channel goes, our
imputation attempts to assign values based on the responses of similar individuals who have
the similar observable characteristics and take the same questions in the same order and so
should incorporate this potential "fatigue" effect.
28

If this were not so, there would be no similar individuals/items/schools to impute from.
In the imputation, we categorize partial credit answers as wrong answers for simplicity. On average
students have only 8% of questions in their exams which allow partial credit.
29

16

A feature of PISA tests is that students get different clusters of questions. Even if two
students have a common cluster of questions, the position of the cluster might differ. We
have seen in Section 3.1 that the position of an item has a substantial effect on student’s
performance on this item. Imputation of an item’s score has to use the relations for other
individuals who answer the same item in the same position. In the PISA test, all students are
assigned a random number which determines the specific science clusters included on the test
as well as their positions. We divide all students into 72 groups so that students in each group
get the same questions in the same order30 . Then we conduct multiple imputations within
each group. By doing multiple imputations we get the probability of a student answering a
given question correctly. From this, we can generate the distribution of total number correct
which follows a Poisson binomial distribution. Ten values are drawn from this distribution
which is unique for each student. Students with no imputations made have the same value
for all ten draws.
Next we describe how to calculate student scores and country rankings based on all
students’ item responses, i.e., in all 72 groups. As different students take different tests,
PISA imputes plausible values for a common test using a population model that combines
item response theory (IRT) and a latent regression model, see chapter 9 of (OECD (2015b))
for details. This is a rather complex procedure that is carried out for PISA by the Educational
Testing Service and is a bit of a black box as the codes are not freely available. Instead of
trying to replicate their approach we use the following method. Let us use the calculation of
original score (OS) as an example. We first calculate fraction correct with skipped items at
the end being counted as incorrect using the raw data and assume that this fraction correct
follows a normal distribution. We then standardize this score for each group that got the same
test (with OECD countries having a mean of 500 and a standard deviation of 100) so that
their performance is comparable. Since students are assigned to the 72 groups randomly,
we can say that the same kinds of students took each test on average. Standardizing as
above controls for different booklets having different levels of difficulty. Since our focus is on
country averages/rankings, it is not necessary to control for the difficulty of each question
within a booklet as done by PISA, once we have controlled for the difficulty of each booklet.
The next step is to standardize the imputed score so that it is both comparable across
booklets and comparable with the original score. If we just followed what we did for the
original score, we would get a score which was comparable across booklets but which could
not be compared to the original score distribution as both would be scaled to have a mean
30

There are 36 random numbers in total which determine the specific science clusters assigned to students.
Moreover, students have science clusters either in the first two sessions or in the last two sessions. Therefore
in total there are 72 groups within which students answer the same questions in the same order.

17

of 500 and a standard error of 100 for OECD countries.
Here we use a similar approach as PISA’s in Chapter 12 of (OECD (2015b)). Going from
fraction correct of the original data to the normalized data involves an adjustment to the
mean and the variance since the distributions are assumed to be normal. For example, if the
original data, X, had mean µ and variance σ 2 , the normalized variable, Y , would be given
by
Y = AX + B
and B = 500 − 100µ
. These 72 pairs of adjustment factors for the mean
where A = 100
σ
σ
and variance are then applied to the imputed fraction correct to get the normalized imputed
scores which are comparable across both booklets and comparable with the original scores.
We do this for each of the ten draws and thus get ten imputed scores for each student. Since
PISA also generates ten plausible values, we follow their approach to calculate the mean and
standard deviation for each country for the original or imputed versions of the normalized
scores31 . Note that our scores and those in the PISA 2015 report are not comparable directly
as they use scores in 2006 for the Science part as the base while we do not.
Table 3 contains the heart of the analysis. In order to understand the effect of being
non-serious on country scores, we compare the scores (always normalized as above) after
we impute the data for items not taken seriously to the scores under the status quo. One
status quo takes the normal practice of assigning zero to all skipped items 32 . These scores
are shown in the first column. The PISA approach (treating skipped items at the end as
not administered) is used as the status quo in the second column33 . In the third column,
the fully imputed score is shown. The fourth column gives the imputed score when skipped
items at the end are ignored. Standard errors are below each score.
The fifth, sixth and seventh columns give the t-statistic for the significance of the difference in column 1 and 3, columns 2 and 3, and 3 and 4 respectively. Comparing columns 1
and 3 we compare the imputed score to the original score when all items count. As seen in
column 4, these are significantly different for 50 out of 58 countries at the 5% level and for
46 of them also at the 1% level. This means that if a country could make its students take
the exam seriously, it could do much better. Comparing columns 2 and 3, we see that using
the PISA approach as the status quo brings these numbers closer. A smaller fraction are
31

See page 148 of OECD (2015b), chapter 9.
This is also the practice used by Gneezy et al. (2017)
33
To quote PISA (page 149 of OECD (2015a))
“Omitted responses prior to a valid response are treated as incorrect responses; whereas, omitted responses
at the end of each of the two one-hour test sessions in both PBA and CBA are treated as not reached/not
administered.”
32

18

significantly different from one another - only 7 differ at the 5% level of significance and 2 at
the 1% level. Thus, the PISA way of treating skipped items at the end as not administered
goes part way toward accounting for non seriousness. Finally, comparing columns 3 and 4,
we see that imputing all the items and imputing only the no response (skipped in the middle
of the exam) and too little time items give results that are essentially the same as none differ
significantly.
Table 4 presents the list of countries and their ranks before and after imputation. Column
1 shows the rank based on the original scores, i.e., column 1 in Table 3. Column 2 shows
the rank based on the imputed scores, i.e., column 3 in Table 3. This corresponds to every
country becoming serious. Column 3 shows the rank if only this country is serious. Column
4 shown the rank if all other countries become serious and this country does not. Below each
rank is the corresponding rank interval at the 95% confidence levels.
Comparing columns 1 and 3, we see that 54 of 58 countries differ in the two columns.
Among them 24 countries have significantly different ranks as the intervals do not overlap.
Notice that countries always move up in the ranking in this thought experiment as their
score can only rise with the imputation. This change captures the extent to which a single
country could strategically raise its rankings by somehow getting its own students to take
the exam seriously.
Similarly, while the rank in columns 1 and 4 (all other countries become serious) differ for
55 countries, only 26 of them are significantly different. If other countries become serious,
while you do not, your ranking can only fall. Again, some countries are less affected than
others. Singapore for example is unaffected even in this case, while Ireland would fall from
18 to 31 if this were to happen.
Finally, the rank between columns 1 and 2 (everyone becomes serious) differ for 36 countries, but only 3 of these are significantly different. In other words, if all countries become
serious, there is little significant change in the rankings. As is evident, some countries rise
in the rankings (Japan) while others fall (Slovenia). However, overall there is a far smaller
change in the rankings. This makes sense. If one country can get its students to be serious
about the exam, it can change its ranking a lot. But if everyone does so, general equilibrium
effects come into play and individual efforts are negated.
Looking at some interesting individual countries, we see that Singapore and Chinese
Taipei (Taiwan) do not change rank between columns 1 and 3 , while Portugal moves up
by 15 places. It is also clear that countries at the top and bottom of the original rankings
tend to move less than countries in the middle. This arises from the score gap between
sequentially ranked countries being large at the top and bottom and smaller in the middle.
For example, Singapore has a score of 564.9 in column 1 of Table 3 while the next ranked
19

country, Taiwan, has a score of 547.8. Similarly, the Dominican Republic which is last has a
score of 365.4 while Tunisia, which is second last, has a score of 395.6.34 Small wonder that
Singapore stays first in all the columns and the Dominican Republic stays last.
Next, we investigate why some countries improve their performance a lot, while others
do not.

5

Proportion, Ability and Extent

When we impute the data for questions not taken seriously, the fraction of questions
correctly answered will typically rise. In this section we decompose the source of this increase
in the fraction correct (y) into three component parts for each country and for serious and
non-serious students separately. The first part depends on the ability (a) of the non-serious
student. The more able the student, the more likely he is to get the question right and
the greater the increase in the fraction correct when we make our corrections. The second
part depends on how prevalent the imputed items are, i.e., the extent (e) to which these
items occur. If they are very prevalent, then our imputation will have a greater impact.
We expect them to be more prevalent for non-serious students than for serious students so
that the correction will have more of an impact for the former. The third part depends
on the proportion (p) of non-serious students in the population: the greater the fraction of
non-serious students, the greater the increase in the fraction correct.

5.1

Sources of Increases in the Fraction Correct

Let Ti be the total number of items in student i’s test as this is individual specific. Let Ci
be the number correct for i in the data and Ĉi be the number correct with the imputed data.
Let Ii = Ĉi − Ci denote the increase in student i’s number correct if he was serious about all
items. A country has S serious students and N S non-serious students. The fraction correct
for this country in the data is
P
Ci
i∈S∪N S
FC = P
Ti
i∈S∪N S

34

These numbers differ slightly from the numbers in the original working paper posted as we used sampling
weights for each student in this version and not in the earlier one. The ranks do not change across the versions.

20

while the fraction correct after imputation is
P
ˆ

FC =

Ĉi

i∈S∪N S

P

Ti

i∈S∪N S

If all students in this country became serious on all items, the increase in the average fraction
correct for this country, IF C, can be expressed as:

P
IF C =

Ii

i∈S∪N S

P

Ti

i∈S∪N S

P
=

i∈N S

P
i∈S∪N S
 P

=

P

Ii

 i∈N S
P

Ti

+

Ii

i∈S

P

(1)

Ti

i∈S∪N S

Ii



P
i∈N S

Ti

i∈N S



P

Ti

P
i∈S∪N S

Ti

+

 i∈S
P

i∈S

Ii



P

Ti

i∈S

Ti



P

Ti

(2)

i∈S∪N S

= IF Cns Pns + IF Cs (1 − Pns )

(3)

= Yns + Ys

(4)

where IF Cns , and IF Cs is the increase in fraction correct for non-serious students and serious
students respectively, and Pns is the proportion of non-serious students in the population.
In the PISA test, students have different numbers of science items, and this is determined
randomly. Thus, on average, non-serious students have the same number of total items as
serious students so that Pns measures the proportion of non-serious students in a country.
Thus, the increase in the fraction correct is a linear combination of the increase in the fraction
Yns
is 0.74 so that most
correct for serious and non-serious students. It is worth noting that IF
C
of the increase comes from non-serious students.
Next we will decompose IF Cns (and IF Cs ) into their component parts. Let N Ii be the
number of non-serious items student i has.35
P
P
P
(Ii )
(Ii )
N Ii
i∈N S
i∈N S
i∈N S
P
IF Cns = P
= P
= Ans Ens
Ti
N Ii
Ti
i∈N S

i∈N S

35

i∈N S

Recall that non-serious items include non-reached, no-response and missing items, and items with too
little time if a student spends too little time on at least three items and the fraction correct for littletime items is lower than that for normal-time ones. Here we also include open response items which are
non-reached or no-response.

21

Ans is the average increase in the fraction correct for non-serious items among non-serious
students. As explained below, we would expect this to be increasing in non-serious students’
ability. Ens is the average of the fraction of non-serious items among all items for non-serious
students, which measures the degree of non-seriousness for non-serious students.
Thus,
Yns = Ans Ens Pns .
The values of Y , A, E and P for each country are provided in Table 6. Dividing both sides
by the geometric mean gives




Ens
Pns
Yns
Ans
=
Ȳns
Āns
Ēns
P̄ns
yns = ans ens pns .

(5)

We de-mean to make sure the regressions below start from the origin. Take the logarithm
on both sides of (5) gives:
ln(yns ) = ln ans + ln ens + ln pns

(6)

If we want to know how much of the variation in ln yns comes from each of the three components, we can use a simple trick. Suppose we run the regression of ln ans , ln ens , ln pns
separately on ln yns , that is,
ln ans = α1 ln yns + a
ln ens = β1 ln yns + d
ln pns = γ1 ln yns + p.
36

Let the OLS estimates be denoted by α̂1 , β̂1 , γ̂1 and note that
α̂1 ln(yns ) + β̂1 ln(yns ) + γ̂1 ln(yns ) =




α̂1 + β̂1 + γ̂1 ln(yns )

= ln ans + ln ens + ln pns
= ln yns
so that αˆ1 +βˆ1 +γˆ1 = 1 and we can use the coefficients αˆ1 , βˆ1 , γˆ1 to measure the contribution of
non-serious students’ ability, extent of non-seriousness and proportion to a country’s increase
in fraction correct by non-serious students.
We can decompose the increase in the fraction correct coming from serious students
36

These three regression lines add up to the 450 line.

22

(what we call partially serious and fully serious) in an analogous manner. Details are in the
Appendix A.5.

5.2

Results of the Decomposition

Table 5 summarizes the decomposition results of yns and ys .37 Column 1 shows that for
non-serious students, proportion accounts for 68% of the increase in fraction correct while
the extent of non-seriousness accounts for about 26%, and least important is ability which
accounts for only 6% of the variation. Column 2 shows the similar results for partiallyserious students. Proportion accounts for 64% of the variation for serious students, while
extent accounts for 32% and ability accounts for 4%.
Figure 6 plots the scatter plot and regression lines above for non-serious students. The
countries with high yns tend to be those who would gain a lot from their students taking
the exam seriously. Where does the gain come from? As is evident from the figure, Brazil
stands to gain the most. This gain is driven by the large proportion of non-serious students
and the high extent of non-seriousness. However, the contribution of ability is relatively
small: even if the exam had been taken seriously, the performance would not have improved
so much as non-serious students in Brazil are of low ability. The same story applies to
Dominican Republic. In contrast, both Russia and Portugal who also have high yns have
the contribution of ability being high since their non-serious students are quite able. Both
Netherlands and Turkey gain very little because the proportion of their non-serious students
are very low, so are these students’ ability and extent of non-seriousness. US’s non-serious
students ability, extent and proportion roughly track their gains as all these values are at a
median level among all countries.

6

Conclusion

The PISA exam which is seen as the gold standard for evaluating how countries are faring
in terms of their education system is a low-stakes exam. As such, there is little incentive for
students to take the exam seriously. It is well understood that this feature limits the accuracy
of the results and biases the resulting rankings. However, there is (i) no attempt to quantify
the score gains across a host of countries from students taking the exam seriously and the
consequent effects on rankings, (ii) no decomposition of score gains into their constituent
parts.

37

Imputed number correct is calculated by taking the mean of ten draws of number correct.

23

We show that scores and rankings change substantially when non-seriousness of the students is taken into account. The comparison between fully imputed score (FIS) and the
original score (OS) shows that most of the countries increased their scores significantly were
a country to make its students take the exam seriously. For example, Brazil’s score increases
by 29 points and its fraction correct increases by 6.72%. This change leads to a rise of 5
places in the rankings from 56 to 51. We also show that 24 out of 58 countries increase their
rank significantly, i.e., rank confidence intervals of OS and FIS do not overlap. A country
can improve by up to 15 places if its students are encouraged to take the exam seriously,
but if all countries become serious, then the change in the rankings would be small. The
PISA approach partially accounts for non-seriousness by treating skipped items at the end
as not administered.38 However, such an approach is subject to manipulation: a country can
game the system by instructing its students to spend as much time as they need on earlier
questions and to quit the latter questions if they do not have time or feel tired.
We decompose the source of the increase in fraction correct into the part that comes from
the proportion, ability, and extent (intensity). Using a standard decomposition, we show that
the contribution of the three components varies widely across countries. For example, the
Dominican Republic has a large increase in fraction correct because it has a high proportion
of non-serious students who take a large fraction of questions non seriously. However, the
contribution of ability is relatively small as its non-serious students are of low-ability. The
Russian Federation has a similar gain in fraction correct despite its proportion of non-serious
students being much lower. The reason is that their non-serious students have much higher
ability. We also show that across all countries, roughly 68% of the increase in fraction correct
comes from the proportion component, 26% comes from the extent component and 6% comes
from the ability component.
This paper thus has a simple bottom line. Using PISA scores and rankings as done
currently paints a distorted picture of where countries stand in both absolute and relative
terms. Simple adjustments like those proposed here help provide a better picture.

Acknowledgements
We are grateful to participants in the Econometrics Society meetings in Shanghai, China in 2018,
in the International Association of Applied Econometrics Conference in Cyprus in June 2019, in
the 9th ifo Dresden Workshop on Labor Economics and Social Policy in May 2019, in the European
Society for Population Economics (ESPE) in Bath, UK in June 2019. We would particularly like
to thank Joris Pinkse, Keisuke Hirano, and Kim Ruhl for their comments and suggestions and
38

Note that they do not account for skipped items in the middle and too little time items.

24

Meghna Bramhachari for help in proofreading. We owe special thanks to colleagues at the OECD
for answering our numerous questions about the data. Huacong Liu was instrumental in our working
on this project, and we thank her for all her help. We are responsible for all errors.

References
Attali, Y., Neeman, Z., and Schlosser, A. (2011). Rise to the challenge or not give a damn: Differential performance in high vs. low stakes tests.
Azmat, G., Calsamiglia, C., and Iriberri, N. (2016). Gender differences in response to big stakes.
Journal of the European Economic Association, 14(6):1372–1400.
Azur, M. J., Stuart, E. A., Frangakis, C., and Leaf, P. J. (2011). Multiple imputation by chained
equations: What is it and how does it work? International journal of methods in psychiatric
research, 20(1):40–49.
Baumert, J. and Demmrich, A. (2001). Test motivation in the assessment of student skills: The
effects of incentives on motivation and performance. European Journal of Psychology of Education,
16(3):441.
Borghans, L. and Schils, T. (2012). The leaning tower of pisa: decomposing achievement test scores
into cognitive and noncognitive components. Unpublished manuscript.
Butler, J. and Adams, R. J. (2007). The impact of differential investment of student effort on the
outcomes of international studies. Journal of Applied Measurement, 8(3):279–304.
Cole, J. S., Bergin, D. A., and Whittaker, T. A. (2008). Predicting student achievement for low
stakes tests with effort and task value. Contemporary Educational Psychology, 33(4):609–624.
Duckworth, A. L., Quinn, P. D., Lynam, D. R., Loeber, R., and Stouthamer-Loeber, M. (2011).
Role of test motivation in intelligence testing. Proceedings of the National Academy of Sciences,
108(19):7716–7720.
Eklöf, H. (2010). Skill and will: test-taking motivation and assessment quality. Assessment in
Education: Principles, Policy & Practice, 17(4):345–356.
Finn, B. (2015). Measuring motivation in low-stakes assessments. ETS Research Report Series,
2015(2):1–17.
Gneezy, U., List, J. A., Livingston, J. A., Sadoff, S., Qin, X., and Xu, Y. (2017). Measuring success
in education: the role of effort on the test itself. Technical report, National Bureau of Economic
Research.

25

Hanushek, E. A., Link, S., and Woessmann, L. (2013). Does school autonomy make sense everywhere? panel estimates from pisa. Journal of Development Economics, 104:212–232.
Hanushek, E. A. and W ößmann, L. (2006).

Does educational tracking affect performance

and inequality? differences-in-differences evidence across countries. The Economic Journal,
116(510):C63–C76.
Huang, J. L., Curran, P. G., Keeney, J., Poposki, E. M., and DeShon, R. P. (2012). Detecting and
deterring insufficient effort responding to surveys. Journal of Business and Psychology, 27(1):99–
114.
Jacob, B. A. (2005). Accountability, incentives and behavior: The impact of high-stakes testing in
the chicago public schools. Journal of public Economics, 89(5-6):761–796.
Jalava, N., Joensen, J. S., and Pellas, E. (2015). Grades and rank: Impacts of non-financial incentives
on test performance. Journal of Economic Behavior & Organization, 115:161–196.
Jerrim, J. (2016). Pisa 2012: How do results for the paper and computer tests compare? Assessment
in Education: Principles, Policy & Practice, 23(4):495–518.
Jerrim, J., Micklewright, J., Heine, J.-H., Salzer, C., and McKeown, C. (2018). Pisa 2015: how big
is the ‘mode effect’and what has been done about it? Oxford Review of Education, 44(4):476–493.
Krosnick, J. A., Narayan, S., and Smith, W. R. (1996). Satisficing in surveys: Initial evidence. New
directions for evaluation, 1996(70):29–44.
Lavy, V. (2015). Do differences in schools’ instruction time explain international achievement gaps?
evidence from developed and developing countries. The Economic Journal, 125(588):F397–F424.
Leys, C., Ley, C., Klein, O., Bernard, P., and Licata, L. (2013). Detecting outliers: Do not use
standard deviation around the mean, use absolute deviation around the median. Journal of
Experimental Social Psychology, 49(4):764–766.
Lounkaew, K. (2013). Explaining urban–rural differences in educational achievement in thailand:
Evidence from pisa literacy data. Economics of Education Review, 37:213–225.
OECD (2015a). Pisa 2015 results(volumn 1): Excellence and equity in education. Technical report,
OECD.
OECD (2015b). Pisa 2015 technical report. Technical report, OECD.
Penk, C. and Richter, D. (2017). Change in test-taking motivation and its relationship to test
performance in low-stakes assessments. Educational Assessment, Evaluation and Accountability,
29(1):55–79.

26

Pintrich, P. R. and De Groot, E. V. (1990). Motivational and self-regulated learning components
of classroom academic performance. Journal of educational psychology, 82(1):33.
Prince Edward Island (2002). Preparing students for pisa, mathematical literacy, teacher’s handbook. Technical report, Prince Edward Island.
Schafer, J. L. and Graham, J. W. (2002). Missing data: Our view of the state of the art. Psychological
Methods, 7:147–177.
Schnipke, D. L. and Scrams, D. J. (1997). Modeling item response times with a two-state mixture
model: A new method of measuring speededness. Journal of Educational Measurement, 34(3):213–
232.
Wise, S. L. (2006). An investigation of the differential effort received by items on a low-stakes
computer-based test. Applied Measurement in Education, 19(2):95–114.
Wise, S. L. and DeMars, C. E. (2005). Low examinee effort in low-stakes assessment: Problems and
potential solutions. Educational Assessment, 10(1):1–17.
Wise, S. L. and Kong, X. (2005). Response time effort: A new measure of examinee motivation in
computer-based tests. Applied Measurement in Education, 18(2):163–183.
Wise, S. L. and Ma, L. (2012). Setting response time thresholds for a cat item pool: The normative
threshold method. In annual meeting of the National Council on Measurement in Education,
Vancouver, Canada.
Wolf, L. F. and Smith, J. K. (1995). The consequence of consequence: Motivation, anxiety, and
test performance. Applied Measurement in Education, 8(3):227–242.
Zamarro, G., Hitt, C., and Mendez, I. (2016). When students don’t care: Reexamining international
differences in achievement and non-cognitive skills.

27

Figure 1: Standardized Time for Serious and Non-serious Students

1

Serious
Non-serious
Nonreached
Missing
No response
Little Time

Standardized Time

0.5

0

-0.5

-1

-1.5

1st Cluster

2nd Cluster

3rd Cluster

4th Cluster

Item Order

Note: Data Source: 2015 PISA Cognitive item dataset. Time spent on each question (by all students who
are faced with the question and who spend some time on it, whether or not they answer it) is standardized
so it has mean zero and variance 1. For each position in a cluster, the median standardized time of the
questions in that position is calculated. The y-axis depicts the median time spent on items in each order.

28

Table 2: Fraction of Non-serious items
Fraction of
Non-reached items (%)
0.62
Singapore
Chinese Taipei
0.58
Estonia
0.92
Japan
0.97
Finland
0.75
Hong Kong
0.65
USA (Massachusetts)
0.45
Canada
1.02
Macao
0.31
Slovenia
1.11
B-S-J-G (China)
0.87
Netherlands
0.71
Korea
1.06
United Kingdom
1.39
Germany
1.38
Australia
1.37
New Zealand
1.46
Ireland
1.05
Poland
1.14
Denmark
1.57
Switzerland
1.50
USA (North Carolina)
0.43
Belgium
1.35
Austria
1.34
Norway
1.75
Czech Republic
1.25
United States
0.61
Spain (Regions)
1.21
France
2.19
Spain
1.21
Portugal
1.37
Latvia
0.82
Sweden
2.06
Italy
1.70
Lithuania
1.41
Luxembourg
1.57
Hungary
1.18
Croatia
1.28
Russian Federation
1.37
Iceland
1.67
Slovak Republic
1.31
Israel
1.96
Greece
1.73
Bulgaria
2.15
Chile
2.26
United Arab Emirates
1.68
Turkey
1.28
Uruguay
2.87
Qatar
3.73
Thailand
0.35
Costa Rica
1.27
Colombia
2.32
Montenegro
2.94
Mexico
1.09
Peru
1.07
Brazil
1.91
Tunisia
5.11
Dominican Republic
14.97
Overall
1.62
Country

Fraction of
No-response items (%)
1.30
1.98
1.83
2.78
2.13
1.60
1.18
2.09
0.98
3.27
2.02
1.61
2.51
3.31
3.43
3.20
3.38
2.10
3.02
3.30
3.47
1.22
3.06
4.00
3.59
3.84
1.44
2.88
4.75
2.91
3.40
2.25
4.76
4.08
3.77
4.27
3.89
4.35
3.47
3.75
4.20
4.37
3.95
6.14
4.05
3.11
4.26
6.44
4.95
1.89
3.22
2.78
9.54
1.98
3.46
5.57
7.20
7.94
3.48

Fraction of
Missing items (%)
0.57
0.19
0.86
1.18
0.72
0.68
1.83
0.86
2.21
0.32
0.75
0.03
0.04
0.52
1.51
2.32
3.14
0.60
0.85
1.61
1.58
1.82
2.42
0.70
1.57
1.10
2.49
1.96
1.67
2.53
3.99
1.08
3.37
1.37
0.80
2.45
1.74
1.06
4.96
1.90
2.04
3.74
0.96
2.91
3.37
0.57
0.14
4.83
0.26
4.22
5.89
3.86
3.61
7.76
12.44
20.40
6.68
1.22
3.04

Fraction of
Too-little-time items (%)
2.15
1.74
1.94
1.65
1.67
2.05
2.06
1.72
2.14
1.77
2.02
2.24
1.87
1.60
1.57
1.25
1.36
1.95
2.13
1.63
1.82
1.84
1.62
1.41
1.53
1.56
1.75
1.83
1.37
1.85
0.97
1.67
1.19
1.40
1.26
1.58
1.52
1.32
1.26
1.44
1.26
1.78
1.67
1.08
1.44
1.42
1.57
0.61
2.02
0.70
1.16
1.20
0.73
1.16
1.01
0.17
0.45
0.62
1.46

Note: In this table non-reached items include non-reached open response items and no-response items include
no-response open response items.

29

Figure 2: Standardized Score for Serious and Non-serious Students

0.2
Serious
Non-serious
Nonreached
Missing
No response
Little Time

0.1

0

Standardized Score

-0.1

-0.2

-0.3

-0.4

-0.5

-0.6

-0.7

-0.8

1st Cluster

2nd Cluster

3rd Cluster

4th Cluster

Item Order

Note: Data Source: 2015 PISA Cognitive item dataset. The score for each question, 0, 0.5 or 1, is standardized so the overall score has mean zero and variance 1. Items that are not reached or missing are dropped
from the sample. The no response items are assigned a score of 0. For each position in a cluster, the average
standardized score of all questions in that position is calculated. The y-axis depicts the mean standardized
score of the items in each order.

30

Figure 3: Time for Correct and Incorrect Answers for Serious and Non-serious Students
(a) Time for Correct Answers
200

Correct/ Non Serious
Correct/ Non Serious(Mean)
Correct/ Serious
Correct/ Serious(Mean)

Residuals of Time (seconds)

150

100

50

0

-50

-100

0

20

40

60

80

100

120

140

160

180

200

Item Difficulty

(b) Time for Incorrect Answers
100

Incorrect/ Non Serious
Incorrect/ Non Serious(Mean)
Incorrect/ Serious
Incorrect/ Serious(Mean)

80

Residuals of Time (seconds)

60

40

20

0

-20

-40

-60

-80

-100

0

20

40

60

80

100

120

140

160

180

200

Item Difficulty

Note: Data Source: 2015 PISA Cognitive item dataset. The residuals of time spent for each student and
question are obtained by running a regression of time spent on each item on type of question (multiple
choice or open-ended), position within a cluster and position of the cluster and getting the residuals. Here
time spent is conditional on having answered the question. The y-axis depicts the mean of the residual time
relative to the difficulty of the items which is measured by the fraction who got the question correct. The
green line is for non-serious students and the red line is for serious students.

31

Figure 4: Time for Correct and Incorrect Answers for Serious and Missing-item Students
(a) Time for Correct Answers
250

Correct/ Missing
Correct/ Missing (Mean)
Correct/ Serious
Correct/ Serious (Mean)

Residuals of Time (seconds)

200

150

100

50

0

-50

-100

0

20

40

60

80

100

120

140

160

180

200

Item Difficulty

(b) Time for Incorrect Answers
250

Incorrect/ Missing
Incorrect/ Missing (Mean)
Incorrect/ Serious
Incorrect/ Serious (Mean)

Residuals of Time (seconds)

200

150

100

50

0

-50

-100

0

20

40

60

80

100

120

140

160

180

200

Item Difficulty

Note: Data Source: 2015 PISA Cognitive item dataset. The residuals of time spent for each student and
question are obtained by running a regression of time spent on each item on type of question (multiple
choice or open-ended), position within a cluster and position of the cluster and getting the residuals. Here
time spent is conditional on having answered the question. The y-axis depicts the mean of the residual time
relative to the difficulty of the items which is measured by the fraction who got the question correct. The
red line is for serious students while the black line is for non-serious students who satisfy criterion 3, or
missing-item students.

32

Figure 5: Time for Correct and Incorrect Answers for Serious and Non-serious Students
After Removing Missing-item Students
(a) Time for Correct Answers
150

Residuals of Time (seconds)

100

50

0

-50

-100

-150

Correct/ Non Serious
Correct/ Non Serious(Mean)
Correct/ Serious
Correct/ Serious(Mean)
0

20

40

60

80

100

120

140

160

180

200

Item Difficulty

(b) Time for Incorrect Answers
60

40

Residuals of Time (seconds)

20

0

-20

-40

-60

-80

Incorrect/ Non Serious
Incorrect/ Non Serious(Mean)
Incorrect/ Serious
Incorrect/ Serious(Mean)

-100

-120

0

20

40

60

80

100

120

140

160

180

200

Item Difficulty

Note: Data Source: 2015 PISA Cognitive item dataset. The residuals of time spent for each student and
question are obtained by running a regression of time spent on each item on type of question (multiple
choice or open-ended), position within a cluster and position of the cluster and getting the residuals. Here
time spent is conditional on having answered the question. The y-axis depicts the mean of the residual time
relative to the difficulty of the items which is measured by the fraction who got the question correct. The red
line is for serious students while the black line is for non-serious students excluding missing-item students.

33

Table 3: Country Scores After Different Imputations

Country
Singapore
Chinese Taip
Estonia
Japan
Finland
Hong Kong
USA (Massachusrtts)
Canada
Macao
Slovenia
B-S-J-G (China)
Netherlands
Korea
United Kingdom
Germany
Australia
New Zealand
Ireland
Poland
Denmark
Switzerland
USA (North Carolina)
Belgium
Austria

t-statistics
Difference
(2)- (3)

OS
(1)

SENA
(2)

FIS
(3)

Imputed SENA
(4)

Difference
(1)- (3)

564.9
(1.06)
547.8
(2.42)
546.8
(1.96)
546.5
(2.75)
544.2
(2.04)
543.6
(2.57)
540.2
(6.34)
538.8
(1.88)
535
(1)
529.9
(1.31)
529.4
(4.27)
526.4
(2.25)
526.1
(3.02)
524.1
(2.25)
523
(2.5)
518.2
(1.4)
517.5
(2.47)
517
(2.15)
515.9
(2.48)
515.5
(2.16)
514.9
(2.64)
513.9
(5.1)
512
(2.06)
511.8

567.3
(1.02)
549
(2.4)
550.8
(2.01)
554.4
(2.83)
549.3
(2.07)
546.5
(2.55)
544.8
(6.08)
542.7
(1.85)
541.3
(0.98)
532.5
(1.31)
532.8
(4.23)
527.2
(2.23)
530.3
(3.02)
527.2
(2.24)
529.1
(2.51)
524.8
(1.4)
527.6
(2.35)
520.3
(2.13)
520.2
(2.47)
521.2
(2.24)
523.3
(2.72)
518.5
(4.95)
520.8
(2.16)
515.4

570.9
(1.32)
553.3
(2.52)
555.1
(2.29)
557
(3.04)
552.4
(2.24)
551.2
(2.71)
548.5
(5.93)
546.6
(2.1)
544
(1.61)
536.9
(1.73)
537.5
(4.38)
530.9
(2.31)
532.1
(3.06)
532.7
(2.48)
535
(2.89)
528.4
(1.93)
532.7
(2.97)
524.7
(2.39)
525.8
(2.82)
525.4
(2.51)
526.7
(3.09)
521.7
(4.97)
525.2
(2.7)
521.7

570.8
(1.4)
553
(2.63)
555.1
(2.31)
560.3
(3.17)
554
(2.33)
551.1
(2.79)
548.5
(5.9)
546.9
(2.11)
544.4
(1.5)
537.2
(1.93)
537.7
(4.43)
530.7
(2.35)
535.1
(3.18)
532.7
(2.6)
535.2
(2.94)
529.3
(1.83)
533.8
(2.72)
524.7
(2.49)
526.1
(3)
526.1
(2.55)
529
(3.03)
521.8
(4.92)
526.2
(2.6)
521.8

4.31

2.14

0.02

2.79

1.26

0.1

3.87

1.42

0.02

5.4

0.62

-0.76

2.69

0.99

-0.51

2.79

1.25

0.04

3.91

0.44

0

1.58

1.41

-0.08

3.37

1.43

-0.2

3.88

2.05

-0.11

2.95

0.77

-0.03

3.61

1.13

0.04

2.97

0.42

-0.68

4.62

1.64

0.01

2.78

1.53

-0.05

2.71

1.52

-0.32

4.34

1.36

-0.27

3.13

1.36

-0.01

2.01

1.51

-0.06

2.02

1.23

-0.2

3.18

0.81

-0.54

4.36

0.46

-0.02

2.37

1.27

-0.27

3.23

1.79

-0.03

(continued on next page)

34

Difference
(3)- (4)

t-statistics
Country

Norway
Czech Republic
United States
Spain (Regions)
France
Spain
Portugal
Latvia
Sweden
Italy
Lithuania
Luxembourg
Hungary
Croatia
Russian Federation
Iceland
Slovak Republic
Israel
Greece
Bulgaria
Chile
United Arab
Turkey
Uruguay
Qatar

OS

SENA

FIS

(1)

(2)

(3)

Imputed
SENA
(4)

(2.33)
510.4
(2.1)
507.7
(1.89)
506.8
(3.01)
506.6
(1.25)
504.9
(1.92)
504.6
(1.85)
502.4
(1.93)
501.6
(1.58)
499.2
(3.19)
493.9
(2.41)
491.9
(2.46)
491.7
(0.97)
491.4
(2.3)
489.7
(2.38)
485.9
(3.09)
484
(1.57)
476.9
(2.25)
476.6
(3.07)
468.2
(3.3)
459.4
(3.83)
457.2
(2.12)
456.3
(2.17)
446.9
(3.72)
443
(1.98)
437.7

(2.31)
517.2
(2.07)
512.5
(1.94)
512.6
(3.01)
513.6
(1.28)
512.9
(1.92)
512
(1.94)
516.5
(2.07)
506
(1.55)
510.5
(3.3)
500.1
(2.45)
495
(2.45)
499.4
(0.94)
499
(2.33)
493.9
(2.39)
500.3
(3.02)
492.1
(1.67)
484.6
(2.26)
487.2
(3.14)
472.7
(3.42)
468.7
(3.95)
467
(2.16)
459.1
(2.2)
448.5
(3.73)
456.6
(2.03)
442.4

(2.66)
523.2
(2.6)
518.8
(2.45)
515.6
(3.21)
518.5
(2.09)
519
(2.63)
517.1
(2.52)
520.1
(2.92)
508.9
(1.96)
517.3
(3.9)
506.3
(2.98)
499.6
(2.71)
505.3
(2.09)
503
(2.85)
500.5
(2.81)
504.6
(3.61)
496.3
(2.34)
488.7
(2.71)
492.4
(3.8)
478.2
(3.7)
475.1
(4.39)
471.8
(3.07)
462.5
(2.39)
453.4
(3.92)
461.7
(3.29)
447.8

(2.84)
524
(2.62)
519.1
(2.61)
516.1
(3.08)
518.8
(1.98)
520.2
(2.66)
517.4
(2.47)
521.4
(2.51)
509.9
(1.98)
518.3
(3.77)
506.9
(3.05)
500
(2.82)
505.9
(2.05)
505.3
(2.9)
500.6
(3.1)
506
(3.34)
498.1
(2.29)
490.1
(2.81)
493.8
(3.52)
478.6
(3.81)
476.4
(4.38)
472.9
(2.83)
463
(2.5)
453.5
(4.06)
464.2
(3.03)
448.5

(continued on next page)

35

Difference

Difference

Difference

(1)-(3)

(2)-(3)

(3)-(4)

3.25

1.8

-0.19

2.56

2.02

-0.06

1.38

0.7

-0.1

2.9

1.99

-0.11

2.11

1.88

-0.31

5.91

1.62

-0.06

4.76

1.02

-0.33

3.9

1.16

-0.36

5.72

1.32

-0.19

1.38

1.61

-0.14

3.94

1.26

-0.1

3.84

2.61

-0.2

4.22

1.08

-0.56

2.65

1.79

-0.03

5.06

0.92

-0.27

4.94

1.47

-0.56

3.94

1.17

-0.36

3.53

1.05

-0.28

3.36

1.09

-0.07

3.22

1.08

-0.21

4.01

1.26

-0.27

3.58

1.06

-0.13

2.88

0.9

-0.02

2.33

1.32

-0.56

1.92

2.67

-0.26

t-statistics
Country

Thailand
Costa Rica
Colombia
Montenegro
Mexico
Peru
Brazil
Tunisia
Dominican Republic

OS

SENA

FIS

(1)

(2)

(3)

Imputed
SENA
(4)

(0.73)
433.5
(2.5)
429
(1.98)
427.6
(1.94)
424.4
(0.96)
422.3
(2.02)
404.6
(2.03)
400
(1.76)
395.6
(1.96)
365.4
(1.69)

(0.72)
441.3
(2.7)
440.2
(1.95)
437.4
(1.99)
436
(1.02)
435.4
(1.91)
420.7
(2.04)
428.6
(2.04)
410.4
(1.79)
378.8
(1.75)

(1.91)
442.5
(2.97)
442.6
(2.9)
438.9
(2.75)
442.5
(3.02)
436.7
(3.12)
422.7
(3.78)
429.1
(5.09)
413.6
(3.45)
385.2
(3.93)

(1.98)
444.1
(2.84)
444.3
(2.35)
441
(2.36)
446.9
(3.07)
438.5
(2.22)
424.8
(2.61)
434.8
(2.87)
417
(2.99)
386.1
(3.01)

Difference

Difference

Difference

(1)-(3)

(2)-(3)

(3)-(4)

4.55

0.31

-0.38

1.21

0.67

-0.45

2.56

0.45

-0.58

2.02

2.03

-1.04

4.87

0.37

-0.46

1.33

0.47

-0.46

4.89

0.09

-0.96

0.96

0.83

-0.75

1.1

1.49

-0.17

Note: Standard errors are in parentheses.
OS: Original score calculated by assigning zero to all skipped items.
SENA: Original score calculated by treating skipped items at the end are not administered.
FIS: Imputed score when skipped items are assigned score of zero.
Imputed SENA: Imputed score when skipped items at the end are ignored.

36

Table 4: Country Ranks After Different Imputations
Country

Singapore
Chinese Taipei
Estonia
Japan
Finland
Hong Kong
USA (Massachusetts)
Canada
Macao
Slovenia
B-S-J-G (China)
Netherlands
Korea
United Kingdom
Germany
Australia
New Zealand
Ireland
Poland
Denmark
Switzerland
USA (North Carolina)
Belgium
Austria

(1)

All Countries
Serious
(2)

Only One
Country Serious
(3)

All Other
Countries Serious
(4)

1
(1,1)
2
(2,6)
3
(2,6)
4
(2,6)
5
(2,7)
6
(2,8)
7
(2,11)
8
(7,8)
9
(9,9)
10
(10,11)
11
(9,15)
12
(10,15)
13
(10,15)
14
(12,15)
15
(12,16)
16
(16,20)
17
(16,22)
18
(16,22)
19
(16,24)
20
(16,24)
21
(16,25)
22
(15,3)
23
(19,25)
24

1
(1,1)
4
(2,7)
3
(2,6)
2
(2,5)
5
(3,7)
6
(3,8)
7
(2,12)
8
(7,9)
9
(9,9)
11
(10,12)
10
(9,16)
16
(12,18)
15
(10,18)
13
(11,17)
12
(10,16)
17
(16,19)
14
(11,17)
22
(17,25)
19
(17,25)
20
(17,25)
18
(16,25)
25
(16,32)
21
(17,25)
24

1
(1,1)
2
(2,2)
2
(2,2)
2
(2,2)
2
(2,2)
2
(2,5)
2
(2,8)
4
(2,7)
6
(3,7)
9
(7,10)
9
(5,11)
10
(9,12)
10
(9,13)
10
(9,12)
10
(7,12)
12
(10,14)
10
(9,12)
14
(11,16)
14
(10,16)
14
(10,16)
12
(10,16)
16
(10,23)
14
(10,26)
16

1
(1,1)
7
(4,9)
7
(5,9)
8
(4,9)
8
(6,9)
9
(6,9)
9
(5,17)
9
(9,12)
11
(9,15)
16
(12,19)
16
(10,25)
18
(16,25)
18
(13,26)
22
(16,28)
23
(16,31)
29
(23,32)
29
(22,32)
31
(23,32)
31
(23,32)
32
(25,32)
32
(25,33)
32
(23,36)
32
(29,33)
32

Original Rank

(continued on next page)

37

Country

Norway
Czech Republic
United States
Spain (Regions)
France
Spain
Portugal
Latvia
Sweden
Italy
Lithuania
Luxembourg
Hungary
Croatia
Russian Federation
Iceland
Slovak Republic
Israel
Greece
Bulgaria
Chile
United Arab Emirates
Turkey
Uruguay
Qatar

Original Rank
(1)

All Countries
Serious
(2)

Only One
Country Serious
(3)

All Other
Countries Serious
(4)

(19,26)
25
(22,28)
26
(25,30)
27
(23,32)
28
(26,30)
29
(26,32)
30
(26,32)
31
(29,33)
32
(30,33)
33
(29,34)
34
(34,38)
35
(34,38)
36
(35,37)
37
(34,38)
38
(34,39)
39
(35,40)
40
(39,50)
41
(41,42)
42
(41,42)
43
(43,43)
44
(44,46)
45
(44,46)
46
(44,46)
47
(47,48)
48
(47,48)
49
(49,49)

(19,30)
23
(18,26)
27
(24,32)
32
(26,32)
29
(26,31)
28
(24,32)
31
(26,32)
26
(23,31)
33
(33,34)
30
(23,32)
34
(33,37)
39
(37,40)
35
(34,36)
37
(34,39)
38
(35,40)
36
(33,39)
40
(39,40)
42
(41,42)
41
(40,42)
43
(43,45)
44
(43,45)
45
(44,45)
46
(46,47)
48
(48,49)
47
(46,47)
49
(49,49)

(12,19)
15
(12,17)
16
(15,22)
20
(16,26)
16
(16,22)
16
(15,23)
18
(16,23)
16
(14,22)
26
(23,29)
18
(14,26)
29
(23,33)
33
(29,34)
29
(26,33)
31
(26,34)
33
(29,34)
29
(25,34)
34
(33,36)
39
(34,41)
35
(33,40)
41
(40,43)
43
(41,44)
43
(41,44)
44
(44,45)
47
(44,47)
44
(44,47)
47
(47,48)

(29,34)
32
(31,35)
33
(32,37)
33
(32,38)
33
(32,37)
36
(32,39)
36
(32,29)
37
(33,39)
37
(34,39)
39
(33,41)
40
(38,42)
41
(39,42)
41
(40,42)
41
(39,42)
41
(40,42)
42
(40,42)
42
(41,42)
43
(42,45)
43
(42,45)
45
(43,47)
47
(45,48)
47
(45,48)
47
(46,48)
49
(48,53)
49
(48,53)
53
(52,54)

(continued on next page)

38

Country

Thailand
Costa Rica
Colombia
Montenegro
Mexico
Peru
Brazil
Tunisia
Dominican Republic

Original Rank
(1)

All Countries
Serious
(2)

Only One
Country Serious
(3)

All Other
Countries Serious
(4)

50
(49,51)
51
(51,52)
52
(51,53)
53
(53,53)
54
(53,54)
55
(55,55)
56
(56,56)
57
(57,57)
58
(58,58)

51
(49,53)
50
(50,53)
53
(50,54)
52
(50,52)
54
(53,54)
56
(56,56)
55
(55,55)
57
(57,57)
58
(58,58)

49
(47,50)
49
(47,50)
49
(48,50)
49
(47,50)
50
(49,51)
54
(51,55)
51
(49,55)
55
(55,55)
58
(58,58)

54
(52,55)
55
(54,55)
55
(54,56)
55
(54,56)
56
(55,56)
57
(57,57)
57
(57,57)
57
(57,57)
58
(58,58)

Note: Column 1 shows the rank based on the original scores, i.e., column 1 in Table 3. Column 2 shows the rank based on the
imputed scores, i.e., column 3 in Table 3. This corresponds to every country becoming serious. Column 3 shows the rank if
only this country is serious. Column 4 shown the rank if all other countries become serious and this country does not. Below
each rank is the corresponding rank interval at the 95% confidence levels.

39

Table 5: Contribution of Factors to yns and ys
Dependent Variable: De-meaned Y

De-meaned A
Coefficients for

De-meaned E
De-meaned P

Non-Serious
Students
0.06
(0.05)
0.26
(0.02)
0.68
(0.04)

Partial Serious
Students
0.04
(0.08)
0.32
(0.06)
0.64
(0.04)

Note: Column 1 shows the coefficients of ln(yns ) by regressing ln(ans ), ln(ens ), ln(pns ) on ln(yns ) respectively. Column 2 shows the coefficients of ln(yps ) by regressing ln(aps ), ln(eps ), ln(pps ) on ln(yps )
respectively. Robust standard errors are in parentheses.

40

Figure 6: yns Versus its Components for Non-Serious Students
1.5

ln(ans )

1

0.5

PRT
RUS

USA
-1.5

-1

0
0

-0.5

ln(yns )
0.5

NLD
TUR

1

BRA 1.5

DOM

-0.5

-1

-1.5

1.5

ln(ens )

1

0.5

DOM

-1.5

-1

0
0USA

NLD
-0.5
TUR

RUS
PRT

0.5

BRA
ln(yns )

1

1.5

-0.5

-1

-1.5

ln(pns )

1.5

BRA

1

DOM

0.5

RUS
PRT
-1.5

-1

-0.5

TUR
NLD

USA
0
0

0.5

ln(yns )
1

1.5

-0.5

-1

-1.5

Note:The top panel plots the scatter plot and regression lines between ln(ans ) and ln(yns ), showing the
contribution of non-serious students’ ability to the increased fraction correct. The middle panel plots the
relationship between ln(ens ) and ln(yns ) for every country, showing the contribution of extent of nonseriousness. The bottom panel plots the relationship between ln(pns ) and ln(yns ) for every country, showing
the contribution of proportion of no-serious students.

41

Table 6: Decomposed Factors for Non-Serious Students
Country
Brazil
Dominican Republic
Russian Federation
Uruguay
Montenegro
Sweden
Tunisia
Peru
Portugal
Israel
Bulgaria
New Zealand
France
Mexico
Costa Rica
Luxembourg
Chile
Norway
Belgium
Spain
Iceland
Switzerland
Germany
Slovak Repubic
Italy
Spain (Region)
Australia
Hungary
Denmark
Colombia
Czech Republic
Croatia
Japan
Qatar
Poland
Austria
Greece
Macao
United States
United Kingdom
Thailand
USA (Massachusetts)
Estonia
Lithuania
Canada
Finland
USA (North Carolina)
Ireland
Slovenia
B-S-J-G (Chi
Hong Kong
Latvia
Turkey
United Arab
Singapore
Korea
Chinese Taipei
Netherlands

IF C(%)

Yns (%)

Ans

Ens

Pns

6.72%
4.32%
4.09%
4.07%
4.01%
3.98%
3.98%
3.93%
3.78%
3.40%
3.38%
3.34%
3.08%
3.05%
2.98%
2.98%
2.97%
2.82%
2.79%
2.71%
2.71%
2.63%
2.58%
2.56%
2.54%
2.52%
2.46%
2.44%
2.40%
2.38%
2.38%
2.36%
2.32%
2.20%
2.17%
2.16%
2.12%
1.98%
1.98%
1.93%
1.92%
1.86%
1.85%
1.82%
1.79%
1.79%
1.70%
1.67%
1.66%
1.64%
1.63%
1.61%
1.44%
1.38%
1.32%
1.31%
1.22%
0.96%

6.25%
3.73%
3.16%
2.98%
2.89%
2.93%
2.77%
3.00%
2.82%
2.69%
2.20%
2.61%
2.02%
2.59%
2.48%
2.02%
2.30%
1.97%
2.07%
1.98%
2.00%
1.85%
1.69%
1.73%
1.60%
1.80%
1.85%
1.50%
1.71%
1.92%
1.37%
1.29%
1.38%
1.73%
1.24%
1.22%
1.25%
1.51%
1.62%
1.18%
1.60%
1.50%
1.22%
0.97%
1.28%
1.16%
1.41%
1.10%
0.99%
1.13%
1.03%
1.05%
0.75%
0.99%
0.93%
0.74%
0.70%
0.71%

0.25
0.18
0.38
0.29
0.24
0.36
0.21
0.22
0.43
0.29
0.28
0.37
0.29
0.25
0.27
0.31
0.27
0.34
0.33
0.31
0.33
0.31
0.33
0.31
0.28
0.31
0.33
0.29
0.33
0.24
0.30
0.29
0.34
0.21
0.29
0.28
0.24
0.34
0.32
0.28
0.28
0.34
0.32
0.25
0.32
0.33
0.33
0.28
0.26
0.27
0.30
0.28
0.20
0.21
0.28
0.24
0.24
0.20

0.37
0.35
0.28
0.29
0.31
0.27
0.35
0.32
0.24
0.29
0.27
0.27
0.27
0.29
0.27
0.25
0.27
0.25
0.25
0.25
0.26
0.24
0.24
0.25
0.25
0.24
0.25
0.25
0.25
0.26
0.23
0.23
0.23
0.27
0.22
0.24
0.24
0.20
0.23
0.24
0.23
0.23
0.22
0.24
0.22
0.22
0.21
0.21
0.22
0.21
0.20
0.21
0.21
0.23
0.20
0.23
0.20
0.24

0.67
0.59
0.29
0.36
0.39
0.30
0.37
0.43
0.27
0.32
0.29
0.27
0.25
0.36
0.34
0.26
0.32
0.23
0.25
0.25
0.24
0.24
0.21
0.22
0.22
0.24
0.22
0.21
0.21
0.31
0.20
0.20
0.18
0.31
0.20
0.18
0.21
0.22
0.23
0.17
0.25
0.19
0.18
0.17
0.18
0.16
0.20
0.19
0.17
0.20
0.17
0.17
0.18
0.20
0.17
0.13
0.14
0.15

42

A

Appendix
This appendix delves into more detail on a number of peripheral facts and issues. In the first

part we present some non causal regressions on who are non serious and which questions tend to be
taken non seriously.
We use the data on questions in the Science clusters only in the body of the paper. Our reason
for doing so is that all students take two Science clusters, but may not be tested in Math or Reading.
One might ask whether the patterns in other parts of the exam are similar. As a check we look at
the fraction of non serious items across subjects in the third part of the Appendix. Their similarity
reassures us that our focus on the Science clusters is warranted.
In the fourth part of the Appendix, we discuss in more detail the behavior patterns of serious
and non-serious students in terms of time spent and accuracy of response as a function of question
position.
In the fifth part, we discuss the exact variables we use in the imputation procedure. In the sixth
part, we explain some details behind the decomposition for partially serious students and present
the results for them.

A.1

What Drives Being Non-serious?

We have seen in Section 3.1 that serious and non-serious students behave very differently. The
next question is, what factors correlate with being non-serious? We explore this in two levels. First,
we look at the correlates of individuals being non-serious. After this, we look at correlates of the
question not being taken seriously.

A.1.1

Summary Statistics

In this section we explain the definition of various factors which are potentially correlated with
non-serious behavior. Table A.1 gives the descriptive statistics for these factors. Scores in the
component parts of the exam (reading, math and science) are scaled so that 500 is the mean and
the standard deviation is 100 for all OECD countries together. Clearly, OECD countries do better
than average as the mean math and science scores overall are 464 and 474 respectively. Students are
on average in the 10th grade and half the students are female. The variable “anxiety” is an index
we constructed by taking questions that asked about this subject (where the ranking was from a
“1” to a “4” in terms of strength of the viewpoint where 1 strongly disagree and 4 is strongly agree)
and taking a simple average of the response. The median is 2.8 suggesting a fair degree of anxiety
on the part of students. Similarly for “ambition” where the median response is 3.2.39 The variable
skipping class/arriving late uses the response for the three questions in ST062 about skipping, its
intensity and arriving late and adds them up. A 1 is never in the last two weeks, a 2 is 1 or 2 times
39

We used the 5 questions in ST118 for the anxiety variable and the 5 questions in ST119 for the ambition
variable.

43

and a 3 is 3 or 4 times, and a 4 is 5 or more times. On average, such behavior exists but is not
endemic.
The median time spent learning out of school is 16 hours per week, while time spent learning in
school is 27 hours per week. Students spend more than 40 hours a week on school related work. The
standard deviations are roughly 15 and 11 suggesting that a fair number of students are spending
well over 60 to 70 hours a week on such work. Standardized test frequency and teacher developed
test frequency is the response to question SC034. A response of 1 means there were no such tests
and a response of 5 means the tests were given more than monthly. The median value is 2 or
the frequency was 1-2 times a year. The variable “Stakes of standardized (teacher developed) tests
comes from the answers to SC035. The question is composed of 11 yes/no sub-questions (where
a yes is a 1 and a 0 is a no) regarding the purpose of these tests. We label each purpose as low,
medium or high stakes for the students giving them a weight of 1, 2 and 3 respectively. Of the 11
sub-questions, 5 are low, 3 are medium and 3 are high stakes. We then add these weighted responses
up to get our index. As the maximum value the index could have taken is 20, the median of 10 and
13 suggest the stakes are high, especially of teacher developed tests.

Table A.1: Summary Statistics

mean

sd

median

min

max

464.46

97.90

463.19

108.15

826.34

ESCS

-0.42

1.15

-0.36

-7.26

4.18

Grade

9.77

0.78

10

7

13

Female

0.50

0.50

0

0

1

Anxiety

2.71

0.67

2.8

1

4

Ambition

3.13

0.60

3.2

1

4

Skipping class/Arriving late

4.32

1.68

4

3

12

Out-of-school learning(hours per week)

19.58

14.69

16

0

70

Time on classes (hours per week)

28.25

11.11

27

0

70

Standardized test frequency

2.07

0.85

2

1

5

Teacher-developed tests frequency

3.96

1.05

4

1

5

Stakes of standardized tests

9.11

7.01

10

0

20

Stakes of teacher-developed tests

12.12

5.78

13

0

20

School average science score

473.62

71.86

478.58

214.86

717.17

Math score

44

A.1.2

Who is Non-serious?

The factors that correlate with a student being non-serious are explored in Table A.2. Column
1 shows the results for all countries. The dependent variable is 1 if the student is non-serious. In
columns 1 to 3, being non-serious is defined as meeting at least one of criterion 1, 2 or 4. In column
4, being non-serious is defined as meeting criterion 3. We make this distinction because the patterns
explored in the previous section differ across these two groups. We also look at high-stake countries,
ones where the standardized tests given in school are high-stakes40 , as well as low-stake countries
as the patterns in the two might be different. If, for example, students are fed up with exams
in high-stakes countries while not in low-stakes countries, we might expect a higher probability of
being non-serious in PISA in high-stake countries. One might want to do these regressions country
by country, but with 58 countries, this would be overkill as this is not the main object of this paper.
Also note that we are not claiming any causal effects, merely pointing out some correlations in the
data.
To begin with, we ask whether better students are more or less likely to be non-serious. Columns
1-3 suggest that higher math scores (a proxy for ability) are associated with a student being less
likely to be non-serious, except when we use criterion 3, suggesting that students with missing items
are a different breed.41 Students with high socioeconomic status (ESCS) and in lower grades are
more likely to be non-serious. Again the sign in column 4 is reversed. This suggests that poor able
students use criterion 3 when they are non serious while the rest use criterion 1, 2 or 4.
Students from richer countries are more likely to be non-serious, though the shape is that of
an inverted U with a turning point at about $33,000 for per capita GDP. However, this pattern is
again reversed in column 4 where the pattern is U shaped with a turning point at about $38,500.
Gender matters: women are less likely to be non-serious in columns 1-3, but are more likely to
be non-serious (by quitting in the middle of the exam) in column 4 suggesting that women “blow
off” the exam in different ways than men. As might be expected, being anxious or ambitious is
associated with being less likely to be non-serious, while being undisciplined, i.e., having a pattern
of skipping class or arriving late, is associated with being non-serious.
One might speculate that students who are over-worked and over-tested, especially with highstakes exams, have test fatigue and passively resist taking yet another test, and therefore are more
likely to not take PISA seriously. There is some evidence in favor of this. First, countries with highstakes exams do seem to make students work harder. The data reveals that on average students
spend 1.3 hours more per week in class and 3.1 hour more on out-of-school learning in all subjects
40

We calculate the stakes of standardized tests given in school as follows. In school questionnaire, school
principles were asked whether the school used standardized tests for 11 different purposes. We mark the
stake of each purpose to be between 1 to 3 and sum up the stakes for each school. Then we sort countries
by their mean stakes and mark the top 36 countries as high-stake countries while the remaining 36 countries
are marked as low-stake ones.
41
Our results are robust to using fraction correct on items that are answered seriously as a measure of
ability.

45

in high-stakes countries relative to low-stakes ones. Working harder seems to be associated with
not taking PISA seriously. In column 1, spending more time on studies out of school is significant
for all countries together, but the effect seems to be coming from high stakes countries. Time
spent in school is positive but not significant for all countries together, but is significant for low
stake countries.42 Having more tests (standardized or teacher-developed) does seem to correlate
positively with being non-serious overall, though the coefficients are not significant. This might be
because the effects differ in high stake and low stake countries. Having more standardized tests
raises the likelihood of being non-serious in high-stakes countries (column 2) but does the opposite
in low-stakes ones (column 3).
When teacher-developed tests are being given, raising the stakes seems to make students more
likely to be serious, not less, suggesting that such testing may be less likely to result in test fatigue.
Students from better schools, as reflected in the log of the school science score, are also less likely
to be non-serious in low stakes countries, but more likely to be non-serious in high stakes countries.
This makes sense if better schools push students more in high stakes countries resulting in fatigue.
In Table A.3 and Table A.4, we present correlates of each non-seriousness criterion for cutoff level
of 10% (as defined in Section 3 ) and 6%, respectively. The results are consistent across different
cutoff levels.
Our results here should be seen as preliminary as there is no causation implied, merely correlation. The patterns described above are suggestive and might be worth exploring in future work.

A.1.3

Which Questions are Not Taken Seriously?

We define a non serious question as those that were not reached, for which there was no response,
were missing, or on which too little time was spent. Non-reached and no-response items were looked
at by the student who then chose not to answer the item despite having time left. Had he taken
the exam seriously, he would have answered to the best of his ability. The student did not even
look at missing items. But he had time left. In general, students have ample time to do the exam.
Not even bothering to even read the question is again an indication of non seriousness. One might
argue that no-response items, i.e., those that were skipped in the middle of the exam, should be
treated differently as this was a computer based exam and students could not go back. Assuming
they knew this, their choosing to skip again indicates the question is not taken seriously. Questions
on which too little time was spent (as explained in criterion 4 for defining non serious students)
are those where the response time is below a threshold which is country specific and for which the
proportion correct is lower than that for normal time items for the same person. This is to prevent
us from mistakenly labeling a question as non serious when in fact the student knew the answer
immediately and so spent little time on it.
We explore the effects of question characteristics on the probability of a question being skipped,
i.e., being not-reached or no-response. We also do the same for the probability of too little time
42

See column 1 and the row for time on classes and out-of-school science learning.

46

being spent on a question. In both cases we run a linear probability model with individual fixed
effects as well as question characteristics. Figure A.1 shows the predicted probability of skipping
a question and the predicted probability of spending too little time on a question for each cluster
as a function of the difficulty of the question.43 In all clusters, as the difficulty of the question
increases, the probability of skipping increases though there is a slight decrease as questions become
very difficult(top panel). In the bottom panel, we see that the probability of spending too little
time is roughly flat: first increasing, then decreasing and finally increasing again. Students seem to
try to answer if the question is easy but as it gets difficult, they seem to give up. There are also
differences between clusters. Consistent with the “fatigue” hypothesis, questions are more likely to
be taken non seriously in the second and fourth clusters.
In Figure A.2, we explore whether question type affects the probability of skipping or spending
too little time as a function of question order. For all questions, the probability of skipping rises
with order, or sequence, in a cluster and jumps down at the beginning of the new cluster and more so
after the break, which is consistent with “fatigue”. The graph of complex multiple choice questions
for the probability of skipping lies between the open response and simple multiple choice questions.
This makes sense as it is easy to guess an answer for simple multiple choice questions so that they
are less likely to be skipped.
Non-serious behavior in terms of spending too little time weakly falls with the order within a
cluster for all question types. However, there is a large jump up at the beginning of the second and
fourth clusters. The above pattern suggests that for open response questions at least, as the exam
proceeds, students substitute towards skipping with a reset at the end of each cluster. Hence we
see a fall with sequence within a cluster and a jump up in each new cluster. While skipping is more
likely for open response questions, spending too little time is less likely for such questions relative
to other question types.
In order to understand the effects of individual characteristics on the probability of being skipped
or spending too little time, we run individual characteristics on estimated individual fixed effects
from our linear probability model, see Table A.5. The results are in line with those of Table A.2.
So far we ran choice regressions as if they were independent. However, the appropriate model
is a multinomial choice one as the student has three mutually exclusive and exhaustive options for
each question: skip, answer with too little time or answer with normal time. We used the linear
probability model as it allowed us to incorporate individual fixed effects, which we could not do
with logit. With logit, we can control for individual characteristics, but as we are unlikely to have
information on all possible characteristics, we might have omitted variable bias.
Table A.6 presents the results of a logit regression where the baseline choice is spending normal
time answering the item. In the regression, we control for the question characteristics and the
individual characteristics used in the previous tables. The first and second columns show the factors
43

For each cluster, the predicted probability at each level of question difficulty in the figure takes the
mean value of the predicted probability at that level of difficulty.

47

affecting the probability of skipping and the probability of spending too little time, respectively.
The position within a cluster is positively correlated with the probability of skipping and negatively
correlated with the probability of spending too little time, consistent with students switching from
spending too little time to skipping as the exam progresses. If a question is in the second, third or
fourth cluster relative to being in the first cluster, it is more likely to be skipped and this likelihood
is much higher in the second and fourth clusters as they are the last clusters in each science session.
Open response and complex multiple choice questions move students towards skipping and away
from spending too little time. However, as the difficulty of the questions increase, the students
become more likely to skip and spend too little time. The coefficients on individual characteristics
are roughly in line with those in Table A.2. The math score of the student is negatively correlated
with the probability of skipping and the probability of spending too little time. Female students
are less likely to skip or spend too little time. Ambitious students are less likely to skip. Consistent
with our previous findings, students from richer countries are more likely to skip and spent too
little time, though the shape is that of an inverted U with a turning point at about $43,000 for per
capita GDP. We control for standardized test frequencies and teacher developed test frequencies to
investigate whether there is any evidence that students are fed up with testing, and as a result do
not take them seriously. We find that as the frequency of the standardized tests increases, students
likelihood of skipping and spending too little time significantly increases which is consistent with
the “fatigue” effect. However, the teacher-developed tests do the exact opposite. This suggests that
students view them very differently.

A.2

Fraction of Non-serious items Across Subjects

Table A.7 shows the fraction of no-response items and the fraction of non-reached items for
the subject of science, reading and math. The fraction of no response items for the reading and
math tests are a bit higher on average than science. Moreover, the fraction of no-response and
non-reached items are highly correlated across subjects. For example, the correlation between the
fraction of no-response items for science and for reading is 0.98, showing that non seriousness is
common across subjects of the test as might be expected.

A.3

Time Spent, Accuracy and Position

Table A.8 shows time per science cluster across positions for serious and non-serious students.
Note that time spent on the cluster falls with the position of the cluster and then jumps back
up after the break at the end of cluster 2 and this is more so for non-serious students. There is
substantial heterogeneity between non-serious students according to the criterion used. Students
with no-response or too-little-time items, not surprisingly, spend less time per cluster than serious
students regardless of cluster position. However, the opposite holds for those with non-reached or
missing items but only for the first and third clusters. For the second and fourth clusters their time

48

spent is 30-40% less than that of serious students. It is also worth noting that for these students,
time is still not a constraint: on average they have more than 15 minutes left. This suggests that
“fatigue” sets in faster for non-serious students.
The upper part of Table A.9 shows proportion correct for all items (not just answered ones)
across positions. Serious students have higher proportion correct than each category of non-serious
students. Accuracy falls in the second cluster compared to the first one, and this is more so for nonserious students, reminiscent of the patterns for time spent. However, non-serious students will have
a lower proportion correct on all items by definition as they skip many items. If we want to know
what their accuracy is we should divide by the number of answered questions as done in the lower
part of Table A.9. The numbers show that even with this correction non-serious students have lower
accuracy than serious ones. In addition, the degree to which accuracy falls across clusters is now
similar (around 2%) for both serious and non-serious students. This is consistent with non-serious
students’ performance experiencing a substantial drop in the second cluster primarily because they
skip more items there.

A.4

Variables Used in Imputation

PISA data has a rich array of information from the student and school questionnaires in the
survey. In the imputation we use variables constructed from these surveys by PISA. We choose the
variables that seem relevant. A list of the variables used is contained in Table A.10. Binary variables
are clearly identified. All others are continuous indices. Details of these are available in the PISA
technical report, (OECD (2015b)), Chapter 16. The imputation also uses the individual’s scores for
all other items and other students’ scores for all items as in the standard MICE imputations. We
also include country fixed effect in the imputations.

A.5

Decomposition for Partially Serious Students

We call fully serious students those who neither skip items nor spend too little time on any
item. These fully serious students, together with what we call partially-serious students, make up
what we have termed serious students. For fully serious students, the number correct will be the
same before and after imputation by definition. The increase in fraction correct for serious students
(Ys ) therefore only comes from imputations for partially serious students who did skip a few items
or spent too little time on a small enough number of items so that they were not classified as nonserious. There are P S partially serious students. Next we will decompose Ys into its component

49

parts.
P

Ii

i∈S

Ys =

P

Ti

i∈S∪N S

P
=

i∈P S

P

P

(Ii )

P

N Ii

i∈P S

P

N Ii

i∈P S

Ti

i∈P S

P

Ti

i∈P S

Ti

i∈S∪N S

= Aps Eps Pps
Aps is the increase in the fraction correct for non-serious items among partially serious students. Eps
is the fraction of non-serious items among all items for partially serious students, which measures the
degree of non-seriousness. Pps approximately measures the proportion of partially serious students
in a country as partially serious students on average have the same number of total items as other
students. The values of Yps , Aps , Eps and Pps for each country are provided in Table A.11.
Similar to the decomposition for non-serious students, we divide both sides by the geometric
mean and get
yps

Yps
=
=
Ȳps



Aps
Āps



Eps
Ēps



Pps
P̄ps


= aps eps pps

(7)

Take the logarithm on both sides of (7) gives:
ln(yps ) = ln aps + ln eps + ln pps

(8)

Next we run the regression of ln aps , ln eps , ln pps separately on ln yps , that is,
ln aps = α2 ln yps + a
ln eps = β2 ln yps + d
ln pps = γ2 ln yps + p.
Let the OLS estimates be denoted by α̂2 , β̂2 , γ̂2 . Similarly we can show that αˆ2 + βˆ2 + γˆ2 = 1 and
the coefficients αˆ2 , βˆ2 , γˆ2 measure the contribution of partially serious students’ ability, extent of
non-seriousness and proportion to a country’s increase in fraction correct. Figure A.3 plots the
scatter plot and regression lines above for partially serious students.

50

Table A.2: Factors Related to Being Non-Serious
Being non-serious (Criterion 1,2,4)
Criterion 3
High stake Low stake
All countries
All countries
countries
countries
Log (math score)
-0.3294***
-0.3383*** -0.3472***
0.0565***
(0.0122)
(0.0163)
(0.0157)
(0.0092)
ESCS
0.0074***
0.0036
0.0195***
-0.0062***
(0.0019)
(0.0027)
(0.0021)
(0.0016)
ESCS^2
0.0004
-0.0000
0.0027**
0.0041***
(0.0009)
(0.0012)
(0.0012)
(0.0008)
Grade
-0.0087***
-0.0078***
0.0026
0.0103***
(0.0021)
(0.0028)
(0.0032)
(0.0019)
Female
-0.0149***
-0.0198***
-0.0074**
0.0210***
(0.0029)
(0.0039)
(0.0037)
(0.0026)
Anxiety
-0.0052**
-0.0037
-0.0090***
0.0111***
(0.0023)
(0.0031)
(0.0029)
(0.0020)
Ambition
-0.0054**
-0.0042
-0.0008
-0.0090***
(0.0025)
(0.0035)
(0.0033)
(0.0022)
Skipping class/Arriving late
0.0032***
0.0031**
0.0042***
-0.0002
(0.0009)
(0.0013)
(0.0012)
(0.0008)
Log per capita GDP
1.4846***
0.9744***
1.8385***
-4.5828***
(0.1159)
(0.1387)
(0.1777)
(0.1051)
(Log per capita GDP)^2
-0.0714***
-0.0473*** -0.0856***
0.2167***
(0.0057)
(0.0068)
(0.0087)
(0.0051)
Out-of-school learning (hrs/week)
0.0005***
0.0005***
0.0001
-0.0005***
(0.0001)
(0.0001)
(0.0001)
(0.0001)
Time on classes
0.0002
-0.0001
0.0005***
-0.0014***
(0.0002)
(0.0002)
(0.0002)
(0.0001)
Log ( school average science score)
-0.0216
0.0768***
-0.2592*** -0.0399***
(0.0167)
(0.0227)
(0.0209)
(0.0137)
Standardized test frequency
0.0022
0.0044*
-0.0080***
0.0016
(0.0018)
(0.0024)
(0.0027)
(0.0017)
Teacher-developed tests frequency
0.0008
-0.0022
0.0034*
0.0075***
(0.0013)
(0.0018)
(0.0018)
(0.0012)
Stakes of Standardized tests
0.0001
0.0000
0.0005
-0.0002
(0.0002)
(0.0004)
(0.0003)
(0.0002)
Stakes of teacher-developed tests
-0.0012***
-0.0017***
0.0000
0.0008***
(0.0003)
(0.0004)
(0.0005)
(0.0003)
Observations
283,674
128,668
155,006
283,674
R-squared
0.033
0.031
0.046
0.084
*** p < 0.01, ** p < 0.05, * p < 0.1. Robust standard errors in parentheses

Note: In column 1-3 being non-serious does not include students meeting criteria 3. The latter group is
analyzed separately in column 4. The number of observations is less than the number of students because
students with missing variables are dropped.

51

Table A.3: Factors Related to Being Non-Serious for Each Criterion (For Cutoff level of
10%)
Variables
Log(math score)
ESCS
ESCS^2
Grade
Female
Anxiety
Ambition
Skipping Class/Arriving Late
Out-of-school learning (hrs/week)
Time on classes
Standardized test frequency
Teacher-developed tests frequency
Stakes of Standardized tests
Stakes of teacher-developed tests
Log ( school average science score)
Log per capita GDP
(Log per capita GDP)^2
Observations
R-squared

non reached

no response

missing

little time

-0.050***
(0.006)
0.003***
(0.001)
-0.000
(0.000)
-0.001
(0.001)
0.003**
(0.001)
0.002
(0.001)
-0.005***
(0.001)
0.001
(0.000)
-0.000**
(0.000)
-0.000
(0.000)
-0.001
(0.001)
0.003***
(0.001)
-0.000
(0.000)
-0.000
(0.000)
-0.050***
(0.007)
-0.032
(0.054)
0.001
(0.003)
283,674
0.0105

-0.215***
(0.008)
0.006***
(0.001)
0.001**
(0.001)
-0.003***
(0.001)
-0.005***
(0.002)
-0.004***
(0.001)
-0.005***
(0.001)
0.002***
(0.001)
0.000*
(0.000)
0.000**
(0.000)
0.000
(0.001)
0.001
(0.001)
0.000
(0.000)
-0.000**
(0.000)
-0.056***
(0.010)
0.690***
(0.071)
-0.034***
(0.003)
283,674
0.0489

0.057***
(0.009)
-0.006***
(0.002)
0.004***
(0.001)
0.010***
(0.002)
0.021***
(0.003)
0.011***
(0.002)
-0.009***
(0.002)
-0.000
(0.001)
-0.000***
(0.000)
-0.001***
(0.000)
0.002
(0.002)
0.008***
(0.001)
-0.000
(0.000)
0.001***
(0.000)
-0.040***
(0.014)
-4.583***
(0.105)
0.217***
(0.005)
283,674
0.0836

-0.148***
(0.010)
0.001
(0.002)
-0.001
(0.001)
-0.006***
(0.002)
-0.018***
(0.002)
-0.004**
(0.002)
0.003
(0.002)
0.002***
(0.001)
0.000***
(0.000)
0.000
(0.000)
0.002
(0.002)
-0.002**
(0.001)
0.000
(0.000)
-0.001***
(0.000)
0.068***
(0.014)
1.192***
(0.093)
-0.057***
(0.005)
283,674
0.0122

*** p < 0.01, ** p < 0.05, * p < 0.1. Robust standard errors in parentheses

52

Table A.4: Factors Related to Being Non-Serious for Each Criterion (For Cutoff level of 6%)
Variables
Log(math score)
ESCS
ESCS^2
Grade
Female
Anxiety
Ambition
Skipping Class/Arriving Late
Out-of-school learning (hrs/week)
Time on classes
Standardized test frequency
Teacher-developed tests frequency
Stakes of Standardized tests
Stakes of teacher-developed tests
Log ( school average science score)
Log per capita GDP
(Log per capita GDP)^2
Observations
R-squared

non reached

no response

missing

little time

-0.050***
(0.006)
0.003***
(0.001)
-0.000
(0.000)
-0.001
(0.001)
0.003**
(0.001)
0.002
(0.001)
-0.005***
(0.001)
0.001
(0.000)
-0.000**
(0.000)
-0.000
(0.000)
-0.001
(0.001)
0.003***
(0.001)
-0.000
(0.000)
-0.000
(0.000)
-0.050***
(0.007)
-0.032
(0.054)
0.001
(0.003)
283,674
0.0105

-0.111***
(0.006)
0.002***
(0.001)
0.000
(0.000)
-0.003***
(0.001)
-0.004***
(0.001)
-0.003***
(0.001)
-0.001
(0.001)
0.001*
(0.000)
0.000
(0.000)
0.000*
(0.000)
-0.000
(0.001)
0.000
(0.001)
0.000
(0.000)
-0.000
(0.000)
-0.025***
(0.007)
0.336***
(0.048)
-0.016***
(0.002)
283,674
0.0294

0.030***
(0.008)
-0.004***
(0.001)
0.005***
(0.001)
0.007***
(0.002)
0.012***
(0.002)
0.010***
(0.002)
-0.009***
(0.002)
-0.000
(0.001)
-0.000***
(0.000)
-0.001***
(0.000)
0.003**
(0.001)
0.005***
(0.001)
0.000
(0.000)
0.000
(0.000)
-0.047***
(0.011)
-3.365***
(0.088)
0.159***
(0.004)
283,674
0.0720

-0.114***
(0.009)
-0.000
(0.001)
-0.001
(0.001)
-0.004***
(0.001)
-0.015***
(0.002)
-0.003*
(0.002)
0.002
(0.002)
0.001**
(0.001)
0.000***
(0.000)
0.000
(0.000)
0.001
(0.001)
-0.001
(0.001)
0.000
(0.000)
-0.001**
(0.000)
0.042***
(0.012)
0.909***
(0.074)
-0.043***
(0.004)
283,674
0.0113

*** p < 0.01, ** p < 0.05, * p < 0.1. Robust standard errors in parentheses

53

Figure A.1: Pr(skip) and Pr(spend too little time) w.r.t. cluster and difficulty

Note: Predicted probabilities are obtained from a linear probability model with individual fixed effects as
well as question characteristics such as cluster, sequence, difficulty and the type of the question. For each
cluster, the predicted probability at each level of question difficulty takes the mean value of the predicted
probability at that level of difficulty. In the figure, lowess-smoothed lines are presented.

54

Figure A.2: Pr(skip) and Pr(spend too little time) w.r.t. sequence and the type of the
question

Note: Predicted probabilities are obtained from a linear probability model with individual fixed effects as
well as question characteristics such as cluster, sequence, difficulty, and the type of the question. For each
question type, the mean value of the predicted probability at each order is presented.

55

Table A.5: Factors affecting Pr(Skip) and Pr(Spend too little time) (Individual Characteristics)
Skip
Log (math score)

Spend too
little time

-0.0790*** -0.0411***
(0.0221)
(0.0024)
Log per capita GDP
0.5659**
0.2814***
(0.2389)
(0.0176)
(Log per capita GDP)^2
-0.0279** -0.0133***
(0.0118)
(0.0009)
ESCS
0.0002
0.0003
(0.0040)
(0.0003)
ESCS^2
-0.0003
-0.0001
(0.0019)
(0.0001)
Grade
-0.0049
-0.0012***
(0.0040)
(0.0003)
Female
0.0077
-0.0043***
(0.0061)
(0.0005)
Anxiety
0.0172*** -0.0010**
(0.0048)
(0.0004)
Ambition
-0.0120**
0.0010**
(0.0053)
(0.0004)
Skipping class/Arriving late
0.0019
0.0005***
(0.0019)
(0.0002)
Additional learning time (hrs/week)
0.0000
0.0001***
(0.0002)
(0.0000)
Time on classes
0.0003
0.0000
(0.0002)
(0.0000)
Standardized test frequency
0.0033
0.0007**
(0.0039)
(0.0003)
Teacher-developed tests frequency
0.0010
-0.0006***
(0.0026)
(0.0002)
Stake of Standardized test
-0.0004
-0.0000
(0.0005)
(0.0000)
Stake of Teacher-developed tests
-0.0006
-0.0001
(0.0007)
(0.0001)
School average science score
-0.0911*** 0.0114***
(0.0330)
(0.0028)
Observations
290,271
290,234
R-Squared
0.00236
0.0207

56

Table A.6: Factors Affecting Pr(Skip) and Pr(Spend too little time) (Logit results)
Skip
0.0675***
(0.0004)
Difficulty
0.6159***
(0.0126)
Cluster 2
0.5932***
(0.0053)
Cluster 3
0.2714***
(0.0056)
Cluster 4
0.6757***
(0.0052)
Complex Multiple Choice
0.3543***
(0.0066)
Open Response
1.6828***
(0.0062)
Log(math score)
-3.1288***
(0.0121)
Log per capita GDP
6.6012***
(0.1043)
(Log per capita GDP)^2
-0.3084***
(0.0050)
ESCS
0.0122***
(0.0023)
ESCS^2
-0.0325***
(0.0012)
Grade
-0.0401***
(0.0024)
Female
-0.0090**
(0.0036)
Anxiety
0.0015
(0.0028)
Ambition
-0.1155***
(0.0030)
Skipping class/Arriving late
0.0505***
(0.0010)
Additional learning time (hrs/week) -0.0026***
(0.0001)
Time on classes
0.0019***
(0.0001)
Standardized test frequency
0.0259***
(0.0024)
Teacher-developed tests frequency
-0.0112***
(0.0018)
Stake of Standardized test
-0.0060***
(0.0003)
Stake of Teacher-developed tests
-0.0099***
(0.0004)
School average science score
-1.6290***
(0.0181)
Observations
9,058,210
Sequence

57

Spend too
little time
-0.0150***
(0.0005)
0.6643***
(0.0150)
0.0265***
(0.0068)
-0.0197***
(0.0069)
0.2926***
(0.0064)
-0.1220***
(0.0056)
-0.7622***
(0.0072)
-1.7452***
(0.0167)
8.0347***
(0.1477)
-0.3668***
(0.0070)
0.0109***
(0.0028)
-0.0026
(0.0016)
-0.0538***
(0.0033)
-0.2169***
(0.0048)
-0.0236***
(0.0036)
0.0106***
(0.0041)
0.0342***
(0.0014)
0.0040***
(0.0002)
0.0014***
(0.0002)
0.0209***
(0.0032)
-0.0154***
(0.0024)
0.0000
(0.0004)
0.0030***
(0.0005)
0.3706***
(0.0253)
9,058,210

Table A.7: Fraction of Non-serious items Across Subjects
Fraction of Non-reached items (%)
science reading
math
0.62
0.46
0.58
Singapore
Chinese Taipei
0.58
0.37
0.58
Estonia
0.92
0.39
0.83
Japan
0.97
0.76
1.19
Finland
0.75
0.52
1.25
Hong Kong
0.65
0.39
0.56
USA (Massachusetts)
0.45
0.18
0.42
Canada
1.02
0.68
1.18
Macao
0.31
0.13
0.27
Slovenia
1.11
0.47
1.38
B-S-J-G (China)
0.87
0.60
0.61
Netherlands
0.71
0.29
0.92
Korea
1.06
0.53
1.16
United Kingdom
1.39
0.84
1.39
Germany
1.38
0.84
1.34
Australia
1.37
0.85
1.49
New Zealand
1.46
0.98
1.34
Ireland
1.05
0.42
0.89
Poland
1.14
0.40
1.15
Denmark
1.57
1.11
1.55
Switzerland
1.50
1.07
1.60
USA (North Carolina)
0.43
0.21
0.53
Belgium
1.35
0.60
1.11
Austria
1.34
0.54
1.35
Norway
1.75
1.28
2.09
Czech Republic
1.25
0.44
1.31
United States
0.61
0.43
0.69
Spain (Regions)
1.21
0.65
1.43
France
2.19
1.68
2.30
Spain
1.21
0.60
1.55
Portugal
1.37
0.38
1.43
Latvia
0.82
0.26
0.88
Sweden
2.06
1.54
2.58
Italy
1.70
0.77
1.63
Lithuania
1.41
0.65
1.24
Luxembourg
1.57
0.76
1.44
Hungary
1.18
0.37
1.20
Croatia
1.28
0.39
1.64
Russian Federation
1.37
0.68
1.24
Iceland
1.67
0.91
1.67
Slovak Republic
1.31
0.56
1.15
Israel
1.96
1.06
2.07
Greece
1.73
0.98
1.59
Bulgaria
2.15
1.15
1.07
Chile
2.26
1.10
1.46
United Arab Emirates
1.68
1.22
1.23
Turkey
1.28
0.56
1.24
Uruguay
2.87
2.39
2.02
Qatar
3.73
3.57
3.02
Thailand
0.35
0.17
0.42
Costa Rica
1.27
0.81
0.94
Colombia
2.32
1.40
1.45
Montenegro
2.94
1.99
2.96
Mexico
1.09
0.52
0.72
Peru
1.07
0.58
0.71
Brazil
1.91
1.40
1.55
Tunisia
5.11
4.43
2.89
Dominican Republic
14.97
11.68
7.89
Country

Fraction
science
1.30
1.98
1.83
2.78
2.13
1.60
1.18
2.09
0.98
3.27
2.02
1.61
2.51
3.31
3.43
3.20
3.38
2.10
3.02
3.30
3.47
1.22
3.06
4.00
3.59
3.84
1.44
2.88
4.75
2.91
3.40
2.25
4.76
4.08
3.77
4.27
3.89
4.35
3.47
3.75
4.20
4.37
3.95
6.14
4.05
3.11
4.26
6.44
4.95
1.89
3.22
2.78
9.54
1.98
3.46
5.57
7.20
7.94

of No-response items (%)
reading
math
2.22
2.26
3.83
3.25
3.30
4.34
6.44
5.94
3.73
5.84
3.22
2.81
1.84
2.49
3.47
4.06
2.02
1.85
5.78
6.36
4.25
2.54
2.08
3.31
4.86
4.53
5.89
6.24
5.51
7.18
5.28
5.97
5.70
6.38
3.13
4.31
5.51
5.29
5.24
5.39
6.34
5.94
2.19
1.64
5.11
5.88
6.74
7.24
5.34
6.92
6.46
6.85
2.52
2.28
4.48
6.85
7.69
8.08
4.62
6.80
6.46
6.95
3.78
4.78
6.35
8.31
5.78
7.42
6.48
6.73
7.36
6.79
7.76
6.82
6.92
8.87
5.05
5.56
6.12
5.91
7.47
5.76
6.90
7.91
6.76
7.28
10.35
8.68
7.04
9.30
5.33
4.36
8.63
6.58
10.00
11.79
8.74
7.20
3.57
2.73
6.45
7.18
5.06
5.30
15.87
14.52
3.59
4.50
6.33
8.30
10.30
9.54
12.14
9.55
13.19
13.55

Note: In this table non-reached items include non-reached open response items and no-response items including no-response open response items.

58

Table A.8: Time Per Science Cluster (Minutes)
Position 1

Position 2

Position 3

Position 4

Serious Students

22.25

17.93

20.20

17.55

Non-Serious Students (Union of 4 criteria)

27.65

12.10

19.70

11.82

Criterion 1 only (Nonreached items)

28.58

12.13

19.34

10.93

Criterion 2 only (No-response items)

20.75

11.20

15.64

10.71

Criterion 3 only (Missing items)

33.46

10.66

31.88

12.01

Criterion 4 only (Little-time items)

18.94

13.32

14.87

11.47

Table A.9: Proportion Correct in Science Clusters
Proportion correct for all items (%)
Position 1

Position 2

Position 3

Position 4

Serious Students

49.20

47.05

49.07

46.07

Non-Serious Students (Union of 4 criteria)

39.46

24.56

34.16

24.15

Criterion 1 only (Nonreached items)

33.81

19.74

27.46

17.85

Criterion 2 only (No-response items)

23.21

18.26

22.24

18.04

Criterion 3 only (Missing items)

43.17

18.23

41.96

18.27

Criterion 4 only (Little-time items)

42.83

36.98

36.46

31.49

Proportion correct for answered items (%)
Position 1

Position 2

Position 3

Position 4

Serious Students

50.44

49.18

50.43

48.04

Non-Serious Students (Union of 4 criteria)

43.30

39.94

38.67

34.01

Criterion 1 only (Nonreached items)

40.17

37.19

36.41

31.83

Criterion 2 only (No-response items)

29.20

27.05

28.29

25.52

Criterion 3 only (Missing items)

46.59

44.94

45.52

41.87

Criterion 4 only (Little-time items)

44.91

41.53

39.22

35.05

59

Table A.10: Variables Used in Imputation
Variable
FEMALE
GRADE
ESCS
BELONG
unfairteacher
TWINS
OUTHOURS
COOPERATE
JOYSCIE
INTBRSCI
DISCLISCI
TEACHSUP
SCIEACT
ANXTEST
MOTIVAT
EMOSUPS
DURECEC
REPEAT
TIMESCIE
NONSERIOUS
CLISIZE
EDUSHORT
STAFFSHORT
PROATCE
CREACTIV
PROSTMAS
STRATIO
PUBLIC
sch_scie
COUNTRY

Description
Female=1, male=0
Grade compared to modal grade of 15-year-old students in country
Index of economic, social and cultural status
Sense of belonging to school
Teacher fairness
Total learning time (minutes per week)
Out-of-school study per week
Enjoy cooperation
Enjoyment of science
Interest in broad science topics
Disciplinary climate in science classes
Teacher support in science classes
Science activities
Test anxiety
Achieving motivation
Parents emotional support
Duration in early childhood education and care
Ever repeated a grade=1, otherwise 0
Total time spent on science clusters in PISA exam
Being non-serious in PISA exam=1, otherwise 0
Class size
Shortage of educational material
Shortage of educational stuff
Proportion of all teachers fully certified
Creative extra-curricular activities
Proportion of science teachers with ISCED level 5A
and a major in science
Student teacher ratio
Public school=1, otherwise 0
School average PISA science score
Country fixed effects

60

Figure A.3: yps Versus its Components for Partially Serious Students
1.5

ln(aps )

1

0.5

RUS
PRT

USA
-1.5

NLD
-1

0
0

-0.5

BRA

ln(yps )
0.5

1

0.5

1

0.5

1

1.5

TUR
DOM

-0.5

-1

-1.5

1.5

ln(eps )

1

DOM
0.5

BRA

-1.5

-1

0TUR
0

-0.5

USA

ln(yps )

RUS
PRT

1.5

NLD
-0.5

-1

-1.5

ln(pps )

1.5

1

0.5

TUR

-1.5

-1

-0.5

0

DOM0

BRA
NLD

USA

-0.5

-1

-1.5

61

PRT
RUS

ln(yps )
1.5

Table A.11: Decomposed Factors for Partially Serious Students
Country
Tunisia
Bulgaria
Montenegro
Uruguay
Croatia
France
Sweden
Czech Republic
Luxembourg
Portugal
Hungary
Japan
Italy
Austria
Poland
Peru
Russian Federation
Germany
Greece
Norway
Lithuania
Slovak Repubic
Switzerland
United Kingdom
New Zealand
Spain
Spain (Region)
Belgium
Israel
Iceland
Denmark
Turkey
Slovenia
Chile
Finland
Estonia
Australia
Hong Kong
Dominican Republic
Korea
Ireland
Latvia
Chinese Taipei
Canada
B-S-J-G (China)
Costa Rica
Macao
Qatar
Brazil
Colombia
Mexico
Singapore
United Arab
United States
USA (Massachusetts)
Thailand
USA (North Carolina)
Netherlands

Y ps(%)

Aps

Eps

P ps

1.21%
1.18%
1.12%
1.09%
1.07%
1.06%
1.06%
1.01%
0.96%
0.96%
0.95%
0.94%
0.94%
0.94%
0.93%
0.93%
0.93%
0.90%
0.87%
0.85%
0.84%
0.83%
0.78%
0.75%
0.73%
0.73%
0.72%
0.72%
0.71%
0.71%
0.70%
0.68%
0.67%
0.67%
0.63%
0.63%
0.62%
0.60%
0.59%
0.57%
0.57%
0.56%
0.52%
0.52%
0.51%
0.50%
0.48%
0.47%
0.47%
0.46%
0.45%
0.39%
0.38%
0.36%
0.36%
0.32%
0.28%
0.25%

0.19
0.27
0.24
0.25
0.30
0.34
0.34
0.31
0.30
0.32
0.29
0.38
0.28
0.31
0.33
0.21
0.33
0.32
0.27
0.34
0.28
0.25
0.32
0.34
0.33
0.34
0.33
0.32
0.26
0.27
0.31
0.21
0.29
0.24
0.37
0.38
0.30
0.42
0.16
0.26
0.32
0.26
0.34
0.36
0.31
0.22
0.40
0.19
0.20
0.21
0.23
0.40
0.20
0.32
0.39
0.23
0.31
0.25

0.13
0.10
0.10
0.10
0.08
0.08
0.08
0.08
0.08
0.08
0.08
0.07
0.08
0.08
0.07
0.13
0.08
0.07
0.08
0.07
0.07
0.08
0.07
0.07
0.07
0.07
0.07
0.07
0.08
0.08
0.07
0.07
0.06
0.08
0.06
0.06
0.07
0.06
0.13
0.07
0.06
0.07
0.07
0.06
0.07
0.08
0.06
0.08
0.11
0.08
0.08
0.06
0.07
0.06
0.06
0.06
0.05
0.05

0.49
0.45
0.45
0.43
0.44
0.39
0.37
0.40
0.39
0.40
0.40
0.34
0.41
0.40
0.39
0.33
0.36
0.37
0.39
0.33
0.43
0.41
0.33
0.32
0.31
0.32
0.32
0.32
0.32
0.34
0.34
0.45
0.36
0.34
0.26
0.27
0.29
0.23
0.28
0.29
0.29
0.32
0.22
0.24
0.22
0.30
0.21
0.32
0.21
0.29
0.25
0.17
0.29
0.19
0.16
0.23
0.18
0.19

62

