NBER WORKING PAPER SERIES

THE CAUSES OF PEER EFFECTS IN PRODUCTION:
EVIDENCE FROM A SERIES OF FIELD EXPERIMENTS
John J. Horton
Richard J. Zeckhauser
Working Paper 22386
http://www.nber.org/papers/w22386

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
July 2016

We thank Iwan Barankay, Raj Chetty, Lydia Chilton, Nicholas Christakis, Malcolm Wiley-Floyd,
Larry Katz, Renata Lemos, and Rob Miller for helpful comments and suggestions. We thank
Robin Horton, David Yerkes, and Heidi Yerkes for their help in preparing the manuscript. Horton
would like to thank the NSF-IGERT Multidisciplinary Program in Inequality and Social Policy
(GrantNo. 0333403), the University of Notre Dame, and the John Templeton Foundation’s
Science of Generosity Initiative for their generous financial support. We were greatly aided by
the excellent research assistance of Alex Breinen, John Comeau, Talia Goldman, Michelle
Lindner, and Justin Keenan. Author contact information, datasets, and code are currently, or will
be, available at http://www.john-joseph-horton.com/. The views expressed herein are those of the
authors and do not necessarily reflect the views of the National Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2016 by John J. Horton and Richard J. Zeckhauser. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.

The Causes of Peer Effects in Production: Evidence from a Series of Field Experiments
John J. Horton and Richard J. Zeckhauser
NBER Working Paper No. 22386
July 2016
JEL No. J01,J24,J3
ABSTRACT
Workers respond to the output choices of their peers. What explains this well documented
phenomenon of peer effects? Do workers value equity, fear punishment from equity-minded
peers, or does output from peers teach them about employers’ expectations? We test these
alternative explanations in a series of field experiments. We find clear evidence of peer effects, as
have others. Workers raise their own output when exposed to high-output peers. They also punish
low-output peers, even when that low output has no effect on them. They may be embracing and
enforcing the employer’s expectations. (Exposure to employer-provided work samples
influences output much the same as exposure to peer-provided work.) However, even
when employer expectations are clearly stated, workers increase output beyond those
expectations when exposed to workers producing above expectations. Overall, the evidence is
strongly consistent with the notion that peer effects are mediated by workers’ sense of fairness
related to relative effort.

John J. Horton
Leonard N. Stern School of Business
Kaufman Management Center
44 West Fourth Street, 8-81
New York, NY 10012
john.horton@stern.nyu.edu
Richard J. Zeckhauser
John F. Kennedy School of Government
Harvard University
79 John F. Kennedy Street
Cambridge, MA 02138
and NBER
richard_zeckhauser@harvard.edu

The Causes of Peer Effects in Production:
Evidence from a Series of Field Experiments
John J. Horton∗
NYU Stern

Richard J. Zeckhauser
Harvard University
June 24, 2016

Abstract
Workers respond to the output choices of their peers. What explains this well documented phenomenon of peer effects? Do workers value equity, fear punishment from equity-minded peers, or
does output from peers teach them about employers’ expectations? We test these alternative explanations in a series of field experiments. We find clear evidence of peer effects, as have others. Workers
raise their own output when exposed to high-output peers. They also punish low-output peers, even
when that low output has no effect on them. They may be embracing and enforcing the employer’s
expectations. (Exposure to employer-provided work samples influences output much the same as
exposure to peer-provided work.) However, even when employer expectations are clearly stated,
workers increase output beyond those expectations when exposed to workers producing above expectations. Overall, the evidence is strongly consistent with the notion that peer effects are mediated
by workers’ sense of fairness related to relative effort.
JEL J01, J24, J3
Keywords: peer effects, productivity, effort, field experiments

1 Introduction
A growing empirical literature documents that workers respond to the output choices of their fellow
workers. Chan et al. (2014), Bandiera et al. (2010), Mas and Moretti (2009), and Falk and Ichino (2006) all
demonstrate that co-workers can exert economically significant effects on their peers, via channels not
explicitly created by their firms. Even a firm that does not explicitly create the channels may welcome
their existence. Kandel and Lazear (1992) argue that peer pressure—or more precisely, peer punishment
∗ We thank Iwan Barankay, Raj Chetty, Lydia Chilton, Nicholas Christakis, Malcolm Wiley-Floyd, Larry Katz, Renata Lemos,

and Rob Miller for helpful comments and suggestions. We thank Robin Horton, David Yerkes, and Heidi Yerkes for their help
in preparing the manuscript. Horton would like to thank the NSF-IGERT Multidisciplinary Program in Inequality and Social
Policy (Grant No. 0333403), the University of Notre Dame, and the John Templeton Foundation’s Science of Generosity Initiative
for their generous financial support. We were greatly aided by the excellent research assistance of Alex Breinen, John Comeau,
Talia Goldman, Michelle Lindner, and Justin Keenan. Author contact information, datasets, and code are currently, or will be,
available at http://www.john-joseph-horton.com/.

1

for low productivity or low effort—ameliorates a firm’s agency problem that workers may slack. An example of this potentially useful form of peer effects (from the firm’s perspective) comes from Mas and
Moretti (2009), who found that grocery checkout clerks were more productive when observed by highly
productive fellow clerks. Mas and Moretti suggest that this higher productivity might arise from the implied threat of punishment. This seems plausible, particularly since, in their setting, there was a direct
free-riding externality: slacking off by one clerk would increase the workload imposed on other clerks.
It is unclear whether this result would extend to settings with no direct free-riding externality present.
Absent such an externality, workers might altruistically punish low-output peers if the actions of those
peers are perceived as unfair (Gachter and Fehr, 2000; Fehr and Gächter, 2000). In light of this possibility,
workers might try to match the output of their peers—or they may simply have an aversion to producing
low output relative to others.
Social learning about the employer’s expectations for employees is an alternative explanation to the
equity-based viewed of peer effects. New employees could use experienced employees as a readily accessible and cheaply available source of information. These veteran peers, given their tenure on the job,
have usually learned how to satisfy their employer’s expectations. Hence, their current behavior presumably represents a “revealed preference” example of effective worker behavior, unlike the statements made
by employers, which might be strategically motivated. This kind of social learning has proven important
in other production settings. For example, Pierce and Snyder (2008) document how vehicle emission inspectors quickly adopt the prevailing ethical norms of those with whom they work. In other production
settings, social learning has explained why different workers eventually make similar choices (Conley
and Udry, 2010), as they observe what “works” and then do the same.1 Nanda and Sørensen (2010) show
that having a colleague with entrepreneurial experiences increases the probability that that worker will
1 There is a literature on another kind of social learning in production settings that focuses on knowledge spill-overs—see
Azoulay et al. (2010) and Waldinger (2012). This would be unlikely to be important in the kinds of low-skilled tasks with observable output that have been the focus of much of the empirical peer effects literature. Output observability seems to be a
necessary but not sufficient condition for peer effects in production; Guryan et al. (2009) exploited the random assignment of
professional golfers to tournament foursomes, finding that peers did not matter in a golf tournament. However, this was also a
setting where extra effort was unlikely to be costly and free-riding externalities were absent. This is quite consistent with Cornelissen et al. (2013) and their finding compensating wages for peer effects only appear in settings where higher productivity is
costly. Further distinguishing the Guryan et al. (2009) setting from the rest of the literature, social learning about expectations
would be unlikely to be important, as professional golfers know precisely what constitutes good performance (though there
could have been learning about how to play a particular hole or situation).

2

also turn to entrepreneurship.2
This paper reports the results of a series of field experiments conducted in an online labor market whose purpose was to test the equity and the social learning explanations for peer effects in production settings. In each experiment, we operated as employers looking to have a series of images
labeled—a common task in the marketplace. This experimenter-as-employer paradigm is a commonly
used approach for creating contextualized field experiments in real world settings—for other examples
see Gilchrist et al. (2016) and Pallais (2014). Each experiment required workers to decide how much costly
output to produce. In some experiments, workers were asked to evaluate the output of their peers. This
setup allowed us to both (a) measure peer-punishment and (b) expose workers to the output choices of
their peers. To detect peer effects, in some experiments we asked workers post-evaluation to complete
another task, for which we could also measure the output.
Our first experiment established three important findings: workers were uncertain about the “firm’s”
expectations, they found output costly, and they responded to signals about the firm’s expectations (as
conveyed by employer-provided work samples). The experiment established that there was a need for
learning about employer expectations. The second experiment introduced a channel for peer effects
by having workers perform a task and then evaluate the output of a peer. The experiment showed that
workers readily punished low-output/low-effort peers, despite the absence of any direct free-riding externalities.
The third experiment added another component to the second experiment: evaluators had to perform a follow-on labeling task. Exposure to high-output peers increased workers’ output on the followon task. Although this experiment demonstrated that peer effects can be easily generated, it could not
distinguish between social learning about employer expectations and equity-based explanations. That
is, workers exposed to high output peers could have revised their beliefs about what the employer expected or revised their beliefs about what is fair (or what their peers would likely regard as fair), or both.
Our fourth experiment was designed to distinguish between social learning about employer expectations and equity-based peer effects. In this experiment, the firm’s exact expectations were clearly
communicated, therefore removing uncertainty about what the firm expected. This communication
2 The notion that peers could have useful information seems well supported empirically. For example, Iyer et al. (2015) shows

that peer evaluations of would-be lenders performs better than screening with purely a credit score.

3

of standards was effective, in that on the first task, nearly all workers complied with the output level
we requested. After this initial task, some evaluators evaluated work that complied with the firm’s expectations, while others evaluated work that exceeded the firm’s specified output expectations. We find
that not only did evaluators not punish high-effort workers (but technically speaking, non-complying
workers), but also they imitated them, increasing output in the subsequent task beyond what the firm
requested.
An implication of the fourth experiment—and the punishment results more generally—is that workers do not simply assess output by whether it perfectly matches with the employer’s stated expectations.
It is likely that the standard applied by the evaluating workers was not the employer’s standard per se, but
rather a judgment of the effort made by each evaluated worker relative to his/her own work. Consistent
with this interpretation, across experiments, punishment of peers increased strongly with the evaluator’s
own output.
This paper’s primary contribution is to establish that equity-considerations are essential to explaining peer effects in production, at least in the context considered. It is the first paper we are aware of that
can distinguish between social learning and equity-based explanations for peer effects. Further, it also
demonstrates that peers will punish even when there are no direct free-riding externalities. These findings speak directly to the micro-foundations of peer effects in production, which in turn matter for our
conceptions of team production, and ultimately the role of the firm. This paper’s secondary contribution
is to add to the large literature documenting the importance and strength of peer effects. We found peer
effects strong enough to “override” the firm’s clearly communicated preferences.
A limitation of our study is that it cannot distinguish between equity-based explanations that depend on a worker’s preference against relatively low output and a worker’s preference against not being
perceived as having relatively low output. That we explicitly had workers evaluate each other may have
planted the seed that being perceived as having low output by a fellow worker could be harmful.
As with any experimental study, our results have strong internal validity. They also have some of the
inherent external validity conferred by being conducted in the field, in a real production setting where
subjects were unaware of the experimental nature of their work. Although the production setting is atypical, it offers some advantages with respect to generalizability. Given the short duration of the “relation-

4

ships” created online and that fact that “peers” never interacted in person and are completely anonymous with respect to both the employer and each other, strong peer effects seemed a priori unlikely, yet
this proved not the case. What seems to matter for peer effects in productivity is the observability of output rather than the social interaction. Also, as each worker was working for us as an employer for the first
time, we expected that social learning about our expectations would be particularly important, yet this
too was not the case. In settings where interactions are longer lasting, any social learning would likely
diminish in importance over time, leaving only equity considerations.
Aside from our scientific interest in the workings of peer effects, our results speak to the efforts by
organizations to harness peer effects.3 While tantalizing, effectively exploiting peer effects has often
proven challenging in practice.4 The greatest successes of exploiting peer effects have not come from
production settings, but rather on product uptake—see Aral and Walker (2014), Aral and Walker (2011)
and Bapna and Umyarov (2015). We find that peer effects are strong and arise readily, but we also show
that they could in some cases be too strong; our final experiment showed that the effects of peers was
strong enough to counteract our clearly communicated output requests. Although this may seem like a
free lunch—higher output at the same pay—we also observed in the first experiment that workers sorted
themselves away from our tasks when they perceived our output standards as being excessive, making
it ambiguous whether actual firms would welcome the peer effects we created in the last experiment.
Consistent with the view that firms would have to compensate workers for higher and thus more costly
productivity, Cornelissen et al. (2013) find peer effects in wages, but only for relatively low-skilled occupations. One interpretation of this finding is that those low-skilled occupations have both observable
output (a prerequisite for peer effects in output) and a more direct relationship between productivity
and the disutility of effort.
The paper is organized as follows: Section 2 describes the empirical setting for our experiments. Section 3 presents the experimental designs and describes the experimental tasks and the features common
3 Harnessing such effects can be of significant value when contracts between a firm and its workers are incomplete, most
importantly when workers do not know the employer’s expectations. Basically, the firm is capitalizing on a behavioral phenomenon. See Koszegi (2014) for an overview of the literature on incorporating insights from behavioral economics into contract theory models.
4 For example, Carrell et al. (2013) offers a cautionary tale about trying to engineer Air Force Academy peer groupings to
improve academic performance. The effort strongly backfired for unforeseen reasons relating to the micro-foundations for
peer effects in education.

5

across experiments. Section 4 presents the results, and Section 5 concludes.

2 Empirical setting
The experiments were conducted on Amazon’s Mechanical Turk (MTurk), an online labor market. MTurk
is one of several online labor markets that have emerged in recent years (Frei, 2009). Researchers in a
number of disciplines have begun using these markets for experimentation.5 Some examples in economics include Mason and Watts (2009), Barankay (2010), Chandler and Kapelner (2013), and Horton
and Chilton (2010). Online experiments offer significant advantages relative to most laboratory experiments in cost, speed of accruing subjects, and representativeness of the subject pool. However, they
tend to be harder to control than conventional laboratory experiments. Despite this difficulty, Horton et
al. (2011) explain how valid causal inferences can be drawn in online labor markets such as MTurk and
oDesk.
Tasks posted on MTurk are called Human Intelligence Tasks (HITs). HITs vary, but most are small,
simple tasks that are difficult or impossible for machines but easy for people, such as transcribing audio clips, classifying and tagging images, reviewing documents, and checking websites for pornographic
content. The originators of HITs are called “requesters.” Requesters and workers are anonymous to each
other. Rarely do they interact on a repeat basis. The requester constructs the user interface for the HIT
and sets the conditions for payment, worker qualifications, and timing (for example, how long a worker
can work on a task).
To become eligible, a would-be worker must have a bank account and must have created a profile
with Amazon.6 Workers can only have one account, and Amazon uses several technical and legal means
to enforce this restriction. Workers can readily see the collection of HITs available to them and, in most
cases, view a sample of the required work. They can work on any task for which they are qualified. Once
they accept a HIT, they can begin work immediately.
Once a worker completes a HIT, the work product is submitted to the requester for review. Requesters
5 For an overview of online labor markets, see Horton (2010).
6 MTurk workers are often called “Turkers.” Their ranks appear to be split approximately evenly between the US and India.

Horton (2011) finds workers generally view online employers as having the same level of trustworthiness as offline, conventional employers. For the demographics of the MTurk population, see Ipeirotis (2010).

6

may “reject” work, in which case the worker is not paid. This ability of requesters to reject work creates
adverse consequences for providing work that falls short of employer expectations. Requesters may also
pay bonuses, allowing tailored payments based on individual performance within a nominally piece-rate
HIT.

3 Experimental design
Our first experiment, E XP-B ASELINE, tested two questions: 1) Do workers view the task as costly?, and
2) Does providing output samples effectively convey employer expectations, as revealed by changes in
output following observation of those samples? The experiment E XP-P UNISH tested a third question: 3)
Will workers punish low effort/low productivity relative to some standard? This test was conducted by
exposing workers to either a high- or low-output peer, and then asking the evaluating worker (a) whether
we, as the employer, should “approve,” that is, accept and pay for the work, and (b) how the evaluator
wanted to split a bonus with the evaluated worker. The third experiment, E XP-P EERS, tested: 4) Does
a worker’s exposure to the output choice of a peer—shown via an evaluation—influence that worker’s
subsequent output? In the fourth experiment, E XP-E XPLICIT, we attempted to remove any uncertainty
about the firm’s expectations in order to test: 5) Will workers thus informed punish work that exceeds
the employer’s stated expectations, and therefore deviates from them? In other words, will evaluators
punish workers for not conforming to what was expected? A worker might be understandably reluctant
to punish high-effort work even if it failed to comply on a technicality, especially if they think the employer has free disposal on “excess” output. The worker might believe as well that they are making the
same decision the employer would have made in the same context. However, the interesting result of
this experiment is whether exposure to this high-effort but non-complying work affects the evaluator’s
subsequent output. If it does, then it suggests a channel by which peer pressure can sustain behaviors or
levels of output that deviate from what the firm stated that it wants.

3.1 Preliminaries and common elements across experiments
In every experiment, before agreeing to participate, would-be workers were told about the task and the
payment for it, and were shown a completed work sample. The work sample was a “screenshot” of the
7

Figure 1: Pre-made Amazon image labeling or “tagging” interface

Notes: Screenshot of the image-tagging interface developed by Amazon.

image-labeling interface as it would look after a worker completed the task.7

3.1.1 Task and interface
Computers have a difficult time recognizing objects in images, yet this task is often valuable for firms.
Thus, image labeling is a common “human computation” task found on MTurk (von Ahn and Dabbish,
2004; Huang et al., 2010). It is one of a handful of canonical MTurk tasks for which Amazon has created
pre-made templates. Figure 1 depicts one of Amazon’s pre-made interfaces.
To label images, workers in our experiment used an interface we developed, shown in Figure 2. To
add a label, workers clicked a button labeled “Add a label” positioned below the image.8 Clicking the
button brought forth a new blank text field to be added to the survey. Workers could add as many labels
as they wanted. When they were finished, they clicked a button labeled “Submit labels.”
7 Because subjects were not informed that they were participating in experiments, the experiments were “natural” field experiments in the Harrison and List (2004) taxonomy.
8 The images themselves were selected from the photo-sharing site Flickr. The images each had a Creative Commons license
and were chosen because they were conducive to labeling (for example, photos depicting elaborate meals with many easily
recognizable different foods).

8

3.1.2 Peer evaluations
In all but E XP-B ASELINE, a worker played two roles: the producer of labels of images and the evaluator of
the work of a peer who had also engaged in the image-labeling task. To evaluate the peer, the evaluating
worker viewed that labeling worker’s assigned image and that worker’s generated labels. The evaluating
worker then made recommendations on two matters: whether to approve (pay for) or not approve (not
pay for) the task, and how to split a 9-cent bonus between him- or herself and the evaluated worker.
All evaluating workers within an experimental group assessed the same worker’s output. This evaluated worker was chosen at random from subjects who had participated in previous experiments and
whose work had exhibited the desired property for that experimental group (either high or low effort).
The workers performing either task – labeling images or evaluating performance – likely regarded the
evaluation and bonus schemes as unremarkable. It is a very common quality-control technique to have
workers evaluate the work of other Turkers. Furthermore, bonuses are frequently used to incentivize
good performance.
For the accept/reject question, the evaluating workers were asked:
Should we approve this work?
They had to answer “yes” or “no.” For the peer evaluation, the evaluators were told:
We want to determine how good this work is. We would like you to decide, based on your
work and the quality of the other work, how to split a 9-cent bonus.
The evaluating workers selected an answer from a list of 9 options of the form “X cents for the other, 9− X
cents for me,” with X ranging from 0 to 9 (9 cents was chosen as the endowment to reduce the salience
of the focal point 50-50 split). Both questions were asked on the same survey page, and the evaluators
could answer them in either order, though the approval question appeared first on the page. At the end
of the experiment, we implemented all choices, with the recommended bonuses paid to the evaluated
workers and their evaluators, in accordance with the evaluator’s preferences.

9

3.1.3 Demographic survey and allocation
In each experiment, subjects answered a short demographic survey before beginning work. Subjects
were asked their sex, nationality, and whether they were doing this work primarily to earn money, learn
new skills, or have fun. Demographics differed slightly across experiments, probably due to differences
in the times the experiments were launched. Although one might conjecture that the survey would raise
suspicions that the task was an experiment, we view this as unlikely. Asking workers for basic demographic information is fairly common in the market.9 In each experiment, subjects were assigned alternately to groups in order of arrival time (for instance, Subject 1 was assigned to treatment, Subject 2 to
control, Subject 3 to treatment, and so on) to give better balance.

3.2 E XP-B ASELINE: Baseline experiment
Participants in this experiment were assigned to either H IGH or L OW. In H IGH , the employer-provided
work sample showed 9 labels, compared to only 2 labels in L OW. Figure 2 shows the two work samples.
After viewing their particular work sample, workers chose to participate and label an image or to exit.
Those who chose to participate then performed a labeling task.
Table 1 reports the means of various demographic measures collected for the E XP-B ASELINE participants. While the set of covariates is limited, there is no indication that the randomization was ineffective.
The same analysis was conducted for the other experiments reported in the paper, but the results mirror those from the first experiment. Although not reported, the full dataset and this auxiliary analysis is
available online.
Table 1: Comparison of covariate pre-treatment means for exerimental groups in E XP-B ASELINE
Variable

HIGH group

LOW group

t-stat

From India
Male
Reports that primary motivation is money
From the US

0.54
0.61
0.78
0.35

0.43
0.55
0.74
0.40

1.13
0.54
0.43
-0.56

Notes: This table reports the means for the H IGH and L OW experimental groups in E XP-B ASELINE. The t-statistic for a difference in means is reported.

9 The IRB approval for these experiments did not require notification that the work was part of a research project.

10

Figure 2: Work samples shown to workers prior to task acceptance in E XP-B ASELINE

(a) H IGH work sample

(b) L OW work sample

Notes: The panels show the work sample given to would-be workers considering accepting the imagelabeling task. Subjects assigned to H IGH were shown the left image with its 9 labels; subjects assigned to
L OW were shown the right image with its 2 labels.

11

Figure 3: Work evaluated by subjects in E XP-P UNISH

(a) G OOD work sample

(b) B AD work sample

Notes: The image in the left panel is the work output evaluated by workers assigned to G OOD. The image in
the right panel is the work output evaluated by workers assigned to B AD.

3.3 E XP-P UNISH: Punishment experiment
The job posting for this experiment was the same as in E XP-B ASELINE with one addition: potential subjects were told that they would be evaluating the work of another worker. Before accepting the task, all
subjects were shown the H IGH work sample displayed in Figure 2. To reward the additional evaluation
work, the participation payment was raised from 30 cents to 40 cents. The requested sample size was
also increased from 100 to 200.10
In this experiment, all subjects first completed the same image-labeling task before they were randomized into the two groups. Subjects assigned to G OOD inspected the output of a worker (selected
from E XP-B ASELINE) who had produced 12 unique labels. Subjects assigned to B AD inspected the output of a worker who had produced only one unique label. The output samples of the evaluated workers
are shown in Figure 3.
10 We determined our sample size by examining what others had done—e.g., Falk and Ichino (2006) had 24 subjects—and
increasing it substantially (by a factor of 4), as is possible in online settings.

12

3.4 E XP-P EERS: Peer experiment
E XP-P EERS added one element to E XP-P UNISH : we asked workers to do a second labeling task after
completing their evaluation task. The two experimental groups are G OOD and B AD. In G OOD, subjects
evaluated a worker who produced 11 labels; in B AD, subjects evaluated a worker who produced only 2
labels. The requested sample size was 300, and each subject’s payment was 40 cents.

3.5 E XP-E XPLICIT: Explicit experiment
E XP-E XPLICIT re-ran E XP-P EERS with one exception: we provided explicit employer instructions to subjects before exposing those workers to the output of peers. The workers were told that they should produce 2 labels per image. The requirement of 2 labels was stated before workers began the task, and was
presented again with each of the two image-labeling tasks, directly above the image and in the instructions. The left panel of Figure 4 shows initial instructions, while the right panel shows the instructions
placed above both the first and the subsequent labeling tasks. In the left panel, instructions explain that
the worker is to provide two labels for an image, rate the work of another worker and then provide two
labels for an additional image. The right panel shows the interface for the subsequent labeling task. Note
that it reiterates the requirement that the worker provide two labels.
After performing the initial task, workers were randomized into one of two groups: M EET, in which
subjects evaluated a work sample showing x = 2, and O VER, in which subjects evaluated a work sample
showing x = 11. After evaluating the work, subjects performed an additional image-labeling task. The
requested sample size was 300, and the payment was 40 cents.

4 Results
4.1 E XP-B ASELINE: Is the labeling task perceived by the workers as costly, and are the conveyed employer expectations salient?
The first experiment shows that having a worker observe employer-provided work samples affects labor supply on the extensive margin. Showing a high-work sample in H IGH without requiring any peer
evaluations effectively conveys the firm’s expectations.

13

Table 2: Overview of the experiments

E XP-B ASELINE
(Groups = H IGH & L OW, Payment = 30¢, N = 93)

Description

Results

Subjects, according to experimental group, viewed one of two

H IGH increased labor supply on intensive margin, but de-

employer-provided work samples, then produced whatever

creased it on the extensive margin.

number of labels they chose (if any). Work samples differed
by experimental group. In H IGH, subjects first viewed a highoutput work sample (many labels). In L OW, subjects first
viewed a low-output work sample (few labels).

E XP-P UNISH
(Groups = G OOD & B AD, Payment = 40¢, N = 167)

Description

Results

Subjects viewed an employer-provided high-output work

G OOD subjects decreased punishment.

sample, then produced whatever number labeles they chose.

workers punished more.

More productive

Subjects then evaluated another worker’s work product. Subjects in G OOD evaluated a high-effort work sample. Subjects
in B AD evaluated a low-effort work sample.

E XP-P EERS
(Groups = G OOD & B AD, Payment = 40¢, N = 275)

Description

Results

Subjects viewed an employer-provided work sample, then

G OOD raised output on the second task. Effects were stronger

chose how many labels to produce. Subjects then evaluated

for subjects with high effort in the first task.

another worker’s work product, then labeled a second image.
Subjects in G OOD evaluated a high-effort work sample. Subjects in B AD evaluated a low-effort work sample.

E XP-E XPLICIT
(Groups = M EET & O VER, Payment = 40¢, N = 272)

Description

Results

Subjects viewed an employer-provided work sample with 2 la-

O VER caused many workers to not comply, even when they

bels. Subjects were told that 2 and only 2 labels should be

complied on the first task. Workers did not punish high-effort

produced. Subjects then labeled an image, evaluated another

but non-complying work.

worker’s work product, then labeled a second image, with the
same instruction to create only 2 labels. In O VER, subjects
evaluated a worker producing too many labels. In M EET, subjects evaluated a worker producing the required number of
labels.

14

Figure 4: E XP-E XPLICIT communication of employer standards

(a) Initial E XP-E XPLICIT instructions

(b) E XP-E XPLICIT instructions repeated

Notes: This screenshot shows the labeling interface in E XP-E XPLICIT. The left panel shows the initial instructions which explain that the worker is to provide 2 labels for an image, rate the work of another worker
and then provide 2 labels for an additional image. The right panel shows the interface for the subsequent
labeling task. Note that it reiterates the requirement that the worker provide 2 labels.

Figure 5: Distribution of labels produced by experimental group in E XP-B ASELINE
HIGH

Number of subjects

16

Mean group output = 4.6

12
8
4

LOW
16

Mean group output = 2.6

12
8
4
0

5

10

15

20

Number of labels produced
Notes: This figure plots the count of experimental subjects producing the number of labels listed on the xaxis. For example, 14 subjects in the H IGH experimental group produced 0 labels. Subjects in H IGH were
shown a work sample consisting of 9 labels prior to performing, while subjects in L OW were shown a work
sample with only 2 labels. Group mean output is shown, with zeros included.

The main results from E XP-B ASELINE are displayed in Figure 5. The number of labels produced is on
the x-axis, while the number of subjects producing that many labels is on the y-axis. The top facet shows
the output for subjects in the H IGH group, while the bottom facet shows output for subjects in L OW. It
shows that 5 subjects in H IGH produced more than 12 labels, but only 1 subject in L OW produced more

15

than 12 labels.
It also reveals that many more subjects in H IGH produced no output at all, namely 14 verus 5. Presumably, they were discouraged by the expectations implied in the image they were shown. In Table 3,
we confirm what Figure 5 suggests: In Column (1), we regress whether the worker produced any labels at
all on the treatment indicator, while in Column (2), we regress the count of labels on that same indicator.
H IGH employer expectations reduced labor supply on the extensive margin, but increased it on the
intensive margin. However, the latter effect outweighed the former: even with the non-participants included as x = 0 observations, subjects in H IGH produced roughly 2 more labels per person, on average
(4.6 versus 2.6). Because this unconditional output rose significantly in H IGH, we know that selection
does not explain the observed increase in output.
Table 3: Effects of perceptions of employer’s output expectations on extensive and intensive labor supply
in E XP-B ASELINE
Dependent variable:
Any output?

Output

(1)

(2)

−0.177∗
(0.084)

Assigned to H IGH

0.872∗∗∗
(0.059)

Intercept

Observations
R2
Adjusted R2
Residual Std. Error (df = 91)
F Statistic (df = 1; 91)

93
0.046
0.036
0.406
4.411∗

1.970∗
(0.939)
2.638∗∗∗
(0.660)
93
0.046
0.036
4.528
4.402∗

Notes: This table reports the results of two robust OLS regressions where the dependent variables are measures of worker labor supply. In this experiment, all subjects were
invited to participate in a paid image-labeling task. Those subjects assigned to H IGH
viewed an employer-provided work sample with 9 labels, while subjects in L OW viewed
a sample with only 2 labels. In Column (1), the outcome variable is labor supply on the
extensive margin, that is, whether the worker accepted the task and generated any labels
at all. In Column (2), the outcome variable is labor supply on the intensive margin, that
is, the number of labels the worker produced, with 0s included. Significance indicators:
p ≤ 0.05 : ∗, p ≤ 0.01 : ∗∗ and p ≤ .001 : ∗ ∗ ∗.

16

This experiment has important implications for our study. These findings indicate that we chose a
task: (a) that workers found personally costly to perform, in a setting where (b) the employer’s expectations were easily conveyable, and (c) that workers factored those expectations into their decision making.

4.2 E XP-P UNISH: Do workers punish low effort/productivity?
In E XP-P UNISH, workers were randomly assigned to evaluate either good work (in G OOD) or bad work (in
B AD). We wanted to test whether evaluators would still punish even in the absence of direct free-riding
externalities.
The main results from E XP-P UNISH can be seen in Figure 6. The figure contains four histograms;
each shows the count of evaluators choosing different allocations of the 9-cent bonus between themselves and their evaluated worker. The plots are defined by experimental group (column) and evaluator
recommendation regarding approval (row). Evaluators in G OOD were very unlikely to recommend rejection, whereas evaluators assigned to B AD were fairly likely to do so. For the B AD/reject evaluators,
the modal transfer was 0 cents. Apart from the evaluators in B AD who recommended rejection, few
evaluators in either group transferred 0 cents.
Most G OOD evaluators, as well as a large number of evaluators who recommended approval despite
being in B AD, chose a more or less even split that gave 4 or 5 cents to the worker.
Figure 6: Distribution of reward and punishment by workers of their peers based upon perceived effort/quality in E XP-P UNISH
Recomend Approval

Recomend Approval

Evaluated BAD

Evaluated GOOD

30

Mean bonus = 4.6

Count of subjects

20

Mean bonus = 5.1

10
0
Recomend Rejection

Recomend Rejection

Evaluated BAD

Evaluated GOOD

Treatment
Evaluated BAD
Evaluated GOOD

30

Mean bonus = 2.4

20

Mean bonus = 3.0

10
0
0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

Bonus Given to Other Player

Notes: This figure shows the bonus split and the accept/reject recommendation by treatment group. Subjects in B AD evaluated work with 1 generic label, while subjects in G OOD evaluated work with 12 specific
and appropriate labels. Group mean bonus size is shown with a vertical line.

17

Table 4 confirms the results the graphical analysis portrays. The dependent variable in Columns (1)
and (2) indicates whether the evaluating worker recommended that we pay the evaluated worker. The
independent variable in Column (1) is the treatment indicator. Evaluators assigned to G OOD were far
more likely to recommend acceptance than those assigned to B AD. The baseline was 50% in B AD and
increased to above 90% in G OOD.
The independent variable in Column (2) is the number of labels the evaluating workers produced
themselves, prior to the evaluation. As is evident, workers who produced more labels themselves were
more likely to recommend rejection. The dependent variable in Columns (3) and (4) is the bonus size.
As with the accept/reject measure, we can see that G OOD increased transfers in the contextualized dictator game. Moreover, these transfers were decreasing in the evaluator’s own output. For both measures
of punishment, higher-output workers were harsher critics; they were both more likely to recommend
rejection and to award smaller bonuses.

4.3 E XP-P EERS: Do peers influence output?
As in E XP-P UNISH, subjects in G OOD in E XP-P EERS were more likely to recommend acceptance and
transfer larger bonuses on average than were subjects in B AD. This was expected, and we do not present
the analysis. Rather, we focus on the evaluating worker’s subsequent output. Recall that, in the E XPP EERS experiment, after the initial output task and evaluation task, workers performed a second labeling
task. As we show, workers assigned to G OOD produced more output than those in B AD in their second
production task.
Figure 7 plots the workers’ subsequent output, x 2 , versus their initial output, x 1 . We fit separate lines
for the two treatment groups, with 95% confidence intervals for the conditional mean illustrated with
shaded regions. The regression line for G OOD lies everywhere above the line for B AD, and it is steeper.
The regression analysis in Table 5 confirms this graphical analysis. The outcome measure in each column is the number of labels each evaluating worker produced in the second labeling task. Column (1)
shows that assignment to G OOD increased the mean number of labels by more than 2, from a baseline
of just 5. Column (2) adds the worker’s pre-treatment output as a regressor. As would be expected, past
performance predicts future performance. Column (3) adds an interaction term between the number of

18

Table 4: Reward and punishment by workers of their peers based upon perceived effort/output in E XPP UNISH
Dependent variable:
Recommend Approval?
(1)
Assigned to GOOD

(2)
∗∗∗

Observations
R2
Adjusted R2
Residual Std. Error (df = 165)
F Statistic (df = 1; 165)

(3)

(4)
∗∗∗

0.418
(0.063)

1.442
(0.390)
−0.055∗∗∗
(0.013)

Initial Output, x 1

Intercept

Bonus Size Awarded

0.500∗∗∗
(0.045)

0.960∗∗∗
(0.065)

167
0.213
0.208
0.404
44.616∗∗∗

167
0.105
0.100
0.431
19.361∗∗∗

−0.259∗∗∗
(0.074)
3.488∗∗∗
(0.278)
167
0.077
0.071
2.518
13.682∗∗∗

5.376∗∗∗
(0.383)
167
0.069
0.064
2.528
12.269∗∗∗

Notes: This table reports the results of robust OLS regressions of two measures of peer punishment in E XP-P UNISH. In G OOD,
workers evaluated the work output of a worker generating 12 specific and highly appropriate labels; the other workers in B AD
evaluated a worker producing only 1 generic label. In Columns (1) and (2), the outcome measure is whether the worker recommended that the employer “approve” the work of the evaluated worker and thus pay them. In Columns (3) and (4), the outcome
measure is the amount of bonus transferred to that evaluated worker, out of an endowment of 9 cents. The key independent
variable in Columns (1) and (3) is the treatment indicator, G OOD, whereas in Columns (2) and (4), the key independent variable
is the evaluating worker’s output. The regressions in (2) and (4) are not causal, but they illustrate the strong negative relationship between own-output and the tendency to punish. Significance indicators: p ≤ 0.05 : ∗, p ≤ 0.01 : ∗∗ and p ≤ .001 : ∗ ∗ ∗.

19

x2 : Labels produced after evaluation

Figure 7: Subsequent number of labels, x 2 , versus initial number of labels, x 1 , by whether subject evaluated G OOD or B AD in E XP-P EERS
GROUP
●

20

BAD
GOOD

●

15
●

●

●

●
●

●

10

●

●

●
●
●

●
●
●

●

●●●
●
●

0

●

●●●●●

●

●
●

● ●
●●

●
●

●

●

●
●

●

●

●

●

●

● ●

●

●●

●

●
●

● ●●
●
●●

●●
●

●

●
● ●●

●
●

●

●

●

●

●

●

●
●●

●

●
●●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●●
●
●
●
●

●●
● ● ●

●

●
●

●
●

●●
●
●

●

●

0

●

●

●

5

●

●●

●

● ●

●

●

●●
●

●
●

●

●

5

10

15

x1 : Labels produced before evaluation

Notes: In this plot the y-axis is subsequent output, x 2 , and the x-axis is initial output, x 1 , in E XP-P EERS, by
experimental group. All subjects performed an identical initial task and chose some number of labels to
provide (shown on the x-axis). Each subject then evaluated another subject’s work that demonstrated either
(B AD) low productivity or (G OOD) high productivity. All output levels are randomly perturbed by a small
amount to prevent over-plotting.

number of labels, x 1 , and the treatment indicator. The positive and significant effect of the interaction
term implies that the initially high-output workers had the greatest subsequent output response to their
exposure.

4.4 E XP-E XPLICIT: How do peer effects change when employer requirements are explicitly
identified? Do evaluators punish high effort if it deviates from employer expectations?
The results from E XP-P EERS show that workers are influenced by peers and not simply by employerprovided sources of information. However, these results do not establish the source of these peer effects.
Are they driven by social learning about the firm’s standards or by social learning about peer standards?11
E XP-E XPLICIT was designed to distinguish between these two potential sources of social learning. In
E XP-E XPLICIT, workers first did a task in which they were given explicit instructions to produce only two
labels. They then evaluated a peer who either precisely met the employer’s expectation (M EET) or who
produced more than two labels (O VER). After this evaluation, the evaluating worker performed another
labeling task. For this subsequent labeling task, our key question was whether having explicit employer
instructions “stamped out” peer effects. Note that this experiment offers a strong test, since the employer
11 Note that in defining what the firm expects for output, we use the terms “expectation,” “requirement” and “standard” inter-

changeably.

20

Table 5: Effects of evaluating a co-worker on subsequent output in E XP-P EERS
Dependent variable:
Number of labels produced after evaluation, x 2

Assigned to GOOD

(1)

(2)

(3)

2.169∗∗∗
(0.488)

2.290∗∗∗
(0.370)

0.971
(0.699)

0.884∗∗∗
(0.063)

0.764∗∗∗
(0.083)

Initial Output, x 1

0.280∗
(0.126)

GOOD ×x 1

5.043∗∗∗
(0.338)

Intercept

Observations
R2
Adjusted R2
Residual Std. Error
F Statistic

265
0.070
0.066
3.970 (df = 263)
19.744∗∗∗ (df = 1; 263)

0.815∗
(0.395)
265
0.469
0.465
3.006 (df = 262)
115.600∗∗∗ (df = 2; 262)

1.388∗∗
(0.470)
265
0.479
0.473
2.984 (df = 261)
79.858∗∗∗ (df = 3; 261)

Notes: This table reports robust OLS regressions from E XP-P EERS. Workers were randomly assigned to evaluate either a
worker exhibiting (G OOD) high productivity/effort or (B AD) low productivity/effort. In each column, the output measure
is the number of labels a worker produced after evaluating another worker, x 2 . Significance indicators: p ≤ 0.05 : ∗,
p ≤ 0.01 : ∗∗ and p ≤ .001 : ∗ ∗ ∗.

21

imposes a ceiling, as well as a floor, on production. The explicit employer instructions are designed to let
the worker know the firm’s precise expectation, potentially sealing the social learning channel for peer
effects.
Figure 8: Numbers of workers in different output bands for the subsequent task in E XP-E XPLICIT, faceted
by initial output, with experimental groups indicated by line type

Notes: For initially complying subjects (middle panel), assignment to O VER increased the relative number
of subjects producing additional (and hence non-complying) output in the second task.

Figure 8 illustrates the main results of this experiment but needs some explanation. The plots show
subjects moving from different pre-treatment output “bins” to post-treatment output bins. The three
side-by-side panels correspond to the three output levels that workers chose for the initial task: x 1 = 1,
x 1 = 2, and x 1 ≥ 3 (labeled 3+ in the figure). Thus, the left panel presents results only for those subjects
that generated x 1 = 1 label; the center panel presents results only for those subjects that initially complied by producing x 1 = 2 labels; and so on. Within each panel, these same bands are used again for
the x-axis, for the count of labels produced in the subsequent task; that is, x 2 = 1, x 2 = 2, and x 2 ≥ 3.
The y-axis indicates the count of subjects in that (x 1 , x 2 ) bin. The two experimental groups are shown
separately by differences in line type: M EET is shown with a dashed line; O VER is shown with a solid
line.
Several interesting results emerge from Figure 8. First, the center panel received most of the observations, indicating that the majority of workers initially complied with employer requirements and
produced exactly 2 labels. In the left and right panels, there are no differences between the experimental
groups; the lines nearly overlap. The center panel shows results that are quite different. For these subjects who complied initially, exposure to O VER reduced compliance in the subsequent task. Specifically,
22

the O VER treatment increased output among those initially complying. (The dotted line lies below the
solid line at x 2 ≥ 3.)
Table 6 confirms our graphical analysis. However, unlike in Figure 8, it restricts attention solely to
those subjects who originally complied, x 1 = 2. In this restricted sample, the chance that worker error or
misunderstanding drove the results is reduced. Subjects in M EET had a 90% compliance rate, whereas
subjects in O VER had only a 70% compliance rate. Note that we can restrict our sample in this way
because the x 1 choice was made pre-treatment. In Columns (1) and (2), the outcome variable is an
indicator for compliance with the employer instructions, 1{x 2 = 2}, in the subsequent task. In Column
(3), the outcome variable is an indicator for choosing 1{x 2 ≥ 3}.
We can see in Column (1) that assignment to M EET increased compliance in the subsequent task.
In Column (3), we can see where the noncompliers went; they almost universally increased their output
to x 2 ≥ 3. Might these results be driven by a misunderstanding of the instructions? The regression in
Column (2) helps us answer that question. In Column (2), we interact the treatment indicator with an
indicator for whether the subject was from India, where English is often not the primary language. Although it is not necessarily the case that subjects from India have worse English skills than those from
the US, in other online labor markets, employers at least act is if they expect workers from less developed
countries to be more likely to have communication difficulties (Agrawal et al., 2013). In our regressions,
we see no country-specific effect, which suggests that exposure to the high-output peer did not cause
the subject to simply second-guess the employer’s written instructions.

5 Conclusion
The findings from these experiments contribute to the literature on peer effects in several ways. Their
primary contribution is to highlight the importance of equity considerations in explaining peer effects.
This finding of importance was not ex ante obvious, as our first experiment showed that workers are
sensitive to the perceived standards of employers. Thus, any peer effects could actually be social learning
about the employer’s expectations. However, subsequent experiments showed that workers evaluate and
altruistically punish other workers on the basis of perceived effort rather than on the basis of simple
adherence to the employer’s expectations. Furthermore, the results showed that workers were strongly
23

Table 6: Effects of exposure to high-effort, non-complying peer work after explicit employer instructions
about compliance
Dependent variable:
Comply

Assigned to M EET

1{x 2 = 2}

1{x 2 = 2}

Over-Produce
1{x 2 > 2}

(1)

(2)

(3)

∗∗∗

∗∗

0.230
(0.059)

0.247
(0.077)

India

0.046
(0.086)

Complied (x 1 = 2) × India

Constant

Observations
R2
Adjusted R2
Residual Std. Error
F Statistic

−0.218∗∗∗
(0.050)

−0.045
(0.122)
0.705∗∗∗
(0.042)

0.688∗∗∗
(0.054)

155
0.089
0.083
0.370 (df = 153)
15.002∗∗∗ (df = 1; 153)

155
0.091
0.073
0.372 (df = 151)
5.038∗∗ (df = 3; 151)

0.231∗∗∗
(0.035)
155
0.110
0.104
0.311 (df = 153)
18.956∗∗∗ (df = 1; 153)

Notes: This table reports robust OLS regressions from E XP-E XPLICIT. In this experiment, subjects were assigned to evaluate either (M EET) a work sample exactly meeting the employer’s expectations or (O VER) a non-complying (but higheffort) sample. In Columns (1) and (2), the outcome variable indicates whether the worker complied with the employer’s
stated output requirement of exactly two labels, 1{x 2 = 2}. In Column (3), the outcome is whether the worker exceeded
the employer’s stated output requirement of 2 labels. Significance indicators: p ≤ 0.05 : ∗, p ≤ 0.01 : ∗∗ and p ≤ .001 : ∗∗∗.

24

influenced by high-effort peers, even when they deviated from the employer’s instructions.
A secondary contribution of our findings is to offer yet another example of peer effects in a real production setting. Our results do not merely show that peer effects exist. They show that these effects
are strong and arise in a setting when there is no personal interaction between peers. Across the experiments, variation in the exposure to different work samples explained a substantial fraction of the
variation in observed output. Moreover, this finding arose despite an experimental setting that offered
very short “interactions” that were ostensibly one-shot. In more traditional settings with longer and
more consequential channels for peer interactions, it seems likely that the incentives landscape created
by peers would substantially affect individual performance.
A natural question for managers is whether they should encourage peers to influence each other,
such as by optimally arranging teams to maximize productivity. Context surely matters greatly. In settings where effort and productivity are tightly coupled and workers can easily monitor each other, peer
pressure would seem to provide a kind of free lunch for the firm. However, recall from our first experiment that workers who infer that the firm had high standards were more likely to exit and complete no
labels, so perhaps “cheap lunch” is a more appropriate term, in that our per-output costs went down
substantially because of the productivity boost from peer effects.
In some contexts, giving workers the ability to punish or reward their peers may hurt, not help. Workers might enforce inefficient standards. For example, in noise-filled environments, where the connection
between effort and productivity is tenuous and where explicit firm-provided incentives are highly muted,
the firm might be worried that otherwise-good workers would feel compelled to feign industry to placate
effort-monitoring peers who punish and/or reward. When such dangers loom, the firm might even want
to go so far as to prevent workers from monitoring each other. In short, enabling peer pressures when
peers may be punishing or rewarding undesired outcomes is risky.
One unexplored organizational implication of these findings is the possibility of a feedback loop or
cascade. The potentially causal dependence between one’s own productivity and the willingness to punish, combined with susceptibility to peer effects, provides one such mechanism. Cascades can be harmful, for example, if after workers observe idiosyncratically bad work, they lower their own output and
punish less, which in turn reduces other workers’ incentives to be highly productive. On the beneficial

25

side, employers will seek to harness peer effects when there is strong potential for a constructive cascade,
when idiosyncratically productive work spurs superior output from those monitoring, who then induce
superior output from others. The potential for hard effort to spread from one worker to another helps to
explain why organizational leaders often use the language of contagion to describe morale, and why so
much of management theory focuses on understanding and influencing organizational culture.

References
Agrawal, Ajay K, Nicola Lacetera, and Elizabeth Lyons, “Does information help or hinder job applicants
from less developed countries in online markets?,” 2013.
Aral, Sinan and Dylan Walker, “Creating Social Contagion Through Viral Product Design: A Randomized
Trial of Peer Influence in Networks,” Management Science, 2011, 57 (9), 1623–1639.
and

, “Tie Strength, Embeddedness, and Social Influence: A Large-Scale Networked Experiment,”

Management Science, 2014, 60 (6), 1352–1370.
Azoulay, Pierre, Joshua Graff Zivin, and Jialan Wang, “Superstar Extinction,” The Quarterly Journal of
Economics, 2010, 125 (2), 549–589.
Bandiera, Oriana, Iwan Barankay, and Imran Rasul, “Social incentives in the workplace,” Review of
Economic Studies, 2010, 77 (2), 417–458.
Bapna, Ravi and Akhmed Umyarov, “Do Your Online Friends Make You Pay? A Randomized Field Experiment on Peer Influence in Online Social Networks,” Management Science, 2015, 61 (8), 1902–1920.
Barankay, Iwan, “Rankings and Social Tournaments: Evidence from a Field Experiment,” Working Paper,
2010.
Carrell, Scott E, Bruce I Sacerdote, and James E West, “From natural variation to optimal policy? The
importance of endogenous peer group formation,” Econometrica, 2013, 81 (3), 855–882.
Chan, Tat Y., Jia Li, and Lamar Pierce, “Compensation and Peer Effects in Competing Sales Teams,”
Management Science, 2014, 60 (8), 1965–1984.
26

Chandler, Dana and Adam Kapelner, “Breaking Monotony with Meaning: Motivation in Crowdsourcing
Markets,” Journal of Economic Behavior & Organization, 2013, 90, 123–133.
Conley, Timothy G and Christopher R Udry, “Learning about a new technology: Pineapple in Ghana,”
The American Economic Review, 2010, pp. 35–69.
Cornelissen, Thomas, Christian Dustmann, and Uta Schönberg, “Peer Effects in the Workplace,” 2013.
Falk, Armin and Andrea Ichino, “Clean evidence on peer effects,” Journal of Labor Economics, 2006, 24
(1).
Fehr, Ernst and Simon Gächter, “Fairness and retaliation: The economics of reciprocity,” The journal of
economic perspectives, 2000, pp. 159–181.
Frei, Brent, “Paid Crowdsourcing: Current State & Progress toward Mainstream Business Use,” Produced
by Smartsheet.com, 2009.
Gachter, Simon and Ernst Fehr, “Cooperation and Punishment in Public Goods Experiments,” American
Economic Review, 2000, 90 (4), 980–994.
Gilchrist, Duncan S, Michael Luca, and Deepak Malhotra, “When 3+ 1> 4: Gift structure and reciprocity
in the field,” Management Science, 2016.
Guryan, Jonathan, Kory Kroft, and Matthew J. Notowidigdo, “Peer effects in the workplace: evidence
from random groupings in professional golf tournaments,” American Economic Journal: Applied Economics, 2009, 1 (4), 34–68.
Harrison, G.W. and John A. List, “Field experiments,” Journal of Economic Literature, 2004, 42 (4), 1009–
1055.
Horton, John, “Online Labor Markets,” Internet and Network Economics, 2010, pp. 515–522.
Horton, John J., “The condition of the Turking class: Are online employers fair and honest?,” Economics
Letters, 2011, 111 (1), 10–12.

27

and Lydia B. Chilton, “The labor economics of paid crowdsourcing,” Proceedings of the 11th ACM
Conference on Electronic Commerce, 2010.
, David G. Rand, and Richard J. Zeckhauser, “The online laboratory: Conducting experiments in a
real labor market,” Experimental Economics, 2011, 14 (3), 399–425.
Huang, E., H. Zhang, D.C. Parkes, K.Z. Gajos, and Y. Chen, “Toward automatic task design: A progress
report,” in “Proceedings of the ACM SIGKDD Workshop on Human Computation (HCOMP)” 2010.
Ipeirotis, Panagiotis G., “Demographics of Mechanical Turk,” Working Paper, 2010.
Iyer, Rajkamal, Asim Ijaz Khwaja, Erzo F. P. Luttmer, and Kelly Shue, “Screening Peers Softly: Inferring
the Quality of Small Borrowers,” Management Science, 2015, 0 (0), null.
Kandel, Eugene and Edward P. Lazear, “Peer pressure and partnerships,” Journal of Political Economy,
1992, pp. 801–817.
Koszegi, Botond, “Behavioral contract theory,” Journal of Economic Literature, December 2014, 52 (4),
1075–1118.
Mas, Alexander and Enrico Moretti, “Peers at work,” American Economic Review, 2009, 99 (1), 112–145.
Mason, Winter and Duncan J. Watts, “Financial incentives and the ‘performance of crowds’,” in “Proc.
ACM SIGKDD Workshop on Human Computation (HCOMP)” 2009.
Nanda, Ramana and Jesper B. Sørensen, “Workplace Peers and Entrepreneurship,” Management Science, 2010, 56 (7), 1116–1126.
Pallais, Amanda, “Inefficient hiring in entry-level labor markets,” The American Economic Review, 2014,
104 (11), 3565–3599.
Pierce, Lamar and Jason Snyder, “Ethical Spillovers in Firms: Evidence from Vehicle Emissions Testing,”
Management Science, 2008, 54 (11), 1891–1903.
von Ahn, L. and L. Dabbish, “Labeling images with a computer game,” in “Proceedings of the ACM
SIGCHI conference on Human factors in computing systems” 2004, pp. 319–326.
28

Waldinger, Fabian, “Peer effects in science: Evidence from the dismissal of scientists in Nazi Germany,”
The Review of Economic Studies, 2012, 79 (2), 838–861.

29

