NBER WORKING PAPER SERIES

PERFORMANCE STANDARDS IN NEED-BASED STUDENT AID
Judith Scott-Clayton
Lauren Schudde
Working Paper 22713
http://www.nber.org/papers/w22713

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
October 2016

The research reported here was supported by the Institute of Education Sciences, U.S.
Department of Education, through Grant R305C110011 to Teachers College, Columbia
University (Center for Analysis of Postsecondary Education and Employment). The opinions
expressed are those of the authors and do not represent views of the Institute or the U.S.
Department of Education. We are grateful to staff and administrators at the community college
system that facilitated data access and interpretation. For helpful feedback, we are grateful to
colleagues at the Community College Research Center, attendees of the NBER education
meetings, and seminar participants at the Federal Reserve Board, the University of Virginia, and
the University of Connecticut. Rina S.E. Park provided excellent research assistance. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National
Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
¬© 2016 by Judith Scott-Clayton and Lauren Schudde. All rights reserved. Short sections of text,
not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including ¬© notice, is given to the source.

Performance Standards in Need-Based Student Aid
Judith Scott-Clayton and Lauren Schudde
NBER Working Paper No. 22713
October 2016
JEL No. I22,I23,I28
ABSTRACT
College attendance is a risky investment. But students may not recognize when they are at risk for
failure, and financial aid introduces the possibility for moral hazard. Academic performance
standards can serve three roles in this context: signaling expectations for success, providing
incentives for increased student effort, and limiting financial losses. Such standards have existed
in federal need-based aid programs for nearly 40 years in the form of Satisfactory Academic
Progress (SAP) requirements, yet have received virtually no academic attention. In this paper, we
sketch a simple model to illustrate not only student responses to standards but also the tradeoffs
faced by a social planner weighing whether to set performance standards in the context of needbased aid. We then use regression discontinuity and difference-in-difference designs to examine
the consequences of SAP failure. In line with theoretical predictions, we find heterogeneous
effects in the short term, with negative impacts on persistence but positive effects on grades for
students who remain enrolled. After three years, the negative effects appear to dominate. Effects
on credits attempted are 2‚Äì3 times as large as effects on credits earned, suggesting that standards
increase the efficiency of aid expenditures. But it also appears to exacerbate inequality in higher
education by pushing out low-performing low-income students faster than their equally lowperforming, but higher-income peers.

Judith Scott-Clayton
Teachers College
Columbia University
525 W.120th Street, Box 174
New York, NY 10027
and NBER
scott-clayton@tc.columbia.edu
Lauren Schudde
Department of Educational Administration
George Sanchez Building
The University of Texas at Austin
Austin, TX 78712
lts2133@tc.columbia.edu

I.

Introduction
College is a risky investment. Not only do prospective students face uncertainty about

their likely income conditional on graduation (Wiswall & Zafar, 2015), but they also face
significant uncertainty regarding how long they will persist and whether or not they will actually
graduate. Uncertainty about completion exists because college is an experience good;
prospective students may not discover their own tastes and abilities for college-level work until
they try it (Altonji, 1993; Manski, 1989). College dropout is in part a manifestation of this
uncertainty; scholars have long noted that some level of dropout is to be expected even in a wellfunctioning postsecondary system, as students learn more about their preferences and abilities
(Fischer, 1987; Manski, 1988, 1989; Manski & Wise, 1983).
Even after enrolling and beginning this learning process, however, students may make
suboptimal decisions about dropout for a number of reasons. Large and growing gaps in
educational attainment by family income, which remain even after controlling for prior measures
of ability, are consistent with credit constraints leading some students to end their schooling too
soon (Bailey & Dynarski, 2011; Belley & Lochner, 2007). Moreover, students, like other people,
may overly discount future payoffs when costly actions are required in the present (Lavecchia,
Liu, & Oreopoulos, 2014). Finally, persistence and completion may generate positive social
externalities in addition to the private benefits valued by students. Any of these factors could
lead to suboptimal college enrollment and persistence, and form the justification for substantial
public subsidies in higher education.
Far less attention has been devoted to the question of whether some low-performing
students might persist longer than they should. Yet the ubiquitous nature of academic
performance standards at postsecondary institutions‚Äîwhich typically require students to

1

maintain a minimum grade point average (GPA) or risk being placed on probation and eventually
dismissed‚Äîsuggests a widespread belief that they are needed. Students may not be wellinformed regarding institutional expectations for graduation and how their performance
compares to it (Scott-Clayton, 2013). Alternatively, they may be informed procrastinators,
always planning to increase effort next semester rather than in the present. In addition, evidence
suggests that students are slow to update beliefs about completion probability even after a period
of poor performance, because they underestimate the role of persistent rather than transitory
factors (Stinebrickner & Stinebrickner, 2012). Finally, the same public subsidies intended to
promote enrollment and completion also introduce the possibility of moral hazard. Some
students might persist in the ‚Äúcollege experiment‚Äù beyond the socially optimal point.
Academic performance standards can serve three roles in this context: sending a clear
signal about institutional expectations for success, providing incentives for increased student
effort early in college, and limiting financial losses (potentially for overoptimistic students
themselves, as well as for taxpayers). Academic performance standards of one form or another
apply to all students, regardless of financial aid status. But the stakes are arguably highest in the
context of financial aid policy, where performance standards are commonplace even in purely
need-based programs.
In this paper, we focus on the consequences of Satisfactory Academic Progress (SAP)
requirements as established by the federal student aid system (‚ÄúTitle IV‚Äù aid). While eligibility
for Pell Grants and student loans is initially based purely on financial need, recipients must meet
SAP standards in order to continue receiving aid (state and institutional need-based programs
often follow the federal rules as well). Institutions have flexibility regarding how they define and
enforce SAP, but they commonly require students to maintain a cumulative GPA of 2.0 or higher

2

and to complete at least two-thirds of the course credits that they attempt. Meeting SAP is a nontrivial hurdle for many students: in a recent related paper, we find that 25‚Äì40 percent of first-year
Pell recipients at public institutions have performance low enough to place them at risk of losing
financial aid, representing hundreds of thousands to over a million college entrants each year
(Schudde & Scott-Clayton, 2016).
Though minimum performance standards have existed in the need-based federal student
aid programs for nearly 40 years‚Äîand have become increasingly strict in recent years‚Äîwe have
found very little academic research specifically relating to their consequences, either theoretical
or empirical. B√©nabou and Tirole (2000) provide a model of how students react to performance
standards in general, but do not consider interactions with financial aid policy. While there is a
substantial literature on the impacts of performance-based scholarships, such scholarships
typically focus on GPA thresholds well above 2.0 (Carruthers & Ozek, 2014; Cornwell, Lee, &
Mustard, 2005; Cornwell, Mustard, & Sridhar, 2006; Dynarski, 2008; Scott-Clayton, 2011) and
examine the marginal effect of receiving extra aid rather than the effect of losing foundational
need-based assistance (Angrist, Lang, & Oreopoulos, 2009; Patel & Valenzuela, 2013; RichburgHayes et al., 2009).
The most closely related empirical work is a study by Lindo, Sanders, and Oreopoulos
(2010) of the causal effects of academic probation for students (regardless of financial aid status)
at one large baccalaureate institution in Canada. Using regression discontinuity, the authors
compare persistence, grades, and graduation rates for students just above and below the first-year
GPA threshold for placement onto academic probation. The authors find that being placed on
probation induces some students to drop out but increases GPA for those that return, with a net
negative impact on graduation for students near the cutoff. A recent paper by Casey, Cline, Ost,

3

and Qureshi (2015) replicates these findings using data from a U.S. public four-year institution
but finds that some of the increase in GPA is due to strategic course selection. Finally, our own
recent work documenting the prevalence of SAP failure (Schudde & Scott-Clayton, 2016)
includes an analysis of the effects of standards for Pell recipients in a different state than
examined here, with our preferred strategy suggesting that the increases in dropout may be larger
for Pell recipients than non-recipients. Taken together, these prior studies provide support for the
B√©nabou and Tirole model of student behavior, yet provide little guidance for evaluating
standards in the context of financial aid policy.
Our paper makes three contributions. First, we sketch out a simple model that not only
illustrates students‚Äô likely responses to performance standards but also highlights the tradeoffs
faced by a social planner weighing whether to set performance standards in the context of needbased aid. Our framework draws upon elements of Manski‚Äôs (1988, 1989) ‚Äúschooling as
experimentation‚Äù model as well as B√©nabou and Tirole‚Äôs (2000, 2002) model of student behavior
under performance standards. The model has three periods: an evaluation period, a warning
period, and an enforcement period. Second, we use this framework to guide an empirical
examination of the consequences of minimum performance standards for a high-risk population:
community college entrants. Utilizing administrative records on aid recipients at more than 20
community colleges in one state, we apply a regression discontinuity (RD) design similar to that
used in prior work. However, our preferred estimates use a difference-in-difference (DID) design
which uses unaided students as a control group to net out any effects of academic probation in
general (that is, any effects of being below the performance threshold that affect all students, not
just those receiving financial aid) and also allows us to estimate effects for students further away
from the performance threshold. While the RD results are causally cleaner, the DID estimates are

4

of greater policy relevance in terms of determining whether SAP requirements are effective
overall. Finally, we examine a broader range of outcomes‚Äîincluding measures of student labor
supply and the estimated value of foregone financial aid‚Äîthat provide a fuller picture of the
costs and benefits of the policy.
In line with theoretical predictions and consistent with Lindo, Sanders, and Oreopoulos
(2010), we find a clear pattern of heterogeneous effects of SAP failure during the warning period
(when students should realize they are at risk but have not yet faced consequences), with students
just below the GPA threshold being less likely to return in the second year, but with positive
effects on grades for students who do return. Also consistent with our model, we find that
discouragement effects are larger for students further below the threshold while encouragement
effects appear larger for those nearest the threshold.
Despite these heterogeneous effects in the short term, negative effects appear to dominate
by the end of our three-year follow-up window, after the point at which consequences of
continued SAP failure would have been enforced. Both the RD and DID specifications indicate
significant reductions in the likelihood that students remain enrolled by that point, with no
improvement in cumulative GPA. Interestingly, we find significant negative effects on credits
attempted, but much smaller effects on credits earned, suggesting that the marginal credit no
longer attempted had a low probability of being completed. Effects on degree completion are
difficult to interpret because of potential floor effects, but the most consistent degree result is a
small negative effect on certificate completion. We find no effect on student earnings in most
specifications, though coefficients are generally negative.
Evaluating SAP policy as a whole requires weighing the value of human capital foregone
against the cost of continuing to subsidize enrollment. While a complete assessment of net

5

benefits is beyond the scope of this paper, if we assume that credits attempted but not completed
have no value, then SAP does appear to improve the efficiency of aid dollars. The reduction of
three credits attempted translates into a reduction of only one credit completed on average, yet
saves at least $444‚Äì$539 in financial aid expenditures per student. 2 Still, the small negative
effects on certification completion may be cause for concern, since such credentials have been
found to increase individuals‚Äô post-enrollment earnings (Belfield, Liu, & Trimble, 2014;
Jacobson & Mokher, 2009; Jepsen, Troske, & Coomes, 2014; Xu & Trimble, 2014). Moreover,
policymakers may be concerned about the equity implications, as our results clearly indicate that
aid recipients with low GPAs leave college more quickly than their similar but financially
unassisted peers.
The remainder of the paper proceeds as follows: Section II provides additional
background on SAP policy. Section III introduces the theoretical framework and key predictions.
Section IV describes our data, Section V describes our empirical strategy, and Section VI
presents our main results. Section VII concludes with a discussion of policy implications and
unanswered questions.
II.

Policy background
SAP regulations have been a part of federal student aid since 1976 when an amendment

to the Higher Education Act of 1965 stipulated that students must demonstrate ‚Äúsatisfactory
progress‚Äù toward a degree in order to continue receiving aid (Bennett & Grothe, 1982). The
regulations give institutions flexibility regarding how they define SAP, though in practice it
2

For comparison, note that a credit is 1/24th of a full-time school year, and in-state tuition in this system was less
than $115/credit over this time period. This is a conservative estimate that assumes students who continue to
enroll continue to receive aid (when in fact some students who remain enrolled may have lost their aid eligibility).
We do not have measures of actual aid received beyond the first year. Instead, we use actual first-year aid receipt
to estimate that students in our sample would qualify for approximately $140‚Äì170 in aid per credit attempted in
subsequent years.

6

appears typical for institutions to require a cumulative grade point average (GPA) of 2.0 or
higher and completion of at least two thirds of the course credits that students attempt (Schudde
& Scott-Clayton, 2016). Our analysis will focus on the GPA criterion, as it is highly correlated
with credit completion percentage in our sample (œÅ = 0.79), but much more continuously
distributed.
SAP policy applies to federal Pell Grant recipients, student loan borrowers, and workstudy participants; and state and institutional need-based aid programs often piggyback their
minimum performance rules on the federal standards. 3 While eligibility for federal aid is initially
based purely on financial need, recipients must meet satisfactory academic progress (SAP)
requirements in order to remain eligible beyond the first year. The federal Pell Grant is the single
largest source of need-based financial aid in the country and the dominant form of aid received at
most community colleges, both in terms of frequency and magnitude of awards (Baum & Payea,
2013). Thus, while our empirical analysis groups together students receiving any type of aid, 4 for
interpretation purposes we recognize that most financial aid recipients in our sample are Pell
recipients. For example, among the aid recipients in our community college sample, 70 percent
received Pell, compared to 44 percent receiving state grants and 20 percent receiving loans.
Average amounts among all aid recipients were $2,385 in Pell compared to just $407 in state
grants and $352 in student loans.
Until recently, institutions have had a great deal of flexibility in terms of how frequently
they evaluate SAP and thus how quickly consequences are enforced. Prior to 2011, institutions
3

In the state we examine, need-based state aid is the second largest source of student grants, and aid
administrators confirmed that the state programs follow the same rules used for federal SAP.
4
A prior version of this paper focused exclusively on Pell recipients and generated a substantively identical pattern
of results; however, moving other aid recipients from the control group to the treated group helps improve power
and is most consistent with how the policy is implemented in practice.

7

were only bound to enforce SAP at the end of the second year; at that point, students whose GPA
fell below the threshold for graduation could no longer receive aid. 5 Many institutions
nonetheless opted to evaluate SAP more frequently, often by the end of the first year, giving
students another full year under either ‚Äúwarning‚Äù or ‚Äúprobation‚Äù status to try to meet the
standard. A ‚Äúwarning‚Äù generally means a student is notified of their precarious status but no
formal action is taken; ‚Äúprobation‚Äù means that the student failed to meet SAP requirements
during the warning period and filed an appeal explaining their poor performance in order to
continue receiving aid‚Äîthough these terms were not always consistently applied.
SAP standards layer on top of ‚Äúacademic good standing‚Äù policies that apply to all
students regardless of aid status. Though community colleges are ‚Äúopen access‚Äù institutions,
meaning that any individual with a high school diploma or its equivalent may enroll initially,
they still have minimum academic standards that students must meet in order to remain enrolled
and to earn a degree. In the state community college system (SCCS) that we examine, a 2.0
cumulative GPA, or a C average, is required to earn a credential (this appears to be a fairly
typical graduation standard at public institutions nationally). Students who fall below a 2.0 in any
semester are placed on academic warning status and a notification will appear on their
transcript. 6 However, besides warning students that they are not on track to graduate, academic
warning in this system has little durable consequence except for students on financial aid (who,
prior to 2011, would lose aid after the end of two years). More immediate consequences are

5

Even defining the ‚Äúend of the second year‚Äù is not particularly straightforward in the context of community
colleges, where many students attend part-time. We found many policies were defined in terms of credits
attempted, where 48+ would correspond to the end of the second year.
6
We find some conflicting information between policy manuals produced at the system level, which suggest
students below 2.0 are placed on academic warning, and catalogs at the college level, at least one of which
describes students as being in ‚Äúgood standing‚Äù as long as they maintain a 1.5 GPA (though they will not be able to
graduate).

8

reserved for students who fall below a 1.5 GPA: these students may be immediately placed on
probation and required to file an appeal in order to continue enrolling or receiving aid. Still, the
system‚Äôs guidelines emphasize that even if an academic warning does not itself lead to dismissal,
students will not be able to earn a degree with a less than 2.0 GPA. 7
Our review of college catalogs describing SAP and academic good standing policies
suggests performance standards may be less than perfectly transparent to students; indeed, we
found them challenging to decipher ourselves. In the years pertaining to our sample, policies
appear to vary somewhat across colleges, and the thresholds and timelines for SAP evaluation do
not always seem to correspond to the thresholds and timelines for broader institutional academic
standards. We will return to this complexity in our discussion section. Nonetheless, for the
period and sample under consideration, all students with below a 2.0 received at least some
notification that they were at academic risk by the end of their first year; but students were
unlikely to face binding consequences (such as financial aid loss or dismissal) before the end of
their second year unless they fell below a 1.5 GPA.
III.

Theoretical framework
Basic model of performance standards. B√©nabou and Tirole (2000) present a simple

principal‚Äìagent model in which agents choose between shirking, a low-effort/low-benefit task,
and a high-effort/high-benefit task. Lindo, Sanders, and Oreopoulos (2010) use an even simpler
version of this model in their analysis of academic probation, focusing on the agent‚Äôs decision.
Because of our interest in optimal policy, we utilize B√©nabou and Tirole‚Äôs original model and
examine its predictions after incorporating financial aid.

7

Information on institutional academic good standing and SAP policies are taken from course catalogs for years
prior to 2011.

9

In the original model, individuals choose one of these: shirking, which yields no costs or
benefits; a low-effort/low-benefit task (Task 1) with a private benefit of V1 and a private effort
cost of c1; or a high-effort/high-benefit task (Task 2) with private benefit of V2 and a private
effort cost of c2. The principal bears no costs but receives a benefit of W1 from Task 1 and a
benefit of W2 for Task 2. In other words, the ranking of costs and benefits is:
0 < ùëâ1 < ùëâ2 ,

0 < ùëä1 < ùëä2 , ùëéùëõùëë

0 < ùëê1 < ùëê2

(1)

Ability is conceptualized as an exogenously determined probability of success at either
task, Œ∏, which for now we assume the agent knows but the principal does not. To ensure that the
problem does not degenerate and that at least some individuals choose each option, the following
assumption is made (intuitively, this assumes that marginal cost of Task 2 relative to Task 1 is
less than the marginal benefit, but the ratio of marginal costs to marginal benefits is higher for
Task 2 than for Task 1):
ùëê1

ùëâ1

<

ùëê2 ‚àíùëê1

ùëâ2 ‚àíùëâ1

<1

(2)

The agent chooses the course of action that maximizes her individual outcome:

So the individual will choose:

max{0, ùúÉùëâ1 ‚àí ùëê1 , ùúÉùëâ2 ‚àí ùëê2 }
ùë°ùëú ùë†‚Ñéùëñùëüùëò: ùëñùëì 0 ‚â§ ùúÉ <
ùëê

(3)

ùëê1
‚â° ùúÉ1
ùëâ1

ùëê ‚àíùëê

ùëáùëéùë†ùëò 1: ùëñùëì ùúÉ1 ‚â° ùëâ1 ‚â§ ùúÉ < ùëâ2 ‚àíùëâ1 ‚â° ùúÉ2
1

ùëáùëéùë†ùëò 2: ùëñùëì

2

ùëê2 ‚àí ùëê1
‚â° ùúÉ2 ‚â§ ùúÉ
ùëâ2 ‚àí ùëâ1

10

1

(4)

If the principal removes the low-effort/low-benefit option, individuals who would
otherwise have chosen that option now are forced to choose between shirking or increasing their
effort. Now, individuals will shirk only if:
ùëê

ùúÉ < ùëâ2 ‚â° ùúÉ ‚àó

(5)

2

The key insight of this model, which Lindo et al. (2010) emphasize, is the heterogeneous impact
that results when performance standards are applied. Higher ability individuals are motivated to
work harder, while lower ability individuals are discouraged and drop out. See Figure 1 for a
graphical illustration.
For our analysis, we are also interested in the principal‚Äôs perspective. Imposing a
standard is only worthwhile for the principal if the increase in value coming from those induced
to work harder exceeds the loss of value attributable to those induced to shirk. This depends not
only on the parameters discussed above but also on the distribution of ability f(Œ∏) in the
population (i.e., how many individuals are in the affected ranges):
ùúÉ‚àó

ùúÉ

ùëÜ(ùúÉ1 , ùúÉ2 ) = ÔøΩ‚à´ùúÉ‚àó2 ùúÉùëì(ùúÉ)ùëëùúÉÔøΩ (ùëä2 ‚àí ùëä1 ) ‚àí ÔøΩ‚à´ùúÉ ùúÉùëì(ùúÉ)ùëëùúÉÔøΩ (ùëä1 ) > 0
1

(6)

Introducing financial aid without standards. The Pell Grant and other scholarship
programs provide another means by which a social planner could encourage greater investment
in education (either because of perceived externalities or because individuals systematically
underestimate the true private benefit). If the social planner provides an upfront scholarship, P,
based on enrollment but not outcomes (i.e., available to those who choose either Task 1 or Task
2 in the model), it is straightforward to show that this will result in a new ùúÉ1ùëÉ < ùúÉ1 , but ùúÉ2ùëÉ = ùúÉ2

(see Figure 2). In other words, the scholarship induces more individuals into the low-effort/lowbenefit Task 1, but no more individuals into Task 2. Manski (1988) also highlights a similar
conclusion in his analysis of the effects of upfront, non-contingent enrollment subsidies: these
11

‚Äúcan induce students to change their enrollment decisions but cannot induce changes in
completion decisions‚Äù (p. 13). As such, enrollment subsidies cannot guarantee the socially
optimal outcome, but they may still be desirable (relative to no aid with no standards) if:
ÔøΩ
ùúÉ

ùúÉ

ÔøΩ‚à´ùúÉùëÉ1 ùúÉùëì(ùúÉ)ùëëùúÉÔøΩ (ùëä1 ) ‚àí ÔøΩ‚à´ùúÉùëÉ ùëì(ùúÉ)ùëëùúÉÔøΩ (ùëÉ) > 0
1

1

(7)

In essence, the new benefits attributable from those induced to enroll with low effort must more
than cover the costs of providing the scholarship to all those who enroll, including potentially
many individuals whose enrollment and effort are unaffected by the scholarship. 8
Adding performance standards in the context of financial aid. If the social planner can
forbid low-effort enrollment in the context of financial aid, the threshold value for choosing the
high-effort option declines to:
ùëê2 ‚àíùëÉ
ùëâ2

‚â° ùúÉ ‚àóùëÉ < ùúÉ ‚àó

(8)

Relative to providing a given P without performance standards, this policy is worthwhile if:
ùúÉ

ùúÉ‚àóùëÉ

2
ÔøΩ‚à´ùúÉ‚àóùëÉ
ùúÉùëì(ùúÉ)ùëëùúÉÔøΩ (ùëä2 ‚àí ùëä1 ) ‚àí ÔøΩ‚à´ùúÉùëÉ (ùúÉùëä1 ‚àí ùëÉ)ùëì(ùúÉ)ùëëùúÉÔøΩ > 0
1

(9)

See Figure 2 for a graphical illustration. If W1, the social value of the forbidden low-effort
option, is lower than the value of the scholarship P, then aid-with-standards is unambiguously
better than aid-without-standards. Assuming that there is some level of low effort that generates
social value less than P, then performance standards are always desirable; the only question is
where to set the dividing line between the acceptable W2 and the unacceptable W1. Intuitively,
the optimal dividing line will depend upon the relative benefits of high-effort versus low-effort

8

If we consider Task 1 to be enrolling but dropping out and Task 2 to be persisting to completion, it is worth noting
that a grant program could be worthwhile even if it increases dropout rates. Manski emphasizes that ‚Äúdropout
statistics per se carry no normative message‚Ä¶among [some] students, society prefers a higher dropout rate than
that generated privately‚Äù (1989, p. 310, italics in original).

12

enrollment, the relative benefits of low-effort enrollment versus no enrollment, the shape of the
ability distribution, and the magnitude of the scholarship. 9
Timing and other unresolved issues. One ambiguity in the B√©nabou & Tirole (2000)
model is the timing of assessment and enforcement, and why we would ever observe students,
even in the absence of aid, enrolling only to perform the ‚Äúprohibited‚Äù low-effort task. Manski‚Äôs
(1989) model of education as experimentation is useful here: students themselves may not know
their ability until they enroll. We can thus think of the model being spread over three periods,
with the first period being an evaluation period in which students learn about their ability, the
second period being a warning period (in which individuals know their ability but still receive
unconditional aid), and the third period being an enforcement period (in which aid recipients
who do not improve above standard lose their aid). These periods are reasonably well-defined for
aid recipients in our sample (as the first, second, and third year of follow-up). Note, however,
that unaided students in our sample do not really face a defined enforcement period: unlike the
probation policies examined by Lindo et al. (2010) and Casey et al. (2015), unaided students can
continue taking classes under a warning status indefinitely, as long as their GPA does not fall
below 1.5 (even though they will be unable to graduate with a GPA below 2.0).
The model sketched above highlights how the interests of policymakers and students may
sometimes diverge, and how performance standards may be socially desirable even while they
necessarily reduce utility for at least some students. However, it is possible to imagine scenarios
under which students themselves benefit from the enforcement of standards. For example, if
students are slow to update beliefs about their own ability (as found by Stinebrickner &

9

Note that it is not obvious that larger scholarships should necessarily warrant higher standards, because they
enter into equation (9) not only directly but also indirectly: both the positively and negatively affected ranges shift
lower in the ability distribution.

13

Stinebrickner 2012), they may actually benefit from leaving school earlier: they may reallocate
their time from unproductive studies to more productive work in the labor market. This suggests
examining changes in students‚Äô labor supply during the warning and enforcement period.
Implications. In the context of SAP policy, we can think of earning less than a 2.0 GPA
in college as the low-effort option. Our empirical analysis will not correspond directly to the
model above, since individuals have a continuum of effort levels to choose from, and we do not
have direct measures of ability or of the benefits associated with different effort levels.
Nonetheless, the model suggests the following implications:
1. Being placed on warning status will induce some individuals to drop out of school, while
those who return will increase effort (as shown in Lindo et al. 2010).
2. The discouragement (dropout) effects will be concentrated among those lower in the
ability distribution, while the encouragement (improved GPA) effects should be
concentrated among those near the threshold.
3. All else equal, encouragement effects of receiving a warning will be bigger in the
presence of financial aid (evident by comparing affected regions of the distribution in
Figure 2).
4. All else equal, whether discouragement effects are bigger or smaller for the aid-eligible
population during the warning period is ambiguous. 10
5. In the enforcement period, effects of failing the standard for aided students should be
unambiguously more negative than for unaided students, as aided students experience the
loss of aid.

10

Aid induces some low-ability students to enroll who are likely to drop out once standards are introduced;
however, some moderate-ability students who would have enrolled even without aid are induced into the higheffort task when standards are introduced.

14

In addition to these hypotheses, the model also provides some insight regarding the key
outcomes to consider from either a student or a social planner perspective. While persistence
rates and GPA may be useful for testing the behavioral implications of the model, they are not of
direct use in terms of evaluating the net benefits of the policy. For that purpose, we will consider
summary measures of human capital accumulated, including total credits completed, cumulative
GPA at the end of the follow-up period, and degree/transfer outcomes. We can then weigh the
value of any impacts on human capital attained against the impacts on estimated scholarship
outlays as well as on students‚Äô foregone earnings.
IV.

Data
We utilize de-identified state administrative data on first-time students who entered one

of more than 20 community colleges in a single eastern state between 2004 and 2010, and follow
all students‚Äô outcomes for three years after initial entry (unless otherwise noted). Only fall
entrants are included in the data; however, the community college system classifies a small
number of students who begin coursework in the summer as fall entrants. We focus on students
who enrolled full-time in their first semester to ensure enough courses are attempted to compute
a reliable first-year GPA (an additional justification is that performance standards may not be
implemented until students have attempted at least 12 credits). The data include limited
demographic information, detailed transcripts, placement test scores, information on financial aid
received in the first year, and credentials earned. The state system also links these institutional
data to two additional databases: the National Student Clearinghouse (NSC), which captures
enrollment and credentials at institutions outside the state community college system, and
individual Unemployment Insurance (UI) records, which indicate quarterly employment and

15

earnings. We use the UI data to construct measures of student labor supply during the second and
third years post-entry, to capture potential earnings foregone by students who remain enrolled.
One thing the data do not include is any explicit measure of academic warning or
probation. Instead, we infer who is subject to these labels based upon students‚Äô cumulative grade
point averages (GPAs). It is possible that not all students below the 2.0 GPA threshold were
placed on a warning status, and that some students above the threshold were. 11 To the extent this
occurs, it will tend to bias our estimated effects towards zero. Similarly, we are not able to
explicitly identify students who lost Pell eligibility as a result of SAP failure; while this would be
interesting, it is not necessary for our analysis, which includes the initial threat of losing Pell
(based on first-year GPA) as a key aspect of treatment rather than only considering students who
experience the actual loss of aid.
Table 1 describes our full sample and provides mean outcome levels for all full-time
entrants to this state system as well as for reference groups relevant to our RD strategy (aid
recipients and non-recipients +/- 0.25 around the cutoff) and DID strategy (recipients and nonrecipients with a GPA between 1.0 and 2.0).
V.

Empirical strategy
Because all students in our sample face performance standards by the end of their first

year, and about 54 percent of our sample receives financial aid in their first year, we can
compare the effects of introducing performance standards for aided and unaided students. The
data suggest two possible approaches: regression discontinuity (RD) analysis for students just
above and below the GPA threshold to remain in good standing, and/or a difference-in-difference
11

As noted above, our analysis focuses on the GPA criterion, as it is highly correlated with credit completion
percentage in our sample (œÅ = 0.79) but much more continuously distributed. Thus, some students above 2.0 may
still have received a warning. Students below the threshold should have received a warning, though there is always
the possibility of some noise in the GPA calculation.

16

(DID) analysis comparing students above and below this threshold for aid recipients versus nonrecipients.
Regression discontinuity. The only assumption required is that the underlying
relationship between first-year GPA and the outcome of interest (in the absence of the
performance standard) is continuous through the threshold. Following Imbens and Lemieux
(2008), we use a local linear specification with the bandwidth restricted to a narrow range around
the 2.0 threshold. We focus on a bandwidth of 0.5, guided both by graphical plots as well as by
the concern that other school policies may come into play below 1.5 or above 2.5. Still, we test
for sensitivity to bandwidth selection by using the preferred bandwidth (0.5), half this bandwidth
(0.25), and twice this bandwidth (1.0). For the 1.0 bandwidth, we use a more flexible local
quadratic specification (though it makes little difference if we stick with a local linear model).
The basic model, which we run on the sample restricted to aid recipients within the given
bandwidth, takes the form:
Yi = B0 + Œ≤1 (Belowi) + Œ≤2 (GPADistancei ‚àó Belowi) + Œ≤3 (GPADistancei ‚àó Abovei)

(10)

+ CollegeFE + CohortFE + Œ≤nXi + Œµi
where Yi represents the outcome for student i, and Œ≤1 is the estimate of the effect of falling below
the SAP cutoff on the outcome. CollegeFE is a vector of institutional fixed effects (entered as a
set of dummy variables indicating the institution initially attended, with one institution
excluded), important because the financial aid officers responsible for enforcing performance
standards are nested within institutions. CohortFE is a vector of cohort fixed effects, a necessary
inclusion because of potential changes in the student population over time. Xi represents a vector
of individual-level covariates including race, gender, age at initial enrollment, whether students

17

were exempt from placement testing in reading and math (indicators of prior achievement),
placement test scores for those who were not exempt, whether the student was predicted to be
assigned to remedial coursework, whether the student had previously enrolled as a high school
student, and dummies for the student‚Äôs degree intent at entry (occupational associate‚Äôs degree,
occupational certificate, or academic associate‚Äôs degree). 12 We also include additional controls to
capture elements of the student‚Äôs first year experience, including total credits attempted in the
first year, whether the student worked for pay, and how much the student earned during the
school year. 13 We test the sensitivity of our results to models with and without covariates.
Grade heaping. Using GPAs as the running variable in an RD introduces an additional
challenge. Given the nature of the state‚Äôs grading system‚Äîin which only whole letter grades are
awarded‚Äîwe find ‚Äúheaping‚Äù in whole number GPAs across the distribution, including at our
cutoff value of 2.0. Moreover, the fewer credits a student has attempted, all else equal, the more
likely they are to have a whole-number GPA. This problem, however, is surmountable. 14
Excluding students with precisely a 2.0, a McCrary test (2008) indicates the distribution of
cumulative GPAs is continuous around the threshold. This suggests that the observed heaping is
due to grading policy and not to students precisely manipulating whether they fall above or
below the threshold. Thus, following the recommendations of Barreca, Lindo, and Waddell
(2011; see also Barreca, Guldi, Lindo, & Waddell, 2011), we rely on ‚Äúdonut-RD‚Äù estimates,
dropping observations with precisely a 2.0 GPA. Figure 3 shows the distribution of first-year
GPAs for aid recipients and non-recipients before removing whole number GPAs.
12

Note we do not include controls for family income or students‚Äô dependency status because these measures are
missing for the 45 percent of students who did not file a FAFSA, including 75 percent of Pell non-recipients.
13
These additional first-year controls make virtually no difference to the point estimates. Results available on
request.
14
Lindo, Sanders, and Oreopoulos (2010), Schudde and Scott-Clayton (2016), and Casey et al. (2015) all find similar
patterns in their samples and implement similar donut-RD designs.

18

RD-Difference-in-Difference. The RD estimates capture the total effect of performance
standards for aid recipients, including general standards at the institution as well as the effects of
SAP policy specifically. To directly test whether the estimated effects of performance standards
are larger for aid recipients, we run the following pooled regression:
(11) Yi = B0 + Œ≤1 (Belowi* Aidi) + Œ≤2 (Belowi) + Œ≤3 (Aidi) + Œ≤4(GPADistancei ‚àó Belowi*
Aidi) + Œ≤5 (GPADistancei ‚àó Abovei* Aidi) + Œ≤6(GPADistancei ‚àó Belowi* NoAidi) +
Œ≤7 (GPADistancei ‚àó Abovei* NoAidi) + CollegeFE + CohortFE + Œ≤nXi + Œµi
The Œ≤1 in this regression provides an estimate of the difference in the two RD estimates; we will
refer to this as the RD-DID estimate. A drawback of this approach is that it is somewhat weakly
powered.
Difference-in-difference. The conceptual model suggests that encouragement effects
should be strongest for those nearer to this threshold, while discouragement effects should be
larger for individuals further below the threshold. Unfortunately, the RD is ill-equipped to test
this important implication, because the RD estimates effects only for those right at the cutoff. 15
The difference-in-difference allows us to examine the effect of performance standards for Pell
recipients, over and above the effects for non-recipients, for a wider range of students affected by
the policy. It also provides much greater power to detect effects than either the RD or the RDDID. The cost of obtaining this broader estimate is that we must make stronger assumptions
about the relationship between first-year GPA and subsequent outcomes, namely by assuming
that the relationship between GPADistance and potential outcomes is the same for aided and
15

Lindo et al. (2010) examine subgroups by HSGPA to get at this question, but are necessarily limited to the
variation in HSGPA that exists for students around the specified college GPA performance threshold. Given the
generally strong correlation between high school and college GPAs, such an analysis may be particularly
susceptible to issues of measurement error.

19

non-aided recipients. We still allow for a very flexible relationship between GPA and potential
outcomes by replacing the Below and GPADistance interactions from equation (11) with a set of
fixed effects for GPA bins (with width of 0.05):
(12) Yi = B0 + Œ≤1 (Belowi* Aidi) + Œ≤2 (GPABin05i) + Œ≤3 (Aidi) + CollegeFE + CohortFE +
Œ≤nXi + Œµi
For the difference-in-difference, we continue to limit the bandwidth above the cutoff to
+0.5 GPA points; however, we vary the bandwidth below the cutoff from -0.15 to -1.0. This
allows us to test our hypothesis that discouragement effects will be bigger in the DID relative to
the RD-DID as we include students further below the threshold, while encouragement effects
may be smaller.
VI.

Main results
Graphical analysis and covariate checks. Figures 4‚Äì7 show average outcomes by first-

year GPA in bins of 0.05 and with the size of the circles reflecting numbers of observations. In
several of these graphs, the outcomes of aid recipients versus non-recipients appears to be more
similar above the cutoff than below the cutoff, although it is difficult to discern sharp
discontinuities visually. For example, reenrollment rates and credits attempted in Year 2 (Figure
4) and Year 3 (Figure 5) separately, as well as measured cumulatively after 3 years (Figure 6),
appear to fall more for aided students below the cutoff than for unaided students. In the bottom
right panel of Figure 6 (showing whether students were still enrolled at the end of the follow-up),
a clear negative discontinuity is evident for aided students but not unaided students.
In Table 2, we test for significant differences in pretreatment observable covariates under
each of our main estimation strategies. The three columns of this table present ‚Äúimpact‚Äù

20

estimates obtained by running equations (10), (11), and (12) above with the relevant background
characteristic as the dependent variable. While we find no differences for most covariates, we
find small positive difference in placement test scores for treated students in each specification.
However, only about two-thirds of individuals have these placement test scores and the
differences are small in magnitude (0.1‚Äì0.2 of a standard deviation). Gender, race, and ever dualenrolled also emerge as significant in at least one specification. Importantly, despite these small
differences in covariates, we will show that it makes virtually no difference to our point
estimates whether or not they are included as controls.
RD and RD-DID results. Table 3 provides the results from the RD and RD-DID
specifications for several aspects of student behavior that our model suggests should be affected,
measured separately during the fall of Year 2 (the first warning period) and Year 3 (the
enforcement period when individuals could be actually prohibited from reenrolling or receiving
aid) as well as cumulatively. The first column of the table shows our preferred RD specification,
while the subsequent rows show alternative specifications. The final column shows the RD-DID
results.
We first examine reenrollment rates, term GPAs, and credits in the fall of Year 2. Note
that for dropouts, term GPAs are imputed to the last known cumulative GPA (this ensures that
impacts only come from those who reenroll, without introducing attrition bias). For aid recipients
near the cutoff, failing SAP appears to have little effect on reenrollment decisions, or on
continuous measures of credits attempted and completed (coefficients are consistently negative
but generally very small in magnitude). However, we do find a significant increase of 0.07 GPA
points (p = .06) in term GPA in the fall of the second year. Because this increase can only come
from the 68 percent of students who reenroll, this implies a 0.10 GPA improvement among

21

students who reenroll. This pattern of findings is quite stable across the RD specifications,
though more pronounced in the narrow-bandwidth estimation. Notably, none of the Year 2
estimates are statistically significant in the RD-DID, although the signs are generally consistent.
However, by the fall of Year 3, any short-term GPA effects have washed out and more
negative effects begin to appear. Most notably, we find a significant reduction of about 1.5
credits attempted (about a 15 percent reduction) and about 0.8 credits completed (a 12 percent
reduction). This is consistent with some students facing actual loss of financial aid during this
year. The negative effects grow over this year to a large, highly significant 8 percentage point
reduction in the likelihood of still being enrolled in the spring of Year 3, consistent across all
specifications. Cumulatively, aid recipients just below the GPA cutoff in Year 1 attempt 2.2
fewer credit and complete 1.4 fewer credits over three years than their counterparts just above
the cutoff. We also find reductions of 2‚Äì3 percentage points in both certificate and associate‚Äôs
degree completion, though statistical significance varies across specification.
Finally, we examine school year earnings (measured in calendar Q4 and Q1
corresponding to the relevant academic year, with the cumulative measure also including Q2 and
Q3 between the second and third academic year) as a measure of the possible opportunity cost of
enrollment (i.e., do students who drop out go back to working instead?). Effects here are positive
in the preferred RD specification but not statistically significant in any specification, and even
the sign varies across specification. The standard errors are quite large (often equivalent to about
10 percent of mean earnings), and thus we are unable to conclude much about students‚Äô labor
supply from these results.
Difference-in-difference results. As discussed in Section III, a drawback of both the RD
and the RD-DID is that effects are estimated only for students near the 2.0 threshold. Yet our

22

model clearly predicts heterogeneous effects by ability. We expect encouragement effects to be
strongest for students just below the threshold, while we expect discouragement effects to grow
as we move further down the GPA distribution. Our DID specification enables us to capture the
effects of SAP policy for a wider range of students affected. Our results are shown in Table 5,
which varies the range of observations included below the threshold while keeping the
bandwidth above the cutoff fixed at 0.5. Results for specifications with no covariates are shown
in Table A1, and are virtually identical.
Indeed, the pattern of impacts on enrollment in fall of Year 2 suggests that
discouragement effects are larger for students further below the cutoff. The estimated 5
percentage point decline in reenrollment for the 0.15 bandwidth is statistically significant but
grows to an 8 percentage point decline for the 1.0 bandwidth. Conversely, the 0.09 GPA point
improvement for the 0.15 bandwidth falls to an insignificant 0.03 points for the 1.0 bandwidth.
This pattern still shows, though much more weakly, in fall of Year 3. On the other hand, credits
attempted and completed do not appear to vary much by bandwidth, either in the short or longer
term. This may be because students near the margin may reduce their course loads in order to
improve their GPAs. Overall, our preferred bandwidth of 0.5 (preferred because it captures a
much wider range than the RD but avoids possible contamination from other policies for students
below 1.5) suggests a decrease of 3.1 credits attempted and 1.1 credits completed after three
years, similar to the RD-DID results.
As in the RD and RD-DID, we find 2‚Äì3 percentage point reductions in certificate
completion in the DID specifications. But in contrast to the RD and RD-DID, the DID suggests
null or even positive effects on associates degrees and large positive effects on likelihood of
transferring to a four-year institution. While in theory the DID examines an estimand of greater

23

policy interest (measuring average effects for a greater range of students below the GPA
threshold), it is surprising that any degree completion/transfer impacts would become more
positive when we include students further below the threshold. Indeed, the impact on transfer is
largest in the DID when the bandwidth is expanded to +/- 1.0 (see Table 4, column 4). The
outcome graphs in Figure 7 provide an additional reason for concern: these degree and transfer
outcomes appear to virtually bottom out at a GPA of 1.5 (a phenomenon not observed for our
other outcomes). 16 Aid recipients tend to have lower levels of degree completion/transfer than
non-recipients regardless of GPA, but the difference narrows as we move down the GPA scale
towards 1.5. The DID will attribute this narrowing difference to SAP policy, when in fact it may
simply be attributable to floor effects in the outcome. For this reason, we are hesitant to take the
DID effects on degree completion/transfer at face value, even while we find the DID estimation
credible for other outcomes.
Finally, for student earnings we continue to see generally negative but insignificant
estimates of a magnitude similar to what we found in the RD-DID. The notable exception is in
the narrow bandwidth sample, for which we find very large and statistically significant
reductions in earnings. Given that these enormous reductions do not show up in any other
specification, are not visually discernable in Figure 7, and are in contrast with the prediction that
labor supply should increase when students drop out, we prefer not to over-interpret this.
Nonetheless, it is worth noting that the earnings coefficients are, at least, quite consistent in their
negative sign across all DID specifications as well as most of the RD specifications. This is
16

While the rate of ‚Äútransfer‚Äù is non-zero even for students with near-zero GPAs, we suspect this is because there
is some baseline noise in the definition of the outcome, combined with true transfers bottoming out around 1.5.
The noise may be due to students who co-enrolled at a four-year in their first year or even prior to their first year;
or, because it is based on NSC data it is possible that some of these students have transferred to for-profit
institutions. We are working to create a cleaner measure that would capture only transfer to a public/non-profit
four-year institution after the first year.

24

suggestive of another channel for earnings effects besides the school‚Äìwork time allocation
tradeoff initially hypothesized.

VII.

Discussion

In this paper, we attempt to provide a conceptual framework for thinking about the role
and consequences of imposing performance standards in the context of financial aid. The
framework suggests that some minimum standard is desirable. Determining whether a standard is
too high or too low will require weighing the value of encouragement effects for those who are
motivated to work harder against the discouragement effects for those who are induced to
dropout.
Consistent with the model and with prior research by Lindo, Sanders, and Oreopoulos
(2010), we find behavioral effects in the expected directions that are particularly strong in the
first term after a warning is issued, the fall of the second year. Also consistent with our
theoretical model, student responses to performance standards appear to be larger for students
receiving financial aid (seen by comparing the RD-DID specifications to the RD).
Discouragement effects appear larger, and encouragement effects smaller, when we include
students further below the GPA threshold (seen by comparing DID specifications of different
bandwidths). In our preferred DID specification (Table 4, third column), aid recipients who fail
the SAP grade standard are 6 percentage points less likely to reenroll in the second year, but
second-year GPAs rise by 0.04 points compared with similar unaided students. These results are
also consistent with Schudde and Scott-Clayton (2016), which applied a DID specification to
examine SAP policy in a different state and found significant negative effects on reenrollment
and positive (but small and insignificant) effects on GPAs in the second year.

25

Over the longer term, our results across all specifications suggest reductions of about
three credits attempted and one credit earned after three years. This pattern suggests that students
are discouraged from attempting credits they were unlikely to complete, and thus SAP policy
may improve the efficiency of aid distributed. If we multiply the decline in credits attempted by
an estimate of students‚Äô per-credit aid eligibility, the decline corresponds to a $440‚Äì$530 decline
in estimated aid disbursed per student in the second and third years. For comparison, $440‚Äì$530
is four to five times the tuition cost of one credit, which was below $115 for this sample and
timeframe. Moreover, this could be a conservative estimate of the cost savings since tuition itself
is subsidized, and because even some of the students in our sample who reenroll after failing
SAP may have done so without aid.
Still, our findings generate a number of dimensions for possible concern. First, we
consistently find declines of about 2‚Äì3 percentage points on certificate completion for aided
students who fail SAP. Because of the shorter length of these programs (often one year or even
less), the second year may already be too late to recover if students have a below-standard GPA
at the end of the first year. Recent estimates of the labor market payoff to certificates, using an
individual fixed-effects approach, suggest an earnings gain of perhaps $1,400 annually for
certificate completers (Di & Trimble, 2014; though it is not clear whether these gains might be
bigger or smaller for students on the margin of failing SAP). If only 2‚Äì3 percent of the sample
experiences this loss, it would take over a decade for the earnings losses to outweigh the
financial aid savings on average. Still, some individuals are clearly worse off as a result: the
discouragement effects of the policy mean that some students who could have earned a degree
are dissuaded from reenrolling. The concentrated consequences they experience may outweigh
the social benefit of reduced aid expenditures, which are dispersed across many. Moreover, it is

26

possible that the students who are least likely to earn a degree are those that benefit the most
from doing so (Brand & Xie, 2010).
More generally, it does not appear to be the case that students themselves receive much
benefit from SAP policy. The short term improvements in GPA are not sustained over the long
term; the isolated positive impact on transfer is sensitive to specification and follows a pattern
that suggests it may be spurious. Regarding the negative effects on credits and enrollment, in
theory, students with a low likelihood of completing the courses they attempt might benefit from
leaving school sooner rather than later in order to devote more time to gaining experience in the
labor market. However, we find little evidence of any positive effects on labor supply; indeed,
most point estimates on earnings were negative.
Taken broadly, the pattern of effects here suggests that SAP policy is at least partly doing
its job (at least from the perspective of a social planner who weights all students equally):
minimizing unproductive reenrollments while providing some encouragement for students to
perform better. This hardly implies that SAP policy is optimized, however. Our review of college
catalogs, as well as anecdotal reports from college staff, suggests that many students may not
learn about SAP until they lose aid. If true, this is a missed opportunity: if students are poorly
informed it will mute the incentive effects of standards, and the longer it takes for students to
realize they are failing, the harder it will be for them to get back above the GPA threshold.
From an equity stance, the implications of SAP policy are complex. Poor academic
performance is widespread across student demographics. SAP policy targets undergraduates
from America‚Äôs most disadvantaged families (median family income among aid recipients in our
sample is about $28,000). Students who are reliant on federal financial aid face the consequences
of academic standards more quickly than students who can afford to pay for college out of

27

pocket. A student with unlimited funds can, theoretically, continue to enroll in community
college for as many iterations as necessary to attain the 2.0 cumulative GPA required for
graduation. A student who relies on federal funding to cover tuition expenses ultimately receives
fewer chances to ‚Äúget it right.‚Äù While SAP standards may help some students avoid
overinvesting time, money, and energy into college schooling, it also may also prevent students
from economically disadvantaged households from an equal chance at earning a diploma.
Finally, an open question is how the effects of SAP may be different following a
significant tightening of the standards in 2011 (too late for us to examine in our sample). Federal
regulations now specify that SAP status must be evaluated at least once annually; only those
institutions evaluating more than once per year can use a ‚Äúwarning‚Äù status (and then only for one
term); and students who file a successful appeal may be placed on ‚Äúprobationary‚Äù status only for
one term (Satisfactory Academic Progress, 2012; U.S. Department of Education, 2014). In effect,
the new regulations mean that students who fail SAP cannot receive aid for more than one
subsequent term without filing an appeal; even if the appeal is successful, students can only
receive aid for one additional term unless they improve sufficiently to pass the SAP standard.
Because of these changes, SAP policy is likely to affect more students more quickly than it has
in the past.
These changes could be beneficial if students are encouraged to improve earlier in their
college careers, but they could be detrimental if enforcement is so draconian that students do not
have sufficient time to improve. Nor is it clear what would happen if standards were set at a
higher level such as 2.5, above the GPA typically required for graduation, as was recently
proposed by the Obama administration with respect to the President‚Äôs ‚Äúfree community college‚Äù
proposal. What is certain is that SAP policy is not going away, and may affect even more

28

students in future years‚Äîso the stakes are high to understand its impacts for both students and
public coffers.

29

References
Angrist, J., Lang, D., & Oreopoulos, P. (2009). Incentives and services for college achievement:
Evidence from a randomized trial. American Economic Journal: Applied Economics,
1(1), 136‚Äì63.
Altonji, J. G. (1993). The demand for and return to education when education outcomes are
uncertain. Journal of Labor Economics, 11(1), 48‚Äì83.
Bailey, M. J., & Dynarski, S. M. (2011). Inequality in postsecondary education. In G. J. Duncan
& R. J. Murnane (Eds.), Whither opportunity? Rising inequality, schools, and children‚Äôs
life choices (pp. 117‚Äì132). New York, NY: Russell Sage Foundation.
Barreca, A. I., Guldi, M., Lindo, J. M., & Waddell, G. R. (2011). Saving babies? Revisiting the
effect of very low birth weight classification. Quarterly Journal of Economics, 126(4),
2117‚Äì2123.
Barreca, A. I., Lindo, J. M., & Waddell, G. R. (2011). Heaping-induced bias in regressiondiscontinuity designs (NBER Working Paper No. 17408). Cambridge, MA: National
Bureau of Economic Research.
Baum, S., & Payea, K. (with Kurose, C.). (2013). Trends in student aid 2013. Retrieved from the
College Board website: http://trends.collegeboard.org/sites/default/files/student-aid2013-full-report-140108.pdf
Belfield, C., Liu, V. Y. T., & Trimble, M. J. (2014). The medium-term labor market returns to
community college awards: Evidence from North Carolina (A CAPSEE Working
Paper). New York, NY: Center for the Analysis of Postsecondary Education and
Employment.
Belley, P., & Lochner, L. (2007). The changing role of family income and ability in determining
educational achievement. Journal of Human Capital, 1(1), 37‚Äì89.
B√©nabou, R., & Tirole, J. (2000). Self-confidence and social interactions (NBER Working Paper
No. 7585). Cambridge, MA: National Bureau of Economic Research.
B√©nabou, R., & Tirole, J. (2002). Self-confidence and personal motivation. Quarterly Journal of
Economics, 117(3), 871‚Äì915.
Bennett, W., & Grothe, B. (1982). Implementation of an academic progress policy at a public
urban university: A review after four years. Journal of Student Financial Aid, 12(1), 33‚Äì39.
Brand, J. E., & Xie, Y. (2010). Who benefits most from college? Evidence for negative selection
in heterogeneous economic returns to higher education. American Sociological Review,
75(2), 273‚Äì302.

30

Carruthers, C., & √ñzek, U. (2013). Losing HOPE: Financial aid and the line between college
and work (Working Paper No. 91). Retrieved from Center for Analysis of Longitudinal
Data in Education Research website:
http://www.caldercenter.org/sites/default/files/wp91.pdf
Casey, M., Cline, J., Ost, B., & Qureshi, J. (2015, February). Academic probation, student
performance and strategic behavior. Paper presented at the annual meeting for the
Association for Education Finance and Policy, Washington, DC.
Cornwell, C., Mustard, D. B., & Sridhar, D. J. (2006). The enrollment effects of merit-based
financial aid: Evidence from Georgia‚Äôs HOPE program. Journal of Labor Economics,
24(4), 761‚Äì786.
Cornwell, C. M., Lee, K. H., & Mustard, D. B. (2005). Student responses to merit scholarship
retention rules. Journal of Human Resources, 40(4), 895‚Äì917.
Dynarski, S. M. (2008). Building the stock of college-educated labor. Journal of Human
Resources, XLIII(3), 577‚Äì610.
Fischer, F. (1987). Graduation-contingent student aid. Change (Nov‚ÄìDec), 40‚Äì47.
Imbens, G. W., & Lemieux, T. (2008). Regression discontinuity designs: A guide to practice.
Journal of Econometrics, 142(2), 615‚Äì635.
Jacobson, L., & Mokher, C. (2009). Pathways to boosting the earnings of low-income students
by increasing their educational attainment. Washington, DC: Hudson Institute.
Jepsen, C., Troske, K., & Coomes, P. (2014). The labor-market returns to community college
degrees, diplomas, and certificates. Journal of Labor Economics, 32(1), 95‚Äì121.
Lavecchia, A. M., Liu, H., & Oreopoulos, P. (2014). Behavioral economics of education:
Progress and possibilities (NBER Working Paper No. 19306). Cambridge, MA: National
Bureau of Economic Research.
Lindo, J. M., Sanders, N. J., & Oreopoulos, P. (2010). Ability, gender, and performance
standards: Evidence from academic probation. American Economic Journal: Applied
Economics, 2(2), 95‚Äì117.
Manski, C. F. (1988, June). Should we subsidize enrollment in or completion of postsecondary
schooling? Paper presented at the Conference on Outcome-Based Policy Options for
Vocational Education, Washington, DC.
Manski, C. F. (1989). Schooling as experimentation: A reappraisal of the postsecondary dropout
phenomenon. Economics of Education Review, 8(4), 305‚Äì312.
Manski, C. F., & Wise, D. (1983). College Choice in America. Cambridge, MA: Harvard
University Press.

31

McCrary, J. (2008). Manipulation of the running variable in the regression discontinuity design:
A density test. Journal of Econometrics, 142(2), 698‚Äì714.
Patel, R., & Valenzuela, I. (with McDermott, D.). (2013). Moving forward: Early findings from
the Performance-Based Scholarship Demonstration in Arizona. New York, NY: MDRC.
Richburg-Hayes, L., Brock, T., LeBlanc, A., Paxson, C., Rouse, C. E., & Barrow, L. (2009).
Rewarding persistence: Effects of a performance-based scholarship program for lowincome parents. New York, NY: MDRC.
Satisfactory Academic Progress, 34 C.F.R. ¬ß 668.34 (2012).
Schudde, L., & Scott-Clayton, J. (2016). Pell grants as performance-based scholarships? An
examination of satisfactory academic progress requirements in the nation‚Äôs largest needbased aid program. Research in Higher Education. doi:10.1007/s11162-016-9413-3
Scott-Clayton, J. (2011). On money and motivation: A quasi-experimental analysis of financial
incentives for college achievement. Journal of Human Resources, 46(3), 614‚Äì646.
Scott-Clayton, J. (2013). Information constraints and financial aid policy. In D. E. Heller & C.
Callender (Eds.), Student financing of higher education: A comparative perspective (pp.
75‚Äì97). New York, NY: Routledge.
Stinebrickner, T., & Stinebrickner, R. (2012). Learning about academic ability and the college
dropout decision. Journal of Labor Economics, 30(4), 707‚Äì748.
U.S. Department of Education, Federal Student Aid. (2014). School-determined requirements. In
Student eligibility [2014-2015 federal student aid handbook] (Vol. 1). Retrieved from
https://ifap.ed.gov/fsahandbook/attachments/1415Vol1Ch1.pdf
Wiswall, M., & Zafar, B. (2015). Determinants of college major choice: Identification using an
information experiment. Review of Economic Studies, 82(2), 791‚Äì824.
Xu, D., & Trimble, M. J. (2014). What about certificates? Evidence on the labor market returns
to non-degree community college awards in two states. New York, NY: Columbia
University, Teachers College, Community College Research Center.

32

Figure 1. Cutoff Values for Choosing Shirking, Task 1, or Task 2 in the Distribution of Ability
Panel A. Task 1 Permitted (no standards)

Œ∏1=c1/V 1

Panel 2. Task 1 Forbidden (standards)

P
Œ∏1P Œ∏1=c1/V 1 Œ∏*P Œ∏* Œ∏2=(c2-c
Œ∏21=Œ∏
)/(V
2 2-V1)

Œ∏2=(c2-c1)/(V 2-V1)

Œ∏*2=c2/V 2

Figure 2. Cutoff Values With Financial Aid P
Panel A. Task 1 permitted (no standards)

Œ∏1P Œ∏1

Panel B. Task 1 Forbidden (standards)

Œ∏1P Œ∏1

Œ∏2=Œ∏2P

33

Œ∏*P Œ∏*

Œ∏2=Œ∏2P

Figure 3. Distribution of GPA Before Eliminating Heaping at GPA = 2.0

Note: Fitted lines are shown for aid recipients above and below the cutoff after disregarding GPA = 2.0.

34

Figure 4. Fall Year 2 Outcomes

35

Figure 5. Year 3 Outcomes

36

Figure 6. Cumulative Outcomes After 3 Years

37

Figure 7. Degree Completion and Earnings Outcomes, End of Year 3

Notes: Aid recipients are in gray; non-recipients are in black. ‚ÄúEarnings, Y2‚ÄìY3‚Äù includes zeros
and covers six quarters beginning in the fall of Year 2 and ending in the spring of Year 3 (Q4‚Äì
Q1‚ÄìQ2‚ÄìQ3‚ÄìQ4‚ÄìQ1).

38

Table 1. Descriptive Statistics, State CC Sample (2004-2010 First-Time Full-Time Entrants)
Full Sample
Near Threshold
Below Threshold
Variable
Aided
No Aid
Aided
No Aid
Aided
No Aid
Background variables
Age (Years)
Female (%)
White (%)
Black (%)
Hispanic (%)
Avg. Total Aid Yr 1 ($)
Pell Recipient (%)
Avg. Pell Amt. ($)
Cum. GPA<2.0, Yr 1
Comp. < 67% creds, Yr 1
Failed SAP, Yr 1
Ever failed/wth, Yr 1
Credits attempted, Yr 1
Credits earned, Yr 1
Took placement test
Needs remed (predicted)
Ever dual-enrolled
Intent: Occ. Associate's
Intent: Occ. Cert.
Intent: Liberal arts AA/AS

21.0
59%
60%
29%
5%
$4,083
70%
$2,385
29%
37%
42%
69%
27.9
20.6
76%
74%
21%
34%
16%
50%

19.9
46%
73%
11%
7%
$0
0%
$0
32%
36%
42%
68%
26.5
19.7
71%
64%
14%
33%
10%
57%

19.9
56%
62%
29%
6%
$4,158
71%
$2,491
45%
41%
61%
91%
29.4
20.8
76%
74%
27%
34%
14%
52%

19.1
42%
74%
11%
7%
$0
0%
$0
45%
35%
59%
90%
28.1
20.5
73%
63%
18%
32%
8%
59%

20.1
55%
55%
35%
5%
$4,111
73%
$2,530
100%
72%
100%
98%
26.8
14.9
79%
78%
21%
35%
14%
51%

19.0
40%
72%
12%
8%
$0
0%
$0
100%
67%
100%
98%
26.1
15.3
75%
66%
15%
33%
9%
58%

Outcome variables
Enrolled, Fall Year 2
Term GPA, Fall Year 2
Credits Attempted, Fall Y2
Credits Earned, Fall Y2
School-Year Earnings, Y2
Any Earnings, Y2
Enrolled, Fall Year 3
Term GPA, Fall Year 3
Total Credits Attempted, Y3
Total Credits Earned, Y3
School-Year Earnings, Y3
Any Earnings, Y3
Total Credits Attempted, Y2-Y3
Total Credits Earned, Y2-Y3
Cumulative GPA, end of Y3

65%
2.25
9.3
7.1
$2,043
41%
36%
2.28
8.23
6.37
$2,557
41%
23.9
18.4
2.32

68%
2.25
9.6
7.3
$2,048
40%
39%
2.30
8.67
6.65
$2,541
39%
24.9
19.1
2.31

68%
1.95
9.0
5.9
$2,080
44%
41%
2.03
8.91
6.40
$2,548
43%
24.2
16.5
2.02

76%
1.99
10.4
7.0
$1,972
41%
49%
2.05
10.95
7.77
$2,543
42%
28.6
19.7
2.04

53%
1.60
6.3
3.6
$2,013
42%
30%
1.65
6.25
4.24
$2,448
42%
16.8
10.4
1.62

66%
1.63
8.3
4.8
$2,025
42%
42%
1.73
8.94
6.05
$2,588
42%
22.6
14.2
1.66

39

Table 1 (cont). Descriptive Statistics, State CC Sample (2004-2010 First-Time Full-Time Entrants)
Full Sample
Near Threshold
Below Threshold
Variable
Aided
No Aid
Aided
No Aid
Aided
No Aid
Earned Certificate, by Y3
Earned AA/AS, by Y3
Transferred to 4Yr, by Y3
Still enrolled, Spring Y3
Earnings, Y2-Y3
Any Earnings, Y2-Y3

10%
17%
22%
31%
$7,111
57%

7%
17%
27%
32%
$7,161
53%

6%
7%
14%
35%
$7,183
59%

5%
9%
19%
42%
$6,997
55%

2%
2%
10%
25%
$6,874
58%

2%
4%
12%
35%
$7,178
57%

Sample size

60,482

52,141

5,111

4,699

8,716

8,008

Source: Authors' calculations using restricted SCCS administrative data, 2004-2010 first time fall entrants
who initially enrolled full-time.
Notes: "Needs remediation" is predicted based on student scoring below typical remedial cutoff scores in
any of three possible tests. These may not correspond to actual cutoffs in use for a given school/cohort.
Percentages computed only for those with at least one test score.

40

Table 2. Covariate Balance Checks, Key Specifications
Outcome

RD (+/- 0.5)
Coef. (S.E)

RD-DID (+/- 0.5)
Coef. (S.E)

DID (+/- 0.5)
Coef. (S.E)

Age
Female
White
Black
Hispanic
Missing race
Took reading test
Took writing test
Took math test
Reading score
Writing score
Math score
Predicted to need remediation
Ever dual enrolled
Intent: Occ AA/AS
Intent: Occ certif.
Credits attempted, Yr 1
Any earnings, Yr 1
Earnings, Yr 1

0.15 (0.20)
0.06 (0.02)
0.02 (0.02)
0.00 (0.02)
-0.02 (0.01)
0.00 (0.01)
0.01 (0.02)
0.01 (0.02)
0.01 (0.02)
1.40 (0.69)
3.95 (1.41)
2.11 (0.98)
-0.05 (0.02)
-0.02 (0.02)
0.01 (0.02)
-0.01 (0.02)
-0.06 (0.43)
-0.01 (0.02)
-$112 (116)

0.24 (0.24)
0.02 (0.03)
0.04 (0.03)
-0.01 (0.03)
-0.01 (0.02)
-0.01 (0.01)
0.00 (0.03)
0.00 (0.03)
0.00 (0.03)
1.14 (0.96)
2.71 (1.93)
3.42 (1.53)
-0.04 (0.03)
-0.04 (0.03)
0.01 (0.03)
0.00 (0.02)
0.07 (0.59)
-0.01 (0.03)
-$18 (167)

0.02 (0.09)
0.01 (0.01)
-0.01 (0.01)
0.02 (0.01)
0.00 (0.01)
0.00 (0.00)
-0.01 (0.01)
0.00 (0.01)
0.00 (0.01)
0.88 (0.37)
0.30 (0.74)
1.13 (0.57)
-0.01 (0.01)
-0.02 (0.01)
0.02 (0.01)
-0.01 (0.01)
-0.11 (0.20)
0.00 (0.01)
$38 (63)

Sample size

0
**
0
0
*
0
0
0
0
**
***
**
**
0
0
0
0
0
$0

13,506

25,557

0
0
0
0
0
0
0
0
0
0
0
**
0
0
0
0
0
0
$0

0
0
0
**
0
0
0
0
0
**
0
**
0
**
0
0
0
0
$0

25,557

Source: Authors' calculations using restricted SCCS administrative data, 2004-2010 first
time fall entrants who initially enrolled full-time.
Notes: Test score rows are italicized because they are calculated only for those students
who have test scores available (approximately 75% took at least one test).

41

Table 3. RD-Estimated Effects of Failing GPA Performance Standard at End of Year 1, Aid Recipients Only

Outcome

Preferred Bandwidth (+/- 0.5)
With Cov.
No Cov.
Model 1
Model 2
Coef. (S.E)
Coef. (S.E)

Alternate BW (with covars.)
RD-0.25
RD-1.0
Model 3
Model 4
Coef. (S.E)
Coef. (S.E)

Enrolled, Fall Year 2
Term GPA, Fall Year 2
Credits Attempted, Fall Y2
Credits Earned, Fall Y2

-0.01 (0.02)
0.07 (0.04)
-0.36 (0.34)
-0.12 (0.29)

0

-0.08 (0.05)
0.11 (0.09)
-0.98 (0.74)
-0.52 (0.64)

Enrolled, Fall Year 3
Term GPA, Fall Year 3
Total Credits Att., Y3
Total Credits Earned, Y3

-0.03 (0.02)
0.00 (0.03)
-1.47 (0.50)
-0.85 (0.42)

-0.03 (0.02)
0.01 (0.03)
*** -1.42 (0.51)
** -0.80 (0.42)

Still enrolled, Spring Y3
Cumulative GPA, end of Y3
Total Credits Att., Y2-Y3
Total Credits Earned, Y2-Y3
Earned Certificate, by Y3
Earned AA/AS, by Y3
Transferred to 4Yr, by Y3

-0.08 (0.02)
0.00 (0.02)
-2.16 (0.91)
-1.35 (0.77)
-0.02 (0.01)
-0.03 (0.01)
0.00 (0.02)

0.01 (0.02)
** -2.07 (0.92)
*
-1.25 (0.78)
*
-0.02 (0.01)
*** -0.03 (0.01)
0
0.00 (0.02)

School-Year Earnings, Y2
Ln(earnings), Y2
School-Year Earnings, Y3
Ln(earnings), Y2
Earnings, Y2-Y3
Ln(earnings), Y2

$83 (141)
0.00 (0.08)
$203 (188)
0.13 (0.09)
$282 (440)
0.04 (0.08)

Sample size

13,506

0
*
0

-0.01 (0.02)
0.08 (0.04)
-0.33 (0.34)
-0.09 (0.29)

0

0

0
**
0
0

-0.02 (0.05)
-0.04 (0.08)
*** -1.36 (1.09)
*
-0.65 (0.91)

*
0
0
0

0

0

0

0
0
0

*** -0.08 (0.02) *** -0.07 (0.05) 0

0

0
0
0
0
0
0

$1 (165)
-0.04 (0.09)
$112 (200)
0.09 (0.09)
$12 (498)
-0.01 (0.09)
13,506

0

**
0
0
**
0
0
0
0
0
0
0

0.02 (0.04)
-3.25 (1.97)
-1.86 (1.70)
-0.04 (0.02)
-0.03 (0.03)
0.00 (0.04)

-$38 (296)
-0.13 (0.17)
-$398 (406)
-0.05 (0.19)
-$620 (944)
-0.12 (0.17)
5,111

0

0
0
*
0
0
0
0
0
0
0
0

0.00 (0.01)
0.05 (0.03)
-0.07 (0.23)
0.09 (0.19)
-0.04 (0.01)
0.02 (0.02)
-1.43 (0.33)
-0.86 (0.28)
-0.06 (0.01)
0.03 (0.01)
-1.66 (0.60)
-0.85 (0.51)
-0.01 (0.01)
0.01 (0.01)
0.02 (0.01)
-$49 (095)
-0.02 (0.06)
-$21 (129)
-0.01 (0.06)
-$134 (297)
-0.02 (0.06)
24,673

0
**
0
0

RD-DID (0.5 bw)
with covars
Model 5
Coef. (S.E)
-0.03 (0.03)
0.04 (0.06)
-0.76 (0.48)
-0.25 (0.42)

0
0
0
0

*** -0.02 (0.03) 0

0

-0.03 (0.05) 0

*** -1.63 (0.75) **

*** -0.83 (0.62) 0

*** -0.08 (0.03) **

**

-0.02 (0.03) 0

*

-1.34 (1.13)
-0.03 (0.01)
-0.02 (0.02)
0.02 (0.02)

*** -2.72 (1.32) **

0
0
**
0
0
0
0
0
0

-$188 (202)
-0.16 (0.12)
-$63 (276)
-0.02 (0.12)
-$555 (640)
0.01 (0.12)

0

**
0
0
0
0
0
0
0
0

25,557

Source: Authors' calculations using restricted SCCS administrative data, 2004‚Äì2010 first time
fall entrants who initially enrolled full-time.
Note: Robust standard errors in parentheses. All specifications use local linear regression with
observations at precisely 2.0 GPA dropped. Control variables include all variables listed in Table
2: age, gender, race dummies, placement test scores if available, placement test flags, flag for
predicted remedial need, flag for ever dual enrolled, first year credits attempted, first year
employment status, and first year earnings. For term GPA estimates, term GPA is imputed to the
last known cumulative GPA (this ensures that any impacts on this measure come only from
students who reenroll without introducing attrition bias).

42

Table 4. DID Estimated Effects of Failing GPA Performance Standard At End of Year 1
(Above vs. Below for Aided vs. Unaided Students)

Outcome

Model 6
DID-0.15
Coef. (S.E)

Model 7
DID-0.25
Coef. (S.E)

Enrolled, Fall Year 2
Term GPA, Fall Year 2
Credits Attempted, Fall Y2
Credits Earned, Fall Y2
School-Year Earnings, Y2
Ln(earnings), Y2

-0.05 (0.02)
0.09 (0.05)
-1.12 (0.40)
-0.28 (0.35)
-$412 (162)
-0.23 (0.10)

**
-0.05 (0.02)
*
0.05 (0.03)
*** -0.98 (0.25)
0
-0.26 (0.21)
** -$103 (104)
**
-0.08 (0.06)

Enrolled, Fall Year 3
Term GPA, Fall Year 3
Total Credits Attempted, Y3
Total Credits Earned, Y3

-0.03 (0.03)
0.02 (0.04)
-1.76 (0.61)
-0.68 (0.51)

0
0
***
0

Still enrolled, end of Y3
Cumulative GPA, End of Y3
Total Credits Attempted, Y2Total Credits Earned, Y2-Y3
Earned Certificate, by Y3
Earned AA/AS, by Y3
Transferred to 4Yr, by Y3

-0.05 (0.03)
0.04 (0.02)
-3.61 (1.08)
-1.34 (0.93)
-0.03 (0.01)
0.00 (0.01)
0.02 (0.02)

*
*
***
0
**
0
0

School-Year Earnings, Y2
Ln(earnings), Y2
School-Year Earnings, Y3
Ln(earnings), Y3
Earnings, Y2-Y3
Ln(earnings), Y2-Y3
Sample size

-$412 (162)
-0.23 (0.10)
-$515 (220)
-0.16 (0.09)
-$1,332 (517)
-0.17 (0.09)

Model 8
DID-0.5
Coef. (S.E)

Model 9
DID-1.0
Coef. (S.E)

***
0
***
0
0
0

-0.06 (0.01)
0.04 (0.02)
-0.91 (0.18)
-0.16 (0.15)
-$97 (077)
-0.08 (0.05)

*** -0.08 (0.01)
**
0.03 (0.02)
*** -1.01 (0.16)
0
-0.19 (0.14)
0
-$45 (069)
*
-0.04 (0.04)

***
0
***
0
0
0

-0.04 (0.02)
0.01 (0.03)
-1.79 (0.38)
-0.85 (0.32)

***
0
***
***

-0.05 (0.01)
-0.01 (0.02)
-1.62 (0.28)
-0.82 (0.23)

***
0
***
***

-0.06 (0.01)
-0.02 (0.02)
-1.58 (0.25)
-0.82 (0.21)

***
0
***
***

-0.06 (0.02)
0.01 (0.01)
-3.37 (0.67)
-1.34 (0.57)
-0.02 (0.01)
0.00 (0.01)
0.04 (0.01)

***
0
***
**
***
0
***

-0.05 (0.01)
0.01 (0.01)
-3.10 (0.49)
-1.13 (0.42)
-0.02 (0.01)
0.01 (0.01)
0.04 (0.01)

***
0
***
***
***
0
***

-0.05 (0.01)
0.00 (0.01)
-3.17 (0.43)
-1.12 (0.37)
-0.02 (0.00)
0.01 (0.01)
0.05 (0.01)

***
0
***
***
***
**
***

-$97 (077)
-0.08 (0.05)
-$127 (104)
-0.01 (0.05)
-$329 (243)
-0.05 (0.04)

0
*
0
0
0
0

-$45 (069)
-0.04 (0.04)
-$167 (094)
-0.02 (0.04)
-$332 (219)
-0.02 (0.04)

0
0
*
0
0
0

** -$103 (104) 0
**
-0.08 (0.06) 0
** -$186 (141) 0
*
-0.08 (0.06) 0
*** -$394 (328) 0
*
-0.06 (0.06) 0

16,326

19,223

25,557

31,562

Source: Authors' calculations using restricted SCCS administrative data, 2004‚Äì2010 first time fall entrants who
initially enrolled full-time.
Note: Robust standard errors in parentheses. Coefficients shown are on the interaction term aided*below. Aid status
is based on first year awards and includes all forms of aid. Range of data above the cutoff is held fixed across
specifications at 0.5; range of data included below the threshold varies by model. All specifications include fixed
effects for first-year GPA bin in increments of 0.05, with observations at precisely 2.0 GPA dropped. Control
variables include all variables listed in Table 2: age, gender, race dummies, placement test scores if available,
placement test flags, flag for predicted remedial need, flag for ever dual enrolled, first year credits attempted, first
year employment status, and first year earnings. For term GPA estimates, term GPA is imputed to the last known

43

cumulative GPA (this ensures that any impacts on this measure come only from students who reenroll without
introducing attrition bias).

Table A1. DID Estimated Effects of Failing GPA Performance Standard At End of Year 1, No Covariates

Outcome

DID-0.15
No Cov.
Coef. (S.E)

Enrolled, Fall Year 2
Term GPA, Fall Year 2
Credits Attempted, Fall Y2
Credits Earned, Fall Y2

-0.05 (0.02)
0.10 (0.05)
-1.15 (0.41)
-0.27 (0.35)

**
**
***
0

-0.05 (0.02)
0.05 (0.03)
-0.98 (0.25)
-0.25 (0.21)

***
*
***
0

-0.06 (0.01)
0.04 (0.02)
-0.89 (0.19)
-0.15 (0.16)

***
**
***
0

-0.08 (0.01)
0.03 (0.02)
-1.03 (0.16)
-0.21 (0.14)

***
0
***
0

Enrolled, Fall Year 3
Term GPA, Fall Year 3
Total Credits Attempted, Y3
Total Credits Earned, Y3

-0.03 (0.03)
0.03 (0.04)
-1.73 (0.61)
-0.64 (0.51)

0
0
***
0

-0.04 (0.02)
0.02 (0.03)
-1.76 (0.38)
-0.82 (0.32)

***
0
***
**

-0.05 (0.01)
-0.01 (0.02)
-1.63 (0.28)
-0.84 (0.23)

***
0
***
***

-0.06 (0.01)
-0.02 (0.02)
-1.61 (0.25)
-0.85 (0.21)

***
0
***
***

Still enrolled, end of Y3
Cumulative GPA, End of Y3
Total Credits Attempted, Y2Total Credits Earned, Y2-Y3
Earned Certificate, by Y3
Earned AA/AS, by Y3
Transferred to 4Yr, by Y3

-0.05 (0.03)
0.04 (0.02)
-3.59 (1.10)
-1.25 (0.95)
-0.03 (0.01)
0.00 (0.01)
0.03 (0.02)

*
**
***
0
**
0
0

-0.06 (0.02)
0.02 (0.01)
-3.31 (0.69)
-1.25 (0.58)
-0.02 (0.01)
0.00 (0.01)
0.04 (0.01)

***
0
***
**
***
0
***

-0.05 (0.01)
0.01 (0.01)
-3.07 (0.50)
-1.12 (0.43)
-0.02 (0.01)
0.01 (0.01)
0.05 (0.01)

***
0
***
***
***
0
***

-0.05 (0.01)
0.00 (0.01)
-3.24 (0.45)
-1.18 (0.38)
-0.02 (0.00)
0.01 (0.01)
0.05 (0.01)

***
0
***
***
***
*
***

**
*
**
0
**
0

-$38 (121)
-0.06 (0.07)
-$145 (150)
-0.08 (0.06)
-$236 (371)
-0.04 (0.06)

0
0
0
0
0
0

-$77 (091)
-0.06 (0.05)
-$122 (112)
-0.01 (0.05)
-$292 (277)
-0.04 (0.05)

0
0
0
0
0
0

-$45 (082)
-0.03 (0.04)
-$178 (101)
-0.02 (0.04)
-$351 (251)
-0.02 (0.04)

0
0
*
0
0
0

School-Year Earnings, Y2
Ln(earnings), Y2
School-Year Earnings, Y3
Ln(earnings), Y3
Earnings, Y2-Y3
Ln(earnings), Y2-Y3
Sample size

-$384 (195)
-0.18 (0.10)
-$505 (235)
-0.13 (0.10)
-$1,272 (589)
-0.13 (0.10)

DID-0.25
No Cov.
Coef. (S.E)

16,326

19,223

44

DID-0.5
No Cov.
Coef. (S.E)

25,557

DID-1.0
No Cov.
Coef. (S.E)

31,562

