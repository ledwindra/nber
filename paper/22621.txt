NBER WORKING PAPER SERIES

A FRAMEWORK FOR ELICITING, INCORPORATING, AND DISCIPLINING IDENTIFICATION
BELIEFS IN LINEAR MODELS
Francis DiTraglia
Camilo García-Jimeno
Working Paper 22621
http://www.nber.org/papers/w22621

NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
September 2016

We thank Daron Acemoglu, Richard Hahn, Mallick Hossain, Hidehiko Ichimura, Laura Liu,
Ulrich Muller, Frank Schorfheide, and Ben Ukert, as well as seminar participants at Princeton,
Penn State, the 2015 NSF-NBER Seminar on Bayesian Inference, the 2015 Midwest
Econometrics Group Meetings, and the 2016 ISBA World Meeting for helpful comments and
suggestions. We thank Alejandro Sanchez for excellent research assistance. Finally, we
acknowledge financial support from a UPenn University Research Foundation award. The views
expressed herein are those of the authors and do not necessarily reflect the views of the National
Bureau of Economic Research.
NBER working papers are circulated for discussion and comment purposes. They have not been
peer-reviewed or been subject to the review by the NBER Board of Directors that accompanies
official NBER publications.
© 2016 by Francis DiTraglia and Camilo García-Jimeno. All rights reserved. Short sections of
text, not to exceed two paragraphs, may be quoted without explicit permission provided that full
credit, including © notice, is given to the source.

A Framework for Eliciting, Incorporating, and Disciplining Identification Beliefs in Linear
Models
Francis DiTraglia and Camilo García-Jimeno
NBER Working Paper No. 22621
September 2016
JEL No. B23,B4,C10,C11,C16,C26,C8
ABSTRACT
To estimate causal effects from observational data, an applied researcher must impose beliefs.
The instrumental variables exclusion restriction, for example, represents the belief that the
instrument has no direct effect on the outcome of interest. Yet beliefs about instrument validity
do not exist in isolation. Applied researchers often discuss the likely direction of selection and the
potential for measurement error in their papers but at present lack formal tools for incorporating
this information into their analyses. As such they not only leave money on the table, by failing to
use all relevant information, but more importantly run the risk of reasoning to a contradiction by
expressing mutually incompatible beliefs. In this paper we characterize the sharp identified set
relating instrument invalidity, treatment endogeneity, and measurement error in a workhorse
linear model, showing how beliefs over these three dimensions are mutually constrained. We
consider two cases: in the first the treatment is continuous and subject to classical measurement
error; in the second it is binary and subject to non-differential measurement error. In each, we
propose a formal Bayesian framework to help researchers elicit their beliefs, incorporate them
into estimation, and ensure their mutual coherence. We conclude by illustrating the usefulness of
our proposed methods on a variety of examples from the empirical microeconomics literature.

Francis DiTraglia
Department of Economics
University of Pennsylvania
3718 Locust Walk
Philadelphia
Pennsylvania
19104
fditra@sas.upenn.edu
Camilo García-Jimeno
Department of Economics
University of Pennsylvania
3718 Locust Walk
Philadelphia, PA 19104
and NBER
gcamilo@sas.upenn.edu

“Belief is so important! A hundred
contradictions might be true.”
— Blaise Pascal, Pensées

1

Introduction

To identify causal effects from observational data, an applied researcher must augment the
data with her beliefs. The exclusion restriction in an instrumental variables (IV) regression,
for example, represents the belief that the instrument has no direct effect on the outcome
of interest. Although this belief can never be tested directly, applied researchers know how
to think about it and how to debate it. In practice, however, not all beliefs are treated
equally. In addition to “formal beliefs” such as the IV exclusion restriction – beliefs that
are directly imposed to obtain identification – researchers often state a number of “informal
beliefs.” Although they are not directly imposed on the problem, informal beliefs play an
important role in interpreting results and reconciling conflicting estimates. Papers that
report IV estimates, for example, almost invariably state the authors’ belief about the sign
of the correlation between the endogenous treatment and the error term but do not exploit
this information in estimation.1 Another common informal belief concerns the extent of
measurement error. When researchers observe an ordinary least squares (OLS) estimate
that is substantially smaller than, but has the same sign as its IV counterpart, classical
measurement error, with its attendant “least squares attenuation bias,” often is suggested
as the likely cause.
In this paper we argue that relegating informal beliefs to second-class status is both
wasteful of information and dangerous; beliefs along different dimensions of the problem
are mutually constrained by each other, the model, and the data. By failing to explicitly
incorporate all relevant information, applied researchers both leave money on the table and,
more importantly, risk reasoning to a contradiction by expressing mutually incompatible
beliefs. Although this point is general, we illustrate its implications here in the context of a
simple linear model
y = βT ∗ + x0 γ + u

(1)

T = T∗ + w

(2)

1

Referring to more than 60 papers published in the top three empirical journals between 2002 and 2005,
Moon and Schorfheide (2009) note that “in almost all of the papers the authors explicitly stated their beliefs
about the sign of the correlation between the endogenous regressor and the error term; yet none of the
authors exploited the resulting inequality moment condition in their estimation.”

1

where T ∗ is a potentially endogenous treatment, y is an outcome of interest, and x is a vector
of exogenous controls. Our goal is to estimate the causal effect of T ∗ on y, namely β, but
we observe only T , a noisy measure of T ∗ polluted by measurement error w. While we are
fortunate to have an instrument z at our disposal, it may not satisfy the exclusion restriction:
z is potentially correlated with u. This scenario is typical in applied microeconomics: endogeneity is the rule rather than the exception, the treatments of greatest interest are often the
hardest to measure, and the validity of a proposed instrument is almost always debatable.
We consider two cases. In the first, T ∗ is continuous and subject to classical measurement
error: T ∗ is independent of w. In the second, z and T ∗ are binary and T ∗ is subject to
non-differential measurement error: the joint distribution of T ∗ and w is unrestricted, but
T is assumed to be conditionally independent of all other variables in the system, given
T ∗ .2 In each case we derive the identified set relating treatment endogeneity, measurement
error, and instrument invalidity in terms of empirically meaningful parameters. We then
use this characterization to construct a framework for Bayesian inference that combines the
information contained in the data with researcher beliefs in a coherent and transparent way.
As we demonstrate through a number of empirical examples, this framework not only allows
researchers to incorporate relevant problem-specific beliefs, but, by identifying any inconsistencies that may be present, provides a tool for refining and disciplining these beliefs.
Although our method employs Bayesian reasoning, it can be implemented in a number of
different ways that should make it appealing both to frequentists and Bayesians.
While measurement error, treatment endogeneity, and invalid instruments have all generated voluminous literatures, to the best of our knowledge this is the first paper to carry out
a partial identification exercise in which all three problems can be present simultaneously.
Our main point is simple but has important implications for applied work that have been
largely overlooked; measurement error, treatment endogeneity, and instrument invalidity are
mutually constrained by each other and the data in a manner that can only be made apparent by characterizing the full identified set for the model. Because the dimension of this
set is strictly smaller than the number of variables used to describe it, the constraints of
the model could easily contradict prior researcher beliefs. Given the shape of the identified
set, the belief that z is a valid instrument, for example, could imply an implausible amount
of measurement error in T ∗ or a selection effect with the opposite of the expected sign. In
this way our framework provides a means of reconciling and refining beliefs that would not
be possible based on introspection alone. We are by no means the first to recognize the
importance of requiring that beliefs be compatible. Kahneman and Tversky (1974), for ex2

These cases require a separate treatment because, as we discuss below, a binary regressor cannot be
subject to classical measurement error.

2

ample, make a closely related point in their discussion of heuristic decision-making under
uncertainty. Even if specific probabilistic assessments appear coherent on their own,
an internally consistent set of subjective probabilities can be incompatible with
other beliefs held by the individual . . . For judged probabilities to be considered adequate, or rational, internal consistency is not enough. The judgements must be compatible with the entire web of beliefs held by the individual. Unfortunately, there can
be no simple formal procedure for assessing the compatibility of a set of probability
judgements with the judge’s total system of beliefs (Kahneman and Tversky, 1974, p.
1130).

Our purpose here is to take up the challenge laid down by Kahneman and Tversky (1974)
and provide just such a formal procedure for assessing the compatibility of researcher beliefs
over treatment endogeneity, measurement error, and instrument invalidity in linear models.
Although the intuition behind our procedure is straightforward, the details are more involved.
For this reason we provide free and open-source software in R and Stata to make it easy for
applied researchers to implement the methods described in this paper.3
Elicitation is a key ingredient of our framework. Before we can impose researcher beliefs
we must express them in intuitive, empirically meaningful terms. In the continuous treatment
setting, we express instrument invalidity in terms of ρuz ≡ Cor(z, u), treatment endogeneity
in terms of ρT ∗ u ≡ Cor(T ∗ , u), and measurement error in terms of κ ≡ Var(T ∗ )/Var(T ),
essentially a signal-to-noise ratio that is conveniently bounded between zero and one. For
the binary treatment and instrument case, measurement error is parameterized in terms of a
pair of misclassification probabilities (α0 , α1 ), defined in Section 5, and instrument invalidity
and treatment endogeneity are more naturally expressed as differences of conditional means.
Specifically, δz is the average difference in unobservables u between individuals with high
and low value for the instrument while δT ∗ is the average difference in u between treated and
untreated individuals. In this paper we impose only relatively weak prior beliefs in the form
of sign and interval restrictions on the aforementioned parameters.4 As we discuss further
in our empirical examples, these are fairly easy to elicit in practice and can be surprisingly
informative about the causal effect of interest.
The addition of researcher beliefs is not only extremely helpful, but unavoidable. As we
show below, the data alone provide no restriction on β, although they do bound the maximum
possible amount of measurement error. Nevertheless, whenever one imposes information
beyond what is contained in the data, it is crucial to make clear how this affects the ultimate
3

See https://github.com/fditraglia/ivdoctr and https://github.com/fditraglia/binivdoctr.
Researchers who feel comfortable imposing more finely-grained beliefs can easily do so within our framework, but elicitation of fully-informative priors is more challenging in practice.
4

3

result. This motivates our use of a transparent parameterization, which decomposes the
problem into a vector of partially-identified structural parameters θ, and a vector of identified
reduced form parameters ϕ in such a way that inference for the identified set Θ for θ depends
on the data only through ϕ. This decomposition has several advantages. First, since the
reduced form parameters are identified, inference for this part of the problem is completely
standard. Second, a transparent parameterization shows us precisely where any identification
beliefs we may choose to impose enter the problem: the data rule out certain values of ϕ,
while our beliefs place restrictions on the conditional identified set Θ(ϕ) which ultimately
yields inference for the causal effect β.
This paper contributes to a small but growing literature on the Bayesian analysis of
partially-identified models, including Poirier (1998), Gustafson (2005), Richardson et al.
(2011), Moon and Schorfheide (2012), Kitagawa (2012), Hahn et al. (2016), Kline and Tamer
(2016), and Gustafson (2015). Some recent contributions to the literature on structural vector autoregression models (Amir-Ahmadi and Drautzburg, 2016; Arias et al., 2016; Baumeister and Hamilton, 2015) also explore related ideas. Our results also relate to a large literature
on estimating the effect of mis-measured binary regressors. An early contribution is Bollinger
(1996) who provides partial identification bounds for an exogenous mis-measured regressor.
van Hasselt and Bollinger (2012) derive additional bounds for the same model and Bollinger
and van Hasselt (2015) propose a Bayesian inference procedure based on these bounds. Because we consider a situation in which an instrumental variable is available, our setting is
more closely related to that considered by Kane et al. (1999), Black et al. (2000), Frazis and
Lowenstein (2003), Lewbel (2007), Mahajan (2006) and Hu (2008). The key lesson from
these papers is that the two-stage least squares (TSLS) estimator is inconsistent even if the
instrument is valid. When the treatment is exogenous, however, it is possible to construct a
non-linear method of moments estimator that recovers the treatment effect using a discrete
instrumental variable.
Unlike these papers, we consider a setting in which the binary treatment of interest may
be endogenous. As shown in DiTraglia and Garcia-Jimeno (2016) the usual instrumental
variable assumption – conditional mean independence – is insufficient to identify the effect
of an endogenous, mis-measured, binary treatment. Although DiTraglia and Garcia-Jimeno
(2016) provide a point identification result under a stronger assumption on the instrument
– full independence – we do not employ this result here. Instead we allow for an invalid
instrument and derive partial identification results.
Two recent papers that similarly consider partial identification under instrument invalidity are Conley et al. (2012) and Nevo and Rosen (2012). Like us, Conley et al. (2012)
adopt a Bayesian approach that allows for a violation of the IV exclusion restriction, but
4

they do not explore the relationship between treatment endogeneity and instrument invalidity. In contrast, Nevo and Rosen (2012) derive bounds for a causal effect in the setting
where an endogenous regressor is “more endogenous” than the variable used to instrument
it is invalid.5 Our framework encompasses the settings considered in these two papers, but
is strictly more general; we allow for measurement error simultaneously with treatment endogeneity and instrument invalidity. More importantly, the central message of our paper is
that it can be misleading to impose beliefs on only one dimension of a partially identified
problem unless one has a way of ensuring their mutual consistency with all other relevant
researcher beliefs. For example, although a single valid instrument solves both the problem
of classical measurement error and treatment endogeneity, we argue that it is insufficient to
carry out a partial identification exercise that merely relaxes the exclusion restriction, as in
Conley et al. (2012). Values for the correlation between z and u that seem plausible when
viewed in isolation could easily imply implausible amounts of measurement error or treatment endogeneity. While our main contribution here is to describe the relationship between
measurement error, treatment endogeneity, and instrument invalidity, we also derive sharp
bounds on the extent of both classical measurement error when the treatment is continuous,
and non-differential measurement error when the treatment is binary. These are, to the best
of our knowledge, new to the literature and could be of interest in their own right.
The remainder of this paper is organized as follows. Section 2 derives the identified
set for a continuous treatment under classical measurement error. Section 3 describes our
approach to inference in this setting and Section 4 presents two empirical examples. Section
5 then derives the identified set for a binary instrument and binary treatment subject to
non-differential measurement error. Section 5.5 explains the differences between inference
for a binary and a continuous treatment, while Section 6 presents two empirical examples in
which the treatment is binary. Section 7 concludes.

2

The Identified Set for a Continuous Treatment

2.1

Model and Assumptions

To simplify the notation, suppose either that there are no exogenous control regressors x
(including a constant), or equivalently, that they have been “projected out.” In Section 2.4
we explain why this assumption is innocuous and how to accommodate control regressors
in practice. With this simplification, Equations 1–2 and the first stage T ∗ = πz + v can be
5

In our notation, ρT ∗ u and ρuz have the same sign but |ρuz | < |ρT ∗ u |.

5

written in matrix form as

y

 T

 z

T∗









 = Γ





u
v
z
w







,





Γ=



1
0
0
0


β βπ 0

1 π 1 

0 1 0 

1 π 0

(3)

where we assume, without loss of generality, that all random variables in the system are mean
zero or have been de-meaned.6 Our goal is to learn the parameter β, the causal effect of
T ∗ . In general, T ∗ is unobserved: we only observe a noisy measure T that has been polluted
by classical measurement error w. We call (u, v, w, z) the “primitives” of the system and
assume that they satisfy the following assumptions.
Assumption 2.1. The covariance matrix Ω of the model primitives (u, v, z, w) is finite and
satisfies

 2
"
#
σ
σ
uv σuz
u
e 0
Ω

e=
(4)
Ω=
, Ω
0 
 σuv σv2
0
2
0 σw
2
σuz 0 σz
e – the covariance matrix of (u, v, z) – is positive definite, and σw2 – the measurement
where Ω
error variance – is non-negative.
Because w represents classical measurement error, it is uncorrelated with u, v, and z as
well as T ∗ . The parameter σuz controls the invalidity of the instrument z: unless σuz = 0, z is
an invalid instrument. Both σuz and σuv control the endogeneity of T ∗ ; σuv is the component
of Cov(T ∗ , u) that is unrelated to z. The matrix Ω is unobserved. We observe only Σ, the
covariance matrix of (y, T, z):



σT2 σT y σT z


Σ =  σT y σy2 σyz  .
σT z σyz σz2

(5)

To ensure that the IV estimand is well-defined and that the elements of Σ are finite, we
impose the following assumption:
Assumption 2.2. The first-stage coefficient π is non-zero and both β and π are finite.
Since π 6= 0, Γ is full rank. Moreover, by Assumption 2.1, Σ is positive definite. The
system we have just finished describing does not identify the treatment effect β. In particular,
6

Equivalently, we can treat the constant term in the first-stage and main equation as exogenous regressors
that have been projected out.

6

neither the OLS nor IV estimators converge in probability to β, instead they approach
βOLS

σT y
= 2 =
σT



and
βIV =

σT2 ∗
σT2 ∗ + σw2



σT ∗ u
β+ 2
σT ∗

σzy
σuz
=β+
σT z
σT z

(6)

(7)

where σT2 ∗ = σT2 − σw2 denotes the variance of the unobserved regressor T ∗ , and
σT ∗ u = σuv + πσuz .

(8)

Because both σuv and σuz are sources of endogeneity for the unobserved regressor T ∗ , there is
an indirect link between instrument invalidity and the OLS estimand. Moreover, while the IV
probability limit depends neither on the extent of measurement error, σw2 , nor on σuv , through
the model and assumptions it nevertheless contains information about both quantities. As a
result, the problems of measurement error, regressor endogeneity, and instrument invalidity
are mutually constrained. Our next task is to characterize the relationship between them
by exploiting all of the implications of Equation 5 and Assumptions 2.1 and 2.2. To aid in
this characterization, we first provide a re-parameterization of the problem, expressing it in
terms of quantities that are empirically meaningful and thus practical for eliciting researcher
beliefs.

2.2

A Convenient Parameterization

Because our ultimate goal is to elicit and incorporate researcher’s beliefs, we work in terms
of the following quantities:
ρuz = Cor(u, z)
ρT ∗ u = Cor(T ∗ , u)
σ2 ∗
σ2 ∗
κ = T2 = 2 T 2 .
σT
σT ∗ + σw

(9)
(10)
(11)

The first quantity, ρuz , is the correlation between the instrument and the main equation
error term u. It measures the endogeneity of the instrument. The exclusion restriction in
IV estimation, for example, corresponds the degenerate belief that ρuz = 0. When critiquing
an instrument, researchers often state a belief about the likely sign of this quantity. The
second quantity, ρT ∗ u , is the correlation between the unobserved regressor T ∗ and the main
equation error term. It measures the overall endogeneity of T ∗ , taking into account both the
7

effect of σuv and σuz . While in practice it would be unusual to be able to articulate a belief
about σuv , researchers almost invariably state their belief about the sign of the quantity ρT ∗ u
before undertaking an IV estimation exercise.
The third quantity, κ, may be somewhat less familiar. When there are no covariates and
∗
T is exogenous, κ measures the degree of attenuation bias present in the OLS estimator: if
ρT ∗ u = 0 then the OLS probability limit is κβ. Equivalently, provided that σyT ∗ 6= 0,

κ=

σT2 ∗
σT2



2
σyT
2
σyT
∗

!


=

2
σyT
σT2 σy2



σT2 ∗ σy2
2
σyT
∗

!
=

ρ2yT
ρ2yT ∗

(12)

so another way to interpret κ is as the ratio of the observed R2 of the main equation and the
unobserved R2 that we would obtain if our regressor had not been polluted with measurement
error.7 A third way to think about κ is in terms of signal and noise. If κ = 1/2, for example,
this means that half of the variation in the observed regressor T is “signal,” T ∗ , and the
remainder is noise, w. While the other two interpretations are specific to the case of no
covariates, this third interpretation is general. We consider it much easier to elicit beliefs
about κ than about σw2 because κ has bounded support: it takes a value in (0, 1]. When
κ = 1, σw2 = 0 so there is no measurement error. The limit as κ approaches zero corresponds
to taking σw2 to infinity.
Expressed in this way, our parameter space is bounded, all of our parameters are scalefree, and most importantly, they are meaningful in real-world applications. Moreover, although the model introduced in the preceding section contains six non-identified parameters
– β, σu2 , σuv , σuz , σv2 , and σw2 – knowledge of any two of the parameters (ρuz , ρT ∗ u , κ) is
sufficient to identify the whole system. In the following section we solve for ρuz in terms of
ρT ∗ u and κ, and go on to characterize the sharp identified set for (ρT ∗ u , ρuz , κ). This fully
describes the information contained in the data and our assumptions.

2.3

Deriving the Identified Set for (ρT ∗ u , ρuz , κ)

We begin by deriving the relationship between ρuz , ρT ∗ u , and κ. The basic idea is to combine
the OLS and IV probability limits, Equations 6 and 7, with the variance decomposition for
y implied by the linear model. After eliminating β and σu2 from the resulting equations, and
re-parameterizing as described in the preceding section, we derive a quadratic equation in
ρuz with coefficients that involve κ and ρT ∗ u . Solving and simplifying, we show that one of
the two roots is extraneous because it implies a negative value for σu , leading to the following
7

This follows because Cov(T, y) = Cov(T ∗ , y) under classical measurement error.

8

equality.8
Proposition 2.1. Under Equation 5 and Assumptions 2.1 and 2.2,

ρuz =

ρT ∗ u ρT z
√
κ

s


− (ρT y ρT z − κρzy )

1 − ρ2T ∗ u

κ κ − ρ2T y

(13)

Equation 13 allows us to solve for ρuz in terms of observable correlations – ρT y , ρT z , and
ρzy – and the unobserved parameters ρT ∗ u and κ. Thus, to fully characterize the relationship
between measurement error, treatment endogeneity, and instrument invalidity, it suffices to
derive the sharp identified set for (ρT ∗ u , κ). To this end, we first list a set of simple conditions
that are Equivalent to Assumption 2.1.
Lemma 2.1. The following conditions are equivalent to Assumption 2.1:
(a) σu2 , σv2 , σz2 , σw2 < ∞
(b) σu2 , σv2 , σz2 > 0, σw2 ≥ 0
(c) ρ2uv + ρ2uz < 1
(d) Cov(w, z) = Cov(w, u) = Cov(w, v) = 0.
Parts (a) and (b) of Lemma 2.1 are straightforward: all variances must be finite and
strictly positive with the exception of σw2 , which equals zero in the absence of measurement
error. Part (c), however, is somewhat less intuitive. Geometrically, it states that (ρuz , ρuv )
must lie within the unit circle: if one of the correlations is very large in absolute value, the
other cannot be. To understand the intuition behind this constraint, recall that since v is
the residual from the projection of T ∗ onto z, it is uncorrelated with z by construction. If
ρuz and ρuv were both sufficiently close to one this would require z and v to be correlated,
leading to a contradiction. Part (d) of the Lemma is simply the classical measurement error
assumption. We now use Lemma 2.1 to derive the sharp identified set for ρT ∗ u and κ.
Proposition 2.2 (Sharp Identified Set for ρT ∗ u and κ.). Under Assumptions 2.1–2.2 and
Equation 5, (ρT ∗ u , κ) ∈ (−1, 1) × (κ, 1] where
κ=

ρ2T y + ρ2T z − 2ρT y ρT z ρzy
1 − ρ2zy

(14)

These bounds are sharp.
8

The convention that standard deviations are positive ensures that correlations have the same sign as
covariances.

9

The proof of Proposition 2.2 proceeds by showing that the two bounds κ < κ ≤ 1 and
|ρT ∗ u | < 1 are equivalent to Assumption 2.1 given the model from Equation 5 and Assumption
2.2.9 Thus, the sharp identified set is a rectangular region: for any allowable value of κ, ρT ∗ u
can take on any value strictly between -1 and 1.
Because Proposition 2.2 provides a lower bound for κ, it places an upper bound on the
extent of measurement error given the observed covariance matrix Σ. This bound relies on
two simpler, but weaker bounds. The first is κ > ρ2T y . In a setting without covariates, this
says that the R-squared of a regression of y on T provides an upper bound for the maximum
possible amount of measurement error. Although typically stated somewhat differently, this
bound is well known: it corresponds to the familiar “reverse regression bound” for β.10 In
our setting this bound is implied by σu2 > 0 since
σu2

=

σy2



κ − ρ2T y
κ (1 − ρ2T ∗ u )


(15)

by Lemma A.1(c) from the Appendix. Note that ρ2T y < κ implies that the solution for
ρuz from Proposition 2.1 is always real-valued. The second of these two weaker bounds is
κ > ρ2T z , which says that the R-squared of the first-stage regression of T on z also provides
an upper bound for the maximum possible amount of measurement error. This follows since,
by Lemma A.1(a) of the Appendix,
σv2 = σT2 (κ − ρ2T z )

(16)

and σv2 must be strictly positive. We doubt that we are the first to notice this bound given its
simplicity. Nevertheless, to the best of our knowledge, it has not appeared in the literature.
After some algebra, these two weak bounds together with the restriction that ρ2uv + ρ2uz < 1
from Lemma 2.1 and the positive-definiteness of Σ yield the sharp lower bound for κ in

Proposition 2.2. This bound is strictly tighter than κ > max ρ2zT , ρ2T y . Note that the
sharp bound incorporates additional information from the reduced form regression of y on
z.
The existence of an upper bound on measurement error, one that tightens as the OLS
and first-stage R-squared values increase, is important because applied econometricians often
explain a substantial discrepancy between OLS and IV estimators by arguing that their data
9

Assumption 2.2 ensures that the coefficient matrix Γ from Equation 5 is full rank. This means that the
positive-definiteness of Σ and of the extended covariance matrix Cov(y, T, z, T ∗ ) is implied by Assumption
2.1 and thus provides no additional restrictions.
10
To see this, suppose that ρT ∗ u = 0, and without loss of generality that β is positive. Then Equation 6
gives βOLS = κβ < β. Multiplying both sides of κ > ρ2T y by β and rearranging gives β < βOLS /ρ2T y , and
hence βOLS < β < βOLS /ρ2T y .

10

is subject to large measurement errors. We are unaware of any cases in which such a belief
has been confronted with these restrictions.11 In addition to bounding the possible amount
of measurement error, our assumptions also bound the instrument invalidity parameter ρuz ,
in spite of the fact that they place no restriction on ρT ∗ u .
Corollary 2.1 (Sharp Bounds for ρuz ). Under the conditions of Proposition 2.2, ρuz has a
√
non-trivial one-sided bound. If ρT y ρT z − κρzy < 0, then ρuz ∈ (−|ρT z |/ κ, 1). Otherwise
√
ρuz ∈ (−1, |ρT z |/ κ), where κ is defined in Proposition 2.2. These bounds are sharp.
Because κ > ρ2T z , Corollary 2.1 always rules out a range of values for ρuz . Notice, however,
that it never rules out ρuz = 0. This is unsurprising given that it is known to be impossible
to test for instrument validity in the model we consider here.
Together, Propositions 2.1 and 2.2 characterize the sharp identified set for ρuz , ρT ∗ u , and
κ showing us exactly how the problems of instrument invalidity, treatment endogeneity, and
measurement error are mutually constrained by each other and the data. From the identified
set for (ρuz , ρT ∗ u , κ) we can easily derive the identified set for any other parameters of the
model in Equation 3. In particular we can find the identified set for β, the main object
of interest to an applied researcher. Unfortunately, and perhaps unsurprisingly, the model
places no restrictions on the causal effect in spite of the bounds it yields for ρuz and κ.
Corollary 2.2 (No Restriction on β). Under the conditions of Proposition 2.2, the sharp
identified set for β is (−∞, ∞).
The only way to learn about β in this model is to impose beliefs. For example, the
standard IV identification assumption (belief) imposes ρuz = 0, which point identifies β. But
this belief could imply an implausible amount of measurement error or selection effect. This
fact highlights the central point of our analysis: our beliefs about ρuz are constrained by any
beliefs we may have about ρT ∗ u and κ. This observation has two important consequences.
First, it provides us with the opportunity to incorporate our beliefs about measurement
error and the endogeneity of the treatment to improve our estimates. Failing to use this
information is like leaving money on the table. Second, it disciplines our beliefs to prevent
us from reasoning to a contradiction. Without knowledge of the form of the identified set,
applied researchers could easily state beliefs that are mutually incompatible without realizing
it. Our analysis provides a tool for them to realize this and adjust their beliefs accordingly.
While we have thus far only discussed beliefs about ρuz , ρT ∗ u and κ, among other things, one
11

A recent contribution pointing out the usefulness of the R2 is Oster (2016). She points out that the use
of information about changes in the R2 is necessary to make inferences about the extent of omitted variable
bias when researchers perform robustness exercises that test for the stability of a coefficient estimate to the
inclusion of additional covariates.

11

could also work backwards from beliefs about β to see how they constrain the identified set.
We explore this possibility in one of our examples below.

2.4

Accommodating Exogenous Controls

At the beginning of Section 2 we assumed either that there were no control regressors or
that they had been projected out. Because the control regressors x are exogenous, this is
innocuous, as we now show. Without loss of generality, suppose that (T ∗ , z, y) are mean
zero or have been demeaned. Let (Te∗ , Te, ye, ze) denote the residuals from a linear projection
of the random variables (T ∗ , T, y, z) on x, e.g. Te∗ = T ∗ − ΣT ∗ x Σ−1
xx x and so on, where Σab
∗
is shorthand for Cov(a, b). Then, provided that (T , T, x, y, z) satisfy the model described
above, it follows that
ye = β Te∗ + u
Te∗ = πe
z+v
Te = Te∗ + w

(17)
(18)
(19)

since x is uncorrelated with u and w by assumption and uncorrelated with v by construction.
The parameters of this transformed system, β and π, are identical to those of the original
system, as are the error terms. And because the transformed system contains no covariates,
the analysis presented above applies directly. Projecting out covariates does, however, alter
the definition of the structural parameters. The equations for the identified set presented
above will involve not (ρuz , ρT ∗ u , κ) but their analogues for the transformed system, namely
κ
e = Var(Te∗ )/Var(Te)
ρee∗ = Cor(Te∗ , u)
T u

ρeuez = Cor(e
z , u).
All of the results derived above continue to apply to the transformed system; they simply refer
to (e
ρuez , ρeTe∗ u , κ
e). This is extremely convenient for both of the correlation parameters. In the
presence of covariates, ρeTe∗ u is the measure of treatment endogeneity over which researchers
are most likely to be able to state a belief because it is net of covariates. Similarly, ρeuez is
the natural measure of instrument invalidity because the usual exclusion restriction is a lack
of correlation between the instrument and the error term after accounting for the effect of
exogeneous controls. For this reason, we elicit researcher beliefs directly over ρeuez and ρeTe∗ u .
The situation is different for the measurement error parameter, κ. This quantity is not

12

defined relative to any causal model – it is simply a function of the signal-to-noise ratio for
the observed treatment T . Thus, irrespective of whether covariates are available, κ rather
than κ
e is the natural quantity over which to elicit researcher beliefs. Fortunately, there is a
simple mapping between κ and κ
e, namely
κ
e=

2
2
σT2 ∗ (1 − ΣT x Σ−1
κ − RT.x
σT2 ∗ − ΣT x Σ−1
xx ΣxT /σT ∗ )
xx ΣxT
=
=
2
2
σT2 − ΣT x Σ−1
σT2 (1 − ΣT x Σ−1
1 − RT.x
xx ΣxT
xx ΣxT /σT )

(20)

2
where RT.x
denotes the population R-squared from a regression of T on x.12 Equation 20
relates κ ≡ σT2 ∗ /σT2 for the original system to the analogue κ
e, purely in terms of an identified
2
quantity: RT.x . Thus, if a researcher states beliefs over κ, we can easily transform them to
the implied beliefs about κ
e simply by using the R-squared that results from the regression
that projects x out of T .
Since we can always reduce a problem with exogenous covariates to one without, and
because we can describe the mapping between the parameters that govern the identified set
of the original problem and those of the transformed system, we can easily accommodate
control variables in the framework derived above. In practice, one simply projects out x
before proceeding, using the R-squared from a regression of T on x to transform between κ
and κ
e.

3

Inference for a Continuous Treatment

Having characterized the identified set for this problem, we now describe how to use it to
carry out statistical inference on quantities of interest. Doing so requires us to tackle two
sources of uncertainty. First, while they must satisfy the restrictions given in Propositions
2.1 and 2.2, the true values of ρuz , ρT ∗ u , and κ are unknown. This source of uncertainty is
the lack of identification. Second, the covariance matrix Σ of the observables (T, y, z), which
pins down the relationship between (ρuz , ρT ∗ u , κ), must be estimated from data and is thus
subject to sampling uncertainty. We can handle these two sources of uncertainty separately
because our parameterization is transparent.13 In a transparent parameterization there are
two groups of parameters: “reduced form” and “structural.” The reduced form parameters,
denoted by ϕ, are directly identified by the data whereas the structural parameters, denoted
by θ, are not: the identified set Θ for θ depends on the data only through ϕ. Thus we write
θ ∈ Θ(ϕ). In the case of a continuous treatment subject to classical measurement error,
12
This expression has appeared elsewhere in the literature, see e.g. Dale and Krueger (2002). It follows
from the fact that ΣT ∗ x = ΣT x since w is classical measurement error.
13
See, e.g., Gustafson (2015).

13

ϕ = Σ while θ = (ρuz , ρT ∗ u , κ).
The approach we follow here is Bayesian, which makes the use of a transparent parameterization particularly convenient for inference. We proceed in two steps. First we generate
posterior draws ϕ(j) for the reduced form parameters. Each of these draws determines a
conditional identified set Θ(ϕ(j) ) for the structural parameters θ. Because this identified set
does not restrict β, producing meaningful inferences for the causal effect of interest requires
a second step in which we impose researcher beliefs on Θ(ϕ(j) ). The usual large-sample
equivalence between Bayesian posterior credible intervals and frequentist confidence intervals holds for ϕ, because the reduced form parameters are identified (Moon and Schorfheide,
2012; Poirier, 1998). This makes the first step uncontroversial. The second step, in contrast,
imposes researcher beliefs that can never be directly falsified by data. Nevertheless, our use
of a transparent parameterization makes clear precisely where any identification beliefs we
may choose to impose enter the problem: the data rule out certain values of ϕ, while our
beliefs amount to placing restrictions on the conditional identified set Θ(ϕ). Whenever one
imposes information beyond what is contained in the data, it is crucial to make clear how
this affects the ultimate result.

3.1

Inference for the Reduced Form Parameters

For the model described in Section 2, the first step of our inference procedure requires
producing posterior draws for the covariance matrix Σ of (T, y, z).14 Because inference for
this part of the problem is standard, the researcher can effectively “drop in” any procedure
that generates posterior draws for Σ. Here we propose two simple possibilities. The first is
based on a large-sample approximation that works well in sufficiently large samples. This
method conditions on T and z and incorporates sampling uncertainty in σT y and σzy only, by
applying the central limit theorem exactly as one does when deriving the frequentist large(j)
(j)
sample distribution of IV and OLS estimators. Specifically, we draw (σT y , σzy ) from a normal
distribution centered at the corresponding maximum likelihood estimates (b
σT y , σ
bzy ) with a
variance matrix estimated from the residuals we obtain by running two auxiliary regressions:
y on T and y on z. Details appear in Appendix A.2.1. An advantage of this approach is that
it is robust to heteroskedasticity without requiring us to model the conditional variance of
the errors. Although it does not involve an explicit prior and likelihood, one can view this
method as an approximation to a non-informative Bayesian analysis.
Unfortunately the large-sample approximation we have just outlined is not guaranteed
to produce positive definite draws for Σ. When the sample size is large this is extremely
14
For simplicity we suppress exogenous covariates throughout this section. If they are present we simply
project them out, as described in Section 2.4, and apply the methods described here to the resulting residuals.

14

unlikely to occur, but in small samples, such as our example from Section 4.1, this can be
problematic. A solution to this problem is to proceed in a fully Bayesian fashion rather than
using an approximation based on the Central Limit Theorem. There are many possible ways
to accomplish this. One simple method is to posit a joint normal likelihood for (T, y, z) and
place a Jeffrey’s prior on Σ, a benchmark noninformative prior that is often used in practice.
Under this model the marginal posterior for Σ is inverse Wishart.15 Draws Σ(j) produced in
this way are guaranteed to be positive definite. Notice that this approach models the joint
distribution of (T, z, y), which may seem odd given that the typical regression problem,
Bayesian or frequentist, models only the conditional distribution of y given T and z. This
is less of a concern in examples featuring a large number of exogenous control regressors.
These are projected out before proceeding, so we are in effect positing a normal distribution
only for the residuals of the regressions of (T, y, z) on x.

3.2

Inference for the Structural Parameters

Every draw Σ(j) from the first step of our inference procedure determines a conditional
identified set Θ(Σ(j) ) for the structural parameters (ρuz , ρT ∗ u , κ). We now discuss several
ways to summarize the information contained in Θ(Σ(j) ), proceeding from most conservative
to least. Whichever summary one chooses, the resulting inference is obtained by averaging
over the reduced form draws Σ(j) .
Recall from our discussion in Section 2 above that Σ restricts neither ρT ∗ u nor β. It does
however provide a lower bound for κ via Proposition 2.2. Computing this bound at each draw
Σ(j) provides posterior inference for the maximum amount of measurement error compatible
with our assumptions, given the data. One could proceed similarly for the one-sided bound
for ρuz using Corollary 2.1. Going beyond this, however, requires imposing beliefs.
Sign and interval restrictions on the degree of measurement error, treatment endogeneity,
and instrument invalidity are often straightforward to elicit in practice. By imposing such
restrictions we can add relatively weak prior information to the problem and restrict Θ
accordingly. In the discussion that follows we denote by R ⊂ (−1, 1) × (−1, 1) × (0, 1] a
user-imposed restriction on the domain of (ρuz , ρT ∗ u , κ). Incorporating beliefs in this way
has the potential to bound the treatment effect. Calculating the bounds implied by each
Θ(Σ(j) ) ∩ R provides posterior inference for the identified set for β under our beliefs over
(ρuz , ρT ∗ u , κ). Yet, even when Θ(Σ(j) ) ∩ R is not particularly informative about β, it can
iid

Specifically we suppose that (Ti , yi , zi ) ∼ N (µ, Σ) and place the prior π(µ, Σ) ∝ |Σ|−2 on the mean
vector and variance matrix. A standard calculation shows that the marginal posterior for Σ is Σ|T, y, z ∼
Inverse-Wishart(n − 1, S) where the scale matrix S equals n − 1 times the sample covariance matrix of
(T, y, z).
15

15

easily rule out a wide range of values for ρuz , ρT ∗ u and κ. For example, suppose a researcher
strongly believes that ρT ∗ u < 0. At a given draw Θ(Σ(j) ) this restriction could very easily rule
out ρuz = 0, as we see from Equation 13. Calculating the proportion of draws Σ(j) that are
compatible with ρuz = 0 gives the posterior probability of a valid instrument under the belief
that ρT ∗ u < 0. If one imposes beliefs over two or more of (ρuz , ρT ∗ u , κ), Θ(Σ(j) ) ∩ R could
even be empty for certain draws Σ(j) . Calculating the proportion of such empty identified
sets gives the posterior probability that our beliefs are mutually incompatible, given the
data. This illustrates an important general point of our approach. By making explicit the
relationship between measurement error, treatment endogeneity, and instrument invalidity,
our method allows researchers to learn whether their beliefs over these different dimensions
of the problem cohere.
An important advantage of the inferences we have described thus far is that, while
Bayesian, they can be given a valid frequentist interpretation under mild regularity conditions.16 This is because they do not impose a prior on the conditional identified set; they
merely intersect it with researcher beliefs, stated as interval restrictions. A more thoroughly
Bayesian treatment, on the other hand, will impose a fully-fledged prior on Θ(Σ(j) ), putting
the two sources of uncertainty – lack of identification, and sampling uncertainty in the reduced form parameters – on equal footing. Although more controversial because it can no
longer be given a frequentist interpretation, this approach has a key advantage: rather than
summarizing only the most extreme points of Θ(Σ(j) )∩R, it provides a more complete picture
by averaging over this set. Inferences that rely only on interval restrictions are necessarily
very sensitive to small changes in R and inherently pessimistic. In contrast, because any
reasonable prior will place only a small amount of probability density near the boundaries,
averaging over Θ(Σ(j) ) ∩ R can produce more robust inferences.
In most cases it will not be feasible to elicit a fully informative prior over the conditional
identified set. Its support, for example, changes with each draw Σ(j) . For this reason
we suggest an approach based on a conditionally uniform reference prior that gives equal
weight to regions of the support with equal area.17 Specifically, we draw uniformly over
the intersection of R with the manifold (ρuz , ρT ∗ u , κ) that describes the identified set for
instrument invalidity, treatment endogeneity, and measurement error, a two-dimensional
manifold embedded in three-dimensional space.18
While a uniform distribution seems like the natural choice for representing prior igno16

These conditions concern the first step of the procedure: inference for the reduced form parameters. See
Moon and Schorfheide (2012) and Kline and Tamer (2016).
17
Moon and Schorfheide (2012) likewise employ a conditionally uniform reference prior in their example
of a two-player entry game.
18
For details see Appendix A.2.2.

16

rance some caution is warranted: uniformity in one parameterization could imply a highly
informative prior in some different parameterization. This is unavoidable. We emphasize,
however, that the uniform serves here as a reference prior only. As such, one need not
take it completely literally but could instead consider, for example, what kind of deviation
from uniformity would be necessary to support a particular belief about β. We explore this
possibility in our examples below.

4

Examples with a Continuous Treatment

We now illustrate the methods proposed in Sections 2 and 3 using two examples drawn from
the applied literature. A summary of results appears in Table 1.

4.1

The Colonial Origins of Comparative Development

Acemoglu et al. (2001) study the effect of institutions on GDP per capita using a crosssection of 64 countries.19 Because institutional quality is endogenous, they use differences in
the mortality rates of early western settlers across colonies as an instrumental variable. We
consider their main specification
log GDP/capita = constant + β (Institutions) + u
Institutions = constant + π (log Settler Mortality) + v
which yields an IV estimate of 0.94 with a standard error of 0.16. This is nearly twice as
large as the corresponding OLS estimate of 0.52 with a standard error of 0.06. The authors
attribute this disparity to measurement error:
This estimate is highly significant . . . and in fact larger than the OLS estimates . . . This
suggests that measurement error in the institutions variables that creates attenuation
bias is likely to be more important that reverse causality and omitted variables biases.
(Acemoglu et al., 2001, p. 1385)

Acemoglu et al. (2001) state a two beliefs that are relevant for our partial identification
exercise. First, their discussion implies there is likely a positive correlation between “true”
institutions and the main equation error term u. This could arise from reverse causality –
wealthier societies can afford better institutions – or omitted variables, such as legal origin
or British culture, which are likely to be positively correlated with present-day institutional
19

Because the sample size is so small in this example, we generate posterior draws for Σ using the Jeffreys
Prior approach described in Section 3 to avoid non-positive definite draws.

17

18

0.19
(0.03)

0.49

-0.54

0.00

0.00
1.00

1.00

0.27

—

P(Valid)

0.10
[0.08, 0.12]
0.10
[0.08, 0.12]

—
[—, —]
-0.47
[−0.71, −0.25]

β

(II) Frequentist-Friendly

0.89
[0.75, 1.03]
0.59
[0.59, 0.59]

—
[—, —]
0.85
[0.70, 1.00]

β̄

0.21
[−0.12, 0.55]
0.08
[−0.13, 0.30]

—
[—, —]
-0.57
[−0.81, −0.15]

ρuz

0.31
[0.12, 0.54]
0.23
[0.10, 0.44]

—
[—, —]
0.49
[0.00, 0.93]

β

(III) Fully Bayesian

Table 1: Results for the empirical examples from Section 4. Panel (I) contains OLS and IV estimates and standard errors, and estimates
of the bounds for κ and ρuz from Proposition 2.2 and Corollary 2.1. If negative, the value in the column ρuz /ρ̄uz gives ρuz and ρ̄uz = 1;
otherwise it gives ρ̄uz and ρuz = −1. Panels (II) and (III) present posterior inferences under interval restrictions on (κ, ρT ∗ u ), as indicated
in the row labels. The column P(∅) gives the fraction of reduced form parameter draws that yield an empty identified set, while P(Valid)
gives the fraction of reduced form parameter draws compatible with a valid instrument (ρuz = 0). The remaining columns give posterior
medians accompanied by 90 percent highest posterior density intervals in square brackets. In Panel (II), β and β̄ report inferences for
the lower and upper boundaries of the identified set for β, obtained by averaging over the reduced form draws. In contrast, Panel (III)
reports fully Bayesian inference for β and ρuz under a uniform prior on the intersection between the restrictions and the conditional
identified set. See Section 3.2 for details.

(κ, ρT ∗ u ) ∈ (0.8, 1] × [−0.9, 0]

(κ, ρT ∗ u ) ∈ (0, 1] × [−0.9, 0]

Was Weber wrong? (n = 452)

0.10
(0.01)

-0.71

P(∅)

0.00

0.54

ρuz /ρ̄uz

(κ, ρT ∗ u ) ∈ (0.6, 1] × [0, 0.9]

0.94
(0.16)

κ

0.27

0.52
(0.06)

IV

(κ, ρT ∗ u ) ∈ (0, 0.6] × [0, 0.9]

Colonial Origins (n = 64)

OLS

(I) Summary Statistics

quality. We encode this belief using the prior restriction 0 < ρT ∗ u < 0.9 below, ruling out
only unreasonably large values of treatment endogeneity. Second, in a footnote that uses
an alternative measure of institutions as an instrument for the first, the authors argue that
measurement error could be substantial.20 Taken at face value, the calculations from this
footnote imply a point estimate of κ = 0.6 which would mean that 40 percent of the variation
in measured institutions is noise.21 Below we consider two alternative ways of encoding this
auxiliary information about κ.
Results for the Colonial Origins example appear in rows 1–3 of Table 1. Estimates
and bounds for β in these rows indicate the percentage increase in GDP per capita that
would result from a one point increase in the quality of institutions, as measured by average
protection against expropriation risk.22 All other values in the table are unitless: they are
either probabilities, correlations, or variance ratios. OLS and IV estimates and standard
errors, along with estimates of the lower bounds for κ and ρuz , appear in the first row of
Panel (I). The first column of Panel (II) gives the fraction of posterior draws for the reduced
form parameters that yield an empty identified set, while the second column gives the fraction
that are compatible with a valid instrument: ρuz = 0. Panel (III), along with the third and
fourth columns of Panel (II), present posterior medians and accompanying 90 percent highest
posterior density intervals. The results in Panel (II) are marked “Frequentist-Friendly”
because they do not involve placing a prior on the conditional identified set: they only
average over reduced form parameter draws under the restriction listed in the corresponding
row label.23 In contrast, those in Panel (III) are “Fully Bayesian”; they place a uniform prior
on the conditional identified set (see Section 3.2).
We first consider a prior under which 0.6 is an upper bound for κ and thus a lower bound
on the extent of measurement error.24 Under this restriction, approximately 27 percent of
the draws for the reduced form parameters Σ yield an empty identified set, as shown in the
first column of Panel (II). Intuitively, this means that there are covariance matrices Σ that
b but which rule out the region (κ, ρT ∗ u ) ∈ (0, 0.6] × [0, 0.9].
are close to the sample estimate Σ
The problem is not the restriction on ρT ∗ u but on κ: the data place no restrictions on the
20

See footnote #19 of Acemoglu et al. (2001).
Suppose T1 and T2 are two measures of institutions that are subject to classical measurement error:
T1 = T ∗ + w1 and T2 = T ∗ + w2 . Both T1 and T1 suffer from precisely the same degree of endogeneity,
because they inherit this problem from T ∗ alone under the assumption of classical measurement error.
Thus, the OLS estimator based on T1 converges to κ(β + σT ∗ u /σT2 ∗ ) while the IV estimator that uses T2 to
instrument for T1 converges to β + σT ∗ u /σT2 ∗ . The ratio identifies κ: 0.52/0.87 ≈ 0.6.
22
See Acemoglu et al. (2001) for a detailed explanation of this measure of institutions.
23
See Section 3 for details.
24
This interpretation comes from personal communication with one of the authors of Acemoglu et al.
(2001). Based on footnote 19 of the paper, he expressed the belief that at least 40 percent of the measured
variation in quality of institutions was likely to be noise.
21

19

extent of treatment endogeneity although they do provide an upper bound on the extent of
measurement error, as shown in Proposition 2.2 from above. Indeed, the proposed a priori
upper bound of 0.6 for κ is only slightly larger than our point estimate of 0.54 for κ. After
accounting for uncertainty in Σ, we find that 27 percent of the posterior density for κ lies
above 0.6. As such, our framework strongly suggests that the belief κ < 0.6 is incompatible
with the data, and we cannot proceed further under this prior.
Instead we consider a second candidate prior that takes 0.6 as a lower bound on κ
and thus an upper bound on the extent of measurement error. We continue to impose
ρT ∗ u ∈ [0, 0.9]. Results for this prior specification appear in the third row of Table 1. Unlike
the specification considered above, this prior does not yield empty identified sets, as we see
from the first column of Panel (II). It does however, strongly suggest that settler mortality
is an invalid instrument: 73 percent of the posterior draws for the reduced form parameters
Σ exclude ρuz = 0 under the restriction (κ, ρT ∗ u ) ∈ (0.6, 1] × [0, 0.9]. Figure 1a makes this
point in a slightly different way, by depicting the identified set for (κ, ρT ∗ u , ρuz ), evaluated
b of the reduced form parameters, in the region where ρT ∗ u is
at the maximum likelihood Σ
positive. The gray region corresponds to κ < κ < 0.6, the largest amount of measurement
b We see from the figure that the plane ρuz = 0 only intersects the
error consistent with Σ.
identified set in the region where measurement error is extremely severe. Moreover, unless
κ = κ, ρuz = 0 implies that ρT ∗ u must be close to zero, which would require that institutions
are approximately exogenous.
Moreover, under the prior (κ, ρT ∗ u ) ∈ (0.6, 1] × [0, 0.9] depicted in shades of red and
blue in Figure 1a, the identified set resides exclusively below the plane ρuz = 0, suggesting
that log settler mortality is negatively correlated with the unobservables in u. The Bayesian
posterior inference for ρuz in column one of Panel (III) shows that, even after accounting
for uncertainty in the reduced form parameters Σ, the sign of ρuz is still almost certainly
negative. The primary question of interest, of course, is not the validity of settler mortality
as an instrumental variable, but the causal effect of institutions on development. The colored
region in Figure 1a shows how κ, ρT ∗ u and ρuz map into corresponding values for β. Blue
indicates a positive treatment effect, red a negative treatment effect, and white a zero treatment effect. In both directions, darker colors indicate larger magnitudes. As seen from the
figure, we cannot rule out negative values for β. The posterior inference for the boundaries
of the identified set for β from columns 3–4 of Panel (II) tell the same story, while accounting
for sampling uncertainty in Σ: the highest posterior density interval for β is comfortably to
the left of zero, while that for β̄ is comfortably to the right of zero. Notice from Figure 1a,
b the identified set only implies negative values
however, that at least when evaluated at Σ,
for β when ρT ∗ u is extremely large and there is very little measurement error (κ is close
20

1.5

0.8

1.0

0.0

0.8

0.6
0.6

0.5

-0.5

0.7

ρT 0.4

0.8

κ

∗

u

0.2

0.9

0.0

0.6
0.4
0.2
0.0
-0.2

ρ uz

β

0.5

0.0 1.0

-1.0

-0.5

0.0

0.5

1.0

β

(a) Identified Set at MLE for Σ

(b) Posterior for Treatment Effect

Figure 1: Results for the Colonial Origins example from Section 4.1. Panel (a) plots the identified
set for (ρuz , ρT ∗ u , κ) evaluated at the maximum likelihood estimate for Σ in the region corresponding
to a positive selection effect: ρT ∗ u ∈ [0, 0.9]. The region in which 0.6 < κ < κ is shaded in gray while
the colors on the remainder of the surface correspond to the implied value of the treatment effect
β. Panel (b) gives the posterior for β under a uniform prior on the intersection of the restriction
(κ, ρT ∗ u ) ∈ [0.6, 1] × [0, 0.9] with the conditional identified set (see Section 3.2 for details). The
dashed red line gives the OLS estimate and the blue line the IV estimate.

to one). Because the posterior for β is determined entirely from these extreme points, the
resulting inference is very conservative, a concern that we raised above in Section 3.2. This
observation motivates the idea of averaging not only over reduced form draws Σ but also over
the conditional identified set itself, as we do in Figure 1b and the second column of Panel
(III), under a uniform reference prior. These results indicate that the conditional identified
sets for (κ, ρT ∗ u , ρuz ) do not contain more than a very small region in which β is negative.25
Indeed, the posterior median for β is 0.49, very close to the OLS estimate from Acemoglu
et al. (2001), while the corresponding 90 percent highest posterior density interval includes
only positive values. In spite of the likely negative correlation between settler mortality and
u under reasonable prior beliefs that accord with the data, the main result of Acemoglu et al.
(2001) continues to hold: it appears that the effect of institutions on income per capita is
almost certainly positive.
25

Because the prior is uniform, “small” refers to the relative area of a region on the identified set: in
Figure 1a, for example, the red region is small compared to the blue and white regions.

21

4.2

Was Weber Wrong?

Becker and Woessmann (2009) study the long-run effect of the adoption of Protestantism
in sixteenth-century Century Prussia on a number of economic and educational outcomes,
using variation across counties in their distance to Wittenberg – the city where Martin
Luther introduced his ideas and preached – as an instrument for the Protestant share of the
Population in the 1870s. Here we consider their estimates of the effect of Protestantism on
literacy, based on the specification
Literacy rate = constant + β (Protestant share) + x0 γ + u
Protestant Share = constant + π (Distance to Wittenberg) + x0 δ + v
where x is a vector of demographic and regional controls.26 Because this example includes
exogenous controls, we define treatment endogeneity and instrument invalidity net of these
covariates, as detailed in Section 2.4. To simplify the notation we write ρuz and ρT ∗ u rather
than ρeuz and ρeT ∗ u below but both of these should be understood as being net of x. In
contrast, κ is not defined net of covariates, again as detailed in Section, 2.4 so it continues
to refer to the ratio σT2 ∗ /σT2 below.
Becker and Woessmann (2009) express beliefs about the three key parameters in our
framework. First, their IV strategy relies on the assumption that ρuz = 0, an assumption
that we will relax below. Second, the authors argue that the 1870 Prussian Census is
regarded by historians to be highly accurate. As such, measurement error in the Protestant
share should be fairly small. Finally, Becker and Woessmann (2009) go through a lengthy
discussion of the nature of the endogeneity of the Protestant share, suggesting that it is most
likely that Protestantism is negatively correlated with the unobservables:
wealthy regions may have been less likely to select into Protestantism at the time of
the Reformation because they benefited more from the hierarchical Catholic structure,
because the opportunities provided by indulgences allured to them, and because the
indulgence costs weighted less heavily on them . . . The fact that “Protestantism” was
initially a “protest” movement involving peasant uprisings that reflected social discontent is suggestive of such a negative selection bias. (Becker and Woessmann, 2009, pp.
556-557)

Results for the “Was Weber wrong?” example appear in rows 4–6 of Table 1. Estimates
and bounds for β in these rows indicate the percentage point change in literacy that a county
26

In this exercise we include the controls listed in Section III of Becker and Woessmann (2009), specifically:
the fraction of the population younger than age 10, of Jews, of females, of individuals born in the municipality,
of individuals of Prussian origin, the average household size, log population, population growth in the
preceding decade, and the fraction of the population with unreported education information.

22

would experience if its share of Protestants were to increase by one percentage point. All
other values in the table are unitless: they are either probabilities, correlations, or variance
ratios. OLS and IV estimates and standard errors, along with the estimates of the lower
bounds for κ and ρuz , appear in row four of Panel (I). The first column of Panel (II) gives the
fraction of posterior draws for the reduced form parameters that yield an empty identified
set, while the second column gives the fraction that are compatible with a valid instrument:
ρuz = 0. Panel (III), along with the third and fourth columns of Panel (II), present posterior
medians and accompanying 90 percent highest posterior density intervals. The results in
Panel (II) are marked “Frequentist-Friendly” because they do not involve placing a prior on
the conditional identified set: they average only over reduced form parameter draws under
the restriction listed in the corresponding row label.27 In contrast, those in Panel (III) are
“Fully Bayesian” in that they place a uniform prior on the conditional identified set.
As we see from Table 1, Becker and Woessmann (2009) obtain an OLS estimate of 0.10
and an IV estimate that is nearly twice as large: 0.19 with a standard error of 0.03. If the
instrument is valid, this corresponds to just under a 0.2 percentage point increase in literacy
from each percentage point increase in the prevalence of Protestantism in a given county. The
estimated lower bound for κ in this example is just under a half, which means that at most 50
percent of the measured variation in the Protestant share can be attributed to measurement
error. Notice that this bound is somewhat weak: it allows for far more measurement error
than one might consider reasonable given the author’s arguments concerning the accuracy
of the Prussian census data.
Figure 2a depicts the identified set for (κ, ρT ∗ u , ρuz ) evaluated at the maximum likelihood
estimate of Σ. As above, the surface is colored to indicate the corresponding value of β: blue
indicates a positive treatment effect, red a negative effect, and zero no effect. In both
directions, darker colors indicate larger magnitudes. We see immediately from the figure,
that unless ρT ∗ u is large and negative, the treatment effect will be positive, irrespective of the
amount of measurement error. The rectangular region surrounded by thick black boundaries
indicates our approximation to the prior beliefs of Becker and Woessmann (2009): positive
selection, and measurement error that is not too severe. This area is well within the blue
region, corresponding to a positive treatment effect. Although it is somewhat harder to see
from the figure, the region enclosed in the black boundary also contains ρuz = 0. The belief
that ρT ∗ u < 0 and measurement error is modest indeed appears to be compatible with a
valid instrument in this example.
Although the substance of this example is apparent from Figure 2a, merely examining
the identified set evaluated at the MLE is insufficient, as it fails to account for uncertainty
27

See Section 3 for details.

23

0.8

4

0.6

3

ρuz
0.0

2

0.2

β

0.4

0.5

1

0.0

0.6
-0.5

0.8
0.5

ρ T∗ u

0

-0.2

κ

0.0
1.0

0.1

0.2

0.3

0.4

0.5

0.6

β

(a) Identified Set at MLE for Σ

(b) Posterior for Treatment Effect

Figure 2: Results for the “Was Weber Wrong?” example from Section 4.2. Panel (a) plots the
identified set for (ρuz , ρT ∗ u , κ) evaluated at the maximum likelihood estimate for Σ. The color of
the surface corresponds to the implied value of the treatment effect β. Panel (b) gives the posterior
for β under a uniform prior on the intersection of the restriction (κ, ρT ∗ u ) ∈ [0.8, 1] × [−0.9, 0]
with the conditional identified set (see Section 3.2 for details). The dashed red line gives the OLS
estimate and the blue line the IV estimate.

in the reduced form parameters Σ. Row six of Table 1 completes our analysis by providing
Bayesian inference for the Weber example under the prior indicated by the black boundary
in Figure 2a: κ < 0.8 and −0.9 < ρT ∗ u < 0. In this example one need not even consult the
fully Bayesian results from Panel (III): the identified set for β comfortably excludes zero, as
we see from columns 3–4 of Panel (II). Indeed, the posterior median for the lower bound for
β equals the OLS estimate which already implies a substantial causal effect of Protestantism
on literacy. This is related to the fact that, as we see from columns 1–2 of the same panel,
100 percent of the reduced form draws for this prior yield an identified set that contains
ρuz = 0. Similarly, the fully Bayesian inference for ρuz in Panel (III) yields a point estimate
of 0.08 and a fairly tight highest posterior density interval to accompany it. If we wish
to report a point a point estimate for β, the posterior median from our uniform reference
prior in the second column of Panel (III) suggests that the IV estimate is approximately
correct, although the highest posterior density interval is skewed somewhat towards even
larger causal effects. Moreover, none of these results is sensitive to the restriction κ < 0.8,
as we see from row five of Table 1 which imposes only −0.9 < ρT ∗ u < 0. In this example,
the authors beliefs are mutually consistent and their result is extremely robust.
24

5
5.1

The Case of a Binary Treatment
Model and Assumptions

Although the logic of our approach from above is general, our characterization of the identified set does not apply when the treatment of interest is binary. This is because, as we
mentioned in the introduction, a binary instrument cannot be subject to classical measurement error. Accordingly, our characterization of the identified set for this common setting
will require a different approach. Let T , T ∗ , and z be binary variables.28 We continue to
allow for treatment endogeneity and instrument invalidity: both T ∗ and z are potentially
correlated with u. For convenience we will absorb the intercept into the error term u as
follows
y = βT ∗ + u

(21)

u=c+ε

(22)

where ε is mean zero but u may not be. For simplicity, we begin by assuming that there are no
covariates. In Section 5.4 we show how to account for the effect of covariates by transforming
the geometry of the problem.29 This is important for elicitation because researcher beliefs
over treatment endogeneity and instrument invalidity are typically conditional on covariates.
5.1.1

Non-differential Measurement Error

Since T and T ∗ are both binary, measurement error is governed by two conditional probabilities:
α0 = P(T = 1|T ∗ = 0)

(23)

α1 = P(T = 0|T ∗ = 1)

(24)

These mis-classification probabilities replace κ from the case of a continuous treatment. As
mentioned above, it is impossible for a binary regressor to be subject to classical measurement
error: the true value T ∗ must be negatively correlated with the measurement error w. To
see why, first note that since T and T ∗ are both binary, w = T − T ∗ can only take on values
28

If a continuous instrument is available, it can always be binarized.
Observe that this is different from our treatment of the continuous treatment case. There we could
simply project out any exogenous covariates from all other observables.
29

25

in the set {−1, 0, 1}. The conditional distribution of w given T ∗ is as follows:
(

T = 0 with prob. 1 − α0
T = 1 with prob. α0

⇐⇒ w = 0
⇐⇒ w = 1

(

T = 0 with prob. α1
T = 1 with prob. 1 − α1

⇐⇒ w = −1
⇐⇒ w = 0

T ∗ = 0 =⇒

T ∗ = 1 =⇒

Hence E[w|T ∗ = 0] = α0 , while E[w|T ∗ = 1] = −α1 . Since classical measurement error is impossible, we assume instead that the measurement error is non-differential, the
closest assumption to classical measurement error in the context of a binary treatment.
Non-differential measurement error requires that w be conditionally independent of all other
random variables in the system given knowledge of true treatment status T ∗ . Consider,
for example, self-reports of smoking behavior. The non-differential measurement error assumption allows for the possibility that smokers are more likely to mis-represent their true
smoking status than nonsmokers. After controlling for true smoking status, however, it rules
out any relationship between measurement error and the instrument, as well as any other
unobserved characteristics that determine the outcome y. The precise assumption we use
below takes the following form:
Assumption 5.1 (Non-differential Measurement Error).
(i) E [ε|T, T ∗ , z] = E [ε|T ∗ , z], E [ε2 |T, T ∗ , z] = E [ε2 |T ∗ , z]
(ii) P(T = 1|T ∗ , z) = P(T = 1|T ∗ )
Because we only work with first and second moments of the observables, Assumption 5.1,
rather than full conditional independence, suffices. We also impose an assumption about the
extent of measurement error, which is standard in the literature on mis-classified binary
regressors.
Assumption 5.2 (Extent of Measurement Error). Assume that α0 + α1 < 1.
As shown in Lemma B.3 , Cov(T, T ∗ ) = (1 − α0 − α1 )Var(T ∗ ) so Assumption 5.2 amounts
to asserting that T and T ∗ are positively correlated, or equivalently that the misclassification is “not so bad . . . that the effective definition of the classification has been reversed”
(Bollinger, 1996, p. 389).
Assumptions 5.1 and 5.2 have two implications that contrast sharply with those of the
classical measurement error case from above. While these have been known in the literature
for some time, they do not appear to be very widely appreciated. First, while the IV estimator
26

is unaffected by classical measurement error (Equation 7), it is affected by non-differential
measurement error. In particular,
βIV =

σzu
β
+
.
1 − α0 − α1 σzT

(25)

as explained in Lemma B.7. Indeed, under a valid instrument βIV is necessarily an overestimate of β: the opposite of the familiar OLS attenuation bias logic for the case of classical
measurement error.30 Second, under non-differential measurement error it is no longer true
that Var(T ∗ ) ≤ Var(T ). Let p = P(T = 1) and p∗ = P(T ∗ = 1). By the law of total
probability,
(p − α0 )(1 − p − α1 )
σT2 ∗ = Var(T ∗ ) = p∗ (1 − p∗ ) =
(26)
(1 − α0 − α1 )2
whereas σT2 = Var(T ) = p(1 − p). The probability limit of the OLS estimator in this case is
accordingly more complicated. In particular,
βOLS



σT ∗ u
σT2 ∗
= 2 β (1 − α0 − α1 ) + 2
σT
σT ∗

(27)

as explained in Lemma B.9. Again, contrast this with the case of classical measurement
error from Equation 6 from above. Although this is not immediately apparent from the form
of Equation 27, if σT ∗ u = 0 then OLS is attenuated towards zero whenever α0 + α1 < 1.31
5.1.2

Notation: Observables and Unobservables

Unlike their counterparts for the continuous case, Equations 25 and 27 do not allow us to
recover β even if both the instrument and regressor are exogenous. This is because they do
not incorporate all information contained in the data for the binary case. Kane et al. (1999),
Black et al. (2000) and Frazis and Lowenstein (2003) show, however, that if the instrument
and regressor are jointly exogenous then β can be consistently estimated via a method of
moments approach that uses strictly more information than is contained in the OLS and IV
estimators.32 Although we do not assume that the treatment and instrument are exogenous,
a full characterization of the identified set relies on the additional information exploited by
these method of moments estimators.
30

Without Assumption 5.2, βIV could have the wrong sign even if the instrument is valid.
To see why note that, by Lemma B.1, p∗ = (p − α0 )/(1 − α0 − α1 ) and 1 − p∗ = (1 − p − α1 )/(1 − α0 − α1 ).
Since both p∗ and 1 − p∗ must be positive, α0 + α1 < 1 implies α0 < p and α1 < 1 − p. After expanding, the
term that multiples β in Equation 27 equals [p(1 − p) − pα1 − α0 (1 − p)]/[p(1 − p) − p(1 − p)α1 − α0 p(1 − p)].
The result follows since the second term in the numerator is greater than the second term in the denominator
and the same holds for the third terms.
32
The required condition is slightly stronger than σT ∗ u = σzu = 0. It is in fact E[ε|T, z] = 0.
31

27

(a) Observables

z=0
T =0

ȳ00
2
σ00

(b) Unobservables

z=1
ȳ01
2
σ01

p00
T =1

ȳ10
2
σ10

T∗ = 0

T∗ = 1

m∗10
s∗2
10

p01
ȳ11
2
σ11

p10

z=0
m∗00
s∗2
00

p11

p∗00
p∗10

z=1
m∗01
s∗2
01
m∗11
s∗2
11

p∗01
p∗11

2 = Var(y|T = t, z = k).
Table 2: We observe ptk = P(T = t, z = k), ȳtk = E[y|T = t, z = k], and σtk
∗
∗
∗
∗2
In contrast, ptk = P(T = t, z = k), mtk = E[u|T = t, z = k], and stk = Var(u|T ∗ = t, z = k) are
unobservables.

The simplest way to incorporate this additional information is to work with the joint
probability distribution of (z, T ) and the conditional means ȳtk ≡ E[y|T = t, z = k] for
t, k ∈ {0, 1} as depicted in Table 2a. First, define p∗k = P(T ∗ = 1|z = k) and pk =
P(T = 1|z = k). Although p∗k is unobserved, it is related to the observed probability pk by
p∗k = (pk − α0 )/(1 − α0 − α1 ) as shown in Lemma B.1. This means that p∗k is observed up
to knowledge of α0 , α1 so we need not consider it a separate unknown. Now, as shown in
Lemma B.11, the observed conditional means can be related to the unobservable ones by
ye0k ≡ (1 − pk )ȳ0k = (β + m∗1k )α1 p∗k + (1 − α0 )(1 − p∗k )m∗0k

(28)

ye1k ≡ pk ȳ1k = (β + m∗1k )(1 − α1 )p∗k + α0 (1 − p∗k )m∗0k

(29)

where m∗tk ≡ E[u|T = t, z = k] for t, k ∈ {0, 1}, as depicted in Table 2b. Because of the
mis-classification, each of the means in Table 2a contains a mixture of treated and untreated
individuals, depending on the values of α0 and α1 . The expression for ȳ00 , for example,
involves not only m∗00 but also β and m∗10 .
In addition to conditional means, ȳtk , we assume that conditional variances of the outcome
2
σtk = Var(y|T = t, z = k) are likewise observed. This information has not been used in
the existing literature because, under joint exogeneity of the instrument and treatment,
one obtains point identification from conditional means alone. Without this assumption
the model is unidentified, and conditional variance information becomes useful. Let s∗2
tk =
∗
Var(u|T = t, z = k). Equations B.13 and B.15 in the Appendix are the counterparts of
2
Equations 28 and 29 for second moments: they relate the observable variances σtk
, depicted
∗2
in Table 2a to the unobservable variances stk depicted in Table 2b. As we show below, the
restriction s∗2
tk > 0 will allow us to tighten our bounds for the mis-classification probabilities.

28

5.2

A Convenient Parameterization

While the m∗tk provide a very convenient way of expressing how misclassification pollutes
the conditional means of y, they depend simultaneously on both the extent of treatment
endogeneity and instrument invalidity. The assumption of an exogenous treatment, σT ∗ u = 0,
is equivalent to
X
1
p∗ m∗ = c
P(T ∗ = t) k tk tk
for t = 0, 1 while that of an exogenous instrument, σzu = 0, is equivalent to
(1 − p∗k )m∗0k + p∗k m∗1k = c
for k = 0, 1 where c is the constant term from Equation 22.33 This shows that the objects
over which researchers often hold and express beliefs – treatment exogeneity and instrument
invalidity – are not the m∗tk themselves, but rather certain functions of them. For this reason,
in the case of a binary treatment and instrument it is more natural to elicit researcher beliefs
in terms of the following quantities
δT ∗ ≡ E[u|T ∗ = 1] − E[u|T ∗ = 0]
δz ≡ E[u|z = 1] − E[u|z = 0]

(30)
(31)

The first, δT ∗ , measures the average difference in unobservables between the treated and untreated; the second, δz , measures the average difference in unobservables between those with
the high value of the instrument and those with the low. Both quantities are linear functions
of m∗tk with coefficients that depend on α0 , α1 , and observables, as shown in Lemma B.12.
Although δT ∗ and δz are not scale-free, both are empirically meaningful and conveniently
measured in units of y. For example, consider the smoking cessation randomized controlled
trial studied in Courtemanche et al. (2016) where z is the randomized offer to participate in
a smoking cessation program, T ∗ is an indicator of true smoking cessation, T is self-reported
smoking cessation, and y is body-mass index (BMI). Here δT ∗ is the selection effect. For
example, those who succeed in quitting smoking are likely more health-conscious overall.
We would expect them to have a lower BMI on average even if they had not quit smoking,
leading to a negative value of δT ∗ . We would also expect a knowledgeable obesity researcher
to be able to put a lower bound on δT ∗ . But what about δz ? Courtemanche et al. (2016)
point out that, in spite of being randomized, the offer to participate in a smoking cessation
33

Without covariates, the exogeneity assumption used by Kane et al. (1999), Black et al. (2000), Frazis
and Lowenstein (2003) is equivalent to m∗tk = c, for t, k ∈ {0, 1}.

29

program may have a direct effect on BMI, making it an invalid instrument. For example,
those who participate in the smoking cessation program may be led to smoke less even if
they fail to quit entirely. Because nicotine is an appetite suppressant, this would likely lead
to a positive value of δz . We would also expect our researcher to be able to provide at least
a reasonable order of magnitude for δz , although in most applications of our framework,
researchers will likely prefer to compute the value of δz consistent with their other beliefs
rather than the reverse.
In addition to δT ∗ and δz , the identified set will also depend on α0 and α1 . Fortunately,
both of these quantities are probabilities so they are already directly intelligible, unitless,
and bounded.

5.3

Deriving the Identified Set for (δT ∗ , δz , α0 , α1 )

We begin by deriving the relationship between δT ∗ , δz , α0 and α1 , the objects over which we
can elicit researcher beliefs, by eliminating m∗tk and β. The derivation proceeds in two steps.
We first manipulate Equations 28 and 29 to yield an expression for m∗10 − m∗11 that depends
only on observables and α0 . Combining this with the definitions of δT ∗ and δz in terms of m∗tk
from Lemma B.12 gives an overdetermined linear system of three equations in (m∗10 , m∗11 )
given (α0 , α1 , δT ∗ , δz ) and observables. Solving to eliminate m∗10 and m∗11 we derive a linear
relationship between δz and δT ∗ given α0 , α1 and observables.
Proposition 5.1. Under Assumption 5.1
δz = B(α0 , α1 ) + S(α0 , α1 )δT ∗ .

(32)

where
g(α1 ) − (p0 − p1 )h (α1 ) (p0 − α0 )(p1 − α0 )∆(α0 )
−
1 − α0 − α1
(p − α0 )(1 − α0 − α1 )
p1 − p0
S(α0 , α1 ) =
,
1 − α0 − α1

B(α0 , α1 ) =

and g, h, and ∆ are simple functions of α0 , α1 and observables defined in the proof.
Note that the slope of the relationship between δz and δT ∗ is directly proportional to the
strength of the instrument: p1 − p0 . The mis-classification probabilities, on the other hand,
enter in nonlinear fashion in both the slope and intercept.
Proposition 5.1 allows us to express δz in terms of δT ∗ , α0 , α1 and the observable probabilities and conditional means from Table 2a. Thus, to fully characterize the relationship
between instrument invalidity, treatment endogeneity, and measurement error it suffices to
30

derive the sharp identified set for δT ∗ , α0 and α1 . Just as we were able to construct bounds
for κ in the case of classical measurement error, we can bound the mis-classification error
rates α0 and α1 in the binary regressor case.
Proposition 5.2 (Sharp Identified Set for δT ∗ , α0 , and α1 ). Suppose that s∗2
tk > 0 for all
t, k. Then, under Assumptions 5.1 and 5.2, (δT ∗ , α0 , α1 ) ∈ (−∞, ∞) × [0, ᾱ0 ) × [0, ᾱ1 ) where


ᾱ0 = min ᾱ0k , ᾱ1 = min f0k (ᾱ0k ) ,
k

k

ᾱ0k is the smallest solution to f0k (α0 ) = f1k (α0 ), and
f0k (α0 ) =

2
(pk − α0 )σ0k
2
(pk − α0 )σ0k
+ (ȳ1k − ȳ0k )2 p2k (1 − α0 )

2
(1 − pk )(pk − α0 )σ1k
− (ȳ1k − ȳ0k )2 (1 − pk )2 α0
f1k (α0 ) =
2
(pk − α0 )σ1k
− (ȳ1k − ȳ0k )2 (1 − pk )2 α0

(33)
(34)

for k = 0, 1. These bounds are sharp.
The result of Proposition 5.2 is analogous to that of Proposition 2.2 for the continuous
treatment case; in each case we obtain a non-trivial upper bound on the extent of measurement error, but no restriction on the extent of treatment endogeneity. The proof in the
binary case proceeds by showing that α0 < ᾱ0 and α1 < ᾱ1 is equivalent to s∗2
tk > 0 for all
t, k = 0, 1. As a result, the variance information places no restrictions on m∗tk . Since the
conditional mean information from Equations 28 and 29 also places no restrictions on m∗tk ,
it follows that δT ∗ is unbounded.
Figure 3 illustrates how to construct the bounds for α0 and α1 , using data from one of
our empirical examples below. The region in which Assumption 5.2 holds (α0 + α1 < 1) is
shaded in light gray. Under this assumption, the equality p∗k = (pk −α0 )/(1−α0 −α1 ) implies
that α0 ≤ mink {pk } and α1 ≤ mink {1 − pk }, as shown in Lemma B.14. These bounds, which
do not incorporate the information contained in the conditional variances of y, are depicted
in light blue. The sharp bounds, depicted in dark blue, are determined by the intersection
of f11 with f01 and f10 with f00 . Each point of intersection provides a bound for both α0 and
α1 . Since all of these bounds must hold simultaneously, however, only the smallest of each
binds. In Figure 3 the intersection of f10 with f00 determines the binding constraint for α1
while the intersection of f11 with f01 determines the binding constraint for α0 .
Finally, as in the continuous treatment case, the assumptions of our model place no
restrictions on β.
Corollary 5.1. Under the Assumptions of Proposition 5.2, the sharp identified set for β
31

1
f10

α1

ᾱ10
f00
f11
ᾱ11

f01

0
0 ᾱ00

ᾱ01

1

α0
Figure 3: The identified set for (α0 , α1 ) from Proposition 5.2. The region where Assumption
5.2 is satisfied (α0 + α1 < 1) is depicted in light gray and the weak bounds α0 < mink {pk },
α1 < mink {1 − pk } are shown in light blue. The region in dark blue gives the sharp bounds
α0 < mink {ᾱ0k } and α1 < min{ᾱ1k }, where (ᾱ0k , ᾱ1k ) is the intersection of f0k with f1k , as defined in
Proposition 5.2. This figure is uses estimates of the observables from the example in Section 6.1.

and δz are both (−∞, ∞).
The careful reader may wonder whether an analogue of the condition that Ω is positive
semi-definite, from Assumption 2.1 in the continuous treatment case, provides any additional
restrictions when the treatment is binary. The answer is no. Because T ∗ , z, w and the
analogue of v are all discrete, the analogue of Ω in the binary case is guaranteed to be positive
semi-definite provided that all probabilities are between zero and one, and all conditional
variances of y are positive.34
Now that we have the identified set for the case of a binary treatment, we can proceed
in the same way as we did for the continuous treatment case described above in Section
2. In particular, we can intersect researcher beliefs over measurement error, treatment endogeneity and instrument invalidity with the identified set itself to check whether these
beliefs are mutually consistent given the data and, if so, harness them to learn about the
treatment effect. As in the continuous treatment case, each point on the identified set implies a corresponding value for the treatment effect β. A convenient way to compute this
34

Let η be a random vector, ξ be a binary random variable, and define Ω = Var(η), Ω0 = Var(η|ξ = 0),
and Ω1 = Var(η|ξ = 1). A straightforward calculation shows that if Ω0 and Ω1 are positive semi-definite, so
is Ω. In our case, we simply apply this fact recursively to condition on both T ∗ and z.

32

value is to use the IV probability limit from Equation 25. Since σzu /σzT = δz /(p1 − p0 ),
β = (1 − α0 − α1 )[βIV − δz /(p1 − p0 )] as explained in Lemma B.10.

5.4

Accommodating Exogenous Covariates

In the presence of covariates we redefine u from Equation 22 as
u = c + x0 γ + ε

(35)

and the following replaces Assumption 5.1:
Assumption 5.3 (Non-differential Measurement Error with Exogenous Covariates).
(i) E [x|T, T ∗ , z] = E [x|T ∗ , z]
(ii) E [ε|T, T ∗ , z] = E [ε|T ∗ , z], E [ε2 |T, T ∗ , z] = E [ε2 |T ∗ , z]
(iii) P(T = 1|T ∗ , z) = P(T = 1|T ∗ ).
Assumption 5.3 allows us to employ the results from above in the presence of exogenous
covariates: the error term u is merely re-defined to make explicit the role of x. In principle we
could continue to express the identified set in terms of δT ∗ and δz with the understanding that
they refer to a u with a slightly different meaning. In practice, however, researchers’ beliefs
about regressor endogeneity and instrument validity are likely to be conditional on covariates.
Because the point is to study the effect of T ∗ net of x, the more natural objects over which
to elicit researcher beliefs regarding treatment endogeneity and instrument invalidity are
δez ≡ E[ε|z = 1] − E[ε|z = 0]
δeT ∗ ≡ E[ε|T ∗ = 1] − E[ε|T ∗ = 0].

(36)
(37)

Unlike the continuous treatment case, here we cannot simply project x out of the system if
we wish to work with the information contained in the four cells from Table 2a. Instead, we
now show how to re-express the identified set from the preceding section in terms of δeT ∗ and
δez rather than δT ∗ and δz .
As shown in Lemma B.22, we can relate δeT ∗ to δT ∗ and δez to δz using the following
expressions:
δz = [E (x|z = 1) − E (x|z = 0)]0 γ + δez
δT ∗ =

p(1 − p)(1 − α0 − α1 )
[E (x|T = 1) − E (x|T = 0)]0 γ + δeT ∗
(p − α0 )(1 − p − α1 )
33

(38)
(39)

If γ were known, these expressions would immediately allow us to re-write the identified set
as desired. The problem, of course, is that γ is unknown. By an argument related to that
of Frazis and Lowenstein (2003, p. 158), we show in Lemma B.1 that the probability limits
of the IV estimators for β and γ are
"

βIV
γ IV

#

"
=

β/(1 − α0 − α1 )
γ

#

"
+ δez q(1 − q)

σ zT
σ xT

#
(40)

where q = P(z = 1) and σ zT and σ xT , defined in the statement of Lemma B.1, depend only
on covariances of the observables (z, T, x). Rearranging Equation 40, we can write γ solely
in terms of observable quantities and δez , namely γ = γ IV − δez q(1 − q)σ xT . Using this fact,
we can eliminate γ from Equations 38 and 39. After doing so, both equations involve δez but
the relationship is linear. Accordingly, using Lemma B.12, we obtain a linear relationship
between δez and δeT ∗ . As shown in Lemma B.22,
e 0 , α1 ) + S(α
e 0 , α1 )δeT ∗
δez = B(α

(41)

e and Se are functions of α0 , α1 and reduced form parameters defined in the Lemma.
where B
Thus, in the presence of exogenous covariates Equation 41 replaces Equation 32 and we
calculate the same bounds for α0 and α1 as given in Proposition 5.2.35

5.5

Inference for a Binary Treatment

Our inference procedure for a binary treatment closely parallels the continuous treatment
case described in Section 3, so we describe here only the differences. As above, we rely
upon a transparent parameterization. In the binary case, the structural parameters are θ =
(δT ∗ , δz , α0 , α1 ), while the reduced form parameters, ϕ, are the joint probability distribution
of (z, T ) along with the conditional means and variances of y given z and T , as shown in
Table 2a. We again propose a simple method for generating posterior draws for the reduced
from parameters that matches the usual large-sample frequentist treatment of estimation
error in IV and OLS regression. Accordingly, we condition on z and T and incorporate
sampling uncertainty in the conditional means of y only, applying the central limit theorem.
In the presence of covariates, we also require posterior draws for γ
bIV . These must be made
35
In the presence of covariates, the bounds for α0 and α1 from Proposition 5.2 are technically no longer
sharp, as one could in principle exploit the additional information contained in x to tighten them. If x is
discrete and sufficient data are available, the sharp bounds can be obtained as follows: simply apply our
bounds separately at every value in the support of the covariates and report the tightest. When one or more
covariates are continuous, as is the case in each of our examples, one would need to model the first-stage
relationship between x and T ∗ , which we prefer to avoid here.

34

jointly with those for the conditional means of y as they are necessarily correlated. Appendix
B.2 describes in detail how to make draws in this fashion, again appealing to a large-sample
approximation based on the central limit theorem. Using these reduced form draws, we can
conduct the same inference exercises for the structural parameters described in Section 3.
The only difference is that there are now four rather than three elements in θ. As in the case
of a continuous treatment we consider fully Bayesian inference under a uniform prior on the
intersection of the conditional identified set and the any user restrictions R.36 We elaborate
further in our discussion of the empirical examples presented below.

6

Examples with a Binary Treatment

We now illustrate the methods proposed in Section 5 using two empirical examples with a
binary treatment and instrument. A summary of results appears in Table 3.

6.1

Afghan Girls RCT

Burde and Linden (2013) study the effect of village schools on the academic performance of
children in rural northwestern Afghanistan, using data from a randomized controlled trial.
Both test scores and reported enrollment rates increased significantly in villages that were
randomly allocated to receive a school compared to those that were not. The effects were
particularly striking for girls, whose enrollment increased by 52 percentage points and test
scores by 0.65 standard deviations. Both effects are statistically significant at the 1 percent
level and remain essentially unchanged after controlling for a host of demographic covariates.
These results quantify the causal effect of establishing a school in a rural village. But the
data from Burde and Linden (2013) are rich enough for us to pose a more specific question
that the authors do not directly address in their paper: what is the causal effect of attending
a village school on the test scores of Afghan girls? With school enrollment as our treatment
of interest, the 0.65 standard deviation increase in test scores becomes an intent to treat
(ITT) effect, while the 52 percent increase in reported enrollment becomes an IV first stage.
In this example we consider the specification
Test score = constant + β (Enrollment) + x0 γ + ε
and instrument enrollment using the experimental randomization: Girls in a village where
36

Because the geometry of the problem is slightly more complex in the binary case, however, we employ
a slightly different method of making the uniform draws. Although this makes little difference in practice it
is more convenient computationally. For details, see https://github.com/binivdoctr.

35

36

2.31
(0.12)

0.86
(0.06)

2.70
(0.65)

1.30
(0.12)

IV

0.16

0.10

ᾱ0

0.43

0.12

ᾱ1

−0.39
[−1.56, 0.60]
−0.60
[−0.77, −0.37]

−0.39
[−0.53, −0.24]
−0.03
[−0.11, 0.04]
−0.04
[−0.11, 0.04]

δ T ∗ /z

1.11
[0.27, 2.07]
0.06
[−0.13, 0.28]

0.11
[0.03, 0.20]
0.72
[0.64, 0.80]
0.73
[0.65, 0.82]

δ̄T ∗ /z

2.27
[1.42, 3.30]
2.37
[2.18, 2.60]

1.03
[0.86, 1.18]
−0.22
[−0.38, −0.07]
−0.24
[−0.39, −0.08]

β

(II) Frequentist-Friendly

2.72
[1.73, 3.98]
4.80
[4.55, 5.09]

1.29
[1.06, 1.45]
1.10
[0.95, 1.27]
1.11
[0.96, 1.26]

β̄

0.27
[−0.96, 1.50]
−0.22
[−0.53, 0.09]

−0.14
[−0.36, 0.06]
0.40
[0.09, 0.69]
0.40
[0.10, 0.71]

δT ∗ /z

2.49
[1.47, 3.58]
3.51
[2.57, 4.37]

1.16
[0.95, 1.36]
0.42
[−0.14, 0.97]
0.41
[−0.15, 0.96]

β

(III) Fully Bayesian

Table 3: Results for the empirical examples from Section 6. Panel (I) contains OLS and IV estimates and standard errors along with
estimates of the upper bounds α0 and α1 from Proposition 5.2. Panels (II) and (III) present posterior medians and 90 percent highest
posterior density intervals under the restrictions on α0 , α1 , δT ∗ and δz indicated in the row labels: e.g. a row marked δz = 0 assumes that
the instrument is valid but does not restrict α0 , α1 or δT ∗ u . The restriction p = p∗ in row four imposes α1 = α0 (1 − p)/p. In columns 1–2
of (II) and 1 of (III), the subscript T ∗ /z indicates that we report inference for δT ∗ when δz is restricted a priori and vice-versa. In a row
marked δz = 0, these columns report inference for δT ∗ ; in a row marked δT ∗ ∈ [a, b] they report inference for δz . As in Table 1, Panel (II)
reports inferences for the boundaries of the identified sets for β, δT ∗ and/or δz while Panel (III) reports fully Bayesian inference under a
uniform prior on the intersection between the restrictions and the conditional identified set. See Sections 3.2 and 5.5 for details.

δT ∗ ∈ [−1.5, 0], α1 = 0

δz = 0, α1 = 0

Smoking & BMI (n = 5446)

δT ∗ ∈ [0, 1], p = p∗

δT ∗ ∈ [0, 1]

δz = 0

Afghan Girls RCT (n = 687)

OLS

(I) Summary Statistics

a school was established have z = 1 and girls in a village where none was have z = 0.
The vector x contains the same covariates used by Burde and Linden (2013).37 Because
this example includes exogenous controls, we define treatment endogeneity and instrument
invalidity net of these covariates, as detailed in Section 5.4. To simplify the notation we
write δz and δT ∗ rather than δez and δeT ∗ below but both of these should be understood as
being net of x. In contrast, α0 and α1 are not defined net of covariates, again as detailed
in Section 5.4. As such, they continue to refer to the probabilities P(T = 1|T ∗ = 0) and
P(T = 0|T ∗ = 1) below.
This dataset has three features that make it an ideal candidate for the methods we have
developed above. First, the enrollment variable measures not whether a girl attended the
newly-established village school, but whether she attended a school of any kind. This means
that our treatment of interest, enrollment, is endogenous: the sample contains 248 girls
who did not enroll despite a school being established in their village, and 49 who attended
school despite the lack of one in their village. In this example a prior that imposes positive
selection, δT ∗ > 0, seems uncontroversial: parents who enroll their daughter in school are
likely to have other unobserved characteristics favorable for their academic performance.
Second, although the allocation of village schools was randomized, this does not necessarily
make it a valid instrument. Indeed, the authors argue that establishing a village school may
affect performance through channels other than increased enrollment alone if, for example,
the village-based schools were of lower quality than the traditional public schools,
and some treatment students who would have otherwise attended traditional public
schools attended village-based schools instead, or if children who were not enrolled
in the treatment group experienced positive spillovers from enrolled siblings or other
peers. (Burde and Linden (2013), p. 36.)

Third, school enrollment status is determined from a household survey and, as such, could
be subject to substantial mis-reporting. Note that non-differential measurement error in
enrollment would not affect the ITT estimate but would bias the estimated causal effect
of establishing a school on enrollment, i.e. the first-stage in our IV analysis. Although
Burde and Linden (2013) concede that misreporting is a possibility, they point out that
the observed enrollment rates in their sample are comparable to official Afghan government
estimates for the region. Even if aggregate enrollment is correctly measured, as the authors
suggest, individual mis-reporting can still bias the IV estimate. Nevertheless, the prior belief
that p = p∗ does impose an informative restriction on α0 and α1 which we explore below.
37

These are: an indicator for whether the girl is a child of the household head, the girl’s age, the number
of years the household has lived in the village, a Farsi dummy, a Tajik dummy, a farmers dummy, the age of
the household head, years of education of the household head, the number of people in the household, Jeribs
of land, number of sheep, distance to the nearest formal school, and a dummy for Chagcharan province.

37

Results for the Afghan Girls RCT example appear in the first four rows of Table 3.
All values other than ᾱ0 and ᾱ1 in columns 3–4 of panel (I) are measured in standard
deviations of test scores. IV and OLS estimates, along with lower and upper bounds ᾱ0
and ᾱ1 for the mis-classification probabilities appear in panel (I). Posterior medians and 90
percent highest posterior density intervals appear in Panels (II) and (III). The results in
Panel (II) are labeled “Frequentist-Friendly” because they do not involve placing a prior on
the conditional identified set: they average only over reduced form parameter draws under
the restriction listed in the corresponding row label.38 In contrast, those in Panel (III) are
“Fully Bayesian” in that they place a uniform prior on the conditional identified set.
The OLS estimate in this example is quite large, 0.86 standard deviations, but the IV estimate is even larger: 1.3 standard deviations. Notice that our bounds on the mis-classification
error rates in this example are very tight: our point estimate of ᾱ0 is 0.10 while that of α¯1 is
0.12 as shown in Table 3 and depicted in Figure 3. To implement our framework, we consider
three prior restrictions. The first assumes that z is a valid instrument, δz = 0, but places
no a priori restrictions on the extent of misclassification, α0 and α1 , or the sign or extent
of treatment endogeneity, δT ∗ . Results for this prior appear in the second row of Table 3.
Even under this fairly strong prior restriction, the IV estimate could still show substantial
bias because the measurement error is non-differential rather than classical. The first two
columns of Panel (II) present posterior inference for the boundaries of the identified set for
δT ∗ under the assumption that z is a valid instrument while the first column for Panel (III)
presents analogous fully Bayesian inference for the parameter δT ∗ . In this example both tell
a similar story; even allowing for measurement error, the assumption that z is a valid instrument requires us to accept the possibility of substantial negative selection into treatment,
and rules out anything beyond a very modest degree of positive selection. This is precisely
the opposite of what most researchers would consider reasonable in this setting.
Figure 4a illustrates this point in a slightly different way, by plotting selected contours
of the identified set for (δT ∗ , δz , α0 , α1 ) in the region where δT ∗ is positive, by evaluating
Equation 41 at the maximum likelihood estimates for the reduced form parameters. If δT ∗
is assumed to be positive, the only way to sustain a valid instrument is by assuming both
that there is essentially zero selection into treatment, and that mis-classification is extremely
severe. These results suggest that Burde and Linden (2013) were right to be suspicious of
the IV exclusion restriction. Note, moreover, that both the inference for the upper and lower
bounds of the identified set for β in columns 3–4 of Panel (II) and the corresponding fully
Bayesian inference for the parameter β in the second column of Panel (III) point to a large
causal effect of enrollment on test scores. This indicates that, provided one is willing to
38

See Sections 3 and 5.5 for details.

38

0.8
0.6

α0 = .05, α1 = .06
α0 = α1 = 0

0.2

0.4

δz

0.4

0.6

assume that the instrument is valid, the effect of measurement error on the IV estimate is
modest in this example.

0.0

0.0

0.2

α0 = .10, α1 = .12

0.0

0.2

0.4

0.6

0.8

1.0

-0.5

δT ∗

0.0

0.5

1.0

1.5

β

(a) Contours of Identified Set at MLE

(b) Posterior for Treatment Effect

Figure 4: Results for Afghan Girls RCT example from Section 6.1. Panel (a) illustrates Equation
41 for δT ∗ ∈ [0, 1] at three pairs of values for (α0 , α1 ). Both δT ∗ and δz are expressed in standard
deviations of the test score distribution, and the reduced form parameters are set equal to their
maximum likelihood estimates. Panel (b) gives the posterior distribution for β under a uniform
prior on the intersection between the restrictions δT ∗ ∈ [0, 1], p = p∗ and the conditional identified
set (see Section 5.5). The dashed red line gives the OLS estimate and the blue line the IV estimate.

The third and fourth rows of Table 3 relax the assumption that δz = 0 and instead
impose δT ∗ ∈ [0, 1]. This amounts to assuming that the selection effect is positive and no
greater than one standard deviation of test scores, after controlling for covariates. The third
row imposes no a priori restrictions on α0 and α1 while the fourth assumes that p = p∗ so
that α1 = α0 (1 − p)/p.39 The results for these two specifications are practically identical,
indicating that p∗ = p is not a particularly informative restriction given the tight bounds the
data already place on α0 and α1 . When δT ∗ ∈ [0, 1], the identified set for δz includes a range
of modestly negative values and a much wider range of comparatively large positive values,
as we see from the second and third columns of Panel (II). The fully Bayesian inference for
δz from the first column on Panel (III) is more conclusive, assigning 90 percent probability
a posteriori to the event that δz is between 0.1 and 0.7 standard deviations. The difference
39

This follows from p∗ = (p − α0 )/(1 − α0 − α1 ).

39

between “Frequentist-Friendly” and fully Bayesian inferences in this case indicates that,
while all draws for the reduced form parameters are compatible with δz < 0, there is only a
very limited combination of values for δT ∗ , α0 , and α1 at which this can occur. This fact is
also apparent from Figure 4a: it is only at extremely small values for δT ∗ and extremely large
values of α0 and α1 that δz can be negative. One need not take our uniform reference prior
from Panel (III) literally: the point is that one would need to place large prior probability
on a very small and implausible region of the identified set in order to obtain a substantial
posterior probability on the proposition that δz < 0. In light of our discussion from Burde
and Linden (2013) from above, this suggests that positive peer effects are more plausible
than negative village-school quality effects.
Inferences for the causal effect of enrollment are less conclusive. Under the restriction
δT ∗ ∈ [0, 1], the identified set for β comfortably contains zero, although it does extend
farther in the positive than the negative direction, as shown in the last two columns of panel
(II) in Table 3. The fully Bayesian inferences from the second column of Panel (III) are
somewhat more suggestive. Although the 90 percent highest posterior density interval for
β does include zero, it is fairly close to the lower limit of the interval. As seen from the
posterior distribution in Figure 4b, the causal effect of enrollment is very likely positive,
although substantially smaller than either the OLS or the IV estimate. Again, the difference
between the inferences for the identified set and the Bayesian posterior for β under a uniform
reference prior indicate that there is only a small region of values for α0 and α1 that are
compatible with a negative value for β, given that δT ∗ ∈ [0, 1]

6.2

Smoking and BMI

We conclude with an example based on data from the Lung Health Study (LHS), a wellknown randomized clinical trial carried out between 1986 and 1994.40 The LHS recruited a
sample of smokers between the ages of 35 and 59, and offered a smoking cessation program
to a random subset. The cessation program consisted of free nicotine gum, an intensive
quit week, and access to support personnel, along with invitations to bring a family member
to the meetings. Some of the individuals offered treatment also were given an inhaled
bronchodilator; the control group received no such offer. The LHS then tracked these subjects
over time, recording information on a variety of clinical outcomes. Our outcome of interest
here is body mass index (BMI), a measure of obesity defined as weight (in kilograms) divided
by squared height (in meters).41 Following Courtemanche et al. (2016), our objective is to
40

See Ohara et al. (1993) and https://www.clinicaltrials.gov for more information on the LHS.
According to the World Health Organization, individuals whose BMI falls below 18.5 are considered
underweight, those whose BMI lies between 18.5 and 25 are fall in the normal range, those whose BMI lies
41

40

determine the causal effect of quitting on BMI. Our specification is
BMI = constant + β(Quit Smoking) + ε
where we instrument for the self-reported treatment variable “Quit Smoking” with the randomized offer of participation in the smoking cessation program: z = 1 for those in the
treatment arm of the LHS while z = 0 for those in the control arm. The effect of quitting
smoking on BMI is a question of some interest to health researchers, because those who quit
smoking are known to experience increases in anxiety and appetite along with physiological
changes that may lead to weight gain. Indeed, some suggest that the marked decrease in
smoking that has occurred in the U.S. over the past 30 years may be partly to blame for
the contemporaneous increase in obesity.42 Access to the raw data for the LHS is strictly
controlled, so we work here with summary statistics provided to us by the authors of Courtemanche et al. (2016). Specifically, we observe conditional means and variances of BMI at
the five-year horizon for all combinations of T (Quit Smoking) and z (Offered Smoking Cessation), as well as the empirical joint distribution of T and z. Because our framework for
inference relies only on the moments that these quantities estimate, as illustrated in Table
2a, we can proceed just as if we observed the micro-data.43
Measurement error, treatment endogeneity, and instrument invalidity are all serious concerns in this example. First, our measure of whether an individual quit smoking is selfreported. It seems quite likely that some people who have failed to quit will nevertheless
claim they have succeeded.44 Fortunately, the mis-classification is almost certainly one-sided
in this example: it is difficult to imagine that someone who successfully quit smoking would
report that she did not. Second, while the offer of smoking cessation is randomized, the
decision to quit smoking is clearly endogenous. Out of the 5446 subjects in the LHS, 451
report quitting smoking despite not being offered the cessation program, while 2018 report
not quitting even though they were offered the program. We might expect subjects who
successfully quit smoking to be more health-conscious overall, and thus, thinner than those
between 25 and 30 are classified as overweight, and those whose BMI exceeds 30 are considered obese. See
http://apps.who.int/bmi/index.jsp?introPage=intro_3.html for further details.
42
For evidence in favor of this claim, see Chou et al. (2004) and Chou et al. (2006); for a contrary view,
see Gruber and Frakes (2006).
43
One issue with using aggregated moments is the inability to include covariates directly. Nevertheless, if
key covariates have discrete support and the sample size is large enough, the analysis can be performed for
each cell in the distribution of the support of the covariates.
44
The LHS data contain three measures of whether an individual has quit smoking, all of which are subject
to measurement error: self-reported quit status, self-reported number of cigarettes smoked per day, and the
results of salivary cotinine tests administered as part of the study. While Courtemanche et al. (2016) consider
all three measures in detail, we focus here on the first for simplicity.

41

to do not quit. Courtemanche et al. (2016) also suggest that the offer of a smoking cessation
program may not constitute a valid instrument:
The validity of the [IV] estimator therefore hinges on the assumption that the
randomized intervention only affected the BMIs of people who fully quit smoking.
To the extent that the intervention also affected the BMIs of those who cut back on
smoking but did not quit entirely, the difference in BMI will be scaled by too small a
number. . . (Courtemanche et al. (2016), p. 9.)

As we mentioned in Section 5.2, this logic would imply δz > 0.
Results for the Smoking and BMI example appear in rows 7–9 of Table 3. All values other
than those in columns 3–4 of Panel (I) are measured in units of BMI. IV and OLS estimates,
along with lower and upper bounds ᾱ0 and ᾱ1 for the mis-classification probabilities appear in
Panel (I), while posterior medians and 90 percent highest posterior density intervals appear
in Panels (II) and (III). Recall the results in Panel (II) are labeled “Frequentist-Friendly”
because they do not involve placing a prior on the conditional identified set: they average
only over reduced form parameter draws under the restriction listed in the corresponding
row label.45 In contrast, those in Panel (III) are “Fully Bayesian;” they place a uniform prior
on the conditional identified set.
Both the OLS and IV estimates in this example are large, positive, and precisely estimated. While one of the mis-classification error bounds for this example is very tight,
α0 < 0.16, the other is not: α1 < 0.43. Fortunately, α1 denotes the fraction of true quitters
who mis-report and claim they did not quit smoking. As discussed above, the true value of
this probability is almost certainly zero so we fix α1 = 0 throughout the remainder of our
analysis. Because the partial identification bound for α1 is so wide, this prior restriction
adds a considerable amount of identifying information to the problem.
As a benchmark, row 9 of Table 3 explores the implications of assuming that δz = 0, so
that the randomized offer of a smoking cessation program constitutes a valid instrument.
Under these beliefs, the inference for the identified set for β from columns 3 and 4 and
the posterior for the parameter β from the second column of Panel (III) indicate a large,
positive causal effect of smoking on BMI, but an effect that is somewhat smaller than the
IV estimate due to the effect of misclassification error. As discussed above, there are good
reasons to doubt the validity of the instrument in this example. Accordingly, row seven
of Table 3 relaxes the assumption that δz = 0 and imposes instead that δT ∗ ∈ [−1.5, 0].
Figure 5a likewise depicts selected contours of the identified set in this range, evaluated at
the maximum likelihood estimates of the reduced form parameters, while Figure 5b plots
45

See Sections 3 and 5.5 for details.

42

0.0

0.6

the posterior distribution for β corresponding to the inferences from column two of Panel
(III) in the table. The sign of δT ∗ under this prior represents the belief that those who quit
successfully are likely to be more health conscious. The lower bound of 1.5 for the magnitude
of the BMI difference corresponds to a third of the distance between the upper end of the
“healthy” weight range and the lower end of the “obese” range. At the average U.S. height,
δT ∗ = −1.5 would say that successful quitters are around 10 pounds lighter on average, a
moderate amount of negative selection.

0.4

-0.2

α0 = 0

0.2

-0.4

δz

α0 = .08

0.0

-0.6

α0 = .16

-1.5

-1.0

-0.5

0.0

2.0

δT ∗

2.5

3.0

3.5

4.0

4.5

5.0

β

(a) Contours of Identified Set at MLE

(b) Posterior for Treatment Effect

Figure 5: Results for Smoking and BMI example from Section 6.2. Panel (a) illustrates Equation
32 for δT ∗ ∈ [−1, 0], α1 = 0 at three of values for α0 . Both δT ∗ and δz are given in units of BMI, and
the reduced form parameters are set equal to their maximum likelihood estimates. Panel (b) gives
the posterior distribution for β under a uniform prior on the intersection between the restriction
δT ∗ ∈ [−1, 0], α1 = 0 and the conditional identified set (see Section 5.5 for details). The red dashed
line gives the OLS estimate and the blue line the IV estimate.

Both the results in the table and the figure imply that δz is very likely negative, the
exact opposite of what we would have expected from above. From Figure 5a, we see that
one would require almost no mis-reporting of true smoking status along with hardly any
selection. The corresponding inferences for β point to a very large treatment effect: the
90 percent highest posterior density interval for the lower bound for β, for example, ranges
from about 2.2 to 2.6, while the median of the posterior for β in Figure 5b is 3.5. The
message of this example is somewhat nuanced compared to our previous ones. If one is
43

certain that δT ∗ is negative, our framework implies that δz too is almost certainly negative,
and the causal effect of quitting smoking on BMI is very large relative to estimates from the
existing literature. If on the other hand one feels confident that δz should be positive, as the
discussion from Courtemanche et al. (2016) suggests, our framework implies that δT ∗ must
be positive: successful quitters are heavier on average.

7

Conclusion and Extensions

Causal inference relies on researcher beliefs. The main message of this paper is that imposing
them requires a formal framework, both to guard against contradiction and to ensure that
we learn everything that the data have to teach us. While this point is general, we have
focused here on a simple but common setting, that of a linear model with a mis-measured,
endogenous treatment and a potentially invalid instrument, presenting both results for the
case of a continuous treatment subject to classical measurement error and that of a binary
treatment subject to non-differential measurement error. By characterizing the relationship
between measurement error, treatment endogeneity, and instrument invalidity in terms of
intuitive and empirically meaningful parameters, we have developed a Bayesian tool for
eliciting, disciplining, and incorporating credible researcher beliefs in the form of sign and
interval restrictions. As we have demonstrated through a wide range of illustrative empirical
examples, even relatively weak researcher beliefs can be surprisingly informative in practice.
The methods we describe above could be extended in a number of directions. One possibility is to allow for multiple instrumental variables, expanding the range of examples to
which our framework could be applied. There is no serious theoretical obstacle to this extension, although it would likely make prior elicitation more challenging. Another possibility is
to consider a wider range of prior specifications on the conditional identified set. One could,
for example, explore more informative priors than a uniform distribution, or undertake a
formal prior robustness exercise, perhaps along the lines of the ε-contaminated class of priors described by Berger and Berliner (1986) or “posterior lower probability” as in Kitagawa
(2012). A limitation of the results presented here is that they assume the treatment effect
is homogeneous. While it would likely be difficult to accommodate heterogeneous treatment
effects when the treatment is continuous, the binary treatment case shows more promise. Under appropriate modifications it may be possible to extend our framework to the estimation
of a local average treatment effect (LATE), possibly by leveraging the testable implications
of the LATE model under a binary treatment described by Kitagawa (2015) and Huber and
Mellace (2015) among others. We leave this possibility for future research.

44

References
Acemoglu, D., Johnson, S., Robinson, J. A., 2001. The colonial origins of comparative development: An
empirical investigation. The American Economic Review 91 (5), 1369–1401.
Amir-Ahmadi, P., Drautzburg, T., 2016. Identification through heterogeneity, Working Paper.
Apostol, T. M., 1969. Calculus, 2nd Edition. Vol. II. John Wiley and Sons, New York.
Arias, J. E., Rubio-Ramı́rez, J. F., Waggoner, D. F., 2016. Inference based on SVARs identified with sign
and zero restrictions: Theory and applications, Working Paper.
Baumeister, C., Hamilton, J. D., September 2015. Sign restrictions, structural vector autoregressions, and
useful prior information. Econometrica 83 (5), 1963–1999.
Becker, S. O., Woessmann, L., 2009. Was Weber wrong? A human capital theory of Protestant economic
history. Quarterly Journal of Economics 124 (2), 531–596.
Berger, J., Berliner, L. M., 1986. Robust bayes and empirical bayes analysis with ε-contaminated priors. The
Annals of Statistics, 461–486.
Black, D. A., Berger, M. C., Scott, F. A., 2000. Bounding parameter estimates with nonclassical measurement
error. Journal of the American Statistical Association 95 (451), 739–748.
Bollinger, C. R., 1996. Bounding mean regressions when a binary regressor is mismeasured. Journal of
Econometrics 73, 387–399.
Bollinger, C. R., van Hasselt, M., 2015. Bayesian moment-based inference in a regression models with
misclassification error, Working Paper.
Burde, D., Linden, L., 2013. Bringing education to Afghan girls: A randomized controlled trial of villagebased schools. American Economic Journal: Applied Economics 5 (3), 27–40.
Chou, S.-Y., Grossman, M., Saffer, H., 2004. An economic analysis of adult obesity: results from the
behavioral risk factor surveillance system. Journal of health economics 23 (3), 565–587.
Chou, S.-Y., Grossman, M., Saffer, H., 2006. Reply to Jonathan Gruber and Michael Frakes. Journal of
Health Economics 25 (2), 389–393.
Conley, T. G., Hansen, C. B., Rossi, P. E., 2012. Plausibly exogenous. The Review of Economics and
Statistics 94 (1), 260–272.
Courtemanche, C., Tchernis, R., Ukert, B., 2016. The effect of smoking on obesity: Evidence from a randomized trial, Working Paper.
Dale, S. B., Krueger, A. B., 2002. Estimating the payoff to attending a more selective college: An application
of selection on observables and unobservables. Quarterly Journal of Economics, 1491–1527.
DiTraglia, F., Garcia-Jimeno, C., 2016. On mis-measured binary regressors: New results and some comments
on the literature, Working Paper.
Frazis, H., Lowenstein, M. A., 2003. Estimating linear regressions with mismeasured, possibly endogenous,
binary explanatory variables. Journal of Econometrics 117 (1), 151–178.
Gruber, J., Frakes, M., 2006. Does falling smoking lead to rising obesity? Journal of health economics 25 (2),
183–197.

45

Gustafson, P., May 2005. On model expansion, model contraction, identifiability and prior information: Two
illustrative examples involving mismeasured variables. Statistical Science 20 (2), 111–140.
Gustafson, P., 2015. Bayesian Inference for Partially Identified Models: Exploring the Limits of Limited
Data. No. 141 in Monographs on Statistics and Applied Probability. CRC Press, Boca Raton.
Hahn, P. R., Murray, J. S., Manolopoulou, I., 2016. A Bayesian partial identification approach to inferring
the prevalence of accounting misconduct. Journal of the American Statistical Association 111 (513).
Hu, Y., 2008. Identification and estimation of nonlinear models with misclassification error using instrumental
variables: A general solution. Journal of Econometrics 144 (1), 27–61.
Huber, M., Mellace, G., 2015. Testing instrument validity for late identification based on inequality moment
constraints. Review of Economics and Statistics 97 (2), 398–411.
Kahneman, D., Tversky, A., 1974. Judgement under uncertainty: Heuristics and biases. Science 185 (4157),
1124–1131.
Kane, T., Rouse, C. E., Staiger, D., July 1999. Estimating the returns to schooling when schooling is
misreported, NBER Working Paper # 7235.
Kitagawa, T., July 2012. Estimation and inference for set-identified parameters using posterior lower probability, Working Paper.
URL http://www.homepages.ucl.ac.uk/~uctptk0/Research/LowerUpper.pdf
Kitagawa, T., 2015. A test for instrument validity. Econometrica 83 (5), 2043–2063.
Kline, B., Tamer, E., July 2016. Bayesian inference in a class of partially identified models. Quantitative
Economics 7 (2).
Lewbel, A., March 2007. Estimation of average treatment effects with misclassification. Econometrica 75 (2),
537–551.
Mahajan, A., 2006. Identification and estimation of regression models with misclassification. Econometrica
74 (3), 631–665.
Melfi, G., Schoier, G., 2004. Simulation of random distributions on surfaces. Societa Italiana di Statistica,
173–176.
Moon, H. R., Schorfheide, F., 2009. Estimation with overidentifying inequality moment conditions. Journal
of Econometrics 153, 136–154.
Moon, H. R., Schorfheide, F., 2012. Bayesian and frequentist inference in partially identified models. Econometrica 80 (2), 755–782.
Nevo, A., Rosen, A. M., 2012. Identification with imperfect instruments. The Review of Economics and
Statistics 94 (3), 659–671.
Ohara, P., Grill, J., Rigdon, M., Connett, J., Lauger, G., Johnston, J., 1993. Design and results of the initial
intervention program for the lung health study. Preventive medicine 22 (3), 304–315.
Oster, E., January 2016. Unobservable selection and coefficient stability: Theory and evidence, Working
Paper.
URL
https://www.brown.edu/research/projects/oster/sites/brown.edu.research.projects.
oster/files/uploads/Unobservable_Selection_and_Coefficient_Stability_0.pdf
Poirier, D. J., 1998. Revising beliefs in nonidentified models. Econometric Theory 14, 483–509.

46

Richardson, T. S., Evans, R. J., Robins, J. A., 2011. Transparent parameterizations of models for potential
outcomes. In: Bayesian Statistics. Vol. 9. pp. 569–610.
van Hasselt, M., Bollinger, C. R., 2012. Binary misclassification and identification in regression models.
Economics Letters 115, 81–84.

A
A.1

Appendices for Continuous Treatment Case
Proofs

Lemma A.1. Under Equation 5 and Assumptions 2.1–2.2,
(a) σv2 = σT2 (κ − ρ2T z )



1−κ
2
σv2 + π 2 σz2
(b) σw
=
κ
#
"
κ − ρ2T y
2
2
(c) σu = σy
κ(1 − ρ2T ∗ u )
√
ρT ∗ u κ − ρuz ρT z
p
.
(d) ρuv =
κ − ρ2T z
2
Proof of Lemma A.1(a). First, σT2 = σw
+π 2 σz2 +σv2 and ρ2T z = π 2 σz2 /σT2 . The result follows by combining
2
these and rearranging, using the definition of κ and the fact that σT2 ∗ = σT2 − σw
.
2
2
= σT2 ∗ (1 − κ)/κ.
, we have σw
Proof of Lemma A.1(b). By definition κ = σT2 ∗ /σT2 . Since σT2 = σT2 ∗ + σw
2 2
2
2
The result follows since σT ∗ = σv + π σz .

Proof of Lemma A.1(c). The result follows by squaring Equation A.11 in the proof of Proposition 2.1.
Proof of Lemma A.1(d). p
From Equation 8, ρT ∗ u = (σv ρuv + πσz ρuz )/σT ∗ . and by Lemma A.1(a) and
the definition of κ, σv /σT∗ = 1 − ρ2T z /κ and πσz /σT ∗ = ρT z . Thus,
q
√
ρT ∗ u = ρuv 1 − ρ2T z /κ + ρuz ρT z / κ
The result follows by solving for ρuv .
Corollary A.1. Under Equation 5 and Assumptions 2.1–2.2, κ > max{ρ2T y , ρ2T z }.
Proof of Corollary A.1. Since σv2 > 0, κ > ρ2T z by Lemma A.1(a). Similarly, since σu2 > 0, κ > ρ2T y by
Lemma A.1(c).
Proof of Proposition 2.1. Rewriting Equation 6
β = (σT y − σT ∗ u )/κσT2

(A.1)

and proceeding similarly for Equation 7, we have
β = (σzy − σuz )/σzT

(A.2)

(σzy − σuz )/σzT = (σT y − σT ∗ u )/κσT2

(A.3)

Combining these,

47


Now, using Equation 3 and Assumption 2.1, σy2 = σu2 + β 2σT ∗ u + βκσT2 . Substituting Equation A.2 for β,
Equation A.1 for βκσT2 , and rearranging,



σzy − σuz
2
2
(σT ∗ u + σT y ) = 0
(A.4)
σu − σy +
σzT
√
The next step is to eliminate σu from our system of equations. First we substitute σT ∗ u = σu κσT ρT ∗ u and
σuz = σu σz ρuz into Equations A.3 and A.4, yielding
(σzy − σu σz ρuz )/σzT = (σT y − σT σu ρT ∗ u )/(κσT2 )
and

σu2 − σy2 +



σzy − σu σz ρuz
σzT



(A.5)


√
σu σT κρT ∗ u + σT y = 0.

(A.6)

Rearranging Equation A.5 and solving for σu , we find that
σu =

σ σ − κσT2 σzy
√ zT T y
σT κσT z ρT ∗ u − σz κσT2 ρuz

(A.7)

Since we have stated the problem in terms of scale-free structural parameters, namely (ρuz , ρT ∗ u , κ), we
may assume without loss of generality that σT = σy = σz = 1. Even if the raw data do not satisfy this
assumption, the relationship between the structural parameters ρuz , ρT ∗ u and κ is unchanged. Imposing this
normalization, Equation A.6 becomes




√
ρzy − σ
eu ρuz
σ
eu2 − 1 +
σ
eu κρT ∗ u + ρT y = 0
(A.8)
ρzT
where
σ
eu = √

ρT z ρT y − κρzy
.
κρT z ρT ∗ u − κρuz

(A.9)

We use the notation σ
eu to indicate that normalizing y to have unit variance does change the scale of σu .
Specifically, σ
eu = σu /σy . This does not introduce any complications because we eliminate σ
eu from the system
by substituting Equation A.9 into Equation A.8. After eliminating σ
eu , Equation A.8 becomes a quadratic in
ρuz that depends on the structural parameters (ρT ∗ u , κ) and the reduced form correlations (ρT y , ρT z , ρzy ).
Solving and simplifying the result, we find that
v


u
ρT ∗ u ρT z
u 1 − ρ2T ∗ u
+
−

√
(A.10)
± (ρT y ρT z − κρzy ) t 
(ρuz , ρuz ) =
κ
κ κ − ρ2
Ty

Although the preceding expression yields two solutions, one of these is extraneous as it implies a negative
value for σ
eu and hence σu . To see why, substitute each solution into the reciprocal of Equation A.9 to yield
s
" √
#
 s
√
2
∗u
∗u
κ(1
−
ρ
)
κ(1 − ρ2T ∗ u )
κρ
ρ
κρ
ρ
∗
T
z
T
T
z
T
T u
σ
eu−1 =
−
±
=∓
2
ρT y ρT z − κρzy
ρT y ρT z − κρzy
κ − ρT y
κ − ρ2T y
and hence

s
σu = ∓σy

κ − ρ2T y
κ(1 − ρ2T ∗ u )

.

(A.11)

This implies that ρ+
uz is always extraneous.
Proof of Lemma 2.1. Assumption 2.1 requires that all the elements of
pΩ are finite, which implies that
2
2
2
2
V ar(u)V ar(v) and Cov(u, z) ≤
σ
,
σ
,
σ
,
σ
<
∞.
The
reverse
implication
follows
since
Cov(u,
v)
≤
pw v z u
V ar(u)V ar(z) by the Cauchy-Schwarz inequality. This establishes that condition (a) of Lemma 2.1 is

48

equivalent to all of the entries of Ω being finite.
e is positive definite if and only if each of its three leading principle minors are positive:
Now, Ω

σz2 (σu2 σv2 −

σu2 σv2
2
σuv ) −

σu2

>

2
− σuv
2 2
σv σuz

>

0

(A.13)

>

0.

(A.14)

0

(A.12)

Thus it is sufficient to show that Equations A.12–A.14 are equivalent to σu2 , σv2 , σz2 > 0 and ρ2uv + ρ2uz < 1.
2
The equivalence is obvious for A.12. By A.12 we can rearrange A.13 to yield σv2 > σuv
/σu2 ≥ 0 implying that
2
2
σv is strictly positive. Dividing through by σv , this implies that |ρuv | < 1. Now, since both σu2 and σv2 are
2
/σu2 ≥ 0. Since
strictly positive, we can divide both sides of A.14 through by σv2 σu2 to obtain σz2 (1−ρ2uv ) > σuz
2
2
2 2 2
ρuv < 1, this implies σz > 0. Thus, dividing Equation A.14 through by σv σu σz and rearranging we find that
ρ2uv + ρ2uz < 1, establishing the “if” direction of the equivalence. For the “only if” direction, ρ2uv + ρ2uz < 1
implies ρ2uv < 1. Mutiplying both sides by σu2 σv2 gives σu2 σv2 ρ2uv < σu2 σv2 since σu2 , σv2 > 0. Substituting
2
ρ2uv = σuv
/(σu2 σv2 ) and rearranging implies A.13. Equation A.14 follows similarly, by multiplying both sides
2
of ρuv + ρ2uz < 1 by σu2 σv2 σz2 and rearranging.
Proof of Proposition 2.2. We first derive the bounds; at the end of the proof we show that they are
sharp. To show that |ρT ∗ u | < 1 we combine the assumption that σu2 < ∞ with the expression from Lemma
2
≥ 0 and the definition of κ.
A.1(c). The fact that κ ≤ 1 follows from σw
The lower bound for κ is more involved. We first restate the inequality from Lemma 2.1(d) so that it no
longer involves ρuv by substituting Lemma A.1(d), yielding
!2
√
ρT ∗ u κ − ρuz ρT z
p
+ ρ2uz < 1
κ − ρ2T z

(A.15)

Using the fact that κ > ρ2T z from Corollary A.1, putting the terms of Equation A.15 over a common
denominator and rearranging,
ρ2T ∗ u + ρ2zu −

2ρT ∗ u ρzu ρT z
κ − ρ2T z
√
<
.
κ
κ

Completing the square, we find

ρzu −

ρT ∗ u ρT z
√
κ

2

< 1 − ρ2T ∗ u





κ − ρ2T z
κ



√
Now, using Equation 13 to substitute for (ρzu − ρT ∗ u ρT z / κ), we find that
"
#


 κ − ρ2T z
1 − ρ2T ∗ u
2
2
(ρT y ρT z − κρzy )
< 1 − ρT ∗ u
κ(κ − ρ2T y )
κ
again using κ > ρ2T y and ρ2T ∗ u < 1. Cancelling a factor of (1 − ρ2T ∗ u )/κ from each side and rearranging yields
an expression that does not involve ρT ∗ u , namely
2

(ρT y ρT z − κρzy ) − (κ − ρ2T y )(κ − ρ2T z ) < 0

(A.16)

again using the fact that κ > ρ2T y . Expanding and simplifying,
(ρ2zy − 1)κ2 + (ρ2T y + ρ2T z − ρT y ρT z ρzy )κ < 0

(A.17)

Since ρ2zy < 1 by positive definiteness, the preceding inequality defines an interval of values that κ cannot
take on, an interval bounded by the roots of a quadratic function that opens downwards. Factoring the

49

quadratic to solve for these roots gives


κ (ρzy − 1)κ + ρ2T y + ρ2T z − 2ρT y ρT z ρzy = 0
Thus one root is zero and the other is κ, as defined in the statement of the proposition. We have shown that
κ cannot take on a value between the two roots. We do not yet know, however, which is larger. The result
of the proposition follows after we establish two more claims. First κ < 1, and second κ > max{ρ2T y , ρ2T z }.
For the first claim, note that the correlation matrix of (y, T, z) must be positive-definite, implying that
1 − ρ2T y − ρ2T z − ρ2zy + 2ρT y ρT z ρzy > 0
Rearranging this inequality using the fact that ρ2zy < 1 establishes that κ < 1. For the second claim
notice that by evaluating the quadratic on the left hand side of A.16 at max{ρ2T y , ρ2zT }, the second term
vanishes leaving us with a squared term. Since this cannot be less than zero, the inequality is violated at
κ = max{ρ2T y , ρ2zT } > 0. This combined with the fact that the parabola opens downwards establishes that
κ is greater than both zero and max{ρ2T y , ρ2zT }. This establishes the lower bound for κ.
Now that we have derived the bounds, we explain why they are sharp. It suffices to show that, for any
values of ρT ∗ u and κ within our bounds, there exist values for all the parameters of Ω, defined in Assumption
2.1, that satisfy the first three equivalent conditions from Lemma 2.1. First, from A.1(a), ρ2T z < κ ≤ 1
implies that σv2 is strictly positive and finite. The bound ρ2T z < κ is implied by κ > κ, as explained in our
derivation above. Second, by A.1(b), the fact that σv2 is positive and finite, along with 0 < κ < κ < 1,
2
is non-negative and finite. Third, by A.1(c), ρ2T y < κ < κ < 1 along with |ρT ∗ u | =
implies that σw
6 1 implies
2
that σu is strictly positive and finite. It remains only to verify that ρ2uv + ρ2uz < 1, but this is immediate
from our derivation of κ from above, since all of our steps were reversible.
Proof of Corollary 2.1. Let f (ρT ∗ u , κ) denote the right hand side of Equation 13. We begin by finding
the optima of f as a function of ρT ∗ u , holding κ fixed. The first derivative of f with respect to ρT ∗ u is
∂f
ρT ∗ u (ρT y ρT z − κρzy )
ρT z
= √ +q
∗
∂ρT u
κ
κ(κ − ρ2T y )(1 − ρ2T ∗ u )

(A.18)

so the first-order condition for ρT ∗ u is
ρT ∗ u = − √

ρT z

a
,
1 + a2

a=

q

κ − ρ2T y

(ρT y ρT z − κρzy )

(A.19)

The second derivative of f with respect to ρT ∗ u is
"
#
∂2f
(ρT y ρT z − κρzy )
ρ2T ∗ u
1
= q
+p
.
∂ρ2T ∗ u
(1 − ρ2T ∗ u )3/2
1 − ρ2T ∗ u
κ(κ − ρ2T y )

(A.20)

Note that the sign of Equation A.20 depends only on the sign of (ρT y ρT z − κρzy ). Define κ
e = ρT y ρT z /ρzy
and suppose first that κ 6= κ̃. If κ < κ̃ then f (ρT ∗ u , κ) is a strictly convex function of ρT ∗ u and thus, holding
κ fixed, has a unique global minimum at the solution to A.19. In contrast, the global maximum is a corner
solution: it occurs either at ρT ∗ u = −1 or 1. Similarly, if κ > κ̃ then f (ρT ∗ u , κ) is a strictly concave function
of ρT ∗ u and thus, holding κ fixed, has a unique global maximum at the solution to A.19. In this case the
global minimum is a corner solution: it occurs either at ρT ∗ u = −1 or 1. In either case, the interior solution
is strictly less than one in absolute value, as we see from Equation A.19 using the fact that κ > ρ2T y by
√
Corollary A.1. If κ = κ̃, then f reduces to ρT ∗ u ρT z / κ̃. In this case both extrema are corner solutions:
they occur at ρT ∗ u = −1 and 1.
We have now fully characterized the values of ρT ∗ u that optimize f for any fixed value of κ. It remains
to find the optimal values of κ within the feasible set (κ, 1]. Using Equation A.19 we can concentrate ρT ∗ u

50

out of f to yield a new function g that imposes the first-order condition for ρT ∗ u , namely
s
(ρT y ρT z − κρzy )2 + ρ2T z (κ − ρ2T y )
g(κ) = −sign{ρT y ρT z − κρzy }
κ(κ − ρ2T y )

(A.21)

and calculate its derivative as follows
g 0 (κ) = −

(κ − ρ2T y )(1 − ρ2zy )
.
2g(κ)(κ − ρ2T y )2

(A.22)

Note that g 0 is positive whenever κ < κ̃ and negative whenever κ > κ̃. Moreover, recall from the proof of
Proposition 2.2 (see Equation A.17) that κ satisfies (ρT y ρT z − κρzy )2 = (κ − ρ2T y )(κ − ρ2T z ). Substituting
this into Equation A.21 we find that
g(κ) = −sign{ρT y ρT z − κρzy }

(A.23)

Equation A.22 applies at any interior optimum for ρT ∗ u . At a corner solution, ρT ∗ u = −1 or 1 and the
objective function reduces to

√
−ρT z /√κ, if ρT ∗ u = −1
h(κ) =
(A.24)
ρT z / κ, if ρT ∗ u = 1
Hence the extrema of h always occur at κ = κ. We now have all the ingredients needed to find the bounds
for ρzu . The rest of the proof proceeds in cases, depending on the values of (ρT y , ρT z , ρzy ).

Case I: κ̃ ∈
/ (κ, 1]. In this case, the second derivative of f with respect to ρT ∗ u has the same sign for all
κ ∈ (κ, 1]. There are two sub-cases.
(a) Suppose first that the second derivative of f is positive. Since this occurs when κ < κ̃, it is equivalent to
ρT y ρT z − κρzy > 0 for all κ ∈ (κ, 1] given that κ̃ ∈
/ (κ, 1]. In this case the function g from Equation A.21
describes the global minimum for ρuz and is a strictly increasing function of κ. Thus, the minimum
value for ρuz equals g(κ) = −1. The global maximum thus occurs at either ρT ∗ u = −1 or 1 and requires
√
us to make h(κ) positive. Thus, the upper bound for ρuz equals |ρT z |/ κ.
(b) Suppose next that the second derivative of f is negative. Since this occurs when κ > κ̃, it is equivalent to
ρT y ρT z − κρzy < 0 for all κ ∈ (κ, 1] given that κ̃ ∈
/ (κ, 1]. In this case the function g from Equation A.21
describes the global maximum for ρuz and is a strictly decreasing function of κ. Thus, the maximum
value for ρuz = g(κ) = 1. The global minimum thus occurs at either ρT ∗ u = −1 or 1 and requires us to
√
make h(κ) negative. Thus, the lower bound for ρuz equals −|ρT z |/ κ.

Case II: κ̃ ∈ (κ, 1]. This case is more complicated because the sign of the second derivative of f with
respect to ρT ∗ u now depends on κ. Again there are two sub-cases.
(a) Suppose first that ρT y ρT z − κρzy > 0 for κ < κ̃ and ρT y ρT z − κρzy < 0 for κ > κ̃. In this case, f is
strictly convex in ρT ∗ u for κ < κ̃. Accordingly, for a fixed κ < κ̃, g gives the minimum value of ρuz over
all ρT ∗ u . Moreover, g is strictly increasing for κ < κ̃ thus giving us a candidate minimum at κ. Since
ρT y ρT z − κρzu > 0 for κ < κ̃, we see from Equation A.23 that g(κ) = −1 hence ρuz = −1 is indeed
the minimum. Now, when κ > κ̃, f is a strictly concave function of ρT ∗ u so for any fixed κ > κ̃, g
gives the maximum value of ρuz over all ρT ∗ u . Moreover, g is strictly decreasing for κ > κ̃. Thus there
cannot be an interior maximum in this region. As mentioned above, when κ = κ̃ the extrema occur
either at ρT ∗ u = −1 or 1, so applying h we find that the maximum value of ρuz at κ = κ̃ is |ρT z |/κ̃.
Notice that this is equal to the limit of g(κ) as κ approaches κ̃ from the right. We have thus identified a
candidate maximum. It is not the global maximum, however, since h(κ) > h(κ̃). Thus, ρuz is maximized
√
at |ρT z |/ κ.
(b) Suppose next that ρT y ρT z − κρzy < 0 for κ < κ̃ and ρT y ρT z − κρzy > 0 for κ > κ̃. In this case, f is
strictly concave in ρT ∗ u for κ < κ̃. Accordingly, for a fixed κ < κ̃, g gives the maximum value of ρuz
over all ρT ∗ u . Moreover, g is strictly decreasing for κ < κ̃ thus giving us a candidate maximum at κ.

51

Since ρT y ρT z − κρzu < 0 for κ < κ̃, we see from Equation A.23 that g(κ) = 1 hence ρuz = 1 is indeed
the maximum. Now, when κ > κ̃, f is a strictly convex function of ρT ∗ u so for any fixed κ > κ̃, g
gives the minimum value of ρuz over all ρT ∗ u . Moreover, g is strictly increasing for κ > κ̃. Thus there
cannot be an interior minimum in this region. As mentioned above, when κ = κ̃ the extrema occur
either at ρT ∗ u = −1 or 1, so applying h we find that the minimum value of ρuz at κ = κ̃ is −|ρT z |/κ̃.
Notice that this is equal to the limit of g(κ) as κ approaches κ̃ from the right. We have thus identified a
candidate minimum. It is not the global minimum, however, since h(κ) < h(κ̃). Thus, ρuz is minimized
√
at −|ρT z |/ κ.
√
From our proof of Proposition 2.2, we know that ρ2T z < κ which implies |ρT z |/ κ < 1, thus the one-sided
bounds are non-trivial. The result follows since ρT y ρT z − κρzy > 0 implies that we are either in case I(a) or
II(a), while ρT y ρT z − κρzy > 0 implies that we are either in case I(b) or II(b).
Proof of Corollary 2.2. First, by Equation 7 β = βIV − σuz /σT z = (ρzy σy − ρuz σu )/(ρT z σT ). Now,
combining Proposition 2.1 and Equation 15,
"
!
#
q
∗u
σy
ρ
T
ρT z κ − ρ2T y p
− (ρT y ρT z − κρzy )
ρuz σu =
κ
1 − ρ2T ∗ u
The result follows since ρT ∗ u can take on any value in (−1, 1) and ρT ∗ u /(1 − ρ2T ∗ u )1/2 tends to +∞ when
ρT ∗ u approaches 1 and −∞ when it approaches −1.

A.2
A.2.1

Inference
Draws for the Reduced Form Parameters

This appendix provides details of our first proposal for drawing the reduced form parameters Σ from Section
3.1 using on a large-sample approximation. Let
εT

=

(y − E[y]) − βT (T − E[T ])

εz

=

(y − E[y]) − βz (z − E[z])

where βT = σT y /σT2 , and βz = σzy /σz2 . While neither βT nor βz equals the true treatment effect β, the
parameters of both of these regressions are identified. Under the standard regularity conditions for linear
regression, we have
 √ 
 


n βbT − βT
 √ 
  → d B MT
(A.25)
Mz
n βbz − βz
where βbT = σ
bT y /b
σT2 and βbz = σ
bzy /b
σz2 are the OLS estimators of βT and βz , (MT , Mz )0 ∼ N (0, V ), and




1/σT2
0
T 2 ε2T
zT εz εT
B=
,
V
=
E
.
(A.26)
0
1/σz2
zT εz εT
z 2 ε2z
Note that V depends not on the structural error u but on the reduced form errors εT , εz . By construction εT
is uncorrelated with T and εz is uncorrelated with z but the reduced form errors are necessarily correlated
with each other. Now, using Equations A.25 and A.26 we see that


 

 √
σT y − σT y )
MT
MT
−1
√n (b
→d B B
=
(A.27)
n (b
σzy − σzy )
Mz
Mz
and thus, in large samples


σ
bT y
σ
bzy




≈N

52

σT y
σzy




, Vb /n

(A.28)

where Vb is the textbook robust variance matrix estimator, namely
n

1X
Vb =
n i=1



Ti2 ε̂2T i
zi Ti ε̂zi ε̂T i

zi Ti ε̂zi ε̂T i
zi2 ε̂2zi



where εbT i denotes the ith residual from the βT regression and εbzi the ith residual from the βz regression.
Since we are working solely with identified parameters, the usual large-sample equivalence between a Bayesian
posterior and frequentist sampling distribution holds. Accordingly, we propose to generate draws for σT y
and σzy according to
"
#



(j)
σT y
σ
bT y
b /n
∼
N
,
V
(A.29)
(j)
σ
bzy
σzy
Combining these draws with the fixed values σ
bT2 , σ
bz2 and σ
bzT , since we are conditioning on z and T , yields
posterior draws for Σ based on a large-sample normal approximation, namely


(j)
σ
bT2 σT y σ
bT z

(j) 
(A.30)
Σ(j) =  σT(j)y σ
by2 σzy 
(j)
2
σ
bz
σ
bT z σzy

A.2.2

Uniform Draws on the Conditional Identified Set

There are at least two different methods of placing a conditionally uniform prior on Θ. The first, and
simplest, draws ρT ∗ u and κ uniformly and independently on (−1, 1) × (κ, 1] ∩ R and then solves for ρuz
at each draw using Equation 13. The second method, which we use in this paper, draws uniformly on
intersection of the manifold (ρuz , ρT ∗ u , κ) – defined by 13 and the identified set for (ρT ∗ u , κ) described in
Proposition 2.2 – with any user restrictions R. This method begins by making the same draws as described
in the first method, but proceeds to re-weight them based on the local surface area of the identified set at
that point (Melfi and Schoier, 2004). By local surface area we refer to the quantity
s

2
2 
∂ρuz
∂ρuz
M (ρT ∗ u , κ) = 1 +
(A.31)
+
∂ρT ∗ u
∂κ
which Apostol (1969) calls the “local magnification factor” of a parametric surface. The derivatives required
to evaluate the function M are
∂ρuz
ρT ∗ u (ρT y ρT z − κρzy )
ρT z
= √ +r 

∂ρT ∗ u
κ
κ κ − ρ2T y (1 − ρ2T ∗ u )
v
(
"
#)
u
∂ρuz
ρT ∗ u ρT z u 1 − ρ2T ∗ u
1
1
1
 ρzy + (ρT y ρT z − κρzy )
=−
+
.
+t 
∂κ
2
κ κ − ρ2T y
2κ3/2
κ κ − ρ2
Ty

(`)

To accomplish the re-weighting, we first evaluate M (`) = M (ρT ∗ u , κ(`) ) at each draw `that was accepted
 in
(`) (`)
(`)
(`)
the first step. We then calculate Mmax = max`=1,...,L M and resample the draws ρuz , ρT ∗ u , κ
with
probability p(`) = M (`) /Mmax .

53

B
B.1

Appendices for Binary Treatment Case
Proofs

Lemma B.1. For k = 0, 1,
p∗ =

p − α0
,
1 − α0 − α1

p∗k =

pk − α0
,
1 − α0 − α1

1 − p∗ =

1 − p − α1
1 − α0 − α1

1 − p∗k =

1 − pk − α1
1 − α0 − α1

Proof. By the Law of Total Probability,
p = (1 − α1 )p∗ + α0 (1 − p∗ ) = (1 − α0 − α1 )p∗ + α0
The result for p∗ and 1 − p∗ follows after rearranging. The argument for p∗k and 1 − p∗k is identical.
Lemma B.2.
P(T ∗ = 0|T = 0, zk )

=

P(T ∗ = 1|T = 0, zk )

= α1 p∗k /(1 − pk )

∗

(1 − α0 )(1 − p∗k )/(1 − pk )

P(T = 0|T = 1, zk )

= α0 (1 − p∗k )/pk

P(T ∗ = 1|T = 1, zk )

=

(1 − α1 )p∗k /pk .

Proof. The result follows from Bayes’ rule and Assumption 5.1.
Lemma B.3. Cov(T ∗ , T ) = Var(T ∗ )(1 − α0 − α1 )
Proof. By the Law of Total Probability p = (1 − α1 ) p∗ + α0 (1 − p∗ ). Hence,
Cov(T, T ∗ ) = P(T = 1, T ∗ = 1) − pp∗ = (1 − α1 ) p∗ − [(1 − α1 ) p∗ + α0 (1 − p∗ )] p∗ = Var(T ∗ )(1 − α0 − α1 ).

Lemma B.4. Cov(T ∗ , w) = Cov(T, T ∗ ) − Var(T ∗ ) = −Var(T ∗ )(α0 + α1 )
Proof. Since w = T − T ∗ , by Lemma B.3 Cov(T ∗ , w) = Cov(T, T ∗ ) − Var(T ∗ ) = −Var(T ∗ )(α0 + α1 ).
Lemma B.5. Cov(z, w) = −(α0 + α1 )Cov(z, T ∗ )
Proof. By iterated expectations and Assumption 5.1 E(zw|T ∗ = 0) = α0 E(z|T ∗ = 0) and similarly
E(zw|T ∗ = 1) = −α1 E(z|T ∗ = 1). Hence, E(zw) = α0 (1 − p∗ )E(z|T ∗ = 0) − α1 p∗ E(z|T ∗ = 1). Now,
since E(zT ∗ ) = p∗ E(z|T ∗ = 1), we have E(z)E(T ∗ ) = p∗ E(z|T ∗ = 1) − Cov(z, T ∗ ) Expanding E(z)E(T )
using iterated expectations,
E(z)E(T ) = [p∗ E(z|T ∗ = 1) + (1 − p∗ )E(z|T ∗ = 0)] [p∗ (1 − α1 ) + (1 − p∗ )α0 ]
= (1 − α1 )p∗ (1 − p∗ )E[z|T ∗ = 0] + α0 p∗ (1 − p∗ )E[z|T ∗ = 1]
+ (1 − α1 )(p∗ )2 E[z|T ∗ = 1] + α0 (1 − p∗ )2 E[z|T ∗ = 0]
Finally, substituting the expressions for E(zw), E(z)E(T ∗ ) and E(z)E(T ∗ ) and simplifying,
Cov(z, w)

= E(z, w) − E(z)E(w) = E(z, w) − [E(z)E(T ) − E(z)E(T ∗ )]
= E(z, w) − E(z)E(T ) + [p∗ E(z|T ∗ = 1) − Cov(z, T ∗ )]
=

(1 − α0 − α1 )p∗ (1 − p∗ ) [E(z|T ∗ = 1) − E(z|T ∗ = 0)] − Cov(z, T ∗ )

=

(1 − α0 − α1 )Var(T ∗ )Cov(z, T ∗ )/Var(T ∗ ) − Cov(z, T ∗ )

=

−(α0 + α1 )Cov(z, T ∗ )

54

Lemma B.6. (1 − α0 − α1 )Cov(z, T ∗ ) = Cov(z, T )
Proof. This follows from Cov(z, T ) = Cov(z, T ∗ ) + Cov(z, w) by substituting Lemma B.5.
Lemma B.7. In the absence of covariates βIV = β/(1 − α0 + α1 ) + σzu /σzT .
Proof. Using Lemma B.6, the result follows from βIV = [βCov(z, T ∗ ) + Cov(z, u)]/Cov(z, T ).
Lemma B.8. Cov(w, u) = −(α0 + α1 )Cov(T ∗ , u)
Proof. By iterated expectations E[wu] = −α1 p∗ E[u|T ∗ = 1] + α1 (1 − p∗ )E[u|T ∗ = 0], using Assumption
5.1. Applying iterated expectations two more times, we find that
Cov(T ∗ , u) = E[T ∗ u] − E[u]E[T ∗ ] = p∗ E[u|T ∗ = 1] − cp∗
E[u] = c = p∗ E[u|T ∗ = 1] + (1 − p∗ )E[u|T ∗ = 0].
Solving the first of the two preceding equalities for p∗ E[u|T ∗ = 1], the second for (1 − p∗ )E[u|T ∗ = 0], and
substituting into the expression for E[wu] gives
E[wu]

= −α1 [Cov(T ∗ , u) + cp∗ ] + α0 (c − p∗ E [u|T ∗ = 1])
= −α1 [Cov(T ∗ , u) + cp∗ ] + α0 {c − [Cov(T ∗ , u) + cp∗ ]}
= −(α0 + α1 )Cov(T ∗ , u) + α0 c − cp∗ (α0 + α1 )

Now, since E[w|T ∗ = 0] = α0 and E[w|T ∗ = 1] = −α1 , we have E[w] = (1 − p∗ )α0 − p∗ α1 . Therefore,
Cov(w, u) = E[wu] − E[w]E[u] = −(α0 + α1 )Cov(T ∗ , u) + α0 c − cp∗ (α0 + α1 ) − [(1 − p∗ )α0 − p∗ α1 ] c
= −(α0 + α1 )Cov(T ∗ , u).

Lemma B.9. In the absence of covariates,



1
(p − α0 )(1 − p − α1 )
βOLS =
β + (1 − α0 − α1 )σT ∗ u .
p(1 − p)
1 − α0 − α1
Proof. The result follows from βOLS = [βCov(T, T ∗ ) + Cov(T ∗ , u) + Cov(w, u)]/Var(T ) by substituting
Lemmas B.3 and B.8.
Lemma B.10. Cov(z, u)/Cov(z, T ) = δz /(p1 − p0 )
Proof. By iterated expectations, E(zu) = qE(u|z = 1) and E(u) = qδz + E(u|z = 0). Thus,
Cov(z, u) = qE(u|z = 1) − q [qδz + E(u|z = 0)] = q(1 − q)δz .
Similarly, E(zT ) = qp1 and E(T ) = p1 q + p0 (1 − q). Thus,
Cov(z, T ) = qp1 − q [p1 q + p0 (1 − q)] = q(1 − q)(p1 − p0 ).

Lemma B.11 (Equations 28 and 29). Under Assumption 5.1,
ye0k = (β + m∗1k )α1 p∗k + (1 − α0 )(1 − p∗k )m∗0k

ye1k = (β + m∗1k )(1 − α1 )p∗k + α0 (1 − p∗k )m∗0k .

55

Proof. By the iterated expectations and Lemma B.2,
E [u|T = 0, zk ] = ET ∗ |T =0,zk [E [u|T ∗ , T = 0, zk ]] = ET ∗ |T =0,zk [E [u|T ∗ , zk ]]
= P(T ∗ = 1|T = 0, zk )m∗1k + P(T ∗ = 0|T = 0, zk )m∗0k
=

α1 p∗k ∗
(1 − α0 )(1 − p∗k ) ∗
m1k +
m0k .
1 − pk
1 − pk

Analogously,
E [u|T = 1, zk ] =

α0 (1 − p∗k ) ∗
(1 − α1 )p∗k ∗
m1k +
m0k .
pk
pk

The result follows by combining these expressions with ȳtk = βE[T ∗ |T = t, zk ] + E [u|T = t, zk ].
Lemma B.12 (Relating δT ∗ and δz to m∗tk .).




1 − p0 − α1 ∗
p1 − α0 ∗
1 − p1 − α1 ∗
p0 − α0 ∗
δT ∗ = (1 − q)
m10 −
m00 + q
m11 −
m01
(B.1)
p − α0
1 − p − α1
p − α0
1 − p − α1
1
[(p1 − α0 )m∗11 − (p0 − α0 )m∗10 + (1 − p1 − α1 )m∗01 − (1 − p0 − α1 )m∗00 ] (B.2)
δz =
1 − α0 − α1

Proof. By iterated expectations and Bayes’ Rule,
E[u|T ∗ = 1]

= Ez|T ∗ =1 [E [u|T ∗ = 1, z]] = P(z = 0|T ∗ = 1)m∗10 + P(z = 1|T ∗ = 1)m∗11
=

p∗1 q ∗
1
p∗0 (1 − q) ∗
m
+
m11 =
[(p0 − α0 )(1 − q)m∗10 + (p1 − α0 )qm∗11 ]
10
∗
∗
p
p
p − α0

where the final equality follows from Lemma B.1. Similarly
E[u|T ∗ = 0]

=

1
[(1 − p0 − α1 )(1 − q)m∗00 + (1 − p1 − α1 )qm∗01 ]
1 − p − α1

The result for δT ∗ now follows from δT ∗ = E[u|T ∗ = 1] − E[u|T ∗ = 0]. Proceeding in the same way
E[u|z = k]

= ET ∗ |z=k [E (u|z = k, T ∗ )] =

1
[(pk − α0 )m∗1k + (1 − pk − α1 )m∗0k ]
1 − α0 − α1

and the result for δz follows from δz = E[u|z = 1] − E[u|z = 0].
Proof of Proposition 5.1. To begin, define
g(α1 ) = (e
y01 − ye00 ) − α1 [(e
y01 − ye00 ) + (e
y11 − ye10 )]
[(1 − q)e
y00 + qe
y01 ] − α1 {[(1 − q)e
y00 + qe
y01 ] + [(1 − q)e
y10 + qe
y11 ]}
1 − p − α1
(1 − α0 )e
y10 − α0 ye00
(1 − α0 )e
y11 − α0 ye01
∆(α0 ) =
−
p0 − α0
p1 − α0
h(α1 ) =

The proof proceeds as follows. First substitute for p∗k using Lemma B.1 in the expression for ye0k from
Equation 28 and then solve for (β + m1k∗ ) to yield
β + m∗1k

=

(1 − α0 − α1 )e
y0k − (1 − α0 )(1 − pk − α1 )m∗0k
α1 (pk − α0 )

Now, substituting the preceding equality into the expression for ye1k from Equation 29, again replacing for

56

p∗k using Lemma B.1 and rearranging, we find that
m∗0k

(1 − α1 )e
y0k − α1 ye1k
1 − pk − α1

=

(B.3)

Next, summing Equations 28 and 29, and solving for (β + m∗1k ) we obtain
(β + m∗1k )

(e
y0k + ye1k ) − (1 − p∗k )m∗0k
.
p∗k

=

(B.4)

Now we subtract the preceding equation evaluated at k = 1 from the same evaluated at k = 0, yielding

 

ye00 + ye10
ye01 + ye11
(1 − p∗` )m∗01
(1 − p∗0 )m∗00
m∗10 − m∗11 =
−
+
−
.
p∗0
p∗1
p∗1
p∗0
Now we eliminate m∗00 and m∗01 from the preceding using Equation B.3 to obtain
m∗10 − m∗11 =

(1 − α0 )e
y11 − α0 ye01
(1 − α0 )e
y10 − α0 ye00
−
.
p0 − α0
p1 − α0

Next we eliminate m∗0k from Equations B.1 and B.2 again using Equation B.3. We obtain,





(1 − q)(p0 − α0 )
q(p1 − α0 )
∗
∗
∗
δT = m10
+ m11
− h(α1 )
p − α0
p − α0
and
δz =

g(α1 )
(p1 − α0 )m∗11 − (p0 − α0 )m∗10
+
1 − α0 − α1
1 − α0 − α1

(B.5)

(B.6)

(B.7)

Equations B.5, B.6 and B.7 constitute an over-determined system of linear equations in (m∗10 , m∗11 ), namely
m∗10 − m∗11
(1 −

q)(p0 − α0 )m∗10 + q(p1
(p0 − α0 )m∗10 − (p1

−
−

α0 )m∗11
α0 )m∗11

=

∆(α0 )

(B.8)

=

(p − α0 ) [δT ∗ + h(α1 )]

(B.9)

= g(α1 ) − (1 − α0 − α1 )δz

(B.10)

Substituting Equation B.8 into B.10 to eliminate m∗10 and rearranging yields
(p0 − α0 ) [m∗11 + ∆(α0 )] − (p1 − α0 )m∗11 = g(α1 ) − (1 − α0 − α1 )δz
and thus
m∗11 =



g(α1 ) − (1 − α0 − α1 )δz − (p0 − α0 )∆(α0 )
p0 − p1



while making the same substitution into Equation B.9 yields


(p − α0 ) [δT ∗ + h(α1 )] − (1 − q)(p0 − α0 )∆(α0 )
∗
.
m11 =
(1 − q)(p0 − α0 ) + q(p1 − α0 )
Finally, equating the two preceding expressions we see that




(1 − q)(p0 − α0 )∆(α0 )
g(α1 ) − (1 − α0 − α1 )δz − (p0 − α0 )∆(α0 )
δT ∗ + h(α1 ) −
=
p − α0
p0 − p1
using the fact that (1 − q)p0 + qp1 = p.

57

(B.11)

(B.12)

Lemma B.13. Under Assumption 5.1,
2
2
(1 − α1 )(1 − pk )σ0k
− α1 pk σ1k
α1 (1 − α1 )pk (1 − pk )(ȳ1k − ȳ0k )2
−
1 − pk − α1
(1 − pk − α1 )2
2
2
(1 − α0 )pk σ1k
− α0 (1 − pk )σ0k
α0 (1 − α0 )pk (1 − pk )(ȳ1k − ȳ0k )2
s∗2
.
−
1k =
pk − α0
(pk − α0 )2


Proof of Lemma B.13. First E(y 2 |T, z) = ET ∗ |T,z E(y 2 |T ∗ , z) and E(y|T, z) = ET ∗ |T,z [E(y|T ∗ , z)] by
iterated expectations. Next, by Lemma B.2,

s∗2
0k =

E(y 2 |T = 0, zk )

=

E(y|T = 0, zk )

=

 (1 − α0 )(1 − p∗k )  2 ∗

α1 p∗k 
E (β + u)2 |T ∗ = 1, zk +
E u |T = 0, zk
1 − pk
1 − pk
(1
−
α
)(1
− p∗k )
α1 p∗k
0
E [β + u|T ∗ = 1, zk ] +
E [u|T ∗ = 0, zk ]
1 − pk
1 − pk

using the fact that y = βT ∗ + u. Combining these and simplifying yields,
2
σ0k
=

α1 p∗k ∗2 (1 − α0 )(1 − p∗k ) ∗2
s +
s0k + V0k (α0 , α1 )(β + m∗1k − m∗0k )2
1 − pk 1k
1 − pk

where
V0k (α0 , α1 ) =

(B.13)

α1 (1 − α0 )(pk − α0 )(1 − pk − α1 )
(1 − pk )2 (1 − α0 − α1 )2

(B.14)

Similarly,
E(y 2 |T = 1, zk )

=

E(y|T = 1, zk )

=

and thus
2
σ1k
=

 α0 (1 − p∗k )  2 ∗

(1 − α1 )p∗k 
E (β + u)2 |T ∗ = 1, zk +
E u |T = 0, zk
pk
pk
α0 (1 − p∗k )
(1 − α1 )p∗k
E [β + u|T ∗ = 1, zk ] +
E [u|T ∗ = 0, zk ]
pk
pk

(1 − α1 )p∗k ∗2 α0 (1 − p∗k ) ∗2
s1k +
s0k + V1k (α0 , α1 )(β + m∗1k − m∗0k )2
pk
pk

where
V1k (α0 , α1 ) =

(B.15)

α0 (1 − α1 )(pk − α0 )(1 − pk − α1 )
p2k (1 − α0 − α1 )2

(B.16)

Now, combining Equations B.3 and B.4 from the proof of Proposition 5.1,
β + m∗1k − m∗0k = (1 − α0 − α1 )

(1 − pk )e
y1k − pk ye0k
.
(pk − α0 )(1 − pk − α1 )

(B.17)

Substituting Equations B.14 and B.17 into Equation B.13, and Equations B.16 and B.17 into Equation B.15,
2

2
σ0k

=

α1 p∗k ∗2 (1 − α0 )(1 − p∗k ) ∗2 α1 (1 − α0 )p2k (ȳ1k − ȳ0k )
s +
s0k +
1 − pk 1k
1 − pk
(pk − α0 )(1 − pk − α1 )

2
σ1k

=

(1 − α1 )p∗k ∗2 α0 (1 − p∗k ) ∗2 α0 (1 − α1 )(1 − pk )2 (ȳ1k − ȳ0k )
s1k +
s0k +
pk
pk
(pk − α0 )(1 − pk − α1 )

2

∗2
The result follows by solving these equations for s∗2
0k and s1k .

Lemma B.14. Under Assumptions 5.1 and 5.2,
α0 ≤ min {pk } , α1 ≤ min {1 − pk } .
k

k

These inequalities are strict unless p∗k is zero or one.

58

(B.18)

Proof of Lemma B.14. Rearranging the result of Lemma B.1,
pk − α0 = (1 − α0 − α1 )p∗k
(1 − pk ) − α1 = (1 − α0 − α1 )(1 − p∗k ).
Now, since p∗k and (1 − p∗k ) are probabilities they are between zero and one which means that the sign of
pk − α0 as well as that of (1 − pk ) − α1 are both determined by that of 1 − α0 − α1 . Thus, under Assumption
5.2, α0 ≤ pk and α1 ≤ (1 − pk ) for all k.
Proof of Proposition 5.2. We first derive the bounds for α0 and α1 . We then show that these bounds
∗2
are sharp and that our assumptions imply no restrictions on δT ∗ . By assumption, s∗2
0k , s1k > 0. Thus, by
Lemma B.13,


2
2
(pk − α0 ) (1 − α0 )pk σ1k
− α0 (1 − pk )σ0k
> α0 (1 − α0 )pk (1 − pk )(ȳ1k − ȳ0k )2
(B.19)


2
2
2
(1 − pk − α1 ) (1 − α1 )(1 − pk )σ0k − α1 pk σ1k > α1 (1 − α1 )pk (1 − pk )(ȳ1k − ȳ0k )
(B.20)
Thus, for each k we obtain a pair of quadratic inequalities that bound α0 and α1 .
Consider first Inequality B.19. Notice that both α0 = 0 and α0 = 1 satisfy the inequality. In contrast
α0 = pk does not: the left hand side becomes zero while the right hand side is strictly positive. This
implies that the quadratic equation defining the boundary of the inequality crosses the α0 -axis and thus
has two real roots. Moreover, one of these is strictly less than pk . Rearranging Inequality B.19, we have
Ak α02 + Bk0 α0 + Ck0 > 0 where
2
2
Ak = pk (1 − pk )(ȳ1k − ȳ0k )2 + (1 − pk )σ0k
+ pk σ1k


2
2
Bk0 = − σ1k
pk (1 + pk ) + pk (1 − pk )σ0k
+ pk (1 − pk )(ȳ1k − ȳ0k )2
2
Ck0 = p2k σ1k
.

Since Ak > 0, the quadratic equation defined by Ak α12 + Bk0 α1 + Ck0 = 0 opens upwards. By Lemma B.14,
α0 < pk so we need only consider the smaller of the two roots. Our bound imposes that α0 be strictly less
than this quantity. Analogous reasoning for Inequality B.20, using 1 − pk rather than pk , shows that the
smaller of the two roots of Ak α12 + Bk1 α1 + Ck1 = 0 bounds α1 from above, where
2
2
Ak = pk (1 − pk )(ȳ1k − ȳ0k )2 + (1 − pk )σ0k
+ pk σ1k


2
2
Bk1 = − pk (1 − pk )σ1k
+ (1 − pk )(2 − pk )σ0k
+ pk (1 − pk )(ȳ1k − ȳ0k )2
2
Ck1 = (1 − pk )2 σ0k
.

Notice that the coefficient of the squared term, Ak , is common to both quadratics. Because the bounds
for α0 and α1 hold for each k, we can take the tighter of each. Now, equating f1k (α0 ) and f0k (α0 ) and
rearranging gives precisely Ak α02 + Bk0 α0 + Ck0 = 0. Equating the inverse functions
−1
f0k
(α1 ) =

2
pk (1 − pk − α1 )σ0k
− p2k (ȳ1k − ȳ0k )2 α1
2
(1 − pk − α1 )σ0k − p2k (ȳ1k − ȳ0k )2 α1

−1
f1k
(α1 ) =

2
pk (1 − pk − α1 )σ1k
2 + (1 − p )2 (ȳ
2
(1 − pk − α1 )σ1k
k
1k − ȳ0k ) (1 − α1 )

and rearranging gives precisely Ak α12 + Bk1 α1 + Ck1 = 0.
We now show that (−∞, ∞) × [0, ᾱ0 ) × [0, ᾱ1 ) is the sharp identified set for (δT ∗ , α0 , α1 ). Because all of
∗2
our steps from above are reversible, so long as ȳ1k 6= ȳ0k , any (α0 , α1 ) ∈ [0, α¯0 ) × [0, ᾱ1 ) implies s∗2
0k , s1k > 0.
∗
∗
Moreover, at any pair (α0 , α1 ) within these bounds, p and pk all lie in the interval [0, 1]. It remains only
∗2
to show that δT ∗ is unrestricted. Notice that the expressions for s∗2
0k and s1k from Lemma B.13 involve only
∗
∗
α0 , α1 and observables: they do not involve mtk . The mtk are only constrained by observables through
Equations 28 and 29. For any fixed values of (α0 , α1 ) these constitute a linear system of four equations in
five unknowns: β, m∗00 , m∗01 , m∗10 and m∗11 . This means that we can solve for each of the other unobservables

59

for any value of the free parameter m∗11 and still satisfy the assumptions of the model. The result follows
since δT ∗ can be written as a linear function of m∗11 and observables only, for any fixed values of (α0 , α1 ),
using Equation B.12.
Lemma B.15. Under Assumption 5.3, (i) P(z = 1|T ∗ , T ) = P(z = 1|T ∗ ), and (ii) E [x|T ∗ , T ] = E [x|T ∗ ].
Proof. By Bayes’ Rule and Assumption 5.3 (iii),
P (z = 1|T ∗ = t∗ , T = t) =

P(T = t|T ∗ = t∗ )P (z = 1|T ∗ = t∗ )
= P(z = 1|T ∗ = 1)
P(T = t|T ∗ = t∗ )

which proves part (i). Now, by iterated expectations and Assumption 5.3 (i)
E[x|T ∗ , T ] = Ez|T ∗ ,T [E (x|T ∗ , T, z)] = Ez|T ∗ ,T [E (x|T ∗ , z)]
and
E [x|T ∗ ] = Ez|T ∗ [E (x|T ∗ , z)]
Part (ii) now follows by part (i), which shows that that the conditional distribution of z given T ∗ , T is the
same as the conditional distribution of z given T ∗ .
Lemma B.16.
E (x|T ∗ = 1) − E (x|T ∗ = 0) =

p(1 − p)(1 − α0 − α1 )
[E (x|T = 1) − E (x|T = 0)]
(p − α0 )(1 − p − α1 )

Proof. By Lemma B.15 (ii), E[x|T ∗ , T ] = E[x|T ∗ ] and hence
E[x|T = 1]

= P(T ∗ = 1|T = 1)E [x|T ∗ = 1] + P(T ∗ = 0|T = 1)E [x|T ∗ = 0]

E[x|T = 0]

= P(T ∗ = 1|T = 0)E [x|T ∗ = 1] + P(T ∗ = 0|T = 0)E [x|T ∗ = 0]

by the law of iterated expectations. Now, by Bayes’ rule,
P(T ∗ = 1|T = 1) = (1 − α1 )p∗ /p,

P(T ∗ = 0|T = 1) = α0 (1 − p∗ )/p

P(T ∗ = 1|T = 0) = α1 p∗ /(1 − p),

P(T ∗ = 0|T = 0) = (1 − α)(1 − p∗ )/(1 − p)

and substituting these four expressions into the preceding two along with Lemma B.1 we find that
pE [x|T = 1]

=

(1 − p)E [x|T = 0]

=

(1 − α1 )(p − α0 )
α0 (1 − p − α1 )
E [x|T ∗ = 1] +
E [x|T ∗ = 0]
1 − α0 − α1
1 − α0 − α1
(1 − α0 )(1 − p − α1 )
α1 (p − α0 )
E [x|T ∗ = 1] +
E [x|T ∗ = 0]
1 − α0 − α1
1 − α0 − α1

This is a system of two equations in two unknowns. The result follows by solving and rearranging.
Lemma B.17.
δz
δT ∗

0
[E (x|z = 1) − E (x|z = 0)] γ + δez
p(1 − p)(1 − α0 − α1 )
0
[E (x|T = 1) − E (x|T = 0)] γ + δeT ∗
=
(p − α0 )(1 − p − α1 )

=

(B.21)
(B.22)

0
Proof. Since u = c + x0 γ + ε where c is a constant, δz = [E(x|z = 1) − E(x|z = 0)] γ + δez and similarly

δT ∗

0

= E[u|T ∗ = 1] − E[u|T ∗ = 0] = [E(x|T ∗ = 1) − E(x|T ∗ = 0)] γ + δeT ∗
p(1 − p)(1 − α0 − α1 )
0
=
[E (x|T = 1) − E (x|T = 0)] γ + δeT ∗
(p − α0 )(1 − p − α1 )

60

where the final equality follows from Lemma B.16.
Lemma B.18. The probability limit of the IV estimators of β and γ is given by

#

 "  zT
 zT 
0
βIV
σ
σ σzT ∗ + σ zx σ xT ∗ β
=
+ σzε

xT
xT
xx
γ IV
σ
γ + σ σzT ∗ + Σ σ xT ∗ β
where


σ zT
σ xT

0

σ zx
Σxx




≡

σzT
σ xT

σ 0zx
Σxx

−1

e be the de-meaned version of y and so on. Then we have ye = β Te∗ + x
e0 γ + ε using the fact
Proof. Let y
that, since ε is mean zero ε = εe. Stacking observations in the usual way,


βbIV
γ
bIV



"

e
e
z0 T/n
0
e
e T/n
X



σ zT
σ xT

=
→p

e
e
z0 X/n
0
e X/n
e
X
0  
zx

σ
Σxx

#−1 ("

#
  0
)
e ∗ /n e
e
e
z ε/n
e
z0 T
zX/n
β
+
e 0 ε/n
e ∗ /n X
e 0T
e X/n
e
γ
X
X

 

σ 0zx
β
σzε
+
Σxx
γ
0

σzT ∗
σ xT ∗

under standard regularity conditions. The result follows since
  0 
 zT
0 
σ 0zx
0
σ
σ zx
=
Σxx
I
σ xT Σxx
by the definition of an inverse matrix.
Lemma B.19. σ xT ∗ = σ xT (σzT ∗ /σzT )
Proof. Since the result is an element-wise equality between two vectors we need only show that
Cov(x, T ∗ )/Cov(x, T ) = Cov(z, T ∗ )/Cov(z, T )
for an arbitrary element x of x. By the Law of Iterated Expectations,
Cov(z, T ∗ ) = p∗ [E (z|T ∗ = 1) − E(z)] ,

Cov(z, T ) = p [E (z|T = 1) − E(z)]

Cov(x, T ∗ ) = p∗ [E (x|T ∗ = 1) − E(x)] ,

Cov(x, T ) = p [E (x|T = 1) − E(x)]

and hence we have
E(z|T ∗ = 1) − E(z)
Cov(z, T ∗ )
=
,
∗
Cov(x, T )
E(x|T ∗ = 1) − E(x)

Cov(z, T )
E(z|T = 1) − E(z)
=
Cov(x, T )
E(x|T = 1) − E(x)

By iterated expectations and Lemma B.15 (i),
E(z|T = 1) = (1 − α1 )

p∗
α0 (1 − p∗ )
E(z|T ∗ = 1) +
E(z|T = 0)
p
p

61

but by the Law of total probability (1 − p∗ )E(z|T ∗ = 0) = E(z) − p∗ E(z|T ∗ = 1) and hence
1
[(1 − α1 )p∗ E(z|T ∗ = 1) + α0 (1 − p∗ )E(z|T = 0)]
p
1
= [(1 − α1 )p∗ E(z|T ∗ = 1) + α0 {E(z) − p∗ E(z|T ∗ = 1)}]
p
1
= [(1 − α0 − α1 )p∗ E(z|T ∗ = 1) + α0 E(z)]
p


1
p − α0
1
∗
=
(1 − α0 − α1 )
E(z|T = 1) + α0 E(z) = [(p − α0 )E(z|T ∗ = 1) + α0 E(z)]
p
1 − α0 − α1
p

E(z|T = 1) =

which implies
p
1
[(p − α0 )E(z|T ∗ = 1) + α0 E(z)] − E(z)
p
p
1
p − α0
= [(p − α0 )E(z|T ∗ = 1) − (p − α0 )E(z)] =
[E(z|T ∗ = 1) − E(z)]
p
p

E(z|T = 1) − E(z) =

An identical argument with x in place of z gives
E(x|T = 1) − E(x) =

p − α0
[E(z|T ∗ = 1) − E(x)]
p

Therefore
Cov(z, T )
Cov(x, T )

E(z|T = 1) − E(z)
(p − α0 ) [E(z|T ∗ = 1) − E(z)] /p
=
E(x|T = 1) − E(x)
(p − α0 ) [E(x|T ∗ = 1) − E(x)] /p
∗
E(z|T = 1) − E(z)
Cov(z, T ∗ )
=
E(x|T ∗ = 1)
Cov(x, T ∗ )

=
=

Lemma B.20.




0
σ zT σzT ∗ + σ zx σ xT ∗ = 1/(1 − α0 − α1 ).

Proof. By the partitioned matrix inverse formula,
σ zT
σ zx

0

=

−1

(σzT − σ 0zx Σxx σ xT )

−1 0
−1 0
= −σzT
σ zx Σxx − σ xT σzT
σ zx

−1

Now, by the Woodbury matrix identity (A − BCD)−1 = A−1 + A−1 B C −1 − DA−1 B
Σxx −

−1
−1 0
σ xT σzT
σ zx

=

Σ−1
xx

+

Σ−1
xx σ xT

σzT −

−1
σ 0zx Σ−1
xx σ xT

σ 0zx Σ−1
xx

=

Σ−1
xx


I+

−1

DA−1 and hence

σ xT σ 0zx Σ−1
xx
σzT − σ 0zx Σ−1
xx σ xT



0

Substituting this into our expression for σ zx gives
0

σ zT σzT ∗ + σ zx σ xT ∗ =

σzT



σzT ∗
σ xT σ 0zx Σ−1
xx
−1 0
−1
−
σ
σ
Σ
I
+
σ xT ∗
zT zx xx
− σ 0zx Σxx σ xT
σzT − σ 0zx Σ−1
xx σ xT

Expressing the right-hand side over a common denominator and simplifying gives
0

σ zT σzT ∗ + σ zx σ xT ∗ =

σzT ∗ − σ 0zx Σxx σ xT ∗
σzT − σ 0zx Σxx σ xT

(B.23)

−1
using the fact that σzT
is a scalar and hence commutes. By Lemma B.6 we have σzT /(1 − α0 − α1 ) = σzT ∗
and by Lemma B.19 we have Cov(z, T )/Cov(z, T ∗ ) = Cov(x, T )/Cov(x, T ∗ ) for each element x of x which

62

implies σ xT /(1 − α0 − α1 ) = σ xT ∗ . The result follows by substituting into Equation B.23.
Lemma B.21. (σ xT σzT ∗ + Σxx σ xT ∗ ) = 0
Proof. By the partitioned matrix inverse formula
σ xT σzT ∗ + Σxx σ xT ∗

= −Σxx



σ xT
σ xT
σzT ∗ + Σxx σ xT ∗ = Σxx σ xT ∗ −
σzT ∗
σzT
σzT

The result follows since σ xT ∗ = σ xT (σzT ∗ /σzT ) by Lemma B.19.
Proposition B.1. The probability limit of the IV estimators of β and γ is given by

 

 zT 
βIV
β/(1 − α0 − α1 )
σ
e
=
+ δz q(1 − q)
γ IV
γ
σ xT
−1

where σ zT = (σzT − σ 0zx Σxx σ xT )

−1 0
and σ xT = − Σxx − σ xT σzT
σ zx

−1

−1
σ xT σzT
.

Proof. We show that δez q(1 − q) = σzε , after which the result follows by combining Lemmas B.18, B.20
and B.21. By iterated expectations, 0 = E[ε] = qE[ε|z = 1] + (1 − q)E[ε|z = 0]. Thus, it follows that
E [ε|z = 0] = −qE [ε|z = 1] /(1 − q) and substituting the definition of δez ,


1
e
E [ε|z = 1]
δz = E [ε|z = 1] − E [ε|z = 0] =
1−q
Now, since ε is mean zero σzε = E[zε] = E [zE [ε|z]] = qE[ε|z = 1] so that δez q(1 − q) = qE[ε|z = 1] = σzε .
Lemma B.22.
e 0 , α1 ) + S(α
e 0 , α1 )δeT ∗
δez = B(α
where
e 0 , α1 ) = S(α0 , α1 )F (α0 , α1 )C3 + B(α0 , α1 ) − C1 ,
B(α
S(α0 , α1 )F (α0 , α1 )C4 − C2 + 1
C1 =

σ 0zx γ IV
,
q(1 − q)

C2 = σ 0zx σ xT ,

e 0 , α1 ) =
S(α

C3 = σ 0xT γ IV ,

S(α0 , α1 )
,
S(α0 , α1 )F (α0 , α1 )C4 − C2 + 1

C4 = q(1 − q)σ 0xT σ xT ,

F (α0 , α1 ) = (1 − α0 − α1 )/[(p − α0 )(1 − p − α1 )], and B(α0 , α1 ), S(α0 , α1 ) are as defined in Proposition 5.1.
Proof. Rearranging the result of Lemma B.1, γ = γ
bIV − δez q(1 − q)σ xT . Substituting this into Equation
B.21 from Lemma B.22,


0
δz = [E (x|z = 1) − E (x|z = 0)] γ
bIV − δez q(1 − q)σ xT + δez


σ 0zx 
σ0 γ
bIV
=
γ
bIV − δez q(1 − q)σ xT + δez = zx
− σ 0zx σ xT − 1 δez
q(1 − q)
q(1 − q)
since E(x|z = 1) − E(x|z = 0), the coefficient from a regression of x on z, equals σ zx /[q(1 − q)]. Thus,
δz = C1 − (C2 − 1)δez .

63

(B.24)

Similarly, substituting into Equation B.22 from Lemma B.22,


0
δT ∗ = [E (x|T ∗ = 1) − E (x|T ∗ = 0)] γ
bIV − δez q(1 − q)σ xT + δeT ∗


p(1 − p)(1 − α0 − α1 )
0
=
[E (x|T = 1) − E (x|T = 0)] γ
bIV − δez q(1 − q)σ xT + δeT ∗
(p − α0 )(1 − p − α1 )

0 

p(1 − p)(1 − α0 − α1 )
σ xT
=
γ
bIV − δez q(1 − q)σ xT + δeT ∗
(p − α0 )(1 − p − α1 ) p(1 − p)


q(1 − q)(1 − α0 − α1 )σ 0xT σ xT e
(1 − α0 − α1 )σ 0xT γ
bIV
=
−
δz + δeT ∗
(p − α0 )(1 − p − α1 )
(p − α0 )(1 − p − α1 )




(1 − α0 − α1 )
(1 − α0 − α1 )
=
C3 −
C4 δez + δeT ∗
(p − α0 )(1 − p − α1 )
(p − α0 )(1 − p − α1 )
since E(x|T = 1) − E(x|T = 0), the coefficient from a regression of x on T , equals σ xT /[p(1 − p)]. Thus,
δT ∗ = F (α0 , α1 )C3 − F (α0 , α1 )C4 δez + δeT ∗ .

(B.25)

Now, substituting Equations B.24 and B.25 into Equation 32 and re-arranging,
 


S(α0 , α1 )
S(α0 , α1 )F (α0 , α1 )C3 + B(α0 , α1 ) − C1
+
δeT ∗ .
δez =
S(α0 , α1 )F (α0 , α1 )C4 − C2 + 1
S(α0 , α1 )F (α0 , α1 )C4 − C2 + 1

B.2

Inference: Draws for the Reduced Form Parameters

In this appendix we provide details of our algorithm
posterior
samples
from
 for producing


 for the reduced
b = (R0 W )−1 R0 y
parameters described in Section 5.5. Define W = 1 T X , R = 1 z X , and b
b converges in probability to the parameter b from the reduced form model y = W b + ρ
where the estimator b
and ρ is a reduced form error. Note that the first element of b is the constant term and the second element
is the reduced-form coefficient for T . The remaining elements are the reduced-form coefficients for x, namely
b Using the definition of b
b and the reduced form errors ρ,
γIV . Define the residuals ρbi = yi − wi0 b.
b = (R0 W )−1 R0 y = (R0 W )−1 R0 (W b + ρ) = b + (R0 W )−1 R0 ρ
b
√
b = (R0 W/n)−1 (R0 ρ/√n). Now, the conditional means of y given z and T can be
and thus n(b − b)
constructed from the parameters of the following regression model:
yi = ξ0 + ξT Ti + ξz zi + ξT z Ti × zi + ωi
where ωi is a reduced-form error that is correlated with xi . Defining
this as y = Aξ + ω. We estimate ξ by OLS leading to residuals ω
bi =
b
estimated parameters of this regression ξ are related as follows:

 
 b
ξ0
1 0 0 0
ȳ00
b
 ȳ01   1 0 1 0  

 
  ξT
 ȳ10  =  1 1 0 0  
 ξbz
ȳ11
1 1 1 1
bT z
| {z } |
{z
} | ξ{z
ȳ

Q

ξ



A = 1 T z T z we can write
b The cell means ȳtk and the
yi − a0i ξ.





}

in other words ȳ = Qb
ξ and similarly for the population parameters: µy = Qξ. Hence,
√


√
−1
n ȳ − µy = Q (A0 A/n) (A0 ω/ n).

64

b and ȳ we need to study the joint limiting behavior of A0 ω/√n
To determine
the joint distribution of b
√
and R0 ρ/ n. By the Central Limit Theorem for iid observations





√ 
n 
1 X ri ρi
R0 ρ/ √n
Mρ
d
=√
→
∼ N (0, Ξ).
A0 ω/ n
Mω
n i=1 ai ωi

We estimate Ξ as follows
n

X
b= 1
Ξ
n − 1 i=1



ri r0i ρb2i
ai r0i ρbi ω
bi

ri a0i ρbi ω
bi
ai a0i ω
bi2

"


=

b ρρ
Ξ
b
(Ξρω )0

b ρω
Ξ
b
Ξωω

#

where

 2

0
A0 diag ω
b A
R0 diag ρb2 R
ρ} diag {b
ω} A
b
b ρω = R diag {b
b
, Ξωω =
, Ξ
.
Ξρρ =
n−1
n−1
n−1
Stacking the two estimators on top of one another,
 # 
" √ 
 −1

 0 √ 

b−b
n b
(R0 W/n)−1
0
ΣRW
0
Mρ
R ρ/ √n
d
→
 =
√
A0 ω/ n
0
Q(A0 A/n)−1
Mω
0
QΣ−1
n ȳ − µy
AA
so we see that the joint limit distribution is N (0, H) with
 −1


0
(Σ−1
ΣRW
0
Ξρρ Ξρω
RW )
H=
−1
Ξωρ Ξωω
0
0
QΣAA
{z
}
|

0
0
(QΣ−1
AA )


.

Ξ

b is the constant
b IV , but the first element of b
We only require the joint sampling distribution for ȳ and γ
b
term while the second corresponds to βIV . We do not need to work explicitly with the constant since in the
equations we use covariances hence in effect de-mean everything. Accordingly, define S to be the submatrix
of H that contains everything except the first and second rows and columns. We have
 
 √ 
n βbIV − βIV
 √
 d
 n (b
 → N (0, S)
)
√ γ IV − γ IV
n ȳ − µy
Treating this as a Bayesian posterior, we draw according to

 

βIV
βbIV
b
 γ IV  ∼  γ
b IV  + N (0, S/n)
µy
ȳ
Note the block of Sb corresponding to ȳ has a very simple structure: since we assume that our data are iid
and the cell means are calculated for non-overlapping groups of individuals, the ȳtk are all independent and
 2
2
2
2
b yy = nSbyy = n(QΣ
b −1 )Ξ
b ωω (QΣ
b −1 )0 = n diag σ
nH
b00 /n00 , σ
b01
/n01 , σ
b10
/n10 , σ
b11
/n11
AA
AA
2
where σ
btk
is the sample variance of those y observations for which T = t and z = k and ntk is the correb
sponding sample size. Recall that we make our draws not from Sb but rather from S/n.
This final division
b
by n yields the familiar variance of the sampling distribution of the sample mean. The other blocks of H,
b are
from which we must remove some elements as described above to yield the corresponding blocks for S,
as follows:

0

−1 0
0
b bb = Σ
b −1 Ξ
b ρρ Σ
b −1
b by = Σ
b −1 Ξ
b
b yb = H
b by
H
, H
H
.
RW
RW
RW ρω QΣAA ,

65

